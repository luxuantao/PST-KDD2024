<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AudioLDM: Text-to-Audio Generation with Latent Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Haohe</forename><surname>Liu</surname></persName>
							<email>&lt;haohe.liu@surrey.ac.uk&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision, Speech and Sig-nal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zehua</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision, Speech and Sig-nal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinhao</forename><surname>Mei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision, Speech and Sig-nal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xubo</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision, Speech and Sig-nal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danilo</forename><surname>Mandic</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision, Speech and Sig-nal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision, Speech and Sig-nal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AudioLDM: Text-to-Audio Generation with Latent Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, Audi-oLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, Audi-oLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generating sound effects, music, or speech according to personalized requirements is of importance for various applications such as augmented and virtual reality, game development, and video editing. Traditionally, audio generation has been achieved through signal processing techniques <ref type="bibr" target="#b0">(Andresen, 1979;</ref><ref type="bibr" target="#b15">Karplus &amp; Strong, 1983)</ref>. In recent years, generative models <ref type="bibr" target="#b40">(Oord et al., 2016;</ref><ref type="bibr" target="#b12">Ho et al., 2020;</ref><ref type="bibr" target="#b54">Song et al., 2021;</ref><ref type="bibr" target="#b55">Tan et al., 2022)</ref>, either unconditional or condi-tioned on other modalities <ref type="bibr" target="#b24">(Kreuk et al., 2022;</ref><ref type="bibr" target="#b61">?elaszczyk &amp; Ma?dziuk, 2022)</ref>, have revolutionized this task. Previous studies primarily worked on the label-to-sound setting with a small set of labels <ref type="bibr">(Liu et al., 2021b;</ref><ref type="bibr" target="#b41">Pascual et al., 2022)</ref> such as the ten sound classes in the UrbanSound8K dataset <ref type="bibr" target="#b50">(Salamon et al., 2014)</ref>. In comparison, natural language is considerably more flexible than labels as they can include fine-grained descriptions of audio signals (e.g., pitch, acoustic environment, and temporal order). The task of generating audio prompted with natural language descriptions is known as text-to-audio (TTA) generation.</p><p>TTA systems are designed to generate a wide range of highdimensional audio signals. To efficiently model the data, we choose to develop the generative model in a learned compact latent space. Similar ideas have been adopted by DiffSound <ref type="bibr" target="#b60">(Yang et al., 2022)</ref>, which employs diffusion models to learn a discrete representation compressed from the mel-spectrogram of audio. DiffSound has been superseded by the autoregressive model in a discrete space of waveform, AudioGen <ref type="bibr" target="#b24">(Kreuk et al., 2022)</ref>. Inspired by StableDiffusion <ref type="bibr" target="#b47">(Rombach et al., 2022)</ref>, which uses latent diffusion models (LDMs) to achieve high-quality image generation, we explore LDMs for TTA generation on a continuous latent representation, instead of learning discrete representations. Additionally, as audio manipulations, such as style transfer <ref type="bibr" target="#b7">(Engel et al., 2020;</ref><ref type="bibr" target="#b41">Pascual et al., 2022)</ref>, are also desired for audio signals, we explore and achieve various zero-shot text-guided audio manipulations with LDMs, which has not been demonstrated before.</p><p>For previous TTA works, a potential limitation for generation quality is the requirement of large-scale high-quality audio-text data pairs, which are usually not readily available and of limited quality and quantity <ref type="bibr">(Liu et al., 2022f)</ref>. To better utilize the data with noisy text captions, several methods for text preprocessing have been proposed <ref type="bibr" target="#b8">(Gemmeke et al., 2017;</ref><ref type="bibr" target="#b60">Yang et al., 2022)</ref>. However, their preprocessing steps inherently limit their generation performance by filtering out the relations of sound events (e.g., a dog is barking at the bark is transformed into dog bark park). In this work, we address this limitation by designing a method that only requires audio data for generative model training, and perform better than using audio-text paired data.</p><p>In this work, we present a TTA system, AudioLDM, which During training, latent diffusion models (LDMs) are conditioned on audio embedding and trained in a continuous space learned by VAE. The sampling process uses text embedding as the condition. Given pretrained LDMs, the zero-shot audio inpainting and style transfer are realized in the reverse process. The block Forward Diffusion denotes the process that corrupt data with gaussian noise (see Equation <ref type="formula" target="#formula_4">2</ref>).</p><p>achieves state-of-the-art generation quality with continuous LDMs, and is advantageous in computational efficiency and text-conditional audio manipulations. The overview of Au-dioLDM design for TTA generation and text-guided audio manipulation is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Specifically, AudioLDM learns to generate the audio prior in a latent space encoded by a mel-spectrogram-based variational auto-encoder (VAE). An LDM conditioned on the contrastive language-audio pretraining (CLAP) latent embedding is developed for prior generation. By leveraging this audio-text-aligned embedding space, we relax the requirement on text data during the training of LDMs, as the condition for prior generation can directly come from the audio itself. We demonstrate that training LDMs with audio only can be even better than training with audio-text data pairs. The proposed AudioLDM achieves state-of-the-art TTA performance on the Audio-Caps dataset with a freshet distance (FD) of 23.31, outperforming DiffSound baseline with FD of 47.68 by a large margin. In the meanwhile, our system enables zero-shot audio manipulations in the sampling process. In summary, our contributions are as follows:</p><p>? We demonstrate the first attempt to develop a continuous LDM for TTA generation and outperform existing methods in both subjective evaluation and objective metrics. ? We utilize CLAP latents to enable TTA generation without using language-audio pairs to train LDMs. We experimentally show that using only audio data in LDM training can obtain a high-quality and computationally efficient TTA system. ? We show that our proposed TTA system can perform text-guided audio styles manipulation, such as audio style transfer, super-resolution, and inpainting, without finetuning the model on a specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Text-to-Audio Generation starts to gain attention recently. Two works <ref type="bibr" target="#b60">(Yang et al., 2022;</ref><ref type="bibr" target="#b24">Kreuk et al., 2022)</ref> explore to learn the audio representations in a discrete space given natural language description, and then decode the representations to audio waveform. DiffSound <ref type="bibr" target="#b60">(Yang et al., 2022)</ref> consists of a text encoder, a decoder, a vector-quantized variational autoencoder (VQ-VAE), and a vocoder. To alleviate the scarcity of language-audio pairs, they propose a mask-based text generation strategy (MBTG) for generating text descriptions from audio labels. For example, the label dog bark, a man speaking will be represented as</p><formula xml:id="formula_0">[M] [M] dog bark [M] man speaking [M]</formula><p>, where [M] represent the mask token. However, with MBTG, the relationship between sound effects is not guided by text descriptions but is expected to be learned from the augmented data, which might potentially limit model performance.</p><p>AudioGen <ref type="bibr" target="#b24">(Kreuk et al., 2022</ref>) uses a transformer-based decoder to generate the discrete tokens that are directly compressed from the waveform. Moreover, they collect 10 datasets and propose data augmentation methods to learn the compositionality of audio. When creating the languageaudio pairs, they process the language descriptions to match the audio label annotation. For example, the text description a dog is barking at the park is transformed to dog bark park. For data augmentation, they mix audio samples according to various signal-to-noise ratios and concatenate the transformed language descriptions. However, in their method, the detailed text descriptions showing the spatial and temporal relationships, are discarded.</p><p>Diffusion Models <ref type="bibr" target="#b12">(Ho et al., 2020;</ref><ref type="bibr" target="#b54">Song et al., 2021)</ref> have achieved the state-of-the-art sample quality in various tasks, such as image generation <ref type="bibr">(Dhariwal &amp; Nichol, 2021;</ref><ref type="bibr" target="#b46">Ramesh et al., 2022;</ref><ref type="bibr" target="#b49">Saharia et al., 2022)</ref>, image restoration <ref type="bibr" target="#b48">(Saharia et al., 2021)</ref>, speech generation <ref type="bibr" target="#b2">(Chen et al., 2021;</ref><ref type="bibr">Kong et al., 2021b;</ref><ref type="bibr" target="#b28">Leng et al., 2022)</ref>, and video generation <ref type="bibr" target="#b51">(Singer et al., 2022;</ref><ref type="bibr" target="#b13">Ho et al., 2022)</ref>. For speech or audio synthesis, diffusion models have been studied for both mel-spectrogram generation <ref type="bibr" target="#b43">(Popov et al., 2021;</ref><ref type="bibr">Chen et al., 2022c)</ref> and waveform generation <ref type="bibr" target="#b26">(Lam et al., 2022;</ref><ref type="bibr" target="#b27">Lee et al., 2022;</ref><ref type="bibr">Chen et al., 2022b)</ref>.</p><p>A major concern of diffusion models is the iterative generation process in a high-dimensional data space will result in slow inference speed. One of the solutions is to employ diffusion models in a small latent space, which has been seen in image generation <ref type="bibr" target="#b56">(Vahdat et al., 2021;</ref><ref type="bibr" target="#b52">Sinha et al., 2021;</ref><ref type="bibr" target="#b47">Rombach et al., 2022)</ref>. For TTA generation, the audio waveform has redundant information <ref type="bibr">(Liu et al., 2022e;</ref><ref type="bibr">c)</ref> that increases modeling complexity and decreases inference speed. Therefore, DiffSound <ref type="bibr" target="#b60">(Yang et al., 2022</ref>) uses textconditional discrete diffusion models to generate discrete tokens compressed from mel-spectrogram. However, their generation quality is limited, and audio manipulation methods are not explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Text-Conditional Audio Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive Language-Audio Pretraining</head><p>The text-to-image generation models have shown stunning sample quality by utilizing Contrastive Language-Image Pretraining (CLIP) for generating the image prior. Inspired by this, we leverage Contrastive Language-Audio Pretraining (CLAP) to facilitate the TTA generation.</p><p>We denote audio samples as x and the text description as y. A text encoder f text (?) and an audio encoder f audio (?) are used to extract the text embedding E y ? R L and the audio embedding E x ? R L respectively, where L is the dimension of CLAP embedding. A recent study <ref type="bibr" target="#b58">(Wu et al., 2022)</ref> has explored different architectures for both text encoder and audio encoder when training the CLAP model. We follow their result to build an audio encoder on HTSAT <ref type="bibr">(Chen et al., 2022a)</ref>, and built a text encoder on RoBERTa <ref type="bibr" target="#b37">(Liu et al., 2019)</ref>. Then, we use the same symmetric cross-entropy loss as training objective <ref type="bibr" target="#b44">(Radford et al., 2021;</ref><ref type="bibr" target="#b58">Wu et al., 2022)</ref>. We provide the training details and employed languageaudio datasets in Appendix A.</p><p>After training the CLAP model, we can extract the representation E x of an audio sample x, containing both audio and text information. The generalization ability of CLAP model has been demonstrated by various downstream tasks such as the zero-shot audio classification <ref type="bibr" target="#b58">(Wu et al., 2022)</ref>. Then, for unseen language or audio samples, CLAP embeddings also provide cross-modal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Conditional Latent Diffusion Models</head><p>In TTA generation, we generate the audio sample x given text description y. With probabilistic generative model LDMs, we are trying to estimate the true conditional data distribution q(z 0 |E y ) with model distribution p ? (z 0 |E y ), where</p><formula xml:id="formula_1">z 0 ? R C? T r ? F</formula><p>r is the prior of an audio sample x in a latent space compressed from the mel-spectrogram X ? R T ?F , and E y is the text embedding obtained by pretrained text encoder f text (?) in CLAP. Here, r denotes the compression level, C denotes the channel of latent representation, X denotes mel-spectrogram, T and F denotes the time-frequency dimensions in mel-spectrogram. With pretrained CLAP to connect audio and text, the audio embedding E x and the text embedding E y share a joint space and both contain cross-modal information. This allows us to provide E x for training LDMs, while providing E y for TTA generation. Diffusion models <ref type="bibr" target="#b12">(Ho et al., 2020;</ref><ref type="bibr" target="#b54">Song et al., 2021)</ref> consists of two processes i) a forward process to transform the data distribution into standard Gaussian distribution with a predefined noise schedule</p><formula xml:id="formula_2">0 &lt; ? 1 &lt; ? ? ? &lt; ? n &lt; . . . ? N &lt; 1,</formula><p>and ii) a reverse process to gradually generate data samples from the noise according to an inference noise schedule.</p><p>In forward process, at each time step n ? [1, . . . , N ], the transition probability is:</p><formula xml:id="formula_3">q(z n |z n-1 ) = N (z n ; 1 -? n z n-1 , ? n I),<label>(1)</label></formula><formula xml:id="formula_4">q(z n |z 0 ) = N (z n ; ? ?n z 0 , (1 -?n ) ),<label>(2)</label></formula><p>where ? N (0, I) denotes injected noise, ? n := 1? n and ?n := n s=1 ? s represent the noise level at each step. At the final time step N , z N ? N (0, I) becomes an isotropic Gaussian noise. For noise estimation, we employ the reweighted training objective <ref type="bibr" target="#b12">(Ho et al., 2020;</ref><ref type="bibr">Kong et al., 2021b;</ref><ref type="bibr" target="#b47">Rombach et al., 2022)</ref>:</p><formula xml:id="formula_5">L n (?) = E z0, ,n -? (z n , n, E x ) 2 2 ,<label>(3)</label></formula><p>where E x is extracted from audio waveform x by the pretrained audio encoder f audio (?) in CLAP. In the reverse process, starting from the Gaussian noise distribution p(z N ) ? N (0, I), for TTA generation, a denoising process conditioned on the text embedding E y gradually generates audio prior z 0 by:</p><formula xml:id="formula_6">p ? (z 0:N |E y ) = p(z N ) N t=n p ? (z n-1 |z n , E y ),<label>(4)</label></formula><formula xml:id="formula_7">p ? (z n-1 |z n , E y ) = N (z n-1 ; ? ? (z n , n, E y ), ? 2 n I),<label>(5)</label></formula><p>The mean function and variance are parameterized as:</p><formula xml:id="formula_8">? ? (z n , n, E y ) = 1 ? ? n (z n - ? n ? 1 -?n ? (z n , n, E y )),<label>(6)</label></formula><formula xml:id="formula_9">? 2 n = 1 -?n-1 1 -?n ? n ,<label>(7)</label></formula><p>where ? (z n , n, E y ) is the predicted noise in generation, and ? 2 1 = ? 1 . Our formulation shows that, in the training stage, we learn the generation of audio prior z 0 given the cross-modal representation E x of an audio sample x. Then, in TTA generation, we provide the text embedding E y to predict the noise ? (z n , n, E y ). Built on the CLAP latents, our LDM realizes TTA generation without text supervision in the training stage. We provide the network architecture in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Conditioning Augmentation</head><p>In text-to-image generation, diffusion-based models have demonstrated strong capability in capturing the compositionality between objects and backgrounds <ref type="bibr" target="#b46">(Ramesh et al., 2022;</ref><ref type="bibr" target="#b49">Saharia et al., 2022;</ref><ref type="bibr">Liu et al., 2022d)</ref>. One of the foundations for this success is the large-scale language-image training pairs. For TTA generation, it is also desired to generate compositional audio signals whose relationships are consistent with natural language descriptions. However, the scale of the language-audio dataset is not comparable to that of the language-image dataset. For data augmentation, AudioGen <ref type="bibr" target="#b24">(Kreuk et al., 2022)</ref> fuses pairs of audio samples and concatenates their respective processed text captions. In this work, as it is shown in Equation 3, as we provide the audio embedding E x as conditioning information when training LDMs, we are able to implement data augmentation for only audio signals instead of language-audio pairs. To strengthen LDMs for modeling compositional audio signals, we fuse audio pairs x 1 and x 2 by:</p><formula xml:id="formula_10">x 1,2 = ?x 1 + (1 -?)x 2 , (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where ? is a scaling factor varying between [0, 1] sampled from a Beta distribution B(5, 5). Here we do not need to consider the corresponding text description y 1,2 . By mixing audio pairs, we increase the training data pair (z 0 , E x ) for LDMs, which makes LDMs robust to CLAP embeddings. Then, in the sampling process, given the text embedding E y from unseen language descriptions, LDMs are expected to generate the corresponding audio prior z 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Classifier-free Guidance</head><p>For diffusion models, controllable generation can be achieved by introducing guidance at each sampling step. After classifier guidance <ref type="bibr" target="#b54">(Song et al., 2021;</ref><ref type="bibr">Nichol &amp; Dhariwal, 2021)</ref>, classifier-free guidance <ref type="bibr">(Ho &amp; Salimans, 2021</ref>; </p><formula xml:id="formula_12">? ? (z n , n, E y ) = w ? (z n , n) + (1 -w) ? (z n , n, E y ),<label>(9)</label></formula><p>where w determines the guidance scale. Compared with Au-dioGen <ref type="bibr" target="#b24">(Kreuk et al., 2022)</ref>, we have two differences. First, they leverage CFG on a transformer-based auto-regressive model, while our LDMs retain the theoretical formulation behind the CFG <ref type="bibr">(Ho &amp; Salimans, 2021)</ref>. Second, our text embedding E y is extracted from unprocessed natural language and therefore enables CFG to make use of the detailed text descriptions as guidance for audio generation. However, AudioGen removed the text details showing spatial or temporal relationships with text preprocessing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Decoder</head><p>We use VAE to compress the mel-spectrogram X ? R T ?F into a small latent space z ? R C? T r ? F r , where r is the compression level of latent space. Our VAE is composed of an encoder and a decoder with stacked convolutional modules. In the training objective, we adopt a reconstruction loss, an adversarial loss, and a gaussian constraint loss. We provide the detailed architecture and training methods in Appendix C. In the sampling process, the decoder is used to reconstruct the mel-spectrogram X from the audio prior ?o generated from LDMs. To explore a compression level r that achieves a small latent space for LDMs without sacrificing sample quality, we test a group of values r ? <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">16]</ref>, and take r=4 as our default setting because of its high computational efficiency and generation quality. Moreover, as we conduct conditioning augmentation for LDMs, we implement data augmentation with Equation <ref type="formula" target="#formula_12">9</ref>for VAE as well in order to guarantee the reconstruction quality of generated compositional samples. For vocoder, we employ HiFi-GAN <ref type="bibr">(Kong et al., 2020a)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Text-Guided Audio Manipulation</head><p>Style Transfer Given a source audio sample x src , we can calculate its noisy latent representation z n0 with a predefined time step n 0 ? N according to the forward process shown in Equation <ref type="formula" target="#formula_4">2</ref>. By utilizing z n0 as the starting point of the reverse process of a pretrained AudioLDM model, we enable the manipulation of audio x src with text input y with a shallow reverse process p ? (z 0:n0 |E y ):</p><formula xml:id="formula_13">p ? (z 0:n0 |E y ) = p(z n0 ) n0 n=1 p ? (z n-1 |z n , E y ), (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where n 0 controls the manipulation results. If we define a n 0 ? N , the information provided by source audio will not be retained and the manipulation would be similar to TTA generation. We show the effect of n 0 in Figure <ref type="figure" target="#fig_3">3</ref>, where larger manipulations can be seen in the setting of n 0 = 3N/4.</p><p>Inpainting and Super-Resolution Both audio inpainting and audio super-resolution refer to generating the missing part given the observed part x ob . We explore these tasks by incorporating the observed part in latent representation z ob into the generated latent representation z. Specifically, in reverse process, starting from p(z N ) ? N (0, I), after each inference step shown in Equation <ref type="formula" target="#formula_7">5</ref>, we modify the generated z n-1 with:</p><formula xml:id="formula_15">z n-1 = (1 -m) z n-1 + m z ob n-1 , (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where z is the modified latent representation, m ? R T r ? F r denotes an observation mask in latent space, z ob n-1 is obtained by adding noise on z ob with the forward process shown by Equation <ref type="formula" target="#formula_4">2</ref>.</p><p>The values of observation mask m depend on the observed part of a mel-spectrogram X. As we adopt a convolutional structure in VAE to learn the latent representation z, we can roughly retain the spatial correspondency in melspectrogram, as it is shown in Figure <ref type="figure" target="#fig_7">7</ref> in Appendix C. Therefore, if a time-frequency bin X t,f is observed, we set the observation mask m t r , f r in latent space as 1. By using m to denote the generation part and observation part in z, according to Equation <ref type="formula" target="#formula_15">11</ref>, we can generate the missing in-formation conditioned on the text prompt with TTA models, while retaining the ground-truth observation z ob .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Training dataset The datasets we used in this paper includes AudioSet (AS) <ref type="bibr" target="#b8">(Gemmeke et al., 2017)</ref>, Audio-Caps (AC) <ref type="bibr" target="#b17">(Kim et al., 2019)</ref>, Freesound (FS)<ref type="foot" target="#foot_1">2</ref> , and BBC Sound Effect library (SFX)<ref type="foot" target="#foot_2">3</ref> . AS is currently the largest audio dataset, with 527 labels and over 5, 000 hours of audio data. AC is a much smaller dataset with around 49, 000 audio clips and text descriptions. Most of the data in both Au-dioSet and AudioCaps are in-the-wild audio from YouTube, so the quality of the audio is not guaranteed. To expand the dataset, especially with high-quality audio data, we crawl the data from the FreeSound and BBC SFX datasets, which have a wide range of categories such as music, speech, and sound effects. We show our detailed data processing methods and training configuration in Appendix E.</p><p>Evaluation dataset We evaluate the model on both AC and AS. Each audio clip in AC has 5 text captions. We generate the evaluation set by randomly selecting one of them as text condition. Because the authors of AC intentionally remove the audio with the label related to music <ref type="bibr" target="#b17">(Kim et al., 2019)</ref>, to evaluate model performance with a wider range of sound, we randomly select 10% audio samples from AS as another evaluation set. Since AS does not contain text descriptions, we use the concatenation of labels as text descriptions, such as Speech, hip hop music, and crowd cheering.</p><p>Evaluation methods We use both metrics for objective evaluation and human subjective evaluation. The main evaluation metrics we use include frechet distance (FD), inception score (IS), and kullback-leibler (KL) divergence. Similar to the frechet inception distance in image generation, the FD in audio indicates the similarity between generated samples and target samples. IS is effective in evaluating both sample quality and diversity. KL is measured at a paired sample level and averaged as the final result. All of these three metrics are built upon a state-of-the-art audio classifier PANNs <ref type="bibr">(Kong et al., 2020b)</ref>. To compare with <ref type="bibr" target="#b24">(Kreuk et al., 2022)</ref>, we also adopt the frechet audio distance (FAD) <ref type="bibr" target="#b16">(Kilgour et al., 2019)</ref>. FAD has a similar idea to FD but it uses VGGish <ref type="bibr" target="#b10">(Hershey et al., 2017)</ref> as a classifier which may have inferior performance than PANNs. To better measure the generation quality, we choose FD as the main evaluation metric in this paper. For subjective evaluation, we recruit six audio professionals to carry on a rating process following <ref type="bibr" target="#b24">(Kreuk et al., 2022;</ref><ref type="bibr" target="#b60">Yang et al., 2022)</ref>. Specifically, the generated samples are rated based The symbol ? marks industry-level computation. DiffSound is trained on 32 V100 GPUs and AudioGen is trained on 64 A100 GPUs, while AudioLDM models are trained on a single GPU, RTX 3090 or A100. The AS and AC stand for AudioSet and AudioCaps datasets respectively. The results of AudioGen are employed from the paper since its implementation has been not publicly available.</p><p>on i) overall quality (OVL); and ii) relevance to the input text (REL) between a scale of 1 to 100. We include the details of human evaluation in Appendix E. We open-source our evaluation pipeline to facilitate reproducibility 4 .</p><p>Models We employ two recently proposed TTA systems, DiffSound <ref type="bibr" target="#b60">(Yang et al., 2022)</ref> and AudioGen <ref type="bibr" target="#b24">(Kreuk et al., 2022)</ref> as our baseline models. DiffSound is trained on AS and AC datasets with around 400M parameters. AudioGen is trained on AS, AC, and eight other datasets with around 285M parameters. Since AudioGen has not released publicly available implementation, we reuse the KL and FAD results reported in their paper. We train two AudioLDM models. One is a small model named AudioLDM-S, which LDM has 181M parameters, and the other is a large model named AudioLDM-L with 739M parameters. We describe the details of UNet architecture in Appendix B. To demonstrate the advantage of our method, we simply train these two models only with the AC dataset. Moreover, to explore the effect of the scale of training data, we develop an AudioLDM-L-Full model which is trained on AC, AS, FreeSound, and BBC SFX datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results</head><p>We show the main evaluation results on the AC test set in  <ref type="figure" target="#fig_4">4</ref> shows the score statistic of different models averaged between all the raters. We notice our model is more concentrated on the higher scores compared with DiffSound. Our spam cases, which are randomly selected real recordings, show high scores, indicating the rating result is reliable.</p><p>To perform the evaluation on audio data that could include music, we further evaluate our model on the AS evaluation set. We compare with DiffSound and show our result in Table <ref type="table" target="#tab_2">2</ref>. Our three AudioLDM models show a similar trend as they perform on the AC test set. We can outperform the DiffSound baseline by a large margin on all the metrics. Conditioning Information As we train LDMs conditioned on the audio embedding E x but provide the text embedding E y to LDMs in TTA generation, a natural concern is that if stronger results could be achieved by directly using the text embedding as training condition. We conduct experiments and show the results in Table <ref type="table" target="#tab_3">3</ref>. For a fair comparison, we also conduct data augmentation and we adopt the strategy from AudioGen. Specifically, we use the same mixing method for audio pairs shown in Section 3.3, and concatenate two text captions as conditioning information.   We believe the primary reason for the result in Table <ref type="table" target="#tab_3">3</ref> is that text embedding can not represent the generation target as good as audio embedding. Firstly, due to the ambiguity and complexity of sound, the text caption is difficult to be accurate and comprehensive. Different human annotators may have different perceptions and descriptions over the same audio, which make training with text-audio pair less stable than with audio only. Moreover, some of the captions are at a highly-abstracted level and can not correctly describe the audio content. For example, there is an audio in the BBC SFX dataset with caption Boats: Battleships-5.25 conveyor space, which is even difficult for humans to imagine how it sounds. This quality of language-audio pairs may hinder model optimization. By comparison, if we use E x from CLAP latents as a condition, it is extracted directly from the audio signal and is aligned with ideally the best text caption, which enables us to provide strong conditioning information to LDMs without considering the noisy labeled text description. Figure <ref type="figure" target="#fig_5">5</ref> shows sample quality as a function of training progress. We notice that i) training with audio embedding can lead to significantly better results than text embedding throughout the entire training process; and ii) larger models may converge slower but can achieve better final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compression Level</head><p>We study the effect of compression level r on generation quality. If we set the compression level as r=1, which means we directly generate mel-spectrogram from CLAP latents, the training process is difficult to implement on a single RTX 3090 GPU. Similar results happen on r=2. Moreover, the inference speed will become slow with r=1, 2. In our studies, r=4 achieves high generation quality while reducing the computational to a reasonable load. Hence, we use it as the default setting in our experiments.</p><p>Text-Guided Audio Manipulation We show the performance of our text-guided audio manipulation methods on two tasks: super-resolution and inpainting. Specifically, for super-resolution, we upsample the audio signal from 8 kHz to 16 kHz sampling rate. For the inpainting task, we remove the audio signal between 2.5 and 7.5 seconds and refill this part by inpainting. Since most studies on audio super-resolution work on speech signal <ref type="bibr">(Liu et al., 2021a;</ref><ref type="bibr">2022a)</ref>, we demonstrate our results on both AudioCaps, and a speech dataset VCTK <ref type="bibr" target="#b59">(Yamagishi et al., 2019)</ref>, which is a multi-speaker speech dataset. For super-resolution, we employ two models AudioUNet <ref type="bibr" target="#b25">(Kuleshov et al., 2017)</ref> and NVSR <ref type="bibr">(Liu et al., 2022a)</ref> as baseline models, and employ log-spectral distance (LSD) <ref type="bibr">(Wang &amp; Wang, 2021)</ref> as the evaluation metric for comparison. For the inpainting task, we use FAD as a metric and establish a baseline for this task. Table <ref type="table" target="#tab_5">5</ref> shows that AudioLDM can outperform the strong AudioUNet baseline, but the result is not as good as NVSR <ref type="bibr">(Liu et al., 2022a)</ref>. Recall that AudioLDM is a model trained on a diverse set of audio signals, including those with heavy background noise. This can lead to the presence of white noise or other non-speech sound events in the output of our super-resolution process, potentially reducing performance. Nevertheless, our contribution would be to open the door to realizing text-guided audio manipulation with the TTA system in a zero-shot way. Further improve- ments could be expected based on our benchmark results.</p><p>We provide several samples of our results in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>Table <ref type="table" target="#tab_6">6</ref> shows the result of our ablation study on AudioLDM-S. By simplifying the attention mechanism in UNet into a one-layer multi-head self-attention (w. Simple attn), the performance in each metric will have a notable decrease, which indicates complex attention mechanism is preferred. Also, we notice the widely used balanced sampling strategy <ref type="bibr" target="#b9">(Gong et al., 2021;</ref><ref type="bibr">Liu et al., 2022b)</ref> in the audio classification does not show improvement on TTA (w. Balance samp). Conditional augmentation (see Section 3.3) shows improvement in the subjective evaluation, but it does not show improvement in the objective evaluation metrics (w. Cond aug). We believe the reason is conditioning augmentation generates training data that is not representative of the AudioCaps dataset, resulting in model outputs that are not well-aligned with the evaluation data, ultimately leading to lower metrics scores. Nevertheless, conditioning augmentation can improve two subjective metrics and we still recommend using it as a data augmentation technique. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DDIM Sampling</head><p>Step The number of inference steps in the reverse process of DDPMs can directly affect the generation quality <ref type="bibr" target="#b12">(Ho et al., 2020;</ref><ref type="bibr" target="#b54">Song et al., 2021)</ref>. Generally, the sample quality increases with more sampling steps and heavier computation at the same time. We explore the effect of DDIM <ref type="bibr" target="#b53">(Song et al., 2020)</ref> sampler steps on our latent diffusion model. Table <ref type="table" target="#tab_7">7</ref> shows that more sampling steps lead to better quality. With enough sampling steps such as 100, the gain of adding sampling steps becomes less significant. The result of 200 steps is only slightly better than that of 100 steps.  Guidance Scale represents a trade-off between conditional generation quality and sample diversity. A suitable guidance scale can improve the consistency between generated samples and conditioning information at an acceptable cost of generation diversity. We show the effect of guidance scale w on TTA in Figure <ref type="figure" target="#fig_6">6</ref>. When w = 3, we achieve the best results in both FD and KL, but not in FAD. We suppose the reason is the audio classifier in FAD is not as good as FD, as mentioned in Section 5. In this case, the improvement in the adherence to detailed language description may become misleading information to the classifier in FAD. Considering previous studies report FAD results instead of FD, we set w = 2 for comparison, but also provide detailed effects of w on FAD, FD, IS, and KL.</p><p>Case Study We conduct case study and show the generated results in Appendix F, including style transfer (see Figure <ref type="figure" target="#fig_9">8</ref>-10), super-resolution (see Figure <ref type="figure" target="#fig_0">11</ref>), inpainting (see , and text-to-audio generation (see Figure <ref type="figure" target="#fig_13">14</ref>-21). Specifically, for text-to-audio, we demonstrate the controllability of AudioLDM, including the control of the acoustic environment, material, sound event, pitch, musical genres, and temporal orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>By realizing text-to-audio (TTA) with contrastive languageaudio pretraining (CLAP) models and latent diffusion models (LDMs), we present AudioLDM, which is advantageous in generation quality, computational efficiency, and audio manipulations. With a single training dataset AudioCaps and a single GPU, AudioLDM achieves SOTA generation quality evaluated by both subjective and objective metrics. Moreover, AudioLDM enables zero-shot text-guided audio style transfer, super-resolution, and inpainting.</p><p>We employ three loss functions in our training objective: the mel-spectrogram reconstruction loss, adversarial losses, and a gaussian constraint loss. The reconstruction loss calculates the mean absolute error between the input sample X ? R T ?F and the reconstructed mel-spectrogram X ? R T ?F . The adversarial losses are employed to enhance the reconstruction quality. Specifically, we adopt the PatchGAN <ref type="bibr" target="#b14">(Isola et al., 2017)</ref> as our discriminator, which will divide the input image into small patches and predict whether each patch is real or fake by outputting a matrix of logits. The PatchGAN discriminator is trained to maximize the logits of correctly identifying real patches while minimizing the logits of incorrectly identifying fake patches. We also apply the gaussian constraint on the latent space of VAE. By enforcing a gaussian constraint on the latent space, the VAE is encouraged to learn a continuous, structured latent space, rather than a disorganized one. This can help the VAE to better capture the underlying structure of the data, which can result in more stabilized and accurate reconstructions <ref type="bibr" target="#b19">(Kingma &amp; Welling, 2013)</ref>.</p><p>We train our VAE using the Adam optimizer <ref type="bibr" target="#b18">(Kingma &amp; Ba, 2014)</ref> with a learning rate of 4.5 ? 10 -6 and a batch size of six. The audio data we use includes AudioSet, AudioCaps, Freesound, and BBC SFX. We perform experiments with three compression-level settings r=4, 8, 16, for which the latent channels are C = 8, 16, 32, respectively. VAEs in all three settings are trained with at least 1.5M steps on a single NVIDIA RTX 3090 GPU. To stabilize training, we do not apply the adversarial loss in the first 50K training steps. We apply the mixup <ref type="bibr">(Kong et al., 2020b)</ref> strategy for data augmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Vocoder</head><p>In this work, we employ HiFi-GAN <ref type="bibr">(Kong et al., 2020a)</ref> as a vocoder, which is widely used for speech waveform generation. It contains two sets of discriminators, a multi-period discriminator, and a multi-scale discriminator, to enhance the perceptual quality. To synthesize the audio waveform, we train it on the AudioSet dataset. For the input samples at the sampling rate of 16, 000Hz, we extract 64 bands mel-spectrogram. Then we follow the default settings of HiFi-GAN V1. The window, FFT, and hop size are set to 1024, 1024, and 160. The f min and f max are set as 0 and 8000. We use the AdamW optimizer with 0.8 and 0.99. The learning rate starts from 2 ? 10 -4 and a learning rate decay of 0.999 is used. We use a batch size of 96 and train the model with 6 NVIDIA 3090 GPUs. We release this pretrained vocoder in our open-source implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiment Details</head><p>Data Processing The duration of the audio samples in AudioSet and AudioCaps is 10 seconds, while it is much longer in FreeSound and BBC SFX datasets. To avoid overusing the data from long audio, which usually have repeated sound, we only use the first thirty seconds of the audio in both the FreeSound and BBC SFX datasets and segment them into ten-second long audio files. Finally, we have in total 3, 302, 553 ten-seconds audio samples for model training. It should be noted that even if some datasets, e.g., AudioCaps and BBC SFX, have text captions for the audio, we do not utilize them during the training of LDMs. We only use the audio samples for training. We resample all the datasets into 16kHz sampling rate and mono format, and all samples are padded to 10 seconds.</p><p>Configuration For each LDM model, we use the compression level r=4 as the default setting. Then, we train AudioLDM-S and AudioLDM-L for 0.6M steps on a single GPU, NVIDIA RTX 3090, with the batch size of 5 and 8, respectively. The learning rate is set as 3 ? 10 -5 . The AudioLDM-L-Full is trained for 1.5M steps on one NVIDIA A100 with a batch size of 8. The learning rate is 10 -5 . For better performance on AudioCaps, we further fine-tune AudioLDM-L-Full on AudioCaps for 0.25M steps before evaluation. It should be noted that we limit our batch size because of the scarcity of GPU. However, this potentially restricts the performance of AudioLDM models. In comparison, DiffSound uses 32 NVIDIA V100 GPUs for model training with a batch size of 16 on each GPU. AudioGen utilizes 64 A100 GPUs with a batch size of 256.</p><p>Human evaluation We construct the dataset for human subjective evaluation with 70 randomly selected samples where 30 audios are from AudioCaps, 30 audios are from AudioSet, and 10 randomly selected real recordings, which we will refer to as spam cases. Therefore, each model should generate 60 audio samples given the corresponding text descriptions. We gather the output from models in one folder and anonymize them with random identifiers. An example questionnaire is shown in Table <ref type="table">9</ref>. The participant will need to fill in the last two columns for each audio file given the text description. Our final result shows that all the human raters have an average score above 90 on the spam cases. Hence, their evaluation result is considered reliable. Table <ref type="table">9</ref>. Example questionnaire for human evaluation. The participant will need to fill in the last two columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Demos Audio Style Transfer</head><p>We show three examples of zero-shot audio style transfer with AudioLDM-S, using the developed shallow reverse process (see Equation <ref type="formula" target="#formula_13">10</ref>). In Figure <ref type="figure" target="#fig_9">8</ref>, we show the transfer from drum beats to ambient music. From left to right, we show the source audio sample drum beats, and the six generated samples guided by text prompt ambient music with different starting points n 0 . Given a smaller n 0 (i.e., the left part of the figure), the generated sample is similar to drum beats, while when we set n 0 = 0.8 ? N for the last sample, the generated sample will be aligned with the text input ambient music. Similarly, we show the source audio trumpt, and the seven generated samples guided by text prompt children singing in Figure <ref type="figure" target="#fig_10">9</ref>. We show the source audio sheep vocalization, and the five generated samples guided by text prompt narration, monologue in Figure <ref type="figure" target="#fig_11">10</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Environment Control</head><p>In Figure <ref type="figure" target="#fig_13">14</ref>, we demonstrate that AudioLDM can control the acoustic environment of generated samples with a text description. The four samples are generated with the same random seed, but with different text prompts. Their common text information is "A man is speaking in", while the specific text information describes the acoustic environment as "a small room", "a huge room", "a huge room without background noise", and "a studio". These samples show the ability of AudioLDM to capture the fine-grained text description about the acoustic environment, and control the corresponding effects on audio samples, such as reverberation or background noise.</p><p>A man is speaking in a huge room without background noise.</p><p>A man is speaking in a studio.</p><p>A man is speaking in a huge room.</p><p>A man is speaking in a small room. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of AudioLDM design for text-to-audio generation (left), and text-guided audio manipulation (right). During training, latent diffusion models (LDMs) are conditioned on audio embedding and trained in a continuous space learned by VAE. The sampling process uses text embedding as the condition. Given pretrained LDMs, the zero-shot audio inpainting and style transfer are realized in the reverse process. The block Forward Diffusion denotes the process that corrupt data with gaussian noise (see Equation2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The samples generated with different scales of the classifier-free guidance. The text prompt is "A cat is meowing".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>to generate the audio sample x from reconstructed mel-spectrogram X. The training details are shown in Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The manipulation result with different starting points n0 of the shallow reverse process. The original signal is Trumpet, and the text prompt for style transfer is Children Singing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The histogram of the human evaluation result. The horizontal axis and vertical axis represent the rating score and frequency, respectively. OVL denotes the overall quality of audio files and REL denotes the relation between text and generated audio. Both OVL and REL are scored on a scale of 1 to 100. Scores on each evaluation file are averaged among all the raters.</figDesc><graphic url="image-18.png" coords="7,213.37,196.07,76.01,76.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The comparison of various evaluation metrics evaluated in the training process of i) AudioLDM-S trained with text embedding (S-Text+Audio) ii) AudioLDM-S (S-Audio), and iii) AudioLDM-L (L-Audio).</figDesc><graphic url="image-19.png" coords="7,134.53,196.07,76.01,76.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The comparison between different classifier-free guidance scales for an AudioLDM-S model that trained on AudioCaps.</figDesc><graphic url="image-21.png" coords="8,319.14,67.06,210.60,122.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Visualization of the time-frequency-masked spectrogram and their corresponding VAE latent. This figure shows VAE encoder roughly preserves the spatial correspondancy between the spectrogram and the latent.</figDesc><graphic url="image-26.png" coords="13,356.57,484.90,135.89,67.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Audio style transfer from drum beats to ambient music.</figDesc><graphic url="image-27.png" coords="15,116.19,225.58,364.51,121.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Audio style transfer from trumpet to children singing.</figDesc><graphic url="image-28.png" coords="15,116.19,387.88,364.51,121.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Audio style transfer from sheep vocalization to narration, monologue.</figDesc><graphic url="image-29.png" coords="15,116.19,550.18,364.51,121.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. The example of zero-shot audio inpainting with AudioLDM-S given different text prompts.</figDesc><graphic url="image-57.png" coords="18,319.98,237.54,221.24,73.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. The examples of controlling acoustic environment with AudioLDM-S.</figDesc><graphic url="image-59.png" coords="18,297.04,496.31,123.68,92.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The comparison between AudioLDM and baseline TTA generation models. Evaluation is conducted on AudioCaps test set.</figDesc><table><row><cell>Model</cell><cell>Datasets</cell><cell cols="2">Text Params</cell><cell>FD ?</cell><cell>IS ?</cell><cell cols="4">KL ? FAD ? OVL ? REL ?</cell></row><row><cell>Ground truth</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.61</cell><cell>80.11</cell></row><row><cell>DiffSound  ? (Yang et al., 2022)</cell><cell>AS+AC</cell><cell></cell><cell>400M</cell><cell>47.68</cell><cell>4.01</cell><cell>2.52</cell><cell>7.75</cell><cell>45.00</cell><cell>43.83</cell></row><row><cell cols="2">AudioGen  ? (Kreuk et al., 2022) AS+AC+8 others</cell><cell></cell><cell>285M</cell><cell>-</cell><cell>-</cell><cell>2.09</cell><cell>3.13</cell><cell>-</cell><cell>-</cell></row><row><cell>AudioLDM-S</cell><cell>AC</cell><cell></cell><cell>181M</cell><cell>29.48</cell><cell>6.90</cell><cell>1.97</cell><cell>2.43</cell><cell>63.41</cell><cell>64.83</cell></row><row><cell>AudioLDM-L</cell><cell>AC</cell><cell></cell><cell>739M</cell><cell>27.12</cell><cell>7.51</cell><cell>1.86</cell><cell>2.08</cell><cell>64.30</cell><cell>64.72</cell></row><row><cell>AudioLDM-L-Full</cell><cell>AS+AC+2 others</cell><cell></cell><cell>739M</cell><cell cols="3">23.31 8.13 1.59</cell><cell>1.96</cell><cell cols="2">65.91 65.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>. Given the single training dataset AC, AudioLDM-</cell></row><row><cell>S can achieve better generation results than the baseline</cell></row><row><cell>models on both objective and subjective evaluations, even</cell></row><row><cell>with smaller model size. By expanding model capacity with</cell></row><row><cell>AudioLDM-L, we further improve the overall results. Then,</cell></row><row><cell>by incorporating AS and the two other datasets into training,</cell></row><row><cell>our model AudioLDM-L-Full achieves the best quality, with</cell></row><row><cell>an FD of 23.31.</cell></row><row><cell>Our human evaluation shows a similar trend as other evalu-</cell></row><row><cell>ation metrics. Our proposed methods have OVL and REL</cell></row><row><cell>of around 64, outperforming DiffSound with OVL of 45.00</cell></row><row><cell>and REL of 43.83 by a large margin. On the AudioLDM</cell></row><row><cell>model size, we notice that the larger model is advantageous</cell></row><row><cell>4 https://github.com/haoheliu/audioldm_</cell></row><row><cell>eval</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The evaluation results on the AudioSet evaluation set.</figDesc><table><row><cell>Model</cell><cell>FD ?</cell><cell>IS ?</cell><cell>KL ?</cell></row><row><cell>DiffSound</cell><cell>50.40</cell><cell>4.19</cell><cell>3.63</cell></row><row><cell>AudioLDM-S</cell><cell>28.08</cell><cell>6.78</cell><cell>2.51</cell></row><row><cell>AudioLDM-L</cell><cell>27.51</cell><cell>7.18</cell><cell>2.49</cell></row><row><cell cols="4">AudioLDM-L-Full 24.26 7.67 2.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Table 3 shows by training LDMs on E x , we can achieve better results than training with E y . The comparison between text embedding and audio embedding as conditioning information on the training of LDMs.</figDesc><table><row><cell>Model</cell><cell>Text Audio FD ?</cell><cell>IS ? KL ?</cell></row><row><cell>AudioLDM-S</cell><cell cols="2">31.26 6.35 2.01</cell></row><row><cell>AudioLDM-S</cell><cell cols="2">29.48 6.90 1.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The effect of compression level on AudioLDM.</figDesc><table><row><cell>Model</cell><cell>r</cell><cell>FD ?</cell><cell>IS ?</cell><cell>KL ?</cell></row><row><cell>AudioLDM-S</cell><cell>4</cell><cell cols="3">29.48 6.90 1.97</cell></row><row><cell>AudioLDM-S</cell><cell>8</cell><cell>33.50</cell><cell>6.13</cell><cell>2.04</cell></row><row><cell cols="3">AudioLDM-S 16 34.32</cell><cell>5.68</cell><cell>2.09</cell></row></table><note><p><p><p>Table</p>4</p>shows the performance comparison with r=4, 8, 16. We observe a decreasing trend with the increase of compression levels. Nevertheless, in the setting of r=16 where we compress the 64-band melspectrogram into only 4 dimensions in the frequency axis, our performance is still on par with AudioGen on KL, and better than DiffSound on all the metrics.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Performance comparisons on zero-shot super-resolution and inpainting, which are evaluated by LSD and FAD, respectively.</figDesc><table><row><cell>Task</cell><cell cols="2">Super-resolution</cell><cell>Inpainting</cell></row><row><cell>Dataset</cell><cell cols="3">AudioCaps VCTK AudioCaps</cell></row><row><cell>Unprocessed</cell><cell>2.76</cell><cell>2.15</cell><cell>10.86</cell></row><row><cell>Kuleshov et al. (2017)</cell><cell>-</cell><cell>1.32</cell><cell>-</cell></row><row><cell>Liu et al. (2022a)</cell><cell>-</cell><cell>0.78</cell><cell>-</cell></row><row><cell>AudioLDM-S</cell><cell>1.59</cell><cell>1.12</cell><cell>2.33</cell></row><row><cell>AudioLDM-L</cell><cell>1.43</cell><cell>0.98</cell><cell>1.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The ablation study on the attention mechanism, the balance sampling technique for training data, and the conditioning augmentation algorithm.</figDesc><table><row><cell>Setting</cell><cell>FD?</cell><cell>IS?</cell><cell cols="3">KL? OVL ? REL ?</cell></row><row><cell>AudioLDM-S</cell><cell cols="3">29.48 6.90 1.97</cell><cell>63.41</cell><cell>64.83</cell></row><row><cell>w. Simple attn</cell><cell>33.12</cell><cell>6.15</cell><cell>2.09</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">w. Balance samp 34.05</cell><cell>6.21</cell><cell>2.16</cell><cell>-</cell><cell>-</cell></row><row><cell>w. Cond aug</cell><cell>31.88</cell><cell>6.25</cell><cell>2.02</cell><cell cols="2">64.49 65.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Effect of sampling steps of LDMs with a DDIM sampler.</figDesc><table><row><cell>DDIM steps</cell><cell>10</cell><cell>25</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>FD</cell><cell cols="5">55.84 42.84 35.71 30.17 29.48</cell></row><row><cell>IS</cell><cell>4.21</cell><cell>5.91</cell><cell>6.51</cell><cell>6.85</cell><cell>6.90</cell></row><row><cell>KL</cell><cell>2.47</cell><cell>2.12</cell><cell>2.01</cell><cell>1.94</cell><cell>1.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>shows the reconstruction performance of our VAE model with different values of r. All three settings achieve comparable metrics score with the GT Mel + Vocoder setting, indicating the autoencoder can perform reliable melspectrogram encoding and decoding.</figDesc><table><row><cell>Setting</cell><cell cols="3">PSNR? SSIM? FD?</cell><cell>IS?</cell><cell>KL?</cell></row><row><cell>GT Mel + Vocoder</cell><cell>25.41</cell><cell>0.86</cell><cell cols="3">8.76 10.71 0.23</cell></row><row><cell>Compressionr=4</cell><cell>25.38</cell><cell>0.86</cell><cell cols="3">9.02 10.67 0.23</cell></row><row><cell>Compressionr=8</cell><cell>25.14</cell><cell>0.84</cell><cell cols="3">9.68 10.50 0.25</cell></row><row><cell>Compressionr=16</cell><cell>24.87</cell><cell>0.82</cell><cell>9.90</cell><cell>9.84</cell><cell>0.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>The objective metrics of VAE reconstruction performance with different compression level r on the AudioSet evaluation set.</figDesc><table><row><cell>Time-masked mel-spectrogram</cell><cell>Frequency-masked mel-spectrogram</cell></row><row><cell>Original mel-spectrogram</cell><cell></cell></row><row><cell>Corresponding VAE latent</cell><cell>Corresponding VAE latent</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://audioldm.github.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://freesound.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://sound-effects.bbcrewind.co.uk/ search</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This research was partly supported by the British Broadcasting Corporation Research and Development (BBC R&amp;D), Engineering and Physical Sciences Research Council (EP-SRC) Grant EP/T019751/1 "AI for Sound", and a PhD scholarship from the Centre for Vision, Speech and Signal Processing (CVSSP), Faculty of Engineering and Physical Science (FEPS), University of Surrey. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Contrastive Language-Audio Pretraining</head><p>We follow the pipeline of the contrastive language-audio pretraining (CLAP) models proposed by <ref type="bibr" target="#b58">(Wu et al., 2022)</ref> to capture the similarity between text and audio, and project them into joint latent space. The training dataset includes the currently largest public dataset LAION-Audio-630K, the AudioSet dataset whose text caption is augmented with keywordto-caption by T5 model <ref type="bibr" target="#b45">(Raffel et al., 2020)</ref>, the AudioCaps dataset and the Clotho dataset <ref type="bibr" target="#b6">(Drossos et al., 2020)</ref>. The LAION-Audio-630K dataset contains 633, 526 language-audio pairs and 4325.39 hours of audio samples. The AudioSet dataset contains 1, 912, 024 pairs and 463.48 hours of audio samples. The AudioCaps dataset contains 49, 274 pairs and 136.87 hours of audio samples. The Clotho dataset contains 3, 839 pairs and 23.99 hours of audio samples. These datasets contain various natural sounds, audio effects, music and human activity.</p><p>Given the audio sample x and the text data y, we use an audio encoder and a text encoder to extract their embedding E x ? R L and E y ? R L respectively, where L is set as 512. We build the audio encoder based on HTSAT <ref type="bibr">(Chen et al., 2022a)</ref> and the text encoder based on RoBERTa <ref type="bibr" target="#b37">(Liu et al., 2019)</ref>. The symmetric cross-entropy loss used to train these contrastive encoders is:</p><p>where ? is a learnable temperature parameter and D is the batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Latent Diffusion Model</head><p>We adopt the UNet backbone of StableDiffusion <ref type="bibr" target="#b47">(Rombach et al., 2022)</ref> as the basic architecture of LDM for AudioLDM. As shown in Equation <ref type="formula">5</ref>, the UNet model is conditioned on both the time step t and the CLAP embedding E. We map the time step into a one-dimensional embedding and then concatenate it with E as conditioning information. Since our condition vector is only one-dimensional, we do not use the cross-attention mechanism in StableDiffusion for conditioning. Instead, we directly use the feature-wise linear modulation layer <ref type="bibr" target="#b42">(Perez et al., 2018)</ref> to merge conditioning information with the feature map of the UNet convolution block. The UNet backbone we use has four encoder blocks, a middle block, and four decoder blocks. With a basic channel number of c u , the channel dimensions of encoder blocks are</p><p>The channel dimensions of decoder blocks are the reverse of encoder blocks, and the channel of the middle block has 5c u dimensions. We add an attention block in the last three encoder blocks and the first three decoder blocks. Specifically, we add two multi-head self-attention layers with a fully-connected layer in the middle as the attention block. The number of heads is determined by dividing the embedding dimension of the attention block with a parameter c h . We set AudioLDM-S and AudioLDM-L with c u =128, c h =32, and c u =256, c h =64, respectively. In the forward process, we use N = 1000 steps.</p><p>A linear noise schedule from ? 1 = 0.0015 to ? N = 0.0195 is used. In sampling, we employ the DDIM <ref type="bibr" target="#b53">(Song et al., 2020)</ref> sampler with 200 sampling steps. For classifier-free guidance, a guidance scale w of 2.0 is used in Equation <ref type="formula">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Variational Autoencoder</head><p>We compress the mel-spectrogram X ? R T ?F of x into a small continuous space z ? R C? T r ? F r with a convolutional VAE, where T and F is the time and frequency dimension size respectively, C is the channel number of the latent encoding, and r is the compression level (downsampling ratio) of latent space. Both the encoder E(?) and the decoder D(?) are composed of stacked convolutional modules. In this way, VAE encoder could preserve the spatial correspondancy between mel-spectrogram and latent space, as it is shown in Figure <ref type="figure">7</ref>. Each module is formed by ResNet blocks <ref type="bibr">(Kong et al., 2021a)</ref> which are made up of convolutional layers and residual connections. The encoding z will be evenly split into two parts, z ? and z ? , with shape ( C 2 , T r , F r ), representing the mean and variance of the VAE latent space. The input of the decoder is a stochastic encoding ? = ?? + ?? ? , ? N (0, I). During generation, the decoder will be used to reconstruct the mel-spectrogram given the generated latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Super-Resolution</head><p>In Figure <ref type="figure">11</ref>, we show four cases of zero-shot audio super-resolution with AudioLDM-S: 1) violin, 2) sneezing sound from a woman, 3) baby crying, and 4) female speech. The sampling rate of input samples (left) is 8 kHz, and that of generated samples (middle) and ground-truth samples (right) is 16 kHz. Our visualization shows we can retain the ground-truth observation in the low-frequency part (below 8 kHz), while generating the high-frequency missing part (from 8 kHz to 16 kHz) with pretrained AudioLDM-S. The generated high-frequency information is consistent with the low-frequency observation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Inpainting</head><p>In Figure <ref type="figure">12</ref>, we show four samples of zero-shot audio inpainting with AudioLDM-S. The time length of each audio sample is 10 seconds. In the unprocessed part, we remove the content between 2.5 and 7.5 seconds from the ground-truth sample as the input of inpainting. In the inpainting result part, we show the generated samples guided by the same text prompt of the ground-truth sample. In the ground truth part, we show the ground-truth sample for comparison. In Figure <ref type="figure">13</ref>, we use one sample to demonstrate the audio inpainting guided by different text prompts. Given the observed audio signal shown in the top row, we guide the inpainting process with four different text prompts: 1) ambient music;</p><p>2) a man is speaking with bird calls in the background; 3) a cat is meowing; 4) raining with wind blowing. As can be seen, the observed audio signal is preserved in each generated sample, while the generated content can be controlled by text input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Music Control</head><p>In Figure <ref type="figure">15</ref>, we show the generated music samples when we control the music characteristics with text input. The first sample is generated by "Theme music with bass drum". Then, we add specific text information "flute", "fast, flute", or "flute in the background", to change the text input. The corresponding variations can be seen in generated mel-spectrograms. We use these samples to demonstrate the ability of AudioLDM to add new musical instruments to music samples, tune the speed of music, and control the foreground-background relations.</p><p>Theme music with bass drum.</p><p>Theme music with flute and bass drum.</p><p>Theme music with flute in the background and bass drum.</p><p>Fast theme music with flute and bass drum. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pitch Control</head><p>In Figure <ref type="figure">16</ref>, we show the ability of AudioLDM to control the pitch of generated samples. Pitch is an important characteristic of sound effects, music and speech. Here, we set the common text information as "Sine wave with ? ? ? pitch", and input the specific text information "low", "medium", and "high". The text-controlled pitch variation can be seen from the three generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sine wave with low pitch</head><p>Sine wave with medium pitch Sine wave with high pitch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material Control</head><p>In Figure <ref type="figure">17</ref>, we show the ability of AudioLDM to control the materials which generate audio samples. We show four samples generated by the common action "hit" between different materials, e.g., wooden object and wooden environment, or metal object and wooden environment.</p><p>Wooden object is hitting the wooden surface Metal object is hitting the wooden surface Two small plastic balls hit and drop on the ground Two small wooden balls hit and drop on the ground </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Order Control</head><p>In Figure <ref type="figure">18</ref>, we show the ability of AudioLDM to control the temporal order between generated compositional audio signals.</p><p>When the text description includes multiple sound effects, AudioLDM can generate the audio signals, and the temporal order between them is consistent with the text input.</p><p>Cat is purring followed by slow and tender meowing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A female speaking followed by footstep sound</head><p>A racing car is passing by and disappear A man is speaking followed by music playing Figure <ref type="figure">18</ref>. The examples of controlling the temporal order between generated compositional audio samples with AudioLDM-S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-to-Audio Generation</head><p>In Figure <ref type="figure">19</ref>, we show four text-to-audio generation results with AudioLDM-S. They include sound effects in natural environment, human speech, human activity, and sound from objects interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rubbing the paper into a ball</head><p>Large thunder storm in the ocean Two metal objects hitting sound.</p><p>A group of people is cheering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novel Audio Generation</head><p>In Figure <ref type="figure">20</ref>, we show four novel audio samples generated with AudioLDM-S. Their text description is rarely seen, e.g., "A wolf is singing a beautiful song.". We use them to exhibit the generalization ability of AudioLDM.</p><p>A music played by dog barking sounds.</p><p>A music played by finger snapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A music played by car horns</head><p>A wolf is singing a beautiful song. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Music Generation</head><p>In Figure <ref type="figure">21</ref>, we show four music samples generated with AudioLDM-S. Here, we are using the labels of AudioSet as text description for music generation, and we are able to specify the music genres of generated samples such as Classical music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swing music A cappella</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Afrobeat Classical music</head><p>Figure <ref type="figure">21</ref>. The examples of music generation with AudioLDM-S.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new way in sound synthesis</title>
		<author>
			<persName><forename type="first">U</forename><surname>Andresen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Audio Engineering Society. Audio Engineering Society</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving diffusion models for vocoder by considering inference in training</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Infergrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Resgrad: Residual denoising diffusion probabilistic models for text to speech</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mandic</surname></persName>
		</author>
		<idno>arXiv preprint:2212.14518</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clotho: an audio captioning dataset</title>
		<author>
			<persName><forename type="first">K</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hantrakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Ddsp</surname></persName>
		</author>
		<title level="m">Differentiable digital signal processing</title>
		<imprint>
			<date type="published" when="2001">2001. 2020</date>
			<biblScope unit="page">4643</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AudioSet: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PSLA: Improving audio tagging with pretraining, sampling, labeling, and aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3292" to="3306" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CNN architectures for largescale audio classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classifier-free diffusion guidance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Imagen video: High definition video generation with diffusion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno>arXiv preprint:2210.02303</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Digital synthesis of pluckedstring and drum timbres</title>
		<author>
			<persName><forename type="first">K</forename><surname>Karplus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Strong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="43" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fr?chet audio distance: A reference-free metric for evaluating music enhancement algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharifi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2350" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Audiocaps: Generating captions for audios in the wild</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="119" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno>arXiv preprint:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv preprint:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17022" to="17033" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><surname>Panns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Decoupling magnitude and phase estimation with deep resunet for music source separation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>arXiv preprint:2109.05418, 2021a</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Audiogen</surname></persName>
		</author>
		<idno>arXiv preprint:2209.15352</idno>
		<title level="m">Textually guided audio generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Audio super resolution using neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Enam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00853</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bilateral denoising diffusion models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Priorgrad: Improving conditional denoising diffusion models with data-driven adaptive prior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Binauralgrad: A two-stage conditional diffusion probabilistic model for binaural audio synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<idno>arXiv preprint:2205.14807</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Voicefixer</surname></persName>
		</author>
		<title level="m">Toward general speech restoration with neural vocoder</title>
		<imprint/>
	</monogr>
	<note>arXiv preprint:2109.13731, 2021a</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural vocoder is all you need for speech superresolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>arXiv preprint:2203.14941, 2022a</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ontology-aware learning and evaluation for audio tagging</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno>arXiv preprint:2211.12195</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning the spectrogram temporal resolution for audio classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno>arXiv preprint:2210.01719</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compositional visual generation with composable diffusion models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional sound generation using neural discrete time-frequency representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Simple pooling front-ends for efficient audio classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv preprint:2210.00943</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Separate what you describe: language-queried audio source separation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv preprint:2203.15147</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno>arXiv preprint:1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno>arXiv preprint:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Wavenet</surname></persName>
		</author>
		<idno>arXiv preprint:1609.03499</idno>
		<title level="m">A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Full-band general audio synthesis with score-based diffusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<idno>arXiv preprint:2210.14661</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grad-tts: A diffusion probabilistic model for text-to-speech</title>
		<author>
			<persName><forename type="first">V</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gogoryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sadekova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno>arXiv preprint:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno>arXiv preprint:2104.07636</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K S</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno>arXiv preprint:2205.11487</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Make-a-video: Textto-video generation without text-video data</title>
		<author>
			<persName><forename type="first">U</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<idno>arXiv preprint:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">D2c: Diffusion-decoding models for few-shot conditional generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>arXiv preprint:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<idno>arXiv preprint:2205.04421</idno>
		<title level="m">End-to-end text to speech synthesis with human-level quality</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lsgm: Score-based generative modeling in latent space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Towards robust speech superresolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2021">2058-2066, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
		<idno>arXiv preprint:2211:06687</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">CSTR VCTK corpus: English multi-speaker corpus for cstr voice cloning toolkit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Discrete diffusion model for textto-sound generation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Diffsound</surname></persName>
		</author>
		<idno>arXiv preprint:2207.09983</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Audio-to-image crossmodal generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>?elaszczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma?dziuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
