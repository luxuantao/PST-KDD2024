<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TCGM: An Information-Theoretic Framework for Semi-Supervised Multi-Modality Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-14">14 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinwei</forename><surname>Sun</surname></persName>
							<email>xinsun@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research-Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yilun</forename><surname>Xu</surname></persName>
							<email>xuyilun@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
							<email>yuqing.kong@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingjing</forename><surname>Hu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Yanjing Medical College</orgName>
								<orgName type="institution" key="instit2">Capital Medical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<email>yizhou.wang@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Adv. Inst. of Info. Tech</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="laboratory">Deepwise AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TCGM: An Information-Theoretic Framework for Semi-Supervised Multi-Modality Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-14">14 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2007.06793v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Total Correlation</term>
					<term>Semi-supervised</term>
					<term>Multi-modality</term>
					<term>Conditional Independence</term>
					<term>Information intersection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fusing data from multiple modalities provides more information to train machine learning systems. However, it is prohibitively expensive and timeconsuming to label each modality with a large amount of data, which leads to a crucial problem of semi-supervised multi-modal learning. Existing methods suffer from either ineffective fusion across modalities or lack of theoretical guarantees under proper assumptions. In this paper, we propose a novel information-theoretic approach -namely, Total Correlation Gain Maximization (TCGM) -for semisupervised multi-modal learning, which is endowed with promising properties: (i) it can utilize effectively the information across different modalities of unlabeled data points to facilitate training classifiers of each modality (ii) it has theoretical guarantee to identify Bayesian classifiers, i.e., the ground truth posteriors of all modalities. Specifically, by maximizing TC-induced loss (namely TC gain) over classifiers of all modalities, these classifiers can cooperatively discover the equivalent class of ground-truth classifiers; and identify the unique ones by leveraging limited percentage of labeled data. We apply our method to various tasks and achieve state-of-the-art results, including the news classification (Newsgroup dataset), emotion recognition (IEMOCAP and MOSI datasets), and disease prediction (Alzheimer's Disease Neuroimaging Initiative dataset).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning with data from multiple modalities has the advantage to facilitate information fusion from different perspectives and induce more robust models, compared with only using a single modality. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, to diagnose whether a patient has a certain disease or not, we can consult to its X-ray images, look into its medical records, or get results from clinical pathology. However, in many real applications, especially in some difficult ones (e.g. medical diagnosis), annotating such large-scale training data is prohibitively expensive and time-consuming. As a consequence, each modality of data may only contain a small proportion of labeled data from professional annotators, leaving a large proportion of unlabeled. This leads to an essential and challenging problem of semi-supervised multi-modality learning: how to effectively train accurate classifiers by aggregating unlabeled data of all modalities?</p><p>To achieve this goal, many methods have been proposed in the literature, which can be roughly categorized into two branches: (i) co-training strategy <ref type="bibr" target="#b4">[6]</ref>; and (ii) learning joint representation across modalities in an unsupervised way <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b26">28]</ref>. These methods suffer from either too strong assumptions or loss of information during fusing. Specifically, the co-training strategy relies largely on the "compatible" assumption that the conditional distributions of the data point labels in each modality are the same, which may not be satisfied in the real settings, as self-claimed in <ref type="bibr" target="#b4">[6]</ref>; while the latter branch of methods fails to capture the higher-order dependency among modalities, hence may end up in learning a trivial solution that maps all the data points to the same representation. A common belief in multi-modality learning <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20]</ref> is that conditioning on ground truth label Y , these modalities are conditionally independent, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. For example, to diagnose if one suffers from a certain disease, an efficient way is to leverage as many as modalities that are related to the disease, e.g., X-ray image, medical records and the clinical pathology. Since each modality captures the characteristics of the disease from different aspects, the information extracted from these modalities, in addition to the label, are not necessarily correlated with each other. This suggests that the ground truth label can be regarded as the "information intersection" across all the modalities, i.e., the amount of agreement shared by all the modalities.</p><p>Inspired by such an assumption and the fact that the Total Correlation <ref type="bibr" target="#b27">[29]</ref> can measure the amount of information shared by M (M ≥ 2) variables, in this paper, we propose Total Correlation Gain (TCG), which is a function of classifiers of all the modalities, as a surrogate goal for maximization of mutual information, in order to infer the ground-truth labels (i.e., information intersection among these modalities). Based on the proposed TCG, we devise an information-theoretic framework called Total Correlation Gain Maximization (TCGM) for semi-supervised multi-modal learning. By maximizing TCG among all the modalities, the classifiers for different modalities cooperatively discover the information intersection across all the modalities. It can be proved that the optimal classifiers for such a Total Correlation Gain are equivalent to the Bayesian posterior classifiers given each modality under some permutation function. With further leverage of labeled data, we can identify the Bayesian posterior classifiers. Furthermore, we devise an aggregator that employs all the modalities to forecast the labels of data. A simulated experiment is conducted to verify this theoretical result.</p><p>We apply TCGM on various tasks: (i) News classification with three pre-processing steps as different modalities, (ii) Emotion recognition with videos, audios, and texts as three modalities and (iii) disease prediction on medical imaging with the Structural magnetic resonance imaging (sMRI) and Positron emission tomography (PET) modalities. On these tasks, our method consistently outperforms the baseline methods especially when a limited percentage of labeled data are provided. To validate the benefit of jointly learning, we visualize that some cases of Alzheimer's Disease whose label are difficult to be predicted via supervised learning with single modality; while our jointly learned single modal classifier is able to correctly classify such hard samples.</p><p>The contributions can be summarized as follows: (i) We propose a novel informationtheoretic approach TCGM for semi-supervised multi-modality learning, which can effectively utilize information across all modalities. By maximizing the total correlation gain among all the modalities, the classifiers for different modalities cooperatively discover the information intersection across all the modalities -the ground truth. (ii) To the best of our knowledge, TCGM is the first in the literature that can be theoretically proved that, under the conditional independence assumption, it can identify the groundtruth Bayesian classifier given each modality. Further, by aggregating these classifiers, our method can learn the Bayesian classifier given all modalities. (iii) We achieve the state-of-the-art results on various semi-supervised multi-modality tasks including news classification, emotion recognition and disease prediction of medical imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semi-supervised multi-modal learning It is commonly believed in the literature that information of label is shared across all modalities. Existing work, which can be roughly categorized into two branches, suffers from either stronger but not reasonable assumptions or failure to capture the information (i.e., label) shared by all modalities. The first branch applies the co-training algorithm proposed by Blum et. al <ref type="bibr" target="#b4">[6]</ref>. <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b9">11]</ref> use weak classifiers trained by the labeled data from each modality to bootstrap each other by generating labels for the unlabeled data. However, the underlying compatible condition of such a method, which assumes the same conditional distributions for data point labels in each modality, may not be consistent with the real settings.</p><p>The second branch of work <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b16">18]</ref> centers on learning joint representations that project unimodal representations all together into a multi-modal space in an unsupervised way and then using the labeled data from each modality to train a classifier to predict the label of the learned joint representation. A representative of such a framework is the soft-Hirschfeld-Gebelein-Rényi (HGR) framework <ref type="bibr" target="#b29">[31]</ref>, which proposed to maximize the correlation among non-linear representations of each modality. However, HGR only measures the linear dependence between pair modalities, since it follows the principle of maximizing the correlation between features of different modalities. In contrast, our framework, i.e., Total Correlation Gain Maximization can pursue information about higher-order dependence. Due to the above reasons, both branches can not avoid learning a naive solution that classifies all data points into the same class.</p><p>To overcome these limitations, we propose an information-theoretic loss function based on Total Correlation which can not only require the assumption in the first branch of work but also can be able to identify the ground-truth label, which is the information intersection among these modalities. Therefore, our method can avoid the trivial solution and can learn the optimal, i.e., the Bayesian Posterior classifiers of each modality.</p><p>Total Correlation/Mutual information maximization Total Correlation <ref type="bibr" target="#b27">[29]</ref>, as an extension of Mutual Information, measures the amount of information shared by M (M ≥ 2) variables. There are several works in the literature that have combined Mutual Information (M = 2) with deep learning algorithms and have shown superior performance on various tasks. Belghazi et allet@tokeeonedot <ref type="bibr" target="#b3">[4]</ref> presents a mutual information neural estimator, which are utilized in a handful of applications based on the mutual information maximization (e.g., unsupervised learning of representations <ref type="bibr" target="#b12">[14]</ref>, learning node representations within graph-structured data <ref type="bibr" target="#b28">[30]</ref>). Kong and Schoenebeck <ref type="bibr" target="#b17">[19]</ref> provide another mutual information estimator in the co-training framework for the peer prediction mechanism, which has been combined with deep neural networks for crowdsourcing <ref type="bibr" target="#b6">[8]</ref>. Xu et.al <ref type="bibr" target="#b30">[32]</ref> proposes an alternative definition of information, which is more effective for structure learning. However, those three estimators can only be applied to two-view settings. To the best of our knowledge, there are no similar studies that focus on a general number of modalities, which is very often in real applications. In this paper, we propose to leverage Total Correlation to fill in such a gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Notations Given a random variable X, X denotes its realization space and x ∈ X denotes an instance. The P X denotes the probability distribution function over X and p(x) ∶= dP X (x) denotes the density function w.r.t the Lebesgue measure. Further, given a finite set X , ∆ X denotes the set of all distributions over X . For every integer M , [M ] denotes the set {1, 2, . . . , M }. For a vector v, v i denotes its i-th element.</p><p>Total Correlation The Total Correlation (TC), as an extension of mutual information, measures the "amount of information" shared by M (≥ 2) random variables:</p><formula xml:id="formula_0">TC(X 1 , ..., X m ) = M i=1 H(X i ) − H(X 1 , ..., X M ),<label>(1)</label></formula><p>where H is the Shannon entropy. As defined, the TC degenerates to mutual information when M = 2. The ∑ M i=1 H(X i ) measures the total amount of information when treating X 1 , ..., X M independently; while the H(X 1 , ..., X M ) measures the counterpart when treating these M variables as a whole. Therefore, the difference between them implies the redundant information, i.e., the information shared by these M variables.</p><p>Similar to mutual information, the TC is equivalent to the Kullback-Leibler (KL)divergence between P X 1 ×...×X M and product of marginal distribution ⊗ M m=1 P X m :</p><formula xml:id="formula_1">TC(X 1 , ..., X M ) = D KL dP X 1 ×...×X M d ⊗ M m=1 P X m = E P X 1 ×...×X M log dP X 1 ×...×X M d ⊗ M m=1 P X m ,<label>(2)</label></formula><p>where D KL (P Q) = E P log dP dQ . Intuitively, larger KL divergence between joint and marginal distribution indicates more dependence among these M variables. To better characterize such a property, we give a formal definition of "Point-wise Total Correlation" (PTC):</p><p>Definition 1 (Point-wise Total Correlation). Given M random variables X 1 , ..., X M , the Point-wise Total Correlation on (x 1 , ..., x m ) ∈ X 1 × ... × X M , i.e., PTC(x 1 , ..., x M ) is defined as:</p><formula xml:id="formula_2">PTC(x 1 , ..., x M ) = log p(x 1 , ..., x M ) p(x 1 )...p(x M ) Further, the R(x 1 , ..., x M ) ∶= p(x 1 ,...,x M ) p(x 1 )...p(x M</formula><p>) is denoted as the joint-margin ratio.</p><p>Remark 1. The Point-wise Total Correlation can be understood as the point-wise distance between joint distribution and the marginal distribution. In more details, as noted from <ref type="bibr" target="#b13">[15]</ref>, by applying first-order Taylor-expansion, we have log p(x) q(x) ≈ log 1 + p(x)−q(x)</p><formula xml:id="formula_3">q(x) = p(x)−q(x) q(x)</formula><p>. Therefore, the expected value of PTC(⋅) can well measure the amount of information shared among these variables, which will be shown later in detailed.</p><p>For simplicity, we denote p</p><formula xml:id="formula_4">[M ] (x) ∶= p(x 1 , ..., x M ) and q [M ] (x) ∶= ∏ M i=1 p(x i ).</formula><p>According to dual representation in <ref type="bibr" target="#b25">[27]</ref>, we have the following lower bound for KL divergence between p and q, and hence TC.</p><p>Lemma 1 (Dual version of f -divergence <ref type="bibr" target="#b25">[27]</ref>).</p><formula xml:id="formula_5">D KL p [M ] q [M ] ≥ sup g∈G E x∼p [M ] [g(x)] − E x∼q [M ] e g(x)−1 (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where G is the set of functions that maps X 1 × X 2 × ⋅ ⋅ ⋅ × X M to R. The equality holds if and only if g(x 1 , x 2 , . . . , x M ) = 1 + PTC(x 1 , ..., x M ), and the supremum is</p><formula xml:id="formula_7">D KL p [M ] q [M ] = E p [M ] (PTC).</formula><p>The Lemma 1 is commonly utilized for estimation of Mutual information <ref type="bibr" target="#b3">[4]</ref> or optimization as variational lower bound in the machine learning literature. Besides, it also informs that the PTC is the optimal function to describe the amount of information shared by these M variables. Indeed, such shared information is the information intersection among these variables, i.e., conditional on such information, these M variables are independent of each other. To quantitatively describe this, we first introduce the conditional total correlation (CTC). Similar to TC, CTC measures the amount of information shared by these M variables conditioning on some variable Z:</p><formula xml:id="formula_8">Definition 2 (Conditional Total Correlation (CTC)). Given M + 1 random variables X 1 , ..., X M , Z, the Conditional Total Correlation (CTC(X 1 , ..., X M Z)) is defined as CTC(X 1 , ..., X M Z) = M i=1 H(X i Z) − H(X 1 , ..., X M Z)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>Problem statement In the semi-supervised muli-modal learning scenario, we have access to an unlabeled dataset D u = {x</p><formula xml:id="formula_9">[M ] i } i and a labeled dataset D l = {(x [M ] i , y i )} i . Each la- bel y i ∈ C, where C denotes the set of classes. Each datapoint x [M ] i ∶= {x 1 i , x 2 i , . . . , x M i x m i ∈ X m } consists of M modalities,</formula><p>where X m denotes the domain of the m-th modality. Datapoints and labels in D l are i.i.d. samples drawn from the joint distribution</p><formula xml:id="formula_10">U X [M ] ,Y (x 1 , x 2 , . . . , x M , y) ∶= Pr(X 1 = x 1 , X 2 = x 2 , . . . , X M = x M , Y = y). Data points in D u are i.i.d. samples drawn from joint distribution U X [M ] (x 1 , x 2 , . . . , x M ) ∶= ∑ c∈C U X [M ] ,Y (x 1 , x 2 , . . . , x M , y = c).</formula><p>Denote the prior of the ground truth labels by p * = (Pr(Y = c)) c . Upon the labeled and unlabeled datasets, our goal is to train M classifiers h [M ] ∶= {h 1 , h 2 , . . . , h M } and an aggregator ζ such that ∀m, h m ∶ X m → ∆ C predicts the ground truth y based on a m-th modality x m and ζ ∶ X 1 ×X 2 ×⋯×X M → ∆ C predicts the ground truth y based on all of the modalities x [M ] .</p><p>Outline We will first introduce the assumptions regarding the ground truth label Y and prior distribution on (X 1 , ..., X M , Y ) in section 4.1. In section 4.2, we will present our method, i.e., maximize the total correlation gain on unlabeled dataset D u . Finally, we will introduce our algorithm for optimization in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Assumptions for Identification of Y</head><p>In this section, we first introduce two basic assumptions to ensure that the ground-truth label can be identified. According to Proposition 2.1 in <ref type="bibr" target="#b0">[1]</ref>, the label Y can be viewed as the generating factor of data X. Such a result can be extended to multiple variables (please refer supplementary for details), which implies that Y is the common generating factor of X 1 , ..., X M . Motivated by this, it is natural to assume that the ground truth label Y is the "information intersection" among X 1 , ..., X M , i.e., all of the modalities are independent conditioning on the ground-truth:</p><formula xml:id="formula_11">Assumption 1 (Conditional Independence). Conditioning on Y , X 1 , X 2 , . . . , X M are independent, i.e., ∀x 1 , . . . , x M , Pr(X [M ] = x [M ] Y = c) = m Pr(X m = x m Y = c), for anyc ∈ C.</formula><p>On the basis of this assumption, one can immediately get the conditional total correlation gain CTC(X 1 , ..., X M Y ) = 0. In other words, conditioning on Y , there is no extra information shared by these M modalities, which is commonly assumed in the literature of semi-supervised learning <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20]</ref>. However, the Y may not be the unique information intersection among these M modalities. Specifically, the following lemma establishes the rules for such information intersection to hold:</p><formula xml:id="formula_12">Lemma 2. Given assumption 1, R(x 1 , ..., x M ) (Joint-marginal ratio definition 1) has R(x 1 , ..., x M ) = c∈C ∏ m Pr(Y = c X m = x m ) (Pr(Y = c)) M −1</formula><p>Further, the optimal g in lemma 1 satisfies g(x 1 , ...</p><formula xml:id="formula_13">, x M ) = 1+ log ∑ c∈C ∏ m Pr(Y =c x m ) (Pr(Y =c)) M −1 .</formula><p>In other words, in addition to {Pr(Y = c X m = x m )} m , p * c , there are other solutions {a x 1 , ..., a x M }, r with a x i ∈ ∆ C (for i ∈ C) and r ∈ ∆ C that can make the g optimal, as long as its joint-marginal ratio is equal to the ground-truth one:</p><formula xml:id="formula_14">c∈C ∏ m a x m (r a c ) M −1 = R(x 1 , ..., x M )<label>(4)</label></formula><p>To make {Pr(Y = c X m = x m )} m , p * c identifiable w.r.t a trivial permutation, we make the following trivial assumption on Pr(X 1 , ..., X M , Y ).</p><p>Assumption 2 (Well-defined Prior). The solutions {a x 1 , ..., a x M }, r a and {b x 1 , ..., b x M }, r b for Eq. (4) are equivalent under the permutation ∏ ∶ C → C:</p><formula xml:id="formula_15">a x m = ∏ b x m , r a = ∏ r b .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Total Correlation Gain Maximization (TCGM)</head><p>Assumption 1 indicates that the label Y is the generating factor of all modalities, and assumption 2 further ensures its uniqueness under permutation. Our goal is to learn the ground-truth label Y which is the information intersection among M modalities. In this section, we propose a novel framework, namely Total Correlation Gain Maximization (TCGM) to capture such an information intersection, which is illustrated in Figure <ref type="figure">.</ref> 2. To the best of our knowledge, we are the first to theoretically prove the identification of ground truth classifiers on semi-supervised multi-modality data, by generalizing <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b6">8]</ref> that can only handle two views in multi-view scenario. The high-level spirit is designing TC-induced loss over classifiers of every modality. By maximizing such a loss, these classifiers can converge to Bayesian posterior, which is the optimal solution of TC as expectation of the loss. First, we introduce the basic building blocks for our method.</p><p>Classifiers h [M ] In order to leverage the powerful representation ability of deep neural network (DNN), each classifier h m (x m ; Θ m ) is modeled by a DNN with parameters Θ m . For each modality m, we denote the set of all such classifiers by H m and H [M ] ∶= {H 1 , H 2 , . . . , H M }.</p><p>Modality Classifiers-Aggregator ζ Given M classifiers for each modality h [M ] and a distribution p = (p c ) c ∈ ∆ C , the aggregator ζ which predicts the ground-truth label by aggregating classifiers of all modalities, is constructed by</p><formula xml:id="formula_16">ζ(x [M ] ; h [M ] , p) = Normalize ∏ m h m (x m i ) c (p c ) M −1 c where Normalize(v) ∶= v ∑ c vc for all v ∈ ∆ C . Reward Function R We define a reward function R ∶ M ∆ C × ... × ∆ C → R</formula><p>that measures the "amount of agreement" among these classifiers. Note that the desired classifiers should satisfy Eq. (4).</p><p>Inspired by Lemma 1, we can take the empirical total correlation gain of N samples, i.e., the lower bound of Total Correlation as our maximization function. Specifically, given a reward function R, the empirical total correlation with respect to classifiers h [M ] , a prior p ∈ ∆ C measures the empirical "amount of agreement" for these M classifiers at the same sample (x 1 i , ..., x M i ) ∈ D u , with additionally a punishment of them on different samples:</p><formula xml:id="formula_17">x 1 i1 ∈ X 1 , ..., x M i M ∈ X M with i 1 ≠ i 2 ≠ ... ≠ i M : T Cg[R]({x [M ] i } N i=1 ; h [M ] , p) ∶= 1 N i R(h 1 (x 1 i ), ..., h M (x M i )) − 1 N ! (N − M )! i1≠i2≠⋯≠i M e R(h 1 (x 1 i 1 ),...,h M (x M i M</formula><p>))−1</p><p>(5)</p><p>for simplicity we denote T Cg[R]({x</p><formula xml:id="formula_18">[M ] i } N i=1 ; h [M ]</formula><p>, p) as T Cg (N ) . Intuitively, we expect our classifiers to be consistent on the same sample; on the other hand, to disagree on different samples to avoid learning a trivial solution that classifies all data points into the same class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3 (Bayesian posterior classifiers/aggregator). The h</head><formula xml:id="formula_19">∀m, h m * (x m ) c = Pr(Y = c X m = x m ); ζ * (x [M ] ) c = Pr(Y = c X [M ] = x [M ]</formula><p>).</p><p>Note that from Eq. ( <ref type="formula">5</ref>) that our maximization goal, i.e., T Cg (N ) relies on the form of reward function R. The following Lemma tells us the form of optimal reward function, with which we can finally give an explicit form of T Cg (N ) . Lemma 3. The R * that maximizes the expectation of T Cg (N ) can be represented as the Point-wise Total Correlation function, which is the function of Bayesian classifiers and the prior of ground truth labels (p * c ) c :</p><formula xml:id="formula_20">R * (h 1 (x 1 ), ..., h M (x M )) = 1 + PTC(x 1 , ..., x M ) = 1 + log c∈C ∏ m h m * (x m ) c (p * c ) M −1</formula><p>Total Correlation Gain Bring R * to Eq. (5), we have:</p><formula xml:id="formula_21">T Cg({x [M ] i } N i=1 ; h [M ] , p) ∶=1 + 1 N i log c∈C ∏ m h m (x m i ) c (p c ) M −1 − 1 N ! (N − M )! i1≠i2≠⋯≠i M c∈C ∏ m h m (x m im ) c (p c ) M −1 . (6)</formula><p>As inspired by Lemma 3, we have that these Bayesian posterior classifiers are maximizers of the expected total correlation gain. Therefore, we can identify the equivalent class of Bayesian posteriors by minimizing −T Cg (N ) on unlabeled dataset D u . By additionally minimize expected cross entropy (CE) loss on D l , we can identify the unique Bayesian classifiers since they are respectively the minimizers of CE loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3 (Main theorem). Define the expected total correlation gain eT Cg(h</head><formula xml:id="formula_22">[M ] , p): eT Cg(h 1 , ..., h M , p) ∶= E x [M ] i ∼U X [M ] T Cg(x [M ] i ; h [M ] , p)</formula><p>Given the conditional independence assumption 1 and well-defined prior assumption 2, we have that the maximum value of eT Cg is Total Correlation of M modalities, i.e., TC(X 1 , ..., X M ). Besides, Ground-truth → Maximizer (h</p><formula xml:id="formula_23">[M ] * , p * ) is a maximizer of eT Cg(h [M ] , p). In other words, ∀h [M ] ∈ H [M ] , p ∈ ∆ C , eT Cg(h 1 * , ..., h M * , p * ) ≥ eT Cg(h 1 , ..., h M , p). The corresponding optimal aggregator is ζ ⋆ , i.e., ζ ⋆ (x [M ] ) c = Pr(Y = c X [M ] = x [M ] ).</formula><p>Maximizer → (Permuted) Ground-truth If the prior is well defined, then for any maximizer of eT Cg, ( h[M] , p), there is a permutation ∏ ∶ C → C such that:</p><formula xml:id="formula_24">hm (x m ) c = P(Y = ˜ (c) X m = x m ), pc = P(Y = ˜ (c))<label>(7)</label></formula><p>The proof is in Appendix A. Note from our main theorem that by maximizing the eT Cg, we can get the total correlation of M modalities, which is the ground-truth label Y , and also the equivalent class of Bayesian posterior classifier under permutation function. In order to identify the Bayesian posterior classifiers, we can further minimize cross-entropy loss on labeled data D l since the Bayesian posterior classifiers are the only minimizers of the expected cross-entropy loss. On the other hand, compared with only using D l to train classifiers, our method can leverage more information from D u , which can be shown in the experimental result later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization</head><p>Since eT Cg is intractable, we alternatively maximize the empirical total correlation gain, i.e., T Cg (N ) to learn the optimal classifiers. To identify the unique Bayesian posteriors, we should further utilize labeled dataset D l in a supervised way. Our whole optimization process is shown in Appendix, which adopts iteratively optimization strategy that is roughly contains two steps in each round: (i) We train the M classifiers using the classic cross entropy loss on the labeled dataset D l and (ii) using our information-theoretic loss function L TC on the unlabeled dataset D u . To learn the Bayesian posterior classifiers more accurately, the (ii) can help to learn the equivalent class of Bayesian Posterior Classifiers and (i) is to learn the correct and unique classifiers. As shown in Figure <ref type="figure" target="#fig_2">2</ref>, by optimizing L (B) TC (Eq. (5) with B denoting the number of samples in each batch), we reward the M classifiers for their agreements on the same data point and punish the M classifiers for their agreements on different data points.</p><p>Loss function L CE for labeled data We use the classic cross entropy loss for labeled data. Formally, for a batch of data points {x</p><formula xml:id="formula_25">[M ] i } B</formula><p>i=1 drawn from labeled data D l , the cross entropy loss L CE for each classifier h m is defined as L CE ({(x</p><formula xml:id="formula_26">[M ] i , y i )} B i=1 ; h m ) ∶= 1 B ∑ i − log(h m (x m i ) yi ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function L (B)</head><p>TC for unlabeled data For a batch of data points {x</p><formula xml:id="formula_27">[M ] i } B i=1 drawn from unlabeled data D u , our loss function L (B)</formula><p>TC ∶= −T Cg (B) that is defined in Eq. ( <ref type="formula">6</ref>) with N replaced by number of batch size B. When N is large, we only sample a fixed number of samples from product of marginal distribution to estimate the second term in Eq. ( <ref type="formula">6</ref>), which makes training more amenable.</p><p>Prediction After optimization, we can get the classifiers {h m } m . The prior p c can be estimated from data, i.e., p c =</p><formula xml:id="formula_28">∑ i∈[N ] 1(yi=c) N</formula><p>. Then based on Eq. ( <ref type="formula" target="#formula_24">7</ref>), we can get the aggregator classifier ζ for prediction. Specifically, given a new sample x[M] , the predicted label is ỹ ∶= arg max c ζ(x [M ] ) c .  We first conduct a simulated experiment on synthetic data to validate our theoretical result of TCGM. Specifically, we will show the effectiveness of Total Correlation Gain T Cg for unsupervised clustering of data. Further, with few labeled data, our TCGM can give accurate classification.</p><p>In more detail, we synthesize the data of three modalities from a specific Gaussian distribution P (X i y) (i = 1, 2, 3). The clustering accuracy is calculated as classification accuracy by assuming the label is known. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, our method TCGM has competitive performance compared to well established clustering algorithms K-means++ <ref type="bibr" target="#b1">[2]</ref> and spectral clustering <ref type="bibr" target="#b23">[25]</ref>. Based on the promising unsupervised learning result, as shown by the light blue line (the top line) in Figure <ref type="figure" target="#fig_3">3</ref>, our method can accurately classify the data even with only a small portion of labeled data. In contrast, HGR <ref type="bibr" target="#b29">[31]</ref> degrades since it can only capture the linear dependence and fail when higher-order dependency exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Applications</head><p>In this section, we evaluate our method on various multi-modal classification tasks: (i) News classification (Newsgroup) (ii) Emotion recognition: IEMOCAP, MOSI and (iii) Disease prediction of Alzheimer's Disease on 3D medical Imaging: Alzheimer's Disease Neuroimaging Initiative (ADNI). Our method TCGM is compared with : CE separately trains classifiers for each modality by minimizing cross entropy loss of only labeled data; HGR <ref type="bibr" target="#b29">[31]</ref> learns representation by maximizing correlation of different modalities; and LMF <ref type="bibr" target="#b21">[23]</ref> performs multimodal fusion using low-rank tensors. The optimal hyperparameters are selected according to validation accuracy, among which the learning rate is optimized from {0.1, 0.01, 0.001, 0.0001}. All experiments are repeated five times with different random seeds. The mean test accuracies and standard deviations of single classifiers the aggregator (ζ) are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">News Classification</head><p>Dataset Newsgroup [5]<ref type="foot" target="#foot_0">8</ref> is a group of news classification datasets. Following <ref type="bibr" target="#b14">[16]</ref>, each data point has three modalities, PAM, SMI and UMI, collected from three different preprocessing steps <ref type="foot" target="#foot_1">9</ref> . We evaluate TCGM and the baseline methods on 3 datasets from Newsgroup: News-M2, News-M5, News-M10. They contain 500, 500, 1000 data points with 2, 5, 10 categories respectively. Following <ref type="bibr" target="#b31">[33]</ref>, we use 60% for training, 20% for validation and 20% for testing for all of these three datasets.</p><p>Implementation details We synthesize two different label rates (the percentage of labeled data points in each modality): {10%, 30%} for each dataset. We follow <ref type="bibr" target="#b31">[33]</ref> for classifiers. Adam with default parameters and learning rate γ u = 0.0001, γ l = 0.01 is used as the optimizer during training. Batch size is set to 32. We further compare with two additional baselines: VAT <ref type="bibr" target="#b22">[24]</ref> uses adversarial training for semi-supervised learning; PVCC <ref type="bibr" target="#b31">[33]</ref> that considers the consistency of data points under different modalities. As shown in Fig. <ref type="figure" target="#fig_4">4</ref>, TCGM achieves the best classification accuracy for both single classifier and aggregators, especially when the label rate is small. This shows the efficacy of utilizing the cross-modal information during training as compared to others that are unable to utilize the cross-modal information. Moreover, we can achieve further improvement by aggregating classifiers on all modalities, which shows the benefit of aggregating knowledge from different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Emotion Recognition</head><p>Dataset We evaluate our methods on two multi-modal emotion recognition datasets: IEMOCAP dataset <ref type="bibr" target="#b5">[7]</ref> and MOSI dataset <ref type="bibr" target="#b32">[34]</ref>. The goal for both datasets is to identify speaker emotions based on the collected videos, audios and texts. The IEMOCAP consists of 151 sessions of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. The MOSI is composed of 93 opinion videos from YouTube movie reviews. We follow the settings in <ref type="bibr" target="#b21">[23]</ref> for the data splits of training, validation and test set. For IEMOCAP, we conduct experiments on three different emotions: happy, angry and neutral emotions; for MOSI dataset we consider the binary classification of emotions: positive and negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We synthesize three label rates for each dataset (the percentage of labeled data points in each modality): {0.5%, 1%, 1.5%} for IEMOCAP and {1%, 2%, 3%} for MOSI. For a fair comparison, we follow architecture setting in <ref type="bibr" target="#b21">[23]</ref>. We adopt the modality encoder architectures in <ref type="bibr" target="#b21">[23]</ref> as the single modality classifiers for CE and TCGM, while adopting the aggregator on the top of modality encoders for LMF and HGR. Adam with default parameters and learning rate γ u = 0.0001, γ l = 0.001 is used as the optimizer. The batch size is set to 32.</p><p>We report the AUC (Area under ROC curve) for the aggregators on all the modalities and single modality classifiers by different methods. We only report the AUC of LMF and HGR on all modalities since they do not have single modality classifiers. For single modality classifiers, we show results on the text modality on happy emotion (d), audio modality on neutral emotion (e) the video modality on angry emotion on IEMOCAP; and (h) the video modality (i) the audio modality on MOSI. Please refer to supplementary material for complete experimental result. As shown in Figure <ref type="figure" target="#fig_5">5</ref>, aggregators trained by TCGM outperform all the baselines given only tiny fractions of labeled data. TCGM improves the AUC of the single modality classifiers significantly, which shows the efficacy of utilizing the cross-modal information during the training of our method. As label rates continue to grow, the advantage of our method over CE decreases since more information is provided for CE to learn the ground-truth label.</p><p>Our method also outperforms other methods in terms of the prediction based on all the modalities, especially when the label rate is small. This shows the superiority of our method when dealing with a limited amount of annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Disease prediction of Alzheimer's Disease</head><p>Dataset Early prediction of Alzheimer's Disease (AD) is attracting increasing attention since it is irreversible and very challenging. Besides, due to privacy issues and high collecting costs, an efficient classifier with limited labeled data is desired. To validate the effectiveness of our method on this challenging task, we only keep labels of a limited percentage of data, which is obtained from the most popular Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset <ref type="foot" target="#foot_2">10</ref> , with 3D images sMRI and PET. DARTEL VBM pipeline <ref type="bibr" target="#b1">[2]</ref> is implemented to pre-process the sMRI data, and then images of PET were reoriented into a standard 91 × 109 × 91 voxel image grid in MNI152 space, which is same with sMRIs'. To limit the size of images, only the hippocampus on both sides are extracted as input in the experiments. We denote subjects that convert to Alzheimer's disease (MCI c ) as AD, and subjects that remain stable (MCI s ) as NC (Normal Control). Our dataset contains 300 samples in total, with 144 AD and 156 NC. We randomly choose 60% for training, 20% for validation and 20% for testing stage. Implementation details We synthesize two different label rates (the percentage of labeled data points): {10%, 50%}. DenseNet is used as the classifier. Two 3D convolutional layers with the kernel size 3 × 3 × 3 are adopted to replace the first 2D convolutional layers with the kernel size 7 × 7. We use four dense blocks with the size of (6, 12, 24, 16).</p><p>To preserve more low-level local information, we discard the first max-pooling layer that follows after the first convolution layer. Adam with default parameters and learning rate γ u = γ l = 0.001 are used as the optimizer during training. We set Batch Size as only 12 due to the large memory usage of 3D images. Random crop of 64 × 64 × 64, random flip and random transpose are applied as data augmentation.</p><p>Figure <ref type="figure">6</ref> shows the accuracy of classifiers for each modality and the aggregator. Our method TCGM outperforms the baseline methods in all settings especially when the label rate is small, which is desired since it is costly to label data. To further illustrate the advantage of our model over others in terms of leveraging the knowledge of another modality, we visualize two MCI c s, denoted as MCI 1 c and MCI 2 c , which are mistakenly classified as NC by CE's classifier for sMRI and PET modality, respectively. The volume and standardized uptake value (SUV) (a measurement of the degree of metabolism), whose information are respectively contained by sMRI and PET data, are linearly mapped to the darkness of the red and blue. Darker color implies smaller volume and SUV, i.e., more probability of being AD. As shown in Figure <ref type="figure">7</ref>, the volume (SUV) of MCI 1 c (MCI 2 c ) is similar to NC, hence it is reasonable for CE to mistakenly classify it by only using the information of volume (SUV). In contrast, TCGM for each modality can correctly classify both cases as AD, which shows the better learning of the information intersection (i.e., the ground truth) during training, facilitated by the leverage of knowledge from another modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose an information-theoretic framework on multi-modal data, Total Correlation Gain Maximization (TCGM), in the scenario of semi-supervised learning. Specifically, we learn to infer the ground truth labels shared by all modalities by maximizing the total correlation gain. Conditioning on a common assumption that all modalities are independent given the ground truth label, it can be theoretically proved our method can learn the Bayesian posterior classifier for each modality and the Bayesian posterior aggregator for all modalities. Extensive experiments on Newsgroup, IEMOCAP, MOSI and ADNI datasets are conducted and achieve promising results, which demonstrates the benefit and utility of our framework. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Multiple modalities are independent conditioning on the ground truth; Ground truth is the "information intersection" of all of the modalities.</figDesc><graphic url="image-1.png" coords="2,192.48,322.80,230.38,83.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and ζ * are called Bayesian posterior classifiers and Bayesian posterior aggregator if they satisfy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Empirical Total Correlation Gain T Cg(N )   </figDesc><graphic url="image-2.png" coords="10,152.06,115.83,311.24,94.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Clustering and classification accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Test accuracies (mean ± std. dev. ) on Newsgroups datasets</figDesc><graphic url="image-3.png" coords="11,134.77,358.76,345.82,92.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a,b,c) AUC of Aggregators on happy, angry and neutral emotion recognition on IEMOCAP data: (d, e,f) AUC on text, audio and video modality modality classifiers on IEMOCAP: AUC of other composition of (modality, emotion) are listed in supplementary material. (g,h,i) AUC on MOSI data: AUC of (g) Aggregators on all modalities and single classifiers on (h) Video modality (i) Audio modality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Test accuracies (mean ± std.) on ADNI dataset</figDesc><graphic url="image-5.png" coords="14,169.35,218.59,276.67,121.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. AUC of single modality classifiers by CE and TCGM.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_0">http://qwone.com/~jason/20Newsgroups/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_1">PAM (Partitioning Around Medoïds preprocessing), SMI (Supervised Mutual Information preprocessing) and UMI (Unsupervised Mutual Information preprocessing)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_2">www.loni.ucla.edu/ADNI</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement Yizhou Wang's work is supported by MOST-2018AAA0102004, NSFC-61625201, DFG TRR169/NSFC Major International Collaboration Project "Crossmodal Learning". Yuqing Kong's work is supported by Science and Technology Innovation 2030 -"The New Generation of Artificial Intelligence" Major Project No.2018AAA0100901, China.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>proof of Lemma 3. Note that the expected form of Eq. ( <ref type="formula">5</ref>) is the right hand side of Eq. ( <ref type="formula">3</ref>), since {x</p><p>. Therefore, from Lemma 1, one can immediately get the conclusion. Lemma 4. Given a joint distribution p(x 1 , ..., x M , y), where y is a discrete random variable, we can always find M independent random variables z 1 , ..., z M such that</p><p>Proof. This proof is a generalization of the Proposition in <ref type="bibr" target="#b0">[1]</ref>. For z 1 , ..., z M i.i.d ∼ U nif orm(0, 1), then from <ref type="bibr" target="#b0">[1]</ref> we have x m y = F −1 y,m (z m ) where F y,m (t) = P(x m ≤ t y) where P(x m ≤ t y) is the cumulative distribution function of p(x m y).</p><p>Lemma 5. Given assumption 1, then the Marginal-joint ratio (definition 1) R(x 1 , ..., x M ) has</p><p>Further, the optimal g to make the equality holds in Lemma 1 has</p><p>One can immediately gets the conclusion for g from Lemma 1.</p><p>Theorem 4 (Main theorem). Define the expected total correlation gain T Cg(h 1 , ..., h M , p)</p><p>Given the conditional independence assumption 1 and well-defined prior assumption 2, we have that Ground-truth → Maximizer (h</p><p>Maximizer → (Permuted) Ground-truth If the prior is well defined, then for any maximizer of T Cg, ( h[M] , p), there is a permutation π ∶ C → C such that:</p><p>Proof. We have</p><p>Ground-truth → Maximizer From definition 1, i.e.,</p><p>Inspired by Lemma 3 and 1, we have that the T Cg(h</p><p>, p ⋆ ) can achieve the maximum value, which equals to TC(X 1 , ..., x M ).</p><p>Maximizer → (Permuted) Ground-truth For any maximizer ( h[M] , p), we have from Lemma 3 that</p><p>which means that the ( h[M] , p) satisfies Eq. ( <ref type="formula">4</ref>). With assumption 2, we have that there exists a permutation pi ∶ C → C such that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithm</head><p>We show the pipeline of TCGM in Alg 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Complexity</head><p>The overall loss function of our TCGM method is composed of</p><p>TC , as the addition of two terms, namely the term (a) ( 1</p><p>) in Eq. ( <ref type="formula">6</ref>). The term (a) with N samples generated from joint distribution p(x 1 , . . . , x M ) for each modality; hence, the optimization of the term (a) is O(M ). For term (b) with N ! (N −M )! samples generated from marginal distribution Π m p(x m ), it is the sum of N terms by grouping terms with h m (x m i ) for each i ∈ [N ] with h m (x m 1 ), . . . , h m (x m N )f orm ∈ [M ] calculated ahead (which is O(M )), hence is linear scale with respect to M . Therefore, the time complexity for our loss function is O(M ).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Extended experiments results</head><p>We show the complete result of single modality classifiers on Emotion Recognition task in Figure <ref type="figure">6</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast diffeomorphic image registration algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ashburner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Co-training and expansion: Towards bridging theory and practice</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Co-clustering of multi-view datasets: a parallelizable approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grimal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Data Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2018. 2012. 2012</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Mine: mutual information neural estimation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
				<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iemocap: interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Max-mig: an information theoretic approach for joint learning from crowds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>ICLR2019</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Correlational neural networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable and effective deep cca via soft decorrelation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning and feature evaluation for rgb-d object recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Co-adaptation of audio-visual speech and gesture classifiers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Multimodal interfaces</title>
				<meeting>the 8th international conference on Multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pac generalization bounds for co-training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Euclidean information theory of networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An improved co-similarity measure for document clustering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grimal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Applications</title>
				<imprint>
			<date type="published" when="2010-12">December 2010</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to extract entities from labeled and unlabeled text</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-modal emotion recognition using semisupervised learning and multiple neural networks in the wild</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
				<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Water from two rocks: Maximizing the mutual information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schoenebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM Conference on Economics and Computation</title>
				<meeting>the 2018 ACM Conference on Economics and Computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The value of agreement, a new boosting algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Learning Theory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised improvement of visual detectors using co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Naive (bayes) at forty: The independence assumption in information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
				<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved multimodal deep learning with variation of information</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The multiinformation function as a tool for measuring stochastic dependence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Studenỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vejnarová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An efficient approach to informative feature extraction from multimodal data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08979</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A theory of usable information under computational constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>ArXiv abs/2002.10689</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-modal learning with incomplete modalities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<idno>ArXiv abs/1606.06259</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
