<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi</title>
				<funder ref="#_YKJVMsD">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_y9Unend #_jQ6rJTB #_yCrXdYm #_MQh7DVx">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_zTgvSDa">
					<orgName type="full">National Key Research Plan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yue</forename><surname>Zheng</surname></persName>
							<email>cczhengy@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guidong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Chenshu</roleName><forename type="first">Yunhao</forename><surname>Liu</surname></persName>
							<email>yunhaoliu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenshu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3307334.3326081</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gesture Recognition</term>
					<term>Channel State Information</term>
					<term>COTS Wi-Fi</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wi-Fi based sensing systems, although sound as being deployed almost everywhere there is Wi-Fi, are still practically difficult to be used without explicit adaptation efforts to new data domains. Various pioneering approaches have been proposed to resolve this contradiction by either translating features between domains or generating domain-independent features at a higher learning level. Still, extra training efforts are necessary in either data collection or model re-training when new data domains appear, limiting their practical usability. To advance cross-domain sensing and achieve fully zero-effort sensing, a domain-independent feature at the lower signal level acts as a key enabler. In this paper, we propose Widar3.0, a Wi-Fi based zero-effort cross-domain gesture recognition system. The key insight of Widar3.0 is to derive and estimate velocity profiles of gestures at the lower signal level, which represent unique kinetic characteristics of gestures and are irrespective of domains. On this basis, we develop a one-fits-all model that requires only one-time training but can adapt to different data domains. We implement this design and conduct comprehensive experiments. The evaluation results show that without re-training and across various domain factors (i.e. environments, locations and orientations of persons), Widar3.0 achieves 92.7% in-domain recognition accuracy and 82.6%-92.4% cross-domain recognition accuracy, outperforming the state-of-the-art solutions. To the best of our knowledge, Widar3.0 is the first zero-effort cross-domain gesture recognition work via Wi-Fi, a fundamental step towards ubiquitous sensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Human-centered computing ? Ubiquitous and mobile computing systems and tools.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human gesture recognition is the core enabler for a wide range of applications such as smart home, security surveillance and virtual reality. Traditional approaches use cameras <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>, wearable devices and phones <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref> or sonar <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref> as the sensing module. While promising, these approaches pose inconvenience due to their respective drawbacks including leakage of privacy, requirement of on-body sensors and limit of sensing range. The need for secure, device-free and ubiquitous gesture recognition interface has triggered extensive research on sensing solutions based on commodity Wi-Fi. Pioneer attempts such as E-eyes <ref type="bibr" target="#b44">[45]</ref>, CARM <ref type="bibr" target="#b43">[44]</ref>, WiGest <ref type="bibr" target="#b0">[1]</ref> and WIMU <ref type="bibr" target="#b37">[38]</ref> have been proposed. In principle, early wireless sensing solutions extract either statistical features (e.g., histograms of signal amplitudes <ref type="bibr" target="#b44">[45]</ref>) or physical features (e.g., power profiles of Doppler frequency shifts <ref type="bibr" target="#b43">[44]</ref>) from Wi-Fi signals and map them to human gestures. However, these primitive signal features usually carry adverse environment information unrelated to gestures. Specifically, due to lack of spatial resolution, wireless signals, and their features as well, are highly specific to environment where the gesture is performed, and the location and orientation of the performer, as Figure <ref type="figure">1</ref> shows. For brevity, we unitedly term these factors irrelevant to gestures as domain. As a result, the classifiers trained with primitive signal features in one domain usually undergo drastically drop in accuracy with another domain.</p><p>Recent innovations in gesture recognition with Wi-Fi have explored cross-domain generalization ability of recognition models. For example, recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref> borrow the ideas from machine learning, such as transfer learning and adversarial learning, and apply advanced learning methodologies to improve cross-domain recognition performance. Another solution, WiAG <ref type="bibr" target="#b38">[39]</ref>, derives a translation function to generate signal features of the target domain for model re-training. While to some extent achieving cross-domain recognition, all existing works require extra training efforts in either data collection or model re-training at each time a new target domain is added into the recognition model. Even worse, correlated with continuous location and orientation of a person, Wi-Fi signals have infinite number of domains, making cross-domain training approaches practically prohibitive.</p><p>A more promising but challenging solution is a "one-fits-all" model that is able to train once, use anywhere. Such ideal model, trained in one domain, can be directly used in new domains without extra efforts, such as data collection, generation, or re-training. Different from all existing approaches, our key idea is to move generalization ability downwardly at the lower signal level, rather than the upper model level. Specifically, we extract domain-independent features reflecting only gesture itself from raw domain-dependent signals. On this basis, we aim to build an explainable cross-domain recognition model that can be applied in new scenarios with zero effort and high accuracy. However, we face three major technical challenges to achieve a one-fits-all model. First, previously used signal features (e.g., amplitude, phase, Doppler Frequency Shift(DFS)), as well as their statistics (e.g., max, min, mean, distribution parameter), are absolutely domain-dependent, meaning that their values vary with different locations, orientations and environments even for the same gesture. Second, it is difficult, for radio signals from only several links, to describe human gestures and actions. For example, kinetic profile of a single gesture still has hundreds of variables, posing the estimation of kinetic profile as a highly under-determined problem. Third, cross-domain generalization often requires sophisticated learning models (e.g., deeper networks, a larger number of parameters, a more complex network structure and more complicated loss functions), which slow down or even obstruct training, over-consume training data, and make the model less explainable.</p><p>To overcome these challenges, we propose Widar3.0, a Wi-Fi based gesture recognition system. Widar3.0 uses channel state information (CSI) portrayed by COTS Wi-Fi devices. Our prior efforts, Widar <ref type="bibr" target="#b31">[32]</ref> and Widar2.0 <ref type="bibr" target="#b32">[33]</ref> track coarse human motion status, e.g., location and velocity, by regarding a person as a single point. Widar3.0, however, aims at recognizing complex gestures that involve multiple body parts. The key component of Widar3.0 is our novel theoretically domain-independent feature body-coordinate velocity profile (BVP) that describes power distribution over different velocities, at which body parts involved in the gesture movements. Our observation is that each type of gestures has its unique velocity profile in the body coordinate system (e.g., the coordinates where the orientation of the person is the positive x axis) no matter in which domain is the gesture performed. To estimate BVP, we approximate BVP from several prominent velocity components and further employ compressive sensing techniques to derive accurate estimates. On this basis, we devise a learning model to capture spatial-temporal characteristics of gestures and finally classify gestures. Through downward movement of model generalization techniques closer to the raw signals, Widar3.0 enables zero-effort cross-domain human gesture recognition with many expected properties simultaneously, including high and reliable accuracy, strong generalization ability, explainable features, reduced amounts of training data. We implement Widar3.0 on COTS Wi-Fi devices and conduct extensive field experiments (16 users, 15 gestures, 15 locations and 5 orientations in 3 environments, and comparisons with three state-of-the-art approaches). Especially, the results demonstrate that Widar3.0 significantly improves the accuracy of gesture recognition to 92.4% in cross-environment cases, while the recognition accuracy with raw CSI and DFS profiles are 40.2% and 77.8% only. Across different types of domain factors including user's location, orientation, environment and user diversity, Widar3.0 achieves average accuracy of 89.7%, 82.6%, 92.4% and 88.9%, respectively.</p><p>In a nutshell, our core contributions are three-fold. First, we present a novel domain-independent feature that captures bodycoordinate velocity profiles of human gestures at the lower signal level. BVP is theoretically irrespective of any domain information in raw Wi-Fi signals, and thus acts as a unique indicator for human gestures. Second, we develop a one-fits-all model on the basis of domain-independent BVP and a learning method that fully exploits spatial-temporal characteristics of BVP. The model enables cross-domain gesture recognition without any extra effort of data collection or model re-training. Third, though trained only once, Widar3.0 achieves on average 89.7%, 82.6%, and 92.4% recognition accuracy across locations, orientations, and environments, respectively, which outperform the state-of-the-art solutions that require re-training in new target domains. Such consistently high performance demonstrates its strong ability of cross-domain generalization. To the best of our knowledge, Widar3.0 is the first zero-effort cross-domain gesture recognition via Wi-Fi, a fundamental step towards ubiquitous sensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MOTIVATION</head><p>Widar3.0 addresses the problem of cross-domain gesture recognition with Wi-Fi signals. Due to the lack of spatial resolution, wireless signals are highly formatted by domain characteristics. Either or not to some extent enabling cross-domain sensing, existing wireless sensing solutions have significant drawbacks in their feature usage. The three main types of features are listed as follows:</p><p>Primitive features without cross-domain capability. Most state-of-the-art activity recognition works extract primitive statistical (e.g., power distribution, waveform) or physical features (e.g., DFS, AoA, ToF) from CSI <ref type="bibr" target="#b45">[46]</ref>. However, due to different locations and orientations of the person and multipath environments, features of the same gesture may vary significantly and fail to serve successful recognition. As a brief example, a person is asked to push his right hand multiple times, yet with two orientations relative to the wireless link. The spectrograms are calculated as in <ref type="bibr" target="#b43">[44]</ref>, and dominant DFS caused by the movement of the hand is extracted. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, while dominant DFS series of gestures with the same domain form compact clusters, they differ greatly in trends and amplitudes between two domains, and thus fail to indicate the same gesture.  Cross-domain motion features for coarse tracking. Devicefree tracking approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref> build quantitative relation between physical features of signal and the motion status of the person, and enable location and velocity measurement across environments. However, these works regard a person as single point, which is infeasible for recognizing complex gestures that involve multiple body parts. Figure <ref type="figure">3</ref> illustrates the spectrogram of a simple hand clap, which contains two major DFS components caused by two hands and a few secondary components.</p><p>Latent features from cross-domain learning methods. Crossdomain learning methods such as transfer learning <ref type="bibr" target="#b49">[50]</ref> and adversarial learning <ref type="bibr" target="#b19">[20]</ref> latently generate features of data samples in the target domain, either by translating samples from the source domain, or learning domain-independent features. However, these works require extra efforts of collecting data samples from the target domain and retraining the classifier each time new target domains are added. As an example, we evaluate the performance of an adversarial learning based model, EI <ref type="bibr" target="#b19">[20]</ref> over different domain factors (e.g., environment, location and orientation of the person). Specifically, the classifier is trained with and without data samples in every type of target domains. As shown in Figure <ref type="figure">4</ref>, the system accuracy obviously drops without the knowledge of the target domains, demonstrating the need of extra data collection and training efforts in these learning methodologies.</p><p>Lessons learned. The deficiency of existing cross-domain learning solutions asks for a new type of domain-independent feature. Should it be achieved, a one-fits-all model could be built upon it to save much data collection and training efforts. Widar3.0 is designed to develop and exploit body-coordinate velocity profile (BVP) to address the issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW OF WIDAR3.0</head><p>Widar3.0 is a cross-domain gesture recognition system using offthe-shelf Wi-Fi devices. As shown in Figure <ref type="figure" target="#fig_3">5</ref>, multiple wireless links are deployed around the monitoring area. Wireless signals, as distorted by the user in the monitoring area, are acquired at receivers and their CSI measurements are logged and preprocessed to remove amplitude noises and phase offsets.</p><p>The major parts of Widar3.0 are two modules, the BVP generation module and the gesture recognition module.</p><p>Upon receiving sanitized CSI series, Widar3.0 divides CSI series into small segments, and generates BVP for each CSI segment via the BVP generation module. Widar3.0 first prepares three intermediate results: DFS profiles, the orientation and location information of the person. DFS profiles are estimated by applying time-frequency analysis to CSI series. The orientation and location information of the person is calculated via motion tracking approaches. Thereafter, Widar3.0 applies the proposed compressed-sensing-based optimization approach to estimate BVP of each CSI segment. The BVP series is then output for following gesture recognition.</p><p>The gesture recognition module implements a deep learning neural network (DNN) for gesture recognition. With the BVP series as input, Widar3.0 performs normalization on each BVP and across the whole series, in order to remove the irrelevant variations of instances and persons. Afterwards, the normalized BVP series is input into a spatial-temporal DNN, which has two main functions. First, the DNN extracts high-level spatial features within each BVP using convolutional layers. Then, recurrent layers are adopted to perform temporal modeling of inter-characteristics between BVPs. Finally, the output of the DNN is used to indicate the type of the gesture performed by the user. In principle, Widar3.0 achieves zero-effort cross-domain gesture recognition, which requires only one-time training of the DNN network, but can be directly adapted to as many as new domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BODY-COORDINATE VELOCITY PROFILE</head><p>Intuitively, human activities have unique velocity distributions across all body parts involved, which can be used as activity indicators. Among all parameters (i.e. ToF, AoA, DFS and attenuation) of the signal reflected by the person, DFS embodies most information of velocity distribution. Unfortunately, DFS is also highly correlated with the location and orientation of the person, circumventing direct cross-domain activity recognition with DFS profiles.</p><p>In this section, we tempt to derive distribution of signal power over velocity components in the body coordinate system, i.e. BVP, which uniquely indicates the type of activities. Preliminary of the CSI model is first introduced ( ? 4.1), followed by the formulation and calculation of BVP ( ? 4.2 and ? 4.3). Finally, prerequisites for calculating BVP are given ( ? 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Doppler Representation of CSI</head><p>CSI portrayed by off-the-shelf Wi-Fi devices describes multipath effects in the indoor environment at arrival time t of packets and  </p><formula xml:id="formula_0">? ( f , t ) = L l =1 ? l ( f , t )e -j2? f ? l (f ,t ) e j? (f ,t ) , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where L is the number of paths, ? l and ? l are the complex attenuation and propagation delay of the l-th path, and ? ( f , t ) is the phase error caused by timing alignment offset, sampling frequency offset and carrier frequency offset. By representing phases of multipath signals with the corresponding DFS, CSI can be transformed as <ref type="bibr" target="#b31">[32]</ref>:</p><formula xml:id="formula_2">? ( f , t ) = H s ( f ) + l ?P d ? l (t )e j2? t -? f D l (u )du e j? (f ,t ) ,<label>(2)</label></formula><p>where the constant H s is the sum of all static signals with zero DFS (e.g., LoS signal), and P d is the set of dynamic signals with non-zero DFS (e.g., signals reflected by the target).</p><p>With conjugate multiplication of CSI of two antennas on the same Wi-Fi NIC calculated, and out-band noises and quasi-static offsets filtered out, random offsets can be removed and only prominent multipath components with non-zero DFS are retained <ref type="bibr" target="#b25">[26]</ref>. Further applying short-term Fourier transform yields power distribution over the time and Doppler frequency domains. One example of the spectrogram of a single link is shown in Figure <ref type="figure">3</ref>. We denote each time snapshot in spectrograms as a DFS profile. Specifically, a DFS profile D is a matrix with dimension as F ? M, where F is the number of sampling points in the frequency domain, and M is the number of transceiver links. Based on DFS profile from multiple links, we then derive domain-independent BVP. </p><formula xml:id="formula_3">V x V y DFS Profile Link #2 v 1 v 2 v 3 u 1 u 2 u 3 u 4 u 5 u 6 Body- Coodrinate Velocity Profile Normal Direction Link #1 Link #3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">From DFS to BVP</head><p>When a person performs a gesture, his body parts (e.g., two hands, two arms and the torso) move at different velocities. As a result, signals reflected by these body parts experience various DFS, which are superimposed at the receiver and form the corresponding DFS profile. As discussed in ? 2, while DFS profile contains the information of the gesture, it is also highly specific to the domain. In contrast, the power distribution over physical velocity in the body coordinate system of the person, is only related to the characteristics of the gesture. Thus, in order to remove the impact of domain, BVP is derived out of DFS profiles. The basic idea of BVP is shown in Figure <ref type="figure" target="#fig_4">6</ref>. For practicality, a BVP V is quantized as a discrete matrix with dimension as N ? N , where N is the number of possible values of velocity components decomposed along each axis of the body coordinates. For convenience, we establish the local body coordinates whose origin is the location of the person and positive x-axis aligns with the orientation of the person. We will discuss approaches of estimating a person's location and orientation in ? 4.4. Currently, it is assumed that the global location and orientation of the person are available. Then the known global locations of wireless transceivers can be transformed into the local body coordinates. Thus, for better clarity, all locations and orientations used in the following derivation are in the local body coordinates. Suppose the locations of the transmitter and the receiver of the i-th link are l</p><formula xml:id="formula_4">(i ) t = (x (i ) t , y (i ) t ), l (i ) r = (x (i ) r , y (i )</formula><p>r ), respectively, then any velocity components v = (v x , v y ) around the human body (i.e. the origin) will contribute its signal power to some frequency component, denoted as of the i-th link <ref type="bibr" target="#b31">[32]</ref>:</p><formula xml:id="formula_5">f (i ) ( v), in the DFS profile</formula><formula xml:id="formula_6">f (i ) ( v) = a (i ) x v x + a (i ) y v y .<label>(3)</label></formula><formula xml:id="formula_7">a (i )</formula><p>x and a</p><formula xml:id="formula_8">(i )</formula><p>y are coefficients determined by locations of the transmitter and the receiver:</p><formula xml:id="formula_9">a (i ) x = 1 ? ( x (i ) t l (i ) t 2 + x (i ) r l (i )<label>r 2</label></formula><p>),</p><formula xml:id="formula_10">a (i ) y = 1 ? ( y (i ) t l (i ) t 2 + y (i ) r l (i ) r 2 ),<label>(4)</label></formula><p>where ? is the wavelength of Wi-Fi signal. As static components with zero DFS (e.g., the line of sight signals and dominant reflections from static objects) are filtered out before DFS profiles are calculated, only signals reflected by the person are retained. Besides, when the person is close to the Wi-Fi link, only signals with one time reflection have prominent magnitudes <ref type="bibr" target="#b32">[33]</ref> as Figure <ref type="figure">3</ref> shows. Thus, Equation 3 holds valid for the gesture recognition scenario. From the geometric view, Equation 3 means that the 2-D velocity vector v is projected on a line whose direction vector is</p><formula xml:id="formula_11">d (i ) = (-a (i ) y , a (i )</formula><p>x ). Suppose the person is on an ellipse curve whose foci are the transmitter and the receiver of the i-th link, then d (i )  is indeed the normal direction of the ellipse at the person's location. Figure <ref type="figure" target="#fig_4">6</ref> shows an example where the person generates three velocity components v j , j = 1, 2, 3, and projection of the velocity components on the DFS profiles of three links.</p><p>Since coefficients a</p><formula xml:id="formula_12">(i )</formula><p>x and a</p><formula xml:id="formula_13">(i )</formula><p>y only depend on the location of the i-th link, the relation of projection of the BVP on the i-th link is fixed. Specifically, an assignment matrix A (i ) F ?N 2 can be defined:</p><formula xml:id="formula_14">A (i ) j,k = 1 f j = f (i ) ( v k ) 0 else ,<label>(5)</label></formula><p>where f j is the j-th frequency sampling point in the DFS profile, and v k is velocity component corresponding to the k-th element of the vectorized BVP V . Thus, the relation between DFS profile of the i-th link and the BVP can be modeled as:</p><formula xml:id="formula_15">D (i ) = c (i ) A (i ) V (6)</formula><p>where c (i ) is the scaling factor due to propagation loss of the reflected signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BVP Estimation</head><p>How to recover BVP from DFS profiles of only several wireless links is another main challenge because the kinetic profile of a single gesture has hundreds of variables, posing the BVP estimation from DFS profiles as a severely under-determined problem with only a limited number of constraints provided by several wireless links. Specifically, in practice, we estimate one BVP from DFS profiles calculated from 100 ms CSI data. Due to the uncertainty principle, the frequency resolution of DFS profiles is only about 10 Hz. Given that the range of human-induced DFS is within ? 60 Hz <ref type="bibr" target="#b43">[44]</ref>, the DFS profile of one link can only provide about 12 constraints. In contrast, we moderately set the range and the resolution of velocities along two axes of the body coordinates as ? 2 m/s and 0.2 m/s, respectively, leading to as much as 400 variables! Fortunately, when a person performs a gesture, only a few dominant distinct velocity components exist, due to the limited number of major reflecting multipath signals. Thus, there is an opportunity to correctly recover the BVP from DFS profiles of only several links. Before a proper solution of BVP developed, it is necessary to understand the minimum number of links required to uniquely recover the BVP. Figure <ref type="figure" target="#fig_4">6</ref> shows an intuitive example with three velocity components v j , j = 1, 2, 3. With only the first two links (blue and green), the three velocity components create three power peaks in each DFS profile. However, when we recover the BVP, there are 9 candidates of velocity components, i.e. v j , j = 1, 2, 3 and</p><formula xml:id="formula_16">u k , k = 1, ? ? ? , 6.</formula><p>And one can easily find an alternate solution, i.e. {u 1 , u 3 , u 6 }, meaning that two links are insufficient.</p><p>By adding the third link (purple), it is able to resolve the ambiguity with high probability no matter how many velocity components exist, if no overlap of projections happens in the third DFS profile. When projections overlap, however, it is possible that adding the third or even more links cannot resolve the ambiguity. For example, suppose the third link in the Figure <ref type="figure" target="#fig_4">6</ref> is in parallel with the y-axis, and there are three overlaps of projections (i.e. {u 1 , v 2 }, {v 3 , u 4 , u 6 } and {u 3 , v 1 }), then the ambiguous solution {u 1 , u 3 , u 6 } is still not resolvable. However, such ambiguity can hardly happen due to its stringent requirement on the distribution of velocity components as well as the orientation of the links. Moreover, we can further reduce the probability of the ambiguity by adding more links. We evaluate the impact of the number of links used by Widar3.0 on system performance in Section 6.5.</p><p>With observing of the sparsity of BVP and validating the feasibility of recovering BVP from multiple links, we adopt the idea of compressed sensing <ref type="bibr" target="#b12">[13]</ref> and formulate the estimation of BVP as an l 0 optimization problem:</p><formula xml:id="formula_17">min V M i=1 |EMD(A (i ) V , D i )| + ? V 0 , (<label>7</label></formula><formula xml:id="formula_18">)</formula><p>where M is the number of Wi-Fi links. The sparsity of the number of the velocity components is coerced by the term ? V 0 , where ? represents the sparsity coefficients and ? 0 is the number of non-zero velocity components. EMD(?, ?) is the Earth Mover's Distance <ref type="bibr" target="#b34">[35]</ref> between two distributions. The selection of EMD rather than Euclidean distance is mainly due to two reasons. First, the quantization of BVP introduces approximation error, i.e. projection of velocity components to the DFS bin might be adjacent to the true one. Such quantization error can be relieved by EMD, which takes the distance between bins into consideration. Second, there are unknown scaling factors between the BVP and DFS profiles, making the Euclidean distance inapplicable.</p><p>Figure <ref type="figure" target="#fig_5">7</ref> shows an example of solved BVP series of a pushing and pulling gesture. The dominant velocity component from the hand and the coupling ones from the arm can be clearly observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Location and Orientation Prerequisites</head><p>Widar3.0 requires the location and orientation of the person to calculate the domain-independent BVP. In common application scenarios of Widar3.0, when a person wants to interact with the device, he or she approaches it and performs interactive gestures for recognition and response. The antecedent movement of the person gives the chance for estimating his location and orientation, which are the location and moving direction of the person at the end of the trace. Since Wi-Fi based passive tracking has been extensively studied, Widar3.0 can exploit existing sophisticated passive tracking systems, e.g., LiFS <ref type="bibr" target="#b40">[41]</ref>, IndoTrack <ref type="bibr" target="#b25">[26]</ref> and Widar2.0 <ref type="bibr" target="#b32">[33]</ref>, to obtain the location and orientation of the person. However, Widar3.0 differs from these passive tracking approaches by estimating BVP rather than main torso velocity, and thus further extends the scope of Wi-Fi based sensing. Note that the state-of-the-art localization errors are within several decimeters, and orientation estimation errors are within 20 degrees. We evaluate the impact of location and orientation error by experiments in Section 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RECOGNITION MECHANISM</head><p>In Widar3.0, we design a DNN learning model to mining the spatialtemporal characteristics of the BVP series. Figure <ref type="figure" target="#fig_6">8</ref> illustrates the overall structure of the proposed learning model. Specifically, the BVP series is first normalized to remove irrelevant variations caused by instances, persons and hardware settings ( ? 5.1). The normalized output is then input into a hybrid deep learning model, which from bottom to top consists of a convolutional neural network (CNN) for spatial feature extraction ( ? 5.2) and a recurrent neural network (RNN) for temporal modeling ( ? 5.3).</p><p>The designed model is a result of the effectiveness of the domainindependent feature BVP. With BVP as input, the hybrid CNN-RNN model can achieve accurate cross-domain gesture recognition although the learning model itself does not possess generalization capabilities. We will verify that the CNN-RNN model is a simple but effective method in Section 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BVP Normalization</head><p>While BVP is theoretically only related to gestures, two practical factors may affect its stability as the gesture indicator. First, the overall power of BVP may vary due to the adjustment of transmission power. Second, in practice, instances of the same type of gesture performed by different persons may have different time length and moving velocities. Moreover, even instances performed by the same person may slightly vary. Thus, it is necessary to remove these irrelevant factors to retain the simplicity of the learning model.</p><p>For signal power variation, Widar3.0 normalizes the element values in each single BVP by adjusting the sum of all elements in BVP to 1. For instance variation, Widar3.0 normalizes the BVP series along the time domain. Specifically, Widar3.0 first sets the standard time length as t 0 . Then, for gesture with time length as t, Widar3.0 scales its BVP series to t 0 . The assumption behind the scaling operation is that the total distance moved by each body part remains fixed. Thus, to change the time length of the BVP series, Widar3.0 first scales coordinates of all velocity component in the BVP by a factor of t t 0 , and then resamples the series to the sampling rate of the original BVP series. After normalization, the output becomes related to gestures only, and is input to the deep learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Spatial Feature Extraction</head><p>The input of the learning model, BVP data, is similar to a sequence of images. Each single BVP describes the power distribution over physical velocity during a sufficiently short time interval. And the continuous BVP series illustrates how the distribution varies corresponding to a certain kind of action. Therefore, to fully understand the derived BVP data, it is intuitive to extract spatial features from each single BVP first and then model the temporal dependencies of the whole series.</p><p>CNN is a useful technique to extract spatial features and compress data <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref>, and it is especially suitable for handling the single BVP, which is highly sparse but preserves spatial locality, as a velocity component usually corresponds to the same body part as its neighbors with similar velocities. Specifically, the input BVP series, denoted as V , is a tensor with dimension as N ? N ?T , where T is the number of BVP snapshots. For the t-th sampling BVP, the matrix V ??t is fed into the CNN. Within the CNN, 16 2-D filters are first applied to V ??t to obtain local patterns in the velocity domain, which form the output V</p><p>(1)</p><formula xml:id="formula_19">??t . Then, max pooling is applied to V (1)</formula><p>??t to down-sample the features and the output is denoted as V</p><p>(2)</p><formula xml:id="formula_20">??t . With V (2) ??t flattened into the vector v (2)</formula><p>??t , two 64-unit dense layers with ReLU as activation functions are used to further extract features in a higher level. Note that one extra dropout layer is added between two dense layers to reduce overfitting. The final output v ??t characterizes the t-th sampling BVP. And the output series is used as the input of following recurrent layers for temporal modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Temporal Modeling</head><p>Besides local spatial features within each BVP, BVP series also contains temporal dynamics of the gesture. Recurrent neural networks (RNN) are appealing in that they can model complex temporal dynamics of sequences. There are different types of RNN units, e.g., SimpleRNN, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b11">[12]</ref>. Compared with original RNNs, LSTMs and GRUs are more capable of learning long-term dependencies, and we choose GRUs because GRU achieves performance comparable to that of LSTM on sequence modeling, but involves fewer parameters and is easier to train with less data <ref type="bibr" target="#b11">[12]</ref>. Specifically, Widar3.0 chooses single-layer GRUs to model the temporal relationships. Inputs { v ??t , t = 1, ? ? ? ,T } output from CNN are fed into GRUs and a 128-dimensional vector v ??r is generated. Furthermore, a dropout layer is added for regularization, and a softmax classifier with cross-entropy loss for category prediction is utilized. Note that for recognition systems which involve more sophisticated activities with longer durations, the GRU-based models can be transformed into more complex versions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b46">47]</ref>. In ? 6.4, we will verify that single-layer GRUs are sufficient for capturing temporal dependencies for short-time human gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>This section presents the implementation and detailed performance of Widar3.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Methodology</head><p>Implementation. Widar3.0 consists of one transmitter and at least three receivers. All transceivers are off-the-shelf mini-desktops (physical size 170mm ? 170mm) equipped with an Intel 5300 wireless NIC. Linux CSI Tool <ref type="bibr" target="#b17">[18]</ref> is installed on devices to log CSI measurements. Devices are set to work in the monitor mode, on channel 165 at 5.825 GHz where there are few interfering radios as interference does pose severe impacts on the collected CSI measurements <ref type="bibr" target="#b53">[54]</ref>. The transmitter activates one antenna and broadcasts Wi-Fi packets at a rate of 1,000 packets per second. The receiver activates all three antennas which are placed in a line. We implement Widar3.0 in MATLAB and Keras <ref type="bibr" target="#b9">[10]</ref>.</p><p>Evaluation setup. To fully explore the performance of Widar3.0, we conduct extensive experiments on gesture recognition in 3 indoor environments: an empty classroom furnished with desks and chairs, a spacious hall and an office room with furniture like sofa and tables. Figure <ref type="figure">9</ref> illustrates the general environmental features and the sensing area in different rooms. Figure <ref type="figure">10</ref> shows a typical example of the deployment of devices and domain configurations in the sensing area, which is a 2m ? 2m square. Note that the 2m ? 2m square is a typical setting to perform interactive gestures for recognition and response, especially in the scenario of smart home, with more Wi-Fi nodes incorporated into smart devices (e.g., smart TV, Xbox Kinect, home gateways, smart camera) to help. We assume that only the gesture performer is in the sensing area as moving entities introduce noisy reflection signals and further result in less accurate DFS profiles of the target gestures. Except for the two receivers and one transmitter placed at the corner of the sensing area, the remaining four receivers can be deployed at random locations outside two sides of the sensing area. As Section 4.3 has mentioned, the deployment of devices hardly pose impacts on Widar3.0 theoretically. All devices are held up at the height of 110 cm, where users with different heights can perform gestures comfortably. In total, 16 volunteers (12 males and 4 females) with different heights (varying from 185 cm to 155 cm) and somatotypes participate in experiments. The ages of the volunteers vary from 22 to 28. And the details of the volunteer information are illustrated in Figure <ref type="figure" target="#fig_8">12</ref>.</p><p>Dataset. We collect gesture data from 5 locations and 5 orientations in each sensing area, as illustrated in Figure <ref type="figure">10</ref>. All experiments are approved by our IRB. Two types of datasets are collected. Specifically, the first dataset consists of common hand gestures used in human-computer interaction, including pushing and pulling, sweeping, clapping, sliding, drawing circle and drawing zigzag. The sketches of the six gestures are plotted in Figure <ref type="figure">11</ref>. This dataset contains 12,000 gesture samples (16 users ? 5 positions ? 5 orientations ? 6 gestures ? 5 instances). The second dataset is collected for a case study of more complex and semantic gestures. Two volunteers (one male and one female) draw number 0 ? 9 in the horizontal plane, and totally 5,000 samples (2 users ? 5 positions ? 5 orientations ? 10 gestures ? 10 instances) are collected. Before collecting the datasets, we ask volunteers to watch the example video of each gesture. The datasets and the example videos are available at website 1 . Prerequisites Acquisition. The position and orientation of the user are prerequisites for calculation of BVP. In general, the last estimation of location and the last estimation of moving direction can be provided by tracking systems <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref>, as the location and orientation of the user in Widar3.0. Note that the function of Widar3.0 is independent of that of the motion tracking system. To fully understand how Widar3.0 works, we record the ground truth of location and orientation of the user in most experiments, and explicitly introduce location and orientation error in the parameter study (Section 6.5) to evaluate the relation between recognition accuracy and location and orientation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Overall Accuracy</head><p>Taking all domain factors into consideration, Widar3.0 achieves an overall accuracy of 92.7%, with 90 and percentage data collected in Room 1 used for training and testing, respectively. Figure <ref type="figure" target="#fig_9">13a</ref> shows the confusion matrix of 6 gestures in dataset 1, and Widar3.0 achieves consistently high accuracy of over 85% for all gestures. We also conduct experiments with gestures of an "unknown" class are additionally added. Volunteers are required to perform arbitrary gestures except for the above 6 gestures. The overall accuracy drops to 90.1% and Widar3.0 can differentiate the unknown class with an accuracy of 87.1%. The reasons are as follows. On one hand, gestures from an "unknown" class might be similar to the predefined ones to a certain degree. On the other hand, the collected "unknown" gestures are still limited. We believe the results can be further improved if we introduce additional filtering mechanisms or modify 1 http://tns.thss.tsinghua.edu.cn/widar3.0/index.html the learning model to solve the issue of "novelty detection", which is another significant topic in recognition problems.</p><p>Figure <ref type="figure" target="#fig_9">13b</ref>, 13c, 13d and 13e further show confusion matrices considering each specific domain factors. For each domain factor, we calculate average accuracy of cases where one out of all domain instances are used for testing, while the rest domain instances are for training. The average accuracy over all gestures are provided as well, and it can be seen that Widar3.0 achieves consistent high performance across different domains, demonstrating its capability of cross-domain recognition.</p><p>We observe that for both in-domain and cross-domain cases, the gestures "pushing and pulling", "drawing circle" and "drawing zigzag" usually correspond to a lower accuracy. While the "pushing and pulling" gesture is the simplest one among all gestures, it is performed just in front of the user torso, and is more likely to be blocked from the perspectives of certain links, which results in less accurate BVP estimation as shown in the following experiments (Section 6.5). When users perform the gesture "drawing circle" or "drawing zigzag", the trajectory has significant changes in vertical direction. However, Widar3.0 is designed to extract BVP only in the horizontal plane, leading to information loss for the two gestures, and decrease in recognition accuracy.</p><p>Case study. We now examine if Widar3.0 still works well for more complex gesture recognition tasks. In this case study, volunteers draw number 0?9 in the horizontal plane and 5,000 samples are collected in total. We divide the dataset into training and testing randomly with the ratio 9:1. As shown in Figure <ref type="figure" target="#fig_9">13f</ref>, Widar3.0 achieves satisfying results of over 90% for 8 gestures and the average accuracy is 92.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Cross-Domain Evaluation</head><p>We now evaluate the overall performance of Widar3.0 on across different domain factors, including environment, person diversity and location and orientation of the person. For evaluation on each domain factor, we keep the other domain factors unchanged, and perform leave-one-out cross validation on the datasets. The system performance, in terms of mean and variance of the accuracy, is shown in Figure <ref type="figure" target="#fig_10">14?17</ref>.</p><p>Location independence. The model is trained on the BVPs of random 4 locations, all 5 orientations and 8 people in Room 1. And the data collected at the last location in the same room is used for testing. As shown in Figure <ref type="figure" target="#fig_10">14</ref>, the average accuracies for all locations uninvolved in training are all above 85%. Widar3.0 achieves best performance of 92.3% with location e, which is at the center of the sensing area, as the target domain. The accuracy descends to 85.3% when testing dataset is collected at location d, as wireless signal reflected by human-body becomes weaker after a longer distance of propagation, which leads to less accurate BVPs. In addition, BVP is modeled from signals reflected by the person. If the person happens to pass his arm through the line-of-sight path of any links, the accuracy will slightly drop, as proved by the result of location b.</p><p>Orientation sensitivity. In this experiment, we select each orientation as the target domain and other 4 orientations as the source domain. Figure <ref type="figure" target="#fig_3">15</ref> shows that the accuracy remains above 80% for orientation 2, 3, 4. Compared with best target orientation 3, whose accuracy is around 90%, the performance at orientation 1&amp;5 declines by over 10%. The reason is that gestures might be shadowed by human body in these two orientations and the number of effective wireless links for BVP generation decreases. For common gesture recognition applications (e.g., TV control), however, it is reasonable to assume that when the user faces towards the TV, his orientation does not deviate much from most wireless devices, a sufficient number of which could be used for accurate gesture recognition.</p><p>Environment diversity. The accuracy across different environments is another significant criterion for performance of crossdomain recognition. In this experiment, gesture samples collected in room 1 are used as the training dataset, and three groups of gesture samples collected in three rooms are used as testing datasets. As Figure <ref type="figure" target="#fig_4">16</ref> depicts, while the accuracy for different rooms slightly drops, the average accuracy preserves over 87% even if the environment changes totally. In a nutshell, Widar3.0 is robust to different environments.</p><p>Person variety. Data collected from different persons may have discrepancy due to their various behavior patterns. Widar3.0 incorporates BVP normalization to alleviate this problem. To evaluate the performance of Widar3.0 on different users, we train the model on a dataset from every combination of 7 persons, and then test with the data of the resting person. Figure <ref type="figure" target="#fig_5">17</ref> shows that the accuracy remains over 85% across 7 persons. The impact of the number of persons used in training the recognition model is further investigated in Section 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Method Comparison</head><p>This section compares the capability of cross-domain recognition with different methods, learning features and structures of learning networks. In the experiment, training and testing datasets are collected separately in Room 1 and 2.</p><p>Comparison with the state-of-the-arts works. We compare Widar3.0 against several alternative state-of-the-arts methodologies, CARM <ref type="bibr" target="#b43">[44]</ref>, EI <ref type="bibr" target="#b19">[20]</ref> and CrossSense <ref type="bibr" target="#b49">[50]</ref>, where the latter two are feasible for cross-domain recognition. Specifically, CARM uses DFS profiles as learning features and adopts HMM model. EI incorporates an adversarial network and specializes the training loss to additionally exploit characteristics of unlabeled data in target domains. CrossSense proposes an ANN-based roaming model to translate signal features from source domains to target domains, and employs multiple expert models for gesture recognition. Figure <ref type="figure" target="#fig_6">18</ref> shows the system performance of the four approaches. Widar3.0 achieves better performance with the state-of-the-art cross-domain learning methodologies, EI and CrossSense, and it does not require extra data from a new domain or model re-training. In contrast, both feature and learning model of CARM do not have cross-domain capability, which is the main reason for its significantly lower recognition accuracy.</p><p>Comparison of input features. We compare three types of features with different levels of abstraction from raw CSI measurements, i.e. denoised CSI, DFS profiles and BVP, by feeding them into the CNN-GRU hybrid deep learning model, similar to that in Widar3.0. Specifically, the size of denoised CSI is 18 (the number of antennas of 6 receivers) ? 30 (the number of subcarriers) ? T (the number of time samples), and the DFS profile has the shape as 6  (the number of receivers) ? F (the number of Doppler frequency samples) ? T (the number of time samples). As shown in Figure <ref type="figure" target="#fig_11">19</ref>, BVP outperforms both denoised CSI and DFS, with an increase of accuracy by 52% and 15%, respectively. The performance improvement of BVP attributes its immunity to changes of layouts of transceivers, which however may significantly influences the other two types of features.</p><p>Comparison of learning model structures. Different deep learning models are further compared and the system performance is demonstrated in Figure <ref type="figure" target="#fig_12">20</ref>. Specifically, the CNN-GRU hybrid model increases the accuracy by around 5% compared with the simple GRU model which merely captures temporal dependencies. The former model benefits from representative high-level spatial features within each BVP snapshot. In addition, we also feed BVP into a two-convolutional-layer CNN-GRU hybrid model and a CNN-Hierarchical-GRU model <ref type="bibr" target="#b10">[11]</ref>. It is shown that a more complex deep learning model does not promote the performance, demonstrating that BVP of different gestures are distinct enough to be discriminated by a simple but effective classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Parameter Study</head><p>Impact of link numbers. In the above experiments, 6 links are deployed for more accurate estimation of BVP. This section studies the impact of the number of links on system performance. As shown in Figure <ref type="figure" target="#fig_0">21</ref>, the accuracy gradually decreases as the number of links reduces from 6 to 3, but experiences a more significant drop when only two links are used. The main reason is that some BVPs cannot be correctly recovered with only 2 links considering the ambiguity mentioned in Section 4.3, and gestures at certain locations or orientations cannot be fully captured due to blockage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of location and orientation estimation error.</head><p>Localizations and orientations provided by Wi-Fi based motion tracking systems usually have errors of about several decimeters and 20 degrees, respectively. Thus, it is necessary to understand how these errors impact the performance of Widar3.0. Specifically, we record ground truth of location and orientation, and calculate errors where gestures are performed. On one hand, as shown in Figure <ref type="figure" target="#fig_1">22</ref>, the overall accuracy remains over 90% when the location error is within 40 cm, but then drops as the error further increases. On the other hand, Figure <ref type="figure" target="#fig_1">23</ref> shows that the overall accuracy gradually drops with more deviation of orientation. While the tracking errors negatively impact the performance of Widar3.0, taking practical location and orientation errors into consideration, we believe existing motion tracking works can still provide location and orientation results with acceptable accuracy.</p><p>Impact of training set diversity. This experiment studies how the number of volunteers in training dataset impacts the performance. Specifically, a varying number of volunteers from 1 to 7 participate in collecting the training dataset, and data from another new person is used to test Widar3.0. Figure <ref type="figure" target="#fig_1">24</ref> shows that the average gesture recognition accuracy decreases from 89% to 74% when the number of people for training varies from 7 to 1. The reasons come from two folds. First, with the training dataset contributed by fewer volunteers, the deep learning model will be less thoroughly trained. Second, the behavior difference between testing persons and training persons will be amplified even if we have adopted BVP normalization. In general, Widar3.0 promises an accuracy of over 85% with more than 4 people in the training set.</p><p>Impact of transmission rates. As Widar3.0 requires packet transmission for gesture recognition, normal communication flow  might be interfered. Therefore, we evaluate the performance of Widar3.0 with different CSI transmission rates. We collect CSI measurements at the initial transmission rate of 1,000 packets per second, and down-sample the CSI series to 750 Hz, 500 Hz, 250 Hz. Figure <ref type="figure" target="#fig_3">25</ref> shows that the accuracy degrades slightly by around 4% when the sampling rate drops to 250Hz, and remains over 85% for all cases. In addition, Widar3.0 can further reduce the impacts on communication with shorter packets used as only CSI measurements are useful for the recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSIONS</head><p>User height. Since transceivers are placed at the same height, CSI measurements mainly capture the horizontal velocity components. Thus, different user heights may impact the recognition performance of Widar3.0, as the devices may observe different groups of velocity components intercepted at this height. However, Widar3.0 still has the capability of recognizing gestures in 3-D space, as common gestures remain their uniqueness even within the fixed height.</p><p>As shown in the experiments, Widar3.0 is able to recognize gestures "draw circle" and "draw zigzag", which both contain vertical velocity components due to the fixed length of arms. By regarding the person as on an ellipsoid whose foci are the transceivers of a link, the BVP can be further generalized to 3-D space. Further work includes optimizing the deployment of Wi-Fi links to enable calculation of 3-D BVP, and revising the learning model with 3-D BVPs as input. Number of Wi-Fi links for gesture recognition. Although three wireless links are sufficient to resolve the ambiguity with high probability for BVP generation, six receivers in total are deployed in the experiments. The reasons are two folds. First, compared with macro activities, the reflected signal of micro gestures is much weaker, since the effective area of hand and arm is much smaller than that of torso and leg, resulting in less prominent DFS profiles. Second, gestures with hands and arms may be opportunistically shadowed by other body parts when the user faces away from the link. For macro activity such as walking, running, jumping and falling, it is believed that the number of Wi-Fi links required for recognition can be reduced. It is worth noting that Widar3.0 does not require fixed deployment of Wi-Fi devices in the environment, as BVP is the power distribution over absolute velocities.</p><p>Applications beyond gesture recognition. While Widar3.0 is a Wi-Fi based gesture system, the feature used in Widar3.0, BVP, can theoretically capture movements over the whole body of the person, and thus is envisioned to be used in other device-free sensing scenarios, such as macro activity recognition, gait analysis and user identification. In these scenarios where users are likely to continuously change their locations and orientations, BVP calculation and motion tracking approaches can be intermittently invoked to obtain BVPs along the whole trace, which then may serve as a unique indicator for user's activity or identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>Our work is highly related to wireless human sensing techniques, which are roughly categorized into model-based and learning-based ones, targeting at localization and activity recognition, respectively.</p><p>Model-based wireless localization. Model-based human sensing explicitly builds physical link between wireless signals and human movements. On the signal side, existing approaches extract various parameters of signals reflected or shadowed by human, including DFS <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref>, ToF <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b20">21]</ref>, AoA/AoD <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> and attenuation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref>. Based on types of devices used, parameters with different extent of accuracy and resolution can be obtained. WiTrack <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> develops FMCW radar with wide bandwidth to accurately estimate ToFs of reflected signals. WiDeo <ref type="bibr" target="#b20">[21]</ref> customizes full-duplex Wi-Fi to jointly estimate ToFs and AoAs of major reflectors. In contrast, though limited by the bandwidth and antenna number, Widar2.0 <ref type="bibr" target="#b32">[33]</ref> improves resolution by jointly estimating ToF, AoA and DFS.</p><p>On the human side, existing model-based works only tracks coarse human motion status, such as location <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>, velocity <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>, gait <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b48">49]</ref> and figure <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>. Though not detailed enough, they provide coarse human movement information, which can further help Widar3.0 and other learning-based activity recognition works to remove domain dependencies of input signal features.</p><p>Learning-based wireless activity recognition. Due to complexity of human activity, existing approaches extract signal features, either statistical <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref> or physical <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> ones, and map them to discrete activities. The statistical methods treat the wireless signal as time series data, extract its waveforms and distributions in both time and frequency domain as fingerprints. E-eyes <ref type="bibr" target="#b44">[45]</ref> is a pioneer work to use strength distribution of commercial Wi-Fi signals and KNN to recognize human activities. Niu et al. <ref type="bibr" target="#b29">[30]</ref> uses signal waveforms for fine-grained gesture recognition. The physical methods take a step further to extract features with clear physical meanings. CARM <ref type="bibr" target="#b43">[44]</ref> calculates power distribution of DFS components as learning features of HMM model. WIMU <ref type="bibr" target="#b37">[38]</ref> further segments DFS power profile for multi-person activity recognition. However, due to fundamental limits of domain dependencies of wireless signals, directly using either statistical or physical features is infeasible to generalize to different domains.</p><p>Tempts to adapt recognition schemes in various domains fall into two categories: virtually generating features for target domains <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref> and developing domain-independent features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref>. In the former type, WiAG <ref type="bibr" target="#b38">[39]</ref> derives translation functions between CSIs from different domains, and generates virtual training data accordingly. CrossSense <ref type="bibr" target="#b49">[50]</ref> adopts the idea of transfer learning, and proposes a roaming model to translate signal features between domains. However, features generated by these types of works are still domain-dependent, which require training of classifier for each individual domain, leading to a waste of training efforts. In contrast, with the help of passive localization, Widar3.0 directly uses domain-independent BVPs as features and trains the classifier only once.</p><p>In the latter type, the idea of adversarial learning is usually adopted to shift the task of separating gesture-related features from domain-related ones. EI <ref type="bibr" target="#b19">[20]</ref> incorporates an adversarial network to obtain domain-independent features from CSI. However, crossdomain learning methodologies require extra data samples from the target domain, increasing data collection and training efforts. Moreover, features generated by learning models are semantically uninterpretable. In contrast, Widar3.0 explicitly extracts domainindependent BVPs, and only needs a simply designed learning model without the capability of cross-domain learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this paper, we propose a Wi-Fi based zero-effort cross-domain gesture recognition system. First, we model the quantitative relation between complex gestures and CSI dynamics, and extract velocity profiles of gestures in body coordinates, which are domainindependent and act as unique indicators of gestures. Then, we develop a one-fits-all deep learning model to fully exploit spatialtemporal characteristics of BVP for gesture recognition. We implement Widar3.0 on COTS Wi-Fi devices and evaluate it in real environments. Experimental results show that Widar3.0 achieves high recognition accuracy across different domain factors, specifically, 89.7%, 82.6%, 92.4% and 88.9% for user's location, orientation, environment and user diversity, respectively. Future work focuses on applying Widar3.0 to fortify various sensing applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 1 :</head><label>21</label><figDesc>Figure 1: Cross-domain gesture recognition, where persons may be at different locations and orientations relative to Wi-Fi links, and environments (e.g., lab, home, etc.). In this example, one male and one female are performing clapping gestures in two domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dominant DFS of gesture differs with person orientations and locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Complex gestures cause multiple DFS components.</figDesc><graphic url="image-1.png" coords="3,227.75,91.10,156.49,97.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: System overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Relationship between the BVP and DFS profiles. Each velocity component in BVP is projected onto the normal direction of a link, and contributes to the power of the corresponding radial velocity component in the DFS profile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The BVP series of a pushing and pulling gesture. The main velocity component corresponding to the person's hand is highlighted with red circles in all snapshots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Structure of gesture recognition model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :Figure 11 :</head><label>911</label><figDesc>Figure 9: Layouts of three evaluation environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Statistics of participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Confusion matrices of different settings with two gesture datasets.</figDesc><graphic url="image-10.png" coords="9,83.81,230.00,123.07,78.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Accuracy distributions for cross-location evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 19 :</head><label>19</label><figDesc>Figure 15: Accuracy distributions for cross-orientation evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Comparison of DNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 21 :Figure 22 :Figure 23 :Figure 24 :Figure 25 :</head><label>2122232425</label><figDesc>Figure 21: Impact of link numbers.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We sincerely thank our shepherd <rs type="person">Professor Yingying Chen</rs> and the anonymous reviewers for their valuable feedback. We also thank <rs type="person">Junbo Zhang</rs>, the undergraduate student at <rs type="affiliation">Tsinghua University</rs>, for helping to build the platform. This work is supported in part by the <rs type="funder">National Key Research Plan</rs> under grant No. <rs type="grantNumber">2016YFC0700100</rs>, <rs type="funder">NSFC</rs> under grants <rs type="grantNumber">61832010</rs>, <rs type="grantNumber">61632008</rs>, <rs type="grantNumber">61672319</rs>, <rs type="grantNumber">61872081</rs>, and <rs type="funder">National Science Foundation</rs> under grant <rs type="grantNumber">CNS-1837146</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zTgvSDa">
					<idno type="grant-number">2016YFC0700100</idno>
				</org>
				<org type="funding" xml:id="_y9Unend">
					<idno type="grant-number">61832010</idno>
				</org>
				<org type="funding" xml:id="_jQ6rJTB">
					<idno type="grant-number">61632008</idno>
				</org>
				<org type="funding" xml:id="_yCrXdYm">
					<idno type="grant-number">61672319</idno>
				</org>
				<org type="funding" xml:id="_MQh7DVx">
					<idno type="grant-number">61872081</idno>
				</org>
				<org type="funding" xml:id="_YKJVMsD">
					<idno type="grant-number">CNS-1837146</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wigest: A Ubiquitous Wifi-based Gesture Recognition System</title>
		<author>
			<persName><forename type="first">Heba</forename><surname>Abdelnasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Youssef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><forename type="middle">A</forename><surname>Harras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM<address><addrLine>Kowloon, Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Capturing the Human Figure Through a Wall</title>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Fadel Adib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzi</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015-11">2015. November 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Person Localization via RF Body Reflections</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Fadel Adib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Kabelac</surname></persName>
		</author>
		<author>
			<persName><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX NSDI</title>
		<meeting>USENIX NSDI<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d Tracking via Body Radio Reflections</title>
		<author>
			<persName><forename type="first">Zach</forename><surname>Fadel Adib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Kabelac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX NSDI</title>
		<meeting>USENIX NSDI<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">See Through Walls with Wi-Fi!</title>
		<author>
			<persName><forename type="first">Fadel</forename><surname>Adib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing keystrokes using WiFi devices</title>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shahzad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1175" to="1190" />
			<date type="published" when="2017-05">2017. May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiple Target Tracking with RF Sensor Networks</title>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Bocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ossi</forename><surname>Kaltiokallio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Patwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1787" to="1800" />
			<date type="published" when="2014-08">2014. August 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Tutorial on Human Activity Recognition Using Body-Worn Inertial Sensors</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Blanke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2014-01">2014. January 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributionally Robust Semi-Supervised Learning for People-Centric Sensing</title>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
	</analytic>
	<monogr>
		<title level="j">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hierarchical Multiscale Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1609.01704</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?aglar</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compressed Sensing</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006-04">2006. April 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BodyScan: A Wearable Device for Contact-less Radio-based Sensing of Bodyrelated Activities</title>
		<author>
			<persName><forename type="first">Biyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Nicholas D Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Boran</surname></persName>
		</author>
		<author>
			<persName><surname>Kawsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiSys</title>
		<meeting>ACM MobiSys<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HeadScan: A Wearable System for Radio-based Sensing of Head and Mouth-related Activities</title>
		<author>
			<persName><forename type="first">Biyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Nicholas D Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Kawsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM/IEEE IPSN</title>
		<meeting>ACM/IEEE IPSN<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting and Recognizing Human-Object Interactions</title>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ensembles of Deep LSTM Learners for Activity Recognition Using Wearables</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Pl?tz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2017-06">2017. June 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tool Release: Gathering 802.11n Traces with Channel State Information</title>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Daniel Halperin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anmol</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><surname>Wetherall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="53" />
			<date type="published" when="2011-01">2011. January 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feasibility and Limits of Wi-Fi Imaging</title>
		<author>
			<persName><forename type="first">Donny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajalakshmi</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamnath</forename><surname>Gollakota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiSys</title>
		<meeting>ACM MobiSys<address><addrLine>Bretton Woods, NH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards Environment Independent Device Free Human Activity Recognition</title>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenglong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuochao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Koutsonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiCom</title>
		<meeting>ACM MobiCom<address><addrLine>New Delhi, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wideo: Fine-grained Device-free Motion Tracing Using RF Backscatter</title>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Bharadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manikanta</forename><surname>Kotaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX NSDI</title>
		<meeting>USENIX NSDI<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One-Handed Gesture Recognition Using Ultrasonic Doppler Sonar</title>
		<author>
			<persName><forename type="first">Kaustubh</forename><surname>Kalgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ICASSP</title>
		<meeting>IEEE ICASSP<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">WiFinger: Talk to Your Smart Devices with Finger-grained Gesture</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM UbiComp</title>
		<meeting>ACM UbiComp<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Practical Human Sensing in the Light</title>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiSys</title>
		<meeting>ACM MobiSys<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic-MUSIC: Accurate Device-Free Indoor Localization</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasha</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM UbiComp</title>
		<meeting>ACM UbiComp<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">IndoTrack: Device-Free Indoor Human Tracking with Commodity Wi-Fi</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2017-09">2017. September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lasagna: Towards Deep Hierarchical Understanding and Searching over Mobile Sensing Data</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongqian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiCom</title>
		<meeting>ACM MobiCom<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SignFi: Sign Language Recognition Using WiFi</title>
		<author>
			<persName><forename type="first">Yongsen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangquan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woosub</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2018-03">2018. March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Covertband: Activity Information Leakage Using Music</title>
		<author>
			<persName><forename type="first">Rajalakshmi</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Takakuwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamnath</forename><surname>Gollakota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2017-09">2017. September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosting Fine-grained Activity Sensing by Embracing Wireless Multipath Effects</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fusang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CoNEXT</title>
		<meeting>ACM CoNEXT<address><addrLine>Heraklion/Crete; Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Whole-Home Gesture Recognition Using Wireless Signals</title>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidhant</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shyamnath Gollakota, and Shwetak Patel</title>
		<meeting><address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Proceedings of ACM Mobi-Com</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Widar: Decimeter-Level Passive Tracking via Velocity Monitoring with Commodity Wi-Fi</title>
		<author>
			<persName><forename type="first">Chenshu</forename><surname>Kun Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Jamieson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiHoc</title>
		<meeting>ACM MobiHoc<address><addrLine>Chennai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Widar2.0: Passive Human Tracking with a Single Wi-Fi Link</title>
		<author>
			<persName><forename type="first">Chenshu</forename><surname>Kun Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiSys</title>
		<meeting>ACM MobiSys<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inferring Motion Direction Using Commodity Wi-Fi for Interactive Exergames</title>
		<author>
			<persName><forename type="first">Chenshu</forename><surname>Kun Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CHI</title>
		<meeting>ACM CHI<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Earth Mover&apos;s Distance as a Metric for Image Retrieval</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000-11">2000. November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">I am a Smartwatch and I can Track my User&apos;s Arm</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romit</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiSys</title>
		<meeting>ACM MobiSys<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A DIRT-T Approach to Unsupervised Domain Adaptation</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirokazu</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-User Gesture Recognition Using WiFi</title>
		<author>
			<persName><forename type="first">H</forename><surname>Raghav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Griffin</forename><surname>Venkatnarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><surname>Shahzad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiSys</title>
		<meeting>ACM MobiSys<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Position and Orientation Agnostic Gesture Recognition Using WiFi</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Virmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shahzad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiSys</title>
		<meeting>ACM MobiSys<address><addrLine>Niagara Falls, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stratified Transfer Learning for Cross-domain Activity Recognition</title>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE PerCom</title>
		<meeting>IEEE PerCom<address><addrLine>Big Island, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LiFS: Low Human-effort, Device-free Localization with Fine-grained Subcarrier Information</title>
		<author>
			<persName><forename type="first">Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiCom</title>
		<meeting>ACM MobiCom<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recurrent Modeling of Interaction Context for Collective Activity Recognition</title>
		<author>
			<persName><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gait Recognition Using WiFi Signals</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shahzad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM UbiComp</title>
		<meeting>ACM UbiComp<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Device-Free Human Activity Recognition Using Commercial WiFi Devices</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanglu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1118" to="1131" />
			<date type="published" when="2017-05">2017. May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">E-eyes: Device-free Location-oriented Activity Identification Using Fine-grained WiFi Signatures</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiCom</title>
		<meeting>ACM MobiCom<address><addrLine>Maui, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From RSSI to CSI: Indoor Localization via Channel Response</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2013-11">2013. November 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DeepSense: A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing</title>
		<author>
			<persName><forename type="first">Shuochao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarek</forename><surname>Abdelzaher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM WWW</title>
		<meeting>ACM WWW<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">BodyScope: A Wearable Acoustic Sensor for Activity Recognition</title>
		<author>
			<persName><forename type="first">Koji</forename><surname>Yatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khai</surname></persName>
		</author>
		<author>
			<persName><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM UbiComp</title>
		<meeting>ACM UbiComp<address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">WiWho: WiFi-Based Person Identification in Smart Spaces</title>
		<author>
			<persName><forename type="first">Yunze</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parth</forename><forename type="middle">H</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasant</forename><surname>Mohapatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM/IEEE IPSN</title>
		<meeting>ACM/IEEE IPSN<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CrossSense: Towards Cross-Site and Large-Scale WiFi Sensing</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petteri Tapio</forename><surname>Nurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM MobiCom</title>
		<meeting>ACM MobiCom<address><addrLine>New Delhi, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Through-Wall Human Pose Estimation Using Radio Signals</title>
		<author>
			<persName><forename type="first">Mingmin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Abu Alsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">RF-Based 3D Skeletons</title>
		<author>
			<persName><forename type="first">Mingmin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Abu Alsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumen</forename><surname>Hristov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Kabelac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cross-People Mobile-Phone Based Activity Recognition</title>
		<author>
			<persName><forename type="first">Zhongtang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Detecting Radio Frequency Interference for CSI Measurements on COTS WiFi Devices</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenshu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ICC</title>
		<meeting>IEEE ICC<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
