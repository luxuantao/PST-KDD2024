<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A new belief-based K-nearest neighbor classification method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-10-11">11 October 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhun-Ga</forename><surname>Liu</surname></persName>
							<email>liuzhunga@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><surname>Pan</surname></persName>
							<email>quanpan@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean</forename><surname>Dezert</surname></persName>
							<email>jean.dezert@onera.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">ONERA -The French Aerospace Lab</orgName>
								<address>
									<postCode>F-91761</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A new belief-based K-nearest neighbor classification method</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-10-11">11 October 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">00BAE9C01D82F0CD7970BEC7EA13FFB3</idno>
					<idno type="DOI">10.1016/j.patcog.2012.10.001</idno>
					<note type="submission">Received 1 November 2011 Received in revised form 2 August 2012 Accepted 1 October 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>K-nearest neighbor Data classification Belief functions DST Credal classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The K-nearest neighbor (K-NN) classification method originally developed in the probabilistic framework has serious difficulties to classify correctly the close data points (objects) originating from different classes. To cope with such difficult problem and make the classification result more robust to misclassification errors, we propose a new belief-based K-nearest neighbor (BK-NN) method that allows each object to belong both to the specific classes and to the sets of classes with different masses of belief. BK-NN is able to provide a hyper-credal classification on the specific classes, the rejection classes and the meta-classes as well. Thus, the objects hard to classify correctly are automatically committed to a meta-class or to a rejection class, which can reduce the misclassification errors. The basic belief assignment (bba) of each object is defined from the distance between the object and its neighbors and from the acceptance and rejection thresholds. The bba's are combined using a new combination method specially developed for the BK-NN. Several experiments based on simulated and real data sets have been carried out to evaluate the performances of the BK-NN method with respect to several classical K-NN approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The K-nearest neighbor (K-NN) method <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1]</ref> is a well known classification algorithm used in pattern recognition. In the original voting K-NN, the object is assigned to the majority class according to its K nearest neighbors (KNNs), and the distances between the object and its neighbors are ignored. In <ref type="bibr" target="#b10">[11]</ref>, a weighted version of K-NN (WK-NN) taking into account the distance between the object and its KNNs has been proposed by Dudani, and it can provide better performance than voting K-NN method in the data classification. In the classification of close 1 data sets, some similar data points originating from different classes are difficult to classify correctly into a specific class and we call them as imprecise data. In such case, the classical K-NN methods cannot well deal with ambiguity and imprecise information because of the limitation of the probabilistic framework, more precisely its inability to make a clear distinction between the full lack of knowledge (circumvented by the insufficient reason argument) and the full knowledge of the equiprobable cases.</p><p>Belief functions theory <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref> also called evidence theory (or Dempster-Shafer Theory (DST)) makes a clear distinction between the different aspects of uncertainty (mainly the randomness and the non-specificity) through the rigorous formalism of belief functions. DST has been already applied in many fields, including parameter estimation <ref type="bibr" target="#b4">[5]</ref>, classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>, clustering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> and decision-making support <ref type="bibr" target="#b19">[20]</ref>. Credal partition (classification) as an extension of the hard classification and the probabilistic (fuzzy) classification has been introduced for the data clustering under the belief functions framework in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>. It allows the objects to belong to not only the specific classes but also the set of classes (i.e. meta-class) with different masses of belief. The imprecise data that are hard to classify correctly in specific classes, are more reasonably committed to the meta-class defined by the disjunction of some specific classes they are close to, which can reduce (avoid) the misclassification errors.</p><p>The K-NN has been extended in the belief functions framework by Denoeux <ref type="bibr" target="#b5">[6]</ref> to better model the uncertain information. Denoeux's method is referred as the evidential K-NN method (or EK-NN for short) in this paper. In EK-NN, a simple construction of the basic belief assignments (bba's) of the object associated to its KNNs was proposed, and only two focal elements (i.e. the specific class and total ignorant class) are involved in each bba. The combination results of these bba's by Dempster's rule (also called DS rule for short) are used to classify the objects. In EK-NN, it works only with one extra focal element (representing the total ignorant class) with respect to what is done within the classical K-NN approach. The meta-classes (i.e. the partial ignorant classes), which can well represent the imprecise data, are not considered as possible classification results in EK-NN. Moreover, it is worth to note that even if one object is far from its KNNs, <ref type="foot" target="#foot_0">2</ref> after the combination of the K bba's the largest mass of belief can be committed to a specific class rather to the total ignorance due to the particular structure of bba's in EK-NN, which is not a good expected behavior.</p><p>A new data classification method called belief-based K-nearest neighbor (BK-NN) is proposed here to overcome the limitations of EK-NN, and the meta-class is taken into account to characterize the imprecise data. BK-NN can reduce misclassification errors by making the classification result partly imprecise and in the meantime robust thanks to the introduction of meta-classes. In many applications (specially those related to defense and security, like in target classification and tracking), it is better to get a robust (and eventually partly imprecise) result that will need to be precisiated with additional techniques, than to obtain directly with high risk a wrong precise classification from which an erroneous fatal decision would be drawn.</p><p>For instance, in the handwritten digits (e.g. USPS data set <ref type="bibr" target="#b15">[16]</ref>) classification problem, some badly written digits are very difficult to recognize, and they should be prudently committed to the associated meta-classes or simply be regarded as an outlier in the extreme case in order to reduce the misclassification errors. Let us consider a simple 2-class of handwritten digits 1 and 4 in USPS data set. The two badly written digits of 4 as Fig. <ref type="figure" target="#fig_0">1</ref>(a) and (b) are similar but not enough to be associated easily with a common handwritten digit 4. The samples 1 and 2 look also quite similar to the written digit 1. In such a difficult case, it is better to commit the samples 1 and 2 to the meta-class f1,4g to avoid a wrong precise classification. The samples 3 and 4 of handwritten digit 4 shown in Fig. <ref type="figure" target="#fig_9">1(c</ref>) and (d) are almost impossible to classify visually as corresponding to the digit 4, and they are neither similar to the digit 1. So these two samples can be considered as an outlier. Such badly handwritten digits belonging to the metaclass or outlier class should be classified manually by human operators or by other methods with additional information.</p><p>In BK-NN, the bba's of one object is constructed according to the distances between the object and its KNNs and two (acceptance and rejection) thresholds. Three focal elements including specific class, the rejection of this specific class (called rejection class), and the ignorant class are involved in the construction of the bba's. The potential unknown (outlier) class is considered being included in the rejection classes. To deal efficiently with the imprecise (ambiguous) data hard to classify, the meta-classes are conditionally selected in the fusion process of the bba's according to the number of their KNNs in each class. A new combination rule inspired by DS rule <ref type="bibr" target="#b25">[26]</ref> and Dubois-Prade (DP) rule <ref type="bibr" target="#b9">[10]</ref> is proposed to combine these bba's, and some partial conflicting beliefs are preserved and committed to the selected meta-classes. Three kinds of classes, which include specific classes, rejection classes and meta-classes, will be involved in the fusion results and this constitutes the hyper-credal classification. Thus, BK-NN works with a more enlarged classification spectrum than EK-NN and K-NN. The specific class refers to the data (object) very close to the training samples in this specific class as in K-NN. The rejection class defined by the complement of several classes refers to data that is quite far (beyond the rejection threshold) from these several classes, but not close (beyond the acceptance threshold) enough to the other classes. If one object is in a rejection class, we can just confirm that it is not in the several specific classes involved in the rejection class, but it does not indicates that it must belong to the other classes in the frame of discernment, since it still could be in the potential outlier class. The meta-class defined by the disjunction of several specific classes refers to the data that is simultaneously close to all the involved specific classes, which are not very distinguishable for the data in this meta-class. If some objects are considered in the meta-classes or in the rejection classes, it means that the available information is not sufficient enough for making a precise (specific) classification of such objects. Actually, the output of BK-NN can also viewed as an interesting tool to justify the use of complementary information sources if very precise classification results are sought whenever BK-NN fails to provide them. In the next section, the background of belief functions theory is briefly introduced. In Section 3, EK-NN method is briefly recalled and analyzed. The details of BK-NN are presented in Section 4. Several experiments are given in Section 5 to show how BK-NN performs with respect to EK-NN. Concluding remarks are given in the last section of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Basics of belief functions theory</head><p>Belief functions theory is also called Dempster-Shafer theory (DST), or evidence theory <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Let us consider Y being a finite discrete set of mutually exclusive and exhaustive hypotheses. Y is called the frame of discernment of the problem under consideration. The power-set of Y, 2 Y , contains all the subsets of Y. The credal partition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref> for data clustering over the frame Y corresponds to the power-set 2 Y .</p><p>For example, if Y ¼ fy 1 ,y 2 ,y 3 g, then 2 Y ¼ f|,y 1 ,y 2 ,y 3 ,y 1 [ y 2 , y 1 [ y 3 ,y 2 [ y 3 ,Yg.</p><p>A basic belief assignment (bba) is a function mðÁÞ from 2 Y to ½0,1 satisfying</p><formula xml:id="formula_0">P A A 2 Y mðAÞ ¼ 1 mð|Þ ¼ 0 8 &lt; :<label>ð1Þ</label></formula><p>The quantity m(A) can be interpreted as a measure of the belief that is committed exactly to A, and not to any of its subsets. The subsets A of Y such that mðAÞ 4 0 are called the focal elements of mðÁÞ, and the set of all its focal elements is called the core of mðÁÞ.</p><p>Several distinct bodies of evidence characterized by different bba's can be combined using DS rule <ref type="bibr" target="#b25">[26]</ref>. Mathematically, the DS rule of combination of two bba's m 1 ðÁÞ and m 2 ðÁÞ defined on 2 Y is defined<ref type="foot" target="#foot_1">3</ref> by m DS ð|Þ ¼ 0 and for A a |, B,C A 2 Y by m DS ðAÞ ¼</p><formula xml:id="formula_1">P B\C ¼ A m 1 ðBÞm 2 ðCÞ 1À P B\C ¼ | m 1 ðBÞm 2 ðCÞ ¼ P B\C ¼ A m 1 ðBÞm 2 ðCÞ P B\C a | m 1 ðBÞm 2 ðCÞ<label>ð2Þ</label></formula><p>In DS rule, the total conflicting belief mass</p><formula xml:id="formula_2">P B\C ¼ | m 1 ðBÞm 2<label>ðCÞ</label></formula><p>is redistributed back to all the focal elements through the normalization.</p><p>To palliate some drawbacks of DS rule, Dubois and Prade have proposed their own rule in <ref type="bibr" target="#b9">[10]</ref>, denoted DP rule in this paper. The basic idea behind DP rule consists to transfer each partial conflicting mass on the disjunction of the elements involved in the partial conflict. Mathematically, DP rule is defined by m DP ð|Þ ¼ 0 and for A A 2 Y \f|g by</p><formula xml:id="formula_3">m DP ðAÞ ¼ X B\C ¼ A m 1 ðBÞm 2 ðCÞþ X B\C ¼ | B[C ¼ A m 1 ðBÞm 2 ðCÞ ð<label>3Þ</label></formula><p>The belief function Bel(A) and the plausibility function Pl(A) <ref type="bibr" target="#b25">[26]</ref> are usually interpreted as lower and upper probabilities of A. They are mathematically defined by</p><formula xml:id="formula_4">BelðAÞ ¼ X A,B A 2 Y ; B D A mðBÞ ð<label>4Þ</label></formula><formula xml:id="formula_5">PlðAÞ ¼ X A,B A 2 Y ; A\B a | mðBÞ ð<label>5Þ</label></formula><p>Pignistic probability transformation BetPðÁÞ (betting probability) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> is commonly chosen to transform any bba mðÁÞ into a probability measure for the decision-making support in the framework of belief functions. Mathematically, BetP(A) is defined for A A 2 Y \f|g by</p><formula xml:id="formula_6">BetPðAÞ ¼ X B A 2 Y , A D B 9A \ B9 9B9 mðBÞ ð<label>6Þ</label></formula><p>where 9X9 is the cardinality of the element X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Brief recall and comments on EK-NN</head><p>The original voting K-NN <ref type="bibr" target="#b12">[13]</ref> is a well-known non-parametric classification method used in the Pattern Recognition field. In this method, an object to classify is assigned to the class represented by a majority of its KNNs in the training set. It has been pointed by Cover and Hart in <ref type="bibr" target="#b2">[3]</ref> that the error rate of the k-NN rule approaches the optimal Bayes error rate if the number N of samples and K both tend to infinity as K=N-0. However, the voting k-NN rule is not guaranteed to be the optimal way of using the information contained in the neighborhood of unclassified patterns in the finite sample case. That is why many attempts have been done to improve K-NN rule, like distance-weighted k-NN rule <ref type="bibr" target="#b10">[11]</ref>, etc. An evidential K-nearest neighbor (EK-NN) method was proposed by Denoeux in <ref type="bibr" target="#b5">[6]</ref> based on DST, and it can provide a global treatment of some important issues: the consideration of the distances information, ambiguity and distance rejection, and the consideration of uncertainty.</p><p>Let us consider a group of objects X ¼ fx 1 , x 2 , . . . , x n g to be classified into several classes, and the frame of the classes is O ¼ fw 1 , . . . ,w h g. In EK-NN <ref type="bibr" target="#b5">[6]</ref>, the bba's of the object x i associated to the training data x j with labeled class w s A O is defined as</p><formula xml:id="formula_7">m x i j ðw s Þ ¼ ae Àg s d b<label>ð7Þ</label></formula><formula xml:id="formula_8">m x i j ðOÞ ¼ 1Àae Àg s d b<label>ð8Þ</label></formula><p>where a, g s and b are tuning parameters, and d is the distance between the objects and the training data. The justification of the choice of these bba's is given by Denoeux in <ref type="bibr" target="#b5">[6]</ref>.</p><p>As shown through formulas ( <ref type="formula" target="#formula_7">7</ref>)- <ref type="bibr" target="#b7">(8)</ref>, in EK-NN only the two focal elements w s and O are involved for a given bba m x i j ðÁÞ, i¼1,y,n. The classification result of each object x i is obtained from the fusion of the K bba's associated with the K nearest neighbors using DS rule <ref type="bibr" target="#b25">[26]</ref>. Because of the particular structure of bba's, the EK-NN based on DS rule can only produce the specific classes and the total ignorance class O as focal elements. Therefore, no meta-classes (partial ignorant classes) like w i [ Á Á Á [ w j fw i , . . . ,w j g can be a focal element, which is not convenient for dealing efficiently with imprecise (non specific) information.</p><p>The particular structure of bba's chosen in EK-NN generates also a serious problem because when K is big enough and if the K-nearest neighbors of the object have the same labeled class w s but are quite far from it, the object will be committed to the class w s rather than to the ignorance O. Such EK-NN behavior is unreasonable as we will show in the following example.</p><p>Example 1. Let us assume that one object x 1 is very far from all the training data, and its K ¼p nearest neighbors belong to the same class w 1 , and they are located at the same distance from x 1 , so that we must consider the following p bba's, with dA½0,1:</p><formula xml:id="formula_9">m i ðw 1 Þ ¼ d m i ðOÞ ¼ 1Àd (<label>ð9Þ</label></formula><formula xml:id="formula_10">where i ¼ 1, . . . ,p.</formula><p>Then, the fusion result obtained by DS rule is</p><formula xml:id="formula_11">mðw 1 Þ ¼ 1Àð1ÀdÞ p , mðOÞ ¼ ð1ÀdÞ p<label>ð10Þ</label></formula><p>One can see that mðw 1 Þ increases when p increases. So even if the value d40 is very small, which means that x 1 is far from its K-nearest neighbors, then the mass of belief committed to w 1 can be very large when p is big enough. It can be easily verified that when p 4log 0:5=logð1ÀdÞ, one has mðw 1 Þ 4 0:5 4mðOÞ. For example, if d ¼ 0:2, and p 4 3, then mðw 1 Þ 40:5 4 mðOÞ. Such behavior of EK-NN approach due to the particular structure of bba's to combine and the mechanism of the DS rule is not very reasonable, nor appropriate in fact. In our opinions, the object x s should be classified as outlier when it is far from its K-nearest neighbors, no matter how many nearest neighbors are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Belief-based K-nearest neighbor method (BK-NN)</head><p>A new data classification method called belief-based K-nearest neighbor method (BK-NN) is proposed to overcome the limitations of EK-NN. In BK-NN, the K bba's associated with one object and its KNNs are computed from the distance between the object and its neighbors, and from the acceptance and rejection thresholds in each class. Thus, each bba contains three focal elements including the specific class (e.g. w i ), the rejection class (e.g. w i ) and the ignorant class O. The potential unknown (outlier) class is taken into account in the rejection class. This new structure of bba's is able to better reveal the degree of one object belonging (or not) to a class label of its close neighbor.</p><p>In the fusion of these bba's, if several specific classes appear undistinguishable for one object according to the number of its KNNs in each class, the meta-class (the disjunction of these specific classes) will be taken into account in the fusion process to avoid false precise classification. The associated partial conflicting beliefs will be kept and committed to the selected available meta-classes by a new fusion rule inspired from DP rule. Hence, BK-NN allows to classify an object not only to a specific classes, but possibly also to the sets of classes (i.e. metaclasses and rejection classes) with the different masses of belief. This enlarged classification, called hyper-credal classification, provides a deeper insight into the data, and allow to reduce the error rate in the classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The construction of bba's in BK-NN</head><p>Let us consider the frame of the classes as O ¼ fw 0 ,w 1 , . . . ,w h g including one potential unknown (outlier) class w 0 . The K nearest neighbors of the data point (object) x i are found at first as with the K-NN method. The acceptance and rejection thresholds with respect to the distance between the object and its KNNs are introduced for the construction of bba's as illustrated in Fig. <ref type="figure" target="#fig_5">2</ref>.</p><p>We assume that one of the K-nearest neighbors x j of the object x i to classify is labeled by class w s . In BK-NN, the determination of bba's associated with x i and x j is based on the given acceptance threshold d given for each class. If the distance d ij between x i and x j is smaller than d ws ta , it indicates that x i lies in the acceptance zone of w s with respect to x j , and x i will be considered most likely to belong to class w s . If d ij is larger than d ws tr , it means that x i is beyond the rejection zone. So x i does not very likely belong to the class w s , and it has more chance to belong to the complementary set (rejection) of w s , denoted w s , rather than w s . If d ws ta rd ij rd ws tr , we consider that x i can be reasonably committed to uncertainty (i.e. the full ignorance w s [ w s ) because there is no strong support in favor of w s w.r.t. w s , and conversely.</p><p>In BK-NN, the bba's are defined as a function of the distance between the object and the training data, and three focal elements are involved in the construction of the bba's. This makes a big difference w.r.t. what was proposed in EK-NN approach as it will be shown in the sequel. So, for any object x i , its bba's associated with one of its K-nearest neighbors x j labeled the class w s will be obtained by the fusion of the two elementary bba's</p><formula xml:id="formula_12">m 1 ðÁ9d ij Þ and m 2 ðÁ9d ij Þ listed in the following table: Focal element m 1 ðÁ9d ij Þ m 2 ðÁ9d ij Þ w s f 1 ðd ij ,d ws ta Þ 0 w s 0 f 2 ðd ij ,d ws tr Þ O 1Àf 1 ðd ij ,d ws ta Þ 1Àf 2 ðd ij ,d ws tr Þ d ij 9dðx i ,x j Þ</formula><p>is the distance between the object x i and its closest neighbor x j . The functions f 1 ðÁÞ and f 2 ðÁÞ characterizing the good (expected) behavior for an efficient bba to a class, its complement, or to ignorance should satisfy the following properties:</p><p>(1) f 1 ðÁÞ should be monotone decreasing, and f 1 ðÁÞ A ½0,1;  <ref type="bibr" target="#b3">[4]</ref> that the two following sigmoid functions, <ref type="foot" target="#foot_3">4</ref>satisfy the four aforementioned requested properties and thus they can be used to construct the bba's m 1 ðÁ9d ij Þ and m 2 ðÁ9d ij Þ: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It has been shown in</head><formula xml:id="formula_14">f 1 ðd ij ,d ws ta Þ9 1 1 þ e l j ðd ij Àd ws ta Þ<label>ð11Þ</label></formula><formula xml:id="formula_15">f 2 ðd ij ,d</formula><p>We recommend to take g ta A ½1,3 and g tr A ½2g ta ,3g ta according to our applications. The bigger d ws ta will cause more objects committed into the meta-classes, and the smaller d ws tr will make more objects considered as outliers. ½d It can be verified that the bba m ws ðÁ9d ij Þ associated with the point x i and class w s satisfying the global expected behavior is then obtained by the fusion of the two simple bba's m 1 ðÁ9d ij Þ and m 2 ðÁ9d ij Þ. Because of the structure of these two simple bba's m 1 ðÁ9d ij Þ and m 2 ðÁ9d ij Þ, the fusion obtained with DS rule <ref type="bibr" target="#b25">[26]</ref>, PCR5 rule <ref type="bibr" target="#b27">[28]</ref>, and even with DSm hybrid (DSmH) rule <ref type="bibr" target="#b27">[28]</ref> provide globally similar results and therefore the choice of the fusion rule here does not really matter to build m x i ðÁ9d ij Þ. For simplicity, DS rule (2) is used here to combine m 1 ðÁ9d ij Þ and m 2 ðÁ9d ij Þ for getting m ws ðÁ9d ij Þ since for historical reasons, it is the most common combination rule used so far in the belief functions framework. <ref type="foot" target="#foot_4">5</ref>Based on this approach, K bba's corresponding to KNNs of x i can be constructed. Moreover, if there is no point in the KNNs of x i belonging to class w p , we consider that x i is impossible to belong to class w p , and in that case the additional particular bba's should be added as</p><formula xml:id="formula_17">m wp ðw p Þ ¼ 1<label>ð15Þ</label></formula><p>This particular bba's is added to eliminate w p from the possible classification results. For example, let us consider O ¼ fw 0 ,w 1 ,w 2 g.</p><p>If the KNNs of one object are all in class w 1 , but this object is quite far from all the training data. Then the focal element w 1 will take the most mass of belief in the construction of the K bba's. If the particular bba's m w 2 ¼ 1 is not included, w 1 ¼ fw 0 ,w 2 g will still take the biggest mass of belief in the combination results of the K bba's. However, we consider that this object should belong to the outlier class w 1 [ w 2 ¼ w 0 . Thus, the particular bba's is necessary to be fused with the combination results of the K bba's in order to remove the impossible class w 2 as</p><formula xml:id="formula_18">w 1 \ w 2 ¼ w 1 [ w 2 ¼ w 0 .</formula><p>It is worth to note that x i A w j does mean nothing but that the point x i is not in the class w j . For example, let us consider a frame O ¼ fw 0 ,w 1 ,w 2 g. If x i A w 1 , it means that x i is not in the class w 1 , but it does not mean that x i belongs to the class w 2 , since x i can still be not close enough to w 2 (beyond the acceptance threshold of w 2 ), and therefore x i could be in fact an outlier, that is why we consider w 1 ¼ fw 2 ,w 0 g a w 2 in BK-NN approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The combination of bba's in BK-NN</head><p>The fusion results of the K bba's (with the additional particular bba's if available) for one object can be used to classify this object. The fusion process of the K bba's consists of two steps: (1) subcombination of the bba's associated with the same class; (2) global fusion of the sub-combination results about different classes.</p><p>In the sub-combination of the bba's associated with the close neighbors labeled by the same class, the conflicting beliefs (e.g. m 1 ðw s Þ Á m 2 ðw s Þ) produced by the conjunction of the bba's do not play a particular role, and they must be distributed to the other focal elements to preserve the normalization of the combined bba. So DS rule (2) can be used here for the combination of the bba's in this first step.</p><p>The sub-combination results m ws ðÁÞ, for s ¼ In order to reach a good compromise between the imprecision rate and the error rate for classification, we argue that the partial conflicting beliefs corresponding to the meta-class should be conditionally preserved according to current context.</p><p>The KNNs of the imprecise objects usually have different class labels, and the number of the KNNs in different classes is similar. So the meta-classes can be automatically selected according to the number of KNNs in different classes for one object.</p><p>Let us consider the KNNs of one object are associated with w 1 , . . . ,w h , and the number of KNNs in each class is ½k 1 , . . . ,k h . The biggest value k max ¼ maxðk 1 , . . . ,k h Þ is found at first. Then if k max Àk i rt,i ¼ 1, . . . ,g (t being a given threshold), the object will be very likely in the union of classes (also called meta-class) c max ¼ fw i 9k max Àk i r tg, and w i ,i ¼ 1, . . . , g are potentially indistinguishable for the object since the number of KNNs in w i ,i ¼ 1, . . . , g is similar. However, the number of KNNs in each class is not sufficient to determine the class of the object, and the object can possibly be in a class which is not included in c max in rare cases. To deal with this problem, all the meta-classes having a cardinality<ref type="foot" target="#foot_5">6</ref> less than or equal to 9c max 9 will be used. By doing this, all the classes in O can be fairly treated. The set of the selected meta-classes is denoted by C. For example, let us consider O ¼ fw 0 ,w 1 ,w 2 ,w 3 g with the number of KNNs in each corresponding class is ½k 1 ,k 2 ,k 3 . If k max ¼ k 2 and c max ¼ fw 2 ,w 3 g9w 2 [ w 3 , then all the meta-classes whose cardinality is not bigger than 9fw 2 ,w 3 g9 ¼ 2 will be selected, and the selected meta-classes are given by C ¼ fw 1 [ w 2 ,w 1 [ w 3 , w 2 [ w 3 g. If the number of KNNs in w max is much bigger than that in the other classes as k max Àk i 4 t, i amax, none meta-class needs to be preserved in order to avoid the high imprecision of the solution.</p><p>The tuning threshold t should become bigger with the increasing of the chosen number of nearest neighbors. So it can be simply determined by</p><formula xml:id="formula_19">t ¼ maxð1,Z Â KÞ ð<label>16Þ</label></formula><p>We recommend to take ZA½0:1,0:3 and t Z1. The bigger t can produce the fewer misclassification errors, but it usually leads to the larger meta-classes. t should be determined according to the expected compromise we want between the imprecision and misclassification of the results. The global fusion rule inspired from DS rule (2) and DP rule (3) is mathematically achieved by the formulas <ref type="bibr" target="#b16">(17)</ref> and <ref type="bibr" target="#b17">(18)</ref>. The bba's of the sub-combination results about the different classes can be fused sequentially by m 1,s ðAÞ ¼</p><formula xml:id="formula_20">P B 1 \B 2 ¼ A A a | m 1,sÀ1 ðB 1 Þm ws s ðB 2 Þ P B 1 \B 2 ¼ | B 1 [B 2 ¼ A m 1,s ðB 1 Þm ws s ðB 2 Þ 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :<label>ð17Þ</label></formula><p>where m 1,s ðÁÞ is the unnormalized combination results of m 1 ðÁÞ, . . . ,m s ðÁÞ, and m 1,1 ðÁÞ ¼ m w 1 1 ðÁÞ. It is worth to note that this combination rule is close to DP rule (3), but the summation of the combined bba is not one (i.e. here one can have P m 1,s ðÁÞ r 1) if some partial conflicting beliefs are not preserved. In DP rule, the focal element A can be any subset of O, whereas in our rule a focal element A can be only a specific class, the ignorant class or just a selected meta-class (but not all possible meta-classes). This unnormalized combination rule is associative here because of the particular structure of bba's m ws s ðÁÞ satisfying the BK-NN modeling, and the combination order of the bba's has no effect on the results.</p><p>The unnormalized fusion results obtained by <ref type="bibr" target="#b16">(17)</ref> will then be normalized once all the bba's have been combined. The conflicting masses that are not committed to the meta-classes will be redistributed to the other focal elements. There are many ways to redistribute the conflicting masses. They can be redistributed to all the focal elements as done by the DS rule, or they can be redistributed to particular focal elements as done by the Proportional Conflict Redistribution Rule no. 5 (PCR5) rule <ref type="bibr" target="#b27">[28]</ref> (Vol. 2), etc. However, the redistribution method does not matter too much here on the fusion results, since the conflicting masses to be redistributed are usually very small, and the majority of the conflicting masses are generally committed to the selected metaclasses. For reducing the computation burden, we use the simple normalization of the fusion results given by mðAÞ ¼ m 1,s ðAÞ</p><formula xml:id="formula_21">P i m 1,s ðB i Þ<label>ð18Þ</label></formula><p>If none meta-class is selected, the global fusion rule will reduce to DS rule. Whereas, if all the meta-classes are preserved, the global fusion rule behaves like DP rule, and all the partial conflicting beliefs are kept for these meta-classes. This global fusion rule can be considered as a compromise between DS rule and DP rule, since there are just part of the meta-classes selected to be available depending on the actual condition.</p><p>The global fusion results can be used for the classification making support, and the belief function BelðÁÞ, the plausibility function PlðÁÞ and pignistic probability BetPðÁÞ introduced in Section 2 can also be used for decision making if necessary.</p><p>Three tuning parameters g ta , g tr and Z are necessary in the BK-NN algorithm. The parameters g ta and g tr can be obtained by a cross-validation using training data. The specific classification of each object is determined according to the classical pignistic probability BetPðÁÞ which allows to transform any bba into a probability measure. One can keep a validation set, and make sure the specific classification of the objects as correct as possible by tuning these two parameters for the given value of K nearest neighbors. The optimized value of the parameters corresponding to the lowest error rate can be found over a grid search. Based on our experiences and tests, one recommends the search scope as g ta A ½1,3 and g tr A ½2g ta ,3g ta . The parameter Z entering in the computation of the threshold t given in ( <ref type="formula" target="#formula_19">16</ref>) allows to control the number of the objects in the meta-classes. This parameter Z has however only a very little influence on the hard classification results of the objects. So Z should be tuned depending on the imprecision degree (i.e. the number of the objects in the metaclasses) one wants to accept.</p><p>The general principle of BK-NN for data classification is summarized in the flowchart of the Fig. <ref type="figure" target="#fig_7">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2. Let us consider the following frame of classes</head><p>O ¼ fw 0 ,w 1 ,w 2 ,w 3 g, and an object x i with 5-nearest neighbors three neighbors labeled by classes w 1 and two neighbors by w 2 with the corresponding bba's satisfying BK-NN bba modeling: We can see that none of the 5-nearest neighbors is in class w 3 , which indicates that x i is impossible to belong to w 3 . So one particular bba's should be additionally introduced as</p><formula xml:id="formula_22">m w 1 1 ðw 1 Þ ¼ 0:8, m w 1 1 ðw 1 Þ ¼ 0:1, m w 1 1 ðOÞ ¼ 0:1 m w 1 2 ðw 1 Þ ¼ 0:6, m w 1 2 ðw 1 Þ ¼ 0:3, m w 1 2 ðOÞ ¼ 0:1 m w 1 3 ðw 1 Þ ¼ 0:4, m w 1 3 ðw 1 Þ ¼ 0:4, m w 1 3 ðOÞ ¼ 0:2 m w2 4 ðw 2 Þ ¼ 0:5, m w2 4 ðw 2 Þ ¼ 0:2, m w2 4 ðOÞ ¼ 0:3 m w 2 5 ðw 2 Þ ¼ 0:6, m w 2 5 ðw 2 Þ ¼ 0:1, m w 2 5 ðOÞ ¼ 0:3 It is worth to note that we assume w 1 ¼ w 0 [ w 2 [ w 3 , w 2 ¼ w 0 [ w 1 [ w 3 and w 3 ¼ w 0 [ w 1 [ w 2 in</formula><formula xml:id="formula_23">m w 3 ðw 3 Þ ¼ 1 m w 1</formula><p>1 ðÁÞ, m w 1 2 ðÁÞ, m w 1 3 ðÁÞ associated with w 1 , and m w 2 4 ðÁÞ,m w 2 5 ðÁÞ associated with w 2 , are respectively combined using DS rule given by Eq. ( <ref type="formula" target="#formula_1">2</ref>). The combination results are</p><formula xml:id="formula_24">m w 1 ðw 1 Þ ¼ 0:8867, m w 1 ðw 1 Þ ¼ 0:1085, m w 1 ðOÞ ¼ 0:0048 m w 2 ðw 2 Þ ¼ 0:7590, m w 2 ðw 2 Þ ¼ 0:1325, m w 2 ðOÞ ¼ 0:1085 We get k max ¼ k 1 ¼ 3. If we select t ¼2, then 9k 1 Àk 2 9 ¼ 93À29 o 2 and 9k 1 Àk 3 9 ¼ 93À09 ¼ 3. So c max ¼ fw 1 ,w 2 g, and the selected meta-classes are C ¼ fw 1 [ w 2 ,w 1 [ w 3 ,w 2 [ w 3 g.</formula><p>The global fusion results of m w1 ðÁÞ,m w2 ðÁÞ and m w3 ðÁÞ using formulas <ref type="bibr" target="#b16">(17)</ref> are given by</p><formula xml:id="formula_25">mðw 1 Þ ¼ ½m w 1 ðw 1 Þðm w 2 ðw 2 Þþm w 2 ðOÞÞ Á m w 3 ðw 3 Þ ¼ 0:2137 mðw 2 Þ ¼ ½m w 2 ðw 2 Þðm w 1 ðw 1 Þþm w 1 ðOÞÞ Á m w 3 ðw 3 Þ ¼ 0:0860 mðw 1 [ w 2 Þ ¼ m w 1 ðw 1 Þm w 2 ðw 2 Þm w 3 ðw 3 Þ ¼ 0:6730 mðw 3 ¼ w 0 [ w 1 [ w 2 Þ ¼ ½m w 1 ðOÞm w 2 ðOÞ Á m w 3 ðw 3 Þ ¼ 0:0005 mðw 1 [ w 3 ¼ w 0 [ w 2 Þ ¼ ½m w 1 ðw 1 Þm w 2 ðOÞ Á m w 3 ðw 3 Þ ¼ 0:0118 mðw 2 [ w 3 ¼ w 0 [ w 1 Þ ¼ ½m w 1 ðOÞm w 2 ðw 2 Þ Á m w 3 ðw 3 Þ ¼ 0:0006 mðw 1 [ w 2 [ w 3 ¼ w 0 Þ ¼ ½m w 1 ðw 1 Þm w 2 ðw 2 Þ Á m w 3 ðw 3 Þ ¼ 0:0144</formula><p>The combination results have been normalized, since there is no conflicting beliefs to be redistributed. So we do not need to apply the normalization step using <ref type="bibr" target="#b17">(18)</ref> in this particular example. The result indicates that x i has got its biggest mass of belief on the metaclass w 1 [ w 2 . In the 5-nearest neighbors of this object, three of them belong to w 1 , whereas the other two nearest neighbors belong to w 2 . This implies that the object is very imprecise with respect to the classes w 1 and w 2 , and the information we used in the example is not sufficient for making a more specific (refined) classification of this object. So the meta-class w 1 [ w 2 can be a good (acceptable) compromise for the classification of this object, which can reduce the risk of the false classification. This solution is also consistent with our intuition. Such fusion result can be considered as a useful source of information to justify the need for other complementary information sources if a more specific decision is absolutely necessary for the problem under consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Expressive power of BK-NN</head><p>The expressive power of a method can be seen as its ability to identify and manipulate complex propositions. Let us examine and compare the expressivity power of BK-NN with respect to EK-NN and K-NN through the following simple example. Example 3. Let us consider the simple frame of classes O ¼ fw 0 ,w 1 ,w 2 g, and an object x i with 2-nearest neighbors respectively labeled by classes w 1 and w 2 with the corresponding bba's satisfying BK-NN bba modeling. The set of focal elements of m 1 ðÁÞ is F BK-NN 1 ¼ fw 1 ,w 1 ,Og and the set of focal elements of m 2 ðÁÞ is F BK-NN 2 ¼ fw 2 ,w 2 ,Og. Therefore, the set of focal elements resulting 7  from the combination rule ( <ref type="formula" target="#formula_20">17</ref>) is</p><formula xml:id="formula_26">F BK-NN 12 ¼ fw 1 ,w 2 ,w 1 ,w 2 ,w 1 [ w 2 ,w 1 [ w 2 ,Og</formula><p>So we see that BK-NN generates seven focal elements (specific classes and meta-classes). With EK-NN method, only three focal elements will be expressed because one would have F EK-NN 1 ¼ fw 1 ,Og and F EK-NN 1 ¼ fw 2 ,Og and therefore F EK-NN 12 ¼ fw 1 ,w 2 ,Og with DS rule. Clearly, BK-NN bba modeling has a better expressivity power than in EK-NN.</p><p>If we consider a larger frame, say O ¼ fw 0 ,w 1 ,w 2 ,w 3 g with three nearest neighbors with classes w 1 , w 2 , and w 3 , then we will get now a set of 15 focal elements</p><formula xml:id="formula_27">F BK-NN 123 ¼ fw 1 ,w 2 ,w 3 ,w 1 ,w 2 ,w 3 ,w 1 [ w 2 ,w 1 [ w 3 ,w 2 [ w 3 , w 1 [ w 2 [ w 3 ,w 1 [ w 2 ,w 1 [ w 3 ,w 2 [ w 3 ,w 1 [ w 2 [ w 3 ,Og</formula><p>As we can see, BK-NN can discriminate 15 elements (the specific and meta-classes) from a frame of discernment having threes classes, whereas K-NN discriminates only three classes w 1 , w 2 , w 3 , and EK-NN discriminates only four focal elements w 1 , w 2 , w 3 and O.</p><p>Clearly BK-NN working with hyper-credal classification is able to express more enlarged classifications than K-NN and EK-NN.</p><p>Let us consider a general n-classes data with the frame O ¼ fw 0 ,w 1 , . . . ,w n g ðn 43Þ, the cardinality of the core</p><formula xml:id="formula_28">F BK-NN 12...n is 2 n þ 1 À1 which indicates that BK-NN can produces 2 n þ 1 À1 classi- fications including n specific classes, 2 n</formula><p>Àn meta-classes and 2 n À1 rejection classes. Whereas K-NN produces only n classifications, and EK-NN provides n þ1 classifications. So BK-NN produces the most enlarged classifications (i.e. hyper-credal classification) and it has a better expressive power in general than K-NN and EK-NN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Applications of BK-NN</head><p>The effectiveness of BK-NN with respect to EK-NN and K-NN will be shown through three experiments in this section. The parameters of EK-NN were automatically optimized using the method introduced in <ref type="bibr" target="#b30">[31]</ref>.</p><p>For BK-NN, several heuristics have been tested to choose the tuning parameters, and it generally produces good results when acceptance and rejection thresholds are g ta ¼ 2 and g tr ¼ 5, which can be considered as default value. In the artificial numerical examples of Experiments 1 and 2, the default value of g ta and g tr are applied. In applications of the real data set as in Experiment 3, the cross validation method is used to select the best parameters. The parameter l s was determined by the average distances according to <ref type="bibr" target="#b12">(13)</ref>. Z ¼ 0:2 has been chosen as the default value for the selection of meta-classes, and the different value of Z is applied to show its influence on the results.</p><p>In order to show the ability of BK-NN to deal with imprecise information, the classification of the objects was done on the basis that the class should receive the maximal mass of belief.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment 1</head><p>Two particular tests have been done in this experiment to explicitly illustrate the difference between EK-NN, K-NN and BK-NN.</p><p>(1) Test 1: Two classes of artificial data sets w 1 and w 2 have been generated from the two 2D-Gaussian distributions N ðm 1 ,S 1 Þ and N ðm 2 ,S 2 Þ characterized by the following mean vectors and covariance matrices:</p><formula xml:id="formula_29">m 1 ¼ ð0,0Þ and S 1 ¼ 2 Â I m 2 ¼ ð5,0Þ and S 2 ¼ 2 Â I</formula><p>where I is the identity matrix of dimension 2 Â 2.</p><p>Each class has 50 training data points and 50 test points. We used K ¼7 nearest neighbors in this test. For notation conciseness, we have denoted w te 9w test , w tr 9w training and w i,...,k 9w i [ Á Á Á [ w k .</p><p>As we can see in Fig. <ref type="figure">4</ref>(a), the data sets of w 1 and w 2 are close to each other with partial overlap. K-NN and EK-NN just commit the objects to two specific classes as shown in Fig. <ref type="figure">4</ref>(b) and (c). BK-NN provides much more enlarged classifications as shown in Fig. <ref type="figure">4</ref>(d) than EK-NN and K-NN. With BK-NN, the seven objects labeled by black plus symbol in the middle of the two classes are committed to the meta-class w 1 [ w 2 , since the two classes are not very distinguishable for these objects. The meta-class w 1 [ w 2 is in fact a more suitable classification for these seven objects. Two objects quite far from its neighbors are reasonably considered as outliers w 0 ¼ w 1 [ w 2 labeled by black pentagram. One point labeled by a blue triangle is far from the class w 1 , but not close enough to its neighbors in w 2 . So this object is considered belonging to the rejection class w 1 . Similarly, one point far from w 2 but not close to w 1 is committed to w 2 labeled by red triangle. This example shows the expressive power of hyper-credal classification on which the BK-NN is based.</p><p>(2) Test 2: In this example, the data set is generated from three 2D Gaussian distributions characterizing the classes w 1 , w 2 and w 3 with the following mean vectors and covariance matrices (I being the 3 Â 3 identity matrix): In Fig. <ref type="figure">5</ref>, we can see that the three close data sets are classified into three distinct classes w 1 , w 2 or w 3 by K-NN and EK-NN. With BK-NN, few objects in the middle of different classes are committed to the meta-classes which is more reasonable. For example, six objects labeled by black plus symbol in middle of w 1 and w 2 are committed to w 1 [ w 2 , since the number of their KNNs in w 1 and w 2 is close. Similarly for the objects classified into w 1 [ w 3 and w 2 [ w 3 . Two points labeled by black pentagram are far from the neighbors, and are classified as outliers by BK-NN. This example shows how BK-NN works and its ability to deal with the meta-classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiment 2</head><p>In this second experiment, we compare the performances of BK-NN with respect to the performances of EK-NN and K-NN on the example given in <ref type="bibr" target="#b5">[6]</ref>. The data set is generated from three 3D Gaussian distributions characterizing the classes w 1 , w 2 and w 3 with the following mean vectors and covariance matrices (I being the 3 Â 3 identity matrix):  An object truly originated from w i generates an error of classification if it is classified into A when w i \ A ¼ |. If one gets w i \ A a| and Aa w i , then one has an imprecise classification. The error rate re is calculated by re ¼ N e =T, where N e is the number of objects wrongly classified, and T is the total number of the objects tested. The imprecision rate ri j is calculated by ri j ¼ N I j =T, where N I j is the number of objects committed to the meta-classes with cardinality j, and T is the total number of the objects tested.</p><p>As we can see in Fig. <ref type="figure" target="#fig_11">6</ref>, K-NN has the biggest error rate, and BK-NN and EK-NN provide better performances than K-NN since the distance information is taken into account in these methods. BK-NN produces the smallest error of classification, because some samples which are difficult to classify into a specific class are reasonably committed to the meta-classes or to rejection classes.</p><p>The error rate of BK-NN can decrease when Z varies from Z ¼ 0:15 (as shown in Fig. <ref type="figure" target="#fig_11">6(a),</ref><ref type="figure">(b</ref>)) to Z ¼ 0:2 (as shown in Fig. <ref type="figure" target="#fig_11">6(c),</ref><ref type="figure">(d))</ref>. For bigger values of Z we have observed a bigger imprecision rate and that is why Z value has been taken in ½0:15; 0:2 to present the best achievable performances obtained by BK-NN. In fact, there is always a compromise to find between the error and imprecision depending on the type of data classification application under concern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiment 3</head><p>In this third experiment, four well known real data sets (i.e. Iris data, Ecoli, Wine and Optical Recognition of Handwritten Digits ) from the UCI repository <ref type="bibr" target="#b11">[12]</ref> are used to evaluate the performance of BK-NN. For Ecoli data set, the three classes named as cp, im and imU are selected in the whole Ecoli data set, since these three classes are close and hard to classify. For Handwritten digits data set, we also choose three classes as 1, 4 and 7, since these three handwritten digits could be similar and difficult to correctly recognize in some cases.</p><p>The main characteristics of the four data sets are summarized in Table <ref type="table" target="#tab_1">1</ref>, and the detailed information can be found in <ref type="bibr" target="#b11">[12]</ref> (http://archive.ics.uci.edu/ml/).</p><p>The k-fold cross validation is performed on the Iris, Ecoli and Digits data sets by different classification methods. In general k remains an unfixed parameter <ref type="bibr" target="#b14">[15]</ref>. We use the simplest 2-fold cross validation here, since it has the advantage that the training and test sets are both large, and each sample is used for both training and testing on each fold. The samples in each classes are randomly assigned to two sets S 1 and S 2 , and the two sets have equal size. We then train on S 1 and test on S 2 , followed by training on S 2 and testing on S 1 . For Digits data set, the test data and training data are separated in UCI repository, and the test data are directly classified using the training data in this experiment. In the training data sets, we use the leave-one-out cross validation method to optimize the tuning parameters g ta and g tr in BK-NN. The best parameters corresponding to the lowest error rate will be found by the grid search with value of step length as 0.1, and the search scope is recommended as g ta A ½1,3 and g tr A ½2g ta ,3g ta .  The classification results of K-NN, EK-NN and BK-NN on different data sets with different values of K ranking from 5 to 15 are respectively shown in Tables <ref type="table">2</ref><ref type="table">3</ref><ref type="table">4</ref><ref type="table">5</ref>. It is worth to note the error rate re, and the imprecision rate ri j have the same meaning as in Experiment 2.</p><p>We can see in Tables 2-5 that BK-NN always provides the lowest error rate of classification since some samples hard to correctly classify are reasonably committed to the associated meta-classes. When Z varies from 0.2 to 0.25, the number of errors of classification decreases but at the price of an increase of the imprecision of the classification result which is normal. The Z parameter must always be tuned according to the imprecision degree that the user is ready to accept. This analysis reveals that the attributes used for the classification are in fact not sufficient for doing a correct specific classification of the objects in the meta-classes, and other complementary information sources will be necessary to get better specific results when absolutely required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>A new belief-based K-nearest neighbor (BK-NN) method has been developed for the classification of the close data sets, and it can reduce the misclassification error by decreasing a bit the specificity of the result. BK-NN is based on the construction of bba's using the acceptance and rejection thresholds related with the distance between the object and its neighbors. These bba's are combined by a new fusion rule which is a compromise between Dempster-Shafer rule and Dubois-Prade rule. The BK-NN offers an enhanced expressive power with respect to K-NN and EK-NN by allowing the hyper-credal classifications on specific classes, on the meta-classes and on the rejection classes as well. The output of BK-NN can be used as a useful mean to indicate when complementary information sources are needed to get more precise classification result in some particular applications, like those involving defense and security. Several experiments based on simulated and real data sets presented in this paper have shown that BK-NN provides a deeper insight into the data, and reduces the misclassification errors by introducing the rejection classes and meta-classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Several different samples of the handwritten digit 4 in the USPS data set. (a) Sample 1. (b) Sample 2. (c) Sample 3. (d) Sample 4.</figDesc><graphic coords="2,313.46,58.64,247.32,63.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ws ta and the given rejection threshold d ws tr with d ws ta o d ws tr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) If d ij r d ws ta , then f 1 ðd ij ,d ws ta Þ Z 0:5; (3) f 2 ðÁÞ should be monotone increasing, and f 2 ðÁÞ A ½0,1; (4) If d ij Z d ws tr , then f 2 ðd ij ,d ws tr Þ Z 0:5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Àl j ðd ij Àd ws tr Þ ð12Þ l=4 is the slope of the tangent at the inflection point. d ws ta and d ws tr are the abscissa of the inflection point of the sigmoid. In the determination of these tuning parameters, the average distance between each training data in w s and its KNNs can be calculated first, and the mean value d ws of all the average distances for the training data in w s can be used to determine d ws ta , d ws tr and l s . The bigger d ws usually leads to the smaller l s , and one suggests to take ta ,g tr Â d ws</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ws ta ,d ws tr is the imprecise zone of the class w s . The bigger gap between d ws ta and d ws tr will lead to more objects in the rejection classes. In fact, the value of d ws ta and d ws tr can be optimized using the training data, and it will be presented in sequel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Acceptance, rejection and imprecision zones for object.</figDesc><graphic coords="4,77.42,58.64,181.44,181.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>the calculation of the mass of belief committed to the focal elements A, A D O.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The flow chart of the BK-NN approach.</figDesc><graphic coords="6,47.47,493.46,241.20,236.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>m 1 ¼</head><label>1</label><figDesc>ð0,0Þ and S 1 ¼ 1:5 Â I m 2 ¼ ð4,0Þ and S 2 ¼ 1:5 Â I m 3 ¼ ð2,4Þ and S 3 ¼ 2 Â I Each class has 50 training data points and 50 test points. We used K¼ 9 nearest neighbors in this test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>m 1 ¼</head><label>1</label><figDesc>Fig.6(a)-(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Classification results by K-NN, EK-NN and BK-NN. (a) Training data and test data. (b) Classification result by K-NN. (c) Classification result by EK-NN. (d) Classification result by BK-NN. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Mean classification error and imprecision rates for different methods. Error and imprecision rates with (a) N ¼ 100, Z ¼ 0:15. (b) N ¼ 200, Z ¼ 0:15. (c) N ¼ 100, Z ¼ 0:2. (d) N ¼ 200, Z ¼ 0:2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1, . . . ,h (h being the number of the classes in the frame of discernment O) issued from the first step will then enter in the global fusion process. If DS rule is used for the global fusion of m ws ðÁÞ, s ¼ 1, . . . ,h, all the conflicting beliefs about the conjunction of different classes (e.g. m w i ðw i Þm w j ðw j Þ) will be redistributed. Consequently, the imprecise objects having KNNs with different classes label and having approximately the same number of the KNNs in each class will be mostly wrongly classified. To illustrate this problem, let us consider one object having 11-nearest neighbors with six neighbors in class w 1 and five neighbors in class w 2 . Such object is usually considered very difficult to classify into a one of the particular (precise) classes w 1 or w 2 . Intuitively, it is more reasonable and prudent to commit such object to the metaclasses w 1 [ w 2 representing the uncertainty one has, which can reduce the risk of false classification. The partial conflicting masses (e.g. m w i ðw i Þm w j ðw j Þ) capture the degree of the objects belonging to the corresponding meta-classes (e.g. w i [ w j ). If all the conflicting masses are simply kept and committed to the associated meta-classes as done by DP rule, it will cause very large number of meta-classes especially when K is big, and too many objects will be classified into the meta-classes, which is not an efficient data classification solution. For example, let us choose K ¼11. One object has 10 nearest neighbors in class w 1 and one neighbor in class w 2 . If the 11 neighbors are all close to the object according to the given acceptance threshold, this means that the class w 1 will get highest belief from the 10 bba's satisfying BK-NN bba modeling, whereas w 2 will get highest belief from the other bba. Then the meta-class w 1 [ w 2 will get the most belief after the combination of the 11 bba's by DP rule because of the existing partial conflict m w 1 ðw 1 Þm w 2 ðw 2 Þ. However, this object is more likely in w 1 than in w 2 , since there are 10 neighbors in w 1 , and only one neighbor belonging to w 2 . So it seems not very reasonable to commit this object into the meta-class w 1 [ w 2 as obtained when applying DP rule. That is why we propose a new global fusion rule which is a hybridization of DS and DP rules.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>The basic information of the used data sets.</figDesc><table><row><cell>Name</cell><cell>Classes</cell><cell>Attributes</cell><cell>Instances</cell></row><row><cell>Iris</cell><cell>3</cell><cell>4</cell><cell>150</cell></row><row><cell>Ecoli</cell><cell>3</cell><cell>7</cell><cell>255</cell></row><row><cell>Wine</cell><cell>3</cell><cell>13</cell><cell>178</cell></row><row><cell>Digits</cell><cell>3</cell><cell>64</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Which means that most of the mass of belief was committed to the ignorance in each bba's to combine.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>This rule is mathematically defined only if the denominator is strictly positive, i.e. the sources are not totally conflicting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Z.-g.Liu  et al. / Pattern Recognition 46 (2013) 834-844</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Of course, some other proper functions (like truncated trapezoidal functions by example) can also be selected (designed) by the users according to their actual applications.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The choice of the combination rule is not the main point of this paper. We prefer to focus on the general presentation of the method. Z.-g. Liu et al. / Pattern Recognition 46 (2013) 834-844</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>The cardinality of a set is the number of the elements it contains. For example, 9w i [ w j 9 ¼ 2. Z.-g. Liu et al. / Pattern Recognition 46 (2013) 834-844</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to the anonymous reviewers for all their remarks which helped us to clarify and improve the quality of this paper. This work has been partially supported by China Natural Science Foundation (No. 61075029, 61135001) and PhD Thesis Innovation Fund from Northwestern Polytechnical University (No.cx201015).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">3</ref> The classification results of different methods for Ecoli data (in %).  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Machine Recognition of Patterns</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Agrawala</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CECM: constrained evidential C-Means algorithm</title>
		<author>
			<persName><forename type="first">V</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Quost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="894" to="914" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Edge detection in color images based on DSmT</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dezert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fusion 2011 International Conference</title>
		<meeting>Fusion 2011 International Conference<address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation from uncertain data in the belief function framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2011.201</idno>
		<ptr target="http://www.hds.utc.fr/tdenoeux/dokuwiki/doku.php?id=en:publi:belief_artS" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A k-nearest neighbor classification rule based on Dempster-Shafer Theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="804" to="813" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classification using belief functions: relationship between case-based and model-based approaches</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics-Part B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1395" to="1406" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">EVCLUS: EVidential CLUStering of proximity data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Masson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics-Part B</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="109" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evidential reasoning in large partially ordered sets. Application to multi-label classification, ensemble clustering and preference aggregation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Masson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="161" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representation and combination of uncertainty with belief functions and possibility measures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="244" to="264" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The distance-weighted k-nearest-neighbor rule</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Dudani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics SMC</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="325" to="327" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Irvine, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>UCI Machine Learning Repository, University of California, School of Information and Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonparametric Discrimination: Consistency Properties</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hodges</surname></persName>
		</author>
		<idno>Project number 21-49-004</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Recognition of Patterns</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Agrawala</surname></persName>
		</editor>
		<meeting><address><addrLine>Randolph Field, TX, USA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1951">1951. 1977</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="261" to="279" />
		</imprint>
		<respStmt>
			<orgName>USAF School of Aviation Medicine</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Discriminatory Analysis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonparametric Discrimination: Small Sample Performance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hodges</surname></persName>
		</author>
		<idno>Project number 21-49-004</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Recognition of Patterns</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Agrawala</surname></persName>
		</editor>
		<meeting><address><addrLine>Randolph Field, TX, USA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1952">1952. 1977</date>
			<biblScope unit="page" from="280" to="322" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Discriminatory Analysis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Predictive Inference: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geisser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman and Hall</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evidential multi-label classification using the random k-label sets approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2nd International Conference on Belief Functions</title>
		<meeting>The 2nd International Conference on Belief Functions<address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combination of supervised and unsupervised classification using the theory of belief functions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Karem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2nd International Conference on Belief Functions</title>
		<meeting>The 2nd International Conference on Belief Functions<address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Belief c-means: an extension of fuzzy c-means algorithm in belief functions framework</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dezert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="291" to="300" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combination of sources of evidence with different discounting factors based on a new dissimilarity measure</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dezert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="133" to="141" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An evidential pattern matching approach for vehicle identification</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Jousselme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maupin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Belief Functions</title>
		<meeting>the 2nd International Conference on Belief Functions<address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ECM: an evidential version of the fuzzy c-means algorithm</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1384" to="1397" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clustering interval-valued data using belief functions</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="171" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble clustering in the belief functions framework</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="109" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classifier fusion in the Dempster-Shafer framework using optimized t-norm based combination rules</title>
		<author>
			<persName><forename type="first">B</forename><surname>Quost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="374" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Shafer</surname></persName>
		</author>
		<title level="m">A Mathematical Theory of Evidence</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">an important contribution to nonparametric discriminant analysis and density estimation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silveman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hodges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="233" to="238" />
			<date type="published" when="1951-12-03">1951. 3 December. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="http://www.gallup.unm.edu/$smarandache/DSmT-book3.pdfS" />
		<title level="m">Advances and Applications of DSmT for Information Fusion</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Smarandache</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Dezert</surname></persName>
		</editor>
		<meeting><address><addrLine>Rehoboth</addrLine></address></meeting>
		<imprint>
			<publisher>American Research Press</publisher>
			<date type="published" when="2004">2004-2009</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Smets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kennes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Transferable Belief Model</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="191" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The combination of evidence in the Transferable Belief Model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="447" to="458" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An evidence-theoretic k-NN rule with parameter optimization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zouhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics-Part C</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="271" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">He received the Bachelor and Master degree from Northwestern Polytechnical University in 2007 and 2010. He started the Ph.D course in Northwestern Polytechnical University since September 2009, and he has studied in Telecom Bretagne since August 2010 for one year</title>
	</analytic>
	<monogr>
		<title level="m">His research work focus on belief functions theory and its application in data classification</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
	<note>Zhun-ga Liu was born in China</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Quan Pan was born in China 1961. He received the Bachelor degree in Huazhong University of Science and Technology, and he received the master and doctor degree in Northwestern Polytechnical University (NWPU) in 1991 and 1997. He has been professor since 1998 in NWPU. His current research interests are data classification and information fusion</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Since 1993, he is senior research scientist in the Information Modeling and Processing Department (DTIM) at ONERA. His current research interest focus on belief functions theory and its applications, especially for DSmT which has been developed by him and Prof</title>
		<imprint>
			<date type="published" when="1962">1962. 1990</date>
			<publisher>Smarandache</publisher>
			<pubPlace>Paris</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University Paris VII and his Ph.D from the University Paris XI, Orsay</orgName>
		</respStmt>
	</monogr>
	<note>He received the electrical engineering degree from Ecole Franc -aise de Radioe ´lectricite ´Electronique and Informatique (EFREI)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
