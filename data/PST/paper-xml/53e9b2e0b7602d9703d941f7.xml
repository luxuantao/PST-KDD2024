<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Worst-case and Smoothed Analysis of the ICP Algorithm, with an Application to the k-means Method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Arthur</surname></persName>
							<email>darthur@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
							<email>sergei@cs.stanford.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Worst-case and Smoothed Analysis of the ICP Algorithm, with an Application to the k-means Method</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF0B44A2DC2288D861975C5A3614BFD9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show a worst-case lower bound and a smoothed upper bound on the number of iterations performed by the Iterative Closest Point (ICP) algorithm. First proposed by Besl and McKay, the algorithm is widely used in computational geometry where it is known for its simplicity and its observed speed. The theoretical study of ICP was initiated by Ezra, Sharir and Efrat, who bounded its worst-case running time between Ω(n log n) and O(n 2 d) d . We substantially tighten this gap by improving the lower bound to Ω(n/d) d+1 . To help reconcile this bound with the algorithm's observed speed, we also show the smoothed complexity of ICP is polynomial, independent of the dimensionality of the data. Using similar methods, we improve the best known smoothed upper bound for the popular k-means method to n O(k) , once again independent of the dimension.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What can be said when an algorithm is known to be fast in practice, but slow in the worst case? The smoothed analysis of Spielman and Teng <ref type="bibr" target="#b16">[17]</ref> helps bridge this gap by considering the expected running time after first randomly perturbing the input. Intuitively, this models how fragile the bad cases are, and whether they could reasonably arise in practice. Smoothed analysis of algorithms remains a very challenging task, but following the seminal work of Spielman and Teng on the complexity of the Simplex method <ref type="bibr" target="#b16">[17]</ref>, there have been several recent successes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>We are interested in smoothed analysis within the context of iterative geometric algorithms. The smoothness assumption is especially plausible in geometry, where many algorithms already explicitly assume points are in "general position," i.e. no three points are collinear, no four are coplanar, etc. We consider two local search heuristics, the Iterative Closest Point (ICP) algorithm and the k-means method. The two techniques are similar -both work to minimize a particular potential function, and both terminate with a locally optimal (but potentially globally suboptimal) solution. While neither algorithm offers any approximation guarantees, they are known for their speed, and both are widely used in practice <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Our primary focus is on ICP, for which we prove an exponential lower bound on the number of iterations required, and a smoothed polynomial upper bound that is independent of dimension. We also consider k-means, for which we have previously shown a superpolynomial lower bound <ref type="bibr" target="#b1">[2]</ref>. Our techniques from this work apply to k-means as well, and we prove a smoothed upper bound of n O(k) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">The ICP algorithm</head><p>An important problem in computational geometry is comparing patterns and shapes that may be viewed with different coordinates. Applications include object recognition and image alignment -for example, arranging overlapping satellite images. Typically, the problem reduces to translating (and possibly rotating or scaling) a given point set A until it is as close as possible to another given point set B. One standard metric for this comparison is the average squared distance from each point in A to its closest point in B.</p><p>Besl and McKay <ref type="bibr" target="#b5">[6]</ref> proposed ICP as a local search solution to this problem. It is simple but generally fast, which has made it a standard tool in practice <ref type="bibr" target="#b14">[15]</ref>. ICP has several variants, but following <ref type="bibr" target="#b6">[7]</ref>, we focus on one that allows only translations of A. For each point a ∈ A, the algorithm begins by computing the point's nearest neighbor N B (a) in B. The algorithm then fixes N B and translates A so as to minimize φ = a a -N B (a) 2 . These two steps are repeated until the nearest neighbor assignment stabilizes. Each step decreases φ and there are only a finite number of possible nearest neighbor assignments (namely |B| |A| ), so ICP is guaranteed to eventually terminate.</p><p>The algorithm's efficiency has been well studied empirically <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, but the theoretical running time has proven more elusive. Recently, Ezra, Sharir, and Efrat <ref type="bibr" target="#b6">[7]</ref> achieved a worst-case lower bound of Ω(n log n) iterations in one dimension, and an upper bound of O(n 2d ) iterations in d dimensions (where n = |A| + |B|). Our first result is an an exponential lower bound that tightens this gap considerably.</p><p>Theorem 1.1 (ICP lower bound). There exist point sets A and B in R d for which the ICP algorithm requires Ω(n/d) d+1 iterations.</p><p>This lower-bound construction relies on a precise placement of points in order to achieve exponential growth. Such situations are unlikely to arise in practice, and we use smoothed analysis to help show this formally.</p><p>Theorem 1.2 (Smoothed ICP complexity). Suppose A and B are chosen according to independent d-dimensional normal distributions with variance σ 2 , and let D denote the maximum diameter of A and B. Then the expected running time of ICP on (A, B) is polynomial in d, n and D σ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">The k-means method</head><p>The k-means method is a well known geometric clustering algorithm based on work by Lloyd <ref type="bibr" target="#b13">[14]</ref> in 1982. Its practical importance and widespread use are difficult to overstate. As one survey of data mining techniques points out, "k-means is by far the most popular clustering algorithm used in scientific and industrial applications" <ref type="bibr" target="#b4">[5]</ref>.</p><p>Given a set of n data points, k-means uses a local search approach to partition the points into k clusters. A set of k initial cluster centers is chosen arbitrarily. Each point is then assigned to the center closest to it, and the centers are recomputed as centers of mass of their assigned points. This is repeated until the process stabilizes. As with ICP, each iteration decreases a potential function, and so the algorithm is guaranteed to eventually terminate.</p><p>Again, the efficiency of k-means is well studied empirically <ref type="bibr" target="#b11">[12]</ref>, but there has been less theoretical progress. The best known upper bound is O(n kd ) <ref type="bibr" target="#b10">[11]</ref>, and it has now remained intact for over a decade. Earlier this year, however, we improved the best known lower bound from Ω(n) <ref type="bibr" target="#b9">[10]</ref> to 2 Ω( √ n) <ref type="bibr" target="#b1">[2]</ref>. Our methods for ICP carry over surprisingly well to the case of k-means, and we show a smoothed upper bound that substantially improves on the O(n kd ) result.</p><p>Theorem 1.3 (Smoothed k-means complexity). Suppose a set of n points X is chosen according to independent ddimensional normal distributions with variance σ 2 , and let D denote the diameter of the resulting point set. Then the expected running time of k-means on X is polynomial in d, n k and D σ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries (ICP)</head><p>Given two point sets A and B in R d , the ICP algorithm seeks a translation x ∈ R d and a nearest neighbor function N B : A → B such that the potential,</p><formula xml:id="formula_0">φ = a∈A a + x -N B (a) 2 is minimized.</formula><p>When x is fixed, φ is minimized by choosing N B (a) to be the point in B closest to a + x. Conversely, suppose N B (a) is fixed for all a. Recall the following result from linear algebra (see <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>).</p><p>Lemma 2.1. Let S be a set of points with center of mass c(S), and let x be an arbitrary point. Then,</p><formula xml:id="formula_1">s∈S s -x 2 = s∈S s -c(S) 2 + |S| • c(S) -x 2 . If we take S = {N B (a) -a}, Lemma 2.1 implies φ is minimized by choosing x = P a∈A N B (a)-a |A|</formula><p>. ICP is a local search algorithm based on these two observations. Formally, it works as follows.</p><p>1. Choose an arbitrary starting translation x. 2. For each a ∈ A, set N B (a) to be the point in B that is closest to a + x (ties can be broken arbitrarily, as long as the method is consistent). There are only finitely many choices for N B (namely |B| |A| ), and as discussed above, each step can only decrease the potential function φ. Thus, the algorithm must terminate after a finite number of steps. Formally, we define an "iteration" to be an execution of Step 2 followed by Step 3. We often consider the first iteration separately because of its arbitrary starting translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Set the next translation x =</head><p>We are interested in analyzing how many iterations ICP requires before it converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Lower bounds for ICP</head><p>In this section, we prove that ICP can require up to Ω(n/d) d+1 iterations in d dimensions. This resolves the worst-case running time of ICP in one dimension, which Ezra, Sharir, and Efrat refer to as a "major open problem" <ref type="bibr" target="#b6">[7]</ref>. It also gives an exponential lower bound for the general problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lower-bound preliminaries</head><p>ICP k : The ICP algorithm explicitly uses the number of points in A each time it calculates the translation x. This dependence is inconvenient, and we remove it via a modest generalization of ICP that we call ICP k . Our generalization is identical to ICP, except that we first fix some k ≥ |A|, and then in Step 3, we choose</p><formula xml:id="formula_2">x = a∈A N B (a) -a k .</formula><p>Note that standard ICP is precisely ICP |A| .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regions:</head><p>Consider a partitioning of point sets</p><formula xml:id="formula_3">A = A 1 ∪ A 2 ∪ • • • ∪ A r and B = B 1 ∪ B 2 ∪ • • • ∪ B r . Let δ = max i max p,q∈{Ai,Bi} p -q .</formula><p>We say (A 1 , B 1 ), (A 2 , B 2 ), . . . , (A r , B r ) is a regiondecomposition of (A, B) if the distance between any two points in distinct regions is greater than 3δ. Lemma 3.1. Suppose (A, B) has region-decomposition (A i , B i ), and suppose ICP k is executed on (A, B) with initial translation 0. If a ∈ A i , then N B (a) remains in the corresponding B i throughout the execution of ICP k .</p><p>Proof. Consider an iteration beginning with translation x where x ≤ δ. For a ∈ A i , b ∈ B i and b ∈ B j (j = i), we have,</p><formula xml:id="formula_4">a + x -b ≤ a -b + x ≤ 2δ &lt; a -b -x ≤ a + x -b ,</formula><p>and hence N B (a) will remain in the corresponding B i during this iteration. Conversely, as long as this condition on N B holds, the next translation x will satisfy x ≤ δ.</p><p>As long as the nearest neighbor assignments N B never cross regions, uniformly translating an entire region does not alter the execution of ICP k in any way. Therefore, it suffices to consider each region in isolation, and to ignore its exact position within (A, B). In fact, we will think of each region as being a smaller ICP configuration, impacted by other regions only via the global translation.</p><p>Finally, we note that any set of ICP configurations can be translated and combined so as to become regions of a larger ICP configuration. Remark 3.1. Given ICP configurations (A 1 , B 1 ), (A 2 , B 2 ), . . . , (A r , B r ), there exists some ICP configuration (A, B) with region-decomposition (A 1 , B 1 ), (A 2 , B 2 ), . . . , (A r , B r ).</p><p>Relative translation: We noted above that different regions can only impact each other via the global translation x. However, a simple observation of Ezra, Sharir and Efrat <ref type="bibr" target="#b6">[7]</ref> shows that even this effect is limited. Remark 3.2. Consider some iteration of ICP k after the first. Let x and x denote the translations before and after this iteration, and let N B and N B denote the nearest neighbor functions before x and x were calculated. Then,</p><formula xml:id="formula_5">x -x = 1 k a N B (a) -N B (a).</formula><p>We will always be interested in the case where only one region changes nearest neighbor assignments during each iteration. In this case, Remark 3.2 implies the change in global translation can be understood by simply restricting to the one unstable region during each iteration, and ignoring the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Building Blocks</head><p>We now describe two widgets that will serve as building blocks for our main lower-bound construction. We will always place each widget in its own region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The Linear Shifter</head><p>The Linear Shifter is a one-dimensional configuration of points (A, B) upon which ICP k can require |B| iterations to terminate even when |A| = 1.</p><p>Define B = {b 0 , b 1 , . . . , b m } by setting b 0 = 0, and Proof. Suppose that during some iteration we compute a translation of x = b i from a nearest neighbor assignment of N B (a) = b i-1 . (This is the case initially with i = 1). We will then update N B (a) to b i , and by Remark 3.2, this will increase x by bi-bi-1 k to b i+1 . The result follows.</p><formula xml:id="formula_6">b i = 1 + 1 k + • • • + 1 k i-1 for i &gt; 0. Let A = {a = 0},</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">The Redirector</head><p>The Redirector is a widget (A, B) that triggers a large shift v of our choosing once the global translation x exceeds some threshold y.</p><p>It has two regions  We say the Redirector "triggers" when x • v increases to be above y • v, and it "untriggers" when x • v decreases to be below y • v. Thus, the Redirector causes an extra shift of v on an iteration in which it triggers, but it causes no extra shift while it remains fixed in one state.</p><formula xml:id="formula_7">A 1 = {a = 1 2 kv -y} and B 1 = {b 1 = 0, b 2 = kv},</formula><formula xml:id="formula_8">that N B (a) is set to b 1 if x • v &lt; y • v,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ω(n 2 ) lower bound in R</head><p>We now proceed with the Ω(n 2 ) lower-bound construction for ICP on the line, which will serve as a base case for our general lower bound. Combined with the known O(n 2 ) upper bound, this fully resolves the worst-case running time of ICP in one dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.4. There exist point sets</head><formula xml:id="formula_9">A, B ⊂ R with |A|, |B| = O(n) for which the ICP algorithm requires Ω(n 2 ) iterations.</formula><p>Instead of proving this result directly for ICP, we prove it holds for ICP k for all k. The number of points in A will not depend on k, so we can choose k = |A| to recover the theorem.</p><p>We construct A and B as follows.</p><p>1. Begin with a region (A, B) consisting of the Linear Shifter with n points described in Section 3.2.</p><formula xml:id="formula_10">1. Let = 1 + 1 k + • • • + 1</formula><p>k n denote the final position of a ∈ A after passing through this shifter. 2. Augment A in the region above, setting it to be {a 0 , a 1 , . . . , a n } where a i = -2i . 3. For each i ∈ {0, 1, . . . , n-1}, add a Redirector region, so that once the total translation is at least (2i + 1) , there is a further shift of + 1 (i.e., use a Redirector with y = (2i + 1) and v = + 1). 4. Suppose ICP k is run on the above configuration with initial translation 0. Let x 1 denote the translation after one iteration. Add a region ({a</p><formula xml:id="formula_11">}, {b }) with b = a + k(1 -x 1 ).</formula><p>Now, suppose we run ICP k on this configuration with initial translation 0. Each Redirector begins untriggered, and initially, N B (a i ) = b 0 for all i. Since N B (a ) = b , Step 4 ensures the translation after the first iteration is 1.</p><p>Every point in A is now stable except for a 0 , and Lemma 3.2 implies that the next n iterations will be taken by a 0 stepping through the Linear Shifter. This will eventually result in a total translation of , which will cause the first Redirector in Step 3 to trigger. This, in turn, will lead to a total translation of 2 + 1, which will force a 1 to pass through the Linear Shifter over another n iterations. This process will repeat n + 1 times, once for each a i , and the Ω(n 2 ) lower bound follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ω(n/d) d+1 lower bound in R d</head><p>We now inductively build upon our Ω(n 2 ) lower bound on the line to prove an exponential lower bound for higher dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.5. There exist point sets</head><formula xml:id="formula_12">A, B ⊂ R d with |A|, |B| = O(n) for which the ICP algorithm requires Ω(n/d) d+1 iterations.</formula><p>This result substantially tightens the gap between the previous bounds of Ω(n log n) and O(n 2 d) d <ref type="bibr" target="#b6">[7]</ref>.</p><p>We prove Theorem 3.5 inductively, once again focusing on ICP k . Suppose we are given A, B ⊂ R d-1 and an initial translation x 0 for which ICP k requires T iterations. Letting x i denote the translation after i iterations, we suppose there exists some vector v ∈ R d-1 for which x i • v &lt; x T • v for all i &lt; T . We show that such a configuration can be lifted to R d and augmented with O(n/d) points to obtain a configuration that now requires T n/d iterations. The theorem follows from applying this augmentation repeatedly to the one-dimensional Ω(n 2 ) construction.</p><p>For notation, we refer to R d-1 × {0} as the "base space" and to {0, 0, . . . , 0} × R as the "lift dimension". We also use p ∈ R d to denote a point in R d that projects down to p ∈ R d-1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Resetting overview</head><p>As above, suppose we are given A, B ⊂ R d-1 , along with an initial translation x 0 , for which ICP k requires T iterations. We begin by embedding this entire ICP configuration into R d with lift coordinate 0. Clearly, ICP k will still require T iterations for this lifted configuration (A, B). We now wish to increase this running time by adding a few points.</p><p>Fix constants H and H , and suppose we could add a "Reset Widget" region that contributes the following translation at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translation Iteration</head><p>Base space Lift dimension &lt; T + 1 (0, 0, . . . , 0) 0 T + 1</p><p>x 0 -x T H &gt; T + 1 (0, 0, . . . , 0) H</p><p>This new region will have no impact on ICP k as it runs to completion on (A, B) over the first T iterations. Therefore, at the beginning of iteration T + 1, we know (A, B) will be contributing a translation of (x T , 0). Combining that with the translation from the new Reset Widget, we have a total translation x T +1 = (x 0 , H).</p><p>Ignoring the lift coordinate, this translation x T +1 is identical to the starting translation x 0 . Since every point in (A, B) has a lift coordinate of 0, the translations x T +1 and x 0 result in the same nearest neighbor assignments for (A, B). Therefore, (A, B) will contribute a translation of precisely (x 1 , 0) at iteration T + 2. Combining this with the Reset Widget, we then have a total translation of x T +2 = (x 1 , H ).</p><p>From this point on, it is easy to check that the translation x T +1+i is always equal to (x i , H ), and so the augmented ICP configuration now requires precisely 2T + 1 iterations.</p><p>Therefore, if we can construct a Reset Widget region with the above properties, we will be able to "reset" ICP k and double the number of iterations it requires. This is the key mechanism that will allow us to prove Theorem 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">The Reset Widget</head><p>We now describe a Reset Widget that accomplishes the task set forth above (except that it requires two iterations to achieve the x 0 -x T shift).</p><p>First fix a large constant height H, and recall our initial assumption on (A, B): there exists v such that</p><formula xml:id="formula_13">x i •v &lt; x T •v for all i &lt; T .</formula><p>1. Add a Redirector with redirection vector (v, H) and with triggering translation y = (x T , 0).</p><p>By our initial assumption, we know (x i , 0) • (v, H) &lt; y • (v, H) for all i &lt; T , so this Redirector will first trigger during iteration T + 1. Now consider an execution of ICP k with this Redirector added. After the Redirector triggers, the nearest neighbors for (A, B) could change, and this could result in another shift during iteration T + 2. We focus on x T +2 , the basespace translation after iteration T + 2. (If there are no iterations after the Redirector triggers, take x T +2 = x T +1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Add a Redirector with redirection vector (x 0 -</head><p>x T +2 , H) and with triggering translation y = (x T + v, H).</p><formula xml:id="formula_14">If H is sufficiently large, (x i , 0)•(x 0 -x T +2 , H) &lt; y•(x 0 - x T +2 , H</formula><p>) for all i ≤ T . Therefore, this Redirector will first trigger during iteration T + 2. Without it, the translation after t + 2 iterations is precisely (x T +2 , H). However, the Redirector causes a further shift of (x 0 -x T +2 , H), which resets the translation to (x 0 , 2H) as desired.</p><p>It remains to reset the base-space translation due to the Reset Widget back to 0.</p><p>3. Add a Redirector with redirection vector (x T +2 -x 0v, H) and with triggering translation y = (x 0 , 2H).</p><p>As above, this Redirector will first trigger during iteration T +3 as long as H is sufficiently large. The total translation due to the Reset Widget is then</p><formula xml:id="formula_15">(v, H) + (x 0 -x T +2 , H) + (x T +2 -x 0 -v, H) = (0, 3H), as required.</formula><p>As long as all three Redirectors remain triggered, the Reset Widget will continue to contribute (0, 3H) to the global translation, and, as discussed in the previous section, ICP k will then require another T iterations. Since the Redirectors will never untrigger if H is sufficiently large, we now have a working Reset Widget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Putting it together</head><p>We prove Theorem 3.5 by stringing together a number of Reset Widgets, but two details remain unresolved.</p><p>First of all, recall that an ICP configuration can only be augmented with a Reset Widget if there exists v such that x i • v &lt; x T • v for all i &lt; T . This holds for the onedimensional base case with v = 1. Moreover, augmenting an ICP configuration with a Reset Widget does not prevent one from augmenting it again: Remark 3.3. Consider (A, B) for which ICP k takes T iterations, and suppose there exists v such that</p><formula xml:id="formula_16">x i • v &lt; x T • v for all i &lt; T .</formula><p>Lift this configuration and augment it with a Reset Widget as described above. Let x T denote the final translation when ICP k is run on the augmented configuration. Then,</p><formula xml:id="formula_17">x i • (v, H) &lt; x T • (v, H) for all i &lt; T .</formula><p>Therefore, we can add a Reset Widget in each dimension to obtain a lower bound of Ω(n 2 ) • 2 d .</p><p>To achieve the stronger bound in Theorem 3.5, we must add Ω(n/d) Reset Widgets in each dimension: Remark 3.4. Consider (A, B) for which ICP k takes T iterations, and suppose there exists v such that</p><formula xml:id="formula_18">x i • v &lt; x T • v for all i &lt; T . Also fix m ≥ 1.</formula><p>Lifting only one dimension, we can add m Reset Widgets to (A, B) that trigger in sequence, each resetting (A, B) (but not each other). Then, ICP k will require at least mT iterations on this augmented configuration.</p><p>We achieve this by adding m Reset Widgets as described above, but then modifying Reset Widget i so that it triggers with lift coordinate 3Hi rather than with lift coordinate 0. It is straightforward to check this configuration has the desired properties, and Theorem 3.5 now follows immediately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A smoothed upper bound for ICP</head><p>Despite the exponential lower bound given by Theorem 3.5, we know ICP usually runs fast in practice. To help explain this, we prove a smoothed polynomial upper bound.</p><p>We assume the points in A and B are chosen from independent d-dimensional normal distributions with variance σ 2 . If A and B have diameter at most D, we then show ICP will run in expected time polynomial in |A|, |B|, d and D σ . Our argument is based on an analysis of the potential function φ = a∈A a + x -N B (a) 2 . We show that with high probability, every ICP iteration after the first will substantially decrease φ. Our result then follows from the fact that after one iteration, φ ≤ |A|(2D) 2 .</p><p>On the one hand, if N B changes value for only a few points during a single iteration, we show that x is likely to change significantly. This causes a corresponding drop in the potential during Step 3 of the algorithm (see Lemma 2.1).</p><p>Conversely, if N B changes value for a larger number of points during a single iteration, we show there must be some a for which N B (a) becomes substantially closer to a + x. We do this by arguing there is no translation x for which many points a+x are almost equidistant between two points in B. In this case, we obtain a large potential drop during Step 2 of the algorithm.</p><p>The main property of the normal distribution we use is that it is not concentrated in any ball. Lemma 4.1. Suppose y is chosen according to a ddimensional normal distribution with variance σ 2 . Then, y is in a fixed ball of radius with probability at most σ d .</p><p>Proof. The probability distribution function for y has maximum value 1 ( √ 2πσ) d . Furthermore, a ball with radius is contained in a hypercube with side length 2 , so the probability y is in such a ball is at most (2 ) </p><formula xml:id="formula_19">d ( √ 2πσ) d &lt; σ d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Case 1: Small changes in N B</head><p>Fix a constant k. We begin by analyzing the case where N B changes value for at most k points. In this case, we show φ remains constant only if two size-k subsets of B have equal centers of mass.</p><formula xml:id="formula_20">Definition 4.1. We say B is (k, δ)-sparse if no pair of dis- tinct size-k multisets B 1 , B 2 ⊂ B satisfies b1∈B1 b 1 - b2∈B2 b 2 ≤ δ.</formula><p>We will later use the same property with k = 1, which we denote as simply δ-sparse.</p><p>We first show that if B is (k, δ)-sparse, then any small change in N B results in a significant potential drop. We conclude by showing that B is likely to be (k, δ)-sparse for k = O(d). Our potential analysis in this case only depends on Step 3 of the algorithm (see Section 2). Proof. Let N 1 and N 2 denote the multiset N B (A) before and after the ICP iteration. Also let </p><formula xml:id="formula_21">N 0 = N 1 ∩ N 2 , B 1 = N 1 -N 2 and B 2 = N 2 -N 1 , so that N 1 = N 0 ∪ B 1 and N 2 = N 0 ∪ B 2 . If N B (A)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Case 2: Large changes in N B</head><p>We now analyze the case where N B changes value for at least k points.</p><p>We show that not too many points in A can be equidistant from two points in B, even after an arbitrary translation. This ensures that large changes in N B lead to a substantial improvement in N B (a) for some a, and to a corresponding potential drop in Step 2 of the algorithm.</p><p>First fix ordered sets Y ⊂ A and Z, Z ⊂ B of size k. We aim to show there is no translation x for which every translated y ∈ Y is within 2 of the hyperplane bisecting the corresponding z ∈ Z and z ∈ Z . Definition 4.2. Given points y, z, z ∈ R d , we say y iscentered between z and z if y is within a distance 2 of the hyperplane bisecting z and z .</p><p>Letting v denote the unit vector in the direction of z -z, we can also phrase this definition in terms of linear algebra. Specifically, y is -centered between z and z if and only if y</p><formula xml:id="formula_22">• v ∈ z+z 2 • v ± 2 .</formula><p>Using this formulation, we can bound the translation x in the case where y +x is -centered between z and z . Remark 4.1. y + x is -centered between z and z if and</p><formula xml:id="formula_23">only if x • v ∈ z+z 2 -y • v ± 2 .</formula><p>Definition 4.3. Let Y = {y 1 , y 2 , . . . , y k }, Z = {z 1 , z 2 , . . . , z k }, and Z = {z 1 , z 2 , . . . , z k } be ordered point sets in R d . We say (Y, Z, Z ) is -centerable if there is a single translation x for which y j +x is -centered between z j and z j for all j. We can now state the conditions required on A and B to guarantee a certain potential drop. Proposition 4.4. Let A and B be point sets in R d . Also, suppose B is δ-sparse and (A, B) is not (k, )-centerable. Consider any ICP iteration after the first on (A, B). If N B changes its value for at least k points during this iteration, it results in a potential drop of at least δ.</p><p>Proof. Suppose a subset Y = {y 1 , y 2 , . . . , y k } of A changes nearest neighbors in one iteration of ICP beginning with translation x. Let Z = {z 1 , z 2 , . . . , z k } and Z = {z 1 , z 2 , . . . , z k } denote the original and new nearest neighbors for each y j . Since (A, B) is not (k, )-centerable, there exists j such that y j + x is not -centered between z j and z j . Now, z j -(y j +x) 2 -z j -(y j +x) 2 = (2(y j +x)z j -z j ) • (z j -z j ). Since y j + x is not -centered between z j and z j , we know 2(y j + x) -z j -z j has magnitude at least in the z j -z j direction. Since B is δ-sparse, we also know that z j -z j ≥ δ, and hence, z j -(y j + x) 2z j -(y j + x) 2 ≥ δ. Thus, after the nearest neighbor recomputation, the potential arising from y j + x -N B (y j ) 2 has decreased by at least δ.</p><p>We have already bounded the probability that B is δsparse. It remains to show that (A, B) is unlikely to be (k, )-centerable. For this part of the analysis, we will assume B is fixed, and we consider only the randomness inherent in A. We begin with a technical lemma. Proof. Decreasing d if necessary, we may assume without loss of generality that Span(V) = R d . Let H(X) denote the hyperplane passing through the points X ∪{0} and let S(X) denote the simplex with vertices X ∪ {0}. We choose V 0 so as to maximize the volume of S(V 0 ). Note this ensures Span(V 0 ) = Span(V). Given any v ∈ V, we may therefore write v = u∈V0 c u u for some scalars c u . It remains to show each |c u | ≤ 1.</p><p>Towards that end, consider u 0 ∈ V 0 . Let X denote V 0 -{u 0 }, and let V 1 = X ∪ {v}. By assumption, we know the volume of S(V 1 ) is at most the volume of S(V 0 ). Since both simplices share the face S(X), this implies the distance from v to H(X) is at most the distance from u 0 to H(X). Letting x denote a vector orthogonal to H(X), we therefore have,</p><formula xml:id="formula_24">x • c u u = |x • v| ≤ |x • u 0 |.</formula><p>However, x is orthogonal to u for u = u 0 so this implies </p><formula xml:id="formula_25">|x • (c u0 u 0 )| ≤ |x • u 0 |,</formula><formula xml:id="formula_26">. If k ≥ d, then (Y, Z, Z ) is -centerable with probability at most (d+1) σ k-d . Proof. Let V = {v 1 , v 2 , . . . , v k }</formula><p>, where v j denotes the unit vector in the direction z j -z j . Without loss of generality, we may assume that V 0 = {v 1 , v 2 , . . . , v d } satisfies the condition given in Lemma 4.5.</p><p>We now prove the result by induction on k. When k = d, it is trivial. Now suppose the result holds for k -1. Let Y 0 = {y 1 , y 2 , . . . , y k-1 } and Y 1 = {y 1 , y 2 , . . . , y d }∪{y k }. Define Z 0 , Z 1 , Z 0 , and Z 1 similarly. Note that (Y, Z, Z ) is -centerable only if both (Y 0 , Z 0 , Z 0 ) and (Y 1 , Z 1 , Z 1 ) are also -centerable. By our inductive hypothesis, we know the former is -centerable with probability at most</p><formula xml:id="formula_27">(d+1) σ k-1-d</formula><p>, and this is independent of y k . We will</p><formula xml:id="formula_28">show that even if Y 0 is fixed, then (Y 1 , Z 1 , Z 1 ) is - centerable with probability at most (d+1) σ</formula><p>. The proposition then follows from independence.</p><p>Towards that end, fix Y 0 . Let X denote the set of translations x for which y j + x is -centered between z j and z j for all j ≤ d. Given</p><formula xml:id="formula_29">x 1 , x 2 ∈ X, it follows from Remark 4.1 that |(x 2 -x 1 ) • v j | ≤ for all j ≤ d. However, we know from Lemma 4.5 that v k = d j=1 c j v j with |c j | ≤ 1, so it follows that |(x 2 -x 1 )•v k | ≤ d .</formula><p>Therefore, X is contained in a slab S 1 with height d in the v k direction. Furthermore, the position of this slab is independent of y k .</p><p>On the other hand, y k + x is -centered between z k and z k only if x is contained in a slab S 2 centered at</p><formula xml:id="formula_30">z k +z k 2</formula><p>-y k with height in the v k direction. Therefore, (Y, Z, Z ) iscenterable only if S 1 and S 2 intersect, but this occurs only if y k • v k is in a fixed interval of length (d + 1) . By Lemma 4.1, we know this happens with probability at most (d+1) σ , which completes the proof. Corollary 4.7. Consider a fixed point set B in R d , and suppose A is chosen according to independent d-dimensional normal distributions with variance σ 2 . Then, (A, B) is (k, )-centerable with probability at most</p><formula xml:id="formula_31">(|A||B| 2 ) k (d+1) σ k-d .</formula><p>Proof. The result follows from Proposition 4.6 and a union bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ICP smoothed complexity</head><p>Theorem 4.8. Suppose A and B are chosen according to independent d-dimensional normal distributions with variance σ 2 , and let D denote the maximum diameter of A and B. Then ICP will run on (A, B) in</p><formula xml:id="formula_32">O |A| 3 |B| 8 • d D σ 2 p -2/d</formula><p>iterations with probability at least 1 -2p.</p><p>Proof. Take k = 2d, δ = σp After the first iteration, however, it is easy to check that a + x -N B (a) ≤ 2D for all a, and hence, φ ≤ 4|A|D 2 . Since φ is strictly decreasing, it follows that ICP can then continue for at most</p><formula xml:id="formula_33">1 + 4|A|D 2 • (d+1)|A| 2 |B| 8 p 2/d σ 2</formula><p>iterations, and the result follows.</p><p>Note that our bound here is in terms of the diameter D of A and B after perturbation. It is easy to check, however, that D only far exceeds the original diameter D 0 with vanishingly small probability.</p><p>Then, since ICP is known to never take more than O(|A||B|d) d iterations <ref type="bibr" target="#b6">[7]</ref>, we can take p =</p><formula xml:id="formula_34">1 O(|A||B|d) d</formula><p>to obtain a polynomial bound on the expected number of iterations. This shows that ICP has polynomial smoothed complexity, independent of the dimension.</p><p>We have made no attempt to optimize constants in Theorem 4.8, but we believe new techniques would be required to obtain a substantial improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">An application to k-means</head><p>Our smoothed analysis techniques for ICP carry over surprisingly well to at least one other case, which we consider here. The k-means method is a local search algorithm for partitioning points into clusters. Given a point set X = {x 1 , x 2 , . . . , x n }, it seeks to find cluster centers C = {c 1 , c 2 , . . . , c k } that minimize the total error,</p><formula xml:id="formula_35">φ km = x∈X min c∈C x -c 2 .</formula><p>Although k-means only computes a local optimum, its simplicity and its observed speed have made it an extremely popular clustering algorithm in practice <ref type="bibr" target="#b4">[5]</ref>.</p><p>A formal definition of k-means is presented below.</p><p>1. Choose an arbitrary set of k cluster centers.</p><p>2. Set the cluster C i to be the set of points in X that are closer to c i than they are to any c j for j = i. 3. Recompute the optimal centers for these clusters by setting c i = 1 |Ci| x∈Ci x. 4. Repeat steps 2 and 3 until the C i partitioning stabilizes.</p><p>As with ICP, each step decreases the potential function φ km , and there are only finitely many partitions C i . Therefore, k-means is guaranteed to eventually terminate.</p><p>We are interested in the convergence rate of k-means, and we show here a smoothed upper bound of n O(k) iterations. Smoothed analysis is motivated by a superpolynomial lower bound for the unsmoothed case <ref type="bibr" target="#b1">[2]</ref>.</p><p>Taken on its own, of course, this n O(k) bound is not as strong as our polynomial upper bound for ICP. On the other hand, it improves upon the best unsmoothed bound of O(n kd ) <ref type="bibr" target="#b10">[11]</ref> by an exponent of O( <ref type="formula">1</ref>d ). This matches exactly the improvement our ICP smoothed upper bound gains on the best ICP unsmoothed bound of O(n 2d ) <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Overview</head><p>Our proof of the n O(k) smoothed upper bound for k-means closely mimics our ICP analysis. We focus on the potential function φ km , which is strictly decreasing during an execution of k-means.</p><p>We consider two cases, analogous to the two ICP cases. First suppose no cluster gains or loses more than 2kd points within a single iteration. Then, by a sparsity argument, we show that after at most 2 k iterations, some cluster center will have substantially shifted. Lemma 2.1 then implies a large potential drop during Step 3 of k-means.</p><p>Conversely, suppose some cluster gains or loses at least 2kd points within a single iteration. Then, one of these points must have been significantly removed from its cluster's Voronoi boundary. When this point is reassigned to the nearest cluster during Step 2 of k-means, a substantial potential drop occurs.</p><p>Either way, we can bound the potential drop over 2 k iterations, and the smoothed upper bound will follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Case 1: Small cluster changes</head><p>We begin with the case where each cluster gains or loses at most 2kd points in each iteration. This is analogous to the first case in our ICP analysis. Throughout this section, we will use c(S) to denote the center of mass of a point set S.</p><p>Definition 5.1. Fix a set of data points X ⊂ R d with |X | = n. A key-value is defined to be any expression of the form,</p><formula xml:id="formula_36">n 1 n 2 • c(S),</formula><p>where S ⊂ X has at most 4kd data points, and where n 1 and n 2 are relatively prime positive integers satisfying n 1 ≤ n 2 and n 2 &lt; n.</p><p>Note that a key-value is a linear combination of data points. For any two such expressions x and y, we write x ≡ y if x and y have identical coefficients for each data point. Let C be a constant to be fixed later. We define an "epoch" to be a sequence of iterations during which the potential decreases by a total of at most C. We first show that if X is δ-km-sparse, then small cluster changes within a single epoch can result in at most two different centers for each cluster, and therefore a total of at most 2 k different configurations.</p><p>Lemma 5.1. Suppose k-means is run on a 2n 2 √ C-kmsparse point set X ⊂ R d . Let S denote the set of points in a fixed cluster, and suppose S never gains or loses more than 2kd points during a single iteration. Then S can take on at most two different centers.</p><p>Proof. Suppose by way of contradiction that S takes on at least three different point sets, starting with S 1 , S 2 and S 3 . We may assume without loss of generality that there is a transition between S 1 and S 2 (either S goes from S 1 to S 2 or vice-versa), and that there is a transition between S 2 and S 3 .</p><p>Let A = S 1 ∩S 2 ∩S 3 and let B i = S i -A for i = 1, 2, 3. Then, </p><formula xml:id="formula_37">|B 1 | ≤ |S 1 -S 1 ∩ S 2 | + |S 1 ∩ S 2 -S 1 ∩ S 2 ∩ S 3 | ≤ |S 1 -S 1 ∩ S 2 | + |S 2 -S 2 ∩ S 3 |.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>√</head><p>C during one iteration, then φ km will drop by at least C (see <ref type="bibr" target="#b9">[10]</ref>). Therefore, c(S 2 ) -c(S 1 ) ≤ √ C. On the other hand,</p><formula xml:id="formula_38">c(S 2 ) -c(S 1 ) = |A|c(A) + |B 2 |c(B 2 ) |A| + |B 2 | - |A|c(A) + |B 1 |c(B 1 ) |A| + |B 1 | . Rearranging, we have |A|(|B 1 | -|B 2 |)c(A) is equal to (|A| + |B 1 |)(|A| + |B 2 |)(c(S 2 ) -c(S 1 )) + |B 1 |(|A| + |B 2 |)c(B 1 ) -|B 2 |(|A| + |B 1 |)c(B 2 ).</formula><p>Furthermore,</p><formula xml:id="formula_39">(|A| + |B 1 |)(|A| + |B 2 |) • c(S 2 ) -c(S 1 ) = |S 1 | • |S 2 | • c(S 2 ) -c(S 1 ) ≤ n 2 √ C. First suppose |B 1 | = |B 2 |. In this case, two key-values a = |B 1 |(|A| + |B 2 |)c(B 1 ) and b = |B 2 |(|A| + |B 1 |)c(B 2 ) are separated by a distance of at most n 2 √ C. Since X is 2n 2 √</formula><p>C-km-sparse, it follows that a ≡ b, and hence that B 1 = B 2 . However, this contradicts the assumption that</p><formula xml:id="formula_40">S 1 = S 2 .</formula><p>Therefore, we may divide through by |B 1 | -|B 2 |, which implies that the value,</p><formula xml:id="formula_41">x = |B 1 |(|A| + |B 2 |) |B 1 | -|B 2 | • c(B 1 ) - |B 2 |(|A| + |B 1 |) |B 1 | -|B 2 | • c(B 2 )</formula><p>is within n 2 √ C of |A|c(A). By the same reasoning applied to the transition between S 2 and S 3 ,</p><formula xml:id="formula_42">y = |B 3 |(|A| + |B 2 |) |B 3 | -|B 2 | • c(B 3 ) - |B 2 |(|A| + |B 3 |) |B 3 | -|B 2 | • c(B 2 )</formula><p>is also within n 2 √ C of |A|c(A). Therefore, the distance between x and y is at most 2n 2 √ C. Since x and y are both differences of two key-values, and since X is 2n 2 √ C-kmsparse, we have x ≡ y. Now suppose there exists some point It remains only to consider the case where B 1 , B 2 , and B 3 are pairwise disjoint. Here, the set of data points for which x has non-zero coefficient is precisely B 1 ∪ B 2 . A similar statement holds for y, which implies B 1 = B 3 , giving another contradiction. Corollary 5.2. Suppose k-means is run on X , which is 2n 2 √ C-km-sparse. Then, after 2 k iterations, either some cluster has gained or lost a total of at least 2kd points, or the potential has decreased by a total of at least C.</p><formula xml:id="formula_43">p ∈ B 1 ∩ B 2 . The coefficient of p in x is |B 1 |(|A| + |B 2 |) |B 1 | -|B 2 | • 1 |B 1 | - |B 2 |(|A| + |B 1 |) |B 1 | -|B 2 | • 1 |B 2 | = -1, but unless p ∈ B 3 , the coefficient of p in y is |A|+|B3| |B2|-|B3| = -1. Therefore, p ∈ B 1 ∩ B 2 ∩ B 3 ,</formula><p>Proof. Until one of the given conditions holds, Lemma 5.1 guarantees that each cluster takes on at most two different centers. This leaves only 2 k different choices for the set of all centers, but k-means can never repeat configurations. Therefore, k-means can proceed for at most 2 k -1 iterations in this case.</p><p>It remains to show that X is likely to be δ-km-sparse. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Case 2: Large cluster changes</head><p>We now consider the case where some cluster gains or loses at least 2kd points within a single iteration. This is analogous to the second case of our ICP analysis. We show some point that switches clusters must be relatively far away from its cluster's Voronoi boundary. The point then contributes significantly less to φ km after switching clusters.</p><p>Throughout this section, we use dist(x, H) to denote the shortest distance from a point x to a hyperplane H. Definition 5.3. Let P be a point set in R d . We say P is -separated if for any hyperplane H, there are at most 2d points in P within distance of H. Proposition 5.4. Suppose k-means is run on anseparated point set X ⊂ R d . If one cluster gains or loses a total of at least 2kd points within a single iteration, then the potential drops by at least 4 2  n . Proof. Consider a cluster C that gains or loses a total of at least 2kd points within a single iteration. Then there must exist some other cluster C with which C exchanges at least 2d + 1 points. Let c and c denote the centers of C and C at the start of the iteration. By assumption, at least one point x that switched between C and C is at a distance of at least from the hyperplane H bisecting c and c .</p><p>Assume without loss of generality that x switched from C to C . Then, during Step 2 of the iteration, the potential φ km decreased by at least c -x 2 -c -x 2 = (2xc -c ) • (c -c). Since x is at a distance of at least from H, we know 2x -c -c has magnitude at least 2 in the c -c direction. On the other hand, since x ∈ C when c was calculated, and since |C| ≤ n, the distance from c to H is at least n . Therefore, c -c ≥ 2 n , and the result follows.</p><p>It remains only to show that X is likely to be δ-separated. We begin by proving an extension of Lemma 4.5 that may be of independent interest. Lemma 5.5. Let P be a set of at least d points in R d , and let H be an arbitrary hyperplane. Then there exists a hyperplane H passing through d points of P that satisfies,</p><formula xml:id="formula_44">max p∈P dist(p, H ) ≤ 2d • max p∈P dist(p, H) .</formula><p>Proof. Let = max p∈P dist(p, H) , and assume without loss of generality that 0 ∈ P.</p><p>For any p ∈ P, let π(p) denote the projection of p onto the hyperplane H, and let V denote the (d -1)-dimensional point set {π(p)-π(0) | p ∈ P}. Construct V 0 from V using Lemma 4.5, and let Q denote the points p so that π(p)π(0) ∈ V 0 . We then define H to be the hyperplane passing through 0 and the d -1 points in Q.</p><p>For p ∈ P, we have p -π(p) ≤ , and consequently, for q ∈ {0} ∪ Q, we also have dist π(q), H ≤ . There- The result follows.</p><p>Proposition 5.6. Suppose a set of n points X is chosen according to independent d-dimensional normal distributions with variance σ 2 . Then X is -separated with probability at least 1 -n 2d 4d σ d .</p><p>Proof. By Lemma 5.5, it suffices to prove that with the same probability, there is no hyperplane H that passes through d points in X and that is within a distance 2d of d other points in X . Towards that end, fix disjoint sets P 1 , P 2 ⊂ X , with |P 1 | = |P 2 | = d, and let H denote the hyperplane passing through each point in P 1 . Consider the distribution of x ∈ P 2 while P 1 is fixed. By Lemma 4.1, we know that x is within a distance 2d of H with probability at most 4d σ . Therefore, H is within a distance 2d of every point in P 2 with probability at most 4d σ d .</p><p>The result now follows from a union bound over all sets P 1 and P 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">k-means smoothed complexity</head><p>Theorem 5.7. Suppose a set of n points X is chosen according to independent d-dimensional normal distributions with variance σ 2 , and let D denote the diameter of the resulting point set. Then k-means will run on X in time polynomial in n k , p -1/d and D σ with probability at least 1 -2p. 4dn 2 . Then, Propositions 5.3 and 5.6 imply that X is both δkm-sparse and -separated with probability at least 1 -2p. In this case, Corollary 5.2 and Proposition 5.4 imply that the potential drops by at least C every 2 k iterations.</p><p>As with Theorem 4.8, the result now follows from the fact that the potential is non-increasing and is at most nD 2 after one iteration.</p><p>Since k-means is known to never take more than O(n kd ) iterations <ref type="bibr" target="#b10">[11]</ref>, we can take p = 1 O(n kd ) to obtain an O(n k ) bound on the expected number of iterations. This shows that k-means has O(n k ) smoothed complexity, independent of the dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and open problems</head><p>We have shown smoothed upper bounds for both ICP and k-means that are independent of dimension and that are substantial improvements over previous results. Smoothed analysis in both cases is strongly motivated by superpolynomial lower bounds, proven here for ICP and in <ref type="bibr" target="#b1">[2]</ref> for k-means.</p><p>We are closer to understanding the full complexity of ICP than we are to understanding k-means, but a number of open questions remain. First, there is interest in the running time of ICP in low dimensions <ref type="bibr" target="#b6">[7]</ref>. We have resolved this question in one dimension, but in all other cases, there is a gap between our lower bound of Ω( n d ) d+1 and the upper bound of O(n 2 d) d . Moreover, while we have shown that the smoothed complexity of ICP is polynomial, our exponent is quite large. This is consistent with most other applications of smoothed analysis, but it could certainly use improvement. We believe any substantial work in this direction would require some new techniques that could be of independent interest. Finally, our result for k-means is not polynomial except for k = O(1). Although k tends to be small in practice, we would very much like to see a proper polynomial bound, and we suspect that one may exist.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>P 4 .</head><label>4</label><figDesc>a∈A N B (a)-a |A| . Repeat Steps 2 and 3 until the algorithm stabilizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and suppose we initially have x = b 1 and N B (a) = b 0 . Lemma 3.2. The ICP k algorithm requires |B| -1 iterations to run on the Linear Shifter described above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and it is set to b 2 otherwise. Also, N B (a ) = b in either case. The result now follows from the fact that (b1-a)+(b -a ) k = 0 and (b2-a)+(b -a ) k = v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 4 . 2 .</head><label>42</label><figDesc>Let A and B be point sets in R d . Suppose B is (k, δ)-sparse, and consider any ICP iteration on (A, B) after the first. If N B changes value for at most k points during this iteration, it results in a potential drop of at least δ 2 |A| , or in the termination of the algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 = δ 2</head><label>22</label><figDesc>changes its value for at most k points, then B 1 and B 2 are each of size at most k. If B 1 and B 2 are identical, then the translation did not change at all this iteration, and the algorithm terminates.Otherwise, we know that since B is (k, δ)-sparse, b1∈B1 b 1 -b2∈B2 b 2 &gt; δ. It follows that the translation x = P a∈A N B (a)-a |A| must change by at least δ |A| during this iteration. By Lemma 2.1, this causes a potential drop of at least |A| • δ |A| |A| . Proposition 4.3. Let B be a point set in R d chosen according to independent d-dimensional normal distributions with variance σ 2 . Then, B is (k, δ)-sparse with probability at least 1 -|B| 2k δ σ d . Proof. Fix distinct subsets B 1 and B 2 of B, and let y = b1∈B1 b 1 -b2∈B2 b 2 . Then, y = b∈B c b b for some integer constants c b . Since B 1 and B 2 are distinct, there exists some b for which c b = 0. Fixing all of B other than this b, we see y ≤ δ if and only if b is in some fixed ball of radius δ c b ≤ δ. However, this happens with probability at most δ σ d by Lemma 4.1. The result now follows from a union bound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Definition 4 . 4 .</head><label>44</label><figDesc>Let A and B be point sets in R d . We say (A, B) is (k, )-centerable if there exist Y ⊂ A and Z, Z ⊂ B where |Y | = |Z| = |Z | = k and (Y, Z, Z ) is -centerable. Here, we allow Z and Z to contain repeated points, but each point in Y must be distinct.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma 4 . 5 .</head><label>45</label><figDesc>Let V denote a point set in R d . Then, there exists V 0 ⊂ V with |V 0 | = d such that any v ∈ V can be expressed as u∈V0 c u u for scalars c u ∈ [-1, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>|A| 2 |B| 4 . Then, Proposition 4.3 and Corollary 4.7 imply that both B is (k, δ)-sparse and (A, B) is not (k, )-centerable with probability at least 1 -2p. In this case, Propositions 4.2 and 4.4 imply that φ decreases by at least p 2/d σ 2 (d+1)|A| 2 |B| 8 during each ICP iteration after the first.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Definition 5 . 2 .</head><label>52</label><figDesc>We say a point set X is δ-km-sparse if any key-values (a, b, c, d) that satisfy a + b -c -d ≤ δ also satisfy a + b ≡ c + d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>which contradicts the original definitions of B 1 , B 2 , and B 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Proposition 5 . 3 .</head><label>53</label><figDesc>Suppose a set of n points X is chosen according to independent d-dimensional normal distributions with variance σ 2 . Then X is δ-km-sparse with probabilityat least 1 -n 16kd+12 • n 4 δ σ d . Proof.Fix key-values a, b, c, d for which a + b ≡ c + d. We can write a + b -c -d ≡ x∈X k x x for rational constants k x with denominator at most n 4 . Since a + b -c -d ≡ 0, there exists some x for which k x = 0. Fixing all points in X -{x}, we have a + b -c -d ≤ δ only if x is in some fixed ball of radius at most δ |kx| ≤ n 4 δ. Lemma 4.1 then implies that a + b -c -d ≤ δ with probability at most n 4 δ σ d . The result now follows from a union bound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>fore, dist(p, H ) is at most,p -π(p) + dist π(p), H ≤ + dist π(0), H + dist π(p) -π(0), H ≤ 2 + dist π(p) -π(0), HAnother application of the triangle inequality, along with the fact that|c q | ≤ 1 implies that dist(p, H ) is at most, 2 + q∈Q dist π(q), H + |Q| • dist π(0), H ≤ 2 + 2(d -1) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Proof. Take C = σ 2 p 2 d</head><label>2</label><figDesc>4n 32k+36 , δ = 2n 2 √C, and = σp 1 d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>as well as A 2 = {a } and B 2 = {b = a + 1 2 kv -y}. Lemma 3.3. Suppose the ICP k algorithm is run on an ICP configuration containing the Redirector described above. If the global translation x satisfies x • v ≥ y • v, then the Redirector contributes v to the translation next iteration. Otherwise, it contributes nothing.Proof. We assume for clarity of exposition that N B (a) is set to b 2 if a + x is equidistant from b 1 and b 2 . In this case, it is straightforward to check</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and the result follows. Proposition 4.6. Consider fixed point sets Z = {z 1 , z 2 , . . . , z k } and Z = {z 1 , z 2 , . . . , z k } in R d , and suppose Y = {y 1 , y 2 , . . . , y k } is chosen according to independent d-dimensional normal distributions with variance σ 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Since we assumed S never changes by more than 2kd points during a single iteration, it follows that |B 1 | ≤ 4kd. Similar arguments allow us to bound the size of |B 2 | and |B 3 |. Now recall that there is a transition between S 1 and S 2 within a single epoch. However, Lemma 2.1 implies that if a cluster center moves a distance of</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Rajeev Motwani for his helpful comments, and Vladlen Koltun for introducing us to the ICP algorithm.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Supported in part by NDSEG Fellowship, NSF Grant ITR-0331640, and grants from Media-X and SNRC.</p><p>† Supported in part by NSF Grant ITR-0331640, and grants from Media-X and SNRC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">k-means projective clustering</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Mustafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS &apos;04: Proceedings of the twenty-third ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="155" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How slow is the k-means method?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCG &apos;06: Proceedings of the twenty-second annual symposium on computational geometry</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Average case and smoothed competitive analysis of the multi-level feedback algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Becchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marchetti-Spaccamela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schäer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vredeveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS &apos;03: Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">462</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random knapsack in expected polynomial time</title>
		<author>
			<persName><forename type="first">R</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vöcking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual ACM Symposium on Theory of Computing (STOC-03)</title>
		<meeting>the 35th Annual ACM Symposium on Theory of Computing (STOC-03)<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Survey of clustering data mining techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Berkhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accrue Software</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>San Jose, CA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A method for registration of 3-D shapes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the ICP algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ezra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCG &apos;06: Proceedings of the twenty-second annual symposium on computational geometry</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometrically stable sampling for the ICP algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ikemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Digital Imaging and Modeling (3DIM)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compression of dynamic 3d geometry data using Iterative Closest Point algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="116" to="130" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How fast is the k-means method?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA &apos;05: Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="877" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Applications of weighted Voronoi diagrams and randomization to variance-based kclustering (extended abstract)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Katoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Computational Geometry</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A local search approximation algorithm for k-means clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Geom</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="89" to="112" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The digital Michelangelo project: 3D scanning of large statues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ginzton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ginsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fulk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Proceedings</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Akeley</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="131" to="144" />
			<date type="published" when="2000">2000. 2000</date>
			<publisher>ACM Press / ACM SIGGRAPH / Addison Wesley Longman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="136" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient variants of the ICP algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Intl. Conf. on 3D Digital Imaging and Modeling</title>
		<meeting>the Third Intl. Conf. on 3D Digital Imaging and Modeling</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ICP registration using invariant features</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Wehe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="102" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="463" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
