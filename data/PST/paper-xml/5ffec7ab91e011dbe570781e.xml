<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">APAC: An Accurate and Adaptive Prefetch Framework with Concurrent Memory Access Analysis</title>
				<funder ref="#_fdfMS5h #_HEKsTET #_xRtYDVG">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoyang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Compute Science</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rujia</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Compute Science</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xian-He</forename><surname>Sun</surname></persName>
							<email>sun@iit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Compute Science</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">APAC: An Accurate and Adaptive Prefetch Framework with Concurrent Memory Access Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICCD50377.2020.00048</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prefetching techniques have been studied for decades. However, there are few studies on how concurrent memory accesses may affect prefetching effectiveness. When there are multiple concurrent memory requests, we can classify them into sub-classes by analyzing the overlapping relationship. In this work, we first propose pure prefetch coverage (PPC), a novel prefetching metric that can identify an accurate prefetch coverage under the concurrent memory access model. Then we propose APAC, an adaptive prefetch framework with PPC metric that can capture the dynamics of applications and adjust the prefetching aggressiveness. Our experimental results show that the PPC metric has a higher IPC correlation compared to the conventional prefetch coverage (PC) metric. For memoryintensive single-thread benchmarks, APAC provides an average performance improvement by 17.3% and 5.9% compared to the state-of-the-art adaptive prefetch framework FDP and NST. In a multi-core system, APAC outperforms FDP and NST by 8.5% and 5.0% IPC on average, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The unbalanced technological advancements in processor and memory over the past decades have led to the "Memory Wall" problem. In addition to utilizing memory hierarchy and data locality to alleviate the performance gap between the CPU and the memory, intensive research has been conducted to improve the concurrency of memory systems. Multi-port cache, multi-banked cache, and pipelined cache are advanced cache design techniques that enhance cache hit concurrency; whereas, non-blocking cache can improve cache miss concurrency. Processor ILP techniques, such as out-of-order execution, multiple issue pipeline, simultaneous multi-threading, can dramatically improve both cache hit and miss concurrency <ref type="bibr" target="#b20">[21]</ref>. With these advanced techniques, it is common to observe concurrent memory accesses.</p><p>Memory concurrency reduces memory stall time by overlapping multiple outstanding memory accesses. Some misses occur concurrently with other hits (hit-miss overlapping), whereas some misses do not (miss-miss overlapping) <ref type="bibr" target="#b13">[14]</ref>. Thus, a single cache miss latency is no longer a determinant factor of the overall memory system performance. The performance loss resulting from a cache miss can be reduced when there is hit-miss overlapping. When a miss has no hit-miss overlapping, it becomes the critical factor that could hurt the performance. Such miss is classified as pure miss ( ?II-A).</p><p>Data prefetching has been proved to be effective in reducing CPU stalled cycles by capturing a program's memory access pattern and then proactively fetching needed data blocks from off-chip memory to the faster on-chip cache ahead of demand access. While the conventional prefetching mechanisms are useful in reducing memory accesses delay, they are not fully utilized in a concurrent data access environment. There is room for improvement. In this work, we propose pure prefetch coverage (PPC), a more comprehensive metric that extends the current prefetch coverage (PC) metric to consider concurrent memory accesses. PPC evaluates the performance of a prefetcher by observing the ratio of pure misses reduced rather than misses reduced to quantify its effectiveness. We show the effectiveness of PPC compared with PC through a correlation analysis against execution time.</p><p>PPC lays a foundation for the APAC, an accurate and adaptive prefetch framework that can auto-tune the aggressiveness of the prefetcher at runtime. The memory access behavior may change phase by phase during its runtime <ref type="bibr" target="#b4">[5]</ref>. Therefore, a prefetcher needs to be adaptive to catch the change of the data access pattern. In APAC, we measure and track the pure prefetch coverage (PPC), prefetch accuracy (PA), as well as the pure miss rate (pMR) to adjust the aggressiveness of the prefetcher dynamically. Our experimental results show that APAC outperforms state-of-the-art adaptive prefetch frameworks, such as FDP <ref type="bibr" target="#b17">[18]</ref> and NST <ref type="bibr" target="#b8">[9]</ref>. Also, the PPC metric can be integrated with other complex prefetchers to enhance the performance further.</p><p>The paper is organized as follows: Section II introduces the background of a concurrent memory access model and the missing piece of current prefetch metrics; Section III presents the related prefetching frameworks; Section IV introduces our proposed PPC metric and the method to measure and implement it on a given system; Section V shows our accurate and adaptive prefetch framework APAC can adjust the prefetching scheme dynamically; Section VI describes our experimental settings and Section VII presents experimental results by comparing and integrating with state-of-the-art prefetching frameworks; Section VIII concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION A. Concurrent Cache Accesses</head><p>Concurrent cache accesses enable requests overlapping. Cache misses may or may not overlap with hit accesses; thus, not all misses have an equal impact on performance. When miss cycles are overlapping with one or more hit cycle, the processor can still work on the hit access(es), and the miss penalty is less significant. However, if a miss cycle has no hit to overlap with, it can severely hurt the performance. We refer a miss cycle without overlapping with any hit cycle the pure miss cycle and refer a miss access which consists of at least one pure miss cycle the pure miss access. In other words, pure miss access is the type of miss access that contains at least one miss cycle which does not have any hit accesses to overlap with <ref type="bibr" target="#b20">[21]</ref>. We use pure miss rate (pMR) to define the cache efficiency when considering concurrent misses: To maximize performance, we can reduce pMR by reducing the number of pure misses via prefetching. On the other hand, we also want to minimize pure miss cycles by maximizing the hit-miss overlapping <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prefetch Evaluation Metrics</head><p>Currently, prefetch accuracy (PA) and prefetch coverage (PC) are the most used metrics in evaluating prefetching techniques <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>. PA reflects the percentage of useful prefetches out of all prefetches. Note that a useful prefetch is defined as a prefetched cache line that was accessed at least once while residing in the prefetch destination. The formal definition of prefetch accuracy is as below: Prefetch Accuracy = Num. of Useful Prefetches Num. of Total Prefetches PC is the fraction of total misses that can be effectively reduced by prefetching <ref type="bibr" target="#b5">[6]</ref>. Without considering concurrent memory accesses, an effective prefetcher usually means to cover as many potential misses as possible. The formal definition of prefetch coverage is: Most prefetching techniques are designed to achieve a balanced high PA and PC. For sequential memory access activities, PC directly reflects the contribution of prefetcher to performance improvement. However, we show the limitations of the PC metric when considering concurrent memory accesses in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Case Studies: The Limitations of PC</head><p>PC may provide inaccurate measurements for a prefetcher when we consider concurrent data access. As discussed in Section II-A, not all misses equally impact performance, when concurrency is paramount. As a result, blindly reducing the number of misses may not be the best for performance. A high value of PC does not mean that a prefetcher can certainly cover a lot of pure misses. Likewise, if the number of cache misses saved by a prefetcher is low, but most are pure misses, a low value of PC may lead to better performance. We provide two conceptual cases in Figure <ref type="figure">1</ref> and Figure <ref type="figure">2</ref> to illustrate why ignoring concurrency information in PC metric may produce less accurate evaluations of the prefetcher's effectiveness. In both cases, each cache hit access consumes two cycles, and each cache miss has four miss penalty cycles. Case 1: Low PC, high performance improvement. Without the help of prefetching, in Figure <ref type="figure">1a</ref>), access A and B are cache hits, access C, D, E are cache misses. When considering the access concurrency, both access C and D have two pure miss cycles (cycle 4 and cycle 5), and access E has four pure miss cycles (cycle 4-7). According to the definition of pure miss, access C, D, and E are all pure misses. With prefetching, access E is saved by prefetch and now becomes E' in Figure <ref type="figure">1b</ref>). Though access C and D are still misses with four miss cycles, these cycles are no longer pure miss cycles because they overlap with the hit cycles of access B and access E'. In this example, prefetching only reduces one misses, so PC is just 1/3. However, all concurrent pure misses now have hit-miss overlap. Even though the PC is relatively low, the performance gain brought by prefetching is noticeable. Case 2: High PC, low performance improvement. The second case study shows the limitation of PC in the opposite way. In Figure <ref type="figure">2 a</ref>), without the help of prefetching, accesses B, D, and E are cache hits, accesses A, C, F are cache misses. Access F is the only pure miss in this example, which leads to 3 pure miss cycles (cycle 6-8). All the miss penalty cycles of accesses A and C are overlapping with hits, so access A and access C are not pure misses. After prefetching, as shown in Figure <ref type="figure">2 b</ref>), accesses A and C are saved by prefetching, and they become prefetch hits A' and C'. In this example, two misses are reduced; we calculate the PC as 2/3, which means that we saved the majority of misses. However, the total cycles spent on memory accesses are not saved. Access F is still a pure miss, with three pure miss cycles. Even though the PC is high, the prefetcher might not be able to improve performance as expected if the pure miss reduction is low. Takeaways: The two case studies demonstrate the limitation of PC metric. When we consider memory concurrency, the correlation between the saved misses by prefetching and the memory stall cycles is loss. In some extreme cases, the correlation may even be negative. An alternative metric that considers memory concurrency is needed. Note that we do not show the performance gain of concurrent hits in Figure <ref type="figure">1</ref> and Figure <ref type="figure">2</ref> above, as they do increase hit bandwidth. Overlapping masks the data access delay of the lower layer of the memory hierarchy, which is in general significantly slower than the current memory hierarchy. The two examples show that hitmiss overlapping can directly reduce the memory stall cycles and enhance performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>In addition to studying prefetching at the algorithm level, adaptive prefetching frameworks are designed to control prefetcher aggressiveness based on runtime estimation of system performance for performance improvement.</p><p>Hur and Lin <ref type="bibr" target="#b9">[10]</ref> introduces a probabilistic prefetching technique that utilizes stream length histograms to capture spatial locality in program execution to adjust the prefetch decision. The limitation of their framework is the lack of versatility. It cannot be adapted to other hardware prefetchers except stream prefetcher. Srinath et al. <ref type="bibr" target="#b17">[18]</ref> design a feedback directed prefetching framework (FDP), which tracks the prefetch accuracy, prefetch lateness, and prefetcher generated cache pollution to adjust the prefetch configuration dynamically. Ebrahimi et al. <ref type="bibr" target="#b6">[7]</ref> focus on controlling the aggressiveness of multiple prefetchers in multi-core systems based on the prefetchercaused inter-core interference in shared memory systems. Alameldeen and Wood <ref type="bibr" target="#b2">[3]</ref> propose an adaptive prefetching mechanism that uses cache compression's extra address tags to detect the number of useless and harmful prefetches. Near-side prefetch throttling (NST) <ref type="bibr" target="#b8">[9]</ref> only adjusts the aggressiveness of prefetching based on the fraction of late prefetchers, which has a relatively small hardware overhead and minimizes cache pollution and memory bandwidth wastage. Although the above adjustment frameworks use different metrics as the basis for adjusting the prefetch aggressiveness, none of these metrics can properly consider access concurrency and reflect the effect of prefetch on memory performance accurately. In particular, concurrency has become the most commonly used technique in modern memory systems. As a result, these frameworks sometimes make erroneous decisions that cause the prefetcher to be too conservative or too aggressive, thereby misleading performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PURE PREFETCH COVERAGE</head><p>In this section, we introduce pure prefetch coverage (PPC), which extends the conventional PC metric with concurrency factors. Unlike PC, PPC can examine concurrent accesses and We first describe the definition and the formulas of PPC. Next, the rationality of PPC is illustrated by revisiting the two case studies. Finally, we show the algorithm and implementation details about how to track the number of pure misses and PPC during the execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition</head><p>The PPC is introduced to quantify how effective a prefetcher works in concurrent access activities. Different from the definition of PC, which relies on the ratio of total misses reduced to evaluate the effectiveness of a prefetcher, PPC focuses on quantifying the ratio of pure misses ( ?II-A) that are reduced by prefetching. PPC is defined as the fraction of the number of pure misses reduced due to prefetching over the overall number of pure misses that will occur without prefetching: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Revisit Case Studies with PPC Analysis</head><p>Recalling the two case studies in ?II-C, we re-evaluated the effectiveness of prefetching using the PPC definition. Table <ref type="table" target="#tab_2">I</ref> shows the value of PC and PPC for these two study cases, respectively. In the first case, one of the three cache misses is successfully reduced by prefetching, the value of PC is 1/3. When considering the access concurrency, all three pure misses are now able to overlap with hits. Therefore, the value of PPC is 3/3 = 1. A high value of PPC accurately reflects the considerable contribution of prefetching to performance gain in this case. In the second case, prefetching removes two of the three misses, so the value of PC is 2/3. Nevertheless, pure miss is not reduced. After prefetching, access F is still a pure miss with three pure miss cycles. So the PPC is calculated as 0/1 = 0. Compared to PC, PPC captures the ratio of pure misses reduced by prefetcher, which contributes directly to performance. PPC can accurately evaluate the effectiveness of prefetching than PC when there are concurrent memory accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Measurement and Implementation</head><p>To compute the PPC during the execution time, we use two counters to track the pure misses: 1) RPM, is used to count the reduced pure misses by the prefetcher; 2) DPM, is used to record the demand pure misses, which are the pure misses that cannot be covered by the prefetcher. In this way, we compute PPC as:</p><formula xml:id="formula_0">PPC = RPM RPM + DPM</formula><p>where the sum of RPM and DPM is equal to the number of total pure misses without the prefetcher.</p><p>The algorithm for detecting pure misses and measuring PPC is shown in Algorithm 1. We declare the bits and counters used for measurement on the top of Algorithm 1. The information of all outstanding cache misses is tracked by the MSHR (Miss Status Holding Register). Each miss is allocated to an MSHR entry before it is served <ref type="bibr" target="#b11">[12]</ref>. The NoHit and OnlyPrefetch bits are used to identify the current cycle status: NoHit is set when there is no hit in this cycle; OnlyPrefetch is set when there are only prefetch hits but no demand hits in this cycle.</p><p>The steps to determine whether a miss is a pure miss are shown from lines 1 to 5. A IsPure bit is used per MSHR entry. If NoHit is set, we know all misses in MSHR are pure misses, so their associated IsPure bits are set.</p><p>Lines 6 to 14 are used to determine how prefetch hits can reduce pure misses. If OnlyPrefetch is set in the cycle, these prefetch hits can be approximately considered as pure misses saved by prefetch. So we increase the RPM counter to record this type of pure miss reduction at line 8. Note that we may have multiple hit cycles, so N is divided by hit cycle to remove repeated counts. Next, to calculate the pure misses reduced by overlapping with prefetch hits, we use an Overlap bit to each MSHR entry. If the OnlyPrefetch bit is set and a miss in the MSHR is not a pure miss, it means that it is converted from a pure miss (w/o the prefetch) to a miss that now can be overlapped with a prefetch hit. So we set its Overlap bit to 1 at line 11.</p><p>Lines 16 to 23 are used for updating of the counters when a miss from MSHR is serviced and removed. When a miss from MSHR entry j is serviced at this cycle, if its IsPure is set, the DMR counter is incremented. Otherwise, if the Overlap is set, the RPM counter is incremented. Then the IsPure and Overlap associated with that MSHR entry are reset. All the counters are updated at every memory cycle, so the number of demand pure misses and the number of pure misses reduced by prefetching are updated every cycle. With RPM and DPM counters, we can calculate PPC periodically based on the definition and use PPC to guide the adaptive prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ADAPTIVE PREFETCH CONSIDERS ACCESS CONCURRENCY (APAC)</head><p>Typically, a hardware prefetcher works by predicting future data access based on observed past access behavior. How aggressive a data prefetcher should be is a problem often discussed. A good data prefetcher should guarantee a sufficient aggressiveness to prefetch data ahead appropriately for the best performance. However, over-aggressive prefetching may bring adverse effects and lead to useless bandwidth consumption and cache pollution <ref type="bibr" target="#b22">[23]</ref>. Even the memory access pattern is correctly predicted, an over-aggressive prefetcher still may create early prefetches issue <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>. When the pattern prediction is inaccurate, blindly using aggressive prefetching will provide a lot of unnecessary data that evicts useful data from the cache, and could drag down the overall system performance.  Various workloads may have completely different behaviors. Even for a given workload, it may have completely different memory access patterns in different phases and show varying sensitivity to prefetch aggressiveness <ref type="bibr" target="#b12">[13]</ref>. Additionally, the influence of concurrent data access should not be ignored if we want to evaluate the performance of systems and prefetchers correctly. To address these issues, we propose an adaptive prefetching framework APAC that takes into account data access concurrency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metrics</head><p>In APAC, we use pure prefetch coverage (PPC), prefetch accuracy (PA) and pure miss rate (pMR) without prefetching as the feedback metrics for adjusting the aggressiveness of the prefetching. The metrics are collected during each execution phase and will guide the prefetching aggressiveness in the next phase.</p><p>We first identify that using pMR can predict the overall effectiveness of the prefetcher. If a phase of an application has a high pMR, that means, in general, an aggressive prefetch algorithm is needed. In contrast, if a phase has a low pMR, in most cases, we should avoid being too aggressive in prefetching to save the bandwidth and reduce pollution.</p><p>The tradeoff between prefetch aggressiveness and effectiveness can be evaluated using IPC, PPC, and PA. Figure <ref type="figure" target="#fig_1">3</ref> shows the behavior of the 437.leslie3d benchmark under different aggressiveness of the IP-based stride prefetcher. The IPC, PPC, and PA all have been normalized to very conservative prefetch configuration (prefetch degree equals to 1). As the prefetch degree increases, stride prefetcher becomes more aggressive, more pure misses may effectively be covered, resulting in increased PPC. However, with the increment of the prefetch degree, useless prefetches are increasing, which is reflected in the continuous drop of PA value. In Figure <ref type="figure" target="#fig_1">3</ref>, when the prefetch degree is 16, severe cache pollution and bandwidth contention resulted in performance degradation, yielding to lower IPC.</p><p>To achieve the optimal prefetch aggressiveness, we need to closely monitor all three metrics, PPC, PA, and pMR. We will discuss the adaptive aggressiveness selection mechanism in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptive Aggressiveness Selection</head><p>In this work, we use the prefetch degree in the IP-based stride prefetcher to determine the prefetching aggressiveness <ref type="bibr" target="#b3">[4]</ref>. The prefetch degree will determine how many prefetch accesses per demand miss will be generated. For example, a prefetch degree of N will bring [A, A + 1, ..., A + N ] when there is a demand miss at address A, when the stride is 1.</p><p>We define five grades of the aggressiveness in this work (degree = 1, 2, 4, 8, 16), from the very conservative (degree 1) to the very aggressive (degree 16). The initial prefetch degree is set to 4. During the application execution time, APAC collects and evaluates the performance of prefetching at the end of each sampling phase. It dynamically adjusts the appropriate prefetching aggressiveness for the next phase based on three feedback metrics: PPC, PA, and pMR without prefetching.</p><p>The measured PPC value needs to be compared with the threshold P P C th to determine whether the current aggressiveness of prefetching can cover enough pure misses. The currently measured PA value is compared with two thresholds P A high and P A low to determine the current prefetch accuracy is high, average or low. The threshold of pMR without prefetching pM R th is used to reflect whether the current phase caused a performance issue due to the excessive number of pure misses. We set these thresholds empirically based on the results of a large number of simulations. Table <ref type="table" target="#tab_5">II</ref> shows the thresholds used to implement APAC and the heuristic policy for dynamic updating the aggressiveness of the prefetcher.</p><p>If the value of pMR without prefetching is smaller than the pM R th , except for Case 2, APAC tends to degrade the aggressiveness of the prefetcher, since we do not need to reduce the pure misses at the cost of accuracy. In Case 2, when the PA is high, and the PPC is smaller than the threshold, APAC suggests increasing the aggressiveness for higher gain from the accurate prediction. If the pMR without prefetch is larger than the pM R th , the prefetcher tends to increase aggressiveness for higher PPC, which decreases the pure misses and improve the performance. Case 8 is an exception. In this case, APAC decreases the aggressiveness of prefetcher to reduce cache pollution and save memory bandwidth because the current phase shows that the prefetcher cannot prefetch effectively and accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hardware Cost and Complexity of APAC</head><p>APAC requires monitoring PPC, PA, and pMR without prefetching during the execution time. We have presented the measurement and implementation for tracking PPC in ?IV-C. The hardware cost is shown in Table <ref type="table" target="#tab_6">III</ref>. The needed bits include IsPure, Overlap, NoHit, and OnlyPrefetch. For a 32entry MSHR, the IsPure and Overlap need a total of 64 bits. The NoHit and OnlyPrefetch just need 1 bit each, which is trivial. In addition, two counters, RPM and DPM are required. The 32-bit wide registers are sufficient to prevent their data overflow.</p><p>To measure PA value, we use a similar method described by Feedback Detected Prefetching (FDP) <ref type="bibr" target="#b17">[18]</ref>. A bit pref-bit per L2 block is required to differentiate whether the data block comes from demand request or prefetch request. For a 256KB L2 cache with 64B cache block size, the total pref-bit size is 0.5KB. With the help of pref-bit, the number of useful prefetches (prefetch hits) and the total number of prefetches can be recorded by two 32-bit wide counters UPF and TPF. The value of PA can be calculated as the ratio between the UPF and TPF.</p><p>The pMR value without prefetching is computed as the ratio of the total number of pure misses that will occur without prefetching to the number of total accesses that will occur without prefetching. We use the method mentioned in ?IV-C to count the number of pure misses that will occur without prefetching. The number of accesses that will occur without prefetching can be obtained by calculating the sum of demand misses, demand hits, and prefetch hits. Therefore, the bits and counters used for PPC and PA can help to compute pMR without prefetching as well.</p><p>In total, the hardware overhead of APAC is around 0.52 KB, which is only 0.2% of the capacity of the baseline 256KB L2 cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL METHODOLOGY</head><p>We implement our adaptive prefetching framework APAC as described in ?V with both a single-core system and a 4-core system. ChampSim <ref type="bibr" target="#b0">[1]</ref> simulator is used to provide an appropriate memory system performance simulation. A detailed out-of-order CPU model in the ChampSim was adopted to achieve the most accurate simulation results. The details of the configuration parameters of our simulation are described in Table <ref type="table" target="#tab_7">IV</ref>. APAC works with an IP-based stride prefetcher at the L2 cache by default. As we mentioned in ?V-B, there are five different prefetch degrees <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16)</ref> available for selection by the stride prefetcher.</p><p>For APAC dynamic prefetching framework, the value of PPC, PA, and pMR without prefetching will be updated every 4096 misses (half the number of blocks in the L2 cache) in L2. Initially, APAC will set the prefetch aggressiveness of the first phase to degree 4. When a given stride prefetcher works under the APAC frame, the prefetch degree will be dynamically adjusted between degree 1 and degree 16, and the prefetcher will never be disabled.</p><p>We select the performance without a prefetcher as the baseline for performance comparison. We compare APAC against two state-of-the-art adaptive prefetching frameworks FDP <ref type="bibr" target="#b17">[18]</ref> and NST <ref type="bibr" target="#b8">[9]</ref>. We also implement a naive adaptive framework called NAP with a similar workflow as APAC. However, NAP makes all decisions without considering concurrency. NAP dynamically adjusts the aggressiveness of prefetching based on prefetch coverage (PC), prefetch accuracy (PA), and miss rate (MR) without prefetching. The importance of comprehensive memory access analysis can be reflected by comparing APAC and NAP.</p><p>We collect SimPoint <ref type="bibr" target="#b15">[16]</ref> traces from SPEC CPU2006 <ref type="bibr" target="#b16">[17]</ref> and SPEC CPU2017 <ref type="bibr" target="#b1">[2]</ref>. For SPEC workloads, we use high intensity workloads with MPKI &gt; 3, as shown in Table <ref type="table" target="#tab_8">V</ref>. For 4-core experiments, we test multi-copy and mixed SPEC workloads. A multi-copy workload has four identical copies of a single benchmark. A mixed workload has four different benchmarks, which are assigned to different cores. CloudSuite <ref type="bibr" target="#b7">[8]</ref> workloads are multi-threaded and are only used for 4-core experiments. Each trace is warmed up with 50M instructions  for all experiments, and simulation results are collected over the next 200M instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL RESULTS</head><p>In this section, we first discuss the concurrency of each SPEC workloads, then verify the correctness of PPC through a performance-metric correlation study. Finally, we show the effectiveness of the APAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Concurrency Analysis</head><p>We use accesses per kilocycles (APKC) <ref type="bibr" target="#b19">[20]</ref> to measure the overall memory concurrency concerning the complexity of modern memory systems. Figure <ref type="figure" target="#fig_2">4</ref> shows the L2 concurrency of each SPEC benchmark in single-core and 4-core multi-copy configurations without prefetching. In the multi-core system, applications run on different cores and share the LLC and main memory, which causes bandwidth contention, especially when all cores are running the same application. This results in the concurrency gap between the single-core and 4-core multi-copy configurations shown in Figure <ref type="figure" target="#fig_2">4</ref>. The single-core cases achieve a geometric mean of 1.9 times higher APKC than 4-core cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Accuracy of PPC Metric</head><p>We show the correlation between PPC and IPC for each evaluated workload to verify the correctness of the PPC metric. Also, we show the correlation between the classical metric PC and IPC for comparison. The higher the correlation is, the better the metric is, whereas a low correlation means the metric is wrong. From a statistical point of view, the correlation coefficient describes the proximity between the changing trends of the two variables. Therefore for each application, five static prefetch configurations(from degree 1 to degree 16) are executed independently. Then, we calculate the r(IP C, P P C) and r(IP C, P C) correlation based on the five different configurations. The correlation coefficient of r of two variables X and Y can be calculated using the following equation: </p><formula xml:id="formula_1">r(X, Y )= n( XY )-( X)( Y ) ? [n X 2 -( X) 2 ][n Y 2 -( Y ) 2 ]</formula><p>where X and Y are the sampling points for two variables.</p><p>Table <ref type="table" target="#tab_9">VI</ref> shows that, in both single-core and 4-core configurations, compare to PC, PPC shows a stronger positive correlation with IPC. This result demonstrates the unique advantage of PPC capturing the concurrency characteristics of modern memory systems and accurately evaluating the efficiency of prefetching. As discussed in multi-core cases, the concurrency of memory accesses will be reduced by bandwidth contention. The accuracy of PPC will be affected by concurrency, which will cause the average gap between r(IP C, P P C) and r(IP C, P C) in 4-core configurations to be smaller than the gap in single-core configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. APAC Performance Evaluation</head><p>Single-core Results: Figure <ref type="figure" target="#fig_3">5</ref> shows the single-core pure prefetch coverage of the various adaptive prefetch frameworks. APAC achieves the highest PPC of all the adaptive prefetch frameworks simulated. APAC covers 40.0% of the demand pure misses at L2, higher than NST's 35.7%, FDP's 29.0%, and NAP's 22.2%. For 459.GemsFDTD and 620.omnetpp, which suffer from indirect accesses, APAC does not show advantages on PPC. The hardware prefetchers do not prove to be useful to these benchmarks with irregular accesses, and all frameworks are almost not conducive to the performance with a less than 0.05 pure miss coverage at the L2.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> shows the single-core speedup achieved by NAP, FDP <ref type="bibr" target="#b17">[18]</ref>, NST <ref type="bibr" target="#b8">[9]</ref> and APAC for the individual memoryintensive SPEC CPU applications, followed by the geomean across all the workloads. All results are normalized to the baseline of no prefetching. In most cases, APAC can match the best static optimum for each specific workload based on feedbacked application phase behavior. APAC provides a 70.0% higher geometric mean IPC over the baseline, 28.9% over NAP, 17.3% over FDP, and 5.9% over NST. Benchmarks 603.bwaves, 619.lbm, 621.wrf, and 623.xalancbmk, benefit the most from APAC, the IPC increased over NST ranging from 8.6% to 49.2%. For these benchmarks, based on the observation of PPC, APAC provides  higher PPC over NST, ranging from 5.3% to 34.0%, which can explain why these benchmarks benefit the most from APAC. APAC fully considers the balance between the reduction of pure misses and the accuracy of the prefetch requests. Therefore APAC can prevent severe cache pollution while ensuring pure miss coverage. 4-core Results: For the evaluation of 4-core systems, we simulate both multi-copy and mixed workloads then compare APAC with other adaptive mechanisms. For multi-copy workloads, Figure <ref type="figure">7</ref> shows that APAC provides superior performance improvement with 12.1% higher geometric mean over the baseline, whereas both FDP and NST only provide 6.6% speedup. Compared with single-core results, the effectiveness of all adaptive prefetch frameworks has decreased. The contention at the LLC and DRAM bandwidth is the primary limiting factor to cause this trend.</p><p>For mixed workloads, Figure <ref type="figure" target="#fig_5">8</ref> shows that APAC achieves an improvement of 62.7% on average, whereas NAP, FDP, and NST improve performance by 28.6%, 40.3%, and 53.1%. In the multi-core system, coordinated throttling is independently applied to the L2 prefetcher of each core, which is essential for mixed workloads with different access patterns and nonuniform bandwidth demands.</p><p>For most CloudSuite benchmarks, it is challenging for most hardware prefetchers to capture their complex access patterns. Since the focus of APAC is not to detect and propose complex prefetching strategy, we achieve similar performance gains compared with other frameworks. As shown in Figure <ref type="figure" target="#fig_6">9</ref>, streaming is the only benchmark where all frameworks work can significantly improve performance. On average, APAC achieves a 10.6% speedup and outperforms NAP by 3.6%. Integrate APAC with a complex prefetcher: The major contribution of this paper is a framework that enables comprehensive concurrent access pattern analysis, and we have shown that with a simple strided prefetcher, we can enhance the performance for most workloads. It is worth noting that our approaches can be easily integrated with more complex prefetching algorithms and extended through multiple memory hierarchies. By adequately integrating our PPC metrics into the system, the performance gain coming from the advanced prefetching algorithms can be further enhanced with our meth-Fig. <ref type="figure">7</ref>: Normalized IPC compared to baseline (4-core, multicopy).   ods. We apply APAC to the open-sourced IPCP <ref type="bibr" target="#b14">[15]</ref>, which is the winner of the 3rd Data Prefetching Championship (DPC-3). IPCP can perform multiple types of prefetching patterns; therefore, prefetching accuracy is relatively high. We show the add-on performance gain in Figure <ref type="figure">10</ref> and Figure <ref type="figure">11</ref>. Compare to utilizing IPCP alone, applying APAC to the IPCP provides additional performance improvement of 3.2% and 3.4% in the single-core and 4-core configuration, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>In this paper, we identify that concurrency of memory accesses is an indispensable factor when evaluating the prefetch effectiveness. We propose pure prefetch coverage (PPC), a comprehensive metric focusing on the effect of prefetching. We develop a detailed implementation of detecting pure misses and the measurement method for PPC. Furthermore, we design an accurate and lightweight, adaptive prefetch framework, APAC, based on concurrency aware metrics. APAC outperforms state-of-the-art adaptive prefetcher frameworks, and it can be easily integrated with other advanced prefetchers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Fig. 1: Case 1: a single prefetch hit can save 2 cycles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The impact of aggressive prefetching on performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: APKC on L2 in single-core and 4-core configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: PPC measured in the single-core configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Normalized IPC compared to baseline (single-core).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Normalized IPC compared to baseline (4-core, mixedcopy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Normalized IPC compared to baseline (4-core, Cloud-Suite).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Fig. 10: Speedup of APAC + IPCP in the single-core configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>PC and PPC of two study cases</figDesc><table><row><cell></cell><cell>Reduced pure misses with prefetch</cell><cell>Overall pure misses w/o prefetch</cell><cell>PPC</cell><cell>PC</cell><cell>Stall cycles reduced</cell></row><row><cell>Case 1</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>1/3</cell><cell>2</cell></row><row><cell>Case 2</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>2/3</cell><cell>0</cell></row><row><cell cols="6">distinguish between different types of concurrent misses. It is a</cell></row><row><cell cols="6">comprehensive metric that evaluates a prefetcher's contribution</cell></row><row><cell cols="6">to pure misses reduction during concurrent memory accesses.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Algorithm 1 Detect and Measure PPC (called every cycle) // Single-bit cycle status identifier NoHit: set if no hit accesses in this cycle OnlyPrefetch: set if only has prefetch hits but no demand hits in this cycle // Additional MSHR entry bit to record pure miss IsPure: set if a miss is pure miss Overlap: set if a pure miss reduced by overlapped with prefetch hits // Counters used to compute PPC RPM: counts the pure miss reduced by prefetching DPM: counts the demand pure miss with prefetching</figDesc><table><row><cell cols="2">1: if NoHit is set then</cell></row><row><cell>2:</cell><cell>for ith outstanding demand miss in M SHR do</cell></row><row><cell>3:</cell><cell>MSHR[i].IsPure = 1</cell></row><row><cell>4:</cell><cell>end for</cell></row><row><cell cols="2">5: end if</cell></row><row><cell cols="2">6: if OnlyPrefetch is set then</cell></row><row><cell>7:</cell><cell>N ? N umber of pref etch hits in this cycle</cell></row><row><cell>8:</cell><cell>RP M + = N/hit cycle</cell></row><row><cell>9:</cell><cell>for ith outstanding demand miss in M SHR do</cell></row><row><cell>10:</cell><cell>if MSHR[i].IsPure = 0 then</cell></row><row><cell>11:</cell><cell>MSHR[i].Overlap = 1</cell></row><row><cell>12:</cell><cell>end if</cell></row><row><cell>13:</cell><cell>end for</cell></row><row><cell cols="2">14: end if</cell></row><row><cell>15:</cell><cell></cell></row><row><cell cols="2">16: for jth serviced miss in MSHR do</cell></row><row><cell>17:</cell><cell>if MSHR[j].IsPure is set then</cell></row><row><cell>18:</cell><cell>DP M ++</cell></row><row><cell>19:</cell><cell>end if</cell></row><row><cell>20:</cell><cell>if MSHR[j].Overlap is set then</cell></row><row><cell>21:</cell><cell>RP M ++</cell></row><row><cell>22:</cell><cell>end if</cell></row><row><cell cols="2">23: end for</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Adjuct prefetch aggressiveness with runtime metrics (L=Low, H=High) th =0.25 P A low =0.15 P A high =0.4 pM R th =0.5</figDesc><table><row><cell cols="2">Case PPC</cell><cell>PA</cell><cell>pMR w/o Prefetch</cell><cell>Aggressiveness Update (reason)</cell></row><row><cell>1</cell><cell>H</cell><cell>H</cell><cell>L</cell><cell>No Change (base case)</cell></row><row><cell>2</cell><cell>L</cell><cell>H</cell><cell>L</cell><cell>Increment (to increase PPC)</cell></row><row><cell>3</cell><cell>H</cell><cell>L</cell><cell>L</cell><cell>Decrement (to reduce pollution)</cell></row><row><cell>4</cell><cell>L</cell><cell>L</cell><cell>L</cell><cell>Decrement (to reduce pollution)</cell></row><row><cell>5</cell><cell>H</cell><cell>H</cell><cell>H</cell><cell>Increment (to increase PPC)</cell></row><row><cell>6</cell><cell>H</cell><cell>L</cell><cell>H</cell><cell>No Change (to keep the high PPC)</cell></row><row><cell>7</cell><cell>L</cell><cell>H</cell><cell>H</cell><cell>Increment (to increase PPC)</cell></row><row><cell>8</cell><cell>L</cell><cell>L</cell><cell>H</cell><cell>Decrement (to reduce pollution)</cell></row><row><cell></cell><cell>P P C</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Hardware cost of APAC</figDesc><table><row><cell>Additional bits</cell><cell>Size</cell><cell>Used for</cell></row><row><cell>IsPure</cell><cell cols="2">1 bit per L2 MSHR PPC, pMR</cell></row><row><cell>Overlap</cell><cell cols="2">1 bit per L2 MSHR PPC, pMR</cell></row><row><cell>NoHit</cell><cell>1 bit</cell><cell>PPC, pMR</cell></row><row><cell>OnlyPrefetch</cell><cell>1 bit</cell><cell>PPC, pMR</cell></row><row><cell>DPM</cell><cell>32 bit</cell><cell>PPC, pMR</cell></row><row><cell>RPM</cell><cell>32 bit</cell><cell>PPC, pMR</cell></row><row><cell>pref-bit</cell><cell>1 bit per L2 block</cell><cell>PA, pMR</cell></row><row><cell>UPF</cell><cell>32 bit</cell><cell>PA, pMR</cell></row><row><cell>TPF</cell><cell>32 bit</cell><cell>PA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Simulated system configurationsProcessor One to four cores, 4GHz, 8-issue width, 256-entry ROB</figDesc><table><row><cell>L1 Cache</cell><cell>split 32KB I/D-cache/core, 8-way, 4-cycle hit latency, 8-entry MSHR, 64B line-size</cell></row><row><cell>L2 Cache</cell><cell>unified 256KB, 8-way, 10-cycle hit latency, 32-entry MSHR, 64B line-size</cell></row><row><cell>L3 Cache</cell><cell>shared 2MB/core, 16-way, 20-cycle hit latency, 64?#cores-entry MSHR, 64B line-size</cell></row><row><cell>DRAM</cell><cell>4GB 1 channel, 64-bit channel, 1600MT/s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Evaluated workloads</figDesc><table><row><cell>Workload</cell><cell>LLC MPKI</cell><cell>Workload</cell><cell>LLC MPKI</cell></row><row><cell>436.cactusADM</cell><cell>4.99</cell><cell>437.leslie3d</cell><cell>3.56</cell></row><row><cell>459.GemsFDTD</cell><cell>6.40</cell><cell>462.libquantum</cell><cell>26.07</cell></row><row><cell>482.sphinx3</cell><cell>11.65</cell><cell>602.gcc</cell><cell>70.06</cell></row><row><cell>603.bwaves</cell><cell>23.19</cell><cell>605.mcf</cell><cell>72.69</cell></row><row><cell>619.lbm</cell><cell>47.23</cell><cell>620.omnetpp</cell><cell>10.64</cell></row><row><cell>621.wrf</cell><cell>19.22</cell><cell>623.xalancbmk</cell><cell>19.10</cell></row><row><cell>649.fotonik3d</cell><cell>8.77</cell><cell>654.roms</cell><cell>32.47</cell></row><row><cell>MIX1</cell><cell>436,437,462,482</cell><cell>MIX2</cell><cell>436,437,602,603</cell></row><row><cell>MIX3</cell><cell>436,437,621,623</cell><cell>MIX4</cell><cell>436,437,649,654</cell></row><row><cell>MIX5</cell><cell>462,482,602,603</cell><cell>MIX6</cell><cell>462,482,621,623</cell></row><row><cell>MIX7</cell><cell>462,482,649,654</cell><cell>MIX8</cell><cell>602,603,621,623</cell></row><row><cell>MIX9</cell><cell>602,603,649,654</cell><cell>MIX10</cell><cell>621,623,649,654</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Performance correlation coefficient analysis</figDesc><table><row><cell>Workload</cell><cell cols="4">single-core r(PPC,IPC) r(PC,IPC) r(PPC,IPC) r(PC,IPC) 4-core</cell></row><row><cell>436.cactusADM</cell><cell>0.97</cell><cell>-0.65</cell><cell>0.99</cell><cell>0.78</cell></row><row><cell>437.leslie3d</cell><cell>0.98</cell><cell>0.89</cell><cell>0.99</cell><cell>0.70</cell></row><row><cell>459.GemsFDTD</cell><cell>0.99</cell><cell>0.40</cell><cell>0.78</cell><cell>0.39</cell></row><row><cell>462.libquantum</cell><cell>0.96</cell><cell>0.92</cell><cell>0.99</cell><cell>0.95</cell></row><row><cell>482.sphinx3</cell><cell>0.99</cell><cell>0.89</cell><cell>0.99</cell><cell>0.89</cell></row><row><cell>602.gcc</cell><cell>0.98</cell><cell>0.86</cell><cell>0.70</cell><cell>0.21</cell></row><row><cell>603.bwaves</cell><cell>0.91</cell><cell>0.52</cell><cell>0.98</cell><cell>0.89</cell></row><row><cell>605.mcf</cell><cell>0.98</cell><cell>0.80</cell><cell>0.99</cell><cell>0.89</cell></row><row><cell>619.lbm</cell><cell>0.83</cell><cell>0.56</cell><cell>0.65</cell><cell>0.34</cell></row><row><cell>620.omnetpp</cell><cell>0.96</cell><cell>0.87</cell><cell>0.66</cell><cell>0.28</cell></row><row><cell>621.wrf</cell><cell>0.99</cell><cell>0.94</cell><cell>0.94</cell><cell>0.84</cell></row><row><cell>623.xalancbmk</cell><cell>0.99</cell><cell>-0.55</cell><cell>0.85</cell><cell>0.75</cell></row><row><cell>649.fotonik3d</cell><cell>0.99</cell><cell>0.95</cell><cell>0.95</cell><cell>0.91</cell></row><row><cell>654.roms</cell><cell>0.99</cell><cell>0.95</cell><cell>0.99</cell><cell>0.91</cell></row><row><cell>Average</cell><cell>0.97</cell><cell>0.60</cell><cell>0.89</cell><cell>0.70</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: FLORIDA INTERNATIONAL UNIVERSITY. Downloaded on June 19,2021 at 17:21:37 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Authorized licensed use limited to: FLORIDA INTERNATIONAL UNIVERSITY. Downloaded June 19,2021 at 17:21:37 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This research is supported in part by the <rs type="funder">National Science Foundation</rs> under grant <rs type="grantNumber">CCF-2008907</rs>, <rs type="grantNumber">CNS-1730488</rs> and <rs type="grantNumber">CCF-1536079</rs>, and by the <rs type="institution">NSF Chameleon computing facility</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fdfMS5h">
					<idno type="grant-number">CCF-2008907</idno>
				</org>
				<org type="funding" xml:id="_HEKsTET">
					<idno type="grant-number">CNS-1730488</idno>
				</org>
				<org type="funding" xml:id="_xRtYDVG">
					<idno type="grant-number">CCF-1536079</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<title level="m">The champsim simulator</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.spec.org/cpu" />
		<title level="m">Spec cpu2017 benchmark suite</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactions between compression and prefetching in chip multiprocessors</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An effective on-chip preloading scheme to reduce data access penalty</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Perceptron-based prefetch filtering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An adaptive data prefetcher for highperformance processors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCGRID</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Coordinated control of multiple prefetchers in multi-core systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clearing the clouds: a study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm sigplan notices</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Near-side prefetch throttling: adaptive prefetching for high-performance many-core processors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Bois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vandriessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Memory prefetching using adaptive stream detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>In MICRO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lockup-free instruction fetch/prefetch cache organization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">When prefetching works, when it doesn&apos;t, and why</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lpm: A systematic methodology for concurrent data access pattern optimization from a matching perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2478" to="2493" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bouquet of instruction pointers: Instruction pointer classifier-based spatial hardware prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pakalapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using simpoint for accurate and efficient simulation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Biesbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="318" to="319" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spec cpu2006 benchmark tools</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Spradling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="130" to="134" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feedback directed prefetching: Improving the performance and bandwidth-efficiency of hardware prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A prefetch taxonomy</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="126" to="140" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Apc: a performance metric of memory systems</title>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="130" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Concurrent average memory access time</title>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data prefetch mechanisms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Vanderwiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="174" to="199" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reducing cache pollution via dynamic data prefetch filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Hsien-Hsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
