<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hard Negative Mixing for Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-02">2 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kalantidis</forename><surname>Yannis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bulent</forename><surname>Mert</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philippe</forename><surname>Pion</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
						</author>
						<author>
							<persName><surname>Larlus</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe Grenoble</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2020)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hard Negative Mixing for Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-02">2 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.01028v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies, either at the image or the feature level, improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e. the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing memory requirements, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection, and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Figure <ref type="figure">1</ref>: MoCHi generates synthetic hard negatives for each positive (query).</p><p>Contrastive learning was recently shown to be a highly effective way of learning visual representations in a self-supervised manner <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">30]</ref>. Pushing the embeddings of two transformed versions of the same image (forming the positive pair) close to each other and further apart from the embedding of any other image (negatives) using a contrastive loss, leads to powerful and transferable representations. A number of recent studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b66">65]</ref> show that carefully handcrafting the set of data augmentations applied to images is instrumental in learning such represen-tations. We suspect that the right set of transformations provides more diverse, i.e. more challenging, copies of the same image to the model and makes the self-supervised (proxy) task harder. At the same time, data mixing techniques operating at either the pixel <ref type="bibr" target="#b71">[70,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b86">85]</ref> or the feature level <ref type="bibr" target="#b70">[69]</ref> help models learn more robust features that improve both supervised and semi-supervised learning on subsequent (target) tasks.</p><p>In most recent contrastive self-supervised learning approaches, the negative samples come from either the current batch or a memory bank. Because the number of negatives directly affects the contrastive loss, current top contrastive approaches either substantially increase the batch size <ref type="bibr" target="#b10">[11]</ref>, or keep large memory banks. Approaches like <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b78">77]</ref> use memories that contain the whole training set, while the recent Momentum Contrast (or MoCo) approach of He et al. <ref type="bibr" target="#b31">[30]</ref> keeps a queue with features of the last few batches as memory. The MoCo approach with the modifications presented in <ref type="bibr" target="#b12">[13]</ref> (named MoCo-v2) currently holds the state-of-the-art performance on a number of target tasks used to evaluate the quality of visual representations learned in an unsupervised way. It is however shown <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">30]</ref> that increasing the memory/batch size leads to diminishing returns in terms of performance: more negative samples does not necessarily mean hard negative samples.</p><p>In this paper, we argue that an important aspect of contrastive learning, i.e. the effect of hard negatives, has so far been neglected in the context of self-supervised representation learning. We delve deeper into learning with a momentum encoder <ref type="bibr" target="#b31">[30]</ref> and show evidence that harder negatives are required to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing approaches, we propose hard negative mixing, i.e. feature-level mixing for hard negative samples, that can be computed on-the-fly with a minimal computational overhead. We refer to the proposed approach as MoCHi, that stands for "(M)ixing (o)f (C)ontrastive (H)ard negat(i)ves".</p><p>A toy example of the proposed hard negative mixing strategy is presented in Figure <ref type="figure">1</ref>; it shows a t-SNE <ref type="bibr" target="#b44">[43]</ref> plot after running MoCHi on 32-dimensional random embeddings on the unit hypersphere. We see that for each positive query (red square), the memory (gray marks) contains many easy negatives and few hard ones, i.e. many of the negatives are too far to contribute to the contrastive loss. We propose to mix only the hardest negatives (based on their similarity to the query) and synthesize new, hopefully also hard but more diverse, negative points (blue triangles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. a)</head><p>We delve deeper into a top-performing contrastive self-supervised learning method <ref type="bibr" target="#b31">[30]</ref> and observe the need for harder negatives; b) We propose hard negative mixing, i.e. to synthesize hard negatives directly in the embedding space, on-the-fly, and adapted to each positive query. We propose to both mix pairs of the hardest existing negatives, as well as mixing the hardest negatives with the query itself; c) We exhaustively ablate our approach and show that employing hard negative mixing improves both the generalization of the visual representations learned (measured via their transfer learning performance), as well as the utilization of the embedding space, for a wide range of hyperparameters; d) We report competitive results for linear classification, object detection and instance segmentation, and further show that our gains over a state-of-the-art method are higher when pre-training for fewer epochs, i.e. MoCHi learns transferable representations faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Most early self-supervised learning methods are based on devising proxy classification tasks that try to predict the properties of a transformation (e.g. rotations, orderings, relative positions or channels) applied on a single image <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b49">48]</ref>. Instance discrimination <ref type="bibr" target="#b78">[77]</ref> and CPC <ref type="bibr" target="#b50">[49]</ref> were among the first papers to use contrastive losses for self-supervised learning. The last few months have witnessed a surge of successful approaches that also use contrastive learning losses. These include MoCo <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">30]</ref>, SimCLR <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, PIRL <ref type="bibr" target="#b47">[46]</ref>, CMC <ref type="bibr" target="#b65">[64]</ref> or SvAV <ref type="bibr" target="#b9">[10]</ref>. In parallel, methods like <ref type="bibr">[5, 8-10, 89, 40]</ref> build on the idea that clusters should be formed in the feature spaces, and use clustering losses together with contrastive learning or transformation prediction tasks.</p><p>Most of the top-performing contrastive methods leverage data augmentations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b65">64]</ref>. As revealed by recent studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b66">65,</ref><ref type="bibr" target="#b74">73]</ref>, heavy data augmentations applied to the same image are crucial in learning useful representations, as they modulate the hardness of the self-supervised task via the positive pair. Our proposed hard negative mixing technique, on the other hand, is changing the hardness of the proxy task from the side of the negatives. As we show in the following sections, by mixing harder negatives, we learn a more uniform embedding space and highly transferable representations.</p><p>A handful of works discuss issues around the selection of negatives in contrastive self-supervised learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b77">76,</ref><ref type="bibr" target="#b79">78]</ref>. Iscen et al. <ref type="bibr" target="#b34">[33]</ref> mine hard negatives from a large set by focusing on the features that are neighbors with respect to the Euclidean distance, but not when using a manifold distance defined over the nearest neighbor graph. Interested in approximating the underlying "true" distribution of negative examples, Chuang et al. <ref type="bibr" target="#b14">[15]</ref> present a debiased version of the contrastive loss, in an effort to mediate the effect of false negatives. In their parametric instance classification framework, Cao et al. <ref type="bibr" target="#b6">[7]</ref> propose a weight update correction for negative samples, to decrease GPU memory consumption caused by weight decay regularization. Finally, Wu et al. <ref type="bibr" target="#b77">[76]</ref> present a variational extension to the InfoNCE objective that is further coupled with modified strategies for negative sampling, e.g. restricting the negative sampling distribution to a region around the query.</p><p>Mixing for contrastive learning. Mixup <ref type="bibr" target="#b86">[85]</ref> and its numerous variants <ref type="bibr" target="#b57">[56,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr">71,</ref><ref type="bibr" target="#b84">83]</ref> have been shown to be highly effective data augmentation strategies when paired with a cross-entropy loss for supervised and semi-supervised learning. Manifold mixup <ref type="bibr" target="#b70">[69]</ref> is a feature-space regularizer that encourages networks to be less confident for interpolations of hidden states. The benefits of interpolating have only recently been explored for losses other than cross-entropy <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b57">56]</ref>. In <ref type="bibr" target="#b57">[56]</ref>, the authors propose using mixup in the image/pixel space for self-supervised learning; in contrast, we create query-specific synthetic points on-the-fly in the embedding space. This makes our method way more computationally efficient and able to show improved results at a smaller number of epochs. The Embedding Expansion <ref type="bibr" target="#b36">[35]</ref> work explores interpolating between embeddings for supervised metric learning on fine-grained recognition tasks. The authors use uniform interpolation between two positive and negative points, create a set of synthetic points and then select the hardest pair as negative. In contrast, the proposed MoCHi has no need for class annotations, performs no selection for negatives and only samples a single random interpolation between multiple pairs. What is more, in this paper we go beyond mixing negatives and propose mixing the positive with negative features, to get even harder negatives, and achieve improved performance. Our work is also related to metric learning works that employ generators <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87]</ref>. Apart from not requiring labels, our method exploits the memory component, something not present in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87]</ref>. It has no extra parameters or loss terms that need to be optimized.</p><p>3 Understanding hard negatives in unsupervised contrastive learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contrastive learning with memory</head><p>Let f be an encoder, i.e. a CNN for visual representation learning, that transforms an input image x to an embedding (or feature) vector z = f (x), z ∈ R d . Further let Q be a "memory bank" of size K, i.e. a set of K embeddings in R d . Let the query q and key k embeddings form the positive pair, which is contrasted with every feature n in the bank of negatives (Q) also called the queue in <ref type="bibr" target="#b31">[30]</ref>. A popular and highly successful loss function for contrastive learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b65">64]</ref> is the following:</p><formula xml:id="formula_0">L q,k,Q = − log exp(q T k/τ ) exp(q T k/τ ) + n∈Q exp(q T n/τ ) , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where τ is a temperature parameter and all embeddings are 2 -normalized. In a number of recent successful approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b66">65]</ref> the query and key are the embeddings of two augmentations of the same image. The memory bank Q contains negatives for each positive pair, and may be defined as an "external" memory containing every other image in the dataset <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b78">77]</ref>, a queue of the last batches <ref type="bibr" target="#b31">[30]</ref>, or simply be every other image in the current minibatch <ref type="bibr" target="#b10">[11]</ref>.</p><p>The log-likelihood function of Eq (1) is defined over the probability distribution created by applying a softmax function for each input/query q. Let p zi be the matching probability for the query and feature z i ∈ Z = Q ∪ {k}, then the gradient of the loss with respect to the query q is given by:</p><formula xml:id="formula_2">∂L q,k,Q ∂q = − 1 τ   (1 − p k ) • k − n∈Q p n • n   , where p zi = exp(q T z i /τ ) j∈Z exp(q T z j /τ ) ,<label>(2)</label></formula><p>and p k , p n are the matching probability of the key and negative feature, i.e. for z i = k and for z i = n, respectively. We see that the contributions of the positive and negative logits to the loss are identical to the ones for a (K + 1)-way cross-entropy classification loss, where the logit for the key corresponds to the query's latent class <ref type="bibr" target="#b1">[2]</ref> and all gradients are scaled by 1/τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hard negatives in contrastive learning</head><p>Hard negatives are critical for contrastive learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b58">57,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b82">81]</ref>. Sampling negatives from the same batch leads to a need for larger batches <ref type="bibr" target="#b10">[11]</ref> while sampling negatives from a memory bank that contains every other image in the dataset requires the time consuming task of keeping a large memory up-to-date <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b78">77]</ref>. In the latter case, a trade-off exists between the "freshness" of the memory bank representations and the computational overhead for re-computing them as the encoder keeps changing. The Momentum Contrast (or MoCo) approach of He et al. <ref type="bibr" target="#b31">[30]</ref> offers a compromise between the two negative sampling extremes: it keeps a queue of the latest K features from the last batches, encoded with a second key encoder that trails the (main/query) encoder with a much higher momentum. For MoCo, the key feature k and all features in Q are encoded with the key encoder.</p><p>How hard are MoCo negatives? In MoCo <ref type="bibr" target="#b31">[30]</ref> (resp. SimCLR <ref type="bibr" target="#b10">[11]</ref>) the authors show that increasing the memory (resp. batch) size, is crucial to getting better and harder negatives. In Figure <ref type="figure">2a</ref> we visualize how hard the negatives are during training for MoCo-v2, by plotting the highest 1024 matching probabilities p i for ImageNet-100<ref type="foot" target="#foot_0">1</ref> and a queue of size K = 16k. We see that, although in the beginning of training (i.e. at epoch 0) the logits are relatively flat, as training progresses, fewer and fewer negatives offer significant contributions to the loss. This shows that most of the memory negatives are practically not helping a lot towards learning the proxy task.</p><p>On the difficulty of the proxy task. For MoCo <ref type="bibr" target="#b31">[30]</ref>, SimCLR <ref type="bibr" target="#b10">[11]</ref>, InfoMin <ref type="bibr" target="#b66">[65]</ref>, and other approaches that learn augmentation-invariant representations, we suspect the hardness of the proxy task to be directly correlated with the difficulty of the transformations set, i.e. hardness is modulated via the positive pair. We propose to experimentally verify this. In Figure <ref type="figure">2b</ref>, we plot the proxy task performance, i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type="bibr" target="#b31">[30]</ref> and MoCo-v2 <ref type="bibr" target="#b12">[13]</ref>. MoCo-v2 enjoys a high performance gain over MoCo by three main changes: the addition of a Multilayer Perceptron (MLP) head, cosine learning rate schedule, and more challenging data augmentation. As we further discuss in the Appendix, only the latter of these three changes makes the proxy task harder to solve. Despite the drop in proxy task performance, however, further performance gains are observed for linear classification. In Section 4 we discuss how MoCHi gets a similar effect by modulating the proxy task through mixing harder negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A class oracle-based analysis</head><p>In this section, we analyze the negatives for contrastive learning using a class oracle, i.e. the ImageNet class label annotations. Let us define false negatives (FN) as all negative features in the memory Q, that correspond to images of the same class as the query. Here we want to first quantify false negatives from contrastive learning and then explore how they affect linear classification performance. What is more, by using class annotations, we can train a contrastive self-supervised learning oracle, where we measure performance at the downstream task (linear classification) after disregarding FN from the negatives of each query during training. This has connections to the recent work of <ref type="bibr" target="#b35">[34]</ref>, where a contrastive loss is used in a supervised way to form positive pairs from images sharing the same label. Unlike <ref type="bibr" target="#b35">[34]</ref>, our oracle uses labels only for discarding negatives with the same label for each query, i.e. without any other change to the MoCo protocol.</p><p>In Figure <ref type="figure">2c</ref>, we quantify the percentage of false negatives for the oracle run and MoCo-v2, when looking at highest 1024 negative logits across training epochs. We see that a) in all cases, as representations get better, more and more FNs (same-class logits) are ranked among the top; b) by discarding them from the negatives queue, the class oracle version (purple line) is able to bring same-class embeddings closer. Performance results using the class oracle, as well as a supervised upper bound trained with cross-entropy are shown in the bottom section of Figure <ref type="figure">1</ref>. We see that the MoCo-v2 oracle recovers part of the performance relative to the supervised case, i.e. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature space mixing of hard negatives</head><p>In this section we present an approach for synthesizing hard negatives, i.e. by mixing some of the hardest negative features of the contrastive loss or the hardest negatives with the query. We refer to the proposed hard negative mixing approach as MoCHi, and use the naming convention MoCHi (N , s, s ), that indicates the three important hyperparameters of our approach, to be defined below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mixing the hardest negatives</head><p>Given a query q, its key k and negative/queue features n ∈ Q from a queue of size K, the loss for the query is composed of logits l(z i ) = q T z i /τ fed into a softmax function. Let Q = {n 1 , . . . , n K } be the ordered set of all negative features, such that: l(n i ) &gt; l(n j ), ∀i &lt; j, i.e. the set of negative features sorted by decreasing similarity to that particular query feature.</p><p>For each query, we propose to synthesize s hard negative features, by creating convex linear combinations of pairs of its "hardest" existing negatives. We define the hardest negatives by truncating the ordered set Q, i.e. only keeping the first N &lt; K items. Formally, let H = {h 1 . . . , h s } be the set of synthetic points to be generated. Then, a synthetic point h k ∈ H, would be given by:</p><formula xml:id="formula_3">h k = hk hk 2 , where hk = α k n i + (1 − α k )n j ,<label>(3)</label></formula><p>n i , n j ∈ QN are randomly chosen negative features from the set QN = {n 1 , . . . , n N } of the closest N negatives, α k ∈ (0, 1) is a randomly chosen mixing coefficient and • 2 is the 2 -norm. After mixing, the logits l(h k ) are computed and appended as further negative logits for query q. The process repeats for each query in the batch. Since all other logits l(z i ) are already computed, the extra computational cost only involves computing s dot products between the query and the synthesized features, which would be computationally equivalent to increasing the memory by s &lt;&lt; K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mixing for even harder negatives</head><p>As we are creating hard negatives by convex combinations of the existing negative features, and if one disregards the effects of the 2 -normalization for the sake of this analysis, the generated features will lie inside the convex hull of the hardest negatives. Early during training, where in most cases there is no linear separability of the query with the negatives, this synthesis may result in negatives much harder than the current. As training progresses, and assuming that linear separability is achieved, synthesizing features this way does not necessarily create negatives harder than the hardest ones present; it does however still stretch the space around the query, pushing the memory negatives further and increasing the uniformity of the space (see Section 4.3). This space stretching effect around queries is also visible in the t-SNE projection of Figure <ref type="figure">1</ref>.</p><p>To explore our intuition to the fullest, we further propose to mix the query with the hardest negatives to get even harder negatives for the proxy task. We therefore further synthesize s synthetic hard negative features for each query, by mixing its feature with a randomly chosen feature from the hardest negatives in set QN . Let H = {h 1 . . . , h s } be the set of synthetic points to be generated by mixing the query and negatives, then, similar to Eq. ( <ref type="formula" target="#formula_3">3</ref>), the synthetic points</p><formula xml:id="formula_4">h k = h k / h k 2 , where h k = β k q + (1 − β k )n j</formula><p>, and n j is a randomly chosen negative feature from QN , while β k ∈ (0, 0.5) is a randomly chosen mixing coefficient for the query. Note that β k &lt; 0.5 guarantees that the query's contribution is always smaller than the one of the negative. Same as for the synthetic features created in Section 4.1, the logits l(h k ) are computed and added as further negative logits for query q. Again, the extra computational cost only involves computing s dot products between the query and negatives. In total, the computational overhead of MoCHi is essentially equivalent to increasing the size of the queue/memory by s + s &lt;&lt; K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion and analysis of MoCHi</head><p>Recent approaches like <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> use a Multi-layer Perceptron (MLP) head instead of a linear layer for the embeddings that participate in the contrastive loss. This means that the embeddings whose dot products contribute to the loss, are not the ones used for target tasks-a lower-layer embedding is used instead. Unless otherwise stated, we follow <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> and use a 2-layer MLP head on top of the features we use for downstream tasks. We always mix hard negatives in the space of the loss.</p><p>Is the proxy task more difficult? Figure <ref type="figure">2b</ref> shows the proxy task performance for two variants of MoCHi, when the synthetic features are included (lines with no marker) and without (lines with triangle marker). We see that when mixing only pairs of negatives (s = 0, green lines), the model does learn faster, but in the end the proxy task performance is similar to the baseline case. In fact, as features converge, we see that max l(h k ) &lt; max l(n j ), h k ∈ H, n j ∈ QN . This is however not the case when synthesizing negatives by further mixing them with the query. As we see from Figure <ref type="figure">2b</ref>, at the end of training, max l(h k ) &gt; max l(n j ), h k ∈ H , i.e. although the final performance for the proxy task when discarding the synthetic negatives is similar to the MoCo-v2 baseline (red line with triangle marker), when they are taken into account, the final performance is much lower (red line without markers). Through MoCHi we are able to modulate the hardness of the proxy task through the hardness of the negatives; in the next section we experimentally ablate that relationship.</p><p>Oracle insights for MoCHi. From Figure <ref type="figure">2c</ref> we see that the percentage of synthesized features obtained by mixing two false negatives (lines with square markers) increases over time, but remains very small, i.e. around only 1%. At the same time, we see that about 8% of the synthetic features are fractionally false negatives (lines with triangle markers), i.e. at least one of its two components is a false negative. For the oracle variants of MoCHi, we also do not allow false negatives to participate in synthesizing hard negatives. From Table <ref type="table" target="#tab_2">1</ref> we see that not only the MoCHi oracle is able to get a higher upper bound (82.5 vs 81.8 for MoCo-v2), further closing the difference to the cross entropy upper bound, but we also show in Table <ref type="table">3</ref> that, after longer training, the MoCHi oracle is able to recover most of the performance loss versus using cross-entropy, i.e. It is noteworthy that by the end of training, MoCHi exhibits slightly lower percentage of false negatives in the top logits compared to MoCo-v2 (rightmost values of Figure <ref type="figure">2c</ref>). This is an interesting result: MoCHi adds synthetic negative points that are (at least partially) false negatives and is pushing embeddings of the same class apart, but at the same time it exhibits higher performance for linear classification on ImageNet-100. That is, it seems that although the absolute similarities of same-class features may decrease, the method results in a more linearly separable space. This inspired us to further look into how having synthetic hard negatives impacts the utilization of the embedding space.</p><p>Measuring the utilization of the embedding space. Very recently, Wang and Isola <ref type="bibr" target="#b74">[73]</ref> presented two losses/metrics for assessing contrastive learning representations. The first measures the alignment of the representation on the hypersphere, i.e. the absolute distance between representations with the same label. The second measures the uniformity of their distribution on the hypersphere, through measuring the logarithm of the average pairwise Gaussian potential between all embeddings. In Figure <ref type="figure" target="#fig_2">3c</ref>, we plot these two values for a number of models, when using features from all images in the ImageNet-100 validation set. We see that MoCHi highly improves the uniformity of the representations compared to both MoCo-v2 and the supervised models. This further supports our hypothesis that MoCHi allows the proxy task to learn to better utilize the embedding space. In fact, we see that the supervised model leads to high alignment but very low uniformity, denoting features targeting the classification task. On the other hand, MoCo-v2 and MoCHi have much better spreading of the underlying embedding space, which we experimentally know leads to more generalizable representations, i.e. both MoCo-v2 and MoCHi outperform the supervised ImageNet-pretrained backbone for transfer learning (see Figure <ref type="figure" target="#fig_2">3c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top1 % (±σ) diff (%)</p><p>MoCo <ref type="bibr">[</ref>  <ref type="figure" target="#fig_3">4</ref> in <ref type="bibr" target="#b57">[56]</ref>. The parameters of MoCHi are (N, s, s ).</p><p>We learn representations on two datasets, the common ImageNet-1K <ref type="bibr" target="#b56">[55]</ref>, and its smaller ImageNet-100 subset, also used in <ref type="bibr" target="#b57">[56,</ref><ref type="bibr" target="#b65">64]</ref>. All runs of MoCHi are based on MoCo-v2. We developed our approach on top of the official public implementation of MoCo-v2 <ref type="foot" target="#foot_1">2</ref> and reproduced it on our setup; other results are copied from the respective papers. We run all experiments on 4 GPU servers. For linear classification on ImageNet-100 (resp. ImageNet-1K), we follow the common protocol and report results on the validation set. We report performance after learning linear classifiers for 60 (resp. 100) epochs, with an initial learning rate of 10.0 (30.0), a batch size of 128 (resp. 512) and a step learning rate schedule that drops at epochs 30, 40 and 50 (resp. <ref type="bibr" target="#b61">60,</ref><ref type="bibr" target="#b81">80)</ref>. For training we use K = 16k (resp. K = 65k). For MoCHi, we also have a warm-up of 10 (resp. 15) epochs, i.e. for the first epochs we do not synthesize hard negatives. For ImageNet-1K, we report accuracy for a single-crop testing. For object detection on PASCAL VOC <ref type="bibr" target="#b18">[19]</ref> we follow <ref type="bibr" target="#b31">[30]</ref> and fine-tune a Faster R-CNN <ref type="bibr" target="#b55">[54]</ref>, R50-C4 on trainval07+12 and test on test2007. We use the open-source detectron2<ref type="foot" target="#foot_2">3</ref> code and report the common AP, AP50 and AP75 metrics. Similar to <ref type="bibr" target="#b31">[30]</ref>, we do not perform hyperparameter tuning for the object detection task. See the Appendix for more implementation details.</p><p>A note on reporting variance in results. It is unfortunate that many recent self-supervised learning papers do not discuss variance ; in fact only papers from highly resourceful labs <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b66">65]</ref> report averaged results, but not always the variance. This is generally understandable, as e.g. training and evaluating a ResNet-50 model on ImageNet-1K using 4 V100 GPUs take about 6-7 days. In this paper, we tried to verify the variance of our approach for a) self-supervised pre-training on ImageNet-100, i.e. we measure the variance of MoCHi runs by training a model multiple times from scratch (Table <ref type="table" target="#tab_2">1</ref>), and b) the variance in the fine-tuning stage for PASCAL VOC and COCO (Tables <ref type="table" target="#tab_4">1, 2</ref>). It was unfortunately computationally infeasible for us to run multiple MoCHi pre-training runs for ImageNet-1K. In cases where standard deviation is presented, it is measured over at least 3 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablations and results</head><p>We performed extensive ablations for the most important hyperparameters of MoCHi on ImageNet-100 and some are presented in Figures <ref type="figure" target="#fig_2">3a and 3b</ref>, while more can be found in the Appendix. In general we see that multiple MoCHi configurations gave consistent gains over the MoCo-v2 baseline <ref type="bibr" target="#b12">[13]</ref> for linear classification, with the top gains presented in Figure <ref type="figure">1</ref> (also averaged over 3 runs). We further show performance for different values of N and s in Figure <ref type="figure" target="#fig_2">3a</ref> and a table of gains for N = 1024 in  Figure <ref type="figure" target="#fig_2">3b</ref>; we see that a large number of MoCHi combinations give consistent performance gains. Note that the results in these two tables are not averaged over multiple runs (for MoCHi combinations where we had multiple runs, only the first run is presented for fairness). In other ablations (see Appendix), we see that MoCHi achieves gains (+0.7%) over MoCo-v2 also when training for 100 epochs. Table <ref type="table" target="#tab_2">1</ref> presents comparisons between the best-performing MoCHi variants and reports gains over the MoCo-v2 baseline. We also compare against the published results from <ref type="bibr" target="#b57">[56]</ref> a recent method that uses mixup in pixel space to synthesize harder images.</p><p>Comparison with the state of the art on ImageNet-1K, PASCAL VOC and COCO. In Table <ref type="table" target="#tab_2">1</ref> we present results obtained after training on the ImageNet-1K dataset. Looking at the average negative logits plot and because both the queue and the dataset are about an order of magnitude larger for this training dataset we mostly experiment with smaller values for N than in ImageNet-100. Our main observations are the following: a) MoCHi does not show performance gains over MoCo-v2 for linear classification on ImageNet-1K. We attribute this to the biases induced by training with hard negatives on the same dataset as the downstream task: Figures <ref type="figure" target="#fig_2">3c and 2c</ref> show how hard negative mixing reduces alignment and increases uniformity for the dataset that is used during training. MoCHi still retains state-of-the-art performance. b) MoCHi helps the model learn faster and achieves high performance gains over MoCo-v2 for transfer learning after only 100 epochs of training. c) The harder negative strategy presented in Section 4.2 helps a lot for shorter training. d) In 200 epochs MoCHi can achieve performance similar to MoCo-v2 after 800 epochs on PASCAL VOC. e) From all the MoCHi runs reported in Table <ref type="table" target="#tab_2">1</ref> as well as in the Appendix, we see that performance gains are consistent across multiple hyperparameter configurations.</p><p>In Table <ref type="table" target="#tab_4">2</ref> we present results for object detection and semantic segmentation on the COCO dataset <ref type="bibr" target="#b42">[41]</ref>. Following He et al. <ref type="bibr" target="#b31">[30]</ref>, we use Mask R-CNN <ref type="bibr" target="#b29">[29]</ref> with a C4 backbone, with batch normalization tuned and synchronize across GPUs. The image scale is in [640, 800] pixels during training and is 800 at inference. We fine-tune all layers end-to-end on the train2017 set (118k images) and evaluate on val2017. We adopt feature normalization as in <ref type="bibr" target="#b31">[30]</ref> when fine-tuning. MoCHi and MoCo use the same hyper-parameters as the ImageNet supervised counterpart (i.e. we did not do any method-specific tuning). From Table <ref type="table" target="#tab_4">2</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we analyze a state-of-the-art method for self-supervised contrastive learning and identify the need for harder negatives. Based on that observation, we present a hard negative mixing approach that is able to improve the quality of representations learned in an unsupervised way, offering better transfer learning performance as well as a better utilization of the embedding space. What is more, we show that we are able to learn generalizable representations faster, something important considering the high compute cost of self-supervised learning. Although the hyperparameters needed to get maximum gains are specific to the training set, we find that multiple MoCHi configurations provide considerable gains, and that hard negative mixing consistently has a positive effect on transfer learning performance. Finally, it is worth noting that our approach could further be implemented on top of any contrastive learning loss that involves a set of negatives, e.g. the n-pair loss <ref type="bibr" target="#b59">[58]</ref>, and for tasks beyond self-supervised learning, e.g. deep metric learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Self-supervised tasks and dataset bias. Prominent voices in the field advocate that self-supervised learning will play a central role during the next years in the field of AI. Not only representations learned using self-supervised objectives directly reflect the biases of the underlying dataset, but also it is the responsibility of the scientist to explicitly try to minimize such biases. Given that, the larger the datasets, the harder it is to properly investigate biases in the corpus, we believe that notions of fairness need to be explicitly tackled during the self-supervised optimization, e.g. by regulating fairness on protected attributes. This is especially important for systems whose decisions affect humans and/or their behaviours.</p><p>Self-supervised learning, compute and impact on the environment. On the one hand, selfsupervised learning involves training large models on very large datasets, on long periods of time. As we also argue in the main paper, the computational cost of every self-supervised learning paper is very high: pre-training for 200 epochs on the relatively small ImageNet-1K dataset requires around 24 GPU days (6 days on 4 GPUs).</p><p>In this paper we show that, by mining harder negatives, one can get higher performance after training for fewer epochs; we believe that it is indeed important to look deeper into self-supervised learning approaches that utilize the training dataset and learn generalizable representations faster.</p><p>Looking at the bigger picture, however, we believe that research in self-supervised learning is highly justified in the long run, despite its high computational cost, for two main reasons. First, the goal of self-supervised learning is to produce models whose representations generalize better and are therefore potentially useful for many subsequent tasks. Hence, having strong models pre-trained by self-supervision would reduce the environmental impact of deploying to multiple new downstream tasks. Second, representations learned from huge corpora have been shown to improve results when directly fine-tuned, or even used as simple feature extractors, on smaller datasets. Most socially minded applications and tasks fall in this situation where they have to deal with limited annotated sets, because of a lack of funding, hence they would directly benefit from making such pretraining models available. Given the considerable budget required for large, high quality datasets, we foresee that strong generalizable representations will greatly benefit socially mindful tasks more than e.g. a multi-billion dollar industry application, where the funding to get large clean datasets already exists. Following our discussion in Section 3.2, we wanted to verify that hardness of the proxy task for MoCo <ref type="bibr" target="#b31">[30]</ref> is directly correlated to the difficulty of the transformations set, i.e. proxy task hardness can modulated via the positive pair. In Figure <ref type="figure" target="#fig_3">4</ref>, we plot the proxy task performance, i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type="bibr" target="#b31">[30]</ref>, MoCo-v2 <ref type="bibr" target="#b12">[13]</ref> and some variants inbetween. In Figure <ref type="figure" target="#fig_3">4</ref>, we track the proxy task performance when progressively moving from MoCo to MoCo-v2, i.e. a) switching to a cosine learning rate schedule (gray line-no noticeable change in performance after 200 epochs); b) adding a Multilayer Perceptron head (cyan line-no noticeable change in performance after 200 epochs); c) adding a more challenging data augmentation (green line-a drop in proxy task performance). The latter is equivalent to MoCo-v2. For completeness we further show a MoCHi run with a large number of synthetic features (red line-large drop in in proxy task performance).</p><p>It is worth noting that the temperature τ of the softmax is a hyper-parameter that highly affects the rate of learning and therefore performance in the proxy task. As mentioned above, all results in Figure <ref type="figure" target="#fig_3">4</ref> are for the same τ = 0.2. One can contrast performance of MoCo in this figure to Figure <ref type="figure">2b</ref>, where we used the (default) value of τ = 0.07.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Hard negative mixing variants not discussed in the main text</head><p>While developing MoCHi, we considered a number of different mixing strategies in feature space. Many of those resulted in lower performance while others performed on par but were unnecessarily more complicated. We found the two strategies presented in Sections 4.1 and 4.2 of the main paper to be both the best performing and also complementary. Here, we briefly mention some other ideas that didn't make the cut.</p><p>Mixing using keys instead of queries. For MoCHi, the "top" negatives are defined via the negative logits, i.e. how far each memory negative is to a query. We also experimented when the ranking comes from a key, i.e. using the key to define the ordering of the set Q. We ablated this for both when mixing pairs of negative as well as when mixing the query with negatives. In the vast majority of the cases, results were on average about 0.2% lower, across multiple configurations. Note that this would also involve having to compute the dot products of the key with the memory, something that would induce further computational overhead.</p><p>Mixing keys with negatives. For MoCHi, in Section 4.2 we propose to synthesize s synthetic hard negative features for each query, by mixing its feature with a randomly chosen feature from the hardest negatives in set QN . We could also create such negatives by mixing the key the same way, i.e. the synthetic points created this way would be given by h k = h / h k 2 , where h k = β k k+(1−β k )n j , and n j is a randomly chosen negative feature from QN , while β k ∈ (0, 0.5) is a randomly chosen mixing coefficient for the key. Ablations showed that this yields at best performance as good as mixing with the query, but on average about 0.1-0.2% lower.</p><p>Weighted contributions for the logits of h . We also tried weighing the contributions of the MoCHi samples according to the percentage of the query they have. That is, the logits of each hard negative h k was scaled by β k to reflect how "negative" this point is. This weighing scheme also resulted in slightly inferior results.</p><p>Sampling negatives non-uniformly. We also experimented when sampling negatives with a probability defined over a function of the logit values. That is, we defined a probability function by adding a softmax on top of the top N negatives, with a τ hyper-parameter. Although we would want to further investigate and thoroughly ablate this approach, early experiments showed that the hard negatives created this way are so hard that linear classification performance decreases by a lot (10-30% for different values of τ ).</p><p>Fixing the percentage of hard negatives in the top-k logits. In an effort to reduce our hyperparameters, we run preliminary experiments on a variant where instead of s and s , we synthesize hard negatives sequentially for each query by alternate between the two mixing methods (i.e. mixing two negatives and mixing the query with one negative) up until X% of the top-N logits correspond to synthetic features. Although encouraging, quantitative results on linear classification were inconclusive; we would however want to further investigate this in the future jointly with curriculum strategies that would decrease this percentage over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Mixing hard negatives vs altering the temperature of the softmax</head><p>Another way of making the contrastive loss more or less "peaky" is through the temperature parameter τ of the softmax function; we see from Eq (2) that the gradients of the loss are scaled by 1/τ . One would therefore assume that tuning this parameter could effectively tune the hardness and speed of learning. One can see MoCHi as a way of going beyond one generic temperature parameter; we start with the best performing, cross-validated τ and generate different negatives adapted to each query, and therefore have adaptive learning for each query that further evolves at each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C More experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Experimental protocol</head><p>In general and unless otherwise stated, we use the default hyperparameters from the official implementation <ref type="foot" target="#foot_4">5</ref> for MoCo-v2. We follow Chen et al. <ref type="bibr" target="#b12">[13]</ref> and use a cosine learning rate schedule during self-supervised pre-training. For both pretraining datasets the initial learning rate is set to 0.03, while the batchsize is 128 for ImageNet-100 and 256 for ImageNet-1K. We train on 4 GPU servers. We further want to note here that, because of the computational cost of self-supervised pre Imagenet. The ImageNet-1K data can be downloaded from this link <ref type="foot" target="#foot_5">6</ref> while the 100 synsets/classes of ImageNet-100 are presented below. For ImageNet-1K the training set is 1.2M images from 1000 categories, while the validation set contains 50 images from each class, i.e. 50,000 images in total.</p><p>ImageNet-100. ImageNet-100 is a subset of ImageNet-1K that consists of the 100 classes presented right below. It was first used in Tian et al. <ref type="bibr" target="#b65">[64]</ref> and recently also used in Shen et al. <ref type="bibr" target="#b57">[56]</ref>. The synsets of ImageNet-100 are: n02869837 n01749939 n02488291 n02107142 n13037406 n02091831 n04517823 n04589890 n03062245 n01773797 n01735189 n07831146 n07753275 n03085013 n04485082 n02105505 n01983481 n02788148 n03530642 n04435653 n02086910 n02859443 n13040303 n03594734 n02085620 n02099849 n01558993 n04493381 n02109047 n04111531 n02877765 n04429376 n02009229 n01978455 n02106550 n01820546 n01692333 n07714571 n02974003 n02114855 n03785016 n03764736 n03775546 n02087046 n07836838 n04099969 n04592741 n03891251 n02701002 n03379051 n02259212 n07715103 n03947888 n04026417 n02326432 n03637318 n01980166 n02113799 n02086240 n03903868 n02483362 n04127249 n02089973 n03017168 n02093428 n02804414 n02396427 n04418357 n02172182 n01729322 n02113978 n03787032 n02089867 n02119022 n03777754 n04238763 n02231487 n03032252 n02138441 n02104029 n03837869 n03494278 n04136333 n03794056 n03492542 n02018207 n04067472 n03930630 n03584829 n02123045 n04229816 n02100583 n03642806 n04336792 n03259280 n02116738 n02108089 n03424325 n01855672 n02090622 PASCAL VOC. For the experiments on PASCAL VOC we use the setup and config files described in MoCo's detectron2 code <ref type="foot" target="#foot_6">7</ref> . The PASCAL VOC dataset can be downloaded from this link <ref type="foot" target="#foot_7">8</ref> . As mentioned in the main text, we fine-tune a Faster R-CNN <ref type="bibr" target="#b55">[54]</ref>, R50-C4 on trainval07+12 and test on test2007. Details on the splits can be found here <ref type="foot" target="#foot_8">9</ref> for the 2007 part and here <ref type="foot" target="#foot_9">10</ref> for the 2012 part.</p><p>COCO. We similar to PASCAL VOC, we build on top of MoCo's detectron2 code. We fine-tune all layers end-to-end on the train2017 set (118k images) and evaluate on val2017. The image scale is in [640, 800] pixels during training and is 800 at inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 More ablations and results on ImageNet-100</head><p>Table <ref type="table">3</ref> presents a superset of the ablations presented in Table <ref type="table" target="#tab_2">1</ref>. Please note that most of the results here are for a single run. Only results that explicitly present standard deviation were averaged over multiple runs. Some further observations from the extended ablations table <ref type="table">:</ref> • From the 100 epoch run results, we see that the gains over MoCo-v2 get larger with longer training. When training longer (see line for 800 epoch pre-training), we see that MoCHi keeps getting a lot stronger, and actually seems to really close the gap even to the supervised case.</p><p>• Looking at the smaller queue ablation, we see that MoCHi can achieve with K=4k performance equal to MoCo-v2 with K=16k.</p><p>• From the runs using "class oracle" (bottom section), i.e. when simply discarding false negatives from the queue, we see that MoCHi comes really close to the supervised case, showing the power of contrastive learning with hard negatives also when labels are present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Results for ImageNet-1K</head><p>In Table <ref type="table">4</ref> we present a superset of the results presented in Table <ref type="table" target="#tab_2">1</ref> for linear classification on ImageNet-1K and PASCAL VOC. We see that, for 200 epoch training performance still remains strong even when N = 64, while the same stands for N = 128 when training for 100 epochs. We also added a couple of recent and concurrent methods in the table, e.g. PCL <ref type="bibr" target="#b41">[40]</ref>, or the clustering approach of <ref type="bibr" target="#b8">[9]</ref>. Both unfortunately use a different setup for PASCAL VOC and their VOC results are not directly comparable. We see however that our performance for linear classification on ImageNet-1K is higher, despite the fact that both methods take into account the class label-based clusters that exist in ImageNet-1K.</p><p>Oracle run for MoCHi. We also present here a (single) run for MoCHi with a class oracle, when training on ImageNet-1K for 1000 epochs. From this very preliminary result we verify that discarding false negatives leads to significantly higher linear classification performance for ImageNet-1K, the training dataset, while at the same time state-of-the-art transfer learning performance on PASCAL VOC is preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D An extended related and concurrent works section</head><p>Although self-supervised learning has been gaining traction for a few years, 2020 is undoubtedly the year when the number of self-supervised learning papers and pre-prints practically exploded. Due to space constraints, it is hard to properly reference all recent related works in the area in Section 2.</p><p>What is more, a large number of concurrent works on contrastive self-supervised learning came out after the first submission of this paper. We therefore present here an extended related work section that complements Section 2 (works mentioned and discussed there are not copied also here), a section that further catalogues a large number of concurrent works.</p><p>Following the success of contrastive self-supervised learning, a number of more theoretical studies have emerged trying to understand the underlying mechanism that make it work so well <ref type="bibr" target="#b68">[67,</ref><ref type="bibr" target="#b74">73,</ref><ref type="bibr" target="#b60">59,</ref><ref type="bibr" target="#b40">39]</ref>, while Mutual Information theory has been the basis and inspiration for a number such studies and self-supervised learning algorithms, e.g. <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b77">76,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">31]</ref> Self-supervised learning based on sequential and multimodal visual data. A number of earlier works that learn representations from videos utilized the sequential nature of the temporal dimension, e.g. future frame prediction and reconstruction <ref type="bibr" target="#b61">[60]</ref>, shuffling and then predicting or verifying the order of frames or clips <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b80">79]</ref>, predicting the "arrow of time" <ref type="bibr" target="#b75">[74]</ref>, pace <ref type="bibr" target="#b73">[72]</ref> or predicting the "odd" element <ref type="bibr" target="#b20">[21]</ref> from a set of clips. Recently, contrastive, memory-based self-supervised learning methods were extended to video representation learning <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" target="#b67">66]</ref>. In an interesting recent study, Purushwalkam and Gupta <ref type="bibr" target="#b53">[52]</ref> study the robustness of contrastive self-supervised learning methods like MoCo <ref type="bibr" target="#b31">[30]</ref> and PIRL <ref type="bibr" target="#b47">[46]</ref> and saw that despite the fact that they learn occlusioninvariant representations, they fail to capture viewpoint and category instance invariance. To remedy that, they present an approach that leverages unstructured videos and leads to higher viewpoint invariance and higher performance for downstream tasks. Another noteworthy paper that learns visual representations in a self-supervised way from video is the work of Emin Orhan et al. <ref type="bibr" target="#b51">[50]</ref>, that utilized an egocentric video dataset recorded from the perspective of several young children and demonstrated the emergence of high-level semantic information.</p><p>A number of works exploit the audio-visual nature of video <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3]</ref> to learn visual representation, e.g. via learning intra-modality synchronization. Apart from audio, other methods have used use automatic extracted text, e.g. from speech transcripts <ref type="bibr" target="#b63">[62,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b45">44]</ref> or surrounding text <ref type="bibr" target="#b23">[24]</ref>.</p><p>Clustering losses. A number of recent works explore representation learning together with clustering losses imposed on the unlabeled dataset they learn on. Although some care about the actual clustering performance on the training dataset <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b54">53,</ref><ref type="bibr" target="#b89">88]</ref>, others further use the clustering losses as means for learning representations that generalize <ref type="bibr">[8-10, 84, 82, 5]</ref>. Following the recent success of contrastive learning approaches, very recently a number of methods try to get the best of both worlds by combining contrastive and clustering losses. Methods like local aggregation <ref type="bibr" target="#b90">[89]</ref>, Prototypical Contrastive learning <ref type="bibr" target="#b41">[40]</ref>, Deep Robust Clustering <ref type="bibr" target="#b89">[88]</ref>, or SwAV <ref type="bibr" target="#b9">[10]</ref> are able to not only create transferable representations, but also are able to reach linear classification accuracy on the training set that is not very far from the supervised case.</p><p>Focusing on the positive pair. Works like SimCLR <ref type="bibr" target="#b10">[11]</ref>, MoCo-v2 <ref type="bibr" target="#b12">[13]</ref> and Tian et al. <ref type="bibr" target="#b66">[65]</ref> make it clear that for contrastive self-supervised learning, selecting a challenging and diverse set of image transformations can highly boost the quality of representations. Recently, papers like SwAV <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b81">80]</ref> demonstrated that even higher gains can be achieved by using multiple augmentations.</p><p>Very recently the BYOL <ref type="bibr" target="#b25">[26]</ref> showed that one can learn transferable visual representations via bootstrapping representations and without negative pairs. Reproducibility studies <ref type="foot" target="#foot_10">11</ref> have shown that batch normalization might play an important role when learning without negatives, preventing mode collapse and helping spread the resulting features in the embedding space. BYOL makes a number of modifications over SimCLR <ref type="bibr" target="#b10">[11]</ref>, e.g. the addition of a target network whose parameter update is lagging similar to MoCo <ref type="bibr" target="#b31">[30]</ref>  Synthesizing for supervised metric learning. Recently, synthesizing negatives was explored in metric learning literature <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87,</ref><ref type="bibr" target="#b36">35]</ref>. Works like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87]</ref> use generators to synthesize negatives in a supervised scenario over common metric learning losses. Apart from not requiring labels, in our case we focus on a specific contrastive loss and exploit its memory component. What is more, and unlike <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">87]</ref>, we do not require a generator, i.e. have no extra parameters or loss terms that need to be optimized. We discuss the relations and differences between MoCHi and the closely related Embedding Expansion <ref type="bibr" target="#b36">[35]</ref> method in Section 2.</p><p>The MoCHi oracle and supervised contrastive learning. A number of recent approaches have explored the connections between supervised and contrastive learning <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b87">86,</ref><ref type="bibr" target="#b43">42]</ref>. Very recently, Khosla et al. <ref type="bibr" target="#b35">[34]</ref> show that training a contrastive loss in a supervised way can lead to improvements even over the ubiquitous cross-entropy loss. Although definitely not the focus and merely a byproduct of the class oracle analysis of this paper, we also show here that MoCo and MoCHi can successfully perform supervised learning for classification, by simply discarding same-class negatives. This is something that is further utilized in <ref type="bibr" target="#b87">[86]</ref>. For MoCHi, we can further ensure that all hard negatives come from other classes. In the bottom section of Table <ref type="table">3</ref> we see that for ImageNet-100, the gap between the cross-entropy and contrastive losses closes more and more with longer contrastive training with harder negatives. An oracle run is also shown in Table <ref type="table">4</ref> for ImageNet-1K after 1000 epochs of training. We see that MoCHi decreases the performance gap to the supervised for linear classification on ImageNet-1K, and performs much better than the supervised pre-training model for object detection on PASCAL. We want to note here that MoCHi oracle experiments on ImageNet-1K are very preliminary, and we leave further explorations on supervised contrastive learning with MoCHi as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>79.0 (MoCHi, 200 epochs) → 82.5 (MoCHi oracle, 200 epochs) → 85.2 (MoCHi oracle, 800 epochs) → 86.2 (supervised).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Accuracy when varying N (x-axis) and s. Accuracy gains over MoCo-v2 when N = 1024. Alignment and uniformity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results on the validation set of ImageNet-100. (a) Top-1 Accuracy for different values of N (x-axis) and s; the dashed black line is MoCo-v2. (b) Top-1 Accuracy gains (%) over MoCo-v2 (top-left cell) when N = 1024 and varying s (rows) and s (columns). (c) Alignment and uniformity metrics [73]. The x/y axes correspond to −L unif orm and −L align , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Proxy task performance over 200 epochs of training on ImageNet-100. For all methods we use the same τ = 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-training, 100 epoch pretraining results are computed from the 100th-epoch checkpoints of a 200 epoch run, i.e. the cosine learning rate schedule still follows a 200 epoch training. Moreover, our longer (800) epoch runs are by restarting training from the 200 epoch run checkpoint, and switching the total number of epochs to 800, i.e., the learning rate jumps back up after epoch 200. C.1.1 Dataset details</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on ImageNet-100 after training for 200 epochs. The bottom section reports results when using a class oracle (see Section 3.3). * denotes reproduced results, ‡ denotes results visually extracted from Figure</figDesc><table><row><cell>30]</cell><cell>73.4</cell><cell></cell></row><row><cell>MoCo + iMix [56]</cell><cell>74.2  ‡</cell><cell>↑0.8</cell></row><row><cell>CMC [64]</cell><cell>75.7</cell><cell></cell></row><row><cell>CMC + iMix [56]</cell><cell>75.9  ‡</cell><cell>↑0.2</cell></row><row><cell>MoCo [30]*</cell><cell>74.0</cell><cell></cell></row><row><cell>MoCo-v2 [13]*</cell><cell>78.0 (±0.2)</cell><cell></cell></row><row><cell>+ MoCHi (1024, 1024, 128)</cell><cell>79.0 (±0.4)</cell><cell>↑1.0</cell></row><row><cell>+ MoCHi (1024, 256, 512)</cell><cell>79.0 (±0.4)</cell><cell>↑1.0</cell></row><row><cell>+ MoCHi (1024, 128, 256)</cell><cell>78.9 (±0.5)</cell><cell>↑0.9</cell></row><row><cell>Using Class Oracle</cell><cell></cell><cell></cell></row><row><cell>MoCo-v2*</cell><cell>81.8</cell><cell></cell></row><row><cell cols="2">+ MoCHi (1024, 1024, 128) 82.5</cell><cell></cell></row><row><cell>Supervised (Cross Entropy)</cell><cell>86.2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>we see that MoCHi displays consistent gains over both the supervised baseline and MoCo-v2, for both 100 and 200 epoch pre-training. In fact, MoCHi is able to reach the AP performance similar to supervised pre-training for instance segmentation (33.2) after only 100 epochs of pre-training.</figDesc><table><row><cell>Method</cell><cell></cell><cell>IN-1k Top1</cell><cell>AP 50</cell><cell>AP</cell><cell>VOC 2007</cell><cell>AP 75</cell></row><row><cell></cell><cell></cell><cell cols="2">100 epoch training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo-v2 [13]*</cell><cell></cell><cell>63.6</cell><cell>80.8 (±0.2)</cell><cell cols="2">53.7 (±0.2)</cell><cell>59.1 (±0.3)</cell></row><row><cell cols="2">+ MoCHi (256, 512, 0)</cell><cell>63.9</cell><cell cols="5">81.1 (±0.1) (↑0.4) 54.3 (±0.3) (↑0.7) 60.2 (±0.1) (↑1.2)</cell></row><row><cell cols="2">+ MoCHi (256, 512, 256)</cell><cell>63.7</cell><cell cols="5">81.3 (±0.1) (↑0.6) 54.6 (±0.3) (↑1.0) 60.7 (±0.8) (↑1.7)</cell></row><row><cell cols="2">+ MoCHi (128, 1024, 512)</cell><cell>63.4</cell><cell cols="5">81.1 (±0.1) (↑0.4) 54.7 (±0.3) (↑1.1) 60.9 (±0.1) (↑1.9)</cell></row><row><cell></cell><cell></cell><cell cols="2">200 epoch training</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">SimCLR [11] (8k batch size, from [13]) 66.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MoCo + Image Mixture [56]</cell><cell>60.8</cell><cell>76.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>InstDis [77]  †</cell><cell></cell><cell>59.5</cell><cell>80.9</cell><cell cols="2">55.2</cell><cell>61.2</cell></row><row><cell>MoCo [30]</cell><cell></cell><cell>60.6</cell><cell>81.5</cell><cell cols="2">55.9</cell><cell>62.6</cell></row><row><cell>PIRL [46]  †</cell><cell></cell><cell>61.7</cell><cell>81.0</cell><cell cols="2">55.5</cell><cell>61.3</cell></row><row><cell>MoCo-v2 [13]</cell><cell></cell><cell>67.7</cell><cell>82.4</cell><cell cols="2">57.0</cell><cell>63.6</cell></row><row><cell>InfoMin Aug. [65]</cell><cell></cell><cell>70.1</cell><cell>82.7</cell><cell cols="2">57.6</cell><cell>64.6</cell></row><row><cell>MoCo-v2 [13]*</cell><cell></cell><cell>67.9</cell><cell>82.5 (±0.2)</cell><cell cols="2">56.8 (±0.1)</cell><cell>63.3 (±0.4)</cell></row><row><cell cols="2">+ MoCHi (1024, 512, 256)</cell><cell>68.0</cell><cell cols="5">82.3 (±0.2) (↓0.2) 56.7 (±0.2) (↓0.1) 63.8 (±0.2) (↑0.5)</cell></row><row><cell cols="2">+ MoCHi (512, 1024, 512)</cell><cell>67.6</cell><cell cols="5">82.7 (±0.1) (↑0.2) 57.1 (±0.1) (↑0.3) 64.1 (±0.3) (↑0.8)</cell></row><row><cell cols="2">+ MoCHi (256, 512, 0)</cell><cell>67.7</cell><cell cols="5">82.8 (±0.2) (↑0.3) 57.3 (±0.2) (↑0.5) 64.1 (±0.1) (↑0.8)</cell></row><row><cell cols="2">+ MoCHi (256, 512, 256)</cell><cell>67.6</cell><cell cols="5">82.6 (±0.2) (↑0.1) 57.2 (±0.3) (↑0.4) 64.2 (±0.5) (↑0.9)</cell></row><row><cell cols="2">+ MoCHi (256, 2048, 2048)</cell><cell>67.0</cell><cell>82.5 (±0.1) ( 0.0)</cell><cell cols="4">57.1 (±0.2) (↑0.3) 64.4 (±0.2) (↑1.1)</cell></row><row><cell cols="2">+ MoCHi (128, 1024, 512)</cell><cell>66.9</cell><cell cols="5">82.7 (±0.2) (↑0.2) 57.5 (±0.3) (↑0.7) 64.4 (±0.4) (↑1.1)</cell></row><row><cell></cell><cell></cell><cell cols="2">800 epoch training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SvAV [10]</cell><cell></cell><cell>75.3</cell><cell>82.6</cell><cell cols="2">56.1</cell><cell>62.7</cell></row><row><cell>MoCo-v2 [13]</cell><cell></cell><cell>71.1</cell><cell>82.5</cell><cell cols="2">57.4</cell><cell>64.0</cell></row><row><cell>MoCo-v2[13]*</cell><cell></cell><cell>69.0</cell><cell>82.7 (±0.1)</cell><cell cols="2">56.8 (±0.2)</cell><cell>63.9 (±0.7)</cell></row><row><cell cols="2">+ MoCHi (128, 1024, 512)</cell><cell>68.7</cell><cell>83.3 (±0.1) (↑0.6)</cell><cell cols="2">57.3 (±0.2) (↑0.5)</cell><cell cols="2">64.2 (±0.4) (↑0.3)</cell></row><row><cell>Supervised [30]</cell><cell></cell><cell>76.1</cell><cell>81.3</cell><cell cols="2">53.5</cell><cell>58.8</cell></row><row><cell>Pre-train</cell><cell>AP bb</cell><cell>AP bb 50</cell><cell>AP bb 75</cell><cell cols="2">AP mk</cell><cell>AP mk 50</cell><cell>AP mk 75</cell></row><row><cell>Supervised [30]</cell><cell>38.2</cell><cell>58.2</cell><cell>41.6</cell><cell cols="2">33.3</cell><cell>54.7</cell><cell>35.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">100 epoch pre-training</cell><cell></cell></row><row><cell>MoCo-v2 [13]*</cell><cell>37.0 (±0.1)</cell><cell>56.5 (±0.3)</cell><cell>39.8 (±0.1)</cell><cell cols="2">32.7 (±0.1)</cell><cell>53.3 (±0.2)</cell><cell>34.3 (±0.1)</cell></row><row><cell>+ MoCHi (256, 512, 0)</cell><cell cols="7">37.5 (±0.1) (↑0.5) 57.0 (±0.1) (↑0.5) 40.5 (±0.2) (↑0.7) 33.0 (±0.1) (↑0.3) 53.9 (±0.2) (↑0.6) 34.9 (±0.1) (↑0.6)</cell></row><row><cell cols="8">+ MoCHi (128, 1024, 512) 37.8 (±0.1) (↑0.8) 57.2 (±0.0) (↑0.7) 40.8 (±0.2) (↑1.0) 33.2 (±0.0) (↑0.5) 54.0 (±0.2) (↑0.7) 35.4 (±0.1) (↑1.1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">200 epoch pre-training</cell><cell></cell></row><row><cell>MoCo [30]</cell><cell>38.5</cell><cell>58.3</cell><cell>41.6</cell><cell cols="2">33.6</cell><cell>54.8</cell><cell>35.6</cell></row><row><cell>MoCo (1B image train) [30]</cell><cell>39.1</cell><cell>58.7</cell><cell>42.2</cell><cell cols="2">34.1</cell><cell>55.4</cell><cell>36.4</cell></row><row><cell>InfoMin Aug. [65]</cell><cell>39.0</cell><cell>58.5</cell><cell>42.0</cell><cell cols="2">34.1</cell><cell>55.2</cell><cell>36.3</cell></row><row><cell>MoCo-v2 [13]*</cell><cell>39.0 (±0.1)</cell><cell>58.6 (±0.1)</cell><cell>41.9(±0.3)</cell><cell cols="2">34.2 (±0.1)</cell><cell>55.4 (±0.1)</cell><cell>36.2 (±0.2)</cell></row><row><cell>+ MoCHi (256, 512, 0)</cell><cell cols="7">39.2 (±0.1) (↑0.2) 58.8 (±0.1) (↑0.2) 42.4 (±0.2) (↑0.5) 34.4 (±0.1) (↑0.2) 55.6 (±0.1) (↑0.2) 36.7 (±0.1) (↑0.5)</cell></row><row><cell cols="8">+ MoCHi (128, 1024, 512) 39.2 (±0.1) (↑0.2) 58.9 (±0.2) (↑0.3) 42.4 (±0.3) (↑0.5) 34.3 (±0.1) (↑0.2) 55.5 (±0.1) (↑0.1) 36.6 (±0.1) (↑0.4)</cell></row><row><cell cols="8">+ MoCHi (512, 1024, 512) 39.4 (±0.1) (↑0.4) 59.0 (±0.1) (↑0.4) 42.7 (±0.1) (↑0.8) 34.5 (±0.0) (↑0.3) 55.7 (±0.2) (↑0.3) 36.7 (±0.1) (↑0.5)</cell></row></table><note>Results for linear classification on ImageNet-1K and object detection on PASCAL VOC with a ResNet-50 backbone. Wherever standard deviation is reported, it refers to multiple runs for the fine-tuning part. For MoCHi runs we also report in parenthesis the difference to MoCo-v2. * denotes reproduced results. † results are copied from<ref type="bibr" target="#b31">[30]</ref>. We bold (resp. underline) the highest results overall (resp. for MoCHi).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Object detection and instance segmentation results on COCO with the ×1 training schedule and a C4 backbone. * denotes reproduced results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>and a predictor MLP. They further use a different optimizer (LARS) and overall report transfer learning results after 1000 epochs with a batchsize of 4096, a setup that is almost impossible to reproduce (the authors claim training takes about 8 hours on 512 Cloud TPU v3 cores). It is hard to directly compare MoCHi to BYOL, as BYOL does not report transfer learning results for the commonly used setup, i.e. after 200 epochs of pre-training. We argue that by employing hard negatives, MoCHi can learn strong transferable representations faster than BYOL.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this section we study contrastive learning for MoCo<ref type="bibr" target="#b31">[30]</ref> on ImageNet-100, a subset of ImageNet consisting of 100 classes introduced in<ref type="bibr" target="#b65">[64]</ref>. See Section 5 for details on the dataset and experimental protocol.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/facebookresearch/moco</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/facebookresearch/detectron2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/SsnL/align_uniform/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/facebookresearch/moco/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">http://image-net.org/challenges/LSVRC/2011/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/facebookresearch/moco/tree/master/detection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">http://host.robots.ox.ac.uk/pascal/VOC/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">http://host.robots.ox.ac.uk/pascal/VOC/voc2007/dbstats.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/dbstats.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10">https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning. html</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices Appendix A Details for the uniformity experiment</head><p>The uniformity experiment in Figure <ref type="figure">3c</ref> is based on Wang and Isola <ref type="bibr" target="#b74">[73]</ref>. We follow the same definitions of the losses/metrics as presented in the paper. The alignment loss is given by: L align (f ; α) := − E (x,y)∼ppos</p><p>while the uniformity loss is given by:</p><p>where α, t are weighting parameters and f is the feature encoder (i.e. minus the MLP head for MoCo-v2 and MoCHi). We set α = 2 and t = 2. All features were L2-normalized, as the metrics are defined on the hypersphere. p pos denotes the joint distribution of pairs of positive samples, and p data is the distribution of the data. Note that p pos is task-specific; here we use the class oracle, i.e. the ImageNet-100 labels, to define the positive samples. We use the publicly available implementation supplied by the authors 4 ; we modify the alignment implementation to reflect the fact that we obtain the positives based on the class oracle. In Figure <ref type="figure">3c</ref> we report the two metrics (−L unif orm and −L align ) for models trained on ImageNet-100 using all embeddings of the validation set.</p><p>Appendix B Further analysis on hard negative mixing   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12667</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A theoretical analysis of contrastive unsupervised representation learning. ICML</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Labelling unlabelled videos from scratch with multi-modal self-supervision</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A critical analysis of self-supervision, or what we can learn from a single image</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Parametric instance classification for unsupervised visual feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14618</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2009">2020. 2, 9</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2009">2020. 1, 2, 3, 4, 6, 7, 9</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020. 1, 2, 4, 6, 7, 9, 15, 16, 19</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Look, listen, and attend: Co-attention network for self-supervised audio-visual representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00224</idno>
		<title level="m">Debiased contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep adversarial metric learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A framework for contrastive self-supervised learning and designing a new approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00104</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Gansbeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised learning of visual features through embedding images into text topic spaces</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rusiñol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Smullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08973</idno>
		<title level="m">Affinity and diversity: Quantifying mechanisms of data augmentation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J.-B</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Memory-augmented dense predictive coding for video representation learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Mask R-CNN</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020. 1, 2, 3, 4, 7, 8, 9, 15, 18, 19</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Representation learning with video deep infomax</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mining on manifolds: Metric learning without labels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<title level="m">Supervised contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Embedding expansion: Augmentation in embedding space for deep metric learning. CVPR</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 3, 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Predicting what you already know helps: Provable selfsupervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01064</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09070</idno>
		<title level="m">Hybrid discriminative-generative training via contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2009">2020. 2, 3, 4, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Self-supervised learning through the eyes of a child</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Orhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16189</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Multi-modal self-supervision from generalized data transformations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13916</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10039</idno>
		<title level="m">Lsd-c: Linearly separable deep clusters</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Rethinking image mixture for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05438</idno>
		<imprint>
			<date type="published" when="2020">2020. 3, 7, 8, 9, 17</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09852</idno>
		<title level="m">Multi-label contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Learning video representations using contrastive bidirectional transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">VideoBERT: A joint model for video and language representation learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Self-supervised video representation learning using inter-intra contrastive framework</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019. 2, 3, 4, 7, 17</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning</title>
				<imprint>
			<date type="published" when="2009">2020. 1, 2, 3, 4, 7, 9</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Unsupervised learning of video representations via dense trajectory clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15731</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Demystifying self-supervised learning: An information-theoretical framework</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05576</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning. ICLR</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Attentive cutmix: An enhanced data augmentation approach for deep learning based image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Walawalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Self-supervised video representation learning by pace prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05861</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2020">2020. 2, 6, 8, 15</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13149</idno>
		<title level="m">On mutual information in contrastive learning for visual representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018. 2, 3, 4, 9, 21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Delving into inter-image invariance for unsupervised visual representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11702</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">K-shot contrastive learning of visual features with multiple instance augmentations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13310</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Hard negative examples are hard, but useful</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">What makes instance discrimination good for transfer learning?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06606</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Hardness-aware deep metric learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03030</idno>
		<title level="m">Deep robust clustering by contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<author>
			<persName><surname>Moco+coslr -Acc</surname></persName>
		</author>
		<idno>74.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title/>
		<author>
			<persName><surname>Moco+coslr+mlp -Acc</surname></persName>
		</author>
		<idno>76.4 MoCo-v2 -Acc: 78.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Moco-V2 +</forename><surname>Mochi</surname></persName>
		</author>
		<idno>Acc: 79.2</idno>
		<imprint>
			<date type="published" when="1024">1024</date>
			<biblScope unit="volume">1024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Method IN-1k VOC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>512, 1024, 512) 63.7 81.3 (±0.1) (↑0.6) 54.7 (±0.4) (↑1.1) 60.6 (±0.5) (↑1.6</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>↑0.7) 60.2 (±0.1) (↑1.2</idno>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">512</biblScope>
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
	<note>7 81.3 (±0.1) (↑0.6) 54.6 (±0.3) (↑1.0) 60.7 (±0.8) (↑1.7</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>66.6</idno>
		<imprint/>
	</monogr>
	<note>±0.1) (↑1.9) 200 epoch training SimCLR [11] (8k batch size</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>128) 68.0 82.3 (±0.2) (↓0.2) 56.8 (±0.1) ( 0.0) 63.8 (±0.4</idno>
		<imprint>
			<date type="published" when="1024">1024</date>
			<biblScope unit="volume">256</biblScope>
			<biblScope unit="page" from="0" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>↑0.5</idno>
		<imprint>
			<date type="published" when="1024">1024</date>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>1024, 0, 512) 67.8 82.7 (±0.1) (↑0.2) 57.0 (±0.1) (↑0.2) 64.0 (±0.2) (↑0.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>↑0.8</idno>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>↑0.5) 64.1 (±0.1) (↑0.8</idno>
		<imprint>
			<biblScope unit="volume">512</biblScope>
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
	<note>7 82.8 (±0.2) (↑0.3) 57.3 (±0.2</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>256) 67.6 82.6 (±0.2) (↑0.1) 57.2 (±0.3</idno>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
	<note>↑0.4) 64.2 (±0.5) (↑0.9</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>67.0 82.5 (±0.1) ( 0.0) 57.1 (±0.2) (↑0.3) 64.4 (±0.2) (↑1.1</idno>
		<imprint>
			<date type="published" when="2048">256, 2048, 2048</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>128, 1024, 512) 66.9 82.7 (±0.2) (↑0.2) 57.5 (±0.3) (↑0.7) 64.4 (±0.4) (↑1.1</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<idno>1024, 512) 66.3 82.6 (±0.1) (↑0.1) 57.3 (±0.1) (↑0.5) 64.4 (±0.5) (↑1.1</idno>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mochi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
