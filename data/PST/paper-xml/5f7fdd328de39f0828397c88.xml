<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How hard is to distinguish graphs with graph neural networks?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-16">16 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
							<email>andreas.loukas@epfl.ch</email>
						</author>
						<title level="a" type="main">How hard is to distinguish graphs with graph neural networks?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-16">16 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2005.06649v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A hallmark of graph neural networks is their ability to distinguish the isomorphism class of their inputs. This study derives hardness results for the classification variant of graph isomorphism in the message-passing model (MPNN). MPNN encompasses the majority of graph neural networks used today and is universal when nodes are given unique features. The analysis relies on the introduced measure of communication capacity. Capacity measures how much information the nodes of a network can exchange during the forward pass and depends on the depth, message-size, global state, and width of the architecture. It is shown that the capacity of MPNN needs to grow linearly with the number of nodes so that a network can distinguish trees and quadratically for general connected graphs. The derived bounds concern both worst-and average-case behavior and apply to networks with/without unique features and adaptive architecture-they are also up to two orders of magnitude tighter than those given by simpler arguments. An empirical study involving 12 graph classification tasks and 420 networks reveals strong alignment between actual performance and theoretical predictions.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A fundamental goal in the analysis of graph neural networks is to determine under what conditions current networks can (or perhaps cannot) distinguish between different graphs <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. The most intensely studied model in the literature has been that of message-passing neural networks (MPNN). Since its inception by Scarselli et al. <ref type="bibr" target="#b6">[7]</ref>, MPNN has been extended to include edge <ref type="bibr" target="#b7">[8]</ref> and global features <ref type="bibr" target="#b8">[9]</ref>. The model also encompasses many of the popular graph neural network architectures used today <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>.</p><p>Roughly two types of analyses of MPNN may be distinguished. The first bound the expressive power of anonymous networks, i.e., those in which nodes do not have any access to node features (also known as labels or attributes) and that are permutation equivariant by design. Xu et al. <ref type="bibr" target="#b2">[3]</ref> and Morris et al. <ref type="bibr" target="#b3">[4]</ref> established the equivalence of anonymous MPNN to the 1st-order Weisfeiler-Lehman (1-WL) graph isomorphism test. A consequence of this connection is that anonymous MPNN cannot distinguish between regular graphs with the same number of nodes, but can recognize trees as long as the MPNN depth exceeds the tree diameter. Other notable findings include the observation that MPNN cannot count simple subgraphs <ref type="bibr" target="#b5">[6]</ref>, as well as the analysis of the power of particular architectures to compute graph properties <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and to distinguish graphons <ref type="bibr" target="#b18">[19]</ref>-see also <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>The aforementioned insights can be pessimistic in the non-anonymous case, where permutation equivariance is either learned from data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> or obtained by design <ref type="bibr" target="#b23">[24]</ref>. With node features acting as identifiers, MPNN were shown to become universal in the limit <ref type="bibr" target="#b22">[23]</ref>, which implies that they can solve the graph isomorphism testing problem if their size is allowed to depend exponentially on the number of nodes <ref type="bibr" target="#b4">[5]</ref>. The node features, for instance, may correspond to a one-hot encoding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref> or a random coloring <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Communication capacity is the maximal amount of information that can be sent across two subgraphs (depicted in orange and green) (b) Communication complexity is the minimal amount of information needed so that two parties jointly compute a function f . (c) To determine whether graphs G and G are isomorphic one may use an MPNN g to test whether g(G) = g(G ).</p><p>At the same time, universality statements carry little insight about the power of practical networks, as they only account for behaviors that occur in the limit. Along those lines, recent work provided evidence that the power of MPNN grows as a function of depth and width for certain graph problems <ref type="bibr" target="#b22">[23]</ref>, showing that (both anonymous and non-anonymous) MPNN cannot solve many tasks when the product of their depth and width does not exceed a polynomial of the number of nodes. Nevertheless, it remains an open question whether similar results hold also for problems relating to the capacity of MPNN to distinguish graphs. Even further, it is unclear whether depth and width needs to grow with the number of nodes solely in the worst-case (as proven in <ref type="bibr" target="#b22">[23]</ref>) or with certain probability over the input distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Communication capacity and its consequences to distinguishing graphs</head><p>Aiming to study the power of MPNN of practical size to distinguish graphs, this paper defines and characterizes communication capacity, a measure of the amount of information that the nodes of the network can exchange during the forward pass (see Figure <ref type="figure" target="#fig_0">1a</ref>). In Section 2 it is shown that the capacity of MPNN depends on the network's depth, width, and message-size, as well as on the cutstructure of the input graph. Communication capacity is an effective generalization of the previously considered product between depth and width <ref type="bibr" target="#b22">[23]</ref>, being able to consolidate more involved properties, as well as to characterize MPNN with global state <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref> and adaptive architecture <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>.</p><p>The paper then delves into the communication complexity of determining the graph isomorphism class. The theory of communication complexity compliments the definition of communication capacity as it provides a convenient mathematical framework to study how much information needs to be exchanged by parties that jointly compute a function <ref type="bibr" target="#b32">[33]</ref> (see Figure <ref type="figure" target="#fig_0">1b</ref>). In this setting, Section 3 derives hardness results for determining the isomorphism class of connected graphs and trees. It is shown that the communication capacity of any MPNN needs to grow at least linearly with the number of nodes so that the network can learn to distinguish trees, and quadratically to distinguish between connected graphs. The analysis stands out from previous relevant works that have studied subcases of isomorphism, such as subgraph freeness <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> or those focused on anonymous networks <ref type="bibr">[3-6, 17, 19]</ref>. In fact, the derived hardness results apply to both anonymous and non-anonymous MPNN and can be up to two orders of magnitude tighter than what can be deduced from simpler arguments. In addition, the proposed lower bounds rely on a new technique which renders them applicable not only to worst-case instances <ref type="bibr" target="#b22">[23]</ref>, but in expectation over the input distribution.</p><p>An empirical study reveals strong qualitative and quantitative agreement between the MPNN test accuracy and theoretical predictions. In the 12 graph isomorphism tasks considered, the performance of the 420 graph neural networks trained was found to depend strongly on their communication capacity. In addition, the proposed theory could consistently predict which networks would exhibit poor classification accuracy as a function of their capacity and the type of task in question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Communication complexity for message-passing networks</head><p>Suppose that a learner is given a graph G = (V, E, a) sampled from a distribution D that is supported over a finite universe of graphs X . Throughout this paper, V will denote the set of nodes of cardinality n, E the set of edges, and a encodes any node and edge features of interest. With G as input, the learner needs to predict the output of function f : X → Y. This work focuses on graph classification, in which case f assigns a class y ∈ Y (i.e., its isomorphism class) to each graph in the universe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Message-passing neural networks (MPNN)</head><p>In MPNN, the node representation x ( ) i of every node v i ∈ V is initialized to be equal to the node's attributes x (0) i = a i and is progressively updated by exchanging messages:</p><formula xml:id="formula_0">x ( ) i = UPDATE x ( −1) i , msg ( ) ij : e ij ∈ E for = 1, . . . , d,</formula><p>where each message msg</p><formula xml:id="formula_1">( ) ij = MESSAGE x ( −1) j , a j , a ij</formula><p>contains some information that is sent to from node v j to v i .</p><p>Every neuron in a network utilizes some finite alphabet S containing s = |S| symbols to encode its state. For this reason, x</p><formula xml:id="formula_2">( )</formula><p>i and msg ( ) ij are selected from S w and S m , where w and m are the width (i.e., number of channels) and the message-size of the -th layer. For instance, to represent whether a neuron is activated one uses binary symbols, whereas a practical implementation could use as symbols the set of numbers represented in floating-point arithmetic.</p><p>MESSAGE and UPDATE are layer-dependent functions whose parameters are selected based on some optimization procedure. It is common to parametrize these functions by feed-forward neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>. The rational is that, by the universal approximation theorem and its variants <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>, these networks can approximate any smooth function that maps vectors onto vectors. If the network's output is required to be independent of the number of nodes, the output is recovered from the representations of the last layer by means of a readout function:</p><formula xml:id="formula_3">g(G) = READOUT x (d) i : v i ∈ V .</formula><p>For simplicity, it is here assumed that no graph pooling is employed <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, though the results may also be easily extended to account for coarsening <ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref>.</p><p>Global state. In the description above, all message exchange needs to occur along graph edges. However, one may also easily incorporate a global state (or external memory) to the model above by instantiating a special node v 0 and extending the edge set to contain edges from every other node to it. Global state is useful for incorporating graph features to the decision making <ref type="bibr" target="#b8">[9]</ref> and there is some evidence that it can facilitate logical reasoning <ref type="bibr" target="#b45">[46]</ref>. Here, I will suppose that x ( ) 0 belongs to set S γ . Adaptive MPNN. The forward-pass of an MPNN concludes after d layers. However, the depth of a network may be adaptive <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. In particular, d may depend on the size and connectivity of the input graph or any adaptive computation time heuristic <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> based on, for example, the convergence of the node representation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. In the same spirit, in the following it will supposed that all hyper-parameters of an MPNN, such as its depth, width, message-size, and global state size, can be adaptively decided based on the input graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Communication capacity</head><p>An MPNN g can be thought of as a communication network N (G, g), having processors as nodes and with connectivity determined by the input graph G. N (G, g) operates in = 1, . . . , d synchronous communication rounds and m symbols are transmitted in round from each processor v i to each one its neighbors v j such that e ij ∈ E. Further, the processors have limited and round-dependent memory: in round the processors corresponding to nodes V can store w symbols, whereas the external memory processor v 0 can store γ symbols.</p><p>The communication complexity of a message-passing neural network corresponds to the maximum amount of information that can be sent in N (G, g) between disjoint sets of nodes: Definition 2.1 (Communication capacity). Let g be an MPNN and fix a graph G = (V, E). For any two disjoint sets V a , V b ⊂ V, the communication capacity c g of g is the maximum number of symbols that N (G, g) can transmit from V a to V b and from V b to V a .</p><p>To understand Definition 2.1, imagine that the node-disjoint subgraphs G a = (V a , E a ) and G b = (V b , E b ) of G are controlled by two parties: Alice and Bob (see Figure <ref type="figure" target="#fig_0">1</ref>). In practice, Alice and Bob correspond to two sub-networks of g. By construction, when Alice needs to send information to Bob, she does so by sending information across some paths that cross between V a and V b . Bob does the same. From this elementary observation, it can be deduced that the number of symbols that can be sent during the forward pass is bounded by the cut between the two subgraphs: Lemma 2.1. Let g be an MPNN of d layers, where each has width w (i.e., number of channels), exchanges messages of size m , and maintains a global state of size γ . For any disjoint partitioning of V into V a and V b , the communication complexity of g is at most</p><formula xml:id="formula_4">c g ≤ cut(V a , V b ) d =1 min{m , w } + d =1 γ ,</formula><p>with cut(V a , V b ) being the size of the smallest cut that separates V a and V b in G.</p><p>Whenever the MPNN involves sending for each e ij ∈ E two messages, i.e., one from v i to v j and one from v j to v i , every edge should be counted twice in the calculation of cut(V a , V b ).</p><p>It is also interesting to remark that c g may be a random quantity. In particular, when G is sampled from a distribution D, the capacity of an adaptive MPNN, i.e., a network whose hyper-parameters change as a function the input, may vary as well. For this reason, the analysis will also consider the expected communication capacity c g (D) of g w.r.t. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Communication complexity</head><p>Let us momentarily diverge from graphs and suppose that Alice and Bob wish to jointly compute a function f : X a × X b → Y that depends on both their inputs. Alice's input is an element x a ∈ X a and Bob sees an element x b ∈ X b . Later on, x a and x b will correspond to G a and G b , respectively, whereas y ∈ Y will be the classification output (see Figure <ref type="figure" target="#fig_0">1b</ref>).</p><p>To compute f (x a , x b ), the two parties need to exchange information based on some communication protocol π. Concretely, π determines for each input (x a , x b ) the sequence π(x a , x b ) = ((ID 1 , s 1 ), (ID 2 , s 2 ), . . .) of symbols that are exchanged, with each symbol s i ∈ S being paired with the id of its sender (Alice or Bob)-for a more detailed description, see Appendix B. The number of symbols exchanged by π to successfully compute f (x a , x b ) are denoted by π(x a , x b ) m , with subscript m ∈ {one, both} indicating whether "successful computation" entails one or both parties figuring out f (x a , x b ) at the end of the exchange.</p><p>Worst-case complexity. The focus of classical theory is on the worst-case input. The communication complexity <ref type="bibr" target="#b32">[33]</ref> of f is defined as</p><formula xml:id="formula_5">c m f := min π max (xa,x b )∈Xa×X b π(x a , x b ) m<label>(1)</label></formula><p>and corresponds to the minimum worst-case length of any protocol that computes f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected complexity.</head><p>In machine learning, one usually cares about the expected behavior of a learner when its input is sampled from a distribution. Concretely, let (X a , X b ) be random variables sampled from a distribution D with domain X a × X b . The expected length of a protocol π is</p><formula xml:id="formula_6">E D c m f (π) := (xa,x b )∈Xa×X b π(x a , x b ) m • P(X a = x a , X b = x b ) ,<label>(2)</label></formula><p>where now the protocol length π(x a , x b ) m is weighted according to the probability of each input. With this in place, I define the expected communication complexity of f as</p><formula xml:id="formula_7">c m f (D) := min π E D c m f (π) ,<label>(3)</label></formula><p>corresponding to the minimum expected length of any protocol that computes f .</p><p>For an overview of the classical theory of communication complexity pertaining to the worst-case and an analysis of the newly-defined expected complexity, the reader may refer to Appendix B.</p><p>To use communication complexity for learning problems f : X → Y from a graph universe X to a set of classes Y one needs to decompose (a subset of) X as X a × X b . As it will be seen in the following sections, the decomposition can be achieved by finding a disjoint partitioning of every graph G ∈ X into subgraphs G a ∈ X a and G b ∈ X b , held by Alice and Bob, respectively. Then, in the worst case, c m f symbols need to be exchanged so that one (m=one) or both (m=both) parties can correctly classify G into class y = f (G). Moreover, if G is sampled from some distribution D, then the two parties need to exchange at least c m f (D) symbols in expectation. Together with Lemma 2.1, the aforementioned bounds can be used to characterize what an MPNN cannot achieve as a function of its worst-case and expected capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hardness results for determining the isomorphism class</head><p>This section derives necessary conditions for the communication capacity of a network that determines the graph isomorphism class of its inputs. This entails finding a mapping f isom : X → Y from a universe of labeled graphs to their corresponding isomorphism classes. Crucially, though the nodes of graph G are assigned some predefined order (which constitutes their label in graph-theory nomenclature), the class f isom (G) should be invariant to this ordering.</p><p>As it will be shown, MPNN of sub-quadratic and sub-linear capacity cannot compute the isomorphism class of connected graphs and trees, respectively: Theorem 3.1. Let g be a MPNN using either a majority-voting or a consensus based readout (defined in Section 3.2). Denote by c g its communication capacity. 1. To compute f isom for every graph and tree of n nodes, it must be that c g = Ω n 2 and c g = Ω (n), respectively.</p><p>2. If each graph is sampled from B n/2,p (defined in Theorem 3.3) to compute f isom in expectation it must be that c g (D) = Ω n 2 . Further, if each graph is a tree sampled from T n/2 (defined in Theorem 3.4) to compute f isom in expectation it must be that c g (D) = Ω (n) .</p><p>For general graphs, these results are one or two orders of magnitude tighter than arguments that compare the receptive field of a neural network with the graph diameter. Specifically, connected graphs have diameter at most n and thus a diameter analysis yields d = Ω(n) without a global state and d = Ω(1) with one (as any two nodes are connected by a path passing through v 0 ).</p><p>The tree distribution was chosen purposefully to demonstrate that the bounds are also relevant for the anonymous case, when MPNN can also be analyzed by equivalence to the 1-WL test <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. For trees, the 1-WL test requires n iterations because there exists a tree of diameter n. However, since MPNN is equivalent to 1-WL only when the former is built using injective aggregation functions (i.e., of unbounded width <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">47]</ref>), the equivalence does not imply a relevant lower bound on the width/message-size/global-state-size of MPNN. Further, the communication complexity analysis introduced here yields tighter results in expectation: it asserts that one needs Ω(n) capacity on average, even though the average tree in</p><formula xml:id="formula_8">T n/2 has O( √ n) diameter (and thus 1-WL would require d = Ω( √ n) in expectation).</formula><p>Graph isomorphism testing. There is also a close relation between f isom and the graph isomorphism testing problem (see Figure <ref type="figure" target="#fig_0">1c</ref>). Specifically, methods for isomorphism testing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> that compare graphs G and G by means of some invariant representation or embedding</p><formula xml:id="formula_9">g(G) = g(G ) if and only if f isom (G) = f isom (G )</formula><p>can be expressed as g = q • f isom for some injective function q. Since q does not involve any exchange of information, the communication complexity of such testing methods is the same as that of f isom .</p><p>The proposed hardness results thus still hold.</p><p>The rest of this section is devoted to proving Theorem 3.1. The analysis consists of two parts: the communication complexity of distinguishing graphs and trees is derived in Section 3.1, and the implications of these results to MPNN are discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Communication complexity analysis</head><p>Rather than focusing directly on the universe of all graphs and trees, respectively, it will be convenient to analyze a strictly smaller universe X containing easily partitioned graphs. As it will be seen, we can utilize such a restriction without significant loss of generality, because the derived worst-case impossibility results also apply to any universe that is a strict superset of X .</p><p>Concretely, X will consist of all labeled graphs G = (V, E) on n nodes admitting to the following (X a , X b , τ ) decomposition:</p><formula xml:id="formula_10">(a) Subgraph G a = (V a , E a ) induced by labels V a = (1, 2, • • • , v) belongs to X a . (b) Subgraph G b = (V b , E b ) induced by labels V b = (v + 1, v + 2, • • • , 2v) belongs to X b . (c) Subgraph G c = (V, E \ (E a ∪ E b )) yields cut(V a , V b ) ≤ τ .</formula><p>An example (X a , X b , τ ) decomposable graph is depicted in Figure <ref type="figure" target="#fig_1">2</ref>. This decomposition is fairly general: the main restriction placed is that the cut between V a and V b is bounded by τ . Families X a and X b can be chosen to contain relevant families of graphs (e.g., all connected graphs or all trees), whereas G c may be selected arbitrarily. To derive lower bounds, it will be imagined that G a and G b are known by Alice and Bob, respectively, while both know G c . The goal of the two parties is to determine</p><formula xml:id="formula_11">f isom (G) = f isom (G a , G b , G c</formula><p>) by exchanging as little information as possible.</p><p>Two main results will be proven: Section 3.1.1 will show that, when X a and X b contain all labeled connected graphs on v nodes, the worst-case and expected communication complexity are both Θ(v 2 ). Moreover, in Section 3.1.2 it is proven that, when X a and X b contain only trees, the two complexities are Θ(v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Distinguishing connected graphs</head><p>When X a and X b contain all connected graphs on v nodes, Alice and Bob should exchange Θ(v 2 ) symbols in the worst case: Theorem 3.2 (Worst-case complexity). When X a and X b each contain the set of all connected graphs on v nodes, the worst-case communication complexity of f isom is at least</p><formula xml:id="formula_12">O(v 2 ) = c both fisom ≥ v 2 log 2 s − 2v log s v √ 2 e − log s 2ve 2 + o(1) = β = Ω v 2 and O(v 2 ) = c one fisom ≥ β−(log 2 s) −1 2 = Ω v 2 .</formula><p>A similar bound holds also in the random graph model G v,p . In G v,p , every graph with v nodes and k edges is sampled with probability</p><formula xml:id="formula_13">P(G ∼ G v,p ) = p k (1 − p) ( v 2 )−k .</formula><p>Effectively, this means the probability of choosing each graph depends only on the number of edges it contains. Moreover, for p = 0.5 each graph is sampled uniformly at random from the set of all possible graphs. The following theorem bounds the expected communication complexity when the subgraphs known to Alice and Bob are sampled from G v,p :</p><formula xml:id="formula_14">Theorem 3.3 (Expected complexity). Let G a and G b be sampled independently from G v,p , with log v/v &lt; p &lt; 1 − s Ω(1) and cut(V a , V \ V a ) = cut(V b , V \ V b ) = 1. Denote by B v,p the resulting distribution. With high probability, O(v 2 ) = c both fisom (B v,p ) ≥ v 2 H s (p) − v 2 log s v e + H s (p) − log s 2ve 2 = β = Ω(v 2 )</formula><p>and</p><formula xml:id="formula_15">O(v 2 ) = c one fisom (B v,p ) ≥ β 2 − v 2 − v(1 − H 2 (p)) + 1 2 log 2 s = Ω(v 2 ),</formula><p>where H s (p) = −(1 − p) log s (1 − p) − p log s p is the binary entropy function (base s).</p><p>The expected complexity, therefore, grows asymptotically with Θ(v 2 ) and is maximized when every graph in the universe is sampled with equal probability, i.e., for p = 0.5. Interestingly, in this setting, the bounds of Theorems 3.2 and Theorem 3.3 match. This implies that, unless there is some strong isomorphism class imbalance in the dataset, the communication complexity lower bound posed by Theorem 3.2 does not only concern rare worst-case inputs, but should be met on average.</p><p>In the theorem it is asserted that log v/v &lt; p &lt; 1 − s Ω (1) . The aforementioned lower bound suffices to guarantee that every G ∼ B v,p will be connected with high probability, whereas the upper bound is needed to ensure that H s (p) = Ω(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Distinguishing trees</head><p>Distinguishing trees (connected acyclic undirected graphs) is significantly easier: Theorem 3.4. Suppose that G a and G b are sampled independently from the set of all trees on v nodes. Denote by T v the resulting distribution. The communication complexity of f isom is at least</p><formula xml:id="formula_16">O(v) = c both fisom ≥ c both fisom (T v ) 2v log s α − 5 log s v + log s 7 = β = Ω(v)</formula><p>and</p><formula xml:id="formula_17">O(v) = c one fisom ≥ c one fisom (T v ) β+log s 2 2 = Ω(v), where α ≈ 2.9557652 and f (n) g(n) means f (n) ≥ g(n) as n grows.</formula><p>Akin to the general case, the expected and worst-case complexities match when every tree is sampled with equal probability. Since a distribution over trees cannot be meaningfully parametrized based a connection probability p (trees always have the same number of edges), by default in T v every G ∈ X is sampled with equal probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Consequences for message-passing neural networks</head><p>Two types of networks are distinguished depending on how the readout function operates:</p><p>1. READOUT performs majority-voting. Specifically, for g to compute f isom (G) there should exist a function r : S w d → Y and a set of nodes M G ⊆ V possibly dependent on G and of cardinality at least</p><formula xml:id="formula_18">|M G | ≥ µ = O(1), such that r(x (d) i ) = f isom (G) for every v i ∈ M G . 2.</formula><p>READOUT performs consensus. This is akin to a majority-voting, with the distinction that</p><formula xml:id="formula_19">M G should contain at least |M G | ≥ n − µ = Ω(n) nodes.</formula><p>The implications of a communication complexity bound to MPNN capacity are as follows: Lemma 3.1. Let D be a distribution over graphs that is supported on a universe X admitting to a (X a , X b , τ ) decomposition. Further, suppose that g is an MPNN whose communication capacity is always bounded from above by c g and is at most c g (D) in expectation. The following hold:</p><p>1. There exists some G ∈ X for which computing f isom (G) necessitates c g ≥ c m fisom . In addition, for every X ⊃ X network g cannot compute f isom (G) for some G ∈ X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">In expectation, computing</head><formula xml:id="formula_20">f isom necessitates c g (D) ≥ c m fisom (D). Moreover, if c g &lt; δ c m fisom (D) for some δ ∈ [0, 1], then g cannot compute f isom (G) with probability at least (1 − δ)/((β m /c m fisom (G)) − δ).</formula><p>Above, with majority-voting one should set m = one and v &gt; (n − µ)/2, whereas with consensus m = both and v &gt; µ. Further, β m is the worst-case length of a protocol with optimal expected length.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical results</head><p>This section tests the developed theory on 12 graph and tree isomorphism classification tasks of varying difficulty. In the 420 neural networks tested, the bounds are found to consistently predict when each network can solve a given task as a function of its capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting</head><p>MPNN of different capacities were tasked with learning the mapping between a universe of graphs their corresponding isomorphism classes.</p><p>Datasets. A total of 12 universes were constructed following the theory: X n graph for n = (6, 8, 10, 12) and X n tree for n = (8, 10, . . . , 22). Each X n graph was built in two steps: First, geng <ref type="bibr" target="#b47">[48]</ref> was used to populate X a and X b with all possible connected graphs on v = n /2 nodes. Then, each G ∈ X n graph was generated by selecting G a and G b from X a and X b and connecting them with an edge, such that τ = 1. The labels added to the nodes of G were the one-hot encoding of a random permutation of (1, . . . , v) and (v + 1, . . . , n). The construction of X n tree differed only in that X a and X b contained all trees on v = n /2 nodes. Then, the 12 datasets were built by sampling graphs from each respective universe. These were split into a training, a validation, and a test set (covering 90%, 5%, and 5% of the dataset, respectively). Additional details are provided in Appendix A.</p><p>Architecture and training. The networks combined multiple GIN0 <ref type="bibr" target="#b2">[3]</ref> layers with batch normalization and a simple sum readout. Their depth and width varied in d ∈ (2, 3, 4, 5, 6, 7, 8) and w ∈ (1, 2, 4, 8, 16), respectively, the message-size was set equal to w, and no global state was used. Each network was trained using Adam with a decaying learning rate. Early stopping was employed when the validation accuracy reached 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Findings</head><p>Let me begin by stating that networks of sufficient size could solve nearly every task up to 100% test accuracy (Table 2 in Appendix A), which corroborates previous theoretical findings that non-anonymous MPNN are universal and can solve graph isomorphism <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref>, as well as that they can learn to be permutation invariant <ref type="bibr" target="#b21">[22]</ref>. On the other hand, anonymous MPNN are always permutation equivariant but cannot distinguish between graphs of more than three nodes <ref type="bibr" target="#b5">[6]</ref>.</p><p>Figures <ref type="figure" target="#fig_4">3a and 3b</ref> summarize the neural network performance for all the tasks considered. The achieved accuracy strongly correlated with communication capacity (computed based on Lemma 2.1) with larger-capacity networks performing consistently better. Moreover, in qualitative agreement with the analysis, solving a task can be seen to necessitate larger capacity when the number of nodes is increased. A case in point, whereas a capacity of 4 suffices to classify 99% of graphs of 6 nodes correctly, for 8, 10, and 12 nodes the required capacity increases to 8, 24, and 112, respectively. This identified correlation between capacity and accuracy could not be explained by the depth or width of the network alone, as, in most instances, tasks that could not be solved by wide and shallow networks could also not be solved by deep networks of the same capacity. The only exception was when receptive field did not cover the entire graph (see Figures <ref type="figure" target="#fig_7">6a and 6b</ref> in Appendix A).</p><p>The gray regions at the bottom of each figure indicate the proposed expected communication complexity lower bounds. Here, |S| = 2 based on the interpretation that each neuron can be either in an activated state or not. There are also two lower bounds plotted since a network that sums the final layer's node representations can learn to differentiably approximate both a majority-voting and a consensus function. The analysis asserts that a network with capacity below the gray dashed lines should not be able to correctly distinguish input graphs for a significant fraction of all inputs (see precise statement in Lemma 3.1). Indeed, networks in the gray region consistently perform poorly. The empirical accuracy appears to match closely the consensus bound, though it remains inconclusive if the network is actually learning to do consensus. A closer inspection (see Figures <ref type="figure" target="#fig_5">5a and 5b</ref> in Appendix A) also reveals that the poor performance of networks in the gray region is not an issue of generalization. In agreement with the theory, networks of insufficient communication capacity do not possess the expressive power to map a fraction of all inputs to the right isomorphism class, irrespective of whether these graphs appear in the training or test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work proposed a hardness-result for distinguishing graphs in the MPNN model by characterizing the amount of information the nodes can exchange during their forward pass (termed communication capacity). From a practical perspective, the results herein provide evidence that, if the amount of training data is not an issue, determining the isomorphism class of graphs is hard but not impossible for MPNN. Specifically, it was argued that the number of parameters needs to increase quadratically with the number of nodes. The implication is that, in the most general case, networks of practical size should be able to solve the problem for graphs with at most a few dozen nodes, but will encounter issues otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>As we rely on neural networks more heavily, we are unfortunately sacrificing some of our ability to understand how our computers solve problems. Our lack of insight hinders us from using our technology to its full potential and can yield mistrust to the public. After all, if we cannot understand what a neural network is (capable of) doing, how can we know whether it is solving the correct problem? Poor understanding of fundamentals can also lead researchers to misguided optimism, believing that, given the right hyper-parameter tweaking and a large enough training set, neural networks can solve their problem. When incorrect, this mindset can lead to a waste of precious resources, such as time and energy.</p><p>In this light, impossibility results, such as those presented in this work, provide an insight into the fundamental limits of neural networks. Hardness results for graph neural networks, in particular, characterize the relational pattern recognition ability of practical networks and provide necessary conditions for using our tools to solve classical graph problems. The central implication of the results presented in this work is that one cannot expect to learn algorithms that distinguish (even approximately) connected graphs and trees unless the network size grows at-least polynomially with the graph size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional empirical results</head><p>This section presents the empirical results more comprehensively.</p><p>First, Table <ref type="table" target="#tab_0">1</ref>  Some example graphs sampled are shown in Figure <ref type="figure">4</ref>:</p><p>Table <ref type="table" target="#tab_2">2</ref> provides empirical evidence that, with a one-hot encoding of the node-ordering given as features and a sufficiently large training set, MPNN of sufficient capacity can solve graph isomorphism.</p><p>In the current experiment, a large network (depth = 10 and width = 32) is seen to solve most isomorphism instances. The network did not achieve perfect classification for larger graphs, but better results can be achieved with more training data.</p><p>The achieved accuracy of all networks considered is shown in Figures <ref type="figure" target="#fig_5">5a and 5b</ref> for graph and tree isomorphism tasks, respectively. In contrast to the figures of Section 4, these plots depict the training Figure <ref type="figure">4</ref>: Example graphs sampled from two (X a , X b , 1) decompositions. Top: X a and X b contain all connected graphs on v = 6 nodes (special case of Theorems 3.2 and 3.3). Bottom: X a and X b contain all trees on 11 nodes (special case of Theorem 3.4). In both cases, there exists a τ = 1 cut between the nodes V a controlled by Alice (in yellow) and nodes V b controlled by Bob (in green).   as well as testing accuracy. For the majority of tasks the test and training accuracy is almost identical. Overfitting can be a problem for larger graphs (e.g., trees of at least 20 nodes). The problem can be mitigated by increasing the size of the training set.</p><p>Finally, Figures <ref type="figure" target="#fig_7">6a and 6b</ref> demonstrate that depth and width are partially exchangeable. This implies that the correlation between capacity and accuracy (see Figures <ref type="figure" target="#fig_4">3a and 3b</ref>) cannot be explained by only looking at the depth or width of a network. Here, the two figures depict the empirical test accuracy (by the marker color and size) as a function of depth and width for all tasks. For each task, the depth and width have been normalized by the square root of the critical capacity, corresponding to the smallest communication capacity of any network that could achieve at least 50% accuracy. As a consequence of the normalization, all networks in the top-right region (in white) possess sufficient capacity for the task at hand. Moreover, networks plotted below (above) the main diagonal are deeper than they are wide (wider than they are deep). As seen, the classification task can be solved by both wide and deep networks of super-critical capacity, as long as the networks are not too shallow. Indeed, networks of very small depth cannot see the entire graph and thus have poor accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Communication complexity: basics and beyond B.1 Basic theory: protocols</head><p>Let us start by denoting by S the common set of symbols <ref type="foot" target="#foot_0">1</ref> Alice and Bob use to communicate and denote by s = |S| its cardinality. A protocol π is described in terms of a rooted s-ary tree, i.e., a tree with a clearly defined root and in which every internal node has exactly s children. In addition, every internal node i is owned by either Alice or Bob and each one of the node's children symbolizes a symbol sent by its owner. Specifically, the protocol associates i with a function π i that maps the input of i's owner to S (or equivalently to one of i's children). The protocol operates as follows: first, both parties set the current node to be the root of the tree. Say that the current node is i. If the owner of i is Alice then she announces symbol π i (x) and otherwise Bob announces π i (y). Both parties then update the current node to point to the child of i indicated by the value of π i . This procedure is repeated until a leaf is found.</p><formula xml:id="formula_21">leaf root A B A π 1 (x a )=0 π 2 (x b )=1 π 3 (x a )=1</formula><p>Figure <ref type="figure">7</ref>: The execution of π over input (x a , x b ) is a path within the s-ary tree. The decision of which symbol to send is taken by the node's owner (Alice or Bob) as a function of the current path and input. In this example there are s = 2 symbols S = {0, 1} and the path moves to the left/right child when the symbol 0/1 is sent. The protocol terminates at the leaf and the output is π(x a , x b ) = ((A, 0), (B, 1), (A, 1)).</p><p>By definition, the number of symbols π(x a , x b ) m Alice and Bob need to send in order to jointly compute f (x a , x b ) using protocol π equals the length of the path from the root to the leaf π(x a , x b ). Moreover, the number of symbols sent by a protocol in the worst case (i.e., for any input) is at most equal to the depth of the protocol tree (Fact 1.1 in <ref type="bibr" target="#b32">[33]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Basic theory: monochromatic rectangles</head><p>To understand how protocols operate one needs to consider the concept of rectangles. A rectangle is a subset of X a × X b that can be expressed as X a × X b for some X a ⊂ X a and X b ⊂ X b . Intuitively, if one represents X a × X b as a matrix X of size |X a | × |X a |, then a rectangle is any principal submatrix X of X, i.e., a matrix that contains a subset of rows and columns.</p><p>As it turns out, every protocol can be described in terms of rectangles. Let R i ⊆ X a × X b be the set of inputs leading to a path that crosses a node i ∈ π. Moreover, define the following sets:</p><formula xml:id="formula_22">X i a = {x ∈ X a : ∃y ∈ X b such that (x a , x b ) ∈ R i } X i b = {y ∈ X b : ∃x ∈ X a such that (x a , x b ) ∈ R i }</formula><p>The following result clarifies the connection between protocols and rectangles. Lemma B.1 (Lemma 1.4 in <ref type="bibr" target="#b32">[33]</ref>). For every protocol π and node i, R i is a rectangle with R i = X i a × X i b . Further, the rectangles R given by the leafs ∈ L π of the protocol tree form a partition of X a × X b .</p><p>Effectively, at any point in a protocol, a rectangle describes the different possible outputs of f given the messages that have been exchanged. Every new message that the two parties exchange, eliminates some possible outputs, decreasing the size of the rectangle.</p><p>With this in place, it is not hard to realize that, for every leaf ∈ L π , the function f should always take the same value at every (x a , x b ) ∈ R in order for both parties to be able to compute the output from π(x a , x b ). Such rectangles are referred to as monochromatic:</p><formula xml:id="formula_23">concretely, a rectangle R ⊂ X a × X b is monochromatic if f (x a , x b ) = f (x a , x b ) for every (x a , x b ), (x a , x b ) ∈ R.</formula><p>Indeed, if leaf rectangles were not monochromatic, Alice and Bob would not be able to identify the output of f based on R .</p><p>The following theorem is obtained by combining Lemma B.1 with the fact that the minimum depth of any s-ary tree with s c leafs is c. Theorem B.1 (Theorem 1.6 by Rao and Yehudayoff <ref type="bibr" target="#b32">[33]</ref>). If the communication complexity of f :</p><formula xml:id="formula_24">X a × X b → Y is c both f , then X a × X b can be partitioned into at most s c both f monochromatic rectangles.</formula><p>The following is a direct corollary: Corollary B.1 (Rao and Yehudayoff <ref type="bibr" target="#b32">[33]</ref>). If X a × X b cannot be partitioned into s c monochromatic rectangles, then c both f ≥ c.</p><p>A simple way to satisfy the requirement of the corollary is to prove that no large monochromatic rectangle exists. For instance, if it is shown that all monochromatic rectangles have size bounded by k 2 then every monochromatic partitioning must contain at least |X a × X b |/k 2 rectangles and the complexity is at least c both f ≥ log s |X a × X b |/k 2 . I will rely on this method in the following to derive lower bounds on the worst-case communication complexity of different functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 A different perspective: expected communication complexity</head><p>The following lemma connects the expected communication complexity E D [c f (π)] to the entropy of the categorical distribution induced by the leafs of the protocol tree. Lemma B.2. Let the random variables X = (X a , X b ) ∼ D be sampled from some distribution D and, moreover, suppose that the random variable L π is the leaf for a protocol π that computes f (X a , X b ). The expected communication complexity of f is</p><formula xml:id="formula_25">min π H s (L π ) ≤ c m f (D) ≤ min π H s (L π ) + 1,</formula><p>where H s (L π ) is the Shannon entropy (base s) of L π under D.</p><p>Proof. The expected length of a protocol π is</p><formula xml:id="formula_26">E D c m f (π) = xa,x b π(x a , x b ) m • P(X a = x a , X b = x b ) = ∈Lπ depth(t) • P(L π = ) = E D [depth(L π )] .</formula><p>Note that the set L π contains the leafs of the protocol tree and L π is a categorical random variable over leafs with</p><formula xml:id="formula_27">P(L π = ) = x,y : π(xa,x b )= P(X a = x a , X b = x b ) ,</formula><p>which is also equal to the probability P (X a , X b ) ∈ R that a randomly drawn input belongs to R .</p><p>To understand c m f (D) it helps to realize the connection between protocols and coding theory: rather than sending information between Alice and Bob, one may think of sending the leafs over a channel by using a codebook. In this analogy, each leaf corresponds to a code and the path from the root of the protocol tree to every internal node at depth t corresponds to code prefix of length t. Furthermore, the probability of encountering the leaf is P (L π = t) and the depth of the protocol tree for every input (x a , x b ) ∈ R is equal to the length of the code required to send the associated symbol.</p><p>From the above it follows that the act of designing a protocol with minimal c m f (π) is equivalent to finding a tree with minimum expected path length from the root to the leafs. The latter is, in turn, equivalent to minimizing the length of the expected code length for a categorical distribution L π . Therefore, based on Shannon's source coding theorem we have that</p><formula xml:id="formula_28">min π H s (L π ) ≤ c m f (D) ≤ min π H s (L π ) + 1,</formula><p>matching the lemma statement. The number of symbols that can be transmitted from Alice to Bob in layer is bounded by the maximum flow of the following multi-source multi-sink maximum flow problem with node capacities:</p><p>• The nodes V a are the senders and the nodes V b are the sinks.</p><p>• Each edge has capacity m .</p><p>• Each node in V has capacity w , whereas v 0 has capacity γ .</p><p>This problem can be reduced to a simple maximum flow problem (single source single-sink without node capacities) in three steps:</p><p>1. All nodes in V a (resp. V b ) are connected to a new node A (resp. B) with edges of infinite capacity. 2. Each node v i (with the exception of A, B and v 0 ) is split into two nodes in i and out i connected by an edge of capacity w . Incoming edges to v i are connected to in i and outgoing edges are connected to out i . 3. The same splitting procedure is performed for node v 0 , but now the internal edge has capacity γ .</p><p>Consider the transformed flow network as shown in Figure <ref type="figure" target="#fig_8">8b</ref>. By the max-flow min-cut theorem, the maximum value of the flow is equal to the minimum capacity over all cuts that separate V a ∪ A from V b ∪ B. The latter however can always be bounded by cut(A, B) + γ . The first term of this equation gives the weight of the smallest cut separating A and B in the reduced graph, excluding those (orange) edges that touch v 0 : since the edges from A to V a have infinite capacity (resp. from B to V b ), every such cut also separates V a and V b . Notice also that every path from A to B includes at least one internal edge of capacity w and one normal edge of capacity m . Combining the previous observations one finds that cut(A, B) ≤ cut(V a , V b ) min{w , m }, where cut(V a , V b ) is the size of the smallest cut that separates V a and V b on G (the undirected and unweighted graph prior to the reduction). The internal edge capacity of v 0 in accounted by term γ . The final expression is obtained by summing the bound over all d layers.</p><p>C.2 Proof of Theorem 3.2</p><p>The proof consists of two main steps. First, the number of monochromatic rectangles of f isom will be controlled using the number of graph isomorphism classes in X . Then, invoking Corollary B.1 will result in a bound for c both fisom . Second, the identified lower bound will be translated to a bound regarding c one fisom based on Lemma D.1.</p><p>There are 2 ( v 2 ) labeled graphs on v nodes (i.e., counting orderings), the overwhelming majority of which are connected. The number of connected labeled graphs on v nodes is</p><formula xml:id="formula_29">|X a | = |X b | = 2 ( v 2 ) 1 − 2v 2 v + o 1 2 v = 2 ( v 2 ) 1 − O v 2 v ,</formula><p>which, for sufficiently large v, is very close to 2 ( v 2 ) <ref type="bibr">[49, p. 138</ref>]. Specifically, one may write</p><formula xml:id="formula_30">log 2 |X a | = log 2 |X b | = log 2 2 ( v 2 ) 1 − O v 2 v = v 2 log 2 2 + log 2 1 − O v 2 v ≥ v(v − 1) 2 − O v 2 v (log(1 − x) ≥ −O(1)x for x = o(1)) = v(v − 1) 2 + o(1)</formula><p>and, similarly,</p><formula xml:id="formula_31">log 2 |X a | = log 2 |X b | ≤ v(v−1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>. The number of permutations on v nodes is v!, which implies that the number c(v) of isomorphism classes of v-node graphs is bounded by</p><formula xml:id="formula_32">log 2 c(v) ≥ log 2 |X a | v! (4) = v(v − 1) 2 − log 2 (v!) + o(1)<label>(5)</label></formula><formula xml:id="formula_33">≥ v(v − 1) 2 − v log 2 v e − log 2 √ ve 2 + o(1) (since x! ≤ √ xe 2 (x/e) x ) = v 2 2 − v log 2 v √ 2 e − log 2 √ ve 2 + o(1)<label>(6)</label></formula><p>By construction, X contains at least c(v)(1 + c(v))/2 classes. To obtain this bound, one assumes that there do not exist any classes that differ only w.r.t. G c and then notes that each unique class of X may be build either by gluing two distinct or identical classes on v nodes (corresponding to graphs in X a and X b ). The bound then follows by counting all pairs of elements (there are c(v) of those) with repetitions (e.g., for {a, b, c} the set of possible pairs are {(aa), (ab), (ac), (bb), (bc), (cc)}).</p><p>The number of monochromatic rectangles of f isom is at least the number of classes and thus Corollary B.1 asserts:</p><formula xml:id="formula_34">c both fisom log 2 s = log 2 minimum number of monochromatic rectangles ≥ log 2 c(v)(c(v) + 1) 2 = 2 log 2 c(v) + log 2 1 + 1 c(v) − 1 ≥ 2 log 2 c(v) − 1<label>(7)</label></formula><p>Substituting ( <ref type="formula" target="#formula_33">6</ref>) into <ref type="bibr" target="#b6">(7)</ref> gives:</p><formula xml:id="formula_35">c both fisom log 2 s ≥ v 2 − 2v log 2 v √ 2 e − 2 log 2 √ ve 2 − 1 + o(1) = v 2 − 2v log 2 v √ 2 e − log 2 2ve 2 + o(1)</formula><p>A bound on c one fisom can be derived with the help of Lemma D.1: </p><formula xml:id="formula_36">c one fisom log 2 s ≥ c both fisom log 2 s − max G b ,Gc log s (|{f (G a , G b , G c ) : G a ∈ X a }|) log 2 s = 2 log 2 c(v) − 1 − log 2 c(v) ≥ v 2 2 − v log 2 v √ 2 e − log 2 2e √ v + o<label>(1)</label></formula><formula xml:id="formula_37">f isom (G) of a graph sampled from G = (G a , G b , G c ) ∼ G is at least c both fisom (G) ≥ min Gc H s (f isom (G)|G c ) .</formula><p>Proof. The first step is to condition the expected communication complexity on G c :</p><formula xml:id="formula_38">c fisom (G) = min π E G [c fisom (π)] = min π Gc P(G c ) E G [c fisom (π)|G c ] (due to the law of total expectation) = min π Gc P(G c ) E G [c fc (π)] (by the definition f c (•, •) := f isom (•, •, G c )) ≥ Gc P(G c ) min π E G [c fc (π)] ≥ min Gc c fc (G).</formula><p>Denote by L π the set of leafs of a protocol π that computes f c and by L π the random variable induced by the distribution G (for brevity, the conditioning on G c remains implicit in the following). We have that</p><formula xml:id="formula_39">H s (L π ) = ∈Lπ P(L π = ) log s 1 P(L π = ) .<label>(8)</label></formula><p>Upon closer consideration, there are |Y| types of leafs such that L π = |Y| y=1 L π,y , with each subset L l π containing all leafs for which the protocol outputs the graph isomorphism class y. From Lemma D.2 and because L π,1 , . . . , L π,|Y| form a partitioning of L π , we may write:</p><formula xml:id="formula_40">H s (L π ) ≥ |Y| y=1 P(L π ∈ L π,y ) log s 1 P(L π ∈ L π,y )</formula><p>.</p><p>The term P(L π ∈ L π,y ) seen above corresponds to the probability that class y will appear in our sample:</p><formula xml:id="formula_41">P(L π ∈ L π,y ) = P(f (G a , G b , G c ) = y) therefore, min π H s (L π ) ≥ H s (f (G)|G c</formula><p>) and the claim follows.</p><p>Coming back to the setting of the main theorem, denote by k y = |E a | + |E b | the number of edges of the graphs in class y (disregarding the edges E c ). For every G c , we have that</p><formula xml:id="formula_42">P(f isom (G) = y | G c ) = i c (v) p ky (1 − p) 2( v 2 )−ky = i c (v) p ky (1 − p) v(v−1)−ky .</formula><p>Term i c (v) corresponds to the size of the corresponding isomorphism class. Specifically, when p is not too small and cut(V a , V \ V a ) = cut(V b , V \ V b ) = 1, it can be inferred that each isomorphism class in the universe contains at most 2(v!) 2 labeled graphs. The remaining n! − 2(v!) 2 permutations yield isomorphic graphs with cut larger than one.</p><p>Claim C.1. For any δ &gt; 0, cut(V a , V \ V a ) = cut(V b , V \ V b ) = 1, and p ≥ (δ + log v )/v, we have i c (v) ≤ 2(v!) 2 with probability at least e −2e −δ + o(1).</p><p>Proof. To see this consider a labeled graph G ∈ X and let G = (V , E ) be a second labeled graph that is isomorphic to G, induced by a the label permutation V = (Π(u) : u ∈ V). I claim that, if there exist v i , v j ∈ V a for which Π(v i ) ∈ V a and Π(v j ) ∈ V b , then G / ∈ X (and the same holds if there exist v</p><formula xml:id="formula_43">i , v j ∈ V b for which Π(v i ) ∈ V b and Π(v j ) ∈ V b ).</formula><p>The claim is proven by contradiction: suppose (for now) that G a and G b are connected. Then, for every set S of cardinality v that is a strict subset of both V a and V b (S corresponds to the nodes with labels (1, • • • , v) in G ) the cut between S and its complement must be cut(S, V \ S) = vi,vj {v i ∈ S and v j / ∈ S} = vi,vj {v i ∈ S and v j ∈ (V a \ S)} + vi,vj {v i ∈ S and v j ∈ (V b \ S)} ≥ 1 + 1. The latter, however, is impossible as we have assumed that ∀G ∈ X , we must have cut(</p><formula xml:id="formula_44">V a , V \V a ) = cut(V b , V \ V b ) = 1.</formula><p>Therefore, the only valid permutations Π are those that abide to either (a) if</p><formula xml:id="formula_45">v i ∈ V a → Π(v i ) ∈ V a and if v i ∈ V b → Π(v i ) ∈ V b (there are (v!) 2 such permutations), or (b) if v i ∈ V a → Π(v i ) ∈ V b and if v i ∈ V a → Π(v i ) ∈ V a (there are (v!) 2 such permutations).</formula><p>In the studied distribution, there is a non-zero probability that a disconnected graph appears. However, the probability is exponentially small when p &gt; log v/v. It is well known (see e.g., Theorem 4.1 by Frieze and Karoński <ref type="bibr" target="#b49">[50]</ref>) that, for any δ &gt; 0 and p = δ+log v v , a random graph on v nodes is connected with probability Based on the above observation, the conditional entropy of f (G) can be rewritten as</p><formula xml:id="formula_46">H 2 (f isom (G)|G c ) = y∈Y P(f isom (G) = y|G c ) log 2 1 P(f isom (G) = y|G c ) ≥ v(v−1) k=0 v(v−1) k i c (v) i c (v) p k (1 − p) v(v−1)−ky log 2 1 i c (v) p k (1 − p) v(v−1)−k = v(v−1) k=0 v(v − 1) k p k (1 − p) v(v−1)−k − log 2 i c (v) + v(v − 1) log 2 1 1 − p + k log 2 1 − p p = log 2 1 − p p   v(v−1) k=0 v(v − 1) k p k (1 − p) v(v−1)−k k   + v(v − 1) log 2 1 1 − p − log 2 i c (v)</formula><p>Let B be a binomial random variable with parameters v(v − 1) and p. The summation term is equivalent to the expectation of B:</p><formula xml:id="formula_47">v(v−1) m=0 v(v − 1) k p k (1 − p) v(v−1)−k k = E[B] = v(v − 1)p</formula><p>and, therefore,  This can be seed to be identical to the worst-case bound encountered above. The derivation thus can be carried out analogously (and the same holds for c one fisom (T v ) by Lemma D.1). Finally, the upper bound O(v) follows by the same argument as in the proof of Theorem 3.2, where now the number of edges of each of G a and G b is v − 1.</p><formula xml:id="formula_48">H 2 (L π ) ≥ log 2 1 − p p v(v − 1)p + v(v − 1) log 2 1 1 − p − log 2 i c (v) = v(v − 1)H 2 (p) − log 2 i c (v) (by definition H 2 (p) = log 2 1−p p p + log 2 1 1−p ) = v(v − 1)H 2 (p) − 2 log 2 v! − 1 (see Claim C.1 i c (v) ≤ 2(v!) 2 ) ≥ v(v −</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Proof of Lemma 3.1</head><p>In general terms, the impossibility statement comes as a consequence of the definition of communication complexity: if the number of required exchanged symbols exceeds the symbols the learner can exchange (i.e., its communication capacity) then the latter will not be able to identify exactly f isom .</p><p>The specifics depend on the appropriate definition: Majority-voting necessitates |M G | ≥ µ, meaning that when |M G | ≥ µ &gt; n − 2v at least one of the two parties should have gathered sufficient information to determine f isom (G) at the final layer. Therefore, m should be "one". With consensus on the other hand, we have that |M G | ≥ n−µ &gt; n−v which implies that both parties need to know the class.</p><p>The worst-case communication complexity definition guarantees that there exists at least one input for which the required number of symbols is c (m) fisom . Thus, since D is densely supported on X , the impossibility must occur with strictly positive probability. The impossibility also applies to any universe X that is a strict superset of X . This can be easily derived by conditioning on X ⊂ X (which can only decrease the communication complexity) and repeating the analysis identically.</p><p>The implications of the expected complexity bound, are two-fold:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) In MPNN, nodes exchange information by sending and receiving messages along edges.Communication capacity is the maximal amount of information that can be sent across two subgraphs (depicted in orange and green) (b) Communication complexity is the minimal amount of information needed so that two parties jointly compute a function f . (c) To determine whether graphs G and G are isomorphic one may use an MPNN g to test whether g(G) = g(G ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: A visual depiction of a graph G = (V, E) chosen from X . G a (in yellow) and G b (in green) are chosen from families X a and X b of graphs with v nodes. The edges of G c (dashed lines) may connect to any node but should induce a (V a , V b )-cut of at most τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Lemma 3.1 in place, the proof of Theorem 3.1 follows from Theorems 3.2, 3.3 and 3.4 by setting v = n/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>r b o u n d (m a jo ri ty ) lo w e r b o u n d (c o n s e n s u s ) nd (m ajo rity ) low er bo un d (co nse nsu s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Test accuracy in terms of communication capacity and the number of nodes for 4 graph (left) and 8 tree isomorphism tasks (right). Each marker corresponds to a trained network. Networks of high (low) accuracy as plotted with large green (small red) markers. The two dashed colored lines connect the smallest-capacity networks that attain 50% and 99% accuracy, respectively. The two gray regions at the bottom of the figure correspond to the proposed distribution-dependent lower bounds for a majority and consensus readout function. Best seen in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training and test accuracy as a function of communication capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Accuracy as a function of capacity-normalized depth and width. Depth and width are partially exchangeable for graph and tree isomorphism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: An example of the reduction employed in the proof of Lemma 2.1. The yellow and green subgraphs correspond respectively to G a and G b . The global state (external memory) is shown in orange. Each edge is annotated based on its capacity in the maximum flow reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>.</head><label></label><figDesc>This lower bound derivation finishes by factoring c one fisom as a function of c both fisom . To conclude the proof, one notes the following elementary upper bound: to compute f isom (G), Bob and Alice can simply send their entire edge-sets to each other and proceed to compute f (G a , G b , G c ) independently. Then, since the number of edges of a graph v nodes are |E a |, |E b | ≤ v(v − 1)/2, it suffices to exchange c fisom ≤ v(v − 1)/ log 2 s = O(v 2 ) symbols. C.3 Proof of Theorem 3.3 I will begin by proving a more general result. Specifically, it will be shown that the expected communication complexity is directly bounded by the entropy of the isomorphism class of a graph sampled from G. Lemma C.1. The expected number of symbols that Alice and Bob need to exchange to jointly compute the isomorphism class</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>P(G a is connected) = P(G b is connected) = e −e −δ + o(1)and, by independence, P(G is connected) = e −2e −δ + o(1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 log s v + 1 2 log s 14 implying c one fisom ≥ β+log s 2 2 .</head><label>22</label><figDesc>log s (|{f (G a , G b , G c ) : G a ∈ X a }|) = c both fisom − log s t(v) ∼ log s α v v −5/2 − log s (c/2) ∼ v log s α −5Let me now consider the case that G is sampled uniformly at random from the set of all trees in X .It is a consequence of Lemma C.1 that when the graph(G a , G b , G c ) ∼ G (conditioned on G c) is sampled uniformly at random from a collection of isomorphism classes, the expected communication complexity is at leastc both fisom (T v ) ≥ minGc log s |{f (G a , G b , G c ) : G ∈ X s.t. G c }|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>provides summary statistics for each of the 12 tasks considered: Details relevant to the 4 graph and 8 tree isomorphism tasks.</figDesc><table><row><cell></cell><cell>X 6 graph</cell><cell>X 8 graph</cell><cell>X 10 graph</cell><cell>X 12 graph</cell><cell>X 8 tree</cell><cell>X 10 tree</cell><cell>X 12 tree</cell><cell>X 14 tree</cell><cell>X 16 tree</cell><cell>X 18 tree</cell><cell>X 20 tree</cell><cell>X 22 tree</cell></row><row><cell>classes</cell><cell>3</cell><cell>21</cell><cell>231</cell><cell>6328</cell><cell>3</cell><cell>6</cell><cell>21</cell><cell>66</cell><cell cols="4">276 1128 5671 22730</cell></row><row><cell>degree (avg.)</cell><cell>4.0</cell><cell>4.7</cell><cell>5.4</cell><cell>6.0</cell><cell>3.5</cell><cell>3.6</cell><cell>3.7</cell><cell>3.7</cell><cell>3.8</cell><cell>3.8</cell><cell>3.8</cell><cell>3.8</cell></row><row><cell>diameter (avg.)</cell><cell>3.7</cell><cell>4.5</cell><cell>5.0</cell><cell>5.4</cell><cell>4.0</cell><cell>4.3</cell><cell>5.0</cell><cell>5.4</cell><cell>6.0</cell><cell>6.4</cell><cell>6.9</cell><cell>7.3</cell></row><row><cell>dataset size</cell><cell>10k</cell><cell>10k</cell><cell>40k</cell><cell>100k</cell><cell>10k</cell><cell>10k</cell><cell>40k</cell><cell>40k</cell><cell>40k</cell><cell>40k</cell><cell>40k</cell><cell>100k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance of a large-capacity MPNN.</figDesc><table><row><cell></cell><cell>0.8 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8 1.0</cell><cell></cell><cell cols="2">train (n=12) test (n=12)</cell></row><row><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0 0.2</cell><cell>2</cell><cell cols="2">4 train (n=6) 8 MPNN capacity 16 32 test (n=6)</cell><cell>64 128</cell><cell>0.0 0.2</cell><cell>2</cell><cell>4</cell><cell>8 MPNN capacity 16 32</cell><cell>64 128 train (n=8) test (n=8)</cell><cell>0.0 0.2</cell><cell>2</cell><cell>4</cell><cell>8 MPNN capacity 16 32</cell><cell>64 128 test (n=10) train (n=10)</cell><cell>0.0 0.2</cell><cell>2</cell><cell>4</cell><cell>8 MPNN capacity 16 32</cell><cell>64 128</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(a) graph isomorphism</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8 1.0</cell><cell></cell><cell cols="2">train (n=12) test (n=12)</cell><cell></cell><cell>0.8 1.0</cell><cell></cell><cell cols="2">train (n=14) test (n=14)</cell></row><row><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0 0.2 0.8 1.0</cell><cell>2</cell><cell cols="2">4 train (n=8) 8 MPNN capacity 16 32 test (n=8) train (n=16) test (n=16)</cell><cell>64 128</cell><cell>0.0 0.2 0.8 1.0</cell><cell>2</cell><cell cols="2">4 train (n=18) 8 MPNN capacity 16 32 test (n=18)</cell><cell>64 128 train (n=10) test (n=10)</cell><cell>0.2 0.0 0.8 1.0</cell><cell>2</cell><cell cols="2">4 train (n=20) 8 MPNN capacity 16 32 test (n=20)</cell><cell>64 128</cell><cell>0.2 0.0 0.8 1.0</cell><cell>2</cell><cell cols="2">4 train (n=22) 8 MPNN capacity 16 32 test (n=22)</cell><cell>64 128</cell></row><row><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell>accuracy</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>2</cell><cell>4</cell><cell>8 MPNN capacity 16 32</cell><cell>64 128</cell><cell>0.0</cell><cell>2</cell><cell>4</cell><cell>8 MPNN capacity 16 32</cell><cell>64 128</cell><cell>0.0</cell><cell>2</cell><cell>4</cell><cell>8 MPNN capacity 16 32</cell><cell>64 128</cell><cell>0.0</cell><cell>2</cell><cell>4</cell><cell>8 MPNN capacity 16 32</cell><cell>64 128</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) tree isomorphism</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1)H 2 (p) − 2 v log 2 one fisom (B v,p ) log 2 s ≥ c both fisom (B v,p ) log 2 s − max G b ,Gc log s (|{f isom (G a , G b , G c ) : G a ∈ X a }|) log 2 s = c both fisom (B v,p ) log 2 s − log 2According to Otter<ref type="bibr" target="#b50">[51]</ref>, the number of unlabeled trees on v nodes grows liket(v) ∼ c α v v −5/2 ,where the values c and α known to be approximately 0.5349496 and 2.9557652 (sequence A051491 in the OEIS). Moreover, it was shown in the proof of Theorem 3.2, the number of monochromatic rectangles is at least (t(v) + 1) t(v)/2.∼ 2 log s α v v −5/2 − log s (c 2 /2) ∼ 2v log s α − 5 log s v + log s 7 = βFurther, from Lemma D.1 one can derive:</figDesc><table><row><cell cols="2">Corollary B.1 then implies</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>c both fisom ≥ log s</cell><cell cols="3">(t(v) + 1) t(v) 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>≥ log s</cell><cell>t(v) 2 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">c one fisom ≥ c both fisom − max</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">v e log 2 ve c |X a | + 1 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">v!</cell></row><row><cell cols="4">= c both fisom (B v,p ) log 2 s −</cell><cell cols="3">v(v − 1) 2</cell><cell cols="2">+ log 2 (v!)</cell></row><row><cell cols="4">≥ v(v − 1) H 2 (p) −</cell><cell>1 2</cell><cell cols="3">− v log 2</cell><cell>v e</cell><cell>+</cell><cell>1 2</cell><cell>log 2 ve 2 − 1</cell></row><row><cell cols="2">= v 2 H 2 (p) −</cell><cell>v 2</cell><cell cols="2">2 log 2</cell><cell>v e</cell><cell cols="3">+ H 2 (p) −</cell><cell>1 2</cell><cell>log 2 2ve 2 −</cell><cell>v 2 − v + vH 2 (p) + 1 2</cell></row></table><note>2 − 1 (since x! ≤ √ xe 2 (x/e) x ) = v 2 H 2 (p) − v 2 log 2 v e + H 2 (p) − log 2 2ve 2Invoking Lemma C.1, one obtains:c both fisom (B v,p ) ≥ min Gc H 2 (f isom (G)|G c ) log 2 s ≥ v 2 H s (p) − v 2 log s v e + H s (p) − log s 2ve 2 = β(9)Then Lemma D.1 gives:= β log 2 s − v 2 + v(1 − H 2 (p)) − 1 2 implying c one fisom (B v,p ) ≥ β 2 − v 2 −v(1−H2(p))+12log 2 s . C.4 Proof of Theorem 3.4 G b ,Gc</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Though usually it is assumed that the parties communicate using binary symbols, i.e., S = {0, 1}, the set could also be defined more abstractly to contain s symbols.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I would like to express gratitude to the anonymous reviewers, as well as Nathanaël Perraudin, Nikolaos Karalias and Giovanni Cherubin for their insightful comments. I am also thankful to the Swiss National Science Foundation for financially supporting this work in the context of the project "Deep Learning for Graph-Structured Data" (grant number PZ00P2 179981).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>First, if g is adaptive, its capacity c g is a random variable over the input distribution. The bound then asserts that E[c g ] ≥ c m fisom (D). For networks of fixed size, one may derive a bound on the probability of error. Specifically, fix π * to be the protocol that achieves minimal expected length and let β m be an upper bound of π * length over all inputs. By Lemma D.3, for any δ ∈ [0, 1] one has</p><p>The above is a bound on the probability of error for a network that satisfies c g ≤ 2δ c m fisom (D). One can also generalize the previous result to distributions D defined on a strict superset X of X that is (up to normalization) identical with D within X :</p><p>Then, the probability of error w.r.t. D is at least c</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Helpful lemmata</head><p>Lemma D.1. In the universe considered in Section 3, the following hold for any D:</p><p>Proof. Consider the setting of c one f , where for a successful termination it suffices for one party to compute the output of f . Suppose w.l.o.g., that this party is Alice. In particular, Alice determines class y = f (G a , G b , G c ) based on a protocol π of minimal length. In this setting, Bob does not know y but he is aware of X b (and G c ), where is the leaf of the protocol tree at input (G a , G b , G c ). Therefore, both parties know that the class must belong to the set {f</p><p>It is a consequence that there exists a protocol π of length</p><p>that results in both parties knowing y. The protocol π entails first simulating π and then Alice sending to Bob the index of y in the set of feasible classes. Moreover, since f corresponds to the graph isomorphism problem, for Alice to know y, she must also know the isomorphism class of Bob. Therefore, the feasible set of classes contains only the feasible subgraph isomorphism classes of G a , which are at most</p><p>The claimed inequalities then follow by the optimality of the protocol π and since the same construction can be repeated for every input.</p><p>Lemma D.2. Let X be a categorical random variable with sample space X . For any partitioning</p><p>Proof. The proof is elementary. It relies on the inequality P(X = x) ≤ P(X ∈ A i ) that holds for all x ∈ A i :</p><p>as claimed.</p><p>Lemma D.3. For any random variable X ≤ β and δ ∈ [0, 1] we have P(X &gt; δ E[X]) ≥ 1−δ r−δ , where r = β/E[X].</p><p>Proof. For any t ≤ β, E[X] = x≤t P(X) x + x&gt;t P(X) x ≤ P(X ≤ t) t + P(X &gt; t) β = (1 − P(X &gt; t))t + P(X &gt; t) β or, equivalently, P(X &gt; t) ≥ (E[X] − t)/(β − t). The final inequality is obtained by setting t = δE[X].</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimrod</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7092" to="7101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15894" to="15902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04025</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the representation power of graph neural networks in learning graph topology</title>
		<author>
			<persName><forename type="first">Nima</forename><surname>Dehmamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15413" to="15423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06157</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The power of graph convolutional networks to distinguish random graph models</title>
		<author>
			<persName><forename type="first">Abram</forename><surname>Magner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Alfred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Information Theory (ISIT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2664" to="2669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Let&apos;s agree to degree: Comparing graph convolutional networks in the message-passing framework</title>
		<author>
			<persName><forename type="first">Floris</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Mazowiecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><forename type="middle">A</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02593</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.04078</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Riberio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML 2019)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building powerful and equivariant graph neural networks with structural message-passing</title>
		<author>
			<persName><forename type="first">Clement</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Coloring graph neural networks for node disambiguation</title>
		<author>
			<persName><forename type="first">George</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dos</forename><surname>Ludovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aladin</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName><surname>Virmaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06058</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03155</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graph warp module: an auxiliary module for boosting the power of graph neural networks</title>
		<author>
			<persName><forename type="first">Katsuhiko</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01020</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adaptive propagation graph convolutional network</title>
		<author>
			<persName><forename type="first">Indro</forename><surname>Spinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Uncini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10306</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural execution of graph algorithms</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matilde</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Yehudayoff</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108671644</idno>
		<title level="m">Communication Complexity: and Applications</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Three notes on distributed property testing</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Even</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orr</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Fraigniaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzlil</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reut</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moti</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Montealegre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Olivetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rotem</forename><surname>Oshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Rapaport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st International Symposium on Distributed Computing</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lower bounds for subgraph detection in the congest model</title>
		<author>
			<persName><forename type="first">Tzlil</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rotem</forename><surname>Oshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Principles of Distributed Systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2018</date>
		</imprint>
	</monogr>
	<note>Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of control, signals and systems</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The expressive power of neural networks: A view from the width</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7203-the-expressive-power-of-neural-networks-a-view-from-the-width.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6231" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00481</idno>
		<title level="m">Mincut pooling in graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Algebraic distance on graphs</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Safro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3468" to="3490" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Advanced coarsening schemes for graph partitioning</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Safro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Algorithmics (JEA)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2" to="2" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spectrally approximating large graphs with smaller graphs</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3237" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph reduction with spectral and cut guarantees</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">116</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph coarsening with preserved spectral properties</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Jaja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4452" to="4462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The logical expressiveness of graph neural networks</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Barceló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Egor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Kostylev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Reutter</surname></persName>
		</author>
		<author>
			<persName><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangcheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<idno>CoRR, abs/1905.13192</idno>
		<ptr target="http://arxiv.org/abs/1905.13192" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Practical graph isomorphism</title>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adolfo</forename><surname>Piperno</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jsc.2013.09.003</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0747717113001193" />
	</analytic>
	<monogr>
		<title level="j">{II}. Journal of Symbolic Computation</title>
		<idno type="ISSN">0747-7171</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="94" to="112" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Analytic combinatorics</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Flajolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sedgewick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>cambridge University press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Introduction to random graphs</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michał</forename><surname>Karoński</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The number of trees</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Otter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="page" from="583" to="599" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
