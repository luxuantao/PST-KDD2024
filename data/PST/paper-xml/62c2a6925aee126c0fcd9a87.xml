<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Whose Track Is It Anyway? Improving Robustness to Tracking Errors with Affinity-based Trajectory Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">NVIDIA Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">NVIDIA Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Aeronautics and Astronautics</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">NVIDIA Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Whose Track Is It Anyway? Improving Robustness to Tracking Errors with Affinity-based Trajectory Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-agent trajectory prediction is critical for planning and decision-making in human-interactive autonomous systems, such as self-driving cars. However, most prediction models are developed separately from their upstream perception (detection and tracking) modules, assuming ground truth past trajectories as inputs. As a result, their performance degrades significantly when using real-world noisy tracking results as inputs. This is typically caused by the propagation of errors from tracking to prediction, such as noisy tracks, fragments and identity switches. To alleviate this propagation of errors, we propose a new prediction paradigm that uses detections and their affinity matrices across frames as inputs, removing the need for errorprone data association during tracking. Since affinity matrices contain "soft" information about the similarity and identity of detections across frames, making prediction directly from affinity matrices retains strictly more information than making prediction from the tracklets generated by data association. Experiments on large-scale, real-world autonomous driving datasets show that our affinity-based prediction scheme 1 reduces overall prediction errors by up to 57.9%, in comparison to standard prediction pipelines that use tracklets as inputs, with even more significant error reduction (up to 88.6%) if restricting the evaluation to challenging scenarios with tracking errors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Tightly integrating multi-object tracking and trajectory prediction methods within an autonomy stack is a significant challenge due to the fact the most prior works develop multi-object tracking <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b47">49]</ref> and trajectory prediction <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b44">46]</ref> approaches in isolation. Recently, <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b43">45]</ref> showed that feeding the out-  puts of a modern tracking method directly into a prediction model can significantly degrade performance (e.g., up to 28× higher prediction error) in comparison to the common idealized prediction setting using perfect inputs (i.e., ground truth trajectories). Unfortunately, this challenge of integration with an explicit consideration of robustness to tracking errors is still largely under-explored in the field.</p><p>Why does prediction accuracy significantly drop when using tracking results as inputs? Prior work <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b43">45]</ref> has shown that tracking errors such as identity switches and fragments, which induce velocity or orientation estimation errors, are to blame. Typically, identity switches occur from mistakes in data association when two different objects are marked as the same due to ambiguity, e.g., their appearance or geometric features are similar or they are very close. Fragments occur when data association fails to find a detection to match with an existing tracklet. Now, is there an effective way to avoid these tracking errors from propagating to prediction? In this work, we argue that there is: skip data association altogether as shown in Fig. <ref type="figure" target="#fig_1">1</ref>, since it may cause identity switches and fragments.</p><p>Our key insight can be summarized as follows: Broadly, data association converts a "soft" affinity matrix (comprised of scalars between 0 and 1) to a boolean matrix (containing only 0 or 1) through a "hard" matching. By removing the data association step, we aim to preserve this "soft" information that is otherwise thrown away when "hard" matching is performed during data association. Because we skip the data association step, our method can be more accurate in the case where the affinity values are similar between two pairs of objects and it is ambiguous to perform correct matching and easy to make data association errors.</p><p>Contributions. We propose to directly use raw detections and intermediate tracking results, i.e., affinity matrices, as inputs to prediction. To fully exploit the raw detections and affinity matrices for prediction, we propose:</p><p>(1) An Affinity-based prediction (Affinipred) framework that removes the need for input past trajectories and also the error-prone data association step. (2) A transformer architecture that models joint attention between all detections at all input frames and can deal with a variable size of detections across frames. (3) An affinity-based attention mechanism that directly incorporates full object affinity information across time.</p><p>In addition to the above contributions, Affinipred also leverages advancements from prior work by using a conditional variational autoencoder (CVAE) <ref type="bibr" target="#b16">[18]</ref> to produce multi-modal predictions <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b30">32]</ref>, performing joint interaction modeling in the decoder to produce scene-consistent predictions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">35]</ref>, and incorporating maps in the inputs to capture environmental information <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b46">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Trajectory Prediction. Significant advancements in trajectory prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">16,</ref><ref type="bibr" target="#b17">[19]</ref><ref type="bibr" target="#b18">[20]</ref><ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b27">[29]</ref><ref type="bibr" target="#b28">[30]</ref><ref type="bibr" target="#b29">[31]</ref> have been made in recent years. Yet, almost invariably, these works address trajectory prediction in an idealized setting, i.e., using ground truth past trajectories as inputs for training and evaluation, with no direct accounting of perception errors. As a result, it is non-trivial to transfer models trained in this idealized setting to the real world, where we typically use noisy tracking results as inputs to prediction. Therefore, mitigating the propagation of errors from tracking to prediction is of critical importance, and serves as the motivation of this work. Different from prior work, our affinity-based prediction does not assume ground truth trajectories as inputs and provides significantly higher robustness to tracking errors.</p><p>Tracking-Prediction Integration. Although the majority of prior work focuses on trajectory prediction separately from tracking, there have been a few works which address the integration of tracking and prediction. For example, <ref type="bibr" target="#b21">[23]</ref> proposed an end-to-end cascading detection, tracking and prediction network that is jointly optimized for these three tasks. Similarly, <ref type="bibr" target="#b41">[43]</ref> proposed a parallelized tracking and prediction framework that can also be jointly optimized. Although these end-to-end methods yield increased performance, they do not explicitly account for tracking errors during prediction. Thus, if tracking results accumulated over frames contain errors, these errors will have a detrimental impact on prediction, as shown in <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b43">45]</ref>. The closest, albeit concurrent, works to ours are <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b43">45]</ref>, which have both identified the impact of tracking errors on prediction and are seeking solutions. <ref type="bibr" target="#b43">[45]</ref> propose enforcing prediction consistency at every frame to fix tracking errors, in turn improving prediction accuracy. Since data association is prone to errors, <ref type="bibr" target="#b38">[40]</ref> propose to use multi-hypothesis data association (MHDA) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">25]</ref> for prediction. Similar to our idea, the use of MHDA also aims to preserve more information in the affinity matrices by generating multiple sets of tracking results, thereby increasing the likelihood of passing accurate tracking results as inputs to prediction. In short, both <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b43">45]</ref> improve prediction robustness by raising the quality of its inputs, i.e., the tracklets generated by tracking, but make no modifications to the prediction model. In contrast, we present a novel prediction framework that uses affinity matrices rather than tracklets as inputs, thereby completely removing the chances of errors occurring in data association and passing more information to prediction.</p><p>Joint Social-Temporal Modeling. In scenarios with many interacting agents, an agent's future behavior depends highly on its belief of other agents' behaviors. As a result, an important aspect of trajectory prediction is agentagent interaction modeling, also known as social-temporal modeling. The most popular approach in prior work is to model social-temporal interactions in a cascaded order, i.e., first temporal modeling followed by social modeling, exemplified by RNNs and Graph Neural Networks (GNNs) <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b41">43]</ref>, or Transformers and RNNs <ref type="bibr" target="#b5">[6]</ref>. However, since social and temporal modeling are not performed jointly, social modeling loses access to agent information in previous frames (this information has already been compressed in temporal modeling). Accordingly, there are recent works which argue that it is beneficial to jointly model socialtemporal interactions, e.g., interleaving social and temporal transformers <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b44">46]</ref> or GNNs <ref type="bibr" target="#b24">[26]</ref> in an iterative manner. We use a similar joint social-temporal modeling mechanism by allowing agent features at any frame to directly attend over features of other agents at any frame. Unlike prior work, we do not have access to agent identity information during joint social-temporal modeling so we use the joint affinity matrix to inject the "soft" identity information.</p><p>3D Multi-Object Tracking. The goal of online tracking is to match previously-computed tracklets with current detections to form new tracklets up to the current frame, and then incrementally obtain trajectories over the entire sequence. During this process, an intermediate affinity matrix is often computed where each entry represents the pairwise similarity between a past tracklet and current detection. A matching algorithm, such as the Hungarian algorithm <ref type="bibr" target="#b35">[37]</ref>, can then be used to perform "hard" matching, assigning de-tections to tracklets. Compared to the standard trackingprediction pipeline that uses tracklets as inputs to prediction, we use the intermediate affinity matrices as inputs, preserving strictly more "soft" information for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>The goal of multi-agent trajectory prediction is to learn a function F that can predict a sequence of future locations for every agent in the scene.</p><formula xml:id="formula_0">Let Y = (Y 1 , Y 2 , • • • , Y N ) ∈ R N ×K×2 denote N frames of K agents' future trajectories. At frame t, Y t = (y 1 t , y 2 t , • • • , y K t ) ∈ R K×2</formula><p>denotes the K agents' ground positions, where each position is represented as a tuple</p><formula xml:id="formula_1">y k t = (u k t , v k t ) ∈ R 2 .</formula><p>In prior work, the inputs to the function F are typically the past trajectories of the same K agents from the previous M frames X = (X 1−M , • • • , X 0 ) and an optional map M of the scene. However, obtaining perfect past trajectories through tracking is challenging and data association errors can easily be propagated to prediction. Accordingly, we seek a different solution that instead uses a list of raw (unassociated</p><formula xml:id="formula_2">) detections D = (D 1−M , • • • , D 0 ) (Sec. 4.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) and their affinity matrices</head><formula xml:id="formula_3">A = (A 1−M 2−M 2 , • • • , A −1 0</formula><p>) between pairs of frames (Sec. 4.2), as shown in Fig. <ref type="figure" target="#fig_3">2</ref> (top left) and (bottom left). Since the future is uncertain and can evolve in many different ways, predictions for each agent should also be multi-modal. Thus, we aim to learn a function F that can map (D, A) (and optionally M) and sampled latent variables Z = (z 1 , • • • , z K ) to future trajectories Y: Y = F(D, A, M, Z).</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Affinity-based Prediction (Affinipred)</head><p>The network of our approach is illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>, it is comprised of five key components: (1) A past embedding layer that encodes features of all flattened detections and optionally a map in past frames; (2) An affinity construction module that converts individual affinity matrices between two frames into one joint affinity matrix between all detections from all frames; (3) A transformer encoder with affinity-based attention for interaction modeling between all detections that outputs context features, which can then be used to generate the prior p θ ; (4) A transformer decoder that predicts future trajectories autoregressively from embeddings, context and sampled latent variables; (5) Another transformer decoder that generates the posterior q ϕ from ground truth (GT) future trajectories and context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Input Representation: Detections</head><p>To avoid using potentially erroneous tracking results to perform prediction, we operate directly from raw detections. While this removes the need for data association, it poses two challenges: incompleteness and loss of identity. Incompleteness. Due to a variety of reasons (occlusion, large distances, sensor failures, small object sizes, etc), state-of-the-art detectors often struggle to provide stable detections across all frames. This means that some objects, although present in the scene, may be not detected in some frames. For example, in Fig. <ref type="figure" target="#fig_3">2</ref> (top left), the blue object is detected in frames t = −2 and t = 0, but not in frame t = −1. Note that the object colors (denoting identity) in Fig. <ref type="figure" target="#fig_3">2</ref> are only used for illustration, and there is actually no identity information in the detections used by our method. Here, we denote raw detections at frame t as</p><formula xml:id="formula_4">D t = (d 1 t , d 2 t , • • • , d Kt t )</formula><p>, where K t is the number of objects detected at time t (which can change across frames).</p><p>Loss of Identity. Since detections across frames are unassociated, we do not have a sequence of data for each object. As a result, standard sequence modeling techniques, e.g., RNNs, cannot be applied to extract trajectory-level features. Further, detections with the same index across frames (i.e., d k t and d k t−1 ) do not necessarily have the same identity. Our Solution. To appropriately process detection inputs while accounting for the above challenges, we propose a joint social-temporal transformer architecture (Sec. 4.3 and 4.5) with affinity-based attention (Sec. 4.4). First, the joint social-temporal transformer models interactions between all detections across all frames so it can operate with a variable number of objects in each frame. Second, we inject affinity matrices into our affinity-based attention module (Sec. 4.4) as separate inputs that preserve "soft" similarity and identity information between all detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Input Representation: Affinity Matrices</head><p>Since affinity matrices provide strictly more information than the identity estimations produced by "hard" matching, affinity is a core facet of our method's input representation. Given K t−1 detections at frame t − 1 and K t detections at frame t, an affinity matrix A t−1 t ∈ R Kt−1×Kt has entries a ij ∈ R that represent the similarity (or probability of corresponding to the same identity, if normalized) between</p><formula xml:id="formula_5">d i t−1 and d j t , where i ∈ [1, K t−1 ] and j ∈ [1, K t ] denote the row and column index of A t−1 t</formula><p>, respectively. Affinity matrices are typically obtained as intermediate results from online multi-object tracking methods <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b47">49]</ref>. However, there are two challenges in using them for prediction: (1) Since online tracking methods aim to solve data association incrementally over frames, affinity matrices are only computed between consecutive frames, e.g., A −2 −1 and A −1 0 , as shown in Fig. <ref type="figure" target="#fig_3">2</ref> (bottom left). As a result, we do not have direct affinity matrices between detections in nonconsecutive frames; (2) Affinity matrices are typically computed between tracklets in the last frame and detections in the current frame, since tracklets are already associated up to the last frame. As a result, the estimated affinity matrix future trajectories Y are also encoded via a transformer decoder to compute the posterior q ϕ so that a KL loss Lkl can be applied between the prior p θ and posterior q ϕ . To produce multimodal predictions, latent variables Z are sampled from the prior (during testing) or the posterior (during training). By combining Z with detections at frame t = 0 (blue, gray, red objects), we can predict their future locations Y in an autoregressive manner via another transformer decoder and an output layer. Finally, a reconstruction loss Lres between Y and Y is also used to trains the network. For illustration purposes, we only show M = 3 past frames (t = −2, t = −1, and t = 0) in the figure <ref type="figure">.</ref> might be inaccurate at a frame if the tracklet construction in the previous frame is inaccurate. We will discuss potential solutions to these challenges in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transformer Encoder</head><p>Past Embedding. We use a transformer encoder to model the joint social-temporal attentions between all detections in all frames. As transformers operate on sequences of data, we first flatten detections into a sequence</p><formula xml:id="formula_6">D f ∈ R Ksum×2 = (d 1 1−M , • • • , d K 1−M 1−M , d 1 2−M , • • • , d 1 0 , • • • , d K0 0 ), where K sum = 0 t=1−M K t is</formula><p>the number of detections in all past frames. Then, a past embedding (fully-connected) layer is used to convert D f into a list of feature embeddings</p><formula xml:id="formula_7">O = (o 1 1−M , • • • , o K0 0 ) ∈ R Ksum×d f . Map Encoding.</formula><p>If available, a map can also (optionally) be included as an input to our model. Specifically, given an object's position d k t = (u k t , v k t ), we crop a local map M k t around the object from the global map M. Then, a convolutional neural network (CNN) encodes the local map as a vector m k t that is concatenated with the object's position before feeding into the past embedding layer. Time Encoding. After flattening detections along the time dimension, the timestamp of each object is lost. To recover the order of time information for embeddings, we apply positional encoding as in <ref type="bibr" target="#b34">[36]</ref>, modified to instead encode time so that objects present at the same timestamp have the same time encoding. This time encoding τ k t is added to the feature embedding o k t for each object to form the overall input embedding e k t to the transformer encoder. Joint Social-Temporal Modeling. Inspired by the original transformer <ref type="bibr" target="#b34">[36]</ref>, we apply a series of self-attention blocks to the input embeddings</p><formula xml:id="formula_8">E = (e 1 1−M , • • • , e K0 0 )</formula><p>, where each block contains a multi-head attention, layer normalization, feed-forward network, and another layer normalization. Each input embedding e k t is also projected into key, query, and value, which are used to compute attention (Sec. 4.4). After B attention blocks, the transformer encoder outputs context features</p><formula xml:id="formula_9">C = (c 1 1−M , • • • , c K0 0 )</formula><p>. Overall, our transformer encoder has two key characteristics: (1) Since the inputs are objects from all past frames, our transformer encoder allows an object at any frame to directly attend to any another object in any frame, enabling We then fill the off-diagonal blocks of A joint with individual twoframe affinity matrices when available, otherwise they are left as zeros. In this example with M = 3 past frames, we can fill in affinity matrices A −1 0 (between frame -1 and 0) and A −2 −1 (between frame -2 and -1). social attention across time, i.e., joint social-temporal modeling. This differs from prior works which use a two-stage approach (e.g., social then temporal or vice versa); (2) Since the inputs are flattened, our transformer encoder can operate on a variable number of objects across frames.</p><formula xml:id="formula_10">𝒜 !" !# Affinity Construction 𝒜 $ !" t = -2 t = -1 t = 0 t = -2 t = -1 t = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Affinity-based Attention</head><p>Joint Affinity Construction. To enable joint attention between all detections in all past frames, we first need to construct the joint affinity matrix A joint ∈ R Ksum×Ksum from individual two-frame affinity matrices A. As an example, let M = 3 (three past frames). In this case, there are two individual affinity matrices, A = (A −2 −1 , A −1 0 ). As shown in Fig. <ref type="figure" target="#fig_4">3</ref>, the joint affinity A joint is a (tridiagonal) block matrix composed of the affinity matrices A −2 −1 , A −1 0 , identity matrix I, and zeros. The diagonal of A joint is identity because the same object k at the same timestamp t must have the same identity to itself. The off-diagonal blocks of A joint are filled with individual two-frame affinity matrices (for consecutive frames) and zeros otherwise.</p><p>Affinity-based Attention. Since an object's future trajectory depends on both its past motion and the motion of other objects, our affinity-based attention models both types of dependency. In general, these types of dependency may have different effects on prediction, so we use separate attention weights to model them by projecting the key K ∈ R Ksum×d f and query Q ∈ R Ksum×d f into two sets:</p><formula xml:id="formula_11">K self = KW K self , Q self = QW Q self ,<label>(2)</label></formula><formula xml:id="formula_12">K other = KW K other , Q other = QW Q other ,<label>(3)</label></formula><p>where K self , Q self ∈ R d f ×d f are used to compute the attention of the object to itself (or to objects that have a high affinity value with it), and K other , Q other are used to compute the attention of the object to others. We then combine these attention weights in an affinity-aware manner by using the joint affinity matrix A joint as a mask:</p><formula xml:id="formula_13">W = A joint ⊙(Q self K T self )+(1−A joint )⊙(Q other K T other ), (<label>4</label></formula><formula xml:id="formula_14">)</formula><p>where ⊙ is an element-wise product and the combined attention weight W ∈ R Ksum×Ksum is a matrix. Finally, we can compute the attended output as in <ref type="bibr" target="#b34">[36]</ref>:</p><formula xml:id="formula_15">V ′ = softmax W d f V,<label>(5)</label></formula><p>where the output embeddings V ′ ∈ R Ksum×d f are used as inputs to the rest of the operations in the attention block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Transformer Decoder</head><p>Different from the transformer encoder that computes the self-attention between all detections, our transformer decoder computes the cross-attention between detections and future positions. As shown in Fig. <ref type="figure" target="#fig_3">2</ref>, our transformer decoder is used in two places: (1) to model attention between detections and predicted trajectories in an autoregressive manner, and (2) to model attention between detections and GT future trajectories to compute the posterior. In both places, the only difference is that the query embeddings are projected from the predicted positions via the prediction embedding layer or from the GT future trajectories via the future embedding layer. Both embedding layers are fully-connected, similar to the past embedding layer as introduced in Sec. 4.3. Affinity-based attention and time encoding are also used in both transformer decoders, and the key and value embeddings are projected from the context feature C from the transformer encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Multi-Modal Prediction</head><p>To produce multi-modal predictions, we apply the standard CVAE framework, sampling latent values Z at runtime and conditioning the model with them to produce multiple plausible joint realizations of agent futures. CVAE Posterior. Following standard CVAE trajectory prediction works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">43]</ref>, the posterior q ϕ (z k t |D, M, Y) is computed with a two-layer Multi-Layer Perceptron followed by two separate fully-connected layers to compute the mean µ k t and variance σ k t of a Gaussian for each input detection</p><formula xml:id="formula_16">d k t . Latent variables Z = (z 1 1−M , • • • , z K0 0 ) ∈ R Ksum×dz can</formula><p>then be sampled for all detected objects during training, which are concatenated with the raw detections D f and (optionally) map features to compute the input embeddings during autoregressive prediction. Finally, the posterior is computed using the outputs from the transformer encoder C and GT future transformer decoder, incorporating information about both the future (Y, M) and the past (D, M). CVAE Prior. The network for computing the prior is similar to the network for computing posterior, except that its inputs are the context feature C that do not contain any information from the future GT trajectories. During testing, we can sample multiple latent variables Z = (z 1  1−M , • • • , z K0 0 ) from the prior p θ (z k t |D, M) to predict multiple samples of object future trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Training Details</head><p>Which agents to predict? Since raw detections are different from trajectory inputs in standard trajectory prediction, a key question is for which objects should we predict? For example, one can predict future locations for all K sum objects detected in past frames. However, some detections in past frames may belong to the same object (although this is unknown in advance), so predicted trajectories may be duplicated, requiring subsequent matching and filtering.</p><p>Another way is to predict trajectories only for the K 0 objects detected in the current frame (t = 0). In this way, no duplicate predictions are made for objects with the same identity, however, some objects might not have predictions if they are not detected in frame t = 0 due to detector failure. In this work, we choose to make K = K 0 rather than K sum , matching standard data processing procedures used in prior works, e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">40]</ref>, and challenges (nuScenes <ref type="bibr" target="#b6">[7]</ref>, ETH/UCY <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b25">27]</ref>), to enable a fair comparison.</p><p>Detection Matching with GT. To train Affinipred, we use GT future trajectories as supervision signals for each detection that needs to be predicted. Accordingly, this requires matching the K 0 detections at frame t = 0 with GT future trajectories. Following the standard matching scheme used for official evaluation in the nuScenes challenge <ref type="bibr" target="#b6">[7]</ref>, we use a distance threshold d thres and Hungarian matching <ref type="bibr" target="#b35">[37]</ref>.</p><p>During training, we filter out detections at frame t = 0 that are not successfully matched with GT (a small percentage), i.e., objects that are most likely false positives.</p><p>Autoregressive Prediction. During both training and testing, we perform autoregressive prediction. Object positions are predicted sequentially, with predictions at t serving as inputs when predicting the next timestamp t + 1. Using Fig. <ref type="figure" target="#fig_3">2</ref> as an example, we begin with detections at frame t = 0 as inputs (blue, gray and red objects). Then, our output layer converts the transformer decoder output to predicted positions</p><formula xml:id="formula_17">Y 1 = ( y 1 1 , • • • , y<label>K0</label></formula><p>1 ) at frame t = 1. By iteratively applying this process, we can obtain predictions in all</p><formula xml:id="formula_18">N future frames Y = ( Y 1 , • • • , Y N ).</formula><p>Training Objectives. We optimize our Affinipred network by minimizing two loss functions: (1) A reconstruction loss between the K 0 predicted Y and GT trajectories Y; and (2) a KL divergence loss between the CVAE prior and posterior for each object being predicted:</p><formula xml:id="formula_19">L = αL res + βL KL = α|| Y − Y|| 2 + β K0 k=1 KL(q ϕ (z k 0 |D, M, Y) || p θ (z k 0 |D, M)),<label>(6)</label></formula><p>where α, β ∈ R &gt;0 are loss function weights. Here, only the prior and posterior of the K 0 objects are used because we only predict these objects detected at frame t = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our affinity-based prediction scheme has two core goals: (1) Improve overall prediction performance in a realistic setting when using tracking results as inputs, and (2) Increase robustness to upstream tracking errors. To evaluate our method's efficacy in addressing these, we make use of standard prediction datasets, but use detection and tracking results as inputs rather than GT past trajectories. In addition to evaluating prediction accuracy for all detected objects ("global evaluation"), we follow <ref type="bibr" target="#b38">[40]</ref> and evaluate prediction performance for objects with certain types of tracking errors ("targeted evaluation"), to evaluate our method's robustness to tracking errors. At a high level, we find that our affinity-based prediction significantly improves real-world prediction performance and robustness to tracking errors, compared to standard tracklet-based prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Hyper-parameters. We use α = β = 1 in Eq. ( <ref type="formula" target="#formula_19">6</ref>), d f = 256 for embeddings, d z = 32 for latent variables, and d thres = 2 when matching detections with GT trajectories.</p><p>Network Architecture. We use a 4-layer CNN to extract 32-dimensional map features. We use one fully-connected layer in all four embedding layers (past, future, prediction embedding, and output). We use B = 2 attention blocks in both the transformer encoder and decoder, and 8 heads in the affinity-based multi-head attention.</p><p>Data Pre-processing. We apply linear interpolation and extrapolation to impute missing data in any of the past M and future N frames. We also apply three types of data augmentation to increase the robustness of prediction with respect to tracking errors. In particular, we inject identity switches (IDS), fragments (FRAG), and noise. For IDS, we swap the identity of two objects in the current frame with 30% probability if they are within 5 meters. For FRAG, we randomly drop data in past frames with 30% probability. Finally, we perturb the u, v positions of each object with a maximum magnitude of 0.4 of its width and length in previous frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Methodology</head><p>Baselines. In all experiments, we compare against a standard tracking-prediction pipeline that is composed of a detector, tracker, and prediction model. On KITTI, these are PointRCNN <ref type="bibr" target="#b32">[34]</ref>, AB3DMOT <ref type="bibr" target="#b39">[41]</ref>, and PTP <ref type="bibr" target="#b41">[43]</ref>, respectively. On nuScenes, these are Megvii <ref type="bibr" target="#b48">[50]</ref>, AB3DMOT <ref type="bibr" target="#b39">[41]</ref>, and PTP <ref type="bibr" target="#b41">[43]</ref>, respectively. We also compare against MTP <ref type="bibr" target="#b38">[40]</ref>, a strong, recent work that tackles the same problem of improving prediction robustness to upstream tracking errors. Specifically, MTP <ref type="bibr" target="#b38">[40]</ref> explicitly accounts for tracking errors in prediction via multi-hypothesis data association. To fairly compare with these baselines, we use the same detection and tracking methods to obtain the inputs.</p><p>Datasets. We use the standard KITTI <ref type="bibr" target="#b12">[13]</ref> and nuScenes <ref type="bibr" target="#b6">[7]</ref> autonomous driving datasets for evaluation. Since there is no official prediction evaluation server for KITTI, we evaluate on the KITTI tracking <ref type="bibr" target="#b0">[1]</ref> validation set, using the standard train/val splits from <ref type="bibr" target="#b31">[33]</ref>. We consider three main object categories during evaluation: cars, pedestrians and cyclists. To fairly compare against <ref type="bibr" target="#b38">[40]</ref>, we use M = 10 past frames and predict N = 10 future frames. To match objects with their GT trajectories, we compute 3D Intersection over Union (IoU) and use a standard threshold of 0.5 <ref type="bibr" target="#b39">[41]</ref>.</p><p>For nuScenes, we follow official prediction challenge guidelines <ref type="bibr" target="#b1">[2]</ref>: (1) using the official train, val, test splits <ref type="bibr" target="#b2">[3]</ref>;</p><p>(2) evaluating only on vehicle classes; (3) using M = 4 past frames to predict N = 12 future frames; and (4) using a threshold distance of 2 meters when matching predictions with GT trajectories to compute metrics. Although we follow official nuScenes evaluation guidelines, it is important to note that the minADE S , minFDE S values in Tables <ref type="table" target="#tab_3">1 through 4</ref> are not comparable to values on the nuScenes leaderboard (which use GT past trajectories for evaluation).</p><p>Metrics. To evaluate prediction accuracy, we use the standard Minimum Average and Final Displacement Error metrics over S predicted samples (minADE S , minFDE S ):</p><formula xml:id="formula_20">minADE S = min s 1 K 0 K0 k=1 1 N k valid N t=1 F k m || y s k t − y k t || 2 , minFDE S = min s 1 K 0 K0 k=1 || y s k T k − y k T k || 2</formula><p>, where K 0 is the number of objects detected in the current frame, y s k t is the predicted position of object k at frame t in the s th sample. Since not all objects have complete GT future trajectories (e.g., they may leave the scene early), a frame-wise mask F k m is used for object k to compute errors only on the frames where GT exists. As a result, the number of total evaluated frames for object k is N k valid ≤ N . Similarly, when computing minFDE S , T k ≤ N is the last frame of object k. Lastly, we evaluate with S = 20 on KITTI and S = 10 on nuScenes to have a fair comparison with <ref type="bibr" target="#b38">[40]</ref>.</p><p>Tracking Evaluation. In order to analyze prediction performance on objects with tracking errors, it is necessary to determine which objects have tracking errors. Following <ref type="bibr" target="#b38">[40]</ref>, we consider two common tracking errors: identity switches (IDS) and fragments (FRAG). We use the standard 3D tracking evaluation code released in <ref type="bibr" target="#b39">[41]</ref> to identify which objects have IDS/FRAG errors per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and Analysis</head><p>Targeted Evaluation. Table <ref type="table" target="#tab_0">1</ref> summarizes the performance of all methods for objects with tracking errors. We can see that our method significantly outperforms all others on every dataset and tracking error type. For instance, on nuScenes objects with IDS, we reduce minADE S by 71.8% (from 3.923 to 1.106), and reduce minFDE S by 74.5% (from 6.210 to 1.584). Importantly, we can directly compare to MTP <ref type="bibr" target="#b38">[40]</ref> because our method uses the same detection and tracking outputs (including intermediate affinity matrices) as MTP <ref type="bibr" target="#b38">[40]</ref>. Thus, the set of considered IDS/FRAG objects is exactly the same for both methods. Our method's performance is further highlighted when comparing against a standard detection-tracking-prediction system that uses the same detection <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b48">50]</ref> and tracking <ref type="bibr" target="#b39">[41]</ref> methods. For example, on nuScenes objects with IDS, we reduce minADE S by 86.7% (from 8.345 to 1.106), and reduce minFDE S by 88.6% (from 13.892 to 1.584). All these results confirm that our affinity-based prediction has significantly improved the robustness of prediction with respect to IDS and FRAG, which is reasonable as the standard detection-tracking-prediction system does not explicitly account for tracking errors.</p><p>Global Evaluation. Table <ref type="table" target="#tab_2">3</ref> summarizes the performance of methods for all detected objects. Similar to the trend in targeted evaluation, our method significantly outperforms a standard detection-tracking-prediction pipeline. For example, we reduce minADE S by 57.9% (from 2.320 to 0.977) on nuScenes. Also, our method significantly outperforms MTP <ref type="bibr" target="#b38">[40]</ref>, even when allowing it 20× more samples. The magnitude of error reduction our method achieves in Table <ref type="table" target="#tab_2">3</ref> is less drastic than in Table <ref type="table" target="#tab_0">1</ref> because global evaluation also includes objects whose future trajectories are easier to predict, i.e., without any upstream detection or tracking errors.</p><p>Ablation Study. Table <ref type="table" target="#tab_1">2</ref> summarizes the results, where the last row corresponds to our full method. In the second-last row, we replace our method's core advancement (the use of affinity and detection inputs) with tracklets, and observe a significant drop in both targeted and global performance. This signifies that our core contribution is effective at both improving overall prediction performance and improving model robustness to tracking errors.</p><p>We then disable GT matching, meaning future tracklets from tracking are used in place of <ref type="bibr">GT</ref>   during training. Since tracklets are noisy, performance sensibly decreases in all settings. Finally, we disable data augmentation and imputation (linear extrapolation and interpolation), and the resulting decrease in performance confirms that these pre-processing steps are also important to improve prediction robustness, especially so for objects with IDS and FRAG errors. Interestingly, even without these data pre-processing steps and affinity-based inputs, our base prediction network performs similarly to MTP <ref type="bibr" target="#b38">[40]</ref>.</p><p>Training with Augmented GT. As we have seen in Table <ref type="table" target="#tab_1">2</ref>, augmenting the inputs with IDS, FRAG, and noises during training improves prediction accuracy and robustness to tracking errors. Seeing this, one may wonder if we can apply the same augmentations to a standard tracklet-based prediction method that is trained with GT trajectories. How would this compare to our Affinipred? To answer this question, we train a tracklet-based version of our base prediction network (i.e., the second row of Table <ref type="table" target="#tab_1">2</ref>) on nuScenes with different tracking error augmentations applied to GT trajectories (as described in Section 5.1). Table <ref type="table" target="#tab_3">4</ref> summarizes the results and shows that, even with the combination of all three input data augmentations, a tracklet-based method trained with GT trajectories is also outperformed by our Affinipred, which is the case for both objects with ID switches and fragments.</p><p>Runtime Speed. The speed of our method depends on the number of objects present per input frame. On a single GeForce RTX 2080, our method can run at 4.7 FPS with an average of 30 detections per input frame. There are a number of ways to make our method faster beyond this, such as switching from autoregressive to batch predictions, optimizing the network architecture to reuse computation, and generally optimizing our (currently unoptimized PyTorch) codebase for performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations and Conclusions</head><p>There are two main limitations (and thus areas of future work) in our affinity-based prediction method (Affinipred). First, how can one obtain affinity matrices between nonconsecutive frames? Further, how can one obtain affinity matrices between two frames of detections rather than between past tracklets and current detections (Sec. 4.2)? Along these lines, an interesting area of future work is to design an affinity estimation network that can directly output the joint affinity matrix A joint . Second, is there an elegant way to predict the futures of all objects present in any past frame, rather than only those detected in the current frame? Such a capability would be useful for handling occlusions, being able to predict for objects that were in a past frame but are occluded in the current frame.</p><p>In conclusion, we present a novel affinity-based prediction scheme that only requires detections and their affinity matrices across frames as inputs, entirely removing the need for error-prone data association. Since affinity matrices contain "soft" information about the similarity and identity of detections across frames, making prediction using affinity matrices retains strictly more information than making prediction using tracklets generated by data association. Experiments on the KITTI and nuScenes autonomous driving datasets show that our affinity-based prediction scheme reduces overall prediction errors by up to 57.9% in comparison to standard tracklet-based prediction pipelines, with even more significant error reductions (up to 88.6%) when restricting evaluation to objects with tracking errors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>past tracklets anymore Directly using affinity matrices as inputs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. In contrast to standard tracking-prediction pipelines, our affinity-based prediction (Affinipred) skips data association, directly using detections and affinity matrices as prediction inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 A 1</head><label>1</label><figDesc>−M 2−M denotes the affinity matrix between frame 2 − M and 1 − M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Affinity-based Prediction. Given inputs of past data (raw detections D, map M, and affinity matrices A between consecutive frames), our goal is to predict multiple samples of future trajectories Y. To model joint social-temporal attention, all detections are flattened D f before extracting O via the past embedding layer. To recover the order of time information in raw detections, a time encoding τ is added to O to form the input embedding E. Then, a transformer encoder containing a series of self-attention blocks is applied to output context embeddings C, with the "soft" object identity information contained in A injected through affinity-based attention. During training,future trajectories Y are also encoded via a transformer decoder to compute the posterior q ϕ so that a KL loss Lkl can be applied between the prior p θ and posterior q ϕ . To produce multimodal predictions, latent variables Z are sampled from the prior (during testing) or the posterior (during training). By combining Z with detections at frame t = 0 (blue, gray, red objects), we can predict their future locations Y in an autoregressive manner via another transformer decoder and an output layer. Finally, a reconstruction loss Lres between Y and Y is also used to trains the network. For illustration purposes, we only show M = 3 past frames (t = −2, t = −1, and t = 0) in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Joint Affinity Construction. To construct A joint , used in affinity-based attention, we first fill its diagonal with identity matrices I, since object identities are fixed in the same timestamp.We then fill the off-diagonal blocks of A joint with individual twoframe affinity matrices when available, otherwise they are left as zeros. In this example with M = 3 past frames, we can fill in affinity matrices A −1 0 (between frame -1 and 0) and A −2 −1 (between frame -2 and -1).</figDesc><graphic url="image-14.png" coords="5,152.88,86.47,133.00,58.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Prediction performance for objects with IDS/FRAG.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Targets Methods</cell><cell cols="2">minADE S minFDE S</cell></row><row><cell>KITTI</cell><cell>IDS</cell><cell>PointRCNN+AB3DMOT+PTP, S=20</cell><cell>2.820</cell><cell>4.514</cell></row><row><cell></cell><cell></cell><cell>MTP [40], S=20</cell><cell>0.747</cell><cell>1.173</cell></row><row><cell></cell><cell></cell><cell>MTP [40], S=400</cell><cell>0.707</cell><cell>1.093</cell></row><row><cell></cell><cell></cell><cell>Affinipred (Ours), S=20</cell><cell>0.516</cell><cell>0.792</cell></row><row><cell>KITTI</cell><cell cols="2">FRAG PointRCNN+AB3DMOT+PTP, S=20</cell><cell>1.621</cell><cell>2.155</cell></row><row><cell></cell><cell></cell><cell>MTP [40], S=20</cell><cell>1.335</cell><cell>1.688</cell></row><row><cell></cell><cell></cell><cell>MTP [40], S=400</cell><cell>1.305</cell><cell>1.627</cell></row><row><cell></cell><cell></cell><cell>Affinipred (Ours), S=20</cell><cell>1.063</cell><cell>1.381</cell></row><row><cell cols="2">nuScenes IDS</cell><cell>Megvii+AB3DMOT+PTP, S=10</cell><cell>8.345</cell><cell>13.892</cell></row><row><cell></cell><cell></cell><cell>MTP [40], S=10</cell><cell>3.923</cell><cell>6.210</cell></row><row><cell></cell><cell></cell><cell>MTP [40], S=200</cell><cell>3.321</cell><cell>5.052</cell></row><row><cell></cell><cell></cell><cell>Affinipred (Ours), S=10</cell><cell>1.106</cell><cell>1.584</cell></row><row><cell cols="3">nuScenes FRAG Megvii+AB3DMOT+PTP, S=10</cell><cell>14.520</cell><cell>21.815</cell></row><row><cell></cell><cell></cell><cell>MTP [40], S=10</cell><cell>8.476</cell><cell>12.105</cell></row><row><cell></cell><cell></cell><cell>MTP [40], S=200</cell><cell>7.697</cell><cell>10.606</cell></row><row><cell></cell><cell></cell><cell>Affinipred (Ours), S=10</cell><cell>4.486</cell><cell>5.600</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Inter. Augmentation GT matching Affinity minADE S minFDE S minADE S minFDE S minADE S minFDE S Ablation experiments on the nuScenes prediction test set. S = 10 samples are used to predict N = 12 future frames.</figDesc><table><row><cell>future trajectories</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Prediction performance for all detected objects.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance of a tracklet-based variant of our method trained with augmented GT trajectories compared to our affinitybased prediction scheme on nuScenes. Data imputation with extra-/interpolation is used for both methods.</figDesc><table><row><cell>Inputs</cell><cell></cell><cell>Augmentation</cell><cell></cell><cell>IDS</cell><cell></cell><cell>FRAG</cell><cell></cell></row><row><cell></cell><cell cols="7">IDS FRAG Noises minADES minFDES minADES minFDES</cell></row><row><cell>GT traj.</cell><cell></cell><cell></cell><cell></cell><cell>2.878</cell><cell>4.136</cell><cell>7.314</cell><cell>9.136</cell></row><row><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>1.383</cell><cell>1.906</cell><cell>5.160</cell><cell>6.264</cell></row><row><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell>2.577</cell><cell>3.695</cell><cell>6.610</cell><cell>7.830</cell></row><row><cell></cell><cell></cell><cell></cell><cell>✓</cell><cell>1.897</cell><cell>2.557</cell><cell>6.060</cell><cell>7.460</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>1.314</cell><cell>1.824</cell><cell>4.773</cell><cell>5.959</cell></row><row><cell>Affi. + Det.</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>1.106</cell><cell>1.584</cell><cell>4.486</cell><cell>5.600</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Our project website is at https://www.xinshuoweng.com/ projects/Affinipred.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dataset</forename><surname>Kitti Tracking</surname></persName>
		</author>
		<ptr target="http://www.cvlibs.net/datasets/kitti/eval_tracking.php.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://www.nuscenes.org/prediction?externalData=all&amp;mapData=all&amp;modalities=Any.7" />
		<title level="m">nuScenes Prediction Challenge Guidelines</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/eval/prediction/splits.py.7" />
		<title level="m">nuScenes Prediction Data Split</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Social LSTM: Human Trajectory Prediction in Crowded Spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Score refinement for confidence-based 3D multi-object tracking</title>
		<author>
			<persName><forename type="first">Nuri</forename><surname>Benbarka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jona</forename><surname>Schroder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Zell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots &amp; Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for Autonomous Driving</title>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Autonomous Driving Workshop at Conf. on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Venice Erin Liong, and Qiang Xu. nuScenes: A Multimodal Dataset for Autonomous Driving</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral Temporal Graph Neural Network for Multivariate Time-series Forecasting</title>
		<author>
			<persName><forename type="first">Defu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Robotics and Automation</title>
				<meeting>IEEE Conf. on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring Dynamic Context for Multi-path Trajectory Prediction</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejiao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monika</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodo</forename><surname>Sester</surname></persName>
		</author>
		<author>
			<persName><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Robotics and Automation</title>
				<meeting>IEEE Conf. on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Efficient Implementation of Reid&apos;s Multiple Hypothesis Tracking Algorithm and Its Evaluation for the Purpose of Visual Tracking</title>
		<author>
			<persName><forename type="first">Ingemar</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><forename type="middle">L</forename><surname>Hingorani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic Crowd GAN: Multimodal Pedestrian Trajectory Prediction using a Graph Vehicle-Pedestrian Attention Network</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Eiffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stewart</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salah</forename><surname>Sukkarieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Nebot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are We Ready for Autonomous Driving? the KITTI Vision Benchmark Suite</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3D Object Detection and Tracking on Streaming Data</title>
		<author>
			<persName><forename type="first">Xusen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Robotics and Automation</title>
				<meeting>IEEE Conf. on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">6 [16] Boris Ivanovic and Marco Pavone. The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018. 2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Int. Conf. on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eager-MOT: 3D Multi-Object Tracking via Sensor Fusion</title>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Robotics and Automation</title>
				<meeting>IEEE Conf. on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Andrew Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<title level="m">Activity Forecasting. European Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks. Conf. on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manmohan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><surname>De-Sire</surname></persName>
		</author>
		<title level="m">Distant Future Prediction in Dynamic Scenes with Interacting Agents. IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Crowds by Example. Computer Graphics Forum</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PnPNet: End-to-End Perception and Prediction with Tracking in the Loop</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal Motion Prediction with Stacked Transformers</title>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An Algorithm for Tracking Multiple Targets</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction</title>
		<author>
			<persName><forename type="first">Abduallah</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Claudel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You&apos;ll Never Walk Alone: Modeling Social Behavior for Multi-Target Tracking</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factor Graph based 3D Multi-Object Tracking in Point Clouds</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Pöschmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Protzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots &amp; Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kris</surname></persName>
		</author>
		<author>
			<persName><surname>Vernaza</surname></persName>
		</author>
		<title level="m">R2P2: A ReparameteRized Pushforward Policy for Diverse, Precise Generative Path Forecasting. European Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Social Etiquette: Human Trajectory Understanding In Crowded Scenes</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<title level="m">Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data. European Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mono-Camera 3D Multi-Object Tracking Using Deep Learning Detections and PMBM Filtering</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Benjaminsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emil</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrit</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Granstr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PointR-CNN: 3D Object Proposal Generation and Detection from Point Cloud</title>
		<author>
			<persName><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Yichuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">Multiple Futures Prediction. Conf. on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Hungarian Method for the Assignment Problem</title>
		<author>
			<persName><forename type="first">H W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PointTrackNet: An End-to-End Network for 3D Object Detection and Tracking from Point Cloud</title>
		<author>
			<persName><forename type="first">Sukai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Robotics and Automation</title>
				<meeting>IEEE Conf. on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint Object Detection and Multi-Object Tracking with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Robotics and Automation</title>
				<meeting>IEEE Conf. on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">MTP: Multi-hypothesis Tracking and Prediction for Reduced Error Propagation</title>
		<author>
			<persName><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09481</idno>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D Multi-Object Tracking: A Baseline and New Evaluation Metrics</title>
		<author>
			<persName><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots &amp; Systems</title>
				<imprint>
			<date type="published" when="2007">2020. 1, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with 2D-3D Multi-Feature Learning</title>
		<author>
			<persName><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunze</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PTP: Parallelized Tracking and Prediction with Graph Neural Networks and Diversity Sampling</title>
		<author>
			<persName><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2006">2021. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction</title>
		<author>
			<persName><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Towards Robust Human Trajectory Prediction in Raw Videos</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08259</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanglan</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Renjie Liao, and Raquel Urtasun. LaneRCNN: Distributed Representations for Graph-Centric Motion Forecasting</title>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots &amp; Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Lingyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Hsun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerrick</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><forename type="middle">Clark</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micol</forename><surname>Marchetti-Bowick</surname></persName>
		</author>
		<title level="m">Map-Adaptive Goal-Based Trajectory Prediction. Conf. on Robot Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust Multi-Modality Multi-Object Tracking</title>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Class-Balanced Grouping and Sampling for Point Cloud 3D Object Detection</title>
		<author>
			<persName><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
