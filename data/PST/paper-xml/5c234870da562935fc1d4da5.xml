<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Processing-in-Memory for Energy-efficient Neural Network Training: A Heterogeneous Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiawen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Merced University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hengyu</forename><surname>Zhao</surname></persName>
							<email>h6zhao@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Matheus</roleName><forename type="first">Almeida</forename><surname>Ogleari</surname></persName>
							<email>mogleari@ucsc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Merced University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
							<email>jzhao@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Processing-in-Memory for Energy-efficient Neural Network Training: A Heterogeneous Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">172430A2ADD06133A163159F3B2A4F6D</idno>
					<idno type="DOI">10.1109/MICRO.2018.00059</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks (NNs) have been adopted in a wide range of application domains, such as image classification, speech recognition, object detection, and computer vision. However, training NNs -especially deep neural networks (DNNs) -can be energy and time consuming, because of frequent data movement between processor and memory. Furthermore, training involves massive fine-grained operations with various computation and memory access characteristics. Exploiting high parallelism with such diverse operations is challenging. To address these challenges, we propose a software/hardware co-design of heterogeneous processing-in-memory (PIM) system. Our hardware design incorporates hundreds of fix-function arithmetic units and ARMbased programmable cores on the logic layer of a 3D die-stacked memory to form a heterogeneous PIM architecture attached to CPU. Our software design offers a programming model and a runtime system that program, offload, and schedule various NN training operations across compute resources provided by CPU and heterogeneous PIM. By extending the OpenCL programming model and employing a hardware heterogeneity-aware runtime system, we enable high program portability and easy program maintenance across various heterogeneous hardware, optimize system energy efficiency, and improve hardware utilization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Neural networks (NNs) have been adopted by a wide range of application domains, such as computer vision, speech recognition, and natural language processing. Today, NN models employ increasingly larger number of parameters and data sets. For example, VGG <ref type="bibr" target="#b0">[1]</ref> and AlexNet <ref type="bibr" target="#b1">[2]</ref> employ 138M and 61M parameters for image classification, respectively. Training such complex models demands immense computation and memory resources, energy and time. One critical energy and performance bottleneck when training NN is data movement in systems. As NN models are becoming deeper and larger, the data volume and the pressure on the runtime system to support data intensive operations substantially increase. Existing research efforts use low-precision data <ref type="bibr" target="#b2">[3]</ref> or prune NN models <ref type="bibr" target="#b3">[4]</ref>. Yet, these efforts impose the difficulty of quantifying the impact of model simplification on NN model accuracy; They do not fundamentally address the data movement problem in NN model training.</p><p>Recent development of processing-in-memory (PIM) techniques have been explored as a promising solution to tackle the data movement challenge in various applications <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. We profile various NN training workloads and reveal that such workloads have diverse memory access patterns, computation intensity, and parallelism (Section II). As a result, NN training *These two authors contributed equally to this paper as the first authors. can significantly benefit from heterogeneous PIM -which incorporates both fixed-function logic and programmable cores in the memory -to achieve optimal energy efficiency and balance between parallelism and programmability. However, such heterogeneous PIM architecture introduces multiple challenges in the programming method and runtime system.</p><p>First, programming PIMs to accelerate NN training is nontrivial. Today, the common machine learning frameworks, such as TensorFlow <ref type="bibr" target="#b6">[7]</ref>, Caffe2 <ref type="bibr" target="#b7">[8]</ref>, heavily rely on a variety of implementations for NN operations on various hardware, and use a middleware to integrate those operations to provide hardware transparency to the user. Such a software design can place high burden on system programmers, because of the increasing hardware heterogeneity and difficulty for program maintenance. Most previous PIM software interfaces <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref> require programmers to have the detailed knowledge of underlying hardware. In order to improve productivity and ease-of-adoption of PIM-based NN training accelerators, we need to develop a programming method that maximizes code reuse without asking the programmer to repeatedly program on different PIMs.</p><p>Second, combining fixed-function logics and programmable cores in PIM further complicates the software design. Fixedfunction and programmable PIMs employ vastly different programming models: Fixed-function PIMs employ ISA-level instructions accessed via assembly-level intrinsics or via library calls; Programmable PIMs employ standard programming paradigms, such as threading packages or GPGPU programming interfaces <ref type="bibr" target="#b9">[10]</ref>. As discussed in recent studies <ref type="bibr" target="#b9">[10]</ref>, most previous PIM designs adopt homogeneous PIM architectures -with either fixed-function or programmable PIMs -which allows a simplified software interface design. But with heterogeneous PIM, it is critical to design a unified programming model that can accommodate both PIM components.</p><p>Finally, the scale of operations in NN training can lead to unbalanced hardware utilization. Ideally, we want to achieve high utilization of PIMs without violating the dependency requirement among NN training operations, by exploiting abundant operation-level parallelism across the host processor and PIMs. However, it can be difficult to achieve so in such a heterogeneous system by pure hardware scheduling, because of the complexity of tracking operation dependency and synchronization. Furthermore, NN training typically adopts a large amount (e.g., tens of thousands) of iterative steps and hundreds of operations per step. Operation dependency across the massive amount of steps and operations can impose synchronization overhead and decrease hardware utilization, when operations are running on multiple computing devices.</p><p>Our goal in this paper is to design a PIM-based NN training acceleration system that can efficiently accelerate unmodified training models written on widely-used machine learning frameworks (e.g., TensorFlow). To achieve our goal, we propose a software/hardware co-design of a heterogeneous PIM framework. Our design consists of three components. First, we adopt a heterogeneous PIM architecture, which integrates both fixed-function logics and programmable cores in 3D diestacked main memory. Second, we extend the OpenCL <ref type="bibr" target="#b10">[11]</ref> programming model to address the programming challenges. The programming model maps the host CPU and heterogeneous PIMs onto OpenCL's platform model and extends OpenCL's execution and memory models for efficient runtime scheduling. Finally, we propose a runtime system, which maximizes PIM hardware utilization and NN-operation-level parallelism. This paper makes the following contributions: </p><formula xml:id="formula_0">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head><p>We motivate our software/hardware coordinated design by discussing the challenges of accelerating machine learning training workloads. We employ three widely used CNN training models -VGG-19 <ref type="bibr" target="#b0">[1]</ref>, AlexNet <ref type="bibr" target="#b1">[2]</ref>, and DCGAN <ref type="bibr" target="#b11">[12]</ref> -as examples in this section. However, our observations can also be applied to various other training workloads (Sections VI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NN Training Characterization</head><p>In order to understand the characteristics of NN training workloads, we develop a profiling framework (Figure <ref type="figure" target="#fig_1">1</ref>) by leveraging TensorBoard <ref type="bibr" target="#b12">[13]</ref> and Intel VTune <ref type="bibr" target="#b13">[14]</ref> to collect software and hardware counter information of training operations. Measuring the number of main memory accesses of individual operations during training can be inaccurate due to the extra cache misses imposed by simultaneously executing operations. As such, we disable inter-operation parallelism to ensure characterization accuracy of individual operations.   We make three key observations. First, only several operations dominate training execution time. For example, top five operations in VGG-19 model consume over 95% of total execution time. Second, the most time-consuming operations are also the most memory intensive. In fact, the top five most time-consuming operations contribute to over 98% of total main memory accesses across all three models. We further classify operations into four classes, shown in Figure <ref type="figure" target="#fig_2">2</ref>. The first class of operations is compute intensive, and does not have to be offloaded to PIMs, but we can offload them when there are idling hardware units in PIMs. The second class of operations is our target to offload to PIMs. The third class is unusual, and the fourth class does not have big performance impact on model training. The above two observations motivate us to adopt a PIM architecture to accelerate NN training in order to reduce data movement between the host processor and the main memory. Third, time-consuming and memory-intensive operations require heterogeneous computation types. It appears that many of such operations are multiplication and addition (e.g., MatMul) or can be decomposed so (e.g., Conv2D). This is inline with previous works on machine learning acceleration <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Yet, significant amount of top time-consuming and memoryintensive operations cannot simply be implemented by pure multiplication and addition. For instance, Relu is an activation function that incorporates conditional statement; MaxPool is a sample-based discretization process; ApplyAdam is a first-order gradient-based optimization of stochastic objective functions. Complex operations, such as Conv2DBackpropFilter and Conv2DBackpropInputs, include other logic and computations beyond multiplication and addition. Such non-multiplyadd operations can consume over 40% of total execution time. Furthermore, studies on modern multi-tenancy <ref type="bibr" target="#b14">[15]</ref> and multimodel training <ref type="bibr" target="#b15">[16]</ref> workloads also demonstrate such heterogeneous computation requirement. This observation motivates us to adopt a heterogeneous PIM architecture that combines fixed-function logic and programmable cores.</p><p>Most previous works on PIM adopt either fixed-function <ref type="bibr" target="#b4">[5]</ref>  or programmable <ref type="bibr" target="#b5">[6]</ref> computation components in the logic layer of 3D die-stacked memory. In the following, we discuss feasibility, challenges, opportunities of accelerating NN training with software/hardware co-design of heterogeneous PIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feasibility of Heterogeneous PIM Architecture</head><p>The logic layer of 3D memory stacks has area, power, and thermal limitations. But previous studies demonstrated the feasibility of adopting both fixed-function and programmable PIMs, while meeting these constraints <ref type="bibr" target="#b16">[17]</ref>. We adopt similar methodologies to ensure the feasibility of our architecture implementation (Section IV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Software Design Challenges and Opportunities</head><p>There are three challenges for the software design (introduced in Section I): (1) How do we enable high productivity of system programmers and ease-of-adoption of PIM-based NN training accelerators? (2) How do we develop a unified programming model that can efficiently accommodate the host processor, fixed-function PIMs, and programmable PIMs? (3) How do we balance hardware utilization at runtime?</p><p>One candidate baseline programming model is OpenCL <ref type="bibr" target="#b10">[11]</ref>, which is widely used in accelerator-based heterogeneous computing platforms (e.g., GPU and FPGA). We adopt OpenCL, due to its portability, expressiveness, and ability to enable high programming productivity to support programming on heterogeneous systems (details are discussed in Section III-B). However, it is not straightforward to adopt OpenCL for NN model training on the heterogeneous PIM architecture. (1) How do we map the platform model of OpenCL to the heterogeneous PIM architecture? (2) Given the execution model of OpenCL with limited considerations on hardware utilization, how do we make the best use of CPU (the host processor) and different types of PIMs? (3) Given the memory model of OpenCL with limited considerations on synchronization between hardware units, how do we meet the requirement of frequent synchronizations from NN operations? Trade-offs between parallelism and programmability. Fixed-function PIMs typically offer high computation parallelism by executing fine-grained, simple operations distributed across massive amount of logic units. However, they are less flexible than programmable PIMs that can be programmed to accommodate a large variety of operations. Furthermore, fixedfunction PIMs can impose high performance overhead by (i) frequent operation-spawning and (ii) host-PIM synchronization. Programmable PIMs typically execute coarse-grained code blocks with less frequent host-PIM synchronization. However, the limited number of computational units in programmable PIMs can lead to much lower parallelism than in fixed-function PIMs. Opportunities in runtime system scheduling. Substantial opportunities exist in leveraging system-level software to optimize resource sharing among various system components. The heterogeneity of our architecture introduces requirements on scheduling model-training operations across the host processor (CPU), fixed-function PIMs and programmable PIMs, based on the dynamic utilization of compute resources on these system components. Yet, we observe that NN training workloads tend to have repeatable (hence predictable) computation behavior over the execution time. As such, system software can accurately predict and dynamically schedule the operations by profiling the resource utilization of various compute elements in the first few steps of modeling training. Such dynamic profiling-based scheduling can achieve the best utilization of computation resources, while improving energy efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CPU vs. GPU -Where to Attach Heterogeneous PIMs?</head><p>Today, NN-training workloads can be executed on both CPUand GPU-based systems. Recent silicon interposer technology allows both types of systems to adopt 3D die-stacked memories closely integrated with logic components. For example, modern GPU device memories <ref type="bibr" target="#b17">[18]</ref> are implemented by high-bandwidth memory technology. High-end CPU servers integrate highbandwidth memories using the DRAM technology adopted from hybrid memory cubes.</p><p>Our heterogeneous PIMs are logic components closely integrated with die-stacked memories. Therefore, they are generally applicable to both CPU or GPU systems. However, this paper focuses on the software design for heterogeneous PIMs attached on CPU systems, due to the constraint of current GPU systems. Today, GPU systems often fuse and organize computation kernels into NN layers rather than fine-grained operations, because of the inefficiency of compute preemption and thread scheduling. This significantly limits the flexibility of operation scheduling on GPU.</p><p>The NVIDIA Volta GPU provides certain support for finegrained acceleration of NN training operations, yet only available with limited number of threads. Modern CPU systems are easy to access and program; this enables easy-to-adopt and flexible programming abstraction and system library functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DESIGN</head><p>To address the aforementioned challenges, we propose a software/hardware co-design of heterogeneous PIM framework to accelerate NN training. Our design consists of a heterogeneous PIM architecture, an extended OpenCL programming model, and a runtime system. Figure <ref type="figure" target="#fig_3">3</ref> depicts our architecture configuration. Figure <ref type="figure" target="#fig_4">4</ref> shows the process of building and executing NN training with our software framework. Given an OpenCL kernel to implement an operation, our system extracts code sections from the kernel and compiles them into a set of binaries to run on CPU, programmable PIM, and fixedfunction PIMs, respectively. After the training workload starts to execute, our runtime scheduler profiles the first step of training to obtain operation characterization. It then performs dynamic scheduling of operations across CPU, programmable PIM, and fixed-function PIMs in the rest of training steps. Our runtime system incorporates two key components: (i) an operationpipeline scheme, which allows multiple NN operations to corun on PIMs to improve hardware utilization and (ii) a recursive operation-execution scheme, which allows the programmable PIM to call fixed-function PIMs to improve hardware utilization and avoid frequent synchronization between CPU and PIMs. Software/hardware co-design principles. Our software design supports our hardware configuration in the following manner. First, our software design offers a portable programming model across the host processor, fixed-function PIMs, and the programmable PIM. Our programming model provides a unified abstract to program various PIMs, which need to be programmed in separate manners in conventional systems. Our runtime scheduling scheme effectively optimizes PIM hardware utilization. Our runtime system also enables recursive calls between the programmable PIM and fixed-function PIMs. Our architecture design supports our software design in two ways: our heterogeneous PIM architecture enables efficient NN training acceleration by exploiting the heterogeneous characteristics of software operations; We employ a set of hardware registers to track PIM hardware utilization information, which is required by our runtime scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Heterogeneous PIM Architecture</head><p>To accommodate various types of operations that are likely to execute on PIMs, we adopt a heterogeneous PIM architecture consisting of (i) a programmable PIM, which is an ARM core and (ii) massive fixed-function PIMs, which are adders and multipliers distributed across all memory banks. While our design can be used with various 3D die-stacked memory devices, we employ a 32-bank memory stack (where a bank is a vertical slice in the stack) as an example in this paper. Figure <ref type="figure" target="#fig_3">3</ref> depicts our architecture configuration. Section IV describes hardware implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Programming Model for Heterogeneous PIM</head><p>We extend the OpenCL programming model to program the heterogeneous PIM. OpenCL has been widely employed to enable program portability across accelerator-based, heterogeneous computing platforms (e.g., GPU and FPGA). We use OpenCL because of the following reasons. First, by treating the fixed-function PIMs and programmable PIM as accelerators, the semantics of OpenCL naturally fit into the heterogeneous PIM environment. Second, writing a program for the heterogeneous PIM based on an abstract and unified hardware model in OpenCL, the programmer can write the program just once but run it on a variety of PIMs. Therefore, by using OpenCL, we can hide hardware variety of the heterogeneous PIM from system programmers, improve their productivity, and enable code portability.</p><p>Other programming models, such as OpenACC <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> and OpenMP <ref type="bibr" target="#b20">[21]</ref>, can also hide hardware heterogeneity and reduce programmers' burden. However, these are higher-level programming models, which rely on compilers to transform programs into a lower-level programming model, such as OpenCL, to enable code portability. We focus on OpenCL in our study, because it provides a foundation for those higherlevel programming models. Overview of our programming model. Table <ref type="table" target="#tab_4">II</ref> summarizes our extension to OpenCL. Our platform model includes multiple types of heterogeneous devices. Such platform model is driven by the characteristics of NN training operations. Our execution model adds (i) recursive kernel invocation to enable kernel invocation between PIMs to support complex NN operations (e.g., Conv2DBackpropFilter) and (ii) operation pipeline to improve hardware utilization for small NN operations with limited parallelism (e.g., Slice). Finally, we extend the memory model to support a single global memory shared between the host processor and accelerators. We also add explicit synchronization across different PIMs and CPU (host processor) to enforce execution orders across NN operations. OpenCL background. The existing OpenCL adopts a hostaccelerator platform model as shown in Figure <ref type="figure">5(a)</ref>. A host processor connects to one or more compute devices (i.e., accelerators). A compute device is divided into one or more   In order to employ OpenCL programming model on the heterogeneous PIM system, we investigate how to map the heterogeneous PIM system onto the OpenCL model, and extend the OpenCL model for efficient runtime scheduling. In the following, we discuss our mapping method from the perspectives of platform model, execution model, and memory model. Table <ref type="table" target="#tab_4">II</ref> summarizes our programming model extension.</p><p>Heterogeneous PIM platform model. Figure <ref type="figure">5</ref>(b) illustrates our platform model. A large number of fixed-function PIMs provide massive parallelism for data processing. Each fixedfunction PIM is a PE (in the OpenCL jargon). All fixedfunction PIMs in all memory banks form a compute device. All fixed-function PIMs in a bank form a compute unit. Each programmable PIM is a compute device; each core of the programmable PIM is a PE. Hence, within the context of OpenCL, a heterogeneous PIM system has heterogeneous compute devices. We expose fixed-function PIM and programmable PIM as distinct compute devices to give control flexibility to the runtime system for operation scheduling. An OpenCL operation can be offloaded to any compute device that supports the operation execution. Execution model. Tasks (i.e., operations in NN model training) to be launched on any PIM are represented as kernels managed by a host program, as in a traditional OpenCL program. If the task includes instructions that cannot be executed on the fixed-function PIM, then the task will not be scheduled by the OpenCL runtime to run on the fixed-function PIM. Otherwise, a task can run anywhere (CPU, fixed-function PIM, and programmable PIM). The OpenCL runtime (on CPU) Fig. <ref type="figure">5</ref>: Enabling OpenCL platform model on heterogeneous PIM systems. is in charge of task scheduling between different PIMs and CPU. Leveraging low-level APIs (Section IV-A) and hardware registers, the runtime can determine whether a specific PIM is busy and whether a specific task is completed. We describe the scheduling algorithm in Section III-C. Binary files for a task to run on CPU, fixed-function PIM, or programmable PIM are generated during the compilation stage. Given an OpenCL kernel for a task, we generate four binary files as shown in Figure <ref type="figure" target="#fig_4">4</ref>. Section IV discusses details of binary generation.</p><p>Binaries (#3) and (#4) in Figure <ref type="figure" target="#fig_4">4</ref> allow recursive PIM kernel, a new execution scheme for our heterogeneous PIM design. A kernel in the programmable PIM can trigger data processing with fixed-function PIMs. This is supported by the programmable PIM runtime and implemented by calling small kernels loadable on fixed-function PIMs. By combining multiple kernels into a single kernel, the recursive PIM kernel scheme reduces overhead of kernel spawning and synchronization between the host and PIMs. Figure <ref type="figure">6</ref> shows an example that further explains the recursive PIM kernel. In the example, we illustrate an NN operation, Conv2DBackpropFilter, which is offloaded to the programmable PIM as a kernel; the kernel includes computation phases 1 and 2 that cannot be offloaded to the fixed-function PIMs. Conv2DBackpropFilter includes convolution computation ("Conv(...)" in the figure); The programmable PIM offloads this portion of computation to fixed-function PIM as a smaller kernel. The computation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory model</head><p>• Multiple types of memory with a relaxed consistency model;</p><p>• The global memory is not shared;</p><p>• No defined synchronization across accelerators.</p><p>• A single global memory with a relaxed consistency model;</p><p>• The global memory is shared;</p><p>• Explicit synchronization across PIMs and CPU.</p><p>Fig. <ref type="figure">6</ref>: An example of the recursive PIM kernel.</p><p>phases 1, 2 and convolution are combined as a single recursive PIM kernel, which reduces the synchronization between CPU and PIMs.</p><p>In general, the four binary files provide convenience for scheduling on CPU, the fixed-function PIMs and programmable PIM, and hence allows the runtime to maximize utilization of CPU and PIMs. Memory model. The existing OpenCL defines four distinct memory regions in a compute device: global, constant, local, and private. On a heterogeneous PIM system, only a single global memory (i.e., the main memory) exists. In addition, the global memory is shared between CPU and PIMs, and addressed within a unified physical address space. This memory model requires synchronization at multiple points: (1) between CPU and PIMs; and (2) between different PIMs. The synchronization is necessary to avoid data race and schedule operations.</p><p>To implement effective synchronization, we employ the programmable PIM to drive the synchronization and avoid frequent interrupts to CPU. In particular, for synchronization between CPU and PIMs, the programmable PIM checks the completion of operations offloaded to PIMs (either programmable or fix function PIMs) and sends the completion information to CPU. For synchronization between different PIMs, the programmable and fix function PIMs synchronize through global variables in main memory.</p><p>Between CPU and PIMs, we introduce explicit synchronization points to synchronize the accesses to shared variables. To the host processor, the whole set of fixed-function PIMs or the programmable PIM appear as another processor. We employ standard synchronization schemes (e.g., barriers and locks), similar to the ones in a shared-memory multiprocessor. For fixed-function PIMs, their operations are atomic and the synchronization points are not expected in the middle of operations. For programmable PIMs, the synchronization points can be in the middle of a kernel. This is feasible based on global lock variables shared between CPU and PIMs. To support memory consistency, we adopt a relaxed memory consistency model, which aims to improve performance and reduce hardware complexity. In particular, an update to a memory location by a fixed-function PIM is not visible to all the other fixed-function PIMs at the same time. Instead, the local view of memory from each fixed-function PIM is only guaranteed to be consistent right after the kernel call to fixed-function PIMs. Between the fixed-function PIMs and programmable PIM, we employ the same consistency scheme: updates to memory locations by the entire set of fixed-function PIMs are not visible until the end of the kernel call to the fixed-function PIMs.</p><p>Because of our shared memory model, there is no data copy overhead before and after PIM kernel calls. Based on the above synchronization schemes, PIM kernel calls can be launched asynchronously to overlap computation on CPU and PIMs. Support for easy program maintenance. To use the extended OpenCL programming model, operations need to be rewritten using OpenCL. To write OpenCL code for operations, one can use OpenACC directives and compilers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> to automatically transform the original code into OpenCL code. This can significantly simplify the programming work. Furthermore, the number of operations for machine learning models is limited (tens of operations). Hence, using OpenCL to implement those machine learning operations is feasible. Other than that, however, the higher level software components (e.g., most of the middleware components, operation APIs, and Python syntax for using machine learning models) remain the same. This enables easy maintenance of machine learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Runtime System Design</head><p>Our runtime system is in charge of scheduling operations to fixed-function PIMs, programmable PIM, and CPU. To minimize NN training time, the runtime strives to maximize utilization of PIMs and CPU to optimize system throughput. The runtime schedules operations based on the following two steps.</p><p>Step 1: profiling. The runtime profiles performance of all operations on CPU. The profiling happens in only one step of NN model training. NN model training typically has a large amount of iterative steps (thousands and even millions of steps). Using one step for profiling has ignorable impact on performance. In addition, all steps almost have the same classes of operations; performance of operations (particularly execution time and the number of main memory access) remains stable across steps. Therefore, one step is sufficient for profiling purpose. During profiling, the runtime executes operations one by one in CPU, collecting execution time and the number of main memory access level cache misses of each operation with hardware counters. Based on the profiling results in the step, the runtime employs the following algorithm to determine the candidate operations to be offloaded to PIMs.</p><p>To determine the candidate operations, the runtime sorts operations into two lists (in descending order) based on execution time and the number of main memory accesses, respectively. Each operation in each of the two lists is correlated to an index, i.e., each operation has two indexes. With each operation, the runtime calculates a global index by adding these two indexes. Based on the global indexes, the runtime sorts operations into a global list. The runtime chooses top operations in the global list to offload to PIMs. Those top operations account for x% of total execution time of one step (x = 90 in our evaluation). The above algorithm is inspired by feature selection process in machine learning <ref type="bibr" target="#b21">[22]</ref>. The goal of this algorithm is to select those operations that are both time-consuming and have a large number of main memory accesses.</p><p>Step 2: scheduling. Given the candidate operations to offload, the runtime makes the scheduling decision based on the following three principles.</p><p>• Scheduling operations to execute on fixed-function PIMs as much as possible. • Scheduling operations to execute on PIMs (not CPU) as much as possible. In case all fixed-function or programmable PIMs are busy, the runtime will schedule the candidate operations to execute on CPU; • Scheduling needs to respect data dependency across operations.</p><p>The first principle favors fixed-function PIMs over other compute units, because fixed-function PIMs are more energy efficient and typically performs faster with higher parallelism than other compute units. The second principle avoids CPU idling and introduces parallelism between CPU and PIMs. The third principle ensures execution correctness. Each operation defined in the machine learning frameworks typically has explicit input and output data objects (e.g., Tensors in TensorFlow), which provides convenience in tracking data dependencies across operations. Operation pipeline. The above scheduling algorithm and principles enable operation pipeline to maximize hardware utilization. In particular, when an operation in a step cannot fully utilize fixed-function PIMs, our runtime schedules an operation in the next step to execute a portion of its computation by utilizing idling fixed-function PIMs as long as the two operations do not depend on each other.</p><p>In essence, these two operations can enable a pipelined execution manner. For instance, in AlexNet, a single convolution operation with a filter size of 11×11 consumes 121 multiplication and 120 addition (241 fixed-function PIMs in total). In case we have 444 fixed-function PIMs in total (Section IV-D), the utilization of fixed-function PIMs is only 54%. To improve hardware utilization, the runtime can schedule multiplication and addition from an operation (or operations) in the next step to execute on fixed-function PIMs. Once the convolution operation in the current step is completed, the partially executed operation(s) from the next step can immediately utilize the newly released fixed-function PIMs to improve hardware utilization and performance. This indicates that an operation can dynamically change its usage of PIMs, depending on the availability of PIMs. Such dynamic nature of operation execution is feasible based on a runtime system running on the programmable PIM (Section IV-C presents implementation details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Low-level APIs for PIM Runtime System</head><p>We introduce several low-level API functions for fixedfunction and programmable PIMs. These API functions allow direct control of individual PIMs, and provide foundation for our runtime. The API achieves the following functionality: (1) offloading a specific operation into specific PIM(s); (2) tracking the status of PIMs, including examining whether a PIM is busy or not; (3) querying the completion of a specific operation; (4) querying the computation location (i.e., which PIM) and input/output data location (i.e, which DRAM banks) for a specific operation. Table <ref type="table" target="#tab_5">III</ref> summarizes our API functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. OpenCL Binary Generation</head><p>To schedule operations to execute on CPU, fixed-function PIMs, or programmable PIM, we generate four binary files (Figure <ref type="figure" target="#fig_4">4</ref>). In order to generate the binary file (#3) that corresponds to a portion of a large operation (an OpenCL kernel) to execute on fixed-function PIMs (e.g., the convolution within the operation Conv2DBackpropFilter), we first extract code sections from the corresponding OpenCL kernel. We then transform these code sections into a set of small kernels to execute on fixed-function PIMs. Finally we compile them into binary file (#3). In the original OpenCL kernel, these extracted code regions are replaced with the kernel calls and then compiled into binary file (#4) to execute on the programmable PIM. Binary files (#1) and (#2) are generated during the regular compilation stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Runtime Implementation</head><p>Our runtime consists of two components, which execute on the CPU and the programmable PIM, respectively. The runtime on CPU. To support our runtime scheduling, we extend the runtime system of TensorFlow by adding approximately 2000 lines of code. The runtime on CPU schedules operations on CPU and PIMs, based on hardware utilization information provided by the low-level APIs. It does not support the implementation of recursive PIM kernels. In  other words, the runtime on CPU is only responsible for offloading a kernel -which can have a part of its computation offloadable to fixed-function PIMs -to the programmable PIM. Our modifications to TensorFlow runtime include (1) device initialization and characterization using OpenCL intrinsics;</p><p>(2) creating a device context and instance for a PIM device;</p><p>(3) providing a new OpenCL device abstraction to other components of Tensorflow; (4) a mechanism to communicate with the runtime on the programmable PIM. This is onetime modification to Tensorflow, but can support various PIM hardware configurations without involving system programmers' future efforts. The runtime on programmable PIM. The runtime on the programmable PIM supports recursive PIM kernels and operation pipeline. In particular, a kernel with a part of its computation replaced with kernel calls to fixed-function PIMs is handled by the runtime on the programmable PIM, which automatically offloads the computation to fixed-function PIMs. In order to keep track of the dynamic utilization of fixedfunction PIMs, our runtime on the programmable PIM records the numbers of additions and multiplications already completed in each operation offloaded to the programmable PIM, as well as the remaining additions and multiplications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hardware Implementation</head><p>Figure <ref type="figure" target="#fig_3">3</ref> and Figure <ref type="figure" target="#fig_5">7</ref> illustrate our hardware implementation. The programmable PIM employs an ARM Cortex-A9 processor with four 2GHz cores. Each core has an in-order pipeline. In individual NN training models, operations that are potentially offloaded to the programmable PIM (e.g., ApplyAdam, MaxPooling, and ReLU) are typically not executed at the same time. Therefore, we only adopt one programmable PIM in our design. Even if we simultaneously train multiple NN models, the chance of having multiple operations to use the programmable PIM at the same time is low according to our evaluation with mixed workload analysis (Section VI-F).</p><p>Because a significant portion of NN training operations can be decomposed to addition and multiplications (Section II-A), we implement our fixed-function PIMs as 32-bit floating point multipliers and adders. We implement equal numbers of multipliers and adders in the pool of fixed-function PIMs. Our low-level APIs allow us to map operations to fixed-function PIMs that are in the same bank as input data of the operations. In addition, we accommodate random memory access pattern in NN computation by adopting buffering mechanisms <ref type="bibr" target="#b4">[5]</ref>. We determine the fixed-function PIM configurations by employing a set of architectural level area, power, and thermal modeling tools, including McPAT <ref type="bibr" target="#b22">[23]</ref> and HotSpot <ref type="bibr" target="#b23">[24]</ref>, to perform design space exploration of the logic die of 3D DRAM. Based on our study, the total number of allowed fixed-function PIMs is limited by the area of the logic die. With our baseline 3D DRAM configuration (Section V), we can distribute 444 fixedfunction PIMs (pairs of multipliers and adders) across the 32 banks in the logic die. It is impossible to distribute these fixedfunction PIMs evenly to each bank. We consider the placement of the fixed-function PIMs on 32 banks based on the following policy: we place more fixed-function PIMs on edge and corner banks than on central banks (Figure <ref type="figure" target="#fig_3">3 (a)</ref>). The rationale behind is that the banks at the edge and corner have better thermal dissipation paths than central banks. Therefore, these banks can support higher computation density.</p><p>Furthermore, we employ a set of registers as shown in Figure <ref type="figure" target="#fig_5">7</ref>. Each register indicates the idling of either a bank of fixed-function PIMs or the programmable PIM. The registers allow our software runtime scheduler to query the completion of any computation and decide the idleness of processing units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Framework</head><p>In order to evaluate the performance of our design, we model fixed-function PIM and programmable PIM architectures, respectively, using Synopsys Design Compiler <ref type="bibr" target="#b24">[25]</ref> and Prime-Time <ref type="bibr" target="#b25">[26]</ref> with Verilog HDL. We adopt HMC 2.0 <ref type="bibr" target="#b26">[27]</ref> timing parameters and configurations for our evaluation of 3D memory stack. Baseline memory frequency is set to 312.5 MHz, which is the same as HMC 2.0 specification <ref type="bibr" target="#b26">[27]</ref>. This is also used as the working frequency of our heterogeneous PIM. We employ a trace generator developed on Pin <ref type="bibr" target="#b27">[28]</ref> to collect instruction trace, when running our OpenCL kernel binaries on CPU. We develop a python-based, trace-driven simulation framework based on our design to evaluate the execution time of various training workload traces. Our simulator also incorporates our runtime scheduling mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Power and Area Modeling</head><p>We adopt 10nm technology node for the host CPU and the logic die of the PIMs; 25nm technology node for the DRAM dies. We measure CPU and GPU power with VTune <ref type="bibr" target="#b28">[29]</ref> and nvidia-smi, respectively. Our power model considers whole system power when we evaluate the power of heterogeneous-PIM-based systems, including CPU and the memory stack. We calculate the power and area of the programmable PIM using McPAT <ref type="bibr" target="#b22">[23]</ref>. We evaluate the power and area of fixedfunction PIMs using Synopsys Design Compiler <ref type="bibr" target="#b24">[25]</ref> and PrimeTime <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Workloads</head><p>We evaluate various training models, including VGG-19 <ref type="bibr" target="#b0">[1]</ref>, AlexNet <ref type="bibr" target="#b1">[2]</ref>, Deep Convolutional Generative Adversarial Networks (DCGAN)) <ref type="bibr" target="#b11">[12]</ref>, ResNet-50 <ref type="bibr" target="#b29">[30]</ref>, Inception-v3 <ref type="bibr" target="#b30">[31]</ref>, Long Short Term Memory (LSTM) with dropout <ref type="bibr" target="#b31">[32]</ref> and Word2vec <ref type="bibr" target="#b32">[33]</ref>. LSTM and Word2vec are evaluated in Section VI-F. The rest models are widely used in recent studies on CNN training and image classification. Training Datasets. We employ ImageNet as training data set of VGG-19, AlexNet, ResNet-50, and Inception-V3. ImageNet is a large image dataset with millions of images belonging to thousands of categories. DCGAN employs MNIST dataset <ref type="bibr" target="#b33">[34]</ref>. LSTM adopts Penn Tree Bank (PTB) <ref type="bibr" target="#b31">[32]</ref> dataset. Word2vec employs "questions-words" dataset <ref type="bibr" target="#b34">[35]</ref> in TensorFlow. Training framework and batch Size. We adopt Tensor-Flow <ref type="bibr" target="#b6">[7]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Real Machine Configurations</head><p>To compare performance and energy efficiency of heterogeneous PIM with GPU and CPU, we run the training models on (1) NVIDIA GeForce GTX 1080 Ti graphic card <ref type="bibr" target="#b35">[36]</ref> and (2) CPU listed in Table <ref type="table" target="#tab_6">IV</ref>. Our GPU-based training evaluations adopt CUDA 8 <ref type="bibr" target="#b36">[37]</ref> and NVIDIA cuDNN 6.0 library <ref type="bibr" target="#b37">[38]</ref>. GPU utilizations of each training model in TensorFlow are: Inception-v3 (average: 62%; peak: 100%); ResNet-50 (average: 44%; peak: 58%); AlexNet (average: 30%; peak: 34%); VGG-19 (average: 63%; peak: 84%); DCGAN (average: 28%; peak 42%. We use NVIDIA's profiling tool <ref type="bibr" target="#b38">[39]</ref> and Intel's VTune <ref type="bibr" target="#b13">[14]</ref> to collect performance and power statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>Our experiments compare among the following five configurations, including our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• CPU -Executing all training operations on CPU;</head><p>• GPU -Executing all training operations on GPU; • Progr PIM -Programmable PIMs only, which executes all operations on as many ARM-based programmable cores as needed by workloads (without our runtime scheduling);</p><p>• Fixed PIM -Fixed-function PIMs only, which executes the operations that can be offloaded on fixed-function PIM and other operations on CPU (without runtime scheduling); • Hetero PIM -Our heterogeneous PIM design (including our runtime scheduling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Execution Time Analysis</head><p>Figure <ref type="figure">8</ref> shows execution (training) time of various NN training models. We break down the execution time into synchronization time, data movement time and operation time (i.e., computation time in CPU, GPU or PIMs). For GPUbased systems, the data movement time is the time for data transfer between main memory and GPU global memory. Certain amount of data transfer time is overlapped with GPU computation, e.g. by copying a minibatch of images to the GPU memory, while the computation on GPU is processing another minibatch. Our breakdown only shows the data transfer time that is not hidden by the computation. For PIM-based systems, the data movement time is the time for data transfer between CPU and the main memory. Our runtime scheduling allows operations to execute concurrently on CPU and PIMs.</p><p>We observe that PIM-based designs (including Fixed PIM, Progr PIM and Hetero PIM) perform much better than CPU, with 19%-28× performance improvement. Compared with Progr PIM and Fixed PIM, our design has 2.5×-23× and 1.4×-5.7× performance improvement, respectively. PIM-based designs also significantly reduce data movement overhead, compared to CPU and GPU. Overall, Hetero PIM leads to the lowest synchronization and data movement overhead among all configurations.</p><p>The performance benefit of Hetero PIM stands out with larger training models and larger working sets due to (i) more reduction in data movement and (ii) higher parallelism between host CPU and PIMs introduced by more offloadable operations. DCGAN has smaller model and working set than others. Therefore, Hetero PIM appears to result in worse performance than GPU with DCGAN; yet, compared with other configurations, our design still significantly improves performance. ResNet is a large training model with large working sets. As a result, Hetero PIM leads to better performance than GPU with ResNet. With other training models, Hetero PIM leads to performance close to (within 10% of) GPU. GPU has good performance because of its massive thread-level parallelism. Our design leads to much better performance than all other configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Energy Consumption Analysis</head><p>Figure <ref type="figure" target="#fig_7">9</ref> shows the dynamic energy consumption of the five NN models with five different configurations. The energy consumption results are normalized to the results of Hetero PIM. We observe substantial energy benefit of using our design: it consumes 3×-24× and 1.3×-5× less energy than CPU and GPU, respectively. CPU consumes higher dynamic energy than Hetero PIM, Fixed PIMs, and GPU, even though its power consumption is the lowest among all of these configurations (note that we take CPU power into account when we calculate the power of PIMs and GPU, in order to evaluate full-system Fig. <ref type="figure">8</ref>: Execution time breakdown of five NN models.   power consumption). This is because CPU has the longest execution (training) time. Furthermore, we notice that the dynamic energy consumption of Progr PIM is higher than that of other configurations, because the speed of Progr PIM is only slightly faster than that of CPU, yet the dynamic power of Progr PIM is higher than that of CPU due to the additional processing units in Progr PIM. Overall, Hetero PIM leads to the lowest dynamic energy consumption across all configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with Prior PIM-based NN Acceleration</head><p>Figure <ref type="figure" target="#fig_9">10</ref> shows a quantitative comparison between our design and a recent PIM-based NN accelerator design, Neurocube <ref type="bibr" target="#b5">[6]</ref> (qualitative comparison is in Section VII). Neurocube also reduces data movement overhead and improves energy efficiency by using PIM technology. However, our work outperforms Neurocube in terms of performance and energy efficiency. With highly compute-intensive models, such as VGG-19 and Inception-V3, our design achieves much higher performance and energy-efficiency improvement than Neurocube. Even with less compute-intensive models, such as DCGAN, our work can achieve at least 3× higher performance and energy efficiency than Neurocube. The reason for the improvement is two-fold: (1) Neurocube only adopts programmable PIMs, while our design employs energy-efficient, highly-parallel fixedfunction PIMs to accelerate fine-grained operations; (2) Our design employs runtime scheduling that effectively optimizes hardware utilization (evaluated in Section VI-E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sensitivity Study</head><p>Frequency Scaling. We adopt three different frequencies for fixed-function PIMs and programmable PIM: their original frequencies (1×), doubling of their frequencies (2×) and quadrupling of their frequencies (4×). We use a phase-locked loop module to change the frequency. We study execution (training) time with the different frequencies.</p><p>Figure <ref type="figure" target="#fig_1">11</ref> shows the results. We observe that with higher frequency, the heterogeneous PIM performs better than GPU. With 2× frequency, Hetero PIM performs 36% and 17% better than GPU, with VGG-19 and AlexNet, respectively. With 4× frequency, Hetero PIM performs 37% and 60% better than GPU, with VGG-19 and AlexNet respectively. We also observe that the synchronization and data movement overheads are reduced, when using higher frequencies. Programmable PIM Scaling. We employ three different configurations for Hetero PIM, while keeping the area of logic die in the memory stack unchanged. We scale the number of Progr PIM (ARM cores) from one to two to 16, while the rest of the logic die area is used to implement Fixed PIM. The three configurations are labeled as 1P, 4P and 16P, respectively.</p><p>Figure <ref type="figure" target="#fig_2">12</ref> shows our results. The figure reveals that the performance difference between the three configurations is relatively small. The performance difference between 16P and 1P is 12%-14%. The reason is two-fold: (1) One Progr PIM is sufficient for the NN models to schedule and pipeline operations; (2) Using more Progr PIMs loses more Fixed PIMs, given the constant area in the logic layer of memory stacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation of Software Impact</head><p>We isolate the the impact of our software (runtime) techniques from that of Hetero PIM hardware. We aim to provide more insightful analysis on the effectiveness of software/hardware co-design. In particular, we study execution time, energy and utilization of Fixed PIM with and without the recursive PIM kernel call (RC) and operation pipeline (OP) -our two major runtime techniques. Without RC and OP, we also compare Hetero PIM hardware design with Fixed PIM and Progr PIM, in terms of execution time and energy. This comparison allows us to study the impact of Hetero PIM architecture with the absence of our runtime techniques. Execution time analysis. As shown in Figure <ref type="figure" target="#fig_3">13</ref>, Hetero PIM without runtime scheduling performs better than Progr PIM and Fixed PIM by up to 8.5×. This demonstrates the necessity of using Hetero PIM architecture. However, comparing with Fixed PIM, the performance benefit of Hetero PIM hardware is not significant (7%-30%). After incorporating the runtime scheduling techniques, the performance of Hetero PIM is improved by up to 3.8×. This result demonstrates the necessity of using an efficient runtime to maximize the benefit of Hetero PIM architecture.   Energy analysis. Figure <ref type="figure" target="#fig_4">14</ref> shows our energy results normalized to the energy of Hetero PIM with RC and OP. We have similar observations as the execution time analysis: Hetero PIM without runtime scheduling performs better than Progr PIM and Fixed PIM by up to 2.7×. With RC and OP, we further reduce the energy of Hetero PIM by up to 3.9×. PIM utilization analysis. Figure <ref type="figure" target="#fig_11">15</ref> shows our utilization results. With RC only, the utilization of Fixed PIM in Hetero PIM is improved by up to 66% (VGG-19); With OP, the utilization of Fixed PIM is further improved by up to 18% (AlexNet); With RC and OP, the utilization of Fixed PIM is close to 100%. The reason for the poor hardware utilization with neither RC nor OP is the lack of scheduling for the operations that do not have sufficient parallelism or cannot be completely offloaded to Fixed PIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Mixed Workloads Analysis</head><p>We also evaluate the case, when multiple models co-run in the same system <ref type="bibr" target="#b39">[40]</ref>. We co-run two NN training models: a CNN model and a non-CNN model. The CNN model can execute on CPU and PIMs, subject to our runtime scheduling; the non-CNN model executes on CPU or the programmable PIM, when they are idle. Figure <ref type="figure" target="#fig_1">16</ref> shows the results of six co-run cases. In each case, "Hetero. PIM" indicates that we The results show that Hetero. PIM achieves 69%-83% performance improvement compared with Sequential Execution. Such improvement comes from high utilization of CPU and the programmable PIM in our design. With Sequential Execution, there can be no operations available to execute even though CPU and the programmable PIM are idle due to dependency between operations within the same model. Hetero. PIM avoids hardware idling, because operations across different models have no dependency and can execute simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Energy Efficiency Analysis</head><p>We study energy efficiency of the PIMs with different frequencies as in SectionVI-D. We use energy-delay-product (EDP) as the metric to evaluate energy efficiency. Figure <ref type="figure" target="#fig_5">17 (a)</ref> shows the results. The figure reveals that the most energy efficient point is not the original frequency for the five models. The 4× frequency is the most energy efficient for the five models. The tradeoff between energy consumption and execution time leads to such results. Thus, we conclude that higher frequency tends to be more energy efficient for NN model training. Figure <ref type="figure" target="#fig_5">17 (b</ref>) compares power consumption between GPU and Hetero PIM with different frequencies. In general, GPU is very power hungry. It consumes 1.5× to 2.6× more power than Hetero PIM with high frequency (4×). Compared with GPU, Hetero PIM can be highly power efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>To our knowledge, this is the first paper to propose a software/hardware co-design of a heterogeneous-PIM-based acceleration framework for NN training. Whereas previous PIM-based accelerator designs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref> investigated Fig. <ref type="figure" target="#fig_5">17</ref>: Energy efficiency and power with 3D memory frequency scaling. the mapping of workloads on either fixed-function or programmable PIMs, it is unclear how to coordinate software and hardware designs to best utilize PIM technologies to support the heterogeneity requirement of NN training workloads.</p><p>Processing-in-memory for machine learning. Recent PIMbased machine learning accelerator designs strive to leverage the memory cells of nonvolatile memory technologies to execute NN inference operations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref>. However, NN training typically incorporates substantial complex operations as we identified. It is difficult to accommodate these complex operations in previous processing-in-memory-cell designs. Azarkhish et al. <ref type="bibr" target="#b48">[49]</ref> and Schuiki et al. <ref type="bibr" target="#b49">[50]</ref> adopt RISC-V cores <ref type="bibr" target="#b50">[51]</ref> and a streaming coprocessor in die-stacked DRAM to accelerate convolution networks or SGD. However, the RISC-V cores are merely used to control the arithmetic elements in the streaming coprocessor. Furthermore, both designs require users to modify code and perform tiling based on new APIs. Schuiki et al.'s study <ref type="bibr" target="#b49">[50]</ref> only focuses on a specific operation (SGD). Azarkhish et al.'s design <ref type="bibr" target="#b48">[49]</ref> primarily aims at inference and requires data to be carefully laid out in memory with 4D tiling. This constraint on data layout leads to inefficient training, because intermediate activations after each layer need to be re-tiled <ref type="bibr" target="#b49">[50]</ref>. Neurocube <ref type="bibr" target="#b5">[6]</ref> accelerates CNN inference and training by integrating programmable processing elements in the logic layer of 3D die-stacked DRAM. However, using programmable PIMs alone cannot provide the massive parallelism and execution efficiency enabled by heterogeneous PIMs. Furthermore, the aforementioned previous studies do not consider dynamic runtime scheduling of operations. Our experiment results demonstrate an efficient heterogeneous PIM design with runtime scheduling.</p><p>Processing-in-memory for general applications. Fujiki et al. <ref type="bibr" target="#b8">[9]</ref> proposed a ReRAM-based in-memory processor architecture and data-parallel programming framework. The study introduces a compact instruction set for memory array with processors. The programming framework combines dataflow and vector processing, employs TensorFlow input, and generates code for in-memory processors. Our work also employs TensorFlow, but optimizes operations scheduling and introduces PIM heterogeneity. Ahn et al. <ref type="bibr" target="#b40">[41]</ref> explores mapping of PIM operations based on data locality of applications, while we schedule operations in multiple dimensions -hardware utilization, data locality, and data dependency. Ahn et al. <ref type="bibr" target="#b44">[45]</ref> introduced PIM for parallel graph processing. The design offers an efficient communication method between memory partitions and develops prefetchers customized for memory access patterns of graph processing. Other works introduce PIM architectures based on 3D-stacked memory. For example, Zhang et al. <ref type="bibr" target="#b51">[52]</ref> presented an architecture for programmable, GPU-accelerated, in-memory processing implemented using 3D die-stacking. The throughput-oriented nature of GPU architectures allows efficient utilizaztion of high memory bandwidth provided by 3D-stacked memory, while offering the programmability required to support a broad range of applications. Akin et al. <ref type="bibr" target="#b41">[42]</ref> presented a set of mechanisms that enable efficient data reorganization in memory using 3D-stacked DRAM. However, the aforementioned studies cannot efficiently accelerate NN training workloads, because they cannot fully accommodate the heterogeneous computing requirement in NN training. Furthermore, these studies do not consider efficient programming model and runtime system to accommodate the hardware heterogeneity as explored in our study. Other accelerator optimization for machine learning. Recent works explored software-and hardware-based approaches for a variety of inference acceleration <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref>. Most of these works focused on improving performance and energy efficiency of NN inference. However, training is much more compute and memory intensive than inference. The data movement overhead in training is much more significant. Several prior studies <ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref> investigated architecture design for NN training. However, these studies focus on addressing the memory capacity constraint issues caused by a large amount of feature maps generated in CNN training. The data movement bottleneck is not fully explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>In this paper, we propose a software and hardware codesign of heterogeneous PIM approach, combining the power of programmable PIM and fixed-function PIMs, for NN training. Our software design enables (1) a portable and unified programming model across CPU, fixed-function PIMs, and programmable PIM; (2) runtime scheduling that effectively optimizes PIM hardware utilization and maximizes NN-operationlevel parallelism. Our design not only allows natively training models to execute on heterogeneous PIM, but also enables easy maintenance of machine learning frameworks. Our design achieves significant improvement in performance and energy efficiency with various NN training workloads.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>We developed a profiling framework to characterize NN training models written on TensorFlow. We identify the heterogeneity requirements across the operations of various NN training workloads. Based on our profiling results, we identify opportunities and key challenges in the software design for efficiently accelerating NN training using PIMs. • We develop a heterogeneous PIM architecture and demonstrate the effectiveness of such an architecture for training NN models. • We propose an extension to OpenCL programming model in order to accommodate the PIM heterogeneity and improve the program maintainability of machine learning frameworks. • We propose a runtime system to dynamically map and schedule NN operations on heterogeneous PIM, based on dynamic profiling of NN operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Our profiling framework for profiling NN training workloads in TensorFlow. of operations and requires thousands of iterative steps to train; In each step, each type of operation can be invoked up to tens of times. We only show results within one training step. But the characteristics remain stable across training steps.We make three key observations. First, only several operations dominate training execution time. For example, top five operations in VGG-19 model consume over 95% of total execution time. Second, the most time-consuming operations are also the most memory intensive. In fact, the top five most time-consuming operations contribute to over 98% of total main memory accesses across all three models. We further classify operations into four classes, shown in Figure2. The first class of operations is compute intensive, and does not have to be offloaded to PIMs, but we can offload them when there are idling hardware units in PIMs. The second class of operations is our target to offload to PIMs. The third class is unusual, and the fourth class does not have big performance impact on model training. The above two observations motivate us to adopt a PIM architecture to accelerate NN training in order to reduce data movement between the host processor and the main memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Four categories of NN training operations.Third, time-consuming and memory-intensive operations require heterogeneous computation types. It appears that many of such operations are multiplication and addition (e.g., MatMul) or can be decomposed so (e.g., Conv2D). This is inline with previous works on machine learning acceleration<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Yet, significant amount of top time-consuming and memoryintensive operations cannot simply be implemented by pure multiplication and addition. For instance, Relu is an activation function that incorporates conditional statement; MaxPool is a sample-based discretization process; ApplyAdam is a first-order gradient-based optimization of stochastic objective functions. Complex operations, such as Conv2DBackpropFilter and Conv2DBackpropInputs, include other logic and computations beyond multiplication and addition. Such non-multiplyadd operations can consume over 40% of total execution time. Furthermore, studies on modern multi-tenancy<ref type="bibr" target="#b14">[15]</ref> and multimodel training<ref type="bibr" target="#b15">[16]</ref> workloads also demonstrate such heterogeneous computation requirement. This observation motivates us to adopt a heterogeneous PIM architecture that combines fixed-function logic and programmable cores.Most previous works on PIM adopt either fixed-function<ref type="bibr" target="#b4">[5]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Architecture overview of the proposed heterogeneous PIM.</figDesc><graphic coords="5,99.02,213.08,415.32,76.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The process of executing NN training with our software framework design. compute units, each of which is further divided into one or more processing elements (PE). An OpenCL program consists of kernels for compute devices and a host program. The host program runs on CPU and enqueues commands to a commandqueue attached to a compute device.In order to employ OpenCL programming model on the heterogeneous PIM system, we investigate how to map the heterogeneous PIM system onto the OpenCL model, and extend the OpenCL model for efficient runtime scheduling. In the following, we discuss our mapping method from the perspectives of platform model, execution model, and memory model. TableIIsummarizes our programming model extension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Heterogeneous PIM implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>as our training framework. We adopt default batch sizes of each training model in TensorFlow. The batch size of VGG-19, AlexNet and Inception-v3 is 32. The batch size of Word2vec and ResNet-50 is 128. DCGAN has a batch size of 64. LSTM employs a batch size of 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Normalized dynamic energy of various NN models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>of Neurocube to Hetero. PIM (a) Execution time comparison. (b) Energy consumption comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Performance and energy comparison with Neurocube.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Fig. 11: Execution time breakdown of various NN models with 3D memory frequency scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: Hardware utilization with and without RC and OP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Operation profiling results for three neural network models. "CI"= computation intensive; "MI"=memory intensive.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>VGG-19</cell><cell></cell><cell></cell></row><row><cell>Top 5 CI Ops</cell><cell>Execution Time(%)</cell><cell>#Invocation</cell><cell>Top 5 MI Ops</cell><cell>#Main Memory Access(%)</cell><cell>#Invocation</cell></row><row><cell>1. Conv2DBackpropFilter</cell><cell>40.15</cell><cell>16</cell><cell>1. Conv2DBackpropFilter</cell><cell>42.52</cell><cell>16</cell></row><row><cell>2. Conv2DBackpropInput</cell><cell>32.68</cell><cell>15</cell><cell>2. BiasAddGrad</cell><cell>35.68</cell><cell>16</cell></row><row><cell>3. BiasAddGrad</cell><cell>11.92</cell><cell>16</cell><cell>3. Conv2DBackpropInput</cell><cell>21.06</cell><cell>15</cell></row><row><cell>4. Conv2D</cell><cell>10.34</cell><cell>16</cell><cell>4. MaxPoolGrad</cell><cell>0.22</cell><cell>16</cell></row><row><cell>5. MaxPoolGrad</cell><cell>1.49</cell><cell>16</cell><cell>5. Relu</cell><cell>0.14</cell><cell>19</cell></row><row><cell>Other 13 ops</cell><cell>3.37</cell><cell>232</cell><cell>Other 13 ops</cell><cell>0.38</cell><cell>229</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AlexNet</cell><cell></cell><cell></cell></row><row><cell>Top 5 CI Ops</cell><cell>Execution Time(%)</cell><cell>#Invocation</cell><cell>Top 5 MI Ops</cell><cell>#Main Memory Access(%)</cell><cell>#Invocation</cell></row><row><cell>1. Conv2DBackpropFilter</cell><cell>33.64</cell><cell>5</cell><cell>1. BiasAddGrad</cell><cell>44.64</cell><cell>3</cell></row><row><cell>2. Conv2DBackpropInput</cell><cell>33.46</cell><cell>4</cell><cell>2. Conv2DBackpropInput</cell><cell>36.61</cell><cell>4</cell></row><row><cell>3. MatMul</cell><cell>13.54</cell><cell>6</cell><cell>3. Conv2DBackpropFilter</cell><cell>14.79</cell><cell>5</cell></row><row><cell>4. Conv2D</cell><cell>10.48</cell><cell>5</cell><cell>4. Relu</cell><cell>1.20</cell><cell>8</cell></row><row><cell>5. BiasAddGrad</cell><cell>4.62</cell><cell>3</cell><cell>5. Conv2D</cell><cell>0.46</cell><cell>5</cell></row><row><cell>Other 13 ops</cell><cell>4.26</cell><cell>121</cell><cell>Other 13 ops</cell><cell>2.30</cell><cell>119</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DCGAN</cell><cell></cell><cell></cell></row><row><cell>Top 5 CI Ops</cell><cell>Execution Time(%)</cell><cell>#Invocation</cell><cell>Top 5 MI Ops</cell><cell>#Main Memory Access(%)</cell><cell>#Invocation</cell></row><row><cell>1. Conv2DBackpropFilter</cell><cell>19.98</cell><cell>4</cell><cell>1. Conv2DBackpropFilter</cell><cell>37.21</cell><cell>4</cell></row><row><cell>2. Conv2DBackpropInput</cell><cell>17.18</cell><cell>4</cell><cell>2. Conv2DBackpropInput</cell><cell>28.09</cell><cell>4</cell></row><row><cell>3. MatMul</cell><cell>14.28</cell><cell>12</cell><cell>3. Slice</cell><cell>17.18</cell><cell>14</cell></row><row><cell>4. Conv2D</cell><cell>10.53</cell><cell>4</cell><cell>4. Conv2D</cell><cell>5.45</cell><cell>4</cell></row><row><cell>5. Mul</cell><cell>9.89</cell><cell>84</cell><cell>5. Mul</cell><cell>2.22</cell><cell>84</cell></row><row><cell>Other 47 ops</cell><cell>28.14</cell><cell>821</cell><cell>Other 47 ops</cell><cell>9.85</cell><cell>819</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Extending OpenCL for the heterogeneous PIM.</figDesc><table><row><cell></cell><cell>Native OpenCL</cell><cell>Extensions for Heterogeneous PIM</cell></row><row><cell>Platform model</cell><cell>Host + accelerators (e.g., host + GPU).</cell><cell>Host + two types of accelerators (fixed-function PIMs and</cell></row><row><cell></cell><cell></cell><cell>programmable PIM) driven by the characteristics of NN</cell></row><row><cell></cell><cell></cell><cell>training.</cell></row><row><cell>Execution model</cell><cell>Host submits work to accelerators.</cell><cell>• Host submits work to accelerators;</cell></row><row><cell></cell><cell></cell><cell>• Accelerators submit work to accelerators (i.e., recursive</cell></row><row><cell></cell><cell></cell><cell>kernel invocation);</cell></row><row><cell></cell><cell></cell><cell>• Work execution pipeline (i.e., operation pipeline);</cell></row><row><cell></cell><cell></cell><cell>• Work scheduling based on dynamic profiling.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Low-level APIs for PIMs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>System configurations.</figDesc><table><row><cell>CPU</cell><cell>Intel Xeon E5-2630 V3@2.4GHz</cell></row><row><cell>Main memory</cell><cell>16GB DDR4</cell></row><row><cell>Operating system</cell><cell>Ubuntu 16.04.2</cell></row><row><cell>GPU</cell><cell>NVIDIA GeForce GTX 1080 Ti (Pascal)</cell></row><row><cell>GPU cores</cell><cell>28 SMs, 128 CUDA cores per SM, 1.5GHz</cell></row><row><cell>L1 cache</cell><cell>24KB per SM</cell></row><row><cell>L2 cache</cell><cell>4096KB</cell></row><row><cell>Memory interface</cell><cell>8 memory controllers, 352-bit bus width</cell></row><row><cell>GPU main memory</cell><cell>11GB GDDR5X</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-1-5386-6240-3/18/$31.00 ©2018 IEEE DOI 10.1109/MICRO.2018.00059</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their valuable feedback. This paper is supported in part by NSF grants 1652328, 1718158, 1617967, 1553645, 171819, and SRC/DARPA Center for Research on Intelligent Storage and Processing-in-memory.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and Optimizing Asynchronous Low-Precision Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalpel: Customizing DNN Pruning to the Underlying Hardware Parallelism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lukefahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Palframan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PRIME: A novel processing-in-memory architecture for neural network computation in ReRAMbased main memory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture</title>
		<meeting>the 43rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neurocube: A programmable digital neuromorphic architecture with high-density 3D memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yalamanchili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture</title>
		<meeting>the 43rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="380" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In-memory data parallel processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fujiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A processing-in-memory taxonomy and a case for studying fixed-function PIM</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Oskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ignatowski</surname></persName>
		</author>
		<editor>WoNDP</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Khronos Group, the open standard for parallel programming of heterogeneous systems</title>
		<ptr target="https://www.khronos.org/opencl/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">TensorBoard: Visualizing learning</title>
		<ptr target="https://www.tensorflow.org/programmersguide/summariesandtensorboard" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://software.intel.com/en-us/get-started-with-vtune/" />
		<title level="m">Vtune user&apos;s guide</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scalable Multi-Framework Multi-Tenant Lifecycle Management of Deep Learning Training Jobs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Herta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ishakian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Muthusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nagpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>in Workshop on ML Systems at NIPS&apos;17</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">MultiModel: Multi-task machine learning across domains</title>
		<ptr target="https://ai.googleblog.com/2017/06/multimodel-multi-task-machine-learning.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Integrated thermal analysis for processing in die-stacking memory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Memory Systems</title>
		<meeting>the Second International Symposium on Memory Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="402" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Titan</forename><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Xp</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/geforce/products/10series/titan-xp/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Openarc</title>
		<ptr target="https://ft.ornl.gov/research/openarc" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ipmacc compiler</title>
		<ptr target="https://github.com/lashgar/ipmacc" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">OpenMP Fortran application program interface, version 1.1</title>
		<author>
			<persName><forename type="first">O</forename><surname>Forum</surname></persName>
		</author>
		<ptr target="http://www.openmp.org" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving accuracy in word class tagging through the combination of machine learning systems</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Halteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zavrel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="229" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 42Nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temperature-aware microarchitecture: modeling and implementation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="125" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Design compiler</title>
		<author>
			<persName><surname>Synopsys</surname></persName>
		</author>
		<ptr target="https://www.synopsys.com/support/training/rtl-synthesis/design-compiler-rtl-synthesis.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Primetime</title>
		<author>
			<persName><surname>Synopsys</surname></persName>
		</author>
		<ptr target="https://www.synopsys.com/support/training/signoff/primetime1-fcd.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Hybrid memory cube specification 2.0</title>
		<author>
			<persName><surname>Hmcc</surname></persName>
		</author>
		<ptr target="http://http://www.hybridmemorycube.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pin: Building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Vtune performance analyzer essentials</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reinders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Intel Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Tensorflow, questions-words dataset</title>
		<ptr target="http://download.tensorflow.org/data/questions-words.txt" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NVIDIA</title>
		<ptr target="https://www.nvidia.com/en-us/geforce/products/" />
	</analytic>
	<monogr>
		<title level="m">GeForce GTX 1080 Ti</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">NVIDIA CUDA</title>
		<ptr target="http://www.nvidia.com/cuda" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">"</forename><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Cudnn</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/cudnn" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NVIDIA</title>
		<ptr target="http://docs.nvidia.com/cuda/profiler-users-guide/" />
	</analytic>
	<monogr>
		<title level="m">Profiler user&apos;s guide</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ease.ml: Towards multi-tenant resource sharing for machine learning workloads</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PIM-enabled instructions: A low-overhead, locality-aware processingin-memory architecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42Nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="336" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data reorganization in memory using 3D-stacked DRAM</title>
		<author>
			<persName><forename type="first">B</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual International Symposium on Computer Architecture</title>
		<meeting>the Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="131" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Instruction offloading with HMC 2.0 standard: A case study for graph traversals</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Symposium on Memory Systems</title>
		<meeting>the 2015 International Symposium on Memory Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="258" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Thermal feasibility of die-stacked processing in memory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WoNDP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A scalable processing-in-memory accelerator for parallel graph processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Strachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture</title>
		<meeting>the 43rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="14" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SNrram: an efficient sparse neural network computation architecture based on resistive random-access memory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Design Automation Conference</title>
		<meeting>the 55th Annual Design Automation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">106</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">CMP-PIM: an energy-efficient comparator-based processing-in-memory neural network accelerator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Angizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Design Automation Conference</title>
		<meeting>the 55th Annual Design Automation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neurostream: Scalable and energy efficient deep learning with smart memory cubes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Azarkhish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Loi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="420" to="434" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A scalable near-memory architecture for training deep neural networks on large in-memory datasets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schuiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Gürkaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<idno>abs/1803.04783</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">RISC-V: The free and open RISC instruction set architecture</title>
		<ptr target="https://riscv.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">TOP-PIM: Throughputoriented programmable processing in memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lyashevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Greathouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ignatowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Symposium on Highperformance Parallel and Distributed Computing</title>
		<meeting>the 23rd International Symposium on Highperformance Parallel and Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Minerva: Enabling low-power, highlyaccurate deep neural network accelerators</title>
		<author>
			<persName><forename type="first">B</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture</title>
		<meeting>the 43rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="267" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Neural acceleration for general-purpose approximate programs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="449" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Fusedlayer CNN accelerators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milder</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">DaDianNao: A machine-learning supercomputer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">vDNN: Virtualized deep neural networks for scalable, memoryefficient neural network design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Compressing DMA engine: Leveraging activation sparsity for training deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="78" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Gist: Efficient data encoding for deep neural network training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
