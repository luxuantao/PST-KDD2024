<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Test-driven Evaluation of Linked Data Quality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Westphal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Leipzig</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">SÃ¶ren</forename><surname>Auer</surname></persName>
							<email>auer@cs.uni-bonn.de</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Leipzig</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roland</forename><surname>Cornelissen</surname></persName>
							<email>roland@metamatter.nl</email>
							<affiliation key="aff5">
								<orgName type="institution">University of Leipzig</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Test-driven Evaluation of Linked Data Quality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F05E7B034C6B34E4F0D0C56AEAC6B3C9</idno>
					<idno type="DOI">10.1145/2566486.2568002</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.0 [DATABASE MANAGEMENT]: General-Security</term>
					<term>integrity</term>
					<term>and protection; D.2.5 [Software Engineering]: Testing and Debugging -Testing tools</term>
					<term>Debugging aids Data Quality</term>
					<term>Linked Data</term>
					<term>DBpedia</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Linked Open Data (LOD) comprises an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowdsourced or extracted data of often relatively low quality. We present a methodology for test-driven quality assessment of Linked Data, which is inspired by testdriven software development. We argue that vocabularies, ontologies and knowledge bases should be accompanied by a number of test cases, which help to ensure a basic level of quality. We present a methodology for assessing the quality of linked data resources, based on a formalization of bad smells and data quality problems. Our formalization employs SPARQL query templates, which are instantiated into concrete quality test case queries. Based on an extensive survey, we compile a comprehensive library of data quality test case patterns. We perform automatic test case instantiation based on schema constraints or semi-automatically enriched schemata and allow the user to generate specific test case instantiations that are applicable to a schema or dataset. We provide an extensive evaluation of five LOD datasets, manual test case instantiation for five schemas and automatic test case instantiations for all available schemata registered with Linked Open Vocabularies (LOV). One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Linked Open Data (LOD) comprises an unprecedented volume of structured data published on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowdsourced and even extracted data of relatively low quality. Data quality is not an absolute measure, but assesses fitness for use <ref type="bibr" target="#b17">[16]</ref>. Consequently, one of the main challenges regarding the wider deployment and use of semantic technologies on the Web is the assessment and ensuring of the quality of a certain, possibly evolving, dataset for a particular use case. There have been few approaches for assessing Linked Data quality. However, these were majorly methodologies, which require (1) a large amount of manual configuration and interaction <ref type="bibr" target="#b3">[2,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b23">22]</ref> or (2) automated, reasoning based methods <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b16">15]</ref>. While reasoning based methods allow more automation, they are either limited to very specific quality aspects (such as link quality <ref type="bibr" target="#b13">[12]</ref>) or lack scalability to the medium and large datasets being increasingly published as Linked Data. In consequence, we observe a shortage of practical quality assessment approaches for Linked Data, which balance between a high degree of automation and scalability to datasets comprising billions of triples.</p><p>In this article, we present a methodology for test-driven Linked Data quality assessment, which is inspired by testdriven software development. In software engineering, a test case can be defined as "an input on which the program under test is executed during testing" and a test set as "a set of test cases for testing a program" <ref type="bibr" target="#b29">[28]</ref>. A basic metric in software unit-testing is test adequacy, which measures the completeness of the test set. A key principle of test-driven software development is to start the development with the implementation of automated test methods before the actual functionality is implemented.</p><p>Compared to software source code testing, where test cases have to be implemented largely manually or with limited programmatic support, the situation for Linked Data quality testing is slightly more advantageous. On the Data Web, we have a unified data model -RDF -which is the basis for both, data and ontologies. In this work, we exploit the RDF data model by devising a pattern-based approach for the data quality tests of RDF knowledge bases. We argue that ontologies, vocabularies and knowledge bases should be accompanied by a number of test cases, which help to ensure a basic level of quality. We present a methodology for assessing the quality of linked data resources, based on a formalization of data quality integrity constraints. Our formalization employs SPARQL query templates, which are instantiated into concrete quality test case queries. Based on an extensive survey, we compile a comprehensive library of quality test case patterns, which can be instantiated for rapid development of more test cases. We provide a method for automatic test case instantiation from these patterns for a particular ontology or vocabulary schema. Furthermore, we support the automatic derivation from OWL schema axioms. Since many schemata of LOD datasets are not very expressive, our methodology also includes semi-automatic schema enrichment. Concrete test cases are equipped with persistent identifiers to facilitate test tracking over time. We devise the notion of RDF test case coverage based on a combination of six individual coverage metrics (four for properties and two for classes).</p><p>As a result, the test case coverage can be explicitly stated for a certain dataset and potential users can thus obtain a more realistic assessment of the quality they can expect. Since the test cases are related to certain parts of the knowledge base (i.e. properties and classes), the quality of particular fragments relevant for a certain use-case can also be easily assessed. Another benefit of test-driven data engineering is support for data evolution. Once test cases are defined for a certain vocabulary, they can be applied to all datasets reusing elements of this vocabulary. Test cases can be re-executed whenever the data is altered. Due to the modularity of the approach, where test cases are bound to certain vocabulary elements, test cases for newly emerging datasets, which reuse existing vocabularies can be easily derived.</p><p>Our approach allows to perform an automatic test case instantiation based on schema constraints or semiautomatically enriched schemata and allows users to generate specific test case instantiations that are applicable for a schema or a dataset. A main contribution of our work is an extensive and unprecedented quantitative evaluation involving (a) manual and automatic test case instantiations for five large-scale LOD datasets (two DBpedia editions, datos.bne.es, Library of Congress authority data and LinkedGeoData) and (b) automatic test case instantiations for all available schemata registered with the Linked Open Vocabularies (LOV)<ref type="foot" target="#foot_0">1</ref> resulting in 32,293 total unique test cases for 297 of the LOV vocabularies. One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics. Finally, our framework implementation is built upon the SPARQL 1.1 standard which makes it applicable for any knowledge bases or triple store implementation.</p><p>The remainder of the article is structured as follows: Section 2 describes the methodology we followed to define Data Quality Test Case Patterns. The elicitation of our pattern library is described in Section 3. We instantiate, run and evaluate the test cases in Section 4 and Section 5, followed by a discussion in Section 6. Section 7 elaborates on related work and we conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TEST-DRIVEN DATA QUALITY METHODOLOGY</head><p>We first introduce the basic notions in our methodology, then describe its workflow and finally define test case coverage criteria analogous to unit tests in software engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic Notions</head><p>Data Quality Test Pattern (DQTP). A data quality test case pattern is a tuple (V, S), where V is a set of typed pattern variables and S is a SPARQL query template with placeholders for the variables from V . Possible types of the pattern variables are IRIs, literals, operators, datatype values (e.g. integers) and regular expressions. With R(v) we denote the value range for a pattern variable in v â V , i.e. the set of values by which the variable can be substituted, and with R(V ) the union of all these sets, i.e.</p><formula xml:id="formula_0">R(V ) = vâV R(v).</formula><p>Ideally, DQTPs should be knowledge base and vocabulary agnostic. Using %%v%% as syntax for placeholders, an example DQTP is: This DQTP can be used for testing whether a value comparison of two properties P 1 and P 2 holds with respect to an operator OP . DQTPs represent abstract patterns, which can be further refined into concrete data quality test cases using test pattern bindings.</p><p>Test Case Pattern Binding. A test case pattern binding is a specific instantiation of a DQTP. It is a triple (Ï, S, C) in which Ï : V â R(V ) is a mapping of variables to valid replacements, S is a SPARQL query template and C â {error, bad smell} is used as classification of the error.</p><p>Data Quality Test Cases. Applying Ï to S results in a SPARQL query, which can then be executed. Each result of the query is considered to be a violation of a unit test. An example test case pattern binding and resulting data quality test case is<ref type="foot" target="#foot_1">2</ref> : A test case has four different results: success (empty result), violation (results are returned), timeout (test case is marked for further inspection) and error (the query cannot be evaluated due to e.g. a network error or SPARQL engine limitations).</p><p>Test case Auto Generators (TAG). Many knowledge bases use RDFS and OWL as modelling languages. While the core of those languages aims at inferring new facts, a number of constructs is also suitable for verifying data quality. In previous work, tools like the Pellet Integrity Constraint Validator <ref type="bibr" target="#b25">[24]</ref> made use of this by viewing OWL axioms as constraints and reporting violations of them. Those are then interpreted via integrity constraint semantics, which uses a closed world assumption and a weaker form of the unique names assumption in which two individuals are considered to be different unless they are explicitly stated to be equal. We pursue the same approach for re-using schema information in our test framework. To achieve this, a test case auto generator (TAG) takes a schema as input and returns test cases. We provide support for the following OWL constructs rdfs:domain, rdfs:range, owl:minCardinality, owl:maxCardinality, owl:cardinality, owl:functional-Property, owl:disjointClass, owl:propertyDisjoint-With, owl:complementOf, owl:InverseFunctional-Property, owl:AsymmetricProperty, owl:Irreflexive-Property. and owl:deprecated.</p><p>Generators consist of a detection and an execution part. The detection part is a query against a schema, for instance:</p><p>SELECT DISTINCT ? T1 ? T2 WHERE { ? T1 owl : disjointWith ? T2 . }</p><p>For every result of a detection query, a test case is instantiated from the respective pattern, for instance: SELECT DISTINCT ? s WHERE { ? s rdf : type %% T1 %% . ? s rdf : type %% T2 %% .}</p><p>Depending on the violation, there is not necessarily a oneto-one mapping between a detection query and the generated test cases. For the owl:cardinality constraint, for example, we use three TAGs: (i) a TAG for the case a cardinality is 0, which checks whether the corresponding triple pattern is instantiated and two generators for values greater than 0, (ii) one to ensure that the property exists (TYPRODEP) and (iii) a second to validate the property occurrences (OWLCARD). The detection queries can be quite complex, we would like to stress, however, that our goal is not to provide complete reasoning and constraint checking, but rather a lightweight mechanism verifying typical violations efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Workflow</head><p>Our methodology is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. As shown in the figure, there are two major sources for creating test cases. One source is stakeholder feedback from everyone involved in the usage of a dataset and the other source is the already existing RDFS/OWL schema of a dataset. Based on this, there are several ways in which test cases can be created:</p><p>1. Using RDFS/OWL constraints directly: As previously explained, test cases can be automatically created via TAGs in this case.</p><p>2. Enriching the RDFS/OWL constraints: Since many datasets provide only limited schema information, we perform automatic schema enrichment as recently studied in <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b5">4]</ref>. These schema enrichment methods can take an RDF/OWL dataset or a SPARQL endpoint as input and automatically suggest schema axioms with a certain confidence value by analysing the dataset. In our methodology, this is used to create further test cases via TAGs. It should be noted that test cases are explicitly labelled, such that the engineer knows that they are less reliable than manual test cases.</p><p>3. Re-using tests based on common vocabularies: Naturally, a major goal in the Semantic Web is to re-use existing vocabularies instead of creating them from scratch for each dataset. We detect the used vocabularies in a dataset, which allows to re-use test cases from a test case pattern library. The creation of that library is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Instantiate existing DQTPs:</head><p>The aim of DQTPs is to be generic, such that they can be applied to different datasets. While this requires a high initial effort of compiling a pattern library, it is beneficial in the long run, since they can be re-used. Instead of writing SPARQL templates themselves, an engineer can select and instantiate the correct DQTP. This does not necessarily require SPARQL knowledge, but can also be achieved via a textual description of a DQTP, examples and its intended usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Write own DQTPs:</head><p>In some cases, test cases cannot be generated by any of the automatic and semi-automatic methods above and have to be written from scratch by an engineer. These DQTPs can then become part of a central library to facilitate later re-use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Coverage and Adequacy</head><p>In software engineering, a test case can be defined as an input on which the program under test is executed during testing and a test set as a set of test cases for testing a program <ref type="bibr" target="#b29">[28]</ref>. A basic metric in software unit testing is Test Adequacy. According to <ref type="bibr" target="#b29">[28]</ref>, adequacy is a notion that measures the completeness of the test set. An Adequacy Stopping Rule (ASR) is a related metric with a range {true|f alse} that defines whether sufficient testing has been done. Many attempts have been made to quantify test adequacy with the main coverage criteria being: a) statement coverage, b) branch coverage, c) path coverage and d) mutation adequacy. It is hard to automate the creation of these tests.</p><p>In RDF, instead of code, the testing subject is data that is stored in triples and adheres to a schema. We define an RDF test case as a data constraint that involves one or more triples and an RDF test set as a set of test cases for testing a dataset. Since no branches and paths in RDF exist, a test adequacy metric can only be related to the selectivity of the test cases. We will subsequently consider coverage as a composite of the following coverage criteria:</p><p>â¢ Property domain coverage (dom): Identifies the ratio of property occurrences, where a test case is defined for verifying domain restrictions of the property.</p><p>â¢ Property range coverage (ran): Identifies the ratio of property occurrences, where a test case is defined for verifying range restrictions of the property.</p><p>â¢ Property dependency coverage (pdep): Identifies the ratio of property occurrences, where a test case is defined for verifying dependencies with other properties.</p><p>â¢ Property cardinality coverage (card): Identifies the ratio of property occurrences, where a test case is defined for verifying the cardinality of the property.</p><p>â¢ Class instance coverage (mem): Identifies the ratio of classes with test cases regarding class membership. A certain property should also be considered to be covered, if the absence of a particular constraint is explicitly stated.</p><p>The above criteria can be computed by coverage computation functions. Each coverage computation function f : Q â 2 E takes a SPARQL query q â Q corresponding to a test case pattern binding as input and returns a set of entities. As an example, the function f dom for computing the domain coverage returns the set of all properties p such that the triple pattern (?s, p, ?o) occurs in q and there is at least one other triple pattern using ?s in q. This can straightforwardly be extended to a function F : 2 Q â 2 E taking a set of SPARQL queries as input and returning a set of entities. F computes how many entities are covered by the test case queries. For properties, F can be further extended to a function F with F (QS, D) = pâF (QS) pf req(p) where pf req(p) is the frequency of a property p, i.e. the number of occurrences of p divided by the number of occurrences of all properties in D. The extension for classes is analogous. This extension weights the entities by their frequency in the dataset. We propose to employ occurrences, i.e. concrete entity usages, instead of properties itself in order to reduce the influence of rarely used properties on the coverage.</p><p>The other coverage criteria are defined as follows: Range coverage fran is analogous to domain coverage. The property dependency coverage f pdep of a query q returns all properties in q if there are at least two different properties and an empty set otherwise. Property cardinality coverage f card of a query q returns the set of all properties p, such that (?s, p, ?o) occurs in q along with GROUP BY ?s as well as HAV-ING(count(?s) op n) aggregates (op is one of â¤, &lt;, =, &gt;, â¥ and n a number) or, analogously, the same criteria for ?o instead of ?s. Class instance coverage fmem of a query q returns the set of all classes c such that (?s, rdf:type, c) oc-curs in q. The class dependency coverage f cdep of a query q returns all classes in q if there are at least two different classes and an empty set otherwise.</p><p>In the above definition, please note that domain and range restrictions are more general than verifying rdfs:domain and rdfs:range as they cover all test cases, which can be performed via SPARQL on subject and objects of triples using a particular property. Please note that many test cases can be expressed in OWL 2, in particular when using the Pellet integrity constraint semantics. For instance, custom datatypes in OWL2<ref type="foot" target="#foot_2">3</ref> can be used for range checking of property values using regular expressions. As noted above, we transparently support the usage of OWL, but some test cases are much easier to implement in SPARQL and others, e.g. the SKOS restriction: "A resource has no more than one value of skos:prefLabel per language tag" cannot be checked in OWL at all, but is a straightforward DQTP in our case (ONELANG in Table <ref type="table" target="#tab_3">1</ref>).</p><p>Formally, we can define RDF test case coverage Cov of a set of test case queries QS with respect to a dataset D as follows:</p><formula xml:id="formula_1">Cov(QS, D) = 1 6 (F dom (QS, D) + F ran (QS, D) + F pdep (QS, D) + F card (QS, D) + F mem (QS, D) + F cdep (QS, D))</formula><p>The coverage is a heuristic in the [0, 1] range, which helps to assess whether the defined test cases are sufficient for data quality assessment. Higher results represent better coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PATTERN ELICITATION AND CRE-ATION</head><p>To start capturing patterns of real data errors we had a closer look at DBpedia, being one of the bigger and best interlinked datasets in the LOD cloud <ref type="bibr" target="#b22">[21]</ref>. We performed three different analyses which led to a comprehensive library of test case patterns summarized in Table <ref type="table" target="#tab_3">1:</ref> 1. Analysis of incidental error reports by the DBpedia user community.</p><p>2. Analysis of error tracking behavior by Wikipedia editors.</p><p>3. Analysis of the ontology schema of the DBpedia OWL ontology.</p><p>Community feedback. We thoroughly reviewed all the DBpedia related mailing lists and QA websites, i.e. the DBpedia discussion <ref type="foot" target="#foot_3">4</ref> and DBpedia developers<ref type="foot" target="#foot_4">5</ref> lists, as well as questions tagged with DBpedia on stackoverflow<ref type="foot" target="#foot_5">6</ref> and Semantic Web Answers <ref type="foot" target="#foot_6">7</ref> . We picked all the data quality related questions and tried to create SPARQL queries for retrieving the same erroneous data. Finally, we grouped similar SPARQL queries together.</p><p>Wikipedia maintenance system. We reviewed the information Wikipedia uses to ensure article quality and tried to reuse it from DBpedia. Such information encompasses special Categories and Templates used by seasoned Wikipedians (e.g. admins and stewards) to administrate and tag errors in the article space <ref type="foot" target="#foot_7">8</ref> . Based on the maintenance categories and templates used, new patterns like the TRIPLE Pattern and the PVT Pattern were derived. These patterns are also applicable to other datasets, e.g. Linked-GeoData <ref type="bibr" target="#b26">[25]</ref>.</p><p>OWL ontology analysis. The main purpose of OWL is to infer knowledge from existing schemata and data. While it can also be used to check constraints, this can be difficult in practice due to the Open World Assumption used and the lack of the Unique Name Assumption. Therefore, in addition to standard OWL inference, it can also be useful to convert OWL ontology axioms to SPARQL queries, which check the constraints expressed by them. This is motivated by research on the Pellet Integrity Constraint Validator<ref type="foot" target="#foot_8">9</ref> using the same idea. Specifically, we analysed the ontology and checked which existing constructs are applicable for constraint checking in DBpedia. We identified constructs such as (inverse) functionality, cardinality, domain and range of properties as well as class disjointness as relevant and included them in our pattern template library. The bindings for those patterns can be created automatically from specific OWL ontology axioms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern Library</head><p>Our Pattern Library consists of 17 DQTPs. Table <ref type="table" target="#tab_3">1</ref> shows a description of all patterns along with two example bindings. In the following, we exemplarily illustrate two patterns in detail and refer the reader to Example bindings:</p><formula xml:id="formula_2">(a) dbo:isbn format is different ' !' from "Ë([iIsSbBnN 0-9-])*$" (b) dbo:postCode format is different '!' from "Ë[0-9]{5}$".</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TEST GENERATION</head><p>To evaluate our methodology, we automatically generated test cases for all available vocabularies in the LOV dataset. Using the implemented TAGs, we managed to create 32,293 total unique reusable test cases for 297 LOV vocabularies <ref type="foot" target="#foot_9">10</ref> . Table <ref type="table" target="#tab_2">2</ref> displays the 10 schemas with the most associated test cases. For brevity we use the vocabulary prefixes as defined in LOV <ref type="foot" target="#foot_10">11</ref> . Test cases are themselves described in RDF and have a stable URI for tracking them over time. The URI is generated under the application namespace concatenated with the schema prefix, the pattern and an MD5 checksum of the SPARQL query string. The following listing displays a test case that checks whether the rdfs:range of foaf:isPrimaryTopicOf instance is a foaf:Document. We store metadata along with every test case which allows us to easily filter test cases based on different criteria.  For every dataset evaluated, we applied automatic schema enrichment as described in Section 2. We used a high level of confidence (0.9; see <ref type="bibr" target="#b5">[4]</ref> for details) on the produced axioms and applied manual post-processing to remove certain axioms. The number of additional test cases instantiated for the considered schemas are shown in Table <ref type="table" target="#tab_4">3</ref>.</p><p>Besides the automatically generated test cases, our methodology supports manual test cases that may apply to a schema or a dataset. The manual schema test cases are reusable across different datasets for all RDF data using that schema. The manual dataset test cases can be applied only to a specific dataset. Manual test cases usually require domain knowledge, which the authors have for a subset of the evaluation datasets. For the purposes of this evaluation, we defined 22 manual test cases for the DBpedia ontology (dbo), six for the LinkedGeoData ontology (lgdo), three for the WGS84 Geo Positioning ontology (geo) and 15 manual test cases for the DBpedia in English dataset. Additionally, we defined 20 manual test cases for the SKOS vocabulary exploiting existing domain expertise <ref type="bibr" target="#b27">[26]</ref>. Table <ref type="table" target="#tab_5">4</ref> presents an aggregation of the defined test cases based on the pattern they stem from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">LINKED DATA QUALITY EVALUATION</head><p>To showcase the re-usability of our automatically and manually generated test cases, we chose the following datasets for evaluation:</p><p>â¢ dbpedia.org 12 extracts data from the English Wikipedia and publishes the data using the following 12 http://dbpedia.org (version 3.9) schemas: owl, dbo, foaf, dcterms, dc, skos, geo and prov <ref type="bibr" target="#b22">[21]</ref>.</p><p>â¢ nl.dbpedia.org<ref type="foot" target="#foot_11">13</ref> extracts data from the Dutch Wikipedia edition using the same vocabularies as the English DBpedia.</p><p>â¢ linkedgeodata.org<ref type="foot" target="#foot_12">14</ref> provides a linked data mirror of OpenStreetMap<ref type="foot" target="#foot_13">15</ref> using the following schemas: ngeo, spatial, lgdo, dcterms, gsp, owl, geo, skos and foaf <ref type="bibr" target="#b26">[25]</ref>.</p><p>â¢ id.loc.gov <ref type="foot" target="#foot_14">16</ref> is a SKOS dataset that publishes Library of Congress authority data using owl, foaf, dcterms, skos, mads, mrel and premis schemas.</p><p>â¢ datos.bne.es <ref type="foot" target="#foot_15">17</ref> provides open bibliographic linked data from the Spanish National Library using owl, frbrer, isbd, dcterms and skos schemas.</p><p>To identify the schemas for each dataset, we used existing information from the LODStats project<ref type="foot" target="#foot_16">18</ref>  <ref type="bibr" target="#b7">[6]</ref>. The English (dben) and Dutch (dbnl) DBpedia share a similar structure, but the actual data differs <ref type="bibr" target="#b19">[18]</ref>. Both DBpedia and the LinkedGeoData (lgd) datasets are generated from crowdsourced content and thus are prone to errors. The Library of Congress authority data (loc) and the Open bibliographic data from the Spanish National Library (datos) were chosen as high quality bibliographic datasets with loc focusing on publishing SKOS and in the case of datos FRBR<ref type="foot" target="#foot_17">19</ref> data. The DBpedia datasets were tested using their online SPARQL endpoints and the other three datasets were loaded in a local triple store <ref type="foot" target="#foot_18">20</ref> .</p><p>Table <ref type="table">5</ref> provides an overview of the dataset quality evaluation. In Table <ref type="table" target="#tab_7">6</ref> we present the total errors aggregated per schema and in Table <ref type="table" target="#tab_8">7</ref> the total errors aggregated per pattern. The test coverage for every dataset is provided in Table <ref type="table" target="#tab_9">8</ref>.</p><p>The occurrence of a high number of errors in the English DBpedia is attributed to the data loaded from external sources. For example, the recent load of transformed Wikidata data <ref type="foot" target="#foot_19">21</ref> almost doubled the rdfs:domain and rdfs:range violations and errors in the geo schema. A common error in DBpedia is the rdfs:range violation. Triples are extracted from data streams and complete object range validation cannot occur at the time of extraction. Example violations from the dbo schema are the rdfs:domain of dbo:sex (1M) and dbo:years (550K) properties. Other dben errors based on the foaf schema are attributed mainly to the incorrect rdfs:domain or rdfs:range of foaf:primaryTopic (12M), foaf:isPrimaryTopicOf (12M) foaf:thumbnail (3M) and foaf:homepage (0.5M).</p><p>Among errors from the manual test cases created for the DBpedia ontology are the following:  <ref type="table">5</ref>: Evaluation overview for the five tested datasets. For every dataset we display the total number of triples and the distinct number of subjects. We mention the total number of test cases (TC) that were run on each dataset, how many tests passed, failed and timed out (TO). Finally we show the total number of errors, as well the total number of errors that occurred from manual (ManEr) and enriched (EnrEr) tests. The last column shows the average errors per distinct subject.</p><p>â¢ 163K (102K in dbnl) resources with wrong postal code format.</p><p>â¢ 7K (137 in dbnl) books with wrong ISBN format.</p><p>â¢ 40K (1.2K in dbnl) persons with a death date and without birth date.</p><p>â¢ 638K persons without a birth date.</p><p>â¢ 197K places without coordinates.</p><p>â¢ 242K resources with coordinates that are not a dbo:Place.</p><p>â¢ 28K resources with exactly the same coordinates with another resource.</p><p>â¢ 9 resources with invalid longitude.</p><p>The lgd dataset also has a high number of errors per resource. Although the LinkedGeoData ontology is big, the information of interest for our methodology is mostly limited to rdfs:domain and rdfs:range axioms. Due to its broad vocabulary, mostly stemming from curated crowdsourced user input, only a few manual test cases were found. In-depth domain knowledge is required to define further test cases. These resulted in 132K errors for resources with a lgdo:fixme predicate and 250 with lgdo:todo, 637 wrong phone numbers and 22 resources having a lgdo:start property but no lgdo:end.</p><p>The datos dataset yielded a total of 28 million errors. In absolute numbers, rdfs:domain and rdfs:range violations were dominant. The isbd:P1016 and isbd:P1185 properties produced the most rdfs:domain violations (2.38M and 2.35M respectively). The schemas used in datos are expressive and there were many violations stemming from owl:disjointWith and owl:propertyDisjointWith constraints. With regards to the manual errors, 6 occurred due to shared literals between skos:prefLabel and skos:altLabel <ref type="bibr" target="#b27">[26]</ref> and 25 because of property disjointness violations between skos:broader, skos:narrower and skos:related.</p><p>The loc dataset generated a total of 9 million errors.</p><p>However, 99.9% originated from one test case: the rdfs:domain of skos:member. Other minor errors occurred in other schemas (cf.</p><p>Table <ref type="table" target="#tab_7">6</ref>), e.g. incorrect rdfs:domain of skos:topConceptOf and incorrect rdfs:domain of foaf:focus. Similar to the datos dataset, 49 manual errors occurred from disjoint properties between skos:broader, skos:narrower and skos:related. The highest test coverage is found in the datos dataset. This is due to the rich frbrer and isbd schemas. Although dben had a bigger test set than datos, it publishes a lot of automatically generated properties under the dbp namespace [21, Section 2] which lowers the coverage scores. The low test coverage for lgd can be attributed to the very large but relatively flat and inexpressive schema. For DBpedia in Dutch we evaluated the Live endpoint and thus could not calculate property and class occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Errors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>The most frequent errors in all datasets were produced from rdfs:domain and rdfs:range test cases. Domain and range are two of the most commonly expressed axioms in most schemas and, thus, produce many automated test cases and good test coverage. Errors from such violations alone cannot classify a dataset as low quality. In DBpedia, a resource is generated for every Wikipedia page and connected with the original article through the foaf:primaryTopic, foaf:isPrimaryTopicOf and prov:wasDerivedFrom predicates. DBpedia neither states that the Wikipedia page is a foaf:Document nor that the DBpedia resource a prov:Entity, as the FOAF and PROV vocabularies demand. This produced a total of 33 million errors (35% of the total errors) in the English DBpedia. In most cases, fixing such errors is easy and dramatically reduces the error rate of a dataset. However, DBpedia, as well as most LOD datasets, do not load all the schemas they reference in their endpoints. Thus, locating such errors by using only local knowledge is not effective, whereas our pattern library can be used without further overhead.</p><p>Testing for external vocabularies. According to our methodology, a dataset is tested against all the schemas it references. Although this approach provides better testing coverage, it can be insufficient when testing against unused data. Properties like foaf:weblog that do not exist in neither evaluated dataset, autogenerate three test cases for rdfs:domain, rdfs:range and owl:InverseFunctionalProperty. In the future, the methodology could be refined to intelligently pre-process a dataset and reduce the number of test cases to run.</p><p>Revision of manually instantiated patterns. Although our pattern library already covers a wide range of data quality errors, there are cases where the mere instantiation of patterns is not sufficient. Binding COMP-a (cf. Table <ref type="table" target="#tab_3">1</ref>), for example, returns 509 results in the English DBpedia. Some of these results have, however, incomplete dates (i.e. just xsd:gMonthDay). Technically, these results are outside of the scope of the binding and the pattern and, therefore, a false positive. This can only be resolved by writing manual test cases or adding another DQTP. In this scenario, the extended test case could be as follows: While axioms in an OWL ontology are intended to be applicable in a global context, our test-driven methodology also depends on domain knowledge to capture more semantics in data. However, there are cases where data constraints can be very application specific and not universally valid. For instance, due to the vast size of DBpedia, it is unrealistic to expect completeness, e.g. that every dbo:Person has a foaf:depiction and a dbo:birthDate. However, in the context of an application like "A day like today in history"<ref type="foot" target="#foot_20">22</ref> these properties are mandatory. Thus, a refinement of the methodology could support manual tests cases associated for an application context.</p><p>The software used to generate the test cases and produce the evaluation results is available as open source <ref type="foot" target="#foot_21">23</ref> . At the project website <ref type="foot" target="#foot_22">24</ref> , we provide a user interface and a dump of all results as RDF .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Previous Data Quality Measurements on DBpedia. The first publication of DBpedia <ref type="bibr" target="#b2">[1]</ref> mainly concentrates on the data source -Wikipedia. Errors in the RDF data are attributed to several shortcomings in the authoring process, e.g. the usage of tables instead of templates, the encoding of layout information like color in templates and so on. Other inaccuracies occur due to an imprecise use of the wiki markup or when duplicate information is given, as in height = 5 <ref type="bibr">'11" (180cm)</ref>. To avoid those errors the authors provide some authoring guidelines in accordance with guidelines created by the Wikipedia community.</p><p>In <ref type="bibr" target="#b21">[20]</ref>, the authors concentrate more on the extraction process, comparing the Generic with the Mapping-based Infobox Extraction approach. It is shown that by mapping Wikipedia templates to a manually created, simple ontology, one can obtain a far better data quality, eliminating data type errors as well as a better linkage between entities of the dataset. Other errors concern class hierarchies e.g. omissions in the automatically created YAGO classification schema.</p><p>Another issue already addressed in the future work section of <ref type="bibr" target="#b21">[20]</ref> is the fusion of cross-language knowledge of the language specific DBpedia instances. This topic as well as other internationalization issues are treated in <ref type="bibr" target="#b19">[18]</ref>. There, different extraction problems of the Greek DBpedia are presented that can also be applied to other languages, especially those using non-Latin characters.</p><p>Another study aimed to develop a framework for the DBpedia quality assessment is presented in <ref type="bibr" target="#b28">[27]</ref> and involves a manual and a semi-automatic process. In the manual phase the authors detects common problems and classify them in a taxonomy. After that, they crowdsource the evaluation of a large number of individual resources and let users structure it according to their taxonomy.</p><p>General Linked Data Quality Assessment. There exist several approaches for assessing the quality of Linked Data. Approaches can be broadly classified into (i) automated (e.g. <ref type="bibr" target="#b13">[12]</ref>), (ii) semi-automated (e.g. <ref type="bibr">[9]</ref>) or (iii) manual (e.g. <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b23">22]</ref>) methodologies. These approaches are useful at the process level wherein they introduce systematic methodologies to assess the quality of a dataset. However, the drawbacks include a considerable amount of user involvement, inability to produce interpretable results, or not al-lowing a user the freedom to choose the input dataset. In our case, we focused on a very lightweight framework and the development of a library based on real user input.</p><p>Additionally, there have been efforts to assess the quality of Web data <ref type="bibr" target="#b6">[5]</ref> on the whole, which included the analysis of 14.1 billion HTML tables from Google's general-purpose web crawl in order to retrieve tables with high-quality relations. In a similar vein, in <ref type="bibr" target="#b15">[14]</ref>, the quality of RDF data was assessed. This study detected the errors occurring while publishing RDF data along with the effects and means to improve the quality of structured data on the Web. In a recent study, 4 million RDF/XML documents were analysed which provided insights into the level of conformance these documents had in accordance to the Linked Data guidelines. On the one hand, these efforts contributed towards assessing a vast amount of Web or RDF/XML data, however, most of the analyses were performed automatically, thereby overlooking the problems arising due to contextual discrepancies. In previous work, we used similar ideas for describing the evolution of knowledge bases <ref type="bibr" target="#b24">[23]</ref>.</p><p>Rules and SPARQL. The approach described in <ref type="bibr" target="#b12">[11]</ref> advocates the use of SPARQL and SPIN for RDF data quality assessment and shares some similarity with our methodology. However, a domain expert is required for the instantiation of test case patterns. SPARQL Inferencing Notation (SPIN) <ref type="bibr" target="#b18">[17]</ref> is a W3C submission aiming at representing rules and constraints on Semantic Web models. SPIN also allows users to define SPARQL functions and reuse SPARQL queries. The difference between SPIN and our pattern syntax is that SPIN functions would not fully support our Pattern Bindings. SPIN function arguments must have specific constraints on the argument datatype or argument class and do not support operators, e.g. '=', '&gt;', '!', '+', '*', or property paths 25 . However, our approach is still compatible with SPIN when allowing to initialise templates with specific sets of applicable operators. In that case, however, the number of templates increases. Due to this restrictions, SPIN defines fewer but more general constraints. The following SPIN example 26 tries to locate all the owl:disjointWith constraint violations:</p><p>1 SELECT ? x WHERE { ? c1 owl : disjointWith ? c2 . 2 ? x a ? c1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>? x a ? c2 . }</p><p>The problems of these types of queries is that: 1) they are more expensive to execute, 2) aggregate all errors in a single result which makes it harder to debug and 3) cannot capture violations like foaf:primaryTopic if the foaf schema is not loaded in the knowledge base itself. One of the advantages of converting our templates to SPIN is that the structure of the SPARQL query itself can be stored directly in RDF, which, however, renders it more complex. From the efforts related to SPIN, we re-used their existing data quality patterns and ontologies for error types. In a similar way, FÃ¼rber et al. <ref type="bibr">[10]</ref> define a set of generic SPARQL queries to identify missing or illegal literal values and datatypes and functional dependency violations.</p><p>Another related approach is the Pellet Integrity Constraint Validator (ICV) 27 . Pellet ICV <ref type="bibr" target="#b25">[24]</ref> translates OWL integrity constraints into SPARQL queries. Similar to our approach, the execution of those SPARQL queries indicate violations. An implication of the integrity constraint semantics of Pellet ICV is that a partial unique names assumption (all resources are considered to be different unless equality is explicitly stated) and a closed world assumption is in effect. We use the same strategy as part of our methodology, but go beyond it by allowing users to directly (re-)use DQTPs not necessarily encoded in OWL and by providing automatic schema enrichment.</p><p>For XML, Schematron 28 is an ISO standard for validation and quality control of XML documents based on XPath and XSLT. We argue that similar adapted mechanisms for RDF are of crucial importance to provide solutions allowing the usage of RDF in settings, which require either high quality data or at least an accurate assessment of its quality.</p><p>In database research, there are related approaches to formulate common integrity constraints <ref type="bibr" target="#b8">[7]</ref> using First Order Logic (FOL). The work presented in <ref type="bibr" target="#b9">[8]</ref> uses FOL to describe data dependencies for quality assessment and suggests repairing strategies. Finally, in <ref type="bibr" target="#b20">[19]</ref>, the authors suggest extensions to RDF by constraints akin to RDBMS in order to validate data using SPARQL as a constraint language. This is achieved by providing an RDF view on top of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we described a novel approach for assessing and improving Linked Data quality. The approach is inspired by test-driven software engineering and is centred around the definition of data quality integrity constraints, which are represented in SPARQL query templates. We compiled a comprehensive set of generic Data Quality Test Patterns (DQTP), which we instantiated for 297 schemas resulting in 32,293 test cases. We reused these test cases to evaluate the quality of five LOD datasets. Our evaluation showed that DQTPs are able to reveal a substantial amount of data quality issues in an effective and efficient way.</p><p>We see this work as the first step in a larger research and development agenda to position test-driven data engineering similar to test-driven software engineering. In the future, we plan to create test-driven validators for web service formats such as the NLP Interchange Format <ref type="bibr" target="#b14">[13]</ref>. Additionally, we aim to tackle automatic repair strategies, i.e. use of templates and bindings to fix problems efficiently. We also plan to implement a test-driven data quality cockpit, which allows users to easily instantiate and run DQTPs based on custom knowledge bases. As a result, we hope that testdriven data quality can contribute to solve one of the most pressing problems of the Data Web -the improvement of data quality and the increase of Linked Data fitness for use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flowchart showing the test-driven data quality methodology. The left part displays the input sources of our pattern library. In the middle part the different ways of pattern instantiation are shown which lead to the Data Quality Test Cases on the right.</figDesc><graphic coords="4,78.91,53.80,451.90,199.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>: P a t t e r n B a s e d T e s t C a s e ; 3 tddo : appliesTo tddo : Schema ; 4 tddo : generated tddo : AutoGenerated ; 5 tddo : source &lt; http :// xmlns . com / foaf /0.1/ &gt; ; 6 tddo : references foaf : Document , foaf : i sP r im a ry T op i cO f ; 7 tddo : testGenerator tddg : RDFSRANGEC . 8 tddo : basedOnPattern tddp : RDFSRANGE ; 9 tddo : binding [ ... ] ; 10 tddo : te s tC a se Lo g Le v el rlog : Error . dbo:deathDate before dbo:birthDate b) dbo:releaseDate after dbo:latestReleaseDate MATCH The literal value of a resource matches/ does not match a certain regex pattern ran a) dbo:isbn does not match "Ë[0-9]{5}$" b) foaf:phone contains any letters ("[A-Za-z]") LITRAN The literal value of a specifically typed resource must (not) be within a given range ran pdep dom mem a) dbo:height of a dbo:Person is not within [0.4,2.5] b) geo:lat of a spatial:Feature is not within [-90,90] TYPE-DEP Type dependency: The type of a resource may imply the attribution of another type. dom cdep a) a resource is a gml:_Feature but not a dbo:Place b) a resource is a foaf:Person but not a dbo:Person TYPRO-DEP A resource of a specific type should have a certain property. dom mem pdep a) a foaf:Document should have a foaf:primaryTopic b) a dbo:Person should have a dbo:birthDate PVT If a resource has a certain value V assigned via a property P1 that in some way classifies this resource, the existence of another property P2 can be assumed. dom pdep a) DBpedia articles stemming from a Geographic location template must have coordinates assigned via georss:point b) DBpedia resources in the category 1907 births should have a dbo:birthDate TRIPLE A resource can be considered erroneous if there are corresponding hints contained in the dataset a) resources stemming from maybe copy-pasted Wikipedia articles having the category Possible cut-and-paste moves b) geographical features (of the linkedgeodata.org dataset) that are marked with the lgdo:fixme property ONE-LANG A literal value should contain at most one literal for a certain language. ran card a) a resource should only have one English foaf:name b) a resource should only have one English rdfs:labelRDFS-DOMAINThe attribution of a resource's property (with a certain value) is only valid if the resource is of a certain type. dom pdep mem a) a resource having a dbo:demographicsAsOf property not being a dbo:PopulatedPlace b) a resource has the "Cities of Africa" category assigned but is not of type dbo:City RDFS-RANGE The attribution of a resource's property is only valid if the value is of a certain type ran pdep mem a) a dbo:Person's spouse not being a dbo:Person b) a resource assigned via the foaf:based_near property not being of type geo:SpatialThing RDFS-RANGED The attribution of a resource's property is only valid if the literal value has a certain datatype ran pdep mem a) the value of the property dbo:isPeerReviewed must be of type xsd:boolean b) the value of the property dbo:successfulLaunches must be of type xsd:nonNegativeInteger INV-FUNC Some values assigned to a resource are considered to be unique for this particular resource and must not occur in connection with other resources. ran a) there must not be more than one resource with the same foaf:homepage b) there must not be more than one country with the same dbo:capital OWL-CARD Cardinality restriction on a property ran card a) dbo:birthDate is a functional property b) there should be just one skos:prefLabel OWL-DISJC Disjoint class constraint cdep a) a foaf:Document is disjoint with foaf:Project b) a dbo:Person is disjoint with dbo:Work :prefLabel is disjoint with skos:hiddenLabel b) dbo:bandMember is disjoint with dbo:birthPlace :parent is irreflexive b) dbo:child is irreflexive</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>http://svn.aksw.org/papers/2014/ WWW_Databugger/public.pdf (see Appendix) for a complete pattern description.COMP Pattern . Depending on the property semantics, there are cases where two different literal values must have a specific ordering with respect to an operator. P1 and P2 Top 10 schemas with descending number of automatically generated test cases.are the datatype properties we need to compare and OP is the comparison operator R(OP ) = { &lt;, &lt;=, &gt;, &gt;=, =, != }. Application logic or real world constraints may put restrictions on the form of a literal value.P1 is the property we need to check against REGEX and NOP can be a not operator ('!') or empty.</figDesc><table><row><cell></cell><cell cols="2">Schema Test Cases</cell><cell cols="3">Schema Test Cases</cell></row><row><cell></cell><cell>dicom</cell><cell>8,229</cell><cell>mo</cell><cell></cell><cell>605</cell></row><row><cell></cell><cell>dbo</cell><cell>5,713</cell><cell>tio</cell><cell></cell><cell>525</cell></row><row><cell></cell><cell>frbrer</cell><cell>2,166</cell><cell>uco</cell><cell></cell><cell>516</cell></row><row><cell></cell><cell>biopax</cell><cell>688</cell><cell>vvo</cell><cell></cell><cell>506</cell></row><row><cell></cell><cell>hdo</cell><cell>682</cell><cell>ceo</cell><cell></cell><cell>511</cell></row><row><cell>1</cell><cell cols="3">SELECT ? s WHERE { ? s %% P1 %% ? v1 .</cell><cell></cell></row><row><cell>2</cell><cell></cell><cell cols="2">? s %% P2 %% ? v2 .</cell><cell></cell></row><row><cell>3</cell><cell></cell><cell cols="3">FILTER ( ? v1 %% OP %% ? v2 ) }</cell></row><row><cell></cell><cell>Example bindings:</cell><cell cols="4">(a) dbo:deathDate before '&lt;'</cell></row><row><cell></cell><cell>dbo:birthDate,</cell><cell>(b)</cell><cell>dbo:releaseDate</cell><cell>after</cell><cell>'&gt;'</cell></row><row><cell></cell><cell cols="2">dbo:latestReleaseDate.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">MATCH Pattern.</cell><cell></cell><cell></cell></row></table><note><p>1 SELECT ? s WHERE { ? s %% P1 %% ? value . 2 FILTER ( %% NOP %% regex ( str (? value ) , %% REGEX %) ) }</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Example templates and bindings. The column Type refers to the coverage type.</figDesc><table><row><cell>Schema</cell><cell>TC</cell><cell>Schema</cell><cell>TC</cell></row><row><cell>dbpedia.org</cell><cell>1,723</cell><cell>id.loc.gov</cell><cell>48</cell></row><row><cell>nl.dbpedia.org</cell><cell>845</cell><cell>datos.bne.org</cell><cell>18</cell></row><row><cell>linkedgeodata.org</cell><cell>61</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Number of additional test cases (TC) instantiated for the enriched schemas.</figDesc><table><row><cell>Pattern</cell><cell cols="2">Test Cases Manual</cell></row><row><cell>RDFSDOMAIN</cell><cell>16,645</cell><cell>3</cell></row><row><cell>RDFSRANGE</cell><cell>9.727</cell><cell>4</cell></row><row><cell>OWLDISJC</cell><cell>5,530</cell><cell>-</cell></row><row><cell>EDFSRANGED</cell><cell>5,073</cell><cell>-</cell></row><row><cell>OWLDISJP</cell><cell>1,813</cell><cell>10</cell></row><row><cell>OWLCARD</cell><cell>1,818</cell><cell>6</cell></row><row><cell>TYPRODEP</cell><cell>746</cell><cell>13</cell></row><row><cell>OWLASYMP</cell><cell>660</cell><cell>-</cell></row><row><cell>OWLIRREFL</cell><cell>342</cell><cell>-</cell></row><row><cell>INVFUNC</cell><cell>338</cell><cell>1</cell></row><row><cell>MATCH</cell><cell>9</cell><cell>9</cell></row><row><cell>LITRAN</cell><cell>5</cell><cell>5</cell></row><row><cell>COMP</cell><cell>4</cell><cell>4</cell></row><row><cell>ONELANG</cell><cell>4</cell><cell>4</cell></row><row><cell>PROPDEP</cell><cell>4</cell><cell>4</cell></row><row><cell>TYPDEP</cell><cell>2</cell><cell>2</cell></row><row><cell>TRIPLE</cell><cell>2</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Number of total and manual test cases per pattern for all LOV vocabularies.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Total errors in the evaluated datasets per schema.</figDesc><table><row><cell>Schema</cell><cell cols="3">TC dben dbnl</cell><cell cols="2">lgd dat.</cell><cell>loc</cell></row><row><cell>dbo</cell><cell>5.7K</cell><cell cols="2">7.9M 716K</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>frbrer</cell><cell>2.1K</cell><cell>-</cell><cell>-</cell><cell cols="2">-11K</cell><cell>-</cell></row><row><cell>lgdo</cell><cell>224</cell><cell>-</cell><cell cols="2">-2.8M</cell><cell>-</cell><cell>-</cell></row><row><cell>isbd</cell><cell>179</cell><cell>-</cell><cell>-</cell><cell cols="2">-28M</cell><cell>-</cell></row><row><cell>prov</cell><cell>125</cell><cell>25M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>foaf</cell><cell>95</cell><cell cols="2">25M 4.6M</cell><cell>-</cell><cell>-</cell><cell>59</cell></row><row><cell>gsp</cell><cell>83</cell><cell>-</cell><cell>-</cell><cell>39M</cell><cell>-</cell><cell>-</cell></row><row><cell>mads</cell><cell>75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-0.3M</cell></row><row><cell>owl</cell><cell>48</cell><cell>5</cell><cell>3</cell><cell>2</cell><cell>5</cell><cell>-</cell></row><row><cell>skos</cell><cell>28</cell><cell>41</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9M</cell></row><row><cell>dcterms</cell><cell>28</cell><cell>960</cell><cell cols="3">881 191K 37K</cell><cell>659</cell></row><row><cell>ngeo</cell><cell>18</cell><cell></cell><cell>-</cell><cell>119</cell><cell>-</cell><cell>-</cell></row><row><cell>geo</cell><cell>7</cell><cell cols="2">2.8M 120K</cell><cell>16M</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Total errors per pattern.</figDesc><table><row><cell>Pattern</cell><cell cols="2">dben dbnl</cell><cell cols="2">lgd datos</cell><cell>loc</cell></row><row><cell>COMP</cell><cell>1.7M</cell><cell>7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>INVFUNC</cell><cell>279K</cell><cell>13K</cell><cell>-</cell><cell cols="2">511 3.5K</cell></row><row><cell>LITRAN</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MATCH</cell><cell cols="2">171K 103K</cell><cell>637</cell><cell>-</cell><cell>-</cell></row><row><cell>OWLASYMP</cell><cell>19K</cell><cell>3K</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OWLCARD</cell><cell>610</cell><cell>291</cell><cell>1</cell><cell>1</cell><cell>3</cell></row><row><cell>OWLDISJC</cell><cell>92</cell><cell>-</cell><cell>-</cell><cell cols="2">8.1K 1.1K</cell></row><row><cell>OWLDISJP</cell><cell>3.4K</cell><cell>7K</cell><cell>-</cell><cell>53</cell><cell>223</cell></row><row><cell>OWLIRREFL</cell><cell>1.4K</cell><cell>14</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PVT</cell><cell cols="2">267K 1.2K</cell><cell>22</cell><cell>-</cell><cell>-</cell></row><row><cell>RDFSDOMAIN</cell><cell cols="2">31M 2.3M</cell><cell>55M</cell><cell>28M</cell><cell>9M</cell></row><row><cell>RDFSRANGE</cell><cell cols="3">26M 2.5M 191K</cell><cell cols="2">320K 111K</cell></row><row><cell cols="4">RDFSRANGED 760K 286K 2.7M</cell><cell>2</cell><cell>-</cell></row><row><cell>TRIPLE</cell><cell>-</cell><cell cols="2">-132K</cell><cell>-</cell><cell>-</cell></row><row><cell>TYPDEP</cell><cell>674K</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TYPRODEP</cell><cell cols="2">2M 100K</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Metric</cell><cell>dben</cell><cell></cell><cell>lgd</cell><cell>datos</cell><cell>loc</cell></row><row><cell>f pdom</cell><cell>20.32%</cell><cell cols="4">8.98% 72.26% 20.35%</cell></row><row><cell>fpran</cell><cell cols="5">23.67% 10.78% 37.64% 28.78%</cell></row><row><cell>f pdep</cell><cell cols="5">24.93% 13.65% 77.75% 29.78%</cell></row><row><cell>f card</cell><cell cols="5">23.67% 10.78% 37.63% 28.78%</cell></row><row><cell>fmem</cell><cell cols="5">73.51% 12.78% 93.57% 58.62%</cell></row><row><cell>f cdep</cell><cell>37.55%</cell><cell></cell><cell cols="3">0% 93.56% 36.86%</cell></row><row><cell cols="2">Cov(QS, D) 33.94%</cell><cell cols="4">9.49% 68.74% 33.86%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Test coverage on the evaluated datasets.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://lov.okfn.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use http://prefix.cc to resolve all name spaces and prefixes. A full list can be found at http://prefix.cc/popular/all</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.w3.org/TR/owl2-primer/#Advanced_Use_of_Datatypes</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://lists.sourceforge.net/lists/listinfo/dbpedia-discussion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://lists.sourceforge.net/lists/listinfo/dbpedia-developers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://stackoverflow.com/questions/tagged/dbpedia</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>http://answers.semanticweb.com/tags/dbpedia/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>http://en.wikipedia.org/wiki/Category:Wikipedia_maintenance</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>http://clarkparsia.com/pellet/icv/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>LOV had 367 vocabularies at the date of last access (5/10/2013) but not all were accessible.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>In addition to the LOV schemas, dbo (http://dbpedia.org/ ontology/), frbrer (http://iflastandards.info/ns/fr/frbr/frbrer/) and isbd (http://iflastandards.info/ns/isbd/elements/) schemas are included as prefixes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11"><p>http://nl.dbpedia.org (live version, accessed on 05/10)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12"><p>http://downloads.linkedgeodata.org/releases/2013-08-14/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13"><p>http://www.openstreetmap.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_14"><p>http://id.loc.gov/download/ (accessed on 05/10/2013)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_15"><p>http://datos.bne.es/datadumps/, (accessed on 05/10/2013)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_16"><p>http://stats.lod2.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_17"><p>www.oclc.org/research/activities/frbr.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_18"><p>We used the Virtuoso V7 triple store, because it supports SPARQL 1.1 property paths.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_19"><p>http://www.mail-archive.com/dbpedia-discussion@lists. sourceforge.net/msg05583.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_20"><p>http://el.dbpedia.org/apps/DayLikeToday/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_21"><p>http://github.com/AKSW/Databugger</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_22"><p>http://databugger.aksw.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by grants from the European Union's 7th Framework Programme provided for the projects LOD2 (GA no. 257943), GeoKnow (GA no. 318159) and DIACHRON (GA no. 601043).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.schematron.com/" />
		<title level="m">ADDITIONAL AUTHORS Additional authors: Amrapali Zaveri (University of Leipzig, email: zaveri@informatik</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What have Innsbruck and Leipzig in common? extracting semantics from wiki content</title>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ESWC</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the ESWC<address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="volume">4519</biblScope>
			<biblScope unit="page" from="503" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quality-driven information filtering using the WIQA policy framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semantics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2009-01">Jan 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal OWL axiom enrichment for large knowledge bases</title>
		<author>
			<persName><forename type="first">L</forename><surname>BÃ¼hmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EKAW 2012</title>
		<meeting>EKAW 2012</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="57" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pattern based knowledge base enrichment</title>
		<author>
			<persName><forename type="first">L</forename><surname>BÃ¼hmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Semantic Web Conference</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10-25">21-25 October 2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Webtables: exploring the power of tables on the web</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="538" to="549" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LODStats -an extensible framework for high-performance dataset analytics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EKAW 2012</title>
		<title level="s">Lecture Notes in Computer Science (LNCS)</title>
		<meeting>the EKAW 2012</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7603</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fol modeling of integrity constraints (dependencies)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Database Systems</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ãzsu</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="1155" to="1161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dependencies revisited for improving data quality</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-seventh ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS &apos;08</title>
		<meeting>the Twenty-seventh ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="159" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Quality characteristics of linked data publishing datasources</title>
		<author>
			<persName><forename type="first">A</forename><surname>Flemming</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Humboldt-UniversitÃ¤t of Berlin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using semantic web resources for data quality management</title>
		<author>
			<persName><forename type="first">C</forename><surname>FÃ¼rber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Engineering and Management by the Masses</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Pinto</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6317</biblScope>
			<biblScope unit="page" from="211" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using SPARQL and SPIN for data quality management on the semantic web</title>
		<author>
			<persName><forename type="first">C</forename><surname>FÃ¼rber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Business Information Processing</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Abramowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Tolksdorf</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="35" to="46" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Assessing linked data mappings using network measures</title>
		<author>
			<persName><forename type="first">C</forename><surname>GuÃ©ret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Extended Semantic Web Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the 9th Extended Semantic Web Conference</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7295</biblScope>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrating nlp using linked data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>BrÃ¼mmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Semantic Web Conference</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10">October 2013. 2013</date>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Weaving the pedantic web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polleres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>LDOW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Radon -repair and diagnosis in ontology networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>StadtmÃ¼ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Traverso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciravegna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>HyvÃ¶nen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mizoguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Oren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">P B</forename><surname>Sabou</surname></persName>
		</editor>
		<editor>
			<persName><surname>Simperl</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5554</biblScope>
			<biblScope unit="page" from="863" to="867" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Quality Control Handbook</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Juran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988-08">August 1988</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
	<note>4th edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">SPINoverview and motivation. W3C Member Submission, W3C</title>
		<author>
			<persName><forename type="first">H</forename><surname>Knublauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Idehen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-02">February 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Internationalization of linked data: The case of the greek dbpedia edition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bratsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Metakides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="51" to="61" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SPARQLing constraints for RDF</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Extending Database Technology: Advances in Database Technology, EDBT &apos;08</title>
		<meeting>the 11th International Conference on Extending Database Technology: Advances in Database Technology, EDBT &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="499" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DBpediaa crystallization point for the web of data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DBpedia -a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web Journal</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sieve: linked data quality assessment and fusion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>MÃ¼hleisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT/ICDT Workshops</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Ari</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="116" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EvoPat -Pattern-Based Evolution and Refactoring of RDF Knowledge Bases</title>
		<author>
			<persName><forename type="first">C</forename><surname>RieÃ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tramp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Semantic Web Conference (ISWC2010)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the 9th International Semantic Web Conference (ISWC2010)<address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards integrity constraints in owl</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sirin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on OWL: Experiences and Directions</title>
		<meeting>the Workshop on OWL: Experiences and Directions</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linkedgeodata: A core for a web of spatial open data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>HÃ¶ffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="354" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving the quality of SKOS vocabularies with skosify</title>
		<author>
			<persName><forename type="first">O</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>HyvÃ¶nen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th international conference on Knowledge Engineering and Knowledge Management, EKAW&apos;12</title>
		<meeting>the 18th international conference on Knowledge Engineering and Knowledge Management, EKAW&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="383" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">User-driven quality evaluation of DBpedia</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zaveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sherif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>BÃ¼hmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 9th International Conference on Semantic Systems, I-SEMANTICS &apos;13</title>
		<meeting>9th International Conference on Semantic Systems, I-SEMANTICS &apos;13<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">September 4-6, 2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Software unit test coverage and adequacy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A V</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H R</forename><surname>May</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="366" to="427" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
