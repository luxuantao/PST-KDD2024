<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-We present Tapestry, a peer-to-peer overlay routing infrastructure offering efficient, scalable, locationindependent routing of messages directly to nearby copies of an object or service using only localized resources. Tapestry supports a generic Decentralized Object Location and Routing (DOLR) API using a self-repairing, soft-state based routing layer. This article presents the Tapestry architecture, algorithms, and implementation. It explores the behavior of a Tapestry deployment on PlanetLab, a global testbed of approximately 100 machines. Experimental results show that Tapestry exhibits stable behavior and performance as an overlay, despite the instability of the underlying network layers. Several widely-distributed applications have been implemented on Tapestry, illustrating its utility as a deployment infrastructure. Index Terms-Tapestry, peer to peer, overlay networks, service deployment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Internet developers are constantly proposing new and visionary distributed applications. These new applications have a variety of requirements for availability, durability, and performance. One technique for achieving these properties is to adapt to failures or changes in load through migration and replication of data and services. Unfortunately, the ability to place replicas or the frequency with which they may be moved is limited by underlying infrastructure. The traditional way to deploy new applications is to adapt them somehow to existing infrastructures (often an imperfect match) or to standardize new Internet protocols (encountering significant intertia to deployment). A flexible but standardized substrate on which to develop new applications is needed.</p><p>In this article, we present Tapestry <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, an extensible infrastructure that provides Decentralized Object Location and Routing (DOLR) <ref type="bibr" target="#b2">[3]</ref>. The DOLR interface focuses on routing of messages to endpoints such as nodes or object replicas. DOLR virtualizes resources, since endpoints are named by opaque identifiers encoding nothing about physical location. Properly implemented, this virtualization enables message delivery to mobile This work was supported by NSF Career Awards #ANI-9985129, #ANI-9985250, NSF ITR Award #5710001344, California Micro Fund Awards #02-032 and #02-035, and by grants from IBM and Sprint.</p><p>or replicated endpoints in the presence of instability in the underlying infrastructure. As a result, a DOLR network provides a simple platform on which to implement distributed applications-developers can ignore the dynamics of the network except as an optimization. Already, Tapestry has enabled the deployment of globalscale storage applications such as OceanStore <ref type="bibr" target="#b3">[4]</ref> and multicast distribution systems such as Bayeux <ref type="bibr" target="#b4">[5]</ref>; we return to this in Section VI.</p><p>Tapestry is a peer-to-peer overlay network that provides high-performance, scalable, and locationindependent routing of messages to close-by endpoints, using only localized resources. The focus on routing brings with it a desire for efficiency: minimizing message latency and maximizing message throughput. Thus, for instance, Tapestry exploits locality in routing messages to mobile endpoints such as object replicas; this behavior is in contrast to other structured peer-to-peer overlay networks <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b10">[11]</ref>.</p><p>Tapestry uses adaptive algorithms with soft-state to maintain fault-tolerance in the face of changing node membership and network faults. Its architecture is modular, consisting of an extensible upcall facility wrapped around a simple, high-performance router. This Applications Programming Interface (API) enables developers to develop and extend overlay functionality when the basic DOLR functionality is insufficient.</p><p>In the following pages, we describe a Java-based implementation of Tapestry, and present both microand macro-benchmarks from an actual, deployed system. During normal operation, the relative delay penalty (RDP) <ref type="foot" target="#foot_0">1</ref> to locate mobile endpoints is two or less in the wide area. Importantly, we show that Tapestry operations succeed nearly 100% of the time under both constant network changes and massive failures or joins, with small periods of degraded performance during selfrepair. These results demonstrate Tapestry's feasibility as a long running service on dynamic, failure-prone networks, such as the wide-area Internet.</p><p>The following section discusses related work. Then, Tapestry's core algorithms appear in Section III, with details of the architecture and implementation in Section IV. Section V evaluates Tapestry's performance. We then discuss the use of Tapestry as an application infrastructure in Section VI. We conclude with Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The first generation of peer-to-peer (P2P) systems included file-sharing and storage applications: Napster, Gnutella, Mojo Nation, and Freenet. Napster uses central directory servers to locate files. Gnutella provides a similar, but distributed service using scoped broadcast queries, limiting scalability. Mojo Nation <ref type="bibr" target="#b11">[12]</ref> uses an online economic model to encourage sharing of resources. Freenet <ref type="bibr" target="#b12">[13]</ref> is a file-sharing network designed to resist censorship. Neither Gnutella nor Freenet guarantee that files can be located-even in a functioning network.</p><p>The second generation of P2P systems are structured peer-to-peer overlay networks, including Tapestry <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, Chord <ref type="bibr" target="#b7">[8]</ref>, Pastry <ref type="bibr" target="#b6">[7]</ref>, and CAN <ref type="bibr" target="#b5">[6]</ref>. These overlays implement a basic Key-Based Routing (KBR) interface, that supports deterministic routing of messages to a live node that has responsibility for the destination key. They can also support higher level interfaces such as a distributed hash table (DHT) or a decentralized object location and routing (DOLR) layer <ref type="bibr" target="#b2">[3]</ref>. These systems scale well, and guarantee that queries find existing objects under non-failure conditions.</p><p>One differentiating property between these systems is that neither CAN nor Chord take network distances into account when constructing their routing overlay; thus, a given overlay hop may span the diameter of the network. Both protocols route on the shortest overlay hops available, and use runtime heuristics to assist. In contrast, Tapestry and Pastry construct locally optimal routing tables from initialization, and maintain them in order to reduce routing stretch.</p><p>While some systems fix the number and location of object replicas by providing a distributed hash table (DHT) interface, Tapestry allows applications to place objects according to their needs. Tapestry "publishes" location pointers throughout the network to facilitate efficient routing to those objects with low network stretch. This technique makes Tapestry localityaware <ref type="bibr" target="#b13">[14]</ref>: queries for nearby objects are generally satisfied in time proportional to the distance between the query source and a nearby object replica.</p><p>Both Pastry and Tapestry share similarities to the work of Plaxton, Rajamaran, and Richa <ref type="bibr" target="#b14">[15]</ref> for a static network. Others <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> explore distributed object location schemes with provably low search overhead, but they require precomputation, and so are not suitable for dynamic networks. Recent works include systems such as Kademlia <ref type="bibr" target="#b8">[9]</ref>, which uses XOR for overlay routing, and Viceroy <ref type="bibr" target="#b9">[10]</ref>, which provides logarithmic hops through nodes with constant degree routing tables. SkipNet <ref type="bibr" target="#b10">[11]</ref> uses a multi-dimensional skip-list data structure to support overlay routing, maintaining both a DNS-based namespace for operational locality and a randomized namespace for network locality. Other overlay proposals <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> attain lower bounds on local routing state. Finally, proposals such as Brocade <ref type="bibr" target="#b19">[20]</ref> differentiate between local and inter-domain routing to reduce wide-area traffic and routing latency.</p><p>A new generation of applications have been proposed on top of these P2P systems, validating them as novel application infrastructures. Several systems have application level multicast: CAN-MC <ref type="bibr" target="#b20">[21]</ref> (CAN), Scribe <ref type="bibr" target="#b21">[22]</ref> (Pastry), and Bayeux <ref type="bibr" target="#b4">[5]</ref> (Tapestry). In addition, several decentralized file systems have been proposed: CFS <ref type="bibr" target="#b22">[23]</ref> (Chord), Mnemosyne <ref type="bibr" target="#b23">[24]</ref> (Chord, Tapestry), OceanStore <ref type="bibr" target="#b3">[4]</ref> (Tapestry), and PAST <ref type="bibr" target="#b24">[25]</ref> (Pastry). Structured P2P overlays also support novel applications (e.g., attack resistant networks <ref type="bibr" target="#b25">[26]</ref>, network indirection layers <ref type="bibr" target="#b26">[27]</ref>, and similarity searching <ref type="bibr" target="#b27">[28]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TAPESTRY ALGORITHMS</head><p>This section details Tapestry's algorithms for routing and object location, and describes how network integrity is maintained under dynamic network conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The DOLR Networking API</head><p>Tapestry provides a datagram-like communications interface, with additional mechanisms for manipulating the locations of objects. Before describing the API, we start with a couple of definitions.</p><p>Tapestry nodes participate in the overlay and are assigned nodeIDs uniformly at random from a large identifier space. More than one node may be hosted by one physical host. Application-specific endpoints are assigned Globally Unique IDentifiers (GUIDs), selected from the same identifier space. Tapestry currently uses an identifier space of 160-bit values with a globally defined radix (e.g., hexadecimal, yielding 40-digit identifiers). Tapestry assumes nodeIDs and GUIDs are roughly evenly distributed in the namespace, which can be achieved by using a secure hashing algorithm like SHA-1 <ref type="bibr" target="#b28">[29]</ref>. We say that node N has nodeID N , and an object O has GUID O .</p><p>Since the efficiency of Tapestry generally improves with network size, it is advantageous for multiple applications to share a single large Tapestry overlay network. To enable application coexistence, every message contains an application-specific identifier, A , which is used to select a process, or application for message delivery at Tapestry routing mesh from the perspective of a single node. Outgoing neighbor links point to nodes with a common matching prefix. Higher-level entries match more digits. Together, these links form the local routing table. Path of a message. The path taken by a message originating from node 5230 destined for node 42AD in a Tapestry mesh.</p><p>the destination (similar to the role of a port in TCP/IP), or an upcall handler where appropriate.</p><p>Given the above definitions, we state the four-part DOLR networking API as follows:</p><p>1) PUBLISHOBJECT(O , A ): Publish, or make available, object Ç on the local node. This call is best effort, and receives no confirmation. 2) UNPUBLISHOBJECT(O , A ): Best-effort attempt to remove location mappings for Ç. This approach is similar to longest prefix routing used by CIDR IP address allocation <ref type="bibr" target="#b29">[30]</ref>. A node N has a neighbor map with multiple levels, where each level contains links to nodes matching a prefix up to a digit position in the ID, and contains a number of entries equal to the ID's base. The primary Ø entry in the Ø level is the ID and location of the closest node that begins with prefix(AE , ½)+" " (e.g., the Ø entry of the Ø level for node 325AE is the closest node with an ID that begins with 3259. It is this prescription of "closest node" that provides the locality properties of Tapestry. Figure <ref type="figure">1</ref> shows some of the outgoing links of a node.</p><p>Figure <ref type="figure">2</ref> shows a path that a message might take through the infrastructure. The router for the Ò Ø hop shares a prefix of length Ò with the destination ID; thus, to route, Tapestry looks in its (Ò • ½µ Ø level map for the entry matching the next digit in the destination ID. This method guarantees that any existing node in the system will be reached in at most ÐÓ ¬ AE logical hops, in a system with namespace size AE, IDs of base ¬, and assuming consistent neighbor maps. When a digit cannot be matched, Tapestry looks for a "close" digit in the routing table; we call this surrogate routing <ref type="bibr" target="#b1">[2]</ref>, where each non-existent ID is mapped to some live node with a similar ID. Figure <ref type="figure">3</ref>    The messages route towards the root node of 4378. When they intersect the publish path, they follow the location pointer to the nearest copy of the object. routing paths. Primary neighbor links shown in Figure <ref type="figure">1</ref> are augmented by backup links, each sharing the same prefix 2 . At the Ò Ø routing level, the neighbor links differ only on the Ò Ø digit. There are ¢ ¬ pointers on a level, and the total size of the neighbor map is ¢ ¬ ¢ ÐÓ ¬ AE. Each node also stores reverse references (backpointers) to other nodes that point at it. The expected total number of such entries is ¢¬ ¢ÐÓ ¬ AE.</p><p>2) Object Publication and Location: As shown above, each identifier has a unique root node Ê assigned by the routing process. Each such root node inherits a unique spanning tree for routing, with messages from leaf nodes traversing intermediate nodes en route to the root. We utilize this property to locate objects by distributing soft-state directory information across nodes (including the object's root).</p><p>A server S, storing an object O (with GUID, O , and root, O Ê</p><p><ref type="foot" target="#foot_2">3</ref> ), periodically advertises or publishes this object by routing a publish message toward O Ê (see Figure <ref type="figure" target="#fig_3">4</ref>). In general, the nodeID of O Ê is different from O ; O Ê is the unique <ref type="bibr" target="#b1">[2]</ref> node reached through surrogate routing by successive calls to NEXTHOP(*, O ). Each node along the publication path stores a pointer mapping, O , S , instead of a copy of the object itself. When there are replicas of an object on separate servers, each server publishes its copy. Tapestry nodes store location mappings for object replicas in sorted order of network latency from themselves.</p><p>A client locates O by routing a message to O Ê (see Figure <ref type="figure" target="#fig_4">5</ref>). Each node on the path checks whether it has a location mapping for O. If so, it redirects the message to S. Otherwise, it forwards the message onwards to O Ê (guaranteed to have a location mapping).</p><p>Each hop towards the root reduces the number of nodes satisfying the next hop prefix constraint by a factor of the identifier base. Messages sent to a destination from two nearby nodes will generally cross paths quickly because: each hop increases the length of the prefix required for the next hop; the path to the root is a function of the destination ID only, not of the source nodeID (as in Chord); and neighbor hops are chosen for network locality, which is (usually) transitive. Thus, the closer (in network distance) a client is to an object, the sooner its queries will likely cross paths with the object's publish path, and the faster they will reach the object. Since nodes sort object pointers by distance to themselves, queries are routed to nearby object replicas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dynamic Node Algorithms</head><p>Tapestry includes a number of mechanisms to maintain routing table consistency and ensure object availability. In this section, we briefly explore these mechanisms. See <ref type="bibr" target="#b1">[2]</ref> for complete algorithms and proofs. The majority of control messages described here require acknowledgments, and are retransmitted where required.</p><p>1) Node Insertion: There are four components to inserting a new node N into a Tapestry network: a) Need-to-know nodes are notified of N, because N fills a null entry in their routing tables. b) N might become the new object root for existing objects. References to those objects must be moved to N to maintain object availability. c) The algorithms must construct a near optimal routing table for N. d) Nodes near N are notified and may consider using N in their routing tables as an optimization. Node insertion begins at N's surrogate S (the "root" node that N maps to in the existing network). S finds Ô, the length of the longest prefix its ID shares with N . S sends out an Acknowledged Multicast message that reaches the set of all existing nodes sharing the same prefix by traversing a tree based on their nodeIDs. As nodes receive the message, they add N to their routing tables and transfer references of locally rooted pointers as necessary, completing items (a) and (b).</p><p>Nodes reached by the multicast contact N and become an initial neighbor set used in its routing table construction. N performs an iterative nearest neighbor search beginning with routing level Ô. N uses the neighbor set to fill routing level Ô, trims the list to the closest nodes 4 , and requests these nodes send their backpointers (see Section III-B) at that level. The resulting set contains all nodes that point to any of the nodes at the previous routing level, and becomes the next neighbor set. N then decrements Ô, and repeats the process until all levels are filled. This completes item (c). Nodes contacted during the iterative algorithm use N to optimize their routing tables where applicable, completing item (d).</p><p>To ensure that nodes inserting into the network in unison do not fail to notify each other about their existence, every node in the multicast keeps state on every node that is still multicasting down one of its neighbors. This state is used to tell each node with in its multicast tree about . Additionally, the multicast message includes a list of holes in the new node's routing table. Nodes check their tables against the routing table and notify the new node of entries to fill those holes.</p><p>2) Voluntary Node Deletion: If node N leaves Tapestry voluntarily, it tells the set of nodes in N's backpointers of its intention, along with a replacement node for each routing level from its own routing table. The notified nodes each send object republish traffic to both N and its replacement. Meanwhile, N routes references to locally rooted objects to their new roots, and signals nodes in when finished. 3) Involuntary Node Deletion: In a dynamic, failureprone network such as the wide-area Internet, nodes generally exit the network far less gracefully due to node and link failures or network partitions, and may enter and leave many times in a short interval. Tapestry improves object availability and routing in such an environment by building redundancy into routing tables and object location references (e.g., the ½ backup forwarding pointers for each routing table entry).</p><p>To maintain availability and redundancy, nodes use periodic beacons to detect outgoing link and node failures. Such events trigger repair of the routing mesh and initiate redistribution and replication of object location references. Furthermore, the repair process is backed by soft-state republishing of object references. Tapestry repair is highly effective, as shown in Section V-C. Despite continuous node turnover, Tapestry retains nearly a 100% success rate at routing messages to nodes and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TAPESTRY NODE ARCHITECTURE AND IMPLEMENTATION</head><p>In this section, we present the architecture of a Tapestry node, an API for Tapestry extension, details of our current implementation, and an architecture for a higher-performance implementation suitable for use on network processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Component Architecture</head><p>Figure <ref type="figure">6</ref> illustrates the functional layering for a Tapestry node. Shown on top are applications that interface with the rest of the system through the Tapestry API. Below this are the router and the dynamic node management components. The former processes routing and location messages, while the latter handles the arrival and departure of nodes in the network. These two components communicate through the routing table. At the bottom are the transport and neighbor link layers, which together provide a cross-node messaging layer. We describe several of these layers in the following:</p><p>1) Transport: The transport layer provides the abstraction of communication channels from one overlay node to another, and corresponds to layer 4 in the OSI layering. Utilizing native Operating System (OS) functionality, many channel implementations are possible. We currently support one that uses TCP/IP and another that uses UDP/IP.</p><p>2) Neighbor Link: Above the transport layer is the neighbor link layer. It provides secure but unreliable datagram facilities to layers above, including the fragmentation and reassembly of large messages. The first time a higher layer wishes to communicate with another node, it must provide the destination's physical address (e.g., IP address and port number). If a secure channel is desired, a public key for the remote node may also be provided. The neighbor link layer uses this information to establish a connection to the remote node.</p><p>Links are opened on demand by higher levels in Tapestry. To avoid overuse of scarce operating system resources such as file descriptors, the neighbor link layer may periodically close some connections. Closed connections are reopened on demand.</p><p>One important function of this layer is continuous link monitoring and adaptation. It provides fault-detection through soft-state keep-alive messages, plus latency and loss rate estimation. The neighbor link layer notifies higher layers whenever link properties change.</p><p>This layer also optimizes message processing by parsing the message headers and only deserializing the message contents when required. This avoids byte-copying of user data across the operating system and Java virtual machine boundary whenever possible. Finally, node authentication and message authentication codes (MACs) can be integrated into this layer for additional security.</p><p>3) Router: While the neighbor link layer provides basic networking facilities, the router implements functionality unique to Tapestry. Included within this layer are the routing table and local object pointers.</p><p>As discussed in Section III-B, the routing mesh is a prefix-sorted list of neighbors stored in a node's routing table. The router examines the destination GUID of messages passed to it and decides their next hop using this table and local object pointers. Mmessages are then passed back to the neighbor link layer for delivery.</p><p>Figure <ref type="figure" target="#fig_5">7</ref> shows a flow-chart of the object location process. Messages arrive from the neighbor link layer at the left. Some messages trigger extension upcalls as discussed in Section IV-B and immediately invoke upcall handlers. Otherwise, local object pointers are checked for a match against the GUID being located. If a match is found, the message is forwarded to the closest node in the set of matching pointers. Otherwise, the message is forwarded to the next hop toward the root.</p><p>Note that the routing table and object pointer database are continuously modified by the dynamic node management and neighbor link layers. For instance, in response to changing link latencies, the neighbor link layer may reorder the preferences assigned to neighbors occupying the same entry in the routing table. Similarly, the dynamic node management layer may add or remove object pointers after the arrival or departure of neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tapestry Upcall Interface</head><p>While the DOLR API (Section III-A) provides a powerful applications interface, other functionality, such as multicast, requires greater control over the details of routing and object lookup. To accommodate this, Tapestry supports an extensible upcall mechanism. We expect that as overlay infrastructures mature, the need for customization will give way to a set of well-tested and commonly used routing behaviors.</p><p>The interaction between Tapestry and application handlers occurs through three primary calls ( is a generic ID-could be a nodeId, N , or GUID, O ):</p><p>1) DELIVER( , A , Msg): Invoked on incoming messages destined for the local node. This is asynchronous and returns immediately. The application generates further events by invoking ROUTE(). 2) FORWARD( , A , Msg): Invoked on incoming upcall-enabled messages. The application must call ROUTE() in order to forward this message on. 3) ROUTE( , A , Msg, NextHopNode): Invoked by the application handler to forward a message on to NextHopNode. Additional interfaces provide access to the routing table and object pointer database. When an upcall-enabled message arrives, Tapestry sends the message to the application via FORWARD(). The handler is responsible for calling ROUTE() with the final destination. Finally, Tapestry invokes DELIVER() on messages destined for the local node to complete routing.</p><p>This upcall interface provides sufficient functionality to implement (for instance) the Bayeux <ref type="bibr" target="#b4">[5]</ref> multicast system. Messages are marked to trigger upcalls at every hop, so that Tapestry invokes the FORWARD() call for each message. The Bayeux handler then examines a membership list, sorts it into groups, and forwards a copy of the message to each outgoing entry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation</head><p>We follow our discussion of the Tapestry component architecture with a detailed look at the current implementation, choices made, and the rationale behind them. Tapestry is currently implemented in Java, and consists of roughly 57,000 lines of code in 255 source files.</p><p>1) Implementation of a Tapestry Node: Tapestry is implemented as an event-driven system for high throughput and scalability. This paradigm requires an asynchronous I/O layer as well as an efficient model for Fig. <ref type="figure">8</ref>. Tapestry Implementation. Tapestry is implemented in Java as a series of independently-scheduled stages (shown here as bubbles) that interact by passing events to one another.</p><p>internal communication and control between components. We currently leverage the event-driven SEDA <ref type="bibr" target="#b30">[31]</ref> application framework for these requirements. In SEDA, internal components communicate via events and a subscription model. As shown in Figure <ref type="figure">8</ref>, these components are the Core Router, Node Membership, Mesh Repair, Patchwork, and the Network Stage.</p><p>The Network Stage corresponds to a combination of the Neighbor Link layer and portions of the Transport layer from the general architecture. It implements parts of the neighbor communication abstraction that are not provided by the operating system. It is also responsible for buffering and dispatching of messages to higher levels of the system. The Network stage interacts closely with the Patchwork monitoring facility (discussed later) to measure loss rates and latency information for established communication channels.</p><p>The Core Router utilizes the routing and object reference tables to handle application driven messages, including object publish, object location, and routing of messages to destination nodes. The router also interacts with the application layer via application interface and upcalls. The Core Router is in the critical path of all messages entering and exiting the system. We will show in Section V that our implementation is reasonably efficient. However, the Tapestry algorithms are amenable to fast-path optimization to further increase throughput and decrease latency; we discuss this in Section IV-D.</p><p>Supporting the router are two dynamic components: a deterministic Node Membership stage and a soft-state Mesh Repair stage. Both manipulate the routing table and the object reference table. The Node Membership stage is responsible for handling the integration of new nodes into the Tapestry mesh as well as graceful (or voluntary) exit of nodes. This stage is responsible for starting each new node with a correct routing tableone reflecting correctness and network locality.</p><p>In contrast, the Mesh Repair stage is responsible for adapting the Tapestry mesh as the network environment changes. This includes responding to alterations in the quality of network links (including links failures), adapting to catastrophic loss of neighbors, and updating the routing table to account for slow variations in network latency. The repair process also actively redistributes object pointers as network conditions change. The repair process can be viewed as an event-triggered adjustment of state, combined with continuous background restoration of routing and object location information. This provides quick adaptation to most faults and evolutionary changes, while providing eventual recovery from more enigmatic problems.</p><p>Finally, the Patchwork stage uses soft-state beacons to probe outgoing links for reliability and performance, allowing Tapestry to respond to failures and changes in network topology. It also supports asynchronous latency measurements to other nodes. It is tightly integrated with the network, using native transport mechanisms (such as channel acknowledgments) when possible.</p><p>We have implemented both TCP-and UDP-based network layers. By itself, TCP supports both flow and congestion control, behaving fairly in the presence of other flows. Its disadvantages are long connection setup and tear-down times, sub-optimal usage of available bandwidth, and the consumption of file descriptors (a limited resource). In contrast, UDP messages can be sent with low overhead, and may utilize more of the available bandwidth on a network link. UDP alone, however, does not support flow control or congestion control, and can consume an unfair share of bandwidth causing wide-spread congestion if used across the widearea. To correct for this, our UDP layer includes TCPlike congestion control as well as limited retransmission capabilities. We are still exploring the advantages and disadvantages of each protocol; however, the fact that our UDP layer does not consume file descriptors appears to be a significant advantage for deployment on stock operating systems.</p><p>2) Node Virtualization: To enable a wider variety of experiments, we can place multiple Tapestry node instances on each physical machine. To minimize memory and computational overhead while maximizing the number of instances on each physical machine, we run all node instances inside a single Java Virtual Machine (JVM). This technique enables the execution of many simultaneous instances of Tapestry on a single node <ref type="foot" target="#foot_3">5</ref> .</p><p>All virtual nodes on the same physical machine share a single JVM execution thread (i.e., only one virtual node executes at a time). Virtual instances only share code; each instance maintains its own exclusive, nonshared data. A side effect of virtualization is the delay introduced by CPU scheduling between nodes. Dur- ing periods of high CPU load, scheduling delays can significantly impact performance results and artificially increase routing and location latency results. This is exacerbated by unrealistically low network distances between nodes on the same machine. These node instances can exchange messages in less than 10 microseconds, making any overlay network processing overhead and scheduling delay much more expensive in comparison. These factors should be considered while interpreting results, and are discussed further in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Toward a Higher-Performance Implementation</head><p>In Section V we show that our implementation can handle over 7,000 messages per second. However, a commercial-quality implementation could do much better. We close this section with an important observation: despite the advanced functionality provided by the DOLR API, the critical path of message routing is amenable to very high-performance optimization, such as might be available with dedicated routing hardware.</p><p>The critical-path of routing shown in Figure <ref type="figure" target="#fig_5">7</ref> consists of two distinct pieces. The simplest piece-computation of NEXTHOP as in Figure <ref type="figure">3</ref>-is similar to functionality performed by hardware routers: fast table lookup. For a million-node network with base-16 (¬ ½ ), the routing table with a GUID/IP address for each entry would have an expected size 10 kilobytes-much smaller than a CPU's cache. Simple arguments (such as in <ref type="bibr" target="#b0">[1]</ref>) show that most network hops involve a single lookup, whereas the final two hops require at most ¬ ¾ lookups. As a result, it is the second aspect of DOLR routingfast pointer lookup-that presents the greatest challenge to high-throughput routing. Each router that a ROUTE-TOOBJECT request passes through must query its table of pointers. If all pointers fit in memory, a simple hashtable lookup provides Ç´½µ complexity to this lookup.</p><p>However, the number of pointers could be quite large in a global-scale deployment, and furthermore, the fast memory resources of a hardware router are likely to be smaller than state-of-the-art workstations.</p><p>To address this issue, we note that most routing hops receive negative lookup results (only one receives a successful result). We can imagine building a Bloom filter <ref type="bibr" target="#b31">[32]</ref> over the set of pointers. A Bloom filter is a lossy representation of a set that can detect the absence of a member of this set quite quickly. The size of a Bloom filter must be adjusted to avoid too many falsepositives; although we will not go into the details here, a reasonable size for a Bloom filter over È pointers is about ½¼È bits. Assuming that pointers (with all their information) are are 100 bytes, the in-memory footprint of a Bloom filter can be two orders of magnitude smaller than the total size of the pointers.</p><p>Consequently, we propose enhancing the pointer lookup as in Figure <ref type="figure">9</ref>. In addition to a Bloom filter front-end, this figure includes a cache of active pointers that is as large as will fit in the memory of the router. The primary point of this figure is to split up the lookup process into a fast negative check, followed by a fast postive check (for objects which are very active), followed by something slower. Although we say "disk" here, this fallback repository could be memory on a companion processor that is consulted by the hardware router when all else fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>We evaluate our implementation of Tapestry using several platforms. We run micro-benchmarks on a local cluster, measure the large scale performance of a deployed Tapestry on the PlanetLab global testbed, and make use of a local network simulation layer to support controlled, repeatable experiments with up to 1,000 Tapestry instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Methodology</head><p>We begin with a short description of our experimental methodology. All experiments used a Java Tapestry implementation (see Section IV-C) running in IBM's JDK 1.3 with node virtualization (see Section V-C). Our micro-benchmarks are run on local cluster machines of dual Pentium III 1GHz servers (1.5 GByte RAM) and Pentium IV 2.4GHz servers (1 GByte RAM).</p><p>We run wide-area experiments on PlanetLab, a network testbed consisting of roughly 100 machines at institutions in North America, Europe, Asia, and Australia. Machines include 1.26GHz Pentium III Xeon servers (1 GByte RAM) and 1.8GHz Pentium IV towers (2 GByte RAM). Roughly two-thirds of the PlanetLab machines are connected to the high-capacity Internet2 network. The measured distribution of pair-wise ping distances are plotted in Figure <ref type="figure" target="#fig_6">10</ref>  this infrastructure to approximate performance under real deployment conditions. Each node in our PlanetLab tests runs a test-member stage that listens to the network for commands sent by a central test-driver. Note that the results of experiments using node virtualization may be skewed by the processing delays associated with sharing CPUs across node instances on each machine.</p><p>Finally, in instances where we need large-scale, repeatable and controlled experiments, we perform experiments using the Simple OceanStore Simulator (SOSS) <ref type="bibr" target="#b32">[33]</ref>. SOSS is an event-driven network layer that simulates network time with queues driven by a single local clock. It injects artificial network transmission delays based on an input network topology, and allows a large number of Tapestry instances to execute on a single machine while minimizing resource consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance in a Stable Network</head><p>We first examine Tapestry performance under stable or static network conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Micro Benchmarks on Stable Tapestry:</head><p>We use microbenchmarks on a network of two nodes to isolate Tapestry's message processing overhead. The sender establishes a binary network with the receiver, and sends a stream of 10,001 messages for each message size. The receiver measures the latency for each size using the inter-arrival time between the first and last messages.</p><p>First, we eliminate the network delay to measure raw message processing by placing both nodes on different ports on the same machine. To see how performance scales with processor speed, we perform our tests on a P-III 1GHz machine and a P-IV 2.4GHz machine. The latency results in Figure <ref type="figure" target="#fig_7">11</ref> show that for very small messages, there is a dominant, constant processing Max Routing Throughput. Maximum sustainable message traffic throughput as a function of message size.</p><p>time of approximately 0.1 milliseconds for the P-IV and 0.2 for the P-III. For messages larger than 2 KB, the cost of copying data (memory buffer to network layer) dominates, and processing time becomes linear relative to the message size. A raw estimate of the processors (as reported by the bogomips metric under Linux) shows the P-IV to be 2.3 times faster. We see that routing latency changes proportionally with the increase in processor speed, meaning we can fully leverage Moore's Law for faster routing in the future.</p><p>We also measure the corresponding routing throughput. As expected, Figure <ref type="figure">12</ref> shows that throughput is low for small messages where a processing overhead dominates, and quickly increases as messages increase in size. For the average 4KB Tapestry message, the P-IV can process 7,100 messages/second and the P-III processes 3,200 messages/second. The gap between this and the estimate we get from calculating the inverse of the per message routing latency can be attributed to  scheduling and queuing delays from the asychronous I/O layer. We also measure the throughput with two 2.4GHz P-IV's connected via a 100Mbit/s ethernet link. Results show that the maximum bandwidth can be utilized at 4 KB sized messages.</p><p>2) Routing Overhead to Nodes and Objects: Next, we examine the performance of routing to a node and routing to an object's location under stable network conditions, using 400 Tapestry nodes evenly distributed on 62 PlanetLab machines. The performance metric is Relative Delay Penalty (RDP), the ratio of routing using the overlay to the shortest IP network distance. Note that shortest distance values are measured using ICMP ping commands, and therefore incur no data copying or scheduling delays. In both graphs (see Figures <ref type="figure" target="#fig_9">13  and 14</ref>), we plot the 90 Ø percentile value, the median, and the minimum.</p><p>We compute the RDP for node routing by measuring all pairs roundtrip routing latencies between the 400 Tapestry instances, and dividing each by the corresponding ping roundtrip time 6 . In Figure <ref type="figure" target="#fig_8">13</ref>, we see that median values for node to node routing RDP start at 3 and slowly decrease to 1. The use of multiple Tapestry instances per machine means that tests under heavy load will produce scheduling delays between instances, resulting in an inflated RDP for short latency paths. This is exacerbated by virtual nodes on the same machine yielding unrealistically low roundtrip ping times.</p><p>We also measure routing to object RDP as a ratio of one-way Tapestry route to object latency, versus the one-way network latency ( ½ ¾ ¢ ping time). For this experiment, we place 10,000 randomly named objects on a single server, planetlab-1.stanford.edu. All 399 other 6 Roundtrip routing in Tapestry may use asymmetric paths in each direction, as is often the case for IP routing. Tapestry nodes begin in unison to send messages to each of the 10,000 objects by GUID. RDP values are sorted by their ping values, and collected into 5 millisecond bins, with 90 Ø percentile and median values calculated per bin (see Figure <ref type="figure" target="#fig_9">14</ref>).</p><p>3) Object Location Optimization: Although the object location results of Figure <ref type="figure" target="#fig_9">14</ref> are good at large distances, they diverge significantly from the optimal IP latency at short distances. Further, the variance increases greatly at short distances. The reason for both of these results is quite simple: extraneous hops taken while routing at short distances are a greater overall fraction of the ideal latency. High variance indicates client/server combinations that will consistently see non-ideal performance and tends to limit the advantages that clients gain through careful object placement. Fortunately, we can greatly  improve behavior by storing extra object pointers on nodes close to the object. This technique trades extra storage space in the network for better routing.</p><p>We investigate this tradeoff by publishing additional object pointers to k backup nodes of the next hop of the publish path, and the nearest (in terms of network distance) l neighbors of the current hop. We bound the overhead of these simple optimizations by applying them along the first m hops of the path. Figure <ref type="figure" target="#fig_4">15</ref> shows the optimization benefits for 90 Ø percentile local-area routing-to-objects RDP. To explore a larger topology, this figure was generated using the SOSS simulator <ref type="bibr" target="#b32">[33]</ref> with a transit stub topology of 1,092 nodes. We place 25 objects on each of 1,090 Tapestry nodes, and have each node route to 100 random objects for various values of k, l, and m.</p><p>This figure demonstrates that optimizations can sig-nificantly lower the RDP observed by the bulk of all requesters for local-area network distances. For instance, the simple addition of two pointers in the local area (one backup, one nearest, one hop) greatly reduces the observed variance in RDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convergence Under Network Dynamics</head><p>Here, we analyze Tapestry's scalability and stability under dynamic conditions.</p><p>1) Single Node Insertion: We measure the overhead required for a single node to join the Tapestry network, in terms of time required for the network to stabilize (insertion latency), and the control message bandwidth during insertion (control traffic bandwidth).</p><p>Figure <ref type="figure" target="#fig_11">16</ref> shows insertion time as a function of the network size. For each datapoint, we construct a Tapestry network of size AE, and repeatedly insert and delete a single node 20 times. Since each node maintains routing state logarithmically proportional to network size, we expect that latency will scale similarly with network size. The figure confirms this belief, as it shows that latencies scale sublinearly with the size of the network.</p><p>The bandwidth used by control messages is an important factor in Tapestry scalability. For small networks where each node knows most of the network (size AE ¾ ), nodes touched by insertion (and corresponding bandwidth) will likely scale linearly with network size. Figure <ref type="figure" target="#fig_5">17</ref> shows that the total bandwidth for a single node insertion scales logarithmically with the network size. We reduced the GUID base to 4 in order to better highlight the logarithmic trend in network sizes of 16 and above. Control traffic costs include all distance measurements, nearest neighbor calculations, and routing table generation. Finally, while total bandwidth scales as Ç´ÄÓ AEµ, the bandwidth seen by any single link or node is significantly lower.  2) Parallel Node Insertion: Next, we measure the effects of multiple nodes simultaneously entering the Tapestry by examining the convergence time for parallel insertions. Starting with a stable network of size 200 nodes, we repeat each parallel insertion 20 times, and plot the minimum, median and ¼ Ø percentile values versus the ratio of nodes being simultaneously inserted (see Figure <ref type="figure">18</ref>). Note that while the median time to converge scales roughly linearly with the number of simultaneously inserted nodes, the 90% value can fluctuate more significantly for ratios equal to or greater than 10%. Much of this increase can be attributed to effects of node virtualization. When a significant portion of the virtual Tapestry instances are involved in node insertion, scheduling delays between them will compound and result in significant delays in message handling and the resulting node insertion.</p><p>3) Continuous Convergence and Self-Repair: Finally, we wanted to examine large-scale performance under controlled failure conditions. Unlike the other experiments where we measured performance in terms of latency, these tests focused on large-scale behavior under failures. To this end, we performed the experiments on the SOSS simulation framework, which allows up to 1,000 Tapestry instances to be run on a single machine.</p><p>In our tests, we wanted to examine success rates of both routing to nodes and objects, under two modes of network change: drastic changes in network membership and slow constant membership churn. The routing to nodes test measures the success rate of sending requests to random keys in the namespace, which always map to some unique nodes in the network. The routing to objects test sends messages to previously published objects, located at servers which were guaranteed to stay alive in the network. Our performance metrics include both the amount of bandwidth used and the success rate, which is defined by the percentage of requests that correctly reached their destination.</p><p>Figures <ref type="figure" target="#fig_14">19 and 20</ref> demonstrate the ability of Tapestry to recover after massive changes in the overlay network membership. We first kill 20% of the existing network, wait for 15 minutes, and insert new nodes equal to 50% of the existing network. As expected, a small fraction of requests are affected when large portions of the network fail. The results show that as faults are detected, Tapestry recovers, and the success rate quickly returns to 100%. Similarly, a massive join event causes a dip in success rate which returns quickly to 100%. Note that during the large join event, bandwidth consumption spikes as nodes exchange control messages to integrate in the new nodes. The bandwidth then levels off as routing tables are repaired and consistency is restored.</p><p>For churn tests, we drive the node insertion and failure rates by probability distributions. Each test includes two churns of a different level of dynamicity. In the first churn, insertion uses a Poisson distribution with average inter-arrival time of 20 seconds and failure uses an exponential distribution with mean node lifetime of 4 minutes. The second churn increases the dynamic rates of insertion and failure, using 10 seconds and 2 minutes as the parameters respectively.   Failure, join and churn on PlanetLab. Impact of network dynamics on the success rate of route to node requests.</p><p>that Tapestry operations succeed with high probability even under high rates of turnover.</p><p>Finally, we measure the success rate of routing to nodes under different network changes on the PlanetLab testbed. Figure <ref type="figure">23</ref> shows that requests experience very short dips in reliability following events such as massive failure and large joins. Reliability also dips while node membership undergoes constant churn (inter-arrival times of 5 seconds and average life-times are 60 seconds) but recovers afterwards. In order to support more nodes on PlanetLab, we use a UDP networking layer, and run each instance in its own JVM (so they can be killed independently). Note that the additional number of JVMs increases scheduling delays, resulting in request timeouts as the size of the network (and virtualization) increases.</p><p>These experiments show that Tapestry is highly resilient under dynamic conditions, providing a near-optimal success rate for requests under high churn rates, and quickly recovering from massive membership change events in under a minute. These results demonstrate Tapestry's feasibility as a long running service on dynamic networks, such as the wide-area Internet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DEPLOYING APPLICATIONS WITH TAPESTRY</head><p>In previous sections, we explored the implementation and behavior of Tapestry. As shown, Tapestry provides a stable interface under a variety of network conditions. Continuing the main theme of the paper, we now discuss how Tapestry can address the challenges facing largescale applications.</p><p>With the increasing ubiquity of the Internet, application developers have begun to focus on large-scale applications that leverage common resources across the network. Examples include application level multicast, global-scale storage systems, and traffic redirection layers for resiliency or security. These applications share new challenges in the wide-area: users will find it more difficult to locate nearby resources as the network grows in size, and dependence on more distributed components means a smaller mean time between failures (MTBF) for the system. For example, a file sharing user might want to locate and retrieve a close-by replica of a file, while avoiding server or network failures.</p><p>Security is also an important concern. The Sybil attack <ref type="bibr" target="#b33">[34]</ref> is an attack where a user obtains a large number of identities to mount collusion attacks. Tapestry addresses this by using a trusted public-key infrastructure (PKI) for nodeID assignment. To limit damage from subverted nodes, Tapestry nodes can work in pairs by routing messages for each other through neighbors and verifying the path taken. <ref type="bibr" target="#b34">[35]</ref> proposes another generalized mechanism to thwart collusion-based routing redirection attacks. Finally, Tapestry will support the use of message authentication codes (MACs) to maintain integrity of overlay traffic.</p><p>As described in Section III, Tapestry supports efficient routing of messages to named objects or endpoints in the network. It also scales logarithmically with the network size in both per-node routing state and expected number of overlay hops in a path. Additionally, Tapestry provides resilience against server and network failures by allowing messages to route around them on backup paths. Applications can achieve additional resilience by replicating data across multiple servers, and relying on Tapestry to direct client requests to nearby replicas.</p><p>A variety of different applications have been designed, implemented and deployed on the Tapestry infrastructure. OceanStore <ref type="bibr" target="#b3">[4]</ref> is a global-scale, highly available storage utility deployed on the PlanetLab testbed. OceanStore servers use Tapestry to disseminate encoded file blocks efficiently, and clients can quickly locate and retrieve nearby file blocks by their ID, all despite server and network failures. Other applications include Mnemosyne <ref type="bibr" target="#b23">[24]</ref>, a stegnographic file system, Bayeux <ref type="bibr" target="#b4">[5]</ref>, an efficient self-organizing application-level multicast system, and SpamWatch <ref type="bibr" target="#b27">[28]</ref>, a decentralized spam-filtering system utilizing a similarity search engine implemented on Tapestry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We described Tapestry, an overlay routing network for rapid deployment of new distributed applications and services. Tapestry provides efficient and scalable routing of messages directly to nodes and objects in a large, sparse address space. We presented the architecture of Tapestry nodes, highlighting mechanisms for lowoverhead routing and dynamic state repair, and showed how these mechanisms can be enhanced through an extensible API.</p><p>An implementation of Tapestry is running both in simulation and on the global-scale PlanetLab infrastructure. We explored the performance and adaptability of our Tapestry implementation under a variety of realworld conditions. Significantly, Tapestry behaves well even when a large percentage of the network is changing. Routing is efficient: the median RDP or stretch starts around a factor of three for nearby nodes and rapidly approaches one. Further, the median RDP for object location is below a factor of two in the wide-area. Simple optimizations were shown to bring overall median RDP to under a factor of two. Finally, several general-purpose applications have been built on top of Tapestry by the authors and others. Overall, we believe that wide-scale Tapestry deployment could be practical, efficient, and useful to a variety of applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Tapestry: A Resilient Global-scale Overlay for Service Deployment Ben Y. Zhao, Ling Huang, Jeremy Stribling, Sean C. Rhea, Anthony D. Joseph, and John D. Kubiatowicz</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Tapestry object publish example. Two copies of an object (4378) are published to their root node at 4377. Publish messages route to root, depositing a location pointer for the object at each hop encountered along the way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Tapestry route to object example. Several nodes send messages to object 4378 from different points in the network. The messages route towards the root node of 4378. When they intersect the publish path, they follow the location pointer to the nearest copy of the object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Message processing. Object location requests enter from neighbor link layer at the left. Some messages are forwarded to an extensibility layer; for others, the router first looks for object pointers, then forwards the message to the next hop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. PlanetLab ping distribution A histogram representation of pair-wise ping measurements on the PlanetLab global testbed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Message Processing Latency. Processing latency (full turnaround time) per message at a single Tapestry overlay hop, as a function of the message payload size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. RDP of Routing to Nodes. The ratio of Tapestry routing to a node versus the shortest roundtrip IP distance between the sender and receiver.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Fig.<ref type="bibr" target="#b13">14</ref>. RDP of Routing to Objects. The ratio of Tapestry routing to an object versus the shortest one-way IP distance between the client and the object's location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 Fig. 15 .</head><label>115</label><figDesc>Fig. 15. 90 Ø percentile RDP of Routing to Objects with Optimization. Each line represents a set of optimization parameters (k backups, l nearest neighbors, m hops), with cost (additional pointers per object) in brackets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 16 .</head><label>16</label><figDesc>Fig.<ref type="bibr" target="#b15">16</ref>. Node Insertion Latency. Time for single node insertion, from the initial request message to network stabilization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 17 .Fig. 18 .</head><label>1718</label><figDesc>Fig. 17.Node Insertion Bandwidth. Total control traffic bandwidth for single node insertion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 19 .</head><label>19</label><figDesc>Fig.<ref type="bibr" target="#b18">19</ref>. Route to Node under failure and joins. The performance of Tapestry route to node with two massive network membership change events. Starting with 830 nodes, 20% of nodes (166) fail, followed 16 minutes later by a massive join of 50% (333 nodes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 20 .</head><label>20</label><figDesc>Fig.<ref type="bibr" target="#b19">20</ref>. Route to Object under failure and joins. The performance of Tapestry route to objects with two massive network membership change events. Starting with 830 nodes, 20% of nodes (166) fail, followed 16 minutes later by a massive join of 50% (333 nodes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figures 21 and 22 Fig. 21 .</head><label>2221</label><figDesc>Figures 21 and 22  show the impact of constant change on Tapestry performance. In both cases, the success rate of requests under constant churn rarely dipped slightly below 100%. These imperfect measurements occur independent of the parameters given to the churn, showing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Route to Object under churn. The performance of Tapestry route to objects under two periods of churn, starting from 830 nodes. Churn 1 uses random parameters of one node every 20 seconds and average lifetime of 4 minutes. Churn 2 uses 10 seconds and 2 minutes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Fig. 23.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>4 is a knob for tuning the tradeoff between resources used and optimality of the resulting routing table.</figDesc><table><row><cell></cell><cell>Single Tapestry Node</cell><cell></cell></row><row><cell>Decentralized</cell><cell>Application−Level</cell><cell cols="2">Collaborative</cell></row><row><cell>File System</cell><cell>Multicast</cell><cell cols="2">Text Filtering</cell></row><row><cell cols="3">Application Interface / Upcall API</cell></row><row><cell>Management Dynamic Node</cell><cell cols="2">Object Pointer Database Routing Table and</cell><cell>Router</cell></row><row><cell></cell><cell>Neighbor Link Management</cell><cell></cell></row><row><cell></cell><cell>Transport Protocols</cell><cell></cell></row><row><cell cols="4">Fig. 6. Tapestry component architecture. Messages pass up</cell></row><row><cell cols="4">from physical network layers and down from application</cell></row><row><cell cols="4">layers. The Router is a central conduit for communication.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">RDP, or stretch, is the ratio between the distance traveled by a message to an endpoint and the minimal distance from the source to that endpoint.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Current implementations keep two additional backups.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Note that objects can be assigned multiple GUIDs mapped to different root nodes for fault-tolerance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">We have run 20 virtual nodes per machine, but have yet to stress the network virtualization to its limit.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Kris Hildrum, Timothy Roscoe, the reviewers, and the OceanStore group for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tapestry: An infrastructure for fault-tolerant wide-area location and routing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Kubiatowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Joseph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-04">Apr 2001</date>
		</imprint>
		<respStmt>
			<orgName>U. C. Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. CSD-01-1141</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed object location in a dynamic network</title>
		<author>
			<persName><forename type="first">Kirsten</forename><surname>Hildrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Kubiatowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPAA</title>
				<meeting>SPAA</meeting>
		<imprint>
			<date type="published" when="2002-08">Aug 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards a common API for structured P2P overlays</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Dabek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kubiatowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPTPS</title>
				<meeting>IPTPS</meeting>
		<imprint>
			<date type="published" when="2003-02">Feb 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pond: The OceanStore prototype</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Rhea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Geels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakim</forename><surname>Weatherspoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FAST</title>
				<meeting>FAST</meeting>
		<imprint>
			<date type="published" when="2003-04">Apr 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayeux: An architecture for scalable and fault-tolerant wide-area data dissemination</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><forename type="middle">H</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NOSSDAV</title>
				<meeting>NOSSDAV</meeting>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A scalable content-addressable network</title>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Handley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Schenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGCOMM</title>
				<meeting>SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2001-08">Aug 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems</title>
		<author>
			<persName><forename type="first">Antony</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Druschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Middleware</title>
				<meeting>Middleware</meeting>
		<imprint>
			<date type="published" when="2001-11">Nov 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chord: A scalable peer-to-peer lookup service for internet applications</title>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Frans</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGCOMM</title>
				<meeting>SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2001-08">Aug 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kademlia: A peerto-peer information system based on the XOR metric</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Maymounkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mazieres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPTPS</title>
				<meeting>IPTPS</meeting>
		<imprint>
			<date type="published" when="2002-03">Mar 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Viceroy: A scalable and dynamic emulation of the butterfly</title>
		<author>
			<persName><forename type="first">Dahlia</forename><surname>Malkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ratajczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PODC</title>
				<meeting>PODC</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skipnet: A scalable overlay network with practical locality properties</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Saroiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Theimer</surname></persName>
		</author>
		<author>
			<persName><surname>Wolman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USITS</title>
				<meeting>USITS</meeting>
		<imprint>
			<date type="published" when="2003-03">Mar 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experiences deploying a large-scale emergent network</title>
		<author>
			<persName><forename type="first">Bryce</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-O'</forename><surname>Hearn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPTPS</title>
				<meeting>IPTPS</meeting>
		<imprint>
			<date type="published" when="2002-03">Mar 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Freenet: A distributed anonymous information storage and retrieval system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Design Issues in Anonymity and Unobservability</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Locality-aware mechanisms for large-scale networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Future Directions in Distributed Computing</title>
				<meeting>International Workshop on Future Directions in Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accessing nearby copies of replicated objects in a distributed environment</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Plaxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajmohan</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">W</forename><surname>Richa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPAA</title>
				<meeting>SPAA</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Concurrent online tracking of mobile users</title>
		<author>
			<persName><forename type="first">Baruch</forename><surname>Awerbuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGCOMM</title>
				<meeting>SIGCOMM</meeting>
		<imprint>
			<date type="published" when="1991-09">Sep 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A data tracking scheme for general networks</title>
		<author>
			<persName><forename type="first">Rajmohan</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andréa</forename><forename type="middle">W</forename><surname>Richa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berthold</forename><surname>Vöcking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gayathri</forename><surname>Vuppuluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPAA</title>
				<meeting>SPAA</meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Koorde: A simple degreeoptimal hash table</title>
		<author>
			<persName><forename type="first">Frans</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPTPS</title>
				<meeting>IPTPS</meeting>
		<imprint>
			<date type="published" when="2003-02">Feb 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple fault tolerant distributed hash table</title>
		<author>
			<persName><forename type="first">Udi</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPTPS</title>
				<meeting>IPTPS</meeting>
		<imprint>
			<date type="published" when="2003-02">Feb 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Brocade: Landmark routing on overlay networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of (IPTPS)</title>
				<meeting>IPTPS</meeting>
		<imprint>
			<date type="published" when="2002-03">Mar 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Application-level multicast using content-addressable networks</title>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Handley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Schenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NGC</title>
				<meeting>NGC</meeting>
		<imprint>
			<date type="published" when="2001-11">Nov 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SCRIBE: The design of a large-scale event notification infrastructure</title>
		<author>
			<persName><forename type="first">Antony</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Marie</forename><surname>Kermarrec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NGC</title>
				<meeting>NGC</meeting>
		<imprint>
			<date type="published" when="2001-11">Nov 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wide-area cooperative storage with CFS</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Frans</forename><surname>Frank Dabek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SOSP</title>
				<meeting>SOSP</meeting>
		<imprint>
			<date type="published" when="2001-10">Oct 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mnemosyne: Peer-to-peer steganographic storage</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPTPS</title>
				<meeting>IPTPS</meeting>
		<imprint>
			<date type="published" when="2002-03">Mar 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Storage management and caching in PAST, a large-scale, persistent peer-to-peer storage utility</title>
		<author>
			<persName><forename type="first">Antony</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Druschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SOSP</title>
				<meeting>SOSP</meeting>
		<imprint>
			<date type="published" when="2001-10">Oct 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SOS: Secure overlay services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Keromytis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGCOMM</title>
				<meeting>SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2002-08">Aug 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Internet indirection infrastructure</title>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelley</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonesh</forename><surname>Surana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGCOMM</title>
				<meeting>SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2002-08">Aug 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Approximate object location and spam filtering on peer-to-peer systems</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Middleware</title>
				<meeting>Middleware</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">MD2, MD4, MD5, SHA and other hash functions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><surname>Robshaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>RSA Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TR-101</note>
	<note>v. 4.0</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An Architecture for IP Address Allocation with CIDR</title>
		<author>
			<persName><forename type="first">Yakov</forename><surname>Rekhter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Li</surname></persName>
		</author>
		<idno>RFC 1518</idno>
		<ptr target="http://www.isi.edu/in-notes/rfc1518.txt" />
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SEDA: An architecture for well-conditioned, scalable internet services</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SOSP</title>
				<meeting>SOSP</meeting>
		<imprint>
			<date type="published" when="2001-10">Oct 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
				<imprint>
			<date type="published" when="1970-07">July 1970</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Probabilistic location and routing</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">C</forename><surname>Rhea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INFOCOM</title>
				<meeting>INFOCOM</meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Sybil attack</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Douceur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPTPS</title>
				<meeting>IPTPS</meeting>
		<imprint>
			<date type="published" when="2002-03">Mar 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Security for structured peer-to-peer overlay networks</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayalvadi</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antony</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">S</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of OSDI</title>
				<meeting>eeding of OSDI</meeting>
		<imprint>
			<date type="published" when="2002-12">Dec 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
