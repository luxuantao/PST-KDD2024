<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unaligned Supervision for Automatic Music Transcription In-the-Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-28">28 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ben</forename><surname>Maman</surname></persName>
							<email>benmaman@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amit</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
							<email>amberman@tauex.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unaligned Supervision for Automatic Music Transcription In-the-Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-28">28 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.13668v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-instrument Automatic Music Transcription (AMT), or the decoding of a musical recording into semantic musical content, is one of the holy grails of Music Information Retrieval. Current AMT approaches are restricted to piano and (some) guitar recordings, due to difficult data collection. In order to overcome data collection barriers, previous AMT approaches attempt to employ musical scores in the form of a digitized version of the same song or piece. The scores are typically aligned using audio features and strenuous human intervention to generate training labels. We introduce Note EM , a method for simultaneously training a transcriber and aligning the scores to their corresponding performances, in a fully-automated process. Using this unaligned supervision scheme, complemented by pseudolabels and pitch-shift augmentation, our method enables training on in-the-wild recordings with unprecedented accuracy and instrumental variety. Using only synthetic data and unaligned supervision, we report SOTA note-level accuracy of the MAPS dataset, and large favorable margins on cross-dataset evaluations. We also demonstrate robustness and ease of use; we report comparable results when training on a small, easily obtainable, self-collected dataset, and we propose alternative labeling to the MusicNet dataset, which we show to be more accurate. Our project page is available at https://benadar293.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic Music Transcription (AMT) is the task of decoding musical notes from an audio signal, and is one of the most central tasks in Music Information Retrieval (MIR). It benefits musicology and music education, musical search, and could even aid in realistic music synthesis. AMT is challenging due to several reasons, such as notes sharing partial frequencies, polyphony (simultaneous notes played together, analogous to occlusions in computer vision), echo effects, and multi-instrument performances, escalating complexity.</p><p>Unsurprisingly, similarly to fields such as Computer Vision and Natural Language Processing, deep neural networks have contributed to AMT as well. However, as DNNs require massive amounts of training data, progress is limited. The main bottleneck is that manual annotation is severely infeasible, even if done by experts, as it requires highly precise timing. For this reason, for most instruments no datasets of highly accurate annotation have been collected. Collection efforts have concentrated mainly on two instruments. Guitar <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> annotations are done semi-automatically with human verification, in a difficult to scale process. For the piano, unique equipment (the Disklavier) logs key activity during performance, making annotation trivial and data collection simpler. Indeed, the guitar dataset we use for evaluation <ref type="bibr" target="#b0">[1]</ref> (which is practically the only available one) consists of only ?3 hours of recordings, compared to ?140 hours of piano material <ref type="bibr" target="#b2">[3]</ref>. It is therefore not surprising that most AMT literature concentrates on the latter, where supervision and evaluation are clean and readily available <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>As it turns out, even within the case of the piano, supervised detectors struggle to generalize to variances in the instrument or environment, let alone from synthetic to real data. For this reason, for example, the accuracy of SOTA methods degrades in cross-dataset evaluation <ref type="bibr" target="#b5">[6]</ref> (e.g., training on the piano recordings of the MAESTRO dataset <ref type="bibr" target="#b2">[3]</ref>, and testing on those of MAPS <ref type="bibr" target="#b6">[7]</ref>). To mitigate these data intensive requirements, a popular approach seeks to annotate existing recordings through alignment of real performances to their corresponding musical score. In other words, an easily obtainable digitized performance (or MIDI) of a musical piece is aligned to a real recorded performance. After the MIDI is warped to best match the recording, it is used as annotation. This is how, for example, the well known Mu-sicNet dataset was constructed (with the support of human verification) <ref type="bibr" target="#b7">[8]</ref>. While promising, the alignment quality this approach demonstrates is not high enough to be used as labeling for network training. Indeed, the aforementioned dataset is notorious for its labeling inaccuracies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>In this work, we observe that the alignment process could be intertwined with the training of the transcriber, through the Expectation Maximization (EM) framework. We introduce Note EM , a framework that supports unaligned supervision, based on easy-to-obtain musical scores to supervise in-the-wild recordings. The process comprises three steps (see Figure <ref type="figure" target="#fig_1">1</ref>): first, we take an off-the-shelf architecture proposed for transcription, and bootstrap its training on synthetic data. Second, for the E-step, we use the resulting network to predict the transcription of unlabeled recordings. The unaligned score is then warped based on the predictions as likelihood terms, and used as labeling. For the M-step, the transcriber itself is trained on the new generated labels. Depending on the metric, best results were obtained when performing one or two such E-M iterations. In any case, alignment based on network predicted likelihoods is considerably more accurate than alignment based on spectral features <ref type="bibr" target="#b7">[8]</ref> (see <ref type="bibr">Section 4)</ref>. It also enables better handling of inconsistencies between the audio and the score, which are inevitable.</p><p>Using this scheme, we achieve transcription accuracy that outperforms all existing methods on cross-dataset evaluations by a large margin for both the note-and frame-level metrics. For example, we reach 89.7% note-level and 77.0% frame-level F1 score on the MAESTRO test set (without using MAESTRO training data), where Gardner et al. <ref type="bibr" target="#b5">[6]</ref> reach 28% and 60% when not including MAESTRO in the train set. Furthermore, we report note-level accuracy that compares or even surpasses fully supervised piano/guitarspecific transcription methods. This is despite our method being trained on synthetic data and unaligned supervision alone.</p><p>Note EM also enables simple and convenient training on different instruments and genres. To demonstrate this, we train our network on other instruments, such as violin, clarinet, harpsichord, and many others -between 11-22 instruments, depending on the configuration. Furthermore, to evaluate the method's usability, we train it using a small-scale self-collected set of musical performances and corresponding unaligned supervision, and observe similar accuracy. We even generate alternative labeling to the aforementioned Mu-sicNet dataset, which we denote MusicNet EM , and demonstrate it is more accurate. Finally, we also witness satisfying generalization capabilities, through the high quality transcription of unseen instruments and genres such as rock or pop (in which case transcription is pitch only).</p><p>Our contributions are as follows:</p><p>? Note EM -A general framework for training polyphonic (multi-instrument) transcribers using unaligned supervision, allowing the use of in-the-wild recordings for training.</p><p>? A new SOTA note-level F1-score on the MAPS dataset of 87.3% (vs. 86.4% of supervised <ref type="bibr" target="#b2">[3]</ref>), and considerable improvement for cross-dataset evaluations. This is even though training is done using less supervision and less data (?34 vs. ? 140 hours).</p><p>? unprecedented generalization to unseen instruments and musical genres. Results on these genres are unfortunately only qualitative due to lack of ground truth, but they are unmistakably favorable non-the-less.</p><p>? Alternative annotation for MusicNet, denoted MusicNet EM , which is shown to be more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The two common forms of transcription are note-level, where start (onset) / end (offset) note events are detected, and frame-level transcription, where pitches are predicted at every given time, implicitly determining the duration of notes. Other forms of transcription include stream-level, where the performance is segmented into different streams or voices. Segmentation can be according to instrument <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, but can also be between instances of the same instrument.</p><p>While early works reduced the task of transcription to detection of active notes per-frame, later works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9]</ref> show the advantage of breaking down the detection into two components: onsets -beginning of notes, and framespresence of notes. This is based on the observation that the more important and distinguished part of a note event is its onset.</p><p>In multi-instrument transcription, the simpler form ignores instrument classes, assigning a single class for each pitch <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Only a handful of works also address, as we do, the problem of note-with-instrument transcription <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>. As we demonstrate (Section 4), our approach provides cleaner and more attainable labeling, thus clearly surpassing the performance of these works.</p><p>For piano transcription, the main benchmarks are MAPS <ref type="bibr" target="#b6">[7]</ref> and MAESTRO <ref type="bibr" target="#b2">[3]</ref>. The MAPS dataset consists of synthetic and real piano performances, where usually the real performances are used for testing. MAESTRO is a large-scale dataset containing 140 hours of classical western piano performances, with fine and accurate annotation, generated using a Disklavier. The accurate annotation allows outstanding transcription quality <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. However, the main drawback of this dataset is the lack of variety: It contains only piano recordings, which prevents generalization to other musical instruments, and even to varieties in recording environments and pianos. Thus, transcription quality degrades significantly even when testing the model on other piano test sets, such as MAPS.</p><p>For annotation of guitar transcription, Xi et al. <ref type="bibr" target="#b0">[1]</ref> rely on hexaphonic pickup (separated to 6 strings), breaking the problem down into annotation of monophonic music which is simpler than polyphonic. Unfortunately, this approach still requires manual labor, which limits broad data collection. This results in a small dataset -3 hours in total. Hence, this dataset can be used for evaluation but is less effective for training in-the-wild transcribers.</p><p>For other instruments, or multi-instrument transcription, the main existing dataset is MusicNet <ref type="bibr" target="#b7">[8]</ref>, which contains 34 hours of classical western music, performed on various instruments. The annotation was obtained by aligning separatesourced (i.e. by other performers) MIDI performances, rendered into audio, with the real recordings, according to low frequencies. This dataset has the clear advantage of variety, both in instruments and in recording environments, as recordings were gathered from many different sources. However, despite being verified by musicians, the alignment is of poor quality, and timing of notes is not precise, significantly inhibiting learning and performance, as we show. Similar datasets exist -SU <ref type="bibr" target="#b12">[13]</ref>, extended SU <ref type="bibr" target="#b8">[9]</ref>, and URMP <ref type="bibr" target="#b13">[14]</ref> datasets, which suffer from similar limitations and are small. On the task of instrument-sensitive transcription (notewith-instrument), few works have been done, because of the aforementioned limitations of multi-instrument datasets. Wu et al. <ref type="bibr" target="#b8">[9]</ref> train and test on MusicNet for this task, but reported note-level accuracies are very low, below 51% on all instruments except for piano and violin, on which the accuracies are ?69% and ?61% respectively. Gardner et al. <ref type="bibr" target="#b5">[6]</ref> train on a mixture of datasets -MAESTRO, GuitarSet, MusicNet and Slakh2100 (Synthetic). They map the spectrogram into a sequence of semantic midi events, taking an NLP seq2seq approach. This setting is flexible and allows to easily represent multi-instrument transcription. However, the performance on the cross-dataset, or zero-shot task, is low (below 33% on note-level F1), and performance on MusicNet is low, even when training on MusicNet (50% note-level F1 at most).</p><p>It is important to note, that none of the latter works propose any framework or method for weakly-or selfsupervised transcription. Cheuk et al. <ref type="bibr" target="#b10">[11]</ref> train instrumentinsensitive transcription without supervision using a reconstruction loss and Virtual Adversarial Training <ref type="bibr" target="#b14">[15]</ref>, but as we show, our framework performs much better, and also allows instrument-sensitive transcription. To our knowledge, our work is the first to propose such a framework for multiinstrument polyphonic music, including instrument-sensitive transcription.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The key observation of our method is that a weak transcriber can still produce accurate predictions if the global content of the outcome is known up to a warping function. These accurate predictions, in turn, can be used as labels to further improve the transcriber itself. As we demonstrate (see Section 4), this approach is more accurate than that of pseudo-labels (see Section 3.3), due to the unaligned known global content. The weak transcriber thus transforms weak supervision into full supervision and refines itself.</p><p>Our method, described in pseudo-code Algorithm 1, relies on Expectation Maximization (EM) (see Section 3.1), and involves three components (see Figure <ref type="figure" target="#fig_1">1</ref>  </p><formula xml:id="formula_0">Input: audio a 1 , . . . a N , unaligned MIDI m 1 , . . . m N Output: transcriber f ? , labels y 1 , . . . y N pre-train f ? (synthetic) y i , d i = N one, ? i = 1, . . . , N repeat for i = 1 to N do y temp i , d temp i = DT W (f ? (a i ), m i ) if d temp i &lt; d i then y i , d i = y temp i , d temp i end if end for ? = argmin 1 N N i=1 L(f ? , a i , y i ) until 1 N N i=1 d i converges return f ? , y 1 , . . . y N</formula><p>ings with separate-source MIDI (Section B.1.1), including deciding which frames to use and which not to (Section 3.3). (III) transcriber refinement, including pitch-shift equivariance augmentations (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Expectation Maximization (EM)</head><p>Expectation Maximization (EM) is a paradigm for unsupervised or weakly-supervised learning, where labels are unknown, and are assigned according to maximum likelihood. It can be formulated as an optimization problem:</p><formula xml:id="formula_1">? * = argmax ? max y1,...,yn P ? (a 1 , . . . , a n , y 1 , . . . , y n )</formula><p>where a 1 , . . . , a n are data samples, and y 1 , . . . , y n are their unknown labels. The optimization problem can be solved by alternating steps, repeated iteratively until convergence (assuming some pre-training or bootstrapping of ?):</p><formula xml:id="formula_2">y 1 , . . . , y n = argmax y1,...,yn P ? (a 1 , . . . , a n , y 1 , . . . , y n ) (1) ? = argmax ? P ? (a 1 , . . . , a n , y 1 , . . . , y n )<label>(2)</label></formula><p>which are referred to as the E-step (1) and the M-step (2).</p><p>In our scenario, the data samples a 1 , . . . , a n are the unlabelled audio recordings, and y 1 , . . . , y n are the unknown per-frame labels. We assume that the recordings are performances of pre-defined musical pieces m 1 , . . . , m n , such as in classical music, in the form of MIDI from other performers. We perform the E-step by aligning m 1 , . . . , m n with the predicted probabilities over a 1 , . . . , a n using dynamic time warping (DTW) <ref type="bibr" target="#b15">[16]</ref>. We initialize ? by training on synthetic data which is (trivially) supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initial training</head><p>We use synthetic data (see Section 4.1 for details) to train the architecture proposed by Hawthorne et al. <ref type="bibr" target="#b2">[3]</ref>. Of course, our training scheme can also be applied to other architectures, but this one has proven to be effective for supervised piano transcription, reaching 95% note-level and 90% frame-level F1 scores. It has separate detection heads for onsets, offsets, and frames, allowing to perform alignment according to semantic information. As we show (see Supplementary), onset information is the most effective for alignment. This initial network is trained to detect only pitch, without instrument, but it can also be further trained to detect instrument as well (see section 4.2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Labeling</head><p>We label real data using dynamic time warping between the initial network's predicted probabilities and the corresponding MIDIs. This is contrary to <ref type="bibr" target="#b7">[8]</ref>, who compute the dynamic time warping in the frequency space. As can be seen in the Supplementary, MIDI guided alignment yields more accurate labels than simple thresholding. It also provides instrument information.</p><p>The alignment process is depicted in Figure <ref type="figure" target="#fig_1">1</ref> middle, and essentially relies on Dynamic Time Warping. Using DTW, we search for a chronologically monotonic mapping between the unaligned labeling and its corresponding recording, such that for each selected note the probability, as predicted by the transcription model, is maximized.</p><p>We argue that using the network's predicted probabilities as local descriptors for DTW has the following advantages:</p><p>(i) Inconsistencies -For a separate-source MIDI (i.e., originating from a different performer), inconsistencies between the performances in inevitable. This includes repetitions of cadenzas, and more subtle nuances, such as trills, or in-chord order changing. Precise onset timing can be adjusted locally for each note independently according to predicted likelihoods. Failed detection, whether false positive or false negative, can be avoided based on network's probabilities, i.e., pseudo-labels can also be leveraged in addition to the alignment.</p><p>(ii) Label refinement -the labeling process can be repeated during training, thus refining the labels, since the network has improved.</p><p>(iii) DTW computation speed -for DTW descriptors, we project the 88 pitches into a single octave (12 pitches) using maximum activation across octave, hence representation length for DTW is 12 rather than 50 <ref type="bibr" target="#b7">[8]</ref>. This has an impact on computation speed because DTW requires quadratic time. After projection, for an audio recording of ?2:30 minutes, DTW takes ?1 second.</p><p>Pseudo Labels As aforementioned, the alignment can produce false detections, whether positive or negative. To avoid this false detection automatically, and still leverage all data, we label classes with predicted confidence above a threshold T pos as positive, and classes with predicted confidence beneath a threshold T neg as negative, regardless of the alignment. Classes with probability 0.5 &lt; p &lt; T pos which were not marked positive are considered unknown and we do not back-propagate loss through them. We do this to allow detection of onsets undetected by the labeling. We do not do the same for negative detection (i.e., T neg &lt; p &lt; 0.5) as there is already a strong bias against onset detection, as onsets are very sparse (an onset lasts a single frame).</p><p>In our experiments we use thresholds T pos = 0.75 and T neg = 0.01 for all classes -onsets, frames and offsets. We can use a low negative threshold since the MIDI performance already constrains the labels, and activations (whether onset, frame, or offset) are sparse, thus mode collapse is less of an issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Tonality -Pitch Shift Equivariance</head><p>Music transcription has a unique inherent structure, where a pitch shift on the waveform induces a corresponding predetermined translation of the labels. We leverage this structure by enforcing consistency across pitch shift: We create 11 additional pitch shifted copies of our data, with pitch shifts (in semitones):</p><formula xml:id="formula_3">s i = i + ? i , -5 ? i ? 5, ? i ? U(-0.1, 0.1),</formula><p>where U(-0.1, 0.1) is the uniform distribution on the interval [0, 1], as suggested by Thickstun et al. <ref type="bibr" target="#b16">[17]</ref>. We compute the labels only for the original copy, and for each copy shift labels accordingly. This not only augments the data by an order of magnitude, but also implicitly enforces consistency across pitch shift, serving as a regularization, forcing the model to learn tonality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Instrument-Sensitive Transcription (note-withinstrument)</head><p>In this setting, we define a distinct class for each combination of pitch and instrument, i.e., the number of classes C is (number of pitches)?(number of instruments).</p><p>We start with instrument-insensitive training on synthetic data. To adjust the transcriber to the new task of detecting also instrument, we duplicate the weights of the final linear layer of the onset stack I +1 times: once for each instrument, and one copy to maintain instrument-insensitive prediction. This redundancy serves as regularization and improves learning. Thus, at the beginning of instrument-sensitive training, upon detection of a note, the transcriber will detect the note as active on all instruments. During training the transcriber will learn to separate instruments, according to the labels. We apply the same labelling process to this scenario as well -the difference only being more classes. We maintain the low representation length of 12 for DTW computation by maximizing activation both across octave and instrument. To allow the transcriber (which is initially insensitive to instrument) to learn instrument separation, we do not use pseudo-labels in the initial labelling, only from the second labelling iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>For all our experiments, we use an architecture similar to the one proposed by Hawthorne et al. <ref type="bibr" target="#b2">[3]</ref>, but wider, to handle variety in instruments: We use LSTM layers of size 384, convolutional filters of size 64/64/128, and linear layers of size 1024.</p><p>We re-sampled all recordings to 16kHz sample rate, and used the log-mel spectrogram with 229 bins as the input representation. We used hop length 512. We used the mean BCE loss, with an Adam optimizer, with gradient clipped to norm 3, and batch size 8. The initial synthetic model was trained for 350K steps. This took 65 hours on a pair of Nvidia GeForce RTX 2080 Ti GPUs. Further training on real data was done for 90 * |Dataset| steps. In the case of MusicNet EM , this is ? 90 * 310 = 28K iterations. For most experiments, labeling is performed twice: once after sythetic training, and once after 45 * |Dataset| steps. Training on MusicNet EM , for 28K iterations including 2 labelling iterations which require DTW, took 16 hours on a pair of Nvidia GeForce RTX 2080 Ti GPUs.</p><p>In the following, we discuss the data we have used during our evaluations (Section 4.1), we report quantitative results (Section 4.2), and compare to previous work (throughout the evaluations of Section 4.2). The affects of the pitch-shift augmentations can be seen in Tables 1, and 3. Further ablations studies, considering various steps, such as the pseudolabeling, EM iterations, alignment quality, and others can be found in the supplementary material (Section B.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data &amp; Instrument Distribution</head><p>In our experiments, we use three datasets: MIDI Pop Dataset <ref type="bibr" target="#b18">[19]</ref> is a large collection of MIDI files. The data consists of almost 80, 000 songs, from which we used ? 8, 500 randomly selected ones. ? 4500 of the performances, of length 278:09:01 hours, are mp3 compressed, and the rest with lossless flac compression. In total 501:11:30 hours of audio were synthesized from MIDI. We use this dataset to bootstrap the process, by training the system to transcribe the rendered audio according to the original MIDI. Note that for flexibility, we only use pitch labels from this data, without instrument specific labels. We use this dataset only for pre-training.</p><p>MusicNet <ref type="bibr" target="#b7">[8]</ref> comprises recordings of multiple instruments in an unbalanced mix. Labels for this dataset are of lower quality, as they are generated by alignment to musical scores, but in preprocess. Most recordings are of a piano (?15 out of ? 34 hours are piano solo, and ?7 other hours include the piano). We use the recordings of this dataset, and their provided unaligned corresponding musical scores. Instead of the provided labels (or aligned scores), we offer MusicNet EM (in the supplementary material) -alternative labeling generated by our framework -and demonstrate their superiority (Section 4).</p><p>Our Self-Collected dataset demonstrates the simplicity of collecting data for our method. We gather 74 additional hours of recordings, including over 30 hours of orchestra, 5 hours of solo guitar (pieces by Albeniz, Sor, and Tarrega), 11 hours of harpsichord (6 hours solo), and more. We use this data to supplement or replace MusicNet in our experiments.</p><p>Qualitative results in the accompanying video are from a model trained on all three datasets (the MIDI Pop dataset used only for pre-train). Our generated annotation for MusicNet, and our code, together with qualitative examples for various genres and instruments, are available at https://benadar293.github.io.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>For our experiments, we train only on MusicNet EM and/or self collected in-the-wild data, where the model is pre-trained on synthetic data. We do not use MAE-STRO, MAPS, or GuitarSet for training. We evaluate our method on piano, guitar, strings, and wind instruments, Table <ref type="table">1</ref>. Piano transcription results (Precision, Recall, and F1 scores). Gardner et al. <ref type="bibr" target="#b5">[6]</ref> was trained for instrument-sensitive transcription. Notice the drop in performance when excluding MAESTRO from training in the zero-shot task (ZS). The rest in an instrument-insensitive setting. 'Synth' is trained only on synthetic data, and is the result of our initial training step. All following models are fine-tuned from it: 'MusicNet' is fine-tuned on the MusicNet annotation. Notice performance reduction compared to Synth, indicating low quality labeling. 'MusicNetEM ' is fine-tuned on our annotation, with two labeling iterations. 'MusicNetEM 1L' is with a single labeling iteration, and 'self-collected' is using ?30 hours of piano and guitar recordings, with our annotation. As can be seen, our approach surpasses fully supervised note-level accuracy on the MAPS test set, and is comparable for MAESTRO. The presented pitch augmentation's effect is evaluated by adding it to 'MusicNet' training, and removing it from 'MusicNetEM '.  <ref type="bibr" target="#b10">[11]</ref> (excluding piano pieces, which are less reliable compared to MAPS and MAESTRO). Results compare the same training on three different datasets (rows), evaluated on both the given MusicNet annotations, and the ones generated using our unaligned supervision scheme (columns). We also compare against Gardner et al. <ref type="bibr" target="#b5">[6]</ref>, which use a different split. in an instrument-sensitive (i.e., note-with-instruments, see Table <ref type="table">4</ref>), or an instrument-insensitive (see Tables 1 (piano), 2 (MusicNet test), and 3 (GuitarSet)) manner. For the latter, only MusicNet is used, while for the former we also train an additional model using the Self-Collected data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAESTRO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note</head><p>For instrument-insensitive transcription (Tables <ref type="table">1,</ref><ref type="table" target="#tab_3">3</ref>, 2) we report the metrics note (onset detection within 50ms or less) and frame (accuracy in detecting if a note is active/not).</p><p>Table <ref type="table" target="#tab_3">3</ref>. Transcription results on GuitarSet. MusicNetEM is the MusicNet recordings with our annotation. Note-level metrics of Xi et al. <ref type="bibr" target="#b0">[1]</ref> and Wiggins and Kim <ref type="bibr" target="#b1">[2]</ref> are unavailable. Note that our results is for an unseen instrument, since MusicNet recordings contain no guitar performances. Gardner et al. <ref type="bibr" target="#b5">[6]</ref> reach high accuracy on GuitarSet when training on GuitarSet, but perform poorly when generalizing from one dataset to another, in the zero-shot task (ZS), where GuitarSet data is excluded from the train set.</p><p>Note F1 Frame F1 Supervised Xi et al. <ref type="bibr">[</ref> Note-with-offset, for varying thresholds, can be found in the Supplementary material. For instrument-sensitive transcription (Table <ref type="table">4</ref>), we report the note-with-instrument metric, which uses the same 50ms timing rule, but only for notes with the correctly predicted instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Piano</head><p>For piano transcription, we evaluate on the MAPS and MAE-STRO test sets. Results can be seen in Tables 1 (instrumentinsensitive) and 4 (instrment-sensitive). It can be seen that note-level accuracy is near-supervised level, even surpassing supervised-level on MAPS. This is despite training on different datasets and no direct supervision, let alone precise labeling of the exact same instrument. For frame-level accuracy, the task is more challenging, since note endings are typically weak and thus harder to decipher. While this expectedly induces lower F 1 score for the MAESTRO dataset, we also see near-supervised performance on MAPS. Note that the same training procedure done using original Music-Net annotations yields much lower accuracy. This strongly indicates our annotation is more accurate. Similar results are achieved with self-collected data of ?30 hours of piano and guitar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Guitar</head><p>For guitar transcription, we evaluate on the GuitarSet dataset. As can be seen in Table <ref type="table" target="#tab_0">2</ref>, on the note-level, we have conclusive results, that our generated annotation used for training performs significantly better than training on the original annotation (over 20% difference) on both test annotations. This indicates the method can flexibly extend to novel material with cheap labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Instrument-Sensitive Transcription</head><p>Training &amp; evaluation For Quantitative evaluation, we use the 11 instrument classes of MusicNet, with the addition of guitar, together 12 instrument classes. We evaluate on the MusicNet test set, on GuitarSet, and on MAPS. In the instrument-sensitive setting, a note is considered correct only if its predicted instrument is correct (note-with-instrument). We train on MusicNet EM together with the self-collected guitar data (to allow guitar detection since guitar data does not exist in the MusicNet recordings). Similar to Table <ref type="table" target="#tab_0">2</ref>, we report MusicNet test results both according to our annotation, and the original annotation. Results can be seen in Table <ref type="table">4</ref>. Metrics are unsurprisingly lower than Table <ref type="table" target="#tab_0">2</ref>, since instrument detection is required, and confusions can occur e.g. between violin and viola.</p><p>We believe the metrics on the original MusicNet test annotation are far from reflecting real performance. We provide a qualitative comparison to Wu et al. <ref type="bibr" target="#b8">[9]</ref> in the Video, clearly demonstrating the better performance of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we presented a method for multi-instrument transcription, from easily attainable unaligned supervision. We have demonstrated the method's strength for in-the-wild transcription, including cross-dataset evaluation. We have also showed the simplicity of collecting data for our framework, which generates annotation on its own in a fullyautomated process. Our work presents unprecedented transcription quality on a wide variety of instruments and genres. Table <ref type="table">4</ref>. Instrument-sensitive Transcription results (note-with-instrument). We show results on the MusicNet test set, and our proposed labels -MusicNetEM . We also compare to Wu et al. <ref type="bibr" target="#b8">[9]</ref> who evaluate on the MusicNet test set. Notice the improvements for horn, bassoon, and clarinet. For Violin, Cello, and Viola, accuracy according to the original annotation is comparable. However, this is probably due to label quality. See Supplementary material for more detail and a qualitative comparison. We also evaluate this task on MAPS, MAESTRO, and GuitarSet. The most challenging instrument is viola, due to the resemblance to both violin and cello, hampering instrument identification accuracy. Besides extending to human voices, additional effects could be added to the detection, including echo, velocity, etc. Beside the added functionality, this would probably improve the basic detection as well. In addition, adding a musical prior, driving predictions to only make sense musically (in a similar manner to a NLP) would also probably boost performance. Another central direction for future work is generative models. DNN based models that synthesize realistic music, although producing realistic timbre, cannot produce coherent music without conditioning on notes. Generating realistic-sounding music conditioned on notes is ideal for musicians as it enables full control over the content of the produced music. We believe the transcriptions produced using our approach can be used as a conditioning signal for training generative models, by learning the reverse mapping from transcriptions to original audio. Finally, additional E-M iterations on small data or specific performances, even during inference, would also be an interesting avenue for future research, which we hope this work would inspire. Where Z is the target label, and is defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MusicNet</head><formula xml:id="formula_4">= max{3 * Q on , 2 * Q f r , 1 * Q of f } ? [3] Ttarget?88</formula><p>where Q on , Q f r , Q of f are defined as in line 4 in the equation in the previous section. Note that</p><formula xml:id="formula_5">Z s ? [3] 88 1 ? s ? T target</formula><p>and the maximum over s in 6 is performed entry-wise. We back-propagate loss only from non-singular points (unless they were marked positive/negative by the pseudo-labeling which we perform afterwards). This enables us to leverage all data, and prevents the need to discard whole pieces because they contain singular points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Local-Max Adjustment</head><p>Because of the aforementioned slight differences in precise onset timing between the real recording and its corresponding MIDI, the alignment can produce small errors in onset timing. We further refine the labels for each note independently by adjusting each note onset to be a local maximum across time (according to the predicted probabilities), which allows labeling with accurate onset timing. We do the same for note offsets. Still, offsets require further investigation since they are harder to detect. This adjustment of onset timing is not possible when aligning spectral features of polyphonic music, as in Thickstun et al. <ref type="bibr" target="#b7">[8]</ref>. A similar local-max adjustment is performed by Xi et al. <ref type="bibr" target="#b0">[1]</ref> for annotation of guitar performances, according to flux novelty (similar to spectral features) rather than a network's predicted probabilities. This however is only possible because the different guitar strings are separated, therefore the annotation is fact of monophonic music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data &amp; Instrument Distribution</head><p>As we mention in the paper, the MusicNet dataset provides recordings of multiple instruments, however, the dataset is imbalanced. Most recordings are of solo piano (?15 out of ? 34 hours are piano solo, and ?7 other hours include piano). We demonstrate the simplicity of collecting data for our method, by gathering 74 additional hours of recordings. The full distribution of instruments can be seen in Table <ref type="table" target="#tab_5">5</ref>. Transcriptions in the video are by a model trained on all data, both MusicNet and the self-collected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>left): (I) Synthetic data initial training (Section 3.2), (II) aligning real record-Algorithm 1 Transcription EM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. NoteEM system overview. Left: the overall EM approach. Given a synthetic or otherwise supervised dataset, and an unaligned domain, we start by training the transcriber T on the synthetic data. Next, we use the transcriber to label the domain (E-step, middle). We use this as supervision for further training, resulting in a stronger T model (M-step, right). Middle: At the core of our unaligned supervision scheme is the alignment step. Probabilities for each note at each timestep are computed using T . Then, the unaligned labels are warped using DTW to maximize said logits. Right: the warped results are accumulated into the aligned dataset, which can be used to retrain T . During training we use pitch shift augmentation, to improve robustness and performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>String and wind instruments separately. In the table we use the same test split as Cheuk et al.</figDesc><table><row><cell>MAPS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 demonstrates</head><label>3</label><figDesc>generalization to a new instrument, since MusicNet EM does not contain guitar performances. For guitar training data in Table 4 we use the self-collected ? 5 hours of guitar recordings together with MusicNet EM . Results are consistent with the piano experiments, indicating significant improvements.</figDesc><table><row><cell>4.2.3 String &amp; Wind Instruments</cell></row><row><cell>As mentioned, existing annotation of the dataset is notori-</cell></row><row><cell>ously inaccurate, and Tables 1, 3 indicate our annotation</cell></row><row><cell>method is more accurate. To further demonstrate this for</cell></row><row><cell>other instruments, we evaluate on the MusicNet test set us-</cell></row><row><cell>ing both the original annotation and ours (Table 2). Test</cell></row><row><cell>annotation is done as described in Section 3, but without</cell></row><row><cell>the pseudo-labels step. Results can be seen in Tables 2</cell></row><row><cell>(instrument-insensitive) and 4 (instrument-sensitive).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Instrument distribution in self-collected data.</figDesc><table><row><cell>INSTRUMENT</cell><cell>LENGTH (HOURS)</cell></row><row><cell>PIANO</cell><cell>13:27:20</cell></row><row><cell>HARPSICHORD</cell><cell>6:20:37</cell></row><row><cell>HARPSICHORD &amp; STRINGS</cell><cell>3:53:21</cell></row><row><cell>HARPSICHORD &amp; FLUTE</cell><cell>1:02:18</cell></row><row><cell>GUITAR</cell><cell>4:46:21</cell></row><row><cell>LUTE</cell><cell>0:19:21</cell></row><row><cell>VIOLIN</cell><cell>2:11:49</cell></row><row><cell>CELLO</cell><cell>3:24:43</cell></row><row><cell>FLUTE</cell><cell>0:09:15</cell></row><row><cell>ORGAN</cell><cell>2:37:10</cell></row><row><cell>ORCHESTRA</cell><cell>25:56:52</cell></row><row><cell>ORCHESTRA &amp; PIANO</cell><cell>7:54:05</cell></row><row><cell>ORCHESTRA &amp; CHOIR</cell><cell>1:49:47</cell></row><row><cell>ALL</cell><cell>73:52:59</cell></row><row><cell>MUSICNET</cell><cell>33:43:07</cell></row><row><cell>ALL, WITH MUSICNET</cell><cell>107:36:06</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material for "Unaligned Supervision for Automatic Music Transcription in The</head><p>Wild"</p><p>A.1. Aligning real data with MIDI from a different source</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Avoiding Singular Points</head><p>Since we align real recordings with external MIDI (i.e., from a different performer), alignment can fail at points with a contradiction in content between the two performances. This can happen when (i) one sequence has a candenza while the other does not, or (ii) because of subtle nuances, and differences in precise timing of adjacent notes (e.g. in trills, or timing of individual notes within a chord). In such cases, the alignment will collapse a long segment of one sequence into a single frame in the other sequence. The long segment can be e.g. 1 minute in case (i), or e.g. 1 second in case (ii). Such frames that are mapped to long segments of the other sequence are called singular points. This issue is discussed by Thickstun et al. <ref type="bibr" target="#b7">[8]</ref>.</p><p>Their solution is to verify alignment by experts, and to exclude recordings where this occurs. This prevents the process from being fully automatic, and is less desired. Our solution is to only assign labels to non-singular points, and mask the loss from singular points. We still might assign pseudo-labels to singular points, see Subsection 3.3 in the paper. This allows us to avoid failed alignment and also leverage all data, in a fully-automated process.</p><p>In more detail, given an audio performance with frames 1, . . . , T , and an unaligned midi performance of the same piece with frames 1, . . . T target , the initial network predicts for each frame 1 ? t ? T and pitch 1 ? f ? 88 probabilities for onset, frame, and offset. We denote these predictions: P on , P f r , P of f ? [0, 1] T ?88 . Similarly, we denote by</p><p>the onset, frame, and offset activations in the corresponding target midi. As local descriptors X, Y for frames of the audio recording and the midi performance respectively, we use a weighted sum:</p><p>where A &gt;&gt; B &gt;&gt; C, i.e., the alignment is based mainly on the onset information. In our experiments we used values A = 100, B = 0.01, C = 0.001. See Table <ref type="table">6</ref> for the significant difference in accuracy, in both note-and frame-level, when aligning according to onset information, compared to aligning according to frame information. Given a pair of sequences X, Y The DTW algorithm returns an optimal alignment in the form of monotone multi-valued mappings (an index in the source can be mapped to multiple indices in the target):</p><p>and similarly for M -1 . We define the set of singular points S = S 1 ? S 2 where</p><p>S 1 is the set of indices mapped to more than w indices in the target domain (interval of length &gt; w in the target collapses into a single frame in the source), and S 2 is the set of indices mapped to indices in the target domain that cover more than w indices in the source domain (interval of length &gt; w in the source collapses into a single frame in the target). These window sizes control a tradeoff between precision and recall. We used values 3 ? w ? 9, w = 100. Results in Tables 1, 2, 3 in the paper were obtained using w = 3, and Table <ref type="table">4</ref> using w = 7. Larger values of w cause noise as they allow imprecise onset timing, and small values of w (e.g., w = 3) result in transcriptions that are entirely staccato.</p><p>We then assign labels to non-singular points in the following manner: Each non-singular frame t in the source sequence, is mapped to a set of frames M (t) in the target sequence, where |M (t)| ? w. We define the label X(t, p) of frame t at pitch p to be the maximum activation of the pitch p across all frames in M (t). Since we have multiple kinds of activations -onset, frame, offset, and none -we use the hierarchy: onset &gt; frame &gt; offset &gt; none.</p><p>We then assign labels only to non-singular points, in the following manner: The possible labels are: 3 -onset, 2 -frame, 1 offset, and 0 -none. We assign labels X: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Further Experiments &amp; Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Alignment Evaluation</head><p>We measure the accuracy of our labeling process on the Maestro validation dataset, for which precise annotation exists. For 46 out of the 105 pieces in the validation dataset, we were able to find additional unaligned midis (to be used instead of those offered with the dataset). We report the note and frame metrics of the alignment w.r.t the ground truth annotation, when aligment is done over precidtions on the model trained on synthetic data. We compare the results to simple thresholding. We also show the higher accuracy of aligning according to onset information rather than frame information, even for the frame-level accuracy. We show results for other parameters as well. Unless otherwise stated, we use local-max adjustment of onset timing with a window size of 7 frames. We do this in an inclusive manner: after the initial alignment, if a neighbor of an onset has a higher onset prediction, we mark it as an onset instead, and repeat this 3 times. We do this for both left and right neighbors, hence the small decrease in precision. All results can be seen in Table <ref type="table">6</ref>. We also measure the accuracy on these 46 pieces, after training on them with the labels computed by the alignment (not the ground truth labels), and evaluate the accuracy of the network on them using the ground truth labels (last row in Table <ref type="table">6</ref>). Main points to note in the table are: (i) Alignment according to onset information yields much more accurate annotations than aligning according to frame information, even in the frame-level metric. (ii) While annotation according to alignment alone yields slightly better annotation than thresholding with threshold 0.5, the combination of alignment, with thresholding with a higher threshold of 0.75, performs significantly better, with improvement of 4%. (iii) The window size parameters w, w control a tradeoff between precision and recall. (iv) Local max adjustment significantly increases note-level recall, also increases frame-level recall, and gives a slight improvement in note-and frame-level F1 score. (v) The actual performance of the network on the 46 pieces after training on them with the computed annotation, is higher than the annotation's accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Alignment Vs. Pseudo-Labels</head><p>To evaluate the contribution of each of the components -alignment with midi and pseudo-labels, we train two additional models -one where we label the real audio recordings only using pseudo-labels obtained by thresholding with a 0.5 threshold, and one where we label only using alignment. Results can be seen in Table <ref type="table">7</ref>. On the note-level, alignment alone performs better than psuedo-labels on all evaluation sets -MAPS, MAESTRO and GuitarSet. On the frame-level, alignment performs better on the MAPS test set and GuitarSet, while psuedo-labels perform better on the MAESTRO test. Our method which combines both, performs best on all three test sets, on both the note-and frame-level. An ablation study measuring the effect of pitch shift augmentation can be seen in Table <ref type="table">8</ref>: we train an additional model without pitch shift augmentation. We train both models for the same time to compensate for the smaller amount of data when training without pitch shift. For piano transcription, this augmentation gives ?2% of improvment in both note-and frame-level F1 score, increasing both precision and recall. For guitar, the improvement is 7.5% note-level and almost 4% frame-level. We perform the same experiment for the guitar dataset (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.4 Label Update Rate</head><p>To evaluate the effect of repeated updates of annotation (repeating the E-step), we train 3 models with different policies: (i) We compute the labels once only, and train on this annotation. (ii) We update the labels 12 times during training in equal intervals. (iii) We update the labels once, in the middle of training. Single labelling had the highest precision, but lower recall. Results can be seen in Table <ref type="table">9</ref>. Policy (iii) produced the best note-level results, while policy (i) gave the best frame-level results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.5 Velocity</head><p>Dynamics and velocity are key components of any musical performance, and are a central part of the expressivity. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> incorporate velocity into their model, i.e., the model predicts the intensity in which each note was played. The designated equipment they use for data annotation (Disklavier) also provides velocity information. However, in a weakly supervised setting such as ours, velocity becomes a challenge, since there is no direct way to recover the original note velocities from the training data, since the audio recording and the midi performance are from different sources, moreover, velocity is not necessarily well-defined. There might be some correlation between the real performances and the corresponding midi performances, but this is not guarantied. Note that velocity annotation only exists for piano datasets (MAESTRO and MAPS) but neither for GuitarSet nor MusicNet. When evaluating on the MAESTRO an MAPS test sets, The best velocity predictions were made by the initial model trained on synthetic data, as it was trained with full supervision over the velocity. I.e., the real data did not improve velocity prediction -see Table <ref type="table">10</ref>. We tried using velocities from the midi (Table <ref type="table">10</ref> AL), and using velocities predicted by the initial model as labels (Table <ref type="table">10</ref> PL), but this did not improve velocity prediction. Since accurate velocity information cannot be derived from separate-source midi, we believe self-supervision is the main direction for training velocity detection, and we leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.6 GuitarSet Full Metrics</head><p>Results can be seen in Table <ref type="table">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.7 Frame &amp; Offset Detection</head><p>Onsets by definition are the initial appearance, or beginning of notes, and their lengths do not vary between notes -long notes and short notes have an onset with the same length, which is typically defined to be a single frame. Thus, there is a strict correspondence between onsets in a real performance and its corresponding midi, up to a warping function. However, frame activation determines the duration of a note, which lasts several frames and can significantly vary between different notes. The musical score of a piece has instructions for note duration, which provides approximate information that enables learning frame-level transcription in the weakly supervised setting. However, small discrepancies can exist between the real and the midi performances, even after warping, as the exact time of offset can slightly vary between performances. Therefore, although there is improvement in frame-level accuracy gained through weak supervision, it is moderate. These small discrepancies in performance explain the gap between supervised and weakly supervised learning in the frame-level accuracy in Table <ref type="table">1</ref> (79.6-81.4% vs. 84.9%) and between note-level accuracy and frame-level accuracy in the weakly supervised setting Table <ref type="table">11</ref>. Transcription results on GuitarSet. MusicNetEM is the MusicNet recordings with our annotation. Note-level metrics of Xi et al. <ref type="bibr" target="#b0">[1]</ref> and Wiggins and Kim <ref type="bibr" target="#b1">[2]</ref> are unavailable. It is important to note that our results demonstrate generalization to a new instrument since the MusicNet recordings contain no guitar performances. Gardner et al. <ref type="bibr" target="#b5">[6]</ref> reach high accuracy on GuitarSet when training on GuitarSet, but perform poorly in the zero-shot task (ZS), where GuitarSet data is excluded from the train set. To measure the accuracy of our trained model in detecting note offsets, we compute the note-with-offset level metrics for different thresholds. The standard tolerance for offset detection is 50 milliseconds, or %20 of the note length, whichever is greater. Results can be seen in Table <ref type="table">12</ref>. It can be seen that the contribution of unaligned supervision to offset detection is small, and increases as the offset tolerance thresholds are increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note</head><p>We believe frame-level detection, together with offset detection, can be further improved through time-stretching consistency, and this is an important direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.8 MAESTRO with unaligned supervision</head><p>An important question that arises is what is the accuracy on the test set, when some samples from the test domain, or samples similar to the test domain, are seen during training, but without labels, only unaligned supervision. To evaluate this, we Table <ref type="table">13</ref>. Training on MAESTRO with unaligned supervision. For ?7 hours of the MAESTRO validation set, we find unaligned MIDI of the same pieces from unrelated performers, and denote this data MAESTROEM . First row -accuracy when training on MAESTROEM and evaluating on MAESTROEM , but w.r.t. the GT labels. Second row -training on both MAESTROEM and MusicNetEM , and evaluating on the MAESTRO test set. Metrics in row 3 from Hawthorne et al. <ref type="bibr" target="#b2">[3]</ref>. Notice the small gap in note-level metrics between rows 1 (unaligned supervision) and 3 (full supervision), while the training scheme in row 1 is applicable to any instrument. searched for midi performances of pieces in the MAESTRO dataset, unaligned and by other performers. We were able to find such performances for 46 pieces from the MAESTRO validation set, of total time 6:57:22. We denote this by MAESTRO EM . We conduct two experiments: (i) We train on MAESTRO EM alone using our method, without the ground truth labels, and then measure accuracy on MAESTRO EM w.r.t. the ground truth labels. (ii) In another experiment, we add MAESTRO EM to MusicNet EM to measure the effect on the MAESTRO test set. Results can be seen in Table <ref type="table">13</ref>, rows 1-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Guitarset: A dataset for guitar transcription</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<idno>ISMIR 2018</idno>
		<ptr target="http://ismir2018.ircam.fr/doc/pdfs/188Paper" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Society for Music Information Retrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><surname>G?mez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Benetos</surname></persName>
		</editor>
		<meeting>the 19th International Society for Music Information Retrieval Conference<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 23-27, 2018. 2018</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>pdf 1, 2, 7, 11</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guitar tablature estimation with a convolutional neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<idno>ISMIR 2019</idno>
		<ptr target="http://archives.ismir.net/ismir2019/paper/000033.pdf1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Flexer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Peeters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Urbano</surname></persName>
		</editor>
		<editor>
			<persName><surname>Volk</surname></persName>
		</editor>
		<meeting>the 20th International Society for Music Information Retrieval Conference<address><addrLine>Delft, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">November 4-8, 2019. 2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enabling factorized piano music modeling and generation with the MAESTRO dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stasyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>id=r1lYRjC9F7 1, 2, 3, 5, 6, 13</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Onsets and frames: Dual-objective piano transcription</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<idno>ISMIR 2018</idno>
		<ptr target="http://ismir2018.ircam.fr/doc/pdfs/19Paper.pdf1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Society for Music Information Retrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><surname>G?mez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Benetos</surname></persName>
		</editor>
		<meeting>the 19th International Society for Music Information Retrieval Conference<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 23-27, 2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence piano transcription with transformers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Swavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
		<ptr target="https://archives.ismir.net/ismir2021/paper/000030.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Society for Music Information Retrieval Conference, ISMIR 2021</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Lerch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Van Kranenburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Srinivasamurthy</surname></persName>
		</editor>
		<meeting>the 22nd International Society for Music Information Retrieval Conference, ISMIR 2021</meeting>
		<imprint>
			<date type="published" when="2021">November 7-12, 2021. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MT3: multi-task multitrack music transcription</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
		<idno>abs/2111.03017</idno>
		<ptr target="https://arxiv.org/abs/2111.030171,2" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Maps -a piano database for multipitch estimation and automatic transcription of music</title>
		<author>
			<persName><forename type="first">V</forename><surname>Emiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning features of music from scratch</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkFBJv" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017. 9gg 1, 2, 3, 4, 5, 10, 11</date>
		</imprint>
	</monogr>
	<note type="report_type">Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-instrument automatic music transcription with self-attention-based instance segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2020.3030482</idno>
		<ptr target="https://doi.org/10.1109/TASLP.2020.30304822" />
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Polyphonic music transcription with semantic segmentation</title>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2019.8682605</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2019.86826052" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-12">2019. May 12-17, 2019. IEEE, 2019</date>
			<biblScope unit="page" from="166" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reconvat: A semi-supervised automatic music transcription framework for low-resource real-world data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Herremans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1145/3474085.3475405</idno>
		<ptr target="https://doi.org/10.1145/3474085.34754052" />
	</analytic>
	<monogr>
		<title level="m">MM &apos;21: ACM Multimedia Conference, Virtual Event</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Cesar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Prabhakaran</surname></persName>
		</editor>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">October 20 -24, 2021. 2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous separation and transcription of mixtures with multiple polyphonic and percussive instruments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP40776.2020.9054340</idno>
		<ptr target="https://doi.org/10.1109/ICASSP40776.2020.90543402" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">May 4-8, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="771" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Escaping from the abyss of manual annotation: New methodology of building polyphonic datasets for automatic music transcription</title>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46282-0_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46282-0203" />
	</analytic>
	<monogr>
		<title level="m">Music, Mind, and Embodiment -11th International Symposium</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aramaki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Ystad</surname></persName>
		</editor>
		<meeting><address><addrLine>Plymouth, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-16">2015. June 16-19, 2015. 2015</date>
			<biblScope unit="volume">9617</biblScope>
			<biblScope unit="page" from="309" to="321" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers, ser</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Creating a musical performance dataset for multimodal music analysis: Challenges, insights, and applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="12" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2858821</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.28588213" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1979">1979-1993, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic time warping</title>
		<author>
			<persName><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval for Music and Motion</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Invariances and data augmentation for supervised music transcription</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8461686</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2018.84616865" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-04-15">2018. April 15-20, 2018. 2018</date>
			<biblScope unit="page" from="2241" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Highresolution piano transcription with pedals by regressing onset and offset times</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2021.3121991</idno>
		<ptr target="https://doi.org/10.1109/TASLP.2021.31219916" />
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3707" to="3717" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Midi dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ai</surname></persName>
		</author>
		<ptr target="https://composing.ai/dataset5" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
