<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Features Driven Author Name Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Qian</forename><surname>Zhou</surname></persName>
							<email>qzhouo@stu.suda.edu.cn.robertchen.xujj.zhaol@suda.edu.cn.teresa.wang@monash.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiajie</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">1Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple Features Driven Author Name Disambiguation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICWS53863.2021.00071</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Author Name Disambiguation (AND) has received more attention recently, accompanied by the increase of academic publications. To tackle the AND problem, existing studies have proposed many approaches based on different types of information, such as raw document feature (e.g., co-author, title, and keywords), fusion feature (e.g., a hybrid publication embedding based on raw document feature), local structural infonnation (e.g., a publication's neighborhood information on a graph), and global structural information (e.g., the interactive infonnation between a node and others on a graph). However, there has been no work taking all the above-mentioned information into account for the AND problem so far. To fill the gap, we propose a novel framework namely MFAND (Multiple Features Driven Author Name Disambiguation). Specifically, we first employ the raw document and fusion feature to construct six similarity graphs for each author name to be disambiguated. Next, the global and local structural information extracted from these graphs is fed into a novel encoder called R3JG, which integrates and reconstructs the above-mentioned four types of information associated with an author, with the goal of learning the latent information to enhance the generalization ability of the MFAND. Then, the integrated and reconstructed information is fed into a binary classification model for disambiguation. Note that, several pruning strategies are applied before the information extraction to remove noise effectively. Finally, our proposed framework is investigated on two real-world datasets, and the experimental results show that MFAND performs better than all state-of-the-art methods.</p><p>Index Terms-author name disambiguation, multiple features, binary classification, pruning strategy I. <ref type="bibr">INTRODUCTION</ref> We have witnessed the unprecedented growth of academic digital records in the past decade <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>. The latest estimation presents that there are more than 271 million publications, 133 million scholars, and 754 million citations on Aminer <ref type="bibr" target="#b2">[3]</ref>. These numbers are much larger than those on Google Scholar database, Digital Bibliography &amp; Library Project (DBLP), and Microsoft Academic <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b4">[5]</ref>. In these databases, many publications meet the AND problem which is referred to as object distinction <ref type="bibr" target="#b5">[6]</ref> and name identification <ref type="bibr" target="#b6">[7]</ref>. The AND problem can cause inconvenience in data mining communities and academic information retrieval <ref type="bibr" target="#b7">[8]</ref> [9] <ref type="bibr" target="#b9">[10]</ref>. For instance, an online search in DBLP for "Michael Jordan" may retrieve professors coming from UC Berkeley, Germany Helmut Schmidt University, Glasgow Caledonian University, and other academic affiliations simultaneously. This phenomenon has been a common problem for many online digital libraries.</p><p>Making the matter worse, the growth of publications and researchers shows an unprecedented increase in recent years, thus the AND problem has been a pressing task <ref type="bibr" target="#b10">[11]</ref>.</p><p>Addressing the above-mentioned problem means splitting a set of publications into different clusters, and each cluster represents publications published by a unique person <ref type="bibr" target="#b11">[12]</ref> [13] <ref type="bibr" target="#b13">[14]</ref>. Many existing studies have made great contributions in this domain, and they roughly use the following four types of information, such as raw document feature (e.g., co-author, title, and keywords), fusion feature (e.g., a hybrid publication embedding based on raw document feature), local structural information (e.g., a publication's neighborhood information on a graph), and global structural information (e.g., the interactive information between a node and others on a graph). For instance, some methods employ the fusion feature to generate the context embedding and extract global structural information <ref type="bibr" target="#b11">[12]</ref> [13], or disambiguate author names based on this feature directly [15] <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr">[17] [9]</ref> employ raw document feature to construct similarity graphs, and extract local structural information based on these graphs to model high-order connections that can capture publications' neighborhood information.</p><p>The former approaches [15] <ref type="bibr">[16] [12]</ref> [13] developed based on fusion feature and global structural information cannot capture a publication's neighborhood information, due to the lack of local structure information. The latter approaches <ref type="bibr" target="#b16">[17]</ref> [9] that only use raw document feature and local structural information have met the problem: missing of raw document feature, which represents the phenomenon that some publications only have a part of raw document features such as titles and authors, and other features (e.g., venue and keywords) are missing. For example, keywords and abstract are often missing for some older publications in Aminer <ref type="bibr" target="#b2">[3]</ref>. This problem brings a great challenge for measuring similarity between publications with raw document feature, thus the local structural information, which is extracted from neighbors on the similarity graph constructed based on raw document feature, cannot characterize an author precisely. Although the raw document feature, fusion feature, and local structural information are taken into account in <ref type="bibr" target="#b8">[9]</ref>, the global structure information, which can be used to further precisely measure the similarity between two publications from the global perspective, is out of consideration.</p><p>The limitations of the existing methods are that they merely use a part of the information, and have not solved the AND problem effectively due to the problem missing of raw document feature. To overcome the drawbacks of existing studies, we propose a unified framework namely MFAND, where all types of information are taken into account, and a novel generating strategy is proposed to extract local structure information from fusion feature and extract global structural information from raw document feature. Our framework can solve the problem missing of raw document feature to a certain extent and disambiguate author name more precisely. Specifically, extracting local structure information from the fusion feature leads to a better representation of a publication, since the fusion feature denotes the interaction information between raw document features in one publication and quantifies the similarity of pairwise publications robustly <ref type="bibr" target="#b12">[13]</ref>. For example, if a publication does not have the raw document feature co-author, the similarity between the publication and other publications based on co-author can not be quantified. And the publication will be an isolated node in the graph Ceo-author constructed based on co-author. On the contrary, we consider the fusion feature as it contains more information even one raw document feature is missing, and this feature can be used to estimate the similarity of pairwise publications robustly. Additionally, the novel encoder R3JG involved in MFAND can integrate and reconstruct the above-mentioned four types of information to enhance the generalization ability of MFAND by learning the latent information.</p><p>Specifically, the proposed framework MFAND consists of the following three components. (1) Construction of raw document feature graph: we construct five similarity graphs based on five raw document features (i.e., co-author, affiliation, venue, title, and keywords). Each node of a graph denotes a publication. Each edge's weight of the graph is the similarity of pairwise publications calculated based one raw document feature. For instance, we construct a similarity graph Ceo-author, where each edge's weight is the similarity of pairwise publications calculated based the raw document feature co-author. We calculate the similarity between one publication and all other publications written by the author with the same name during the construction, which allows our method to measure the similarity from the global perspective. (2) Construction of fusion feature graph: each node of the graph denotes a publication and each edge is the similarity of pairwise publications calculated based the fusion feature(i.e., generated in the form of context that uses coauthor, affiliation, venue, title, and keywords to concatenate simply). Following this construction, we apply the random walk on this fusion feature graph to generate walks that represent the local structural information of nodes. The local structural information is represented by walks that can be considered as nodes' neighborhood information, and we show the details in Section III-C. (3) Construction of the R3JG encoder and triplets decoder: an encoder namely R3JG is designed to integrate and reconstruct the above information to enhance the generalization ability of MFAND by learning the latent information. The R3JG consists of three JACNN (Joint multiple features information with addition and concatena-tion of graph convolutional network) layers with a residual block. The JACNN, which is designed for reconstructing and integrating all information, is comprised of two modules: Concatenation and Addition, where Concatenation is used to reconstruct the raw document feature and global structural information, and Addition is designed to rebuild fusion feature and local structural information. In the triplets decoder, the nodes' embeddings and the edges' embeddings constitute the triplets and they are fed into Multilayer Perceptron (MLP) to disambiguate whether two publications belong to the unique author. Additionally, we use different pruning strategies on different feature similarity graphs to remove noise effectively and details are introduced in Section III-B and Section III-C.</p><p>Our main contributions are outlined as follows:</p><p>• We propose a unified framework called MFAND, where multiple features information are taken into account, with the goal of addressing the AND problem more effectively. • We design a novel generating strategy that extracts local structure information from fusion feature, and extracts global structural information from raw document feature to solve the problem missing of raw document feature to a certain extent. Our framework consists of a novel module namely R3JG that integrates and reconstructs the multiple features information to enhance the generalization ability of MFAND by learning the latent information, and a triplets decoder that formulates the AND problem as a binary classification task. Moreover, when raw document and fusion feature graphs are constructed, different pruning strategies are designed to remove noise effectively. • We evaluate the framework namely MFAND on two real-world datasets. The experimental results verify the advantages of our method over state-of-the-art methods.</p><p>Our codes are publicly available on github l . This paper is structured as follows. Section II shows the definition of the AND problem and similarity graph. Section III discusses the solution of dealing with the AND problem. Section IV shows the performance evaluation results and the comparative results. Section V presents the related work. In Section VI, conclusions and future work are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>Formally, given an author name a, pa = {Pl,P2, ""PN} is a set of N publications associated with the author name a. And pi = {Xl, X2, ... , X K} E pa is a publication that contains a set of raw document features and Xi is a raw document feature, such as co-author, affiliation, title, keywords, or venue. The fusion feature is generated in the form of context that uses all of these raw document features to concatenate simply. We use 1JJ(pi,Pj) to describe whether the pi and pj have the same identity (corresponding real-world person) <ref type="bibr" target="#b11">[12]</ref> or not.</p><p>If pi and pj have the same identity, we have 1JJ(pi,Pj) = 1. Otherwise, we have 1JJ(pi,Pj) = O. In addition, we omit the superscript a in the following description if there is no </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED FRAMEWORK</head><p>In this section, we introduce the proposed framework MFAND in detail as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We first describe how to estimate the similarity of pairwise publications by IDF (Inverse Document Frequency) based on raw document and fusion feature, which is a fundamental part to construct the similarity graph. Then, we construct five similarity graphs based on five raw document features (i.e., co-author, affiliation, venue, title, and keywords), and each similarity graph is used to extract the global structural information. This can make our method measure the similarity between two publications from the global perspective. Next, modeling the similarity graph based on the fusion feature is represented, and this similarity graph is applied to extract the local structural information and can represent the neighborhood information of one node better. Finally, we show how to perform the encoder and decoder to integrate multiple features information for disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Similarity of Pairwise Publications</head><p>Given a publication set P = {PI,P2, ... ,PN} associated with author name a, zx = {z'1, z2, ... , z';'.} is the set of the words based on feature x in these publications, where each word in zx is not repetitive and x can be a raw document feature (e.g., co-author or affiliation) or the fusion feature. yx = {Y'1, Y2, ... ,Y';'.} is the set of the weight of the words in zx, where yi calculated by IDF is the weight of the word .</p><p>And <ref type="bibr" target="#b11">[12]</ref> has proved that IDF is a good choice to embed the words. yx = {Yr, Y2, ... ,Y';'.} is the normalized representation of yx. Given two publications Pi and Pj, Zi C zx and zx C zx are the word sets of Pi and Pj based on feature</p><p>x J respectively, where each word is not repetitive. The sum of normalized weight of the words in Zi n Zj is utilized to denote the pairwise similarity between two publications, and we formalize it as follows.</p><p>ambiguity, e.g., pa ---+ P, pi ---+ Pi. Given this, we define the problem of author name disambiguation as follows.</p><p>Author Name Disambiguation. Given a publication set p = {PI,P2, .. •,PN} associated with author name a, the task of author name disambiguation is to find a function 8 to partition P into a series of disjoint clusters based on multiple features information, such as raw document feature, fusion feature, local structural, and global structural information, i.e.,</p><p>where C is the set of disjoint clusters, Ck is the cluster that only contains publications of the same identity, i.e., V(Pi,Pj) E Ck x Ck, "'(Pi,Pj) = 1, and different clusters contain publications of different identities, i.e., V(Pi, Pj) E Ck X Ck l , kef k l , "'(Pi,Pj) = o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Similarity Graph</head><p>Given a publication set P = {PI, P2, ... , P N} associated with author name a, we construct a similarity graph</p><formula xml:id="formula_1">C x = (D E x S w)</formula><p>where D and E are the set of nodes and edg~s i ~the' graph respectively. In detail, D i E D is a node and represents the publication Pi. E ij E E is an edge and represents that two publications Pi and Pj have a certain degree of similarity. The feature x is a raw document feature (e.g., co-author or affiliation) or the fusion feature. Sx is the similarity function to quantify the similarity of pairwise publications based on feature x. The weight Wij is the similarity of pairwise publications Pi and Pj calculated by Sx, and if pairwise publications Pi and Pj do not have the feature x or the intersecting set based on the feature x, the weight Wij is zero. The similarity graph is an undirected weighted graph. Therefore, we have</p><formula xml:id="formula_2">E ij = Wij = E ji = Wji.</formula><p>And each node has a self-loop, which is an edge that connects a vertex to itself. We show an example of similarity graph in Fig. <ref type="figure">2</ref>. Note that, we construct six similarity graphs, i.e., Ceo-author, Cajjiliation, C title , Ckeywords, Cvenue and Cjusion based on five common raw document features and the fusion feature.</p><formula xml:id="formula_3">-x Ym,<label>(2)</label></formula><p>Wa l k 0... •• Node 0 Embedding 1 09 0 07 1 08 0 Fig. <ref type="figure">2</ref>. An example of the similarity graph and the graph pruning. Fig. <ref type="figure">3</ref>. An example of embeddings of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MX+MX</head><formula xml:id="formula_4">M X . = IJ JI (4) IJ 2</formula><p>This pruning strategy filters some weak edges by setting a threshold which is the mean value of each row and column.</p><p>Specifically, the elements of MX, which are less than the threshold, can be set to zero. We give an example in Fig. <ref type="figure">2</ref> for the process. The pruned adjacent matrix MX is the edges' embeddings and denotes the global structural information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Construction of Fusion Feature Graph</head><p>Apart from the raw document feature graphs, we also build a fusion feature graph based on the fusion feature and the similarity of pairwise publications. The fusion feature is generated in the form of context that uses the five raw document features (i.e., co-author, affiliation, venue, title, and where iJ' :n is the normalized weight of the word z':n. Note that, Sim(pi,pj)X is set to zero on condition that Zi n Zj = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Construction of Raw Document Feature Graph</head><p>Based on the similarity of pairwise publications, we employ five raw document features (i.e., co-author, affiliation, venue, title, and keywords) to build five similarity graphs. Five similarity graphs are used to extract the global structural information, which makes our method measure the similarity between two publications from the global perspective.</p><p>Raw Document Feature Graph: Let P = {Pl,P2, ... ,PN} be a set of publications written by authors with name a, we employ a N x N adjacent matrix £;fX to denote the raw document feature graph G x = (D, E), where x is one of five raw document features. Mx ij is the similarity of pairwise publications Pi and Pj, and denotes the weight of edge E ij between D i and D j .</p><p>If the similarity of pairwise publications is low, it represents that the edges are weak between pairwise publications and these edges are considered as noise. We adopt a similar pruning strategy like [18] to prune these weak edges for each raw document feature graph.</p><p>Raw Document Feature Graph Pruning <ref type="bibr" target="#b17">[18]</ref>: The N x N adjacent matrix MX denotes the raw document feature graph G x , where x is one of five raw document features. The pruned adjacent matrix MX is defined as:</p><p>(</p><p>if MI &lt; e IJ , else,</p><formula xml:id="formula_6">MI = {O" IJ MI .. IJ,</formula><p>keywords) to concatenate simply. The fusion feature graph aims at extracting the local structural information. In order to represent the local structural information precisely, we must hold information that is more relevant to the current node on intuition. Hence, the following pruning strategy is adopted for the fusion feature graph to filter some weak edges. Fusion Feature Graph Pruning: For a given N x N adjacent matrix MI, the element of MI is the similarity of pairwise publications based on fusion feature. The process of calculation is shown in Section III-A. f is the fusion feature, which uses the five raw document features (i.e., co-author, affiliation, venue, title, and keywords) to concatenate simply. The pruned adjacent matrix M I is defined as:</p><formula xml:id="formula_7">e= a x I: ; : ' =~Mfi W + max(MI i ) a+ 1 '</formula><p>where eis a pruning threshold for each row. a is a parameter that balances the mean and maximum to get the better pruning results. Fusion Feature Graph: For a given pruned N x N adjacent matrix M I, a fusion feature graph GI = (D, E) is built by the triples, i.e., if Mh is nonzero, the triple (D i , D j, E ij ) is built, where D i and D j are the nodes of graph and represent the documents d i and d j , E ij is the weight of edge between D i and D j , and E ij equals to Mh' After constructing the fusion feature graph, the random walk <ref type="bibr" target="#b18">[19]</ref>, which can capture the neighborhood information of nodes, is used to construct the walks on the fusion feature graph. And the nodes' embeddings are generated by refining these walks and represent the local structural information. An example of this process is shown in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embeddings of Nodes:</head><p>We denote the embeddings of nodes D = {Dd as V = {Vi}. For a given fusion feature graph GI = (D, E), we first utilize random walk to generate some walks IIi = {wj} for each node D i , and one walk wj consists of some nodes around the node D i , i.e., wj = {D il , D i2 , }.</p><p>For the node D i and its random one walk wj = {D iI, D i2, }, this walk is redefined by using the weight of the edges between</p><p>,</p><formula xml:id="formula_9">",N-1!VI" if Mij &lt; LW-N 'W, else,<label>(8)</label></formula><formula xml:id="formula_10">Vk+l = O"(Norm(VCon(Vk + V k -1 ) + VAdd(Vk + V k -1 ))). (<label>12</label></formula><formula xml:id="formula_11">)</formula><p>Decoder: We employ the embeddings of pairwise nodes Vi and Vj with corresponding edges' embeddings Conx(Mij) to make a triplet T m . Next, the generated triplets are fed into a MLP classifier to disambiguate. We formalize it as follows.</p><p>(13) ( <ref type="formula">14</ref>)</p><formula xml:id="formula_12">Tm = Con[Vi, Vj, Conx(Mtj)], P r = MLP(T), 1 N-l L = -N 2...= (q(m)log(p~m)) + (1-q(m))log(l-p~m))), m=O<label>(15)</label></formula><p>where pr E P r is the vector of predicted probabilities and q E Q is the binary label vector.</p><p>We optimize the objective with the Adam optimization algorithm <ref type="bibr" target="#b20">[21]</ref> simultaneously. The process for the author name disambiguation is summarized in Algorithm 1.</p><p>where x represents one of five raw document features, Con denotes the concatenation operation, Add denotes the addition operation, Norm is the normalization operation. 0" is an activation function, and RELU is used. V k denotes the nodes' embeddings matrix of the k th layer. W~on and W~dd are the parameter of the Concatenation and Addition operation in the k th layer.</p><p>Since <ref type="bibr" target="#b19">[20]</ref> introduces a deep residual learning framework to address the degradation problem, we introduce a residual block in the encoder to reduce the loss of information during the training process. The nodes' embeddings V and M are fed into an encoder namely R3JG which consists of three JACNN layers with a residual block. We formalize them as below, where T is the concatenated triplets matrix and mE {O, N 2 }. P r denotes the N 2 vector of predicted probabilities and is used to distinguish whether pairwise publications belong to a unique author or not.</p><p>For the convenience of calculation, we define a graph Gc = (D, E) as the ground truth for the given document set P = {Pi} associated with each author name and its annotation result C = {Cl,C2, ... ,cd <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b17">[18]</ref>. From C, we first generate positive edge set E p and negative edge set En. i.e.,</p><formula xml:id="formula_13">E p = {Eij = 1,V(Pi,Pj) E Ck x Ck,Ck E C},E n = {Eij = 0, V(Pi,Pj) E Ck x Ck l , k ef k / },</formula><p>and G c consists of E p U En = E and D i E D that denotes the publication Pi.</p><p>Then, the N x N adjacent matrix of the ground truth graph G c is compressed into N 2 vector Q as labels. Note that, the elements of the adjacent matrix of the ground truth graph G c are Eij E E, where Eij can be 0 or 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training</head><p>The model is trained by illlll1illlzmg the negative loglikelihood loss function. Given an author name 0, the loss function of the model is defined as: Encoder: The core part of the encoder is the Joint multiple features information with addition and concatenation of graph convolutional network (JACNN), which is comprised of two components: Addition and Concatenation. The encoder is shown in Fig. <ref type="figure" target="#fig_2">4</ref> and formalized as follows.</p><p>The JACNN can integrate and reconstruct the above four types of information to enhance the generalization ability of MFAND. Specifically, we first employ the convolutional operation to integrate the four types of information. Then, we use Addition and Concatenation to enhance the generalization ability of MFAND by reconstructing the four types of information. The Concatenation operation is considered to reconstruct the raw document feature and global structural information. The Addition operation is considered to reconstruct the fusion feature and local structural information. Next, we feed the all reconstructed information into the Addition and Normalization layer to integrate. We formalize them as below.</p><formula xml:id="formula_14">VCon(Vk) = Conx(MxVk-l)W~onJ (9)</formula><p>VAdd(V k ) = Norm(Addx(MXVk-l))W~dd' <ref type="bibr" target="#b9">(10)</ref> D i and the nodes in this walk. Then, this redefined walk is employed to embed the node D i , i.e., Vi = {Vk}, Vk = E ik . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Supervised Graph Neural Network model</head><p>In addition to the generation of the above four types of information, another core part of our framework is how to integrate multiple features information for disambiguation. To achieve the goal, we design an encoder that can integrate and reconstruct the above four types of information to enhance the generalization ability of MFAND by learning the latent information. We design the decoder that makes the AND problem be a binary classification between each pairwise publications for disambiguation.</p><p>Normalization: Firstly, row normalization is used to normalize each N x N pruned raw document feature adjacency   <ref type="formula" target="#formula_3">2</ref>); 2 Utilize the pruning strategy according to Eq. ( <ref type="formula" target="#formula_8">3</ref>) and ( <ref type="formula">4</ref>) to construct raw document feature graphs based on the five raw document features, and generate the edges' embeddings set M = {MX};</p><p>3 Utilize the pruning strategy according to Eq. ( <ref type="formula" target="#formula_5">5</ref>), ( <ref type="formula">6</ref>) and <ref type="bibr" target="#b6">(7)</ref> to construct the fusion feature graph; 4 Perform random walk on the fusion feature graph and generate the nodes' embeddings V = {Vi}; 5 while model not converge do 6 Feed M and V into the module R3JG with JACNN to integrate and reconstruct four types of information acorrding to Eq. ( <ref type="formula">9</ref>), ( <ref type="formula">10</ref>), ( <ref type="formula">11</ref>) and (12); 7 Put the integrated and reconstructed information into the triplets decoder to disambiguate, according to Eq. ( <ref type="formula">13</ref>) and ( <ref type="formula">14</ref>); 8 Update all parameters according to Eq. ( <ref type="formula" target="#formula_12">15</ref>) and Adam optimization algorithm; 9 end Output: Use \If (pi, pj) to distinguish whether pi and pj belong to a unique author or not, then achieve 8(pa) ---+ ca.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We use two real-world AND datasets to evaluate our experiments: OAG-WhoisWh0<ref type="foot" target="#foot_1">2</ref> and AMiner-AND<ref type="foot" target="#foot_2">3</ref> . The OAG-WhoisWho dataset contains 608,363 documents and 57,138 distinct authors with 642 equivocal author names <ref type="bibr" target="#b21">[22]</ref>. In the OAG-WhoisWho dataset, we sample 320 author names and split them into 200, 60, 60 for the training, validating, and testing <ref type="bibr" target="#b17">[18]</ref>, which contains 341,457 publications. Each publication has six features, such as co-author, affiliation, venue, year, title, and keywords. The AMiner-AND dataset is released by <ref type="bibr" target="#b11">[12]</ref>, which contains 500 author names as the training set and 100 author names as the testing set, and includes 203, 078 publications. Each publication has seven features, such as coauthor, affiliation, venue, year, title, abstract, and keywords. We analyze the datasets to illustrate the phenomenon: missing of raw document feature. From table II, we find that the publications, which lose the co-author, the affiliation, the venue, and the keywords, are 0.09%, 32.81%, 7.43%, and 41.15% of the OAG-WhoisWho dataset respectively. The publications missing the affiliation, the year, and the keywords are 0.07%, 0.001%, and 24.19% of the AMiner-AND dataset respectively. No publications lose the title and year on the OAG-WhoisWho dataset, and no publications lose the coauthor, title, and venue on the AMiner-AND dataset. As the OAG-WhoisWho dataset does not have the abstract, this feature is not considered.   <ref type="figure" target="#fig_0">-,----,-----.------,,------.----,------r----,------1</ref> 8 Fig. <ref type="figure">5</ref>. The effect of different length of random walks of nodes for the result of the AND problem. Observed from table III, there are 110,566 publications losing one raw document feature, 63,038 losing two raw document features, and 13,866 losing three raw document features, which are 32.38%, 18.46%, and 4.06% of the OAG-WhoisWho dataset respectively. There are 49, 155 publications losing one raw document feature and 57 losing two raw document features, which are 24.2% and 0.03% of the AMiner-AND dataset respectively. For the AMiner-AND dataset and OAG-WhoisWho dataset, the maximum amount of the raw document features of publications, which are missing, is three. Therefore, we show that the statistical results are the number of publications that lose one, two, and three raw document features.</p><p>From these statistics, the problem of the missing of raw document feature is very serious, and the publications that lose the raw document feature are 54.9% and 24.23% of the OAG-WhoisWho dataset and AMiner-AND dataset respectively. Hence, it is necessary to make full use of and integrate multiple features information in the raw publications in a proper way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>To validate the performance of our proposed approach, we compare our method with four state-of-art author name disambiguation methods.</p><p>Beard <ref type="bibr" target="#b15">[16]</ref>: This method trains a distance function to measure the similarity between each pair of papers by a set of well-designed similarity features, including author names, titles, institute names, etc. A semi-supervised HAC algorithm is used to determine clusters.</p><p>AGAND <ref type="bibr" target="#b16">[17]</ref>: This method builds three graphs based on document similarity and co-author relationship, and the triplets, which are employed to improve graph embedding, are sampled from these graphs. The final result is generated by agglomerative hierarchical clustering.</p><p>AMiner <ref type="bibr" target="#b11">[12]</ref>: This method designs a supervised global stage to fine-tune the word2vec result, and designs an unsupervised local stage based on the first stage.</p><p>GANAND <ref type="bibr" target="#b8">[9]</ref>: This method builds a generative adversarial framework. The discriminative module distinguishes whether two papers are from the same author, and the generative module selects possibly homogeneous papers from the heterogeneous information network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Results</head><p>We evaluate our method by using pairwise precision, recall, and F l on each sampled author name and using microprecision, micro-recall, miCro-Fl, macro-precision, macrorecall, and macro-Fl over the whole testing set. Table <ref type="table" target="#tab_1">I</ref> shows the performance of different AND methods on sampled author names of different sizes, and the sampled author names are sampled from the OAG-WhoisWho dataset. Benefiting from the multiple features information and the module of R3JG, our method outperforms other state-of-the-art methods at most samples, and the average of F l of 13 samples outperforms others at least + 12.66%. We verify the advantages of our method over state-of-the-art methods by experimental results on two real-world datasets. and +2.04% in macro-Fl score on the whole AMiner-AND. On OAG-WhoisWho, our method outperforms the baselines in terms of macro-Fl (+26.69% over AGAND, +9.8% over Beard, +7.23% over AMiner, and +22.76% over GANAND relatively). And our method outperforms the baselines in terms of macro-Fl (+12.15% over AGAND, +11.86% over Beard, +7.17% over AMiner, and +2.04% over GANAND relatively) on AMiner-AND. Observed from the experimental results, our framework has a better performance in F l score than all stateof-the-art methods since our framework pays more attention to the balance of precision and recall. And we find that the recall of our model is better than other methods from Table <ref type="table" target="#tab_1">IV</ref>. We consider that the global structural information and the effective use of raw document feature (most methods do not have) can be the reason to achieve better recall. This proof is when we add the Concatenation module that reconstructs raw document feature and global structural information, the recall greatly boosts + 13.93% on OAG-WhoisWho, as shown in Table <ref type="table" target="#tab_7">VI</ref>. From Table <ref type="table" target="#tab_1">IV</ref>, GANAND has the better precision than ours. Beard achieves the better recall than ours. This is because GANAND uses adversarial representation learning, which generates high-quality samples. And Beard proposes phonetic-based blocking strategies to increase recall <ref type="bibr" target="#b15">[16]</ref>. To evaluate the performance of each module, we also present our performance at different stages in Table <ref type="table" target="#tab_7">VI</ref>. It can be seen that Addition, which reconstructs local structural information and fusion feature, achieves a good performance on AMiner-AND, while the result on OAG-WhoisWho is low in terms of macro-Fl' This is because OAG-WhoisWho meets serious missing of raw document feature problem. Compared with other models, we find that this module's result in macro-Fl is better than most models such as AGAND and Beard, which do not use fusion feature. When we add the Concatenation module, which reconstructs the global structural information extracted from five raw document features (i.e., co-author, affiliation, venue, title, and keywords), into the encoder, we can achieve better results which are 75.64 and 74.96 in macro-Fl on two datasets respectively. This is why Aminer, which has fusion feature and global structural information, achieves better results on OAG-WhoisWho and others do not. But Aminer does not consider the local structural information and make full use of the raw document feature, which makes Aminer not solve AND better. We take all the above-mentioned information into consideration and achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameters of Walk</head><p>The node' embedding, which represents the fusion feature and the local structural information, is an important part of our proposed method. We examine how the nodes' neighborhood parameters (number of walks and walk length) affect the performance on OAG-WhoisWho and AMiner-AND. Observed from Fig. <ref type="figure">5</ref>, when the length of walks reaching around 16 and 32, our proposed method achieves better results on OAG-WhoisWho and AMiner-AND respectively. Similarly, when the number of walks is set to 10, our proposed method can achieve better performance, observed from Fig. <ref type="figure" target="#fig_4">6</ref>. This is because the noise is introduced when the length of walks and the number of walks are set to large values. Conversely, when the length of walks and the number of walks are set to small values, nodes will contain less useful information. Both these parameters have a relative impact on the performance of our proposed method, but the performance differences are not large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Selection of Raw Document Feature</head><p>The edge' embedding, which represents the raw document feature and the global structural information, is another important aspect of our method. Each feature has an effect on the AND problem. We take the missing of raw document feature</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>1https://github.com/wx-qzhou/MFAND. Overview of the proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>rode Embedding dg'Emb,ddin g . . . . . . . . . &amp;&lt;' i i ' ._I;Q~EEEE3 ...........'I!IIII.IIIII (a) JACNN Then, a set of the normalized pruned raw document feature adjacency matrices are denoted by M = {MX}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The architecture of JACNN and R3JG module.(b) R3JG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-----.-----,----,---------.---</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The effect of different number of random walks of nodes for the result of the AND problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The effect of selecting different raw document features by adding the features one by one for the result of the AND problem. C, A, T, V, K, and Y represent Co-author, Affiliation, Title, Venue, Keywords, and Year respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>matrix MX, where x is one of five raw document features, i.e., MX MX. =</figDesc><table><row><cell>IJ</cell><cell>IJ ",N-l MX Lm=o tm</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>DETAILED RESULTS ON OAG-WHOISWHO</figDesc><table><row><cell>Name Akio_Kobayashi</cell><cell cols="9">Size -Prec Rec Fl Prec Rec F1 Prec Rec F1 Prec Kec F1 Prec Kec Fl MFAND GANAND AMiner Beard AGAND 229 81.91 79.87 80.88 68.98 53.12 60.02 72.89 67.96 70.34 85.94 73.63 79.31 94.77 90.69 92.68</cell></row><row><cell>Yutaka Shimada</cell><cell>307</cell><cell cols="4">97.96 94.35 96.12 74.32</cell><cell>72.5</cell><cell>73.4</cell><cell>76</cell><cell>62.89 68.83 90.47 85.54 87.94 97.09 40.53 57.19</cell></row><row><cell>Xiaomin~Xie</cell><cell>479</cell><cell cols="3">88.18 90.51 89.33</cell><cell>74.3</cell><cell cols="4">57.84 65.04 87.77 93.58 90.59 84.87 74.58</cell><cell>79.4</cell><cell>90.17 17.48 29.28</cell></row><row><cell>Suqin Liu</cell><cell>518</cell><cell cols="5">92.76 99.68 96.09 84.87 49.59</cell><cell>62.6</cell><cell cols="2">95.19 60.54</cell><cell>74</cell><cell>89.88 55.75 68.82 80.91</cell><cell>17.9</cell><cell>29.32</cell></row><row><cell>Wensheng_Yang</cell><cell>540</cell><cell>96.6</cell><cell cols="7">83.17 89.39 79.27 54.42 64.54 95.99 56.84</cell><cell>71.4</cell><cell>86.88 86.22 86.55 76.16</cell><cell>6.49</cell><cell>11.97</cell></row><row><cell>Junyi Li</cell><cell>611</cell><cell cols="8">99.92 92.85 96.26 73.64 26.15 38.59 98.17 85.65 91.48 85.41</cell><cell>99.2</cell><cell>91.79 94.41 19.57 32.42</cell></row><row><cell>Feng Deng</cell><cell>703</cell><cell cols="2">99.29 99.11</cell><cell>99.2</cell><cell cols="3">74.62 48.86 59.05</cell><cell>99.2</cell><cell>50.48 66.91 97.84 77.29 86.36 82.28 11.54 20.24</cell></row><row><cell>Dan Wu</cell><cell>887</cell><cell cols="5">95.93 71.84 82.15 62.83 22.75</cell><cell>33.4</cell><cell cols="2">71.97 37.61</cell><cell>49.4</cell><cell>90.4</cell><cell>49.71 64.15 81.23 48.88 61.03</cell></row><row><cell>Xiaodong He</cell><cell>895</cell><cell cols="8">92.67 90.74 91.69 74.33 57.62 64.92 94.66 56.82 71.01 86.43 59.41 70.42 81.22 19.22 31.09</cell></row><row><cell>Xiaohua Liu</cell><cell cols="2">1335 98.09</cell><cell>98.4</cell><cell cols="2">98.25 68.02</cell><cell>35</cell><cell>46.22</cell><cell>95.3</cell><cell>91.3</cell><cell>93.26 98.51 69.38 81.42 82.87 13.58 23.34</cell></row><row><cell>Weimin Liu</cell><cell cols="3">1485 76.32 96.17</cell><cell>85.1</cell><cell cols="5">72.31 41.29 52.56 98.33 44.41 61.18 77.43 86.08 81.53 91.36</cell><cell>13.3</cell><cell>23.22</cell></row><row><cell>Min Yang</cell><cell cols="9">2245 68.96 68.06 68.51 66.14 18.76 29.22 56.08 19.91 29.39 76.26 32.14 45.22 74.41 24.12 36.43</cell></row><row><cell>LiYang</cell><cell cols="7">3200 74.78 47.57 58.15 68.28 35.52 46.73</cell><cell>63.1</cell><cell>17.78 27.74 66.94 32.38 43.65 70.81 35.07</cell><cell>46.9</cell></row><row><cell>Avg.</cell><cell>-</cell><cell cols="8">89.49 85.56 87.01 72.45 44.11 54.83 84.97 57.37 68.49 8).94 67.79 74.35 84.44 27.57 41.56</cell></row></table><note>Algorithm 1: The proposed framework: MFAND Data: Publication set pa = {Pl, P2, ... ,PN } Result: 8(pa) ---+ C a = {cl,c2, ... ,ck} 1 Evaluate similarity of pairwise publications according to Eq. (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table IV and Table V demonstrate that our method outperforms other baselines by at least +7.23% in macro-Fl score and + 7.45% in miCro-Fl score on the whole OAG-WhoisWho,Fig. 7. The effect of different raw document feature for the result of the AND problem. C, A, T, V, K, and Y represent Co-author, Affiliation, Title, Venue, Keywords, and Year respectively.</figDesc><table><row><cell>_OAG-W _AMine</cell><cell>....</cell><cell>.... .... ....</cell><cell>....</cell><cell>. . .-</cell><cell>....</cell><cell>.... .......</cell></row></table><note>.-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI CONTRIBUTION</head><label>VI</label><figDesc>OF EACH COMPONENT. Rec I Fl Prec I Rec I Fl Addition 74.49 I 62.46 I 67.95 73.57 I 67.27 I 70.28</figDesc><table><row><cell cols="3">OAG-Whois Who</cell><cell cols="3">AMiner-AND</cell></row><row><cell>Module Prec I Concatenation 74.91</cell><cell>76.39</cell><cell>75.64</cell><cell>81.39</cell><cell>69.47</cell><cell>74.96</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:44:13 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">2https://www.aminer.cn/billboard/whoiswho.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">3https://www.aminer.cnlna-data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by the Major Program of the Natural Science Foundation of Jiangsu Higher Education Institutions of China under Grant Nos. 19KJA610002 and 19KJB520050, and the National Natural Science Foundation of China under Grant No.61902270.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>into consideration to investigate the importance of each raw document feature used in our approach. As the co-author of an author is considered to be a strong discriminative feature by <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, the Fig. <ref type="figure">7</ref> shows that co-author and affiliation are the strong discriminative feature. The title, venue, and keywords have some influence on the AND problem, and the year has the least influence. And, we examine how different raw document features affect the performance of our proposed approach by adding the features one by one and starting with co-author and affiliation. Observed from Fig. <ref type="figure">8</ref>, when several raw document features, which are selected, are coauthor, affiliation, title, venue, and keywords, our proposed approach can get better performance on two datasets. The year has a negative effect on our method, and the result is expected, because co-author, affiliation, title, venue, and keywords are highly related to the author, but the year is not. Generally, the authors with the same name may publish articles in the same year, and the authors, who have a unique identity, may publish articles in different years. Therefore, the year is not useful for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>Author name disambiguation has been studied by various methods. Existing methods only exploit parts of the different types of information that consist of raw document feature, fusion feature, local structural, and global structural information. The state-of-the-art methods for author name disambiguation can be divided into two categories: Context-based and Graphbased.</p><p>Context-based: Context-based methods consider all raw document features to be the context, which is represented by the feature vectors, and leverages the supervised learning method to learn a pairwise function between publications based on these feature vectors. Then, these methods predict whether two publications written by people with the same name belong to unique people for disambiguation. Han et al. <ref type="bibr" target="#b24">[25]</ref> utilize Naive Bayes and SVM to deal with the AND problem. <ref type="bibr">Han et al. [15]</ref> employ TF-IDF and NTF to define similarity functions to calculate the similarity of the documents and employs k-way spectral clustering for disambiguation. Yoshida et al. <ref type="bibr" target="#b25">[26]</ref> propose a two-stage clustering method to learn better feature representation via the first clustering step. Milller et al. <ref type="bibr" target="#b26">[27]</ref> use a deep neural network to solve the AND problem. Kim et al. <ref type="bibr" target="#b12">[13]</ref> introduce a hybrid method that extracts structure-aware features and global features, and gradient boosted trees (GBT) and DNN are introduced to experiment with the result of disambiguation respectively. Jhawar et al. <ref type="bibr" target="#b13">[14]</ref> conduct experiments with two ensemblebased classification algorithms, namely, random forest and gradient boosted decision trees, on a publicly available corpus of manually disambiguated author names from PubMed.</p><p>Graph-based: Graph-based methods consider graphical models by utilizing graph topology and capturing the information of neighbors to deal with the AND problem. Fan et al. <ref type="bibr" target="#b27">[28]</ref> construct a graph by collapsing all the co-authors with identical names to one single node, and the distance between two nodes is measured based on the number of valid paths. Tang et al. <ref type="bibr" target="#b28">[29]</ref> solve the problem by employing Hidden Markov Random Fields (HMRF) to model node features and edge features in a unified probabilistic framework. Zhang et al. <ref type="bibr" target="#b16">[17]</ref> construct three graphs based on document similarity and co-author relationship, and learns graph embedding from three graphs. Zhang et al. <ref type="bibr" target="#b11">[12]</ref> design a supervised global stage to fine-tune the word2vec result and an unsupervised local stage based on the first stage. Wang et al. <ref type="bibr" target="#b8">[9]</ref> propose a generative adversarial framework, and the discriminative module distinguishes whether two papers are from the same author, the generative module selects possibly homogeneous papers directly from the heterogeneous information network.</p><p>Though lots of work has been performed, to the best of our knowledge, there has been no work taking all abovementioned information into account for the AND problem so far, and they ignore the phenomenon: missing of raw document feature. To overcome the drawbacks of existing studies, we propose a unified framework namely MFAND to reduce the effect of missing of raw document feature and deal with the AND problem sufficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a unified framework namely MFAND, where all types of information consisting of raw document feature, fusion feature, local structural, and global structural information are taken into account, with the goal of addressing the AND problem more effectively. We design a novel generating strategy that extracts local structure information from fusion feature and extracts global structural information from raw document feature precisely. The generating strategy can solve the problem missing of raw document feature to a certain extent and disambiguate author name more precisely. The R3JG module, which consists of three JACNN modules that is comprised of Addition and Concatenation, is an encoder used to integrate and reconstruct multiple features information to enhance the generalization ability of MFAND by learning the latent information. Meanwhile, we employ different pruning strategies for different feature graphs to remove noise effectively. Experimental results on two realworld datasets verify the advantages of our method over stateof-the-art methods. In the future, we can apply the proposed framework to recommender systems.</p><p>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:44:13 UTC from IEEE Xplore. Restrictions apply.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An approach for focused crawler to harvest digital academic documents in online digital libraries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duhau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bausal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. In! Retr. Res</title>
		<imprint>
			<biblScope unit="page" from="23" to="47" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilingual author matching across different academic databases: a case study on kaken, dblp, aud pubmed</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chikazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Katsurai</surname></persName>
		</author>
		<author>
			<persName><surname>Ohmukai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="page" from="2311" to="2327" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automatic Disambiguation of Author Names in Bibliographic Repositories</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gon\ialves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model reuse in machine learning for author name disambiguation: An exploration of transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Owen-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEEAccess</title>
		<imprint>
			<biblScope unit="page" from="188378" to="188389" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object distinction: Distinguishing objects with identical names</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1242" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identification and tracing of ambiguous names: Discriminative and generative approaches</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Morie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="419" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A graph combination with edge pruning-based approach for author name disambiguation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pooja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Assoc. In! Sci. Technol</title>
		<imprint>
			<biblScope unit="page" from="69" to="83" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Author name disambiguation on heterogeneous information network with adversarial representation learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A graph-based author name disambiguation method and analysis via information theory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="page">416</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Author name disambiguation based on rule and graph model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="617" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Name disambiguation in aminer: Clustering, maintenance, and human in the loop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1002" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hybrid deep pairwise classification for author name disambiguation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2369" to="2372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Author name disambiguation in pubmed using ensemble-based classification algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jhawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCDL</title>
		<imprint>
			<biblScope unit="page" from="469" to="470" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Name disambiguation in author citations using a k-way spectral clustering method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCDL</title>
		<imprint>
			<biblScope unit="page" from="334" to="343" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ethnicity sensitive author disambiguation using semi-supervised learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Ai-Natsheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Susik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Maguire</surname></persName>
		</author>
		<editor>KESW</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">649</biblScope>
			<biblScope unit="page" from="272" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Name disambiguation in anonymized graphs using network embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A framework for constructing a huge name disambiguation dataset: algorithms, visualization and human collaboration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2007">2007.02086. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepwalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ai-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="I" to="IS" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conna: Addressing name disambiguation on the fly</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno>1O.1109/TKDE.2020.3021256</idno>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An unsupervised heuristic-based hierarchical method for name disambiguation in bibliographic citations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gon\ialves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Assoc. In! Sci. Technol</title>
		<imprint>
			<biblScope unit="page" from="1853" to="1870" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the combination of domain-specific heuristics for author name disambiguation: the nearest cluster method</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gon\ialves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Digit. Libr</title>
		<imprint>
			<biblScope unit="page" from="229" to="246" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two supervised learning approaches for name disambiguation in author citations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCDL</title>
		<imprint>
			<biblScope unit="page" from="296" to="305" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person name disambiguation by bootstrapping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic author name disambiguation with word embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPDL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="300" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On graph-based name disambiguation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM J. Data In! Qual</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A unified probabilistic framework for name disambiguation in digital library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C M</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="page" from="975" to="987" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>SIS Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:44:13 UTC from IEEE Xplore. Restrictions apply</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
