<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SibNet: Sibling Convolutional Encoder for Video Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York</orgName>
								<address>
									<settlement>New York</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhou</forename><surname>Ren</surname></persName>
							<email>zhou.ren@snapchat.com</email>
							<affiliation key="aff1">
								<orgName type="department">Snap Research</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@buffalo.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">State University of New York</orgName>
								<address>
									<settlement>New York</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SibNet: Sibling Convolutional Encoder for Video Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7E7A137C1DAC9C13595A74E1A9D39B1C</idno>
					<idno type="DOI">10.1145/3240508.3240667</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video captioning</term>
					<term>visual-semantic joint embedding</term>
					<term>autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video captioning is a challenging task owing to the complexity of understanding the copious visual information in videos and describing it using natural language. Different from previous work that encodes video information using a single flow, in this work, we introduce a novel Sibling Convolutional Encoder (SibNet) for video captioning, which utilizes a two-branch architecture to collaboratively encode videos. The first content branch encodes the visual content information of the video via autoencoder, and the second semantic branch encodes the semantic information by visual-semantic joint embedding. Then both branches are effectively combined with soft-attention mechanism and finally fed into a RNN decoder to generate captions. With our SibNet explicitly capturing both content and semantic information, the proposed method can better represent the rich information in videos. Extensive experiments on YouTube2Text and MSR-VTT datasets validate that the proposed architecture outperforms existing methods by a large margin across different evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Video captioning aims to understand videos and summarize them concisely using natural language sentences <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b50">51]</ref>. Such an ability, which is a key element of machine intelligence, is crucial to many multimedia applications such as video retrieval, human-computer interaction, and video surveillance. By understanding the semantics of videos, video captioning characterizes visual information into languages and provides concise summarization of video data, which facilitates the effectiveness and efficiency of indexing, searching, and querying large video corpus. We leverage autoencoder and visual-semantic joint embedding to impose fine-grained regularization that pushes content branch to capture visual contents and pushes semantic branch to encode video semantics.</p><p>Motivated by the success of neural machine translation (NMT) <ref type="bibr" target="#b0">[1]</ref> and neural image captioning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b53">54]</ref>, deep neural network models with encoder-decoder pipeline have been applied to video captioning recently and achieved excellent performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51]</ref>. To transfer a sequence of images into a sequence of words, the encoder, e.g., an LSTM or CNN, compresses a video into a vector representation, and then a decoder, e.g., a RNN, helps to further transfer it into a sentence, i.e., a sequence of words following the syntax. Such a sequence-to-sequence learning pipeline has shown promising capacity of "translating" videos into sentences. However, the translation performance often relies on <ref type="bibr" target="#b0">(1)</ref> the encoder that captures the visual information of the video, and also <ref type="bibr" target="#b1">(2)</ref> the decoder that generates the sentence. Although the decoder can help ensure that the generated sentence is meaningful, we argue that the encoder would be even more important, because the information lost in the encoding phase could not be fully recovered by the decoder, thus resulting in imprecise or incomplete translation.</p><p>Existing video encoders choose to represent the whole video by merging conventional CNN features with average pooling or RNN that can capture the video's temporal structures. However, most of them only consider using one single branch to encode the video information. Different from a single image, a video is a sequence of images and conveys much richer information. Therefore, a singlebranch video encoder may not provide sufficient representation of the video contents. To make a more holistic representation of videos, we propose Sibling Convolutional Encoder (SibNet), which is composed of two branches, i.e., the content branch and the semantic branch, to jointly encode the videos. The content branch explicitly learns visual representation of a video with an autoencoder, while the semantic branch encodes a video via visual-semantic joint embedding, which leverages the ground truth captions in the training data to generate semantic-specific representation. Finally, both branches are effectively combined with soft-attention mechanism and fed into the RNN decoder for caption generation. Our SibNet is specifically designed for video captioning task and it brings the following two advantages: (1) the content branch is able to faithfully capture the visual contents of the video. As it is a pure visual encoder, it can better capture the video details to provide more precise video captioning; (2) the semantic branch leverages visual-semantic joint embedding to produce semantic-specific representation. Such representation can capture how important certain frame is semantically, thus providing complementary information of the content branch.</p><p>To jointly train the encoder and decoder, we design a new loss function composed of three loss terms: (1) content loss from the content branch, (2) semantic loss from the semantic branch, and (3) decoder loss from the RNN decoder. In our joint optimization framework, these three loss terms regularize each other to ensure our SibNet generates an effective video representation that works well for video captioning. To model the temporal structures of the video and also make it compatible with our two-branch structure, our SibNet chooses to use temporal convolutional architecture, i.e., temporal convolutional block (TCB), to bring efficient video temporal encoding.</p><p>We evaluate the proposed SibNet on two standard video captioning benchmarks, YouTube2Text (MSVD) <ref type="bibr" target="#b2">[3]</ref> and MSR-VTT <ref type="bibr" target="#b49">[50]</ref>. The comparisons with previous results validate that although we only use a basic RNN decoder (LSTM), our SibNet significantly outperforms previous state-of-the-art methods across different evaluation metrics, thanks to the strong encoder that SibNet provides to capture richer and complementary information of video contents. We also analyze the contribution of each component and other design details of SibNet on the overall performance by performing comprehensive ablation studies, and the results further verify that the proposed two-branch architecture possesses unique merits for video captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deep learning-based encoder-decoder architecture <ref type="bibr">[4, 9, 17, 18, 21, 25-27, 31, 34, 41, 48, 52-54, 56]</ref> has shown its effectiveness in video captioning. Specifically, those methods first adopt an encoder to represent videos into feature vectors and then use a decoder to generate natural language captions.</p><p>Although for the task of image captioning, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> have become a standard to encode image content in most state-of-the-art methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>, for the task of video captioning, how to effectively encode video content is still an open problem. Venugopal et al. <ref type="bibr" target="#b45">[46]</ref> proposed to map a sequence of video features to a fixed-length vector with an average pooling layer. Venugopal et al. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> also presented approaches to mine information from large natural language corpus or utilize temporal information of videos. With the success of attention mechanism in neural video classification <ref type="bibr" target="#b29">[30]</ref>, neural machine translation <ref type="bibr" target="#b0">[1]</ref> and neural image captioning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref>. Yao et al. <ref type="bibr" target="#b52">[53]</ref> introduced it into video captioning. Pan et al. <ref type="bibr" target="#b24">[25]</ref> proposed a video encoder composed of hierarchical RNN. Gan et al. <ref type="bibr" target="#b8">[9]</ref> and Pan et al. <ref type="bibr" target="#b26">[27]</ref> improved existing models by detecting manually defined semantic concepts. Comparing with the aforementioned methods, which mostly encode video information in a single flow, our proposed SibNet learns to explicitly and effectively encode the visual content and semantic information of videos using a two-branch architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head><p>In video captioning, the task is to generate a natural language description, a sentence y for a given input video V . Let X = [x 1 , x 2 , . . . , x n ] denote the ordered feature vectors of n frames in video V , X ∈ R n×d . Given X as input, an encoder generates a compact embedded representation Z, which is either a fixed-length vector or a matrix composed of n vectors, to encode the visual information in X. Then, the decoder decodes the video representation Z into sentence y = [y 1 , y 2 , . . . , y m ] as a sequence of m words.</p><p>Our method follows the encoder-decoder pipeline but proposes a novel Sibling Convolutional Encoder (SibNet) to encode videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sibling Convolutional Encoder (SibNet)</head><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, SibNet is comprised of two branches, namely the content branch and the semantic branch, which are denoted as CNN c and CNN s , respectively. The content branch is designed to encode visual content information, while the semantic branch is designed to encode video semantic information. Unlike existing encoders, whose encoded feature Z is either a fixed-length vector or a matrix, the representation Z in SibNet is composed of two matrices Z c and Z s . As we see in Figure <ref type="figure" target="#fig_1">2</ref>, CNN c and CNN s share common properties: firstly, they have the same input X and the number of their output vectors n are the same. Besides, both branches are formed by a stack of temporal convolutional blocks (TCBs) (will be introduced in Section 3.1.2). Now let us introduce both branches in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Content Branch</head><p>The role of our content branch is to encode visual content information. Autoencoders have been widely used for unsupervised representation learning. It encodes visual content into a vector and then tries to recover the visual signal from such a vector.</p><p>In order to explicitly encode the video content, we propose to implement our content branch with an autoencoder, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. As we see, the autoencoder takes X as input, then passes it through the content branch CNN c to encode the video into representation Z c . Our CNN c is composed of 3 TCBs (will be introduced in Section 3.1.2). After that, CNN a , which is composed of 3 temporal convolutional layers, reconstructs the original visual content from Z c . We use X ′ to denote the reconstructed content generated by CNN a . Here, X ′ ∈ R n×d is of the same size as X.</p><p>Euclidean distance between each element of the original sequence X and the reconstructed sequence X ′ is used to measure the content reconstruction loss L c , which is defined as follows:</p><formula xml:id="formula_0">L c = n i=1 ||x i -x ′ i ||,<label>(1)</label></formula><p>where x i and x ′ i denote the i-th vectors of X and X ′ , respectively; || • || is the notation for L2-norm. This unsupervised reconstruction loss of autoencoder is incorporated to the final training loss, which pushes our content branch to play its role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Temporal Convolutional Block (TCB)</head><p>Now we introduce Temporal Convolutional Block (TCB), which is the basic component in both our content and semantic branches. Videos have temporal structures. Therefore, temporal structure modeling of videos is essential for video representation. Instead of using a RNN, we choose to use a simpler temporal modeling architecture, temporal convolutional block (TCB) as shown in Figure <ref type="figure" target="#fig_3">4</ref>, which works effectively in our experiments.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, our content and semantic branches both consist of a stack of TCBs. Let</p><formula xml:id="formula_1">X k = [x k 1 , x k 2 , . . . , x k n ] denote input of the k-th TCB in either branch, where each x k i is a d k -dimensional vector, X k ∈ R n×d k .</formula><p>Firstly, the k-th TCB passes X k through TCN, a temporal convolutional layer with kernel size 3. The output of TCN is then passed through a ReLU <ref type="bibr" target="#b23">[24]</ref> activation layer. To ease training of our SibNet, we adopt residual connection <ref type="bibr" target="#b11">[12]</ref> by adding the output of ReLU activation layer with the original input of the k-th TCB X k :</p><formula xml:id="formula_2">F(X k ) = ReLU(W k * X k ) ⊕ X k .<label>(2)</label></formula><p>Here F(X k ) ∈ R n×d k represents the output of the k-th TCB, W k ∈ R 3×d k denotes learnable parameters of TCN, * and ⊕ represent convolutional operator and element-wise addition, respectively. F(X k ) then becomes the input of the (k + 1)-th TCB.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, our content branch is composed of a stack of 3 TCBs , while the semantic branch is composed of a stack of 6 TCBs. In Section 4.3, we will investigate the impact of the TCB numbers in both branches and thus explain why we choose such numbers as above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Semantic Branch</head><p>The task of our semantic branch is to learn a representation of X that encodes high-level semantics. Inspired by the success of visual-semantic joint embedding in image retrieval <ref type="bibr" target="#b32">[33]</ref> and image classification <ref type="bibr" target="#b31">[32]</ref>, we propose to implement our semantic branch via visual-semantic embedding.</p><p>As shown in Figure <ref type="figure" target="#fig_4">5</ref>, our visual-sem.antic joint embedding model is composed of two sub-modules, video embedding module and caption embedding module, which map videos and captions into a common semantic space. In such space, a video and its corresponding caption should be embedded closely, thus the distance in this space is empowered with semantic meaning. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, the video embedding module first maps the input X to the matrix Z s using our semantic branch CNN s . Then a self-attentive network (SAN) <ref type="bibr" target="#b19">[20]</ref> is employed to map Z s into a video embedding vector v e . Instead of averaging all the n vectors in Z s as v e , we use SAN in video embedding module, because it has been proven that the embedding produced by SAN is better at capturing more meaningful information contained in certain frames. Thus, given a sequence Z s = [z s 1 , z s 2 , . . . , z s n ], SAN embeds it into a vector v e by merging all z s i according to their relative importance to the final embedding. Similarly, in the caption embedding module, in order to embed the sentence y = [y 1 , y 2 , . . . , y m ] into a caption embedding vector c e , it first constructs word vectors w i ∈ R d w by <ref type="bibr" target="#b41">[42]</ref>, and then utilizes another SAN <ref type="bibr" target="#b19">[20]</ref> to embed it into vector c e .</p><p>In order to make CNN s effectively encode semantic information, we follow <ref type="bibr" target="#b33">[34]</ref> to utilize bi-directional ranking loss as our semantic training loss. Specifically, we define semantic loss L s as follows:</p><formula xml:id="formula_3">L s = v e c - e max(0, m -v e • c e + v e • c - e )) + c e v - e max(0, m -c e • v e + c e • v - e )),<label>(3)</label></formula><p>where • designates dot product operation. The margin m is set to be 0.1 by cross-validation. Given a video V with embedding vector v e , c e denotes embedding of its ground truth caption, c - e denotes embedding of a negative caption that describes video other than v e ; and vice-versa with v - e . This semantic loss, which pushes X k our semantic branch to play its role, is incorporated into our final training loss.</p><formula xml:id="formula_4">TCN 3 x temporal conv ReLU X k identity ℱ X k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder</head><p>After we obtain the encoded representation Z, i.e., {Z c , Z s }, we follow previous work to use a RNN to decode it into a sentence y. More specifically, given Z c and Z s , the decoder predicts joint probability p(y) of caption y by sequentially predicting the probability of each word y i in y. It can be seen from Figure <ref type="figure" target="#fig_1">2</ref> that our decoder is auto-regressive, indicating that it takes the output at all previous time steps as additional inputs. We maximize the probability of generating ground truth captions by minimizing cross-entropy loss. Our decoder loss L d is defined as follows:</p><formula xml:id="formula_5">L d = -log(p(y|Z c , Z s )).</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Soft-attention Mechanism</head><p>How to effectively combine Z c and Z s is the key problem in decoding process. We utilize a soft-attention mechanism. Originally proposed in <ref type="bibr" target="#b0">[1]</ref>, variants of soft attention have been successfully applied to machine translation <ref type="bibr" target="#b0">[1]</ref>, image captioning <ref type="bibr" target="#b51">[52]</ref> and video captioning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b52">53]</ref>, etc. Different from standard soft-attention mechanism <ref type="bibr" target="#b0">[1]</ref> which returns a fixed-length vector encoding information of one single matrix, our soft-attention mechanism merges visual information of two matrices Z c and Z s in a fixed-length vector. At the i-th decoding time step (when generating the i-th word), our soft-attention mechanism computes the input vector u i of the RNN decoder as follows:</p><formula xml:id="formula_6">u i = n j=1 softmax j (s i ) • z c j j ∈ [1, n],<label>(5)</label></formula><p>where softmax j (•) denotes the j-th value of the softmax result vector, z c j is the j-th element of Z c that encodes video content information, and</p><formula xml:id="formula_7">s i = [s i,1 , s i,2 , . . . , s i,n</formula><p>] is defined as follows:</p><formula xml:id="formula_8">s i,k = W s T tanh(W h h i + W z z s k ) k ∈ [1, n].<label>(6)</label></formula><p>Here, s i,k is a real value; W s , W h and W z are learnable weight matrices; h i , a fixed-length vector, denotes the hidden state of the RNN decoder at the i-th time step; z s k is the k-th element of Z s , which encodes video semantic information.</p><p>As shown in Equation 5 and 6, the soft-attention mechanism utilizes semantic information in Z s to determine a weighting value s i , which then effectively combine the visual content representation Z c to generate a input vector u i for RNN decoder. Such soft-attention mechanism is able to ensure our decoder pay more "attention" to the visual content of certain frames if they contain important semantic information. As we can see, by using the proposed soft-attention mechanism, the content and semantic branch in SibNet are effectively combined in a complementary fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>We jointly train all the components of our model, the content branch, the semantic branch, and the RNN decoder in an end-to-end manner. As introduced before, autoencoder and visual-semantic embedding are utilized to impose more fine-grained supervision for both branches of SibNet. Thus, we define the final training loss function by adding three different losses together:</p><formula xml:id="formula_9">L = L d + αL c + βL s ,<label>(7)</label></formula><p>where L d , L c and L s denote the decoder loss, content loss and semantic loss, as defined in Equation <ref type="formula">4</ref>, Equation 1 and Equation <ref type="formula" target="#formula_3">3</ref>, respectively; α and β are two scalars that control the influence of content loss and semantic loss during training. We set α and β to be 0.4 and 1 by cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We test the proposed SibNet on two video captioning benchmarks, YouTube2Text (MSVD) <ref type="bibr" target="#b2">[3]</ref> and MSR-VTT <ref type="bibr" target="#b49">[50]</ref>. For fair comparison, all the reported results are obtained using Microsoft COCO caption evaluation tool <ref type="bibr" target="#b5">[6]</ref>. We utilize Bleu <ref type="bibr" target="#b27">[28]</ref>, METEOR <ref type="bibr" target="#b6">[7]</ref>, ROUGE <ref type="bibr" target="#b18">[19]</ref> 𝐜 𝑒 𝐯 𝑒 Semantic loss 𝐿 s = 𝐿 𝑠 ({𝐯 𝑒 , 𝐜 𝑒 }) and CIDEr <ref type="bibr" target="#b42">[43]</ref> as our evaluation metrics, which are commonly used for performance evaluation of video captioning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets: YouTube2Text is composed of 1970 YouTube videos and 78, 800 captions (40 captions per video, on average) annotated by Amazon Mechanical Turk (AMT) annotators. For fair comparison, we adopt the same evaluation scheme proposed in <ref type="bibr" target="#b44">[45]</ref>, which used 1200 videos for training, 100 videos for validation and 670 videos for testing. MSR-VTT is a large-scale video captioning dataset, which is comprised of 10, 000 videos and 200, 000 captions (20 unique captions per video). We adopt the standard dataset splits proposed in <ref type="bibr" target="#b49">[50]</ref>, which used 6513 videos for training, 497 videos for validation and 2990 videos for testing.</p><p>For both YouTube2Text and MSR-VTT datasets, we uniformly sample the videos with a sampling rate of 3 frames per second. We then extract visual features using GoogLeNet <ref type="bibr" target="#b37">[38]</ref> for YouTube2Text dataset and Inception <ref type="bibr" target="#b38">[39]</ref> for MSR-VTT dataset. Both GoogLeNet and Inception are trained by Wang et al. <ref type="bibr" target="#b48">[49]</ref>. It is worth noting that most state-of-the-art methods <ref type="bibr">[5, 9, 25-27, 35, 51, 53, 55]</ref> take a combination of multiple complementary features, including framelevel CNN features (ResNet <ref type="bibr" target="#b11">[12]</ref>, Inception <ref type="bibr" target="#b38">[39]</ref>, GoogleNet <ref type="bibr" target="#b37">[38]</ref>), clip-level CNN features (C3D <ref type="bibr" target="#b39">[40]</ref>) and audio features (MFCC <ref type="bibr" target="#b21">[22]</ref>), as input to their encoders. We do not adopt feature combination in our experiments.</p><p>Network Architecture: For the content branch and the semantic branch, we set the output dimension of the TCN, a temporal convolutional layer, in each TCB to be 512. We adopt 1-layer LSTM with 1024-dimensional hidden state as our RNN decoder. <ref type="foot" target="#foot_1">1</ref> Many variants of RNN have been proposed in literature, e.g., GRU. Some stateof-the-art methods have utilized them and have reported better performance. Although we choose a basic LSTM as decoder in our experiments, our method is modular w.r.t. the decoder architecture.</p><p>Training Details: We train our model using Adam <ref type="bibr" target="#b15">[16]</ref> algorithm with β 1 = 0.9, β 2 = 0.999 and ϵ = 1e -8. For YouTube2Text dataset, we set the batch size to be 32. The initial learning rates are set to be 8e-5 for the encoder and 4e-5 for the decoder, respectively. For MSR-VTT dataset, we set the batch size to be 64. The initial learning rates are set to be 6e-5 for the encoder and 3e-5 for the decoder. For both datasets, the learning rates are divided by 5 after 10 epochs. We perform gradient clipping with a threshold of 2, and adopt weight initialization method proposed in <ref type="bibr" target="#b9">[10]</ref>. We also regularize our model by applying dropout <ref type="bibr" target="#b36">[37]</ref> to the output of each TCB with a rate of 0.2. Additional regularization methods, e.g., weight decay, are not utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with the State-of-the-Art</head><p>On YouTube2Text: In Table utilized by LSTM-TSA <ref type="bibr" target="#b26">[27]</ref>, SCN <ref type="bibr" target="#b8">[9]</ref>, LSTM-YT <ref type="bibr" target="#b45">[46]</ref> and GloVe + DeepFussion <ref type="bibr" target="#b43">[44]</ref>. Surprisingly, even without using extra training data, our method significantly outperforms all of them. Besides, both LSTM-TSA <ref type="bibr" target="#b26">[27]</ref> and SCN <ref type="bibr" target="#b8">[9]</ref> rely on hundreds of datasetspecific "semantic attributes", which are manually selected from thousands of candidates. The laborious "semantic attribute" selection prevents <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> to be applied to large dataset with more candidates. On the contrary, our method automatically learns representation of high-level semantics using the proposed semantic branch.</p><p>On MSR-VTT: In Table <ref type="table" target="#tab_1">2</ref>, we show a comparison of SibNet and previous state-of-the-art methods on MSR-VTT dataset. We also compare SibNet with methods that occupy top-4 positions of the Leaderboard of MSR-VTT Challenge <ref type="bibr" target="#b22">[23]</ref>, denoted as Rank1: v2tnavigator <ref type="bibr" target="#b14">[15]</ref>, Rank2: Aalto <ref type="bibr" target="#b35">[36]</ref>, Rank3: VideoLAB <ref type="bibr" target="#b30">[31]</ref> and Rank4: ruc-uva <ref type="bibr" target="#b7">[8]</ref>. Our method achieves the best performance across three of the four metrics. Note that the current best performing method, M2M <ref type="bibr" target="#b28">[29]</ref>, not only relies on two large-scale external datasets UCF101 and SNLI for training, but also utilizes an ensemble of multiple models. However, our SibNet is trained without extra training data and tested without ensemble.</p><p>From Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, it can be seen that SibNet consistently outperforms state-of-the-art methods by a large margin even without extra training data and model ensemble, which validates the effectiveness of encoding video contents using the proposed two-branch architecture.</p><p>Qualitative Analysis: Figure <ref type="figure" target="#fig_6">6</ref> shows some qualitative results of SibNet. It can be seen that our method can generate captions that correctly describe their corresponding videos. In addition, our method is able to handle challenging situations, such as scene changes. As shown in the second example in the second row, our method generates a correct caption whose subject "man", verb "talk" and object "football game" are extracted from different scene frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Since SibNet differs from the encoders employed by existing video captioning approaches fundamentally, in this section we perform detailed ablation study to get a better understanding of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">How much does each component contribute?</head><p>In order to analyze the impact of different components of our proposed model on the performance of video captioning, we evaluate five variants of our model, denoted as: Single (3-layer), Single (6layer), Ours (Sib-DL), Ours (CL) and Ours (SL), respectively. First of all, Single (3-layer) and Single (6-layer) denote two single-branch encoders which only consist of 3 and 6 identical TCBs. These two variants, which encode visual information using a single branch, could be viewed as the baseline of our model. And both of them are trained using decoder loss L d alone. To validate the superiority of the proposed two-branch architecture over the baseline, we construct Ours (Sib-DL), which has both the content branch and the semantic branch. But Ours (Sib-DL) is also trained with decoder loss L d alone. To evaluate the effectiveness of our proposed training scheme which provides more fine-grained training supervision, we construct two variants: Ours (CL) and Ours (SL). Ours (CL) incorporates the autoencoder to impose a content loss L c as defined in Equation <ref type="formula" target="#formula_0">1</ref>to Ours (Sib-DL). Likewise, Ours (SL) incorporates visual-semantic embedding to impose a semantic loss L s as defined in Equation <ref type="formula" target="#formula_3">3</ref>to Ours (Sib-DL). Lastly, we evaluate Ours (Full), which is our full model.</p><p>From Table <ref type="table" target="#tab_2">3</ref> that shows the results of all variants above on both MSR-VTT and YouTube2Text, we observe that:</p><p>(1) Comparing with Ours (Sib-DL), Single (3-layer) and Single (6-layer) have worse performance. This indicates the necessity of encoding visual information using our proposed twobranch architecture. It is worth noting that the performance of Single (3-layer) and Single (6-layer) is on a par with many existing methods, which validates the effectiveness of modeling video temporal structures of videos using TCB as described in Section 3.1.2. (2) By adding content loss L c to decoder loss L d used by Ours (Sib-DL), Ours (CL) achieves better performance than Ours (Sib-DL). This verifies the efficiency of regularizing the content branch using autoencoder. Similarly, by adding semantic loss L s , Ours (SL) also outperforms Ours (Sib-DL) by a large margin. This validates the importance of regularizing the semantic branch by leveraging visual-semantic joint embedding. Finally, we can see that Ours (Full) performs slightly better than both Ours (CL) and Ours (SL). Hence, we can conclude that our autoencoder and visual-semantic embedding   Figure <ref type="figure">7</ref>: Evaluation of the impact of both branches' depths on the performance of our method. First row: impact of the TCB block number in content branch, where the TCB number in semantic branch is fixed to 6. Second row: impact of the TCB block number in semantic branch, where the TCB number in content branch is fixed to 3.</p><p>collaboratively provide complementary training guidance to the proposed encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Why semantic branch is deeper than content branch?</head><p>In this section, we discuss the impact of the depths of the two branches. We first increase the number of TCB blocks in the content branch from 1 to 8 while the number of blocks in the semantic branch to is fixed to 6. As shown in the first row of Figure <ref type="figure">7</ref>, the performance drops in a monotonic manner as number of blocks in the content branch goes from 3 to 1 or from 3 to 8. We notice that when the number of blocks is 3, our method can achieve the best performance overall. We also change the number of blocks in the semantic branch from 1 to 9 while fixing the number of blocks in the content branch as 3. The results are demonstrated in the second row of Figure <ref type="figure">7</ref>. We can see that consistent performance drop exists when number of blocks in the semantic branch goes from 6 to 1 or from 6 to 9. In particular, using less than 3 blocks in the semantic branch severely affects the performance. This validates that in order to encode semantic information, which has a high level of abstraction, it is better to use deeper semantic branch. Another benefit for stacking more blocks is that, as the number of blocks in our semantic branch goes up, the temporal receptive field of it increases, which enables it to model longer temporal dynamics of videos. Based on the results shown in Figure <ref type="figure">7</ref>, we empirically choose 3 TCBs to form the content branch and 6 TCBs to form the semantic branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Number of parameters</head><p>The number of parameters in SibNet and previous state-of-the-art models are reported in Table <ref type="table">4</ref>. The reported numbers do not include parameters in the decoder's fully connected layer, whose output is then normalized by softmax function to generate probability distribution of words in the vocabulary. Because the number of parameters in it is proportional to the vocabulary size, it is not reported in most previous work. It can be seen from Table <ref type="table">4</ref> that SibNet has much smaller number of parameters than previous stateof-the-art approaches (44% of <ref type="bibr" target="#b44">[45]</ref>, 57% of <ref type="bibr" target="#b8">[9]</ref>, 77% of <ref type="bibr" target="#b28">[29]</ref> and 90% of <ref type="bibr" target="#b26">[27]</ref>). It is worth noting that the number of parameters in our encoder (2.3M) is less than 25% of that of the RNN decoder (9.2M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4:</head><p>The number of parameters of SibNet and previous state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Parameters</head><p>S2VT <ref type="bibr" target="#b44">[45]</ref> 26.4M SCN <ref type="bibr" target="#b8">[9]</ref> 20.1M M2M <ref type="bibr" target="#b28">[29]</ref> 14.9M LSTM-TSA <ref type="bibr" target="#b26">[27]</ref> 12.8M Ours 11.5M -CNN s 1.5M -CNN c 0.8M -RNN d 9.2M</p><p>Our encoder is able to achieve greater representation power with far less number of parameters than existing encoders employed by previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we propose SibNet, which encodes rich video information using a two-branch architecture. The content branch learns video representation using visual information with an autoencoder, while the semantic branch learns video semantic-specific representation with visual-semantic joint embedding. To jointly optimize the encoder and decoder, we propose a new loss function that includes the content loss, semantic loss, and decoder loss. Extensive experiments conducted on standard video captioning benchmarks show that by jointly optimizing the proposed loss function, our SibNet can encode better video representations thus achieving better video captioning results. The comparisons with existing results validate that SibNet outperforms previous state-of-the-art models by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENT</head><p>This work is supported in part by start-up funds from State University of New York at Buffalo and gift grant from Snap.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed SibNet, which employs a two-branch architecture to collaboratively encode videos. The proposed loss function contains three components: a content loss L c , a semantic loss L s , and a decoder loss L d .We leverage autoencoder and visual-semantic joint embedding to impose fine-grained regularization that pushes content branch to capture visual contents and pushes semantic branch to encode video semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Illustration of the proposed Sibling Convolutional Encoder (SibNet), which is composed of the content branch and the semantic branch, denoted as CNN c and CNN s , respectively. We construct both branches by stacking 3 and 6 identical temporal convolutional blocks (TCBs) (we will introduce TCB in Section 3.1.2). A soft-attention mechanism is utilized in our RNN decoder.</figDesc><graphic coords="2,237.49,154.18,74.03,51.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the content branch CNN c implemented via autoencoder. Note that the content loss of autoencoder is one component of our final training loss.</figDesc><graphic coords="4,120.76,101.68,67.67,65.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of our temporal convolutional block (TCB), which is the basic component of both the content branch and the semantic branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of the semantic branch CNN s implemented via visual-semantic joint embedding. Note that the semantic loss of visual-semantic embedding is one component of our final training loss.</figDesc><graphic coords="5,394.03,111.47,81.54,70.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>running in the play ground Ours: a woman in a red shirt is running on a track GT: a man is about to shoot someone in forest Ours: a man is shooting a gun GT: a girl is singing on stage Ours: a girl is singing on stage GT: a man is talking about football Ours: a man is talking about a football game GT: a crowd of fireworks Ours: fireworks are exploding in the sky GT: a group of people dance on the beach Ours: a group of people are dancing on the beach GT: two wrestlers are fighting in the ring Ours: two men are wrestling in a ring GT: a man is swimming in the pool Ours: a man is swimming in the water</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Qualitative results of our method on MSR-VTT dataset. "GT" denotes ground truth captions; "Ours" denotes captions generated by our method.</figDesc><graphic coords="7,475.89,398.70,77.81,58.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons on YouTube2Text (MSVD) dataset. * indicates that external datasets were used to train these models.</figDesc><table><row><cell>Methods</cell><cell cols="4">Bleu-4 METEOR CIDEr ROUGE</cell></row><row><cell>S2VT [45]</cell><cell>37.0</cell><cell>29.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Temporal Attention [53]</cell><cell>41.9</cell><cell>29.6</cell><cell>51.7</cell><cell>-</cell></row><row><cell>GRU-RCN [2]</cell><cell>43.3</cell><cell>31.6</cell><cell>68.0</cell><cell>-</cell></row><row><cell>aLSTM [11]</cell><cell>44.9</cell><cell>30.4</cell><cell>60.1</cell><cell>-</cell></row><row><cell>LSTM-E [26]</cell><cell>45.3</cell><cell>31.0</cell><cell>-</cell><cell>-</cell></row><row><cell>HRNE + Attention [25]</cell><cell>46.7</cell><cell>33.9</cell><cell>-</cell><cell>-</cell></row><row><cell>p-RNN [55]</cell><cell>49.9</cell><cell>32.6</cell><cell>65.8</cell><cell>-</cell></row><row><cell>Latent Topic [5]</cell><cell>48.8</cell><cell>34.4</cell><cell>80.5</cell><cell>-</cell></row><row><cell>AF [13]</cell><cell>52.4</cell><cell>32.0</cell><cell>68.8</cell><cell>-</cell></row><row><cell>mGRU [57]</cell><cell>49.5</cell><cell>33.4</cell><cell>75.5</cell><cell>-</cell></row><row><cell>MA-LSTM [51]</cell><cell>52.3</cell><cell>33.6</cell><cell>70.4</cell><cell>-</cell></row><row><cell>RecNet [47]</cell><cell>52.3</cell><cell>34.1</cell><cell>80.3</cell><cell>69.8</cell></row><row><cell cols="2">GloVe + DeepFussion* [44] 42.1</cell><cell>31.4</cell><cell>-</cell><cell>-</cell></row><row><cell>LSTM-YT* [46]</cell><cell>31.2</cell><cell>26.9</cell><cell>-</cell><cell>-</cell></row><row><cell>SCN* [9]</cell><cell>50.2</cell><cell>33.4</cell><cell>77.0</cell><cell>-</cell></row><row><cell>LSTM-TSA* [27]</cell><cell>52.8</cell><cell>33.5</cell><cell>74.0</cell><cell>-</cell></row><row><cell>Ours</cell><cell>54.2</cell><cell>34.8</cell><cell>88.2</cell><cell>71.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>1, we present the results of SibNet and existing methods on YouTube2Text dataset. As we can see, our method achieves the best performance across all metrics, improving Bleu-4 from 52.8 to 54.2, METEOR from 34.4 to 34.8, CIDEr from 80.5 to 88.2 respectively. It is worth noting that large-scale external datasets (at least two times larger than YouTube2Text dataset) are Performance comparisons on the test set of MSR-VTT: comparisons with state-of-the-art methods and methods that rank top-4 on the Leaderboard of MSR-VTT Challenge. * indicates that extra training data was used during training. e indicates that the reported performance was achieved by an ensemble of multiple models. (As WSDC<ref type="bibr" target="#b34">[35]</ref> conducts extensive data augmentation, which none of the others conducts, we report the performance of<ref type="bibr" target="#b34">[35]</ref> achieved on the validation set under similar settings as our method.)</figDesc><table><row><cell>Methods</cell><cell cols="4">Bleu-4 METEOR CIDEr ROUGE</cell></row><row><cell cols="2">Rank1: v2t-navigator [15] 40.8</cell><cell>28.2</cell><cell>44.8</cell><cell>60.9</cell></row><row><cell>Rank2: Aalto [36]</cell><cell>39.8</cell><cell>26.9</cell><cell>45.7</cell><cell>59.8</cell></row><row><cell>Rank3: VideoLAB [31]</cell><cell>39.1</cell><cell>27.7</cell><cell>44.1</cell><cell>60.6</cell></row><row><cell>Rank4: ruc-uva [8]</cell><cell>38.7</cell><cell>26.9</cell><cell>45.9</cell><cell>58.7</cell></row><row><cell>Mean Pooling [46]</cell><cell>30.4</cell><cell>23.7</cell><cell>35.0</cell><cell>52.0</cell></row><row><cell>Temporal Attention [53]</cell><cell>28.5</cell><cell>25.0</cell><cell>37.1</cell><cell>53.3</cell></row><row><cell>S2VT [45]</cell><cell>31.4</cell><cell>25.7</cell><cell>35.2</cell><cell>55.9</cell></row><row><cell>MA-LSTM [51]</cell><cell>36.3</cell><cell>26.3</cell><cell>40.1</cell><cell>59.1</cell></row><row><cell>aLSTM [11]</cell><cell>38.0</cell><cell>26.1</cell><cell>-</cell><cell>-</cell></row><row><cell>STAT [41]</cell><cell>37.4</cell><cell>26.6</cell><cell>41.5</cell><cell>-</cell></row><row><cell>AF [13]</cell><cell>39.4</cell><cell>25.7</cell><cell>40.4</cell><cell>-</cell></row><row><cell>RecNet [47]</cell><cell>39.1</cell><cell>26.6</cell><cell>42.7</cell><cell>59.3</cell></row><row><cell>M2M  * e [29]</cell><cell>40.8</cell><cell>28.8</cell><cell>47.1</cell><cell>60.2</cell></row><row><cell>WSDC v [35]</cell><cell>39.0</cell><cell>27.7</cell><cell>44.0</cell><cell>60.1</cell></row><row><cell>Ours</cell><cell>40.9</cell><cell>27.5</cell><cell>47.5</cell><cell>60.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of different variants of our method on YouTube2Text and MSR-VTT datasets.</figDesc><table><row><cell>Methods</cell><cell>Dataset</cell><cell cols="5">L d L c L s Bleu-4 METEOR CIDEr ROUGE</cell></row><row><cell>Single (3-layer)</cell><cell></cell><cell>✓</cell><cell>38.9</cell><cell>26.4</cell><cell>44.8</cell><cell>59.2</cell></row><row><cell>Single (6-layer)</cell><cell></cell><cell>✓</cell><cell>39.0</cell><cell>26.8</cell><cell>43.7</cell><cell>59.4</cell></row><row><cell>Ours (Sib-DL) Ours (CL)</cell><cell>MSR-VTT</cell><cell>✓ ✓ ✓</cell><cell>39.4 40.0</cell><cell>26.9 27.1</cell><cell>45.3 46.2</cell><cell>59.6 60.1</cell></row><row><cell>Ours (SL)</cell><cell></cell><cell>✓</cell><cell>✓ 40.4</cell><cell>27.1</cell><cell>46.8</cell><cell>60.0</cell></row><row><cell>Ours (Full)</cell><cell></cell><cell cols="2">✓ ✓ ✓ 40.9</cell><cell>27.5</cell><cell>47.5</cell><cell>60.2</cell></row><row><cell>Ours (Sib-DL)</cell><cell></cell><cell>✓</cell><cell>51.9</cell><cell>33.1</cell><cell>81.9</cell><cell>69.9</cell></row><row><cell>Ours (CL) Ours (SL)</cell><cell>YouTube2Text</cell><cell>✓ ✓ ✓</cell><cell>52.8 ✓ 53.3</cell><cell>34.0 34.5</cell><cell>85.6 86.0</cell><cell>71.1 71.2</cell></row><row><cell>Ours (Full)</cell><cell></cell><cell cols="2">✓ ✓ ✓ 54.2</cell><cell>34.8</cell><cell>88.2</cell><cell>71.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>MM'18, October 22-26, 2018, Seoul, Republic of Korea</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Although SibNet, abbreviation for Sibling Convolutional Encoder, only refers to the encoder of our method, we also use it to refer to a combination of our encoder and the RNN decoder in the following sections.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">StructCap: Structured Semantic Embedding for Image Captioning</title>
		<author>
			<persName><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video captioning with guidance of multimodal latent topics</title>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1838" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft COCO captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effect of rosuvastatin on progression of carotid intima-media thickness in low-risk individuals with subclinical atherosclerosis: the METEOR Trial</title>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>John R Crouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ward</forename><forename type="middle">A</forename><surname>Raichlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">W</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">K</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H O'</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederick</forename><forename type="middle">E</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><surname>Grobbee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Michiel</surname></persName>
		</author>
		<author>
			<persName><surname>Bots</surname></persName>
		</author>
		<author>
			<persName><surname>Study Group</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">297</biblScope>
			<biblScope unit="page" from="1344" to="1353" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Early embedding and late reranking for video captioning</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1082" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention-based LSTM with semantic consistency for videos captioning</title>
		<author>
			<persName><forename type="first">Lianli</forename><surname>Zhao Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng Tao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="357" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng-Yok</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Describing videos using multi-modal fusion</title>
		<author>
			<persName><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1087" to="1091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fluency-Guided Cross-Lingual Image Captioning</title>
		<author>
			<persName><forename type="first">Weiyu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1549" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Summarization-based video caption via deep neural networks</title>
		<author>
			<persName><forename type="first">Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on Multimedia Conference</title>
		<meeting>the 2015 ACM on Multimedia Conference</meeting>
		<imprint>
			<biblScope unit="page" from="1191" to="1194" />
		</imprint>
	</monogr>
	<note>year=2015, organization=ACM</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boosting video description generation by explicitly translating from frame-level captions</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongchao</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="631" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mel frequency cepstral coefficients for music modeling</title>
		<author>
			<persName><forename type="first">Beth</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ISMIR</title>
		<imprint>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<ptr target="http://ms-multimedia-challenge.com/2017/challenge" />
		<title level="m">MSR-VTT Challenge</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video captioning with transferred semantic attributes</title>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL). ACL</title>
		<meeting>the Annual Meeting on Association for Computational Linguistics (ACL). ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-task video captioning with video and entailment generation</title>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
		<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream Collaborative Learning with Spatial-Temporal Attention for Video Classification</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal video description</title>
		<author>
			<persName><forename type="first">Vasili</forename><surname>Ramanishka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1092" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint image-text representation by gaussian visual-semantic embedding</title>
		<author>
			<persName><forename type="first">Hailin</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="207" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiple Instance Visual-Semantic Embedding</title>
		<author>
			<persName><forename type="first">Hailin</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the British Machine Vision Conference (BMVC)</title>
		<meeting>eeding of the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning-based Image Captioning with Embedding Reward</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xutao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised dense video captioning</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Frame-and segment-level features and candidate pool evaluation for video caption generation</title>
		<author>
			<persName><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorma</forename><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1073" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video Description with Spatial-Temporal Attention</title>
		<author>
			<persName><forename type="first">Yunbin</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1014" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL). ACL</title>
		<meeting>Association for Computational Linguistics (ACL). ACL</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving lstm-based video description with linguistic knowledge mined from text</title>
		<author>
			<persName><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
	<note>North American Chapter</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reconstruction Network for Video Captioning</title>
		<author>
			<persName><forename type="first">Bairui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image captioning with deep bidirectional LSTMs</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="988" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (ICCV)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (ICCV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning Multimodal Attention LSTM Networks for Video Captioning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="537" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4584" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bidirectional multirate reconstruction for temporal modeling in videos</title>
		<author>
			<persName><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
