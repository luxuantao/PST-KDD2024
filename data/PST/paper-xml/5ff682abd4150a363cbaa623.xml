<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Deception Detection using Real-Life Trial Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">M</forename><surname>Umut</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>¸en</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ver</forename><surname>Ónica P Érez-Rosas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Berrin</forename><surname>Yanikoglu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Abouelenien</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mihai</forename><surname>Burzo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
						</author>
						<title level="a" type="main">Multimodal Deception Detection using Real-Life Trial Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TAFFC.2020.3015684</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.3015684, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.3015684, IEEE Transactions on Affective Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>real-life trial</term>
					<term>deception detection</term>
					<term>classification</term>
					<term>multimodal</term>
					<term>visual</term>
					<term>acoustic</term>
					<term>linguistic !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hearings of witnesses and defendants play a crucial role when reaching court trial decisions. Given the high-stakes nature of trial outcomes, developing computational models that assist the decision-making process is an important research venue. In this paper, we address the identification of deception in real-life trial data. We use a dataset consisting of videos collected from public court trials. We explore the use of verbal and non-verbal modalities to build a multimodal deception detection system that aims to discriminate between truthful and deceptive statements provided by defendants and witnesses. In particular, three complementary modalities (visual, acoustic and linguistic) are evaluated for the classification of deception at the subject level. The final classifier is obtained by combining the three modalities via score-level classification, achieving 83.05% accuracy in subject-level deceit detection. To place our results in perspective, we present a human deception detection study where we evaluate the human capability of detecting deception using different modalities and compare the results to the developed system. The results show that our system outperforms the average non-expert human capability of identifying deceit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With thousands of trials and verdicts occurring daily in courtrooms around the world, there is a high chance of using deceptive statements and testimonies as evidence. Given the high-stake nature of trial outcomes, implementing accurate and effective computational methods to evaluate the honesty of provided testimonies can offer valuable support during the decision-making process.</p><p>The consequences of falsely accusing the innocents and freeing the guilty can be severe. For instance, in the U.S. alone there are tens of thousands of criminal cases filed every year. In 2013, there were 89,936 criminal cases filings in U.S. District Courts and in 2014 the number was 80,262. 1  Moreover, the average number of exonerations per year increased from <ref type="bibr">3.03 in 1973-1999</ref>  1. www.uscourts.gov case <ref type="bibr" target="#b0">[1]</ref>. Hence, the need arises for a reliable and efficient system to aid the task of detecting deceptive behavior and discriminate between liars and truth-tellers. Traditionally, law enforcement entities have made use of the polygraph test as a standard method to identify deceptive behavior. However, this approach becomes impractical in some cases, as it requires the use of skin-contact devices and human expertise to get accurate readings and interpretation. In addition, the final decisions are subject to error and bias not only from the device itself but also from human judgment <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Furthermore, using proper countermeasures, offenders can deceive these devices as well as human experts.</p><p>Given the difficulties associated with the use of polygraph-like methods, machine learning-based approaches have been proposed to address the deception detection problem using several modalities, including text <ref type="bibr" target="#b3">[4]</ref> and speech <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Unlike the polygraph method, learningbased methods for deception detection rely mainly on data collected from deceivers and truth-tellers. The data is usually elicited from human contributors, in a lab setting or via crowd-sourcing <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, for instance by asking subjects to narrate stories deceptively and truthfully <ref type="bibr" target="#b6">[7]</ref>, by performing one-on-one interviews, or by participating in "mock crime" scenarios <ref type="bibr" target="#b7">[8]</ref>.</p><p>Despite their potential benefits, an important drawback in data-driven research on deception detection is the lack of real data and the absence of true motivation while eliciting deceptive behavior. Because of the artificial setting, the subjects may not be emotionally aroused or highly motivated to lie, thus making it difficult to generalize findings to real-life scenarios.</p><p>In this paper, we present a multimodal system that detects deception in real-life trial data using verbal, acoustic and visual modalities. The data consists of video clips obtained from real court trials, initially presented in <ref type="bibr" target="#b8">[9]</ref>.</p><p>Unlike previous work on this dataset, which focuses on detecting deception at the video-level, we aim to detect deception at the subject-level. We believe this is more in line with the ground-truth for this dataset since it was also obtained at the subject-level: defendants who are found guilty at the end of the trial are labeled as deceptive since they had not admitted to their guilt during the hearings. In the remainder of the paper, we will refer to this task as a subject-level deception classification.</p><p>Our main contributions are as follows: • We introduce the subject-level deception detection as a novel take on the problem; we argue that it is also more appropriate for the problem given the way groundtruth is established. • We explore the effectiveness of a diverse set of features extracted from the linguistic, visual, and acoustic channels, both separately and in combination (using early and late fusion methods). • We repeat the experiment 3 times with different random seed for each test sample in the leave-one-out crossvalidation, to obtain more reliable scores with the small dataset.</p><p>• We present a semi-automatic system that can identify deception with 83.05% accuracy using a combination of automatically extracted and manually annotated features, as well as a fully-automatic system that reaches almost 73% accuracy. • We place our results in context by performing a study where humans evaluate the presence of deception in the real-life trial dataset. • We present insights into the problem by analyzing the importance of features obtained manually and automatically, as well as the linguistic differences among deceptive and truthful subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DATASET</head><p>During our experiments, we use a multimodal deception dataset obtained from real-life court trials. The dataset description is included here for completeness; further details can be found in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Overview</head><p>The dataset consists of trial hearing recordings obtained from public sources. The videos were carefully selected to be of reasonably good audio-visual quality and portray a single subject with his/her face visible during most of the clip duration. Videos are collected from trials with different outcomes: guilty verdict, non-guilty verdict, and exoneration. For guilty verdicts, deceptive clips are collected from a defendant in a trial and truthful videos are collected from witnesses in the same trial. In some cases, deceptive videos are collected from a suspect denying a crime he committed and truthful clips are taken from the same suspect when answering questions concerning some facts that were verified by the police as truthful. For the witnesses, testimonies that were verified by police investigations are labeled as truthful whereas testimonies in favor of a guilty suspect are The dataset includes several famous trials (including trials of Jodi Arias, Donna Scrivo, Jamie Hood, and others), police interrogations, and also statements from the "The Innocence Project" website. <ref type="foot" target="#foot_0">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Subject-level Ground-truth</head><p>In the original dataset, the ground-truth was obtained at video level, by carefully identifying and labeling truthful and deceptive video clips from trial's recordings <ref type="bibr" target="#b8">[9]</ref>.</p><p>In this work, we focus on deception at the subject level for two reasons: 1) it is difficult to know the ground-truth of all video clips with certainty and 2) the ultimate goal is to determine whether an individual is being deceptive or not, rather than pinpoint exactly when s/he is lying. Note that subject-level decision is what human jurors are also asked to accomplish during real life trials consisting of several interrogation episodes.</p><p>To obtain subject-level ground truth, we only used the trial outcomes to indicate the subject as deceptive or not (deceptive in case of a guilty verdict vs not-deceptive in case of non-guilty verdict or exoneration). The resulting subjectlevel dataset has 59 instances, and the distributions of male vs female and deceptive vs truthful are given in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Note that a subject-level deception detection system can be evaluated fairly, by comparing its predictions to the subject-level ground-truth, which is the trial outcome, with the assumption that the trial outcome is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transcriptions</head><p>The transcriptions are obtained using Amazon Mechanical Turk in the original dataset. In video clips where multiple speakers are portrayed (i.e., defendants or witnesses being questioned by attorneys), the AMT workers were asked to transcribe only the subject's speech, including word repetitions, fillers such as um, ah, and uh, and intentional silences encoded as ellipsis.</p><p>The final set of transcriptions consists of 8,055 words, with an average of 66 words per transcript. Table <ref type="table" target="#tab_3">2</ref> shows transcriptions of sample deceptive and truthful statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Visual Behavior Annotations</head><p>Gesture annotations are also available in the dataset. <ref type="foot" target="#foot_1">3</ref> The annotation was conducted using the MUMIN <ref type="bibr" target="#b9">[10]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truthful</head><p>Deceptive We proceeded to step back into the living room in front of the fireplace while William was sitting in the love seat. And he was still sitting there in shock and so they to repeatedly tell him to get down on the ground. And so now all three of us are face down on the wood floor and they just tell us "don't look, don't look" And then they started rummaging through the house to find stuff... No, no. I did not and I had absolutely nothing to do with her disappearance. And I'm glad that she did. I did. I did. Um and then when Laci disappeared, um, I called her immediately. It wasn't immediately, it was a couple of days after Laci's disappearance that I telephoned her and told her the truth. That I was married, that Laci's disappeared, she didn't know about it at that point. modal scheme, which includes several different facial expressions associated with overall facial expressions, eyebrows, eyes and mouth movements, gaze direction, as well as head and hand movements. Sample screenshots showing facial displays and gestures by deceptive and truthful subjects in the dataset are shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>This annotation was done at the video-level by identifying the facial displays and hand gestures that were most frequently observed during the entire clip duration. Two annotators independently labeled a sample of 56 videos. The inter-annotator agreement for this task is shown in Table <ref type="table" target="#tab_4">3</ref>. The agreement measure represents the percentage of times the two annotators agreed on the same label for each gesture category. For instance, 80.03% of the time the annotators agreed on the labels assigned to the Eyebrows category. On average, the observed agreement was measured at 75.16%, with a Kappa of 0.57 (macro-averaged over the nine categories).</p><p>As a preliminary analysis, Figure <ref type="figure">2</ref> shows the percentages of all the non-verbal features for which we observe noticeable differences for the deceptive and truthful groups. The figure suggests eyebrow (rise) helps differentiate between the deceptive and truthful conditions. Twyman et al. reported that deceivers' right hand moves less during a mock crime experiment <ref type="bibr" target="#b10">[11]</ref>. This coincides with our single and both hands movement analysis as depicted in Figure <ref type="figure">2</ref>. ten Brinke and Porter <ref type="bibr" target="#b11">[12]</ref> reported that deceptive people blink at a faster rate than genuinely distressed individuals; which also coincides with our findings that deceivers display more frequent occurrence of rapid eye closures, as seen in Fig. <ref type="figure">2</ref>.). Interestingly, deceivers seem to shake their head (Side-Turn-R) and nod (Down-R) less frequently than truthtellers while true-tellers seem to move their hands more frequently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEATURES FOR DECEPTION DETECTION</head><p>Aiming to explore the subject-level deception detection with different levels of supervision, we conduct two main experiments using features obtained either manually or semi-automatically. We first present a semi-automatic system where the linguistic and visual feature extraction is done based on manual annotations, as described in Section 2. Second, we build a fully-automatic system that does not rely on human input. Finally, we compare the results with that of human performance on deception detection.</p><p>Given the multimodal nature of our dataset, we were interested to evaluate the usefulness of the linguistic, visual, and acoustic components of the recordings, both individually and in combination. Note that automatic temporal analysis of the videos would be significantly more complicated to accomplish and would require a larger dataset to prevent overfitting; hence it is outside of the scope of this paper. The feature extraction process is detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linguistic Features</head><p>We experimented with linguistic features that have been previously found to correlate with deception cues <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. These features are derived from the text transcripts of the subjects' statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unigrams</head><p>We extract unigrams derived from the bag of words representation of each transcript. Each feature consists of frequency counts of unique words in the transcript. For this set, we keep only words with a frequency greater than or equal to 10. The threshold cut was experimentally obtained in a small development set. LIWC We use features derived from the Linguistic Inquire Word Count (LIWC) lexicon <ref type="bibr" target="#b13">[14]</ref>. These features consist of word counts for each of the 80 semantic classes in LIWC. For instance, the class "I" includes words associated with the self (e.g., I, me, myself); "Other" includes words associated with others (e.g., he, she, they); etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotated Visual Behaviour Features</head><p>One set of visual features are derived from the annotations performed using the MUMIN coding scheme described in Section 2.4. We create a binary feature for each of the 40 available gesture labels. Each feature indicates the presence of a gesture only if it is observed during the majority of the interaction. The generated features represent nine different gesture categories listed in Table <ref type="table" target="#tab_4">3</ref>, covering 32 facial displays and 7 hand gestures. Facial Displays. These are facial expressions or head movements displayed by the speaker during the deceptive or truthful interaction. They include overall facial expressions such as smiling and scowling; eyebrows, eyes and mouth movements (e.g. repeated eye closing or protruded lips); gaze direction (e.g. looking down or towards the interlocutor); and as well as head movements (e.g. repeated nodding or shaking) and hand movements. Hand Gestures. The second broad category covers gestures made with the hands, including movements of one or both hands and their trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Automatically Extracted Visual Features</head><p>We automatically extract a second set of visual features consisting of assessments of several facial movements as described below: Facial Action Units (FACS). These features denote the presence of facial muscle movements that are commonly used for describing and classifying expressions <ref type="bibr" target="#b14">[15]</ref>. We use the OpenFace library <ref type="bibr" target="#b15">[16]</ref> with the default multiperson detection model to obtain 18 binary indicators of Action Units (AUs) for each frame in our videos. These include: AU1 (inner brow raiser), AU2 (outer brow raiser), AU4 (brow lowerer), AU5 (upper lid raiser), AU6 (cheek raiser), AU7 (eyelid tightener), AU9 (nose wrinkler), AU10 (upper lip raiser), AU12 (lip corner puller), AU14 (dimpler), AU15 (lip corner depressor), AU17 (chin raiser), AU20 (lip stretcher), AU23 (lip tightener), AU25 (lips part), AU26 (jaw drop), AU28 (lip suck), and AU45 (blink). We average these binary indicators through the frames and obtain a single AU feature for each video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Acoustic Features</head><p>Previous work has suggested that pitch is an indicator of deceit, and showed that people tend to increase their pitch when they are being deceptive <ref type="bibr" target="#b16">[17]</ref>. This motivated us to explore whether subjects will show particular pitch differences in their speech while telling the truth or deceiving.</p><p>In addition to pitch, we extracted acoustic features for voiced segments and pauses, based on previous findings showing that deceivers produce slightly shorter utterances and pause more frequently than true-tellers <ref type="bibr" target="#b17">[18]</ref>. The extracted acoustic features are as follows. Pitch. We derive features from pitch measurements in the audio portion of each video in the dataset. To estimate pitch, we obtained the fundamental frequency (f0) of the defendants' speech using the STRAIGHT toolbox <ref type="bibr" target="#b18">[19]</ref>. Since f 0 is defined only over voiced parts of the speech, we remove unvoiced speech frames from our calculations. We then derive two features (mean and standard deviation) from the raw f 0 measurements: mean−f 0 and stdev−f 0 . Silence and Speech Histograms. To obtain these features, we run a voice activity detection (VAD) algorithm <ref type="bibr" target="#b19">[20]</ref> to obtain the speech and silent segments in the subject's speech. Since the performance of VAD algorithms is affected by the segmentation threshold θ, i.e., high values of θ result on over-segmentation while low values produce under segmentation, we experiment with two values of θ to improve the VAD segmentation in our data: 0.01 and 0.2. After manual inspection, we observed that using a threshold of 0.2, the algorithm segment the audio into words rather than full sentences while a threshold of 0.01 produces full sentence segmentation. Using a VAD threshold of 0.2, with the intent of capturing short pauses, we extract the histograms (using 25 bins) of both voiced and silent segments as features. Figure <ref type="figure" target="#fig_1">3</ref> shows the distribution of the mean and standard deviation of pitch frequencies for the deceptive and truthful groups by gender. As can be seen in this figure, pitch mean values depend on the gender, while standard deviation seems more correlated with deception. Figure <ref type="figure" target="#fig_2">4</ref> depicts the histograms of speech and silent lengths by deceptive and truthful subjects. Interestingly, the plot shows that deceptive individuals tend to make shorter pauses more frequently than truthful individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Subject-level Feature Integration</head><p>Since our feature extraction is performed in each video clip separately for visual features and there are cases where there is more than one video for a single subject, we devised two strategies to aggregate the features across all videos from the same subject. First, taking the maximum values per feature across feature vectors corresponding to every subject's video. Second, averaging the feature values across feature vectors corresponding to each subject's video.</p><p>Taking the maximum of the feature values aims to represent single events (e.g., eyes blinking), even if it is observed in just one of the videos belonging to a subject. Averaging the feature values, on the other hand, aims to reduce potential noise introduced during the manual annotation.</p><p>During our initial experiments, we found that the averaging strategy outperforms the use of maximum values, hence the former is used during the rest of the experiments reported in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CLASSIFIERS</head><p>We chose the Random Forest (RF), Support Vector Machine (SVM) with Radial Basis Function kernel and Neural Network (NN) classifiers, due to their success in many other machine learning problems. For the RF and SVM, we use their implementations as available in Matlab. We use the PyTorch library for the implementation of the NN classifiers <ref type="bibr" target="#b20">[21]</ref>. During our experiments, all classifiers are evaluated using accuracy and area under the curve (AUC) as our main performance metrics.</p><p>For the SVM classifiers, we performed parameter tuning over the training set using 4-fold cross-validation separately for each test instance. Specifically, we tune the penalty (C) and the γ parameters of the RBF kernel using grid-search. We applied a 3 × 3 averaging filter to the resulting loss matrix of the grid search to smooth the parameter tuning results to reduce the noise that results from the low number of data points.</p><p>For the RF classifiers, we used the default value for the number of trees (100) and minimum leaf size of 3, without doing parameter optimization.</p><p>For the NN classifier, we used a two hidden layers network (100 and 500 nodes for the hidden layers) along with a softmax activation function and a cross-entropy loss function. L 2 regularization is applied with a weight of 1E − 5, to prevent over-fitting.</p><p>A strong advantage of using RF and the NN classifiers is that they are quite insensitive to the values of their meta-parameters. For instance, when evaluated with different number of hidden nodes in either layer {(10, 100), (100, 100), (500, 500), (500, 10), (100, 10), (10, 500)}, the NN showed a performance variation of only 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SEMI-AUTOMATIC DECEPTION DETECTION</head><p>We develop a semi-automatic system using features derived from manually annotated modalities (visual and linguistic), along with automatically extracted features (speech). Thus, we run several comparative experiments using leave-oneout cross-validation where we test in a single test subject and train in the remaining ones. Furthermore, we run all experiments three times with different random seeds and report the mean and the standard deviation of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results for Individual Modalities</head><p>We initially conduct experiments using each feature set independently and then experiment with different feature combinations using the SVM, RF, and NN classifiers. Table <ref type="table" target="#tab_6">4</ref> shows the results for individual and combined sets of features in each modality.</p><p>Among the different classifiers, the RF classifier is the best classifier for linguistic and acoustic features, while the NN performs best with the visual features. For the visual features, the best results are achieved with the facial displays, reaching an accuracy of 80.79% and an AUC score of 0.94. These results also constitute the best results across individual feature sets. For the acoustic features, the best performing feature is the pitch stdv, which represents the standard deviation of the subject's pitch, resulting in an accuracy of 71.19% and an AUC score of 0.79. The rest of the acoustic features obtain significantly lower performance than pitch stdv alone. For the linguistic features, the classifier built with the unigram features outperformed both LIWC features alone and its combination with LIWC features. The highest accuracy with lexical features is 64.41% with the RF classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results for Combined Modalities</head><p>For the multi-modal approach, we conduct experiments using two different integration strategies of the three modalities in our dataset: early fusion and late fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Early Fusion</head><p>First, we experiment with early fusion by concatenating the best performing feature sets from the three modalities and using the different classifiers. Results are shown in Table <ref type="table" target="#tab_7">5</ref> During these experiments, the NN classifier consistently obtains the best results among different feature combinations as well as the lowest standard deviation through 3 repetitions of the experiments. Among the different combinations, the combination of features encoding the facial displays, pitch and silence and speech histograms achieve the highest accuracy (83.05%), improving the accuracy obtained with facial display features only by 2.26% points. However, in terms of the AUC, the combination of facial displays and the pitch standard deviation performs the best (0.95).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Late Fusion</head><p>Second we use score-level fusion with classifiers built for individual modalities. For these experiments, we use only the best classifiers and features, leaving out the SVM classifier and hand's gesture features. The aggregated score s i is obtained as shown in Equation <ref type="formula" target="#formula_0">1</ref>, where s ij is the score of class c i obtained with the classifier h j and w j is the weight assigned to the classifier h j .</p><formula xml:id="formula_0">s i = j w j s ij<label>(1)</label></formula><p>We use different classifier weights for the facial displays using increments of 0.1 (the remaining weights are assigned equally to the other classifiers) and report results on the test set. Thus, the best scoring setting is obtained a posteriori.</p><p>Classification results obtained with this strategy are shown in Table <ref type="table" target="#tab_8">6</ref>. We observe that the best result (84.18%) is obtained using the NN classifier and the combination of visual features and acoustic features. This result is higher than the best result obtained with early fusion since it finds the best weights over the test set; but the improvement is very small. The best early fusion results are reported as the proposed system's result, throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FULLY-AUTOMATIC DECEPTION DETECTION</head><p>We also conducted a set of experiments where we explore how well fully automatic feature extraction would work, for our task. Since our acoustic features are already obtained using automatic methods, we focus on the automatic extraction of linguistic and visual features.</p><p>We used the OpenFace library <ref type="bibr" target="#b15">[16]</ref> with the default multi-person detection model, to obtain the facial action units (see Section 3.3) for the subject in the video. To address cases where the model identifies multiple persons in the frames, we select the person who is present in the majority of frames as the person of interest. We manually verified the result of this heuristic and confirmed that in most cases the selection corresponds to the main subject in the video. The software was unable to identify the subject's face in four videos in the dataset, due to the low video quality. These videos are nonetheless included in the evaluation, so as to measure the performance of the system under realistic conditions.</p><p>1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. To extract the linguistic features, we applied Automatic Speech Recognition (ASR) to the videos using the Google Cloud Speech API <ref type="bibr" target="#b21">[22]</ref> and obtained the corresponding transcriptions. Then, as in the manual system, we use these transcriptions to extract unigram features. One shortcoming of the automation here is that the transcriptions also contain the interviewer's speech. Furthermore, the ASR failed to recognize any speech for 10 videos, which correspond to three subjects in the dataset. The obtained transcriptions resulted in an average Word Error Rate (WER) of 0.603 and an insertion rate of 0.152.</p><p>The results of the automatic deception system are depicted in Table <ref type="table" target="#tab_9">7</ref>. We see that the performance obtained by classifiers build with automatic visual features falls behind the performance obtained when using manual annotations, while automatic extraction of the linguistic features results in a similar performance. As for combined modalities, we see that the best result, 72.88%, (obtained with the fully automatic system, score-level combination, and the NN classifier) is significantly lower than the best performance with the semi-automatic system, 83.05%. However, we would expect the performance gap would to be smaller when using videos that have better visual quality e.g., videos obtained with high-resolution cameras focused on the subject's face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">HUMAN PERFORMANCE</head><p>As part of our work analyzing the importance of multimodal features in deception detection, we conduct a study where we evaluate the human ability to identify deceit on trial recordings when exposed to four different modalities: We create an annotation interface that shows instances for each modality in random order to each annotator, and ask him or her to select a label of either "Deception" or "Truth" according to his or her perception of truthfulness or falsehood. The annotators did not have access to any information that would reveal the true label of an instance. The only exception to this could have been the annotators' previous knowledge of some of the public trials in our dataset. A discussion with the annotators after the annotation took place, indicated however that this was not the case.</p><p>To avoid annotation bias, we show the modalities in the following order: first we show either Text or Silent video, then we show Audio, followed by Full video. Note that apart from this constraint, which is enforced over the four modalities belonging to each video clip, the order in which instances are presented to an annotator is random.</p><p>Three annotators labeled all 121 video clips in our dataset, which portray 59 different subjects. To calculate the agreement at the subject-level, we apply majority voting to the labels assigned by each annotator over all the clips belonging to the same subject. We resolve ties by randomly choosing between the deceptive and truthful labels. Table <ref type="table" target="#tab_11">8</ref> shows the observed agreement and Kappa statistics among the three annotators for each modality. 4 We observe that the agreement for most modalities is rather low and the Kappa scores show mostly poor agreement. As noted before by Ott et al. <ref type="bibr" target="#b22">[23]</ref>, this low agreement can be interpreted as an indication that people are poor judges of deception.</p><p>In addition, we compare the performance of the three individual annotators and the developed systems, over the four different modalities in the dataset. As shown in Table <ref type="table">9</ref>, we observe a positive trend in human accuracy in the subject-level deceit detection when using multiple modalities. The trend could be explained by having more deception cues available to them. On average, the poorest accuracy is obtained on text only, followed by Audio, Silent video, and Full video, where the annotators have the highest performance. Interestingly, we notice a similar pattern for the developed systems, where we see that having a greater amount of multimodal cues does help to improve the system performance. The fully-automatic system outperforms the average human performance when using each modality individually and in combination (72.88% versus 71.79%). Furthermore, it achieves almost 30% reduction in error compared to the lowest performing human annotator's performance. The semi-automatic system further improves the results of the fully automatic system when using the three modalities (full video), thus suggesting that the feature fusion strategy is also an important aspect when building these models.. Overall, our study indicates that detecting deception is indeed a difficult task for humans and further verifies previous findings where the average human ability to spot liars was found to be slightly better than chance <ref type="bibr" target="#b23">[24]</ref>. Moreover, 4. Inter-rater agreement with multiple raters and variables. https:// nlp-ml.io/jg/software/ira/ the performance of the human annotators appears to be significantly below that of the developed systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">INSIGHTS FOR DECEPTION DETECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Visual features</head><p>We compute the feature importance scores using the predic-torImportance function of Matlab <ref type="bibr" target="#b24">[25]</ref> that bases its estimate on the performance change in the random forest classifier, with the use of each feature. Importance measures of visual AU features are depicted in Figure <ref type="figure" target="#fig_3">5</ref>. We see that features describing actions of lips reveal substantial deception information (Upper Lip Raiser, Lip Stretcher, Lip Tightener, Lip Corner Depressor, Lip Corner Puller). In addition, (eye)Lid Tightener, Nose Wrinkler, Brow Lowerer and Inner Brow Raiser also have high importance scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Deception Language in Trials</head><p>To obtain insights into linguistic behaviors displayed by liars during court hearings, we explore patterns in word usage according to their ability to distinguish between the subjects' deceptive and truthful statements. We thus trained a binary Naive Bayes (NB) classifier that discriminates between liars and true-tellers using the unigram features obtained from the subject's statements. We then use the NB model to infer the expected probabilities of each word given its class label. We then sort the words by importance using the following scoring formula:</p><formula xml:id="formula_1">s i = E[f i |class = deceptive]/E[f i |class = truthf ul],<label>(2)</label></formula><p>In this equation, the expectation E of the word f i is compared across the deceptive and truthful classes. Note that expectation values are obtained from the resulting NB model rather than empirically from the dataset. The words that are more strongly associated with the deceptive and truthful groups are shown below : Deceptive Words: not, he, do, 'm, would, his, no, an, mean, with, uh, just, n't, at, but, want, did, if, a, her, any, very, never , . . . Truthful Words: . . ., by, so, then, other, was, had, all, through, started, up, on, the, years, two, my, when, of, to, from, um. In each set, words are shown in decreasing score order i.e., from most deceptive ("not") to most truthful ("um"). We see that negative words such as "not", "no" and "n't" have higher scores, suggesting that deceptive subjects often focus on denying the accusations, whereas truthful subjects are more focused on explaining past events. This coincides with the meta-analysis work of Hauch et al. which shows that deceptive statements have slightly more negative utterances than truthful statements.</p><p>Also extreme quantifiers (i.e. "any", "never", "very") occur more frequently in deceptive statements. Houch et al. investigated the effect of certainty on deception and, altough certainty indicating words did not have significant effects on deception, they revelead that "deceptive accounts contained slightly fewer tentative words (such as 'may', 'seem', 'perhaps') than truthful accounts" <ref type="bibr" target="#b25">[26]</ref>. They commented on the possibility of liers' motivation to credible. Our findings do not coincide exactly, but they are in the same direction.</p><p>Newman et al. have found that deceivers have a tendency to use fewer self-referencing expressions, such as "I", "my", "mine" <ref type="bibr" target="#b5">[6]</ref>. This coincides with our findings, because self-referencing words do not appear among the most deceptive words; while the word "my" is one of the most truth-indicating words.</p><p>Interestingly, the word "uh" indicates deception whereas the word "um" indicates truthfulness despite both words having the function of pausing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Verbal Deception Detection</head><p>Initial work on deception detection focused on statistical methods to identify verbal cues associated with deceptive behavior. Bachenko et al. selected 12 linguistic indicators of deception, including lack of commitment to a statement or declaration, negative expressions, and inconsistencies with respect to verb and noun forms <ref type="bibr" target="#b26">[27]</ref>. They extracted and analyzed the effect of these indicators on deception for a textual database of criminal statements, police interrogations, depositions and legal testimony. Hauch et al. conducted a meta-study covering 44 studies with a total of 79 linguistic deception cues and obtained a robust analysis of verbal deceptive indicators <ref type="bibr" target="#b25">[26]</ref>.</p><p>To date, works on verbal-based deception detection have explored the identification of deceptive content in a variety of domains, including online dating websites <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, forums <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, social networks <ref type="bibr" target="#b32">[32]</ref>, and consumer report websites <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b33">[33]</ref>. Research findings have shown the effectiveness of features derived from text analysis, which frequently includes basic linguistic representations such as n-grams and sentence count statistics <ref type="bibr" target="#b6">[7]</ref>, and also more complex linguistic features derived from syntactic CFG trees and part of speech tags <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b34">[34]</ref>. Some studies have also incorporated the analysis of psycholinguistics aspects related to the deception process. Some research work has relied on the Linguistic Inquiry and Word Count (LIWC) lexicon <ref type="bibr" target="#b13">[14]</ref> to build deception models using machine learning approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b35">[35]</ref> and showed that the use of psycholinguistic information was helpful for the automatic identification of deceit. Following the hypothesis that deceivers might create less complex sentences to conceal the truth and being able to recall their lies more easily, several researchers have also studied the relation between text syntactic complexity and deception <ref type="bibr" target="#b36">[36]</ref>.</p><p>There is a also significant amount of social science literature that statistically analyzes verbal indicators for deception. Burns et al. extracted LIWC indicators from transcriptions of a set of 911 calls <ref type="bibr" target="#b37">[37]</ref>. They fed these indicators as features to machine learning classifiers and obtained an accuracy of 84%. Burgoon et al. examined linguistic and acoustic features extracted from a company's quarterly conference call recordings using the Structured Programming for Linguistic Cue Extraction (SPLICE) toolkit <ref type="bibr" target="#b38">[38]</ref>. They analyzed the strategic and nonstrategic behaviors of deceivers by annotating utterances as prepared (presentation) and unprepared (Q&amp;A) responses and reported significant differences between these two, in terms of deceptive feature statistics. Larcker and Zakolyukina also applied linguistic analysis on conference call recordings from CEOs and CFOs and obtained significantly better deception prediction than a random guess <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>. Fuller et al. analyzed verbal cues developed by Zhou et al. <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref> and their revised framework using written statements prepared by suspects and victims of crimes on military bases <ref type="bibr" target="#b43">[43]</ref>. Braun et al. used LIWC indicators to investigate deceptive statements made by politicians labeled by editors of the politifact.com website and reported deceptive linguistic indicators in interactive and scripted settings separately <ref type="bibr" target="#b44">[44]</ref>.</p><p>While most of the data used in related research was collected under controlled settings, only a few works have explored the used of data from real-life scenarios. This can be partially attributed to the difficulty of collecting such data, as well as the challenges associated with verifying the deceptive or truthful nature of real-world data. To our knowledge, there is very little work focusing on real-life high-stake data. The work presented by <ref type="bibr" target="#b45">Vrij and Mann (2001)</ref> was the first study, to the best of our knowledge, on a real-life high-stake scenario including police interviews of murder suspects <ref type="bibr" target="#b45">[45]</ref> 1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.3015684, IEEE Transactions on Affective Computing public community for the return of a missing relative <ref type="bibr" target="#b11">[12]</ref>. The work closest to ours is presented by Fornaciari and Poesio <ref type="bibr" target="#b46">[46]</ref>, which targets the identification of deception in statements issued by witnesses and defendants using a corpus collected from hearings in Italian courts. Following this line of work, we present a study on deception detection using real-life trial data and explore the use of multiple modalities for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Non-verbal Deception Detection</head><p>Earlier approaches to non-verbal deception detection relied on polygraph tests to detect deceptive behavior. These tests are mainly based on physiological features such as heart rate, respiration rate, and skin temperature. Several studies <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b47">[47]</ref> indicated that relying solely on such physiological measurements can be biased and misleading. Chittaranjan et al. <ref type="bibr" target="#b48">[48]</ref> created audio-visual recordings of the "Are you a Werewolf?" game to detect deceptive behavior using non-verbal audio cues and to predict the subjects' decisions in the game. In order to improve lie detection in criminal-suspect interrogations, Sumriddetchkajorn and Somboonkaew <ref type="bibr" target="#b49">[49]</ref> developed an infrared system to detect lies by using thermal variations in the periorbital area and by deducing the respiration rate from the thermal nostril areas. Granhag and Hartwig <ref type="bibr" target="#b50">[50]</ref> proposed a methodology using psychologically informed mind-reading to evaluate statements from suspects, witnesses, and innocents. Facial expressions also play a critical role in the identification of deception. Ekman defined micro-expressions as relatively short involuntary expressions, which can be indicative of deceptive behavior <ref type="bibr" target="#b51">[51]</ref>. Moreover, these expressions were analyzed using smoothness and asymmetry measurements to further relate them to an act of deceit <ref type="bibr" target="#b52">[52]</ref>. Ekman and Rosenberg <ref type="bibr" target="#b53">[53]</ref> developed the Facial Action Coding System (FACS) to taxonomize facial expressions and gestures for emotion-and deceit-related applications. Bartlett et al. <ref type="bibr" target="#b54">[54]</ref> introduced a real-time system to identify deceptive behavior from facial expressions using FACS. Tian et al. <ref type="bibr" target="#b55">[55]</ref> considered features such as face orientation and facial expression intensity. Owayjan et al. <ref type="bibr" target="#b56">[56]</ref> extracted geometric-based features from facial expressions, and Pfister and Pietikainen <ref type="bibr" target="#b57">[57]</ref> developed a micro-expression dataset to identify expressions that are clues for deception. Blob analysis was used to detect deceit by tracking the hand movements of subjects and extracting color features using hierarchical Hidden Markov Model <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref>. Meservy et al. <ref type="bibr" target="#b60">[60]</ref> used individual frames as well as videos to extract geometric features related to the hand and head motion to identify deceptive behavior. Caso et al. <ref type="bibr" target="#b61">[61]</ref> identified particular hand gestures that can be related to an act of deception using data collected from simulated interviews including truthful and deceptive responses. Cohen et al. <ref type="bibr" target="#b62">[62]</ref> determined that fewer iconic hand gestures were a sign of a deceptive narration using data collected from participants with truthful and deceptive responses. To further analyze the characteristics of hand gestures, a taxonomy of such gestures was developed for multiple applications such as deception and social behaviour <ref type="bibr" target="#b63">[63]</ref>. Hillman et al. <ref type="bibr" target="#b64">[64]</ref> determined that increased speech prompting gestures were associated with deception while increased rhythmic pulsing gestures were associated with truthful behavior. Vrij and Mann analyzed visual and acoustic features on a dataset of police interviews of murder suspects and reported that convicted subjects "showed more gaze aversion, had longer pauses, spoke more slowly and made more non-ah speech disturbances" when lying than telling the truth <ref type="bibr" target="#b45">[45]</ref>. Ten Brinke et al. manually extracted codings depicting speech, body language and emotional facial expressions for a collection of televised footage in which individuals pleading to the public community for the return of a missing relative <ref type="bibr" target="#b11">[12]</ref>. They report informative codings that reflect deception, e.g. liars use fewer words but more tentative words.</p><p>Recently, features from different modalities were integrated to find a combination of multimodal features with superior performance <ref type="bibr" target="#b65">[65]</ref>, <ref type="bibr" target="#b66">[66]</ref>. An extensive review of approaches for evaluating human credibility using physiological, visual, acoustic, and linguistic features is available in <ref type="bibr" target="#b67">[67]</ref>. Burgoon et al. <ref type="bibr" target="#b65">[65]</ref> combined verbal and nonverbal features such as speech act profiling, feature mining, and kinetic analysis for improved deception detection rates. Jensen et al. <ref type="bibr" target="#b66">[66]</ref> extracted features from acoustic, verbal, and visual modalities following a multimodal approach. Mihalcea and Burzo <ref type="bibr" target="#b68">[68]</ref> developed a multimodal deception dataset composed of linguistic, thermal, and physiological features. Nunamaker et al. <ref type="bibr" target="#b67">[67]</ref> provided a review of approaches for evaluating human credibility using physiological, visual, acoustic, and linguistic features. A multimodal deception dataset consisting of linguistic, thermal, and physiological features was introduced in <ref type="bibr" target="#b69">[69]</ref>, which was then used to develop a multimodal deception detection system that integrated linguistic, thermal, and physiological features from human subjects to create a reliable deception detection system <ref type="bibr" target="#b70">[70]</ref>, <ref type="bibr" target="#b71">[71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">COMPARISON TO STATE-OF-ART</head><p>Our work extends the work of Pérez-Rosas et al., where the real-life trial dataset was first presented, together with a video-clip level deception detection system <ref type="bibr" target="#b8">[9]</ref>. iv) Different from the earlier work, our evaluations are conducted using 3 repetitions for each test sample in the leave-one-subject-out cross-validation, to obtain more robust results. Furthermore, we obtained both accuracy and AUC metrics. v) Finally, our work obtained improved results on the deception detection task (83.05% accuracy and 0.95 AUC with feature-level fusion and 84.18% accuracy and 0.94 AUC with score-level fusion), which are also more reliable due to the cross-validation settings.</p><p>Among the studies that report results on this database, Jaiswal et. al. used the OpenFace toolkit <ref type="bibr" target="#b72">[72]</ref> to extract visual features and the OpenSmile toolkit <ref type="bibr" target="#b73">[73]</ref> to extract acoustic features which are then fed to an SVM classifier <ref type="bibr" target="#b74">[74]</ref>. They report a 78.95% accuracy with feature-level fusion, after excluding videos (21 of 121) that are either too short or portray many people such that OpenFace is unable to recognize the subject.</p><p>Wu et. al. labeled short segments of video clips to train a micro-expression classifier whose outputs are fed to the deception classification system <ref type="bibr" target="#b75">[75]</ref>. They report that even though the micro-expression classifier has low performance, Authorized licensed use limited to: Middlesex University. Downloaded on September 05,2020 at 16:03:57 UTC from IEEE Xplore. Restrictions apply.</p><p>1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.3015684, IEEE Transactions on Affective Computing its output probabilities are useful to improve the performance of the overall system. They also use GloVe (Global Vectors for Word Representation) embeddings <ref type="bibr" target="#b76">[76]</ref> for the linguistic representation and MFCC features for the acoustic modality. They report an AUC score of 0.92 obtained with a Logistic Regression classifier on a subset of the dataset (104 videos), pruning videos with either significant scene change or human editing. The resulting semi-automatic system achieves an AUC score of 0.98 and an accuracy of 96.14%, thus obtaining the best results reported so far on this dataset. However as the authors also acknowledge, there is a possibility that the results may not generalize as well on larger datasets, due to overfitting or learning the idiosyncrasies of the small dataset.</p><p>Karimi et al. developed a multimodal deception detection system with automated features <ref type="bibr" target="#b77">[77]</ref>. They employ CNNs followed by a Long-Short Term Memory (LSTM) model to extract the temporal information in the visual and vocal input, along with an attention mechanism focusing on the frames that include visual cues of deception. Their system achieves an accuracy of 84.16% for video-level classification.</p><p>In summary, existing research on this dataset has approached the problem at the video-level only, obtaining classification performances ranging from 78% to 97%. However, the experimental evaluations are not fully compatible, it is difficult to compare their results directly. For instance, in some work, the videos where the subject is not clearly seen are removed from the dataset; and a subject-based cross validation is note performed in others.</p><p>Our results are also not directly comparable with the state-of-art since we detect deception at the subject-level rather than at the video-level. Nonetheless, our best figures obtained with the semi-automatic system (AUC of 0.9462 obtained with a feature-level combination and an AUC of 0.9323 obtained with a score-level combination of all modalities) are on par with the results of the semi-automatic system of <ref type="bibr" target="#b78">[78]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">CONCLUSIONS</head><p>In this paper, we presented a study of multimodal deception detection using real-life high-stake occurrences of deceit. We use a dataset from public real trials to perform both qualitative and quantitative experiments. We built classifiers relying on the individual or combined sets of verbal and non-verbal features and showed that a system using scorelevel combination can detect deceptive subjects with an accuracy of 84.18%. Our analysis of non-verbal behaviors occurring in deceptive and truthful videos brought insight into the gestures that play a role in deception. Additional analyses showed the role played by the various feature sets used in the experiments.</p><p>We also performed a study of the human ability to detect deception with single or multimodal data streams of real-life trial data. The study revealed high disagreement and low deception detection accuracies among human annotators. Our automatic system using all the modalities outperformed the average non-expert human performance by more than 6% points, and the lowest human annotator's performance by more than 11% points.</p><p>In the future, we will work on improving automatic gesture identification and automatic speech transcription, with the goal of taking steps towards a real-time deception detection system.</p><p>1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample screenshots showing facial displays and hand gestures from real-life trial clips. Starting at the top left-hand corner: deceptive trial with forward head movement (Move forward), deceptive trial with both hands movement (Both hands), deceptive trial with one hand movement (Single hand), truthful trial with raised eyebrows (Eyebrows raising), deceptive trial with scowl face (Scowl), and truthful trial with an up gaze (Gaze up).</figDesc><graphic url="image-1.png" coords="3,60.37,155.57,491.13,226.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Pitch standard deviation vs pitch mean by gender.</figDesc><graphic url="image-2.png" coords="5,61.73,43.70,221.87,173.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Histograms of speech and silence length (measured in seconds) using 25 bins. In all cases, the last bin contains speech or silence segments with duration greater than 3 seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visual feature importance for automatically extracted AU features.</figDesc><graphic url="image-3.png" coords="9,48.00,43.70,258.00,147.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. Ten Brinke et al. worked on a collection of televised footage from individuals pleading to the Authorized licensed use limited to: Middlesex University. Downloaded on September 05,2020 at 16:03:57 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.3015684, IEEE Transactions on Affective Computing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to 4.29 between 2000 and 2013. The National Registry of Exonerations reported on 873 exonerations from 1989 to 2012, with a tragedy behind each • Verónica Pérez-Rosas (corresponding author) and Rada Mihalcea are with the Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI. (Emails: {vrncapr, mihalcea}@umich.edu)</figDesc><table><row><cell>• Mohamed Abouelenien is with the Department of Computer and</cell></row><row><cell>Information Science, University of Michigan-Dearborn, Dearborn, MI.</cell></row><row><cell>(Email: {zmohamed}@umich.edu)</cell></row><row><cell>• Mihai Burzo is with the Department of Mechanical Engineering,</cell></row><row><cell>University of Michigan Flint, Flint, MI. (Email: {mburzo}@umich.edu)</cell></row></table><note>• M. Umut S ¸en and Berrin Yanikoglu are with the Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey. This work is partially supported by a T ÜB İTAK 2219 scholarship to B. Yanikoglu and 2211-A scholarship to M. Umut S ¸en. (Emails: {umutsen,berrin}@sabanciuniv.edu)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Distribution of gender in the two categories after aggregating individual videos.</figDesc><table><row><cell></cell><cell>Female</cell><cell>Male</cell><cell>Total</cell></row><row><cell>Deceptive</cell><cell>11</cell><cell>13</cell><cell>24</cell></row><row><cell>Truthful</cell><cell>12</cell><cell>23</cell><cell>35</cell></row><row><cell>Total</cell><cell>23</cell><cell>36</cell><cell>59</cell></row><row><cell cols="4">labeled as deceptive. Exoneration testimonies are collected</cell></row><row><cell>as truthful statements.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.3015684, IEEE Transactions on Affective Computing</figDesc><table><row><cell>IEEE TRANSACTIONS ON AFFECTIVE COMPUTING</cell><cell>3</cell></row></table><note>multi-1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Sample transcripts for deceptive and truthful clips in the dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 Gesture annotation agreement</head><label>3</label><figDesc></figDesc><table><row><cell>Gesture Category</cell><cell>Agreement</cell><cell>Kappa Score</cell></row><row><cell>General Facial Expressions</cell><cell>66.07%</cell><cell>0.328</cell></row><row><cell>Eyebrows</cell><cell>80.03%</cell><cell>0.670</cell></row><row><cell>Eyes</cell><cell>64.28%</cell><cell>0.465</cell></row><row><cell>Gaze</cell><cell>55.35%</cell><cell>0.253</cell></row><row><cell>Mouth Openness</cell><cell>78.57%</cell><cell>0.512</cell></row><row><cell>Mouth Lips</cell><cell>85.71%</cell><cell>0.690</cell></row><row><cell>Head Movements</cell><cell>69.64%</cell><cell>0.569</cell></row><row><cell>Hand Movements</cell><cell>94.64%</cell><cell>0.917</cell></row><row><cell>Hand Trajectory</cell><cell>82.14%</cell><cell>0.738</cell></row><row><cell>Average</cell><cell>75.16%</cell><cell>0.571</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Citation information: DOI 10.1109/TAFFC.2020.3015684, IEEE Transactions on Affective Computing Individual feature performance: accuracy (%) and AUC scores. Best results in each line are shown in bold.</figDesc><table><row><cell>IEEE TRANSACTIONS ON AFFECTIVE COMPUTING</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Early fusion results using individual best performing features: accuracy and AUC scores. Best results are shown in bold.</figDesc><table><row><cell>Modalities</cell><cell>SVM Accuracy</cell><cell>AUC</cell><cell>RF Accuracy</cell><cell>AUC</cell><cell>NN Accuracy</cell><cell>AUC</cell></row><row><cell>Facial Displays</cell><cell cols="2">76.27 ± 0.00 0.8581</cell><cell>76.84 ± 0.80</cell><cell cols="3">0.9270 80.79 ± 0.98 0.9416</cell></row><row><cell>+ Pitch (std-f 0 )</cell><cell>53.11 ± 5.45</cell><cell>0.7148</cell><cell>62.71 ± 1.69</cell><cell cols="3">0.6511 82.49 ± 0.98 0.9462</cell></row><row><cell>+ Pitch (std-f 0 ) + Sil.Sp.Hist.</cell><cell>68.93 ± 0.98</cell><cell cols="5">0.8585 66.10 ± 11.86 0.8649 83.05 ± 1.69 0.9166</cell></row><row><cell>+ All Acoustic</cell><cell>72.32 ± 0.98</cell><cell>0.8604</cell><cell>67.80 ± 2.94</cell><cell cols="3">0.8482 82.49 ± 0.98 0.9153</cell></row><row><cell>+ LIWC</cell><cell>54.24 ± 1.69</cell><cell>0.8173</cell><cell>63.84 ± 1.96</cell><cell cols="3">0.7778 75.14 ± 1.96 0.8961</cell></row><row><cell>+ Unigrams</cell><cell>55.93 ± 3.39</cell><cell>0.7928</cell><cell>63.28 ± 0.98</cell><cell cols="3">0.7310 78.53 ± 0.98 0.8903</cell></row><row><cell>+ Pitch (std-f 0 ) + Sil.Sp.Hist. + LIWC</cell><cell>53.67 ± 3.53</cell><cell>0.8281</cell><cell>65.54 ± 1.96</cell><cell cols="3">0.7852 75.14 ± 1.96 0.8780</cell></row><row><cell>+ All Acoustic + Unigrams</cell><cell>57.06 ± 0.98</cell><cell>0.8072</cell><cell>63.84 ± 3.53</cell><cell cols="3">0.7494 75.71 ± 0.98 0.8778</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Late fusion results using best performing features and different classifier weight combinations. Face refers to facial displays and pitch refers to std-f 0 . The results are obtained a posteriori and best results are shown in bold.</figDesc><table><row><cell>Score</cell><cell>+Acoustic</cell><cell>RF +Linguistic</cell><cell>+Acoustic +Linguistic</cell><cell>+Acoustic</cell><cell>NN +Linguistic</cell><cell>+Acoustic +Linguistic</cell></row><row><cell>w f ace = 1.0</cell><cell></cell><cell cols="2">76.84 ± 0.80</cell><cell></cell><cell cols="2">80.79 ± 0.98</cell></row><row><cell>w f ace = 0.9</cell><cell cols="2">77.40 ± 0.98 77.97 ± 1.69</cell><cell>77.40 ± 0.98</cell><cell cols="2">81.92 ± 0.98 83.05 ± 0.00</cell><cell>81.92 ± 0.98</cell></row><row><cell>w f ace = 0.8</cell><cell cols="2">77.40 ± 2.59 77.97 ± 1.69</cell><cell>77.40 ± 0.98</cell><cell cols="2">83.62 ± 1.96 79.66 ± 0.00</cell><cell>83.05 ± 0.00</cell></row><row><cell>w f ace = 0.7</cell><cell cols="2">79.10 ± 2.59 76.84 ± 0.98</cell><cell>78.53 ± 1.96</cell><cell cols="2">84.18 ± 0.98 80.79 ± 0.98</cell><cell>81.92 ± 1.96</cell></row><row><cell>w f ace = 0.6</cell><cell cols="2">76.84 ± 0.98 76.27 ± 0.00</cell><cell>76.84 ± 1.96</cell><cell cols="2">84.18 ± 1.96 79.66 ± 1.69</cell><cell>82.49 ± 0.98</cell></row><row><cell>w f ace = 0.5</cell><cell cols="2">76.27 ± 1.69 68.93 ± 2.59</cell><cell>74.01 ± 5.18</cell><cell cols="2">81.92 ± 0.98 78.53 ± 0.98</cell><cell>83.62 ± 1.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="4">Fully-automatic system: classification accuracies with individual and</cell></row><row><cell cols="3">combined modalities (%)</cell><cell></cell></row><row><cell>Modality</cell><cell>SVM</cell><cell>RF</cell><cell>NN</cell></row><row><cell>Visual (Action Units)</cell><cell>53.67%</cell><cell>61.58%</cell><cell>57.63%</cell></row><row><cell>All Acoustic</cell><cell>56.50%</cell><cell>63.28%</cell><cell>61.02%</cell></row><row><cell>Linguistic (Unigrams)</cell><cell>57.06%</cell><cell>63.28%</cell><cell>71.75%</cell></row><row><cell>All (Early Fusion)</cell><cell>58.76%</cell><cell>68.36%</cell><cell>70.06%</cell></row><row><cell>All (Combiner)</cell><cell>56.50%</cell><cell>63.28%</cell><cell>72.88%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.3015684, IEEE Transactions on Affective Computing of the audio track of the clip; Silent video, consisting of only the video with muted audio; and Full video where audio and video are played simultaneously.</figDesc><table /><note>Text, consisting of the language transcripts; Audio, consisting 1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8</head><label>8</label><figDesc>Agreement among three human annotators on text, audio, silent video, and full video modalities.</figDesc><table><row><cell>Modality</cell><cell></cell><cell cols="2">Agreement</cell><cell>Kappa</cell></row><row><cell>Text</cell><cell></cell><cell>30.76%</cell><cell></cell><cell>0.014</cell></row><row><cell>Audio</cell><cell></cell><cell>53.84%</cell><cell></cell><cell>0.040</cell></row><row><cell cols="2">Silent video</cell><cell>53.84%</cell><cell></cell><cell>0.040</cell></row><row><cell>Full video</cell><cell></cell><cell>53.84%</cell><cell></cell><cell>0.050</cell></row><row><cell></cell><cell></cell><cell>TABLE 9</cell><cell></cell></row><row><cell cols="5">Classification accuracy of three annotators (A1, A2, A3) and the</cell></row><row><cell cols="5">developed systems on the real-deception dataset over four modalities.</cell></row><row><cell></cell><cell>Text</cell><cell>Audio</cell><cell cols="2">Silent video Full video</cell></row><row><cell>A1</cell><cell cols="2">69.23% 69.23%</cell><cell>69.23%</cell><cell>61.53%</cell></row><row><cell>A2</cell><cell cols="2">53.84% 61.53%</cell><cell>61.53%</cell><cell>76.92%</cell></row><row><cell>A3</cell><cell cols="2">69.23% 76.92%</cell><cell>76.92%</cell><cell>76.92%</cell></row><row><cell>Average</cell><cell cols="2">64.10% 69.22%</cell><cell>69.22%</cell><cell>71.79%</cell></row><row><cell>Fully-autom. sys.</cell><cell cols="2">71.75% 63.28%</cell><cell>61.58%</cell><cell>72.88%</cell></row><row><cell>Semi-autom. sys.</cell><cell cols="2">64.41% 63.28%</cell><cell>80.79%</cell><cell>75.71%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">. http://www.innocenceproject.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">. As done in the Human Computer Interaction Community, "gesture" is used as a broad term that refers to body movements, including facial expressions and hand gestures.Authorized licensed use limited to: Middlesex University. Downloaded on September 05,2020 at 16:03:57 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Authorized licensed use limited to: Middlesex University. Downloaded on September 05,2020 at 16:03:57 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M. Umut Sen</head><p>Her research interests include machine learning, natural language processing, computational linguistics, affect recognition, and multimodal analysis of human behavior. Her research focuses on developing computational methods to analyze, recognize, and predict human affective responses during social interactions. She has authored papers in leading conferences and journals in Natural Language Processing and Computational linguistics, and served as a program committee member for multiple international journals and conferences in the same fields. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Berrin</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exonerations in the united states, 1989 -2012</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Warden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Registry of Exonerations</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detecting Lies and Deceit: The Psychology of Lying and the Implications for Professional Practice, ser. Wiley series in the psychology of crime</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<title level="m">Risk Assessment and the Polygraph</title>
				<imprint>
			<publisher>John Wiley and Sons Ltd</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="129" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Syntactic stylometry for deception detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2390665.2390708" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
	<note>ser. ACL &apos;12</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinguishing deceptive from non-deceptive speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Enos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gilman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Graciarena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kathol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Michaelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2005">2005 -Eurospeech, 2005</date>
			<biblScope unit="page" from="1833" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lying words: Predicting deception from linguistic styles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The lie detector: Explorations in the automatic recognition of deceptive language</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
				<meeting>the Association for Computational Linguistics<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human behaviour: Seeing through the face of deception</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="issue">6867</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deception detection using real-life trial data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
				<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The mumin coding scheme for the annotation of feedback, turn management and sequencing phenomena</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cerrato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jokinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Navarretta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paggio</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-007-9061-5</idno>
		<ptr target="http://dx.doi.org/10.1007/s10579-007-9061-5" />
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="273" to="287" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A rigidity detection system for the guilty knowledge test</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Twyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HICSS-44 Symposium on Credibility Assessment and Information Quality in Government and Business</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cry me a river: Identifying the behavioral consequences of extremely high-stakes interpersonal deception</title>
		<author>
			<persName><forename type="first">L</forename><surname>Brinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Law and Human Behavior</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">469</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cues to deception</title>
		<author>
			<persName><forename type="first">B</forename><surname>Depaulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Muhlenbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="page" from="74" to="118" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Linguistic inquiry and word count: LIWC</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Francis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>erlbaum Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Facial action coding system: The manual on cd rom</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="77" to="254" />
			<pubPlace>Salt Lake City</pubPlace>
		</imprint>
	</monogr>
	<note>A Human Face</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Openface 2.0: Facial behavior analysis toolkit</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pitch changes during attempted deception</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Streeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Krauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Geller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Apple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">345</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Some evidence for unconscious lie detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Brinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Carney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="page">0956797614524421</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Development of exploratory research tools based on tandem-straight</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Banno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings: APSIPA ASC 2009: Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference. Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference</title>
				<meeting>APSIPA ASC 2009: Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference. Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference</meeting>
		<imprint>
			<publisher>International Organizing Committee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-complexity variable frame rate analysis for speech recognition and voice activity detection</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lindberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="798" to="807" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cloud speech-to-text recognition</title>
		<ptr target="https://cloud.google.com/speech-to-text/" />
		<imprint>
			<biblScope unit="page" from="2019" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding deceptive opinion spam by any stretch of the imagination</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hancock</surname></persName>
		</author>
		<idno>ser. HLT &apos;11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Who can best catch a liar? a metaanalysis of individual differences in detecting deception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Custer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forensic Examiner</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="11" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<idno>version 7.10.0</idno>
		<title level="m">MATLAB</title>
				<meeting><address><addrLine>Natick, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>The MathWorks Inc</publisher>
			<date type="published" when="2010">R2010a. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Are computers effective lie detectors? a meta-analysis of linguistic cues to deception</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bland Ón-Gitlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Sporer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and social psychology Review</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="307" to="342" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Verification and implementation of language-based deception indicators in civil and criminal narratives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bachenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schonwetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008)</title>
				<meeting>the 22nd International Conference on Computational Linguistics (COLING 2008)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reading between the lines: linguistic cues to deception in online dating profiles</title>
		<author>
			<persName><forename type="first">C</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work, ser. CSCW &apos;10</title>
				<meeting>the 2010 ACM Conference on Computer Supported Cooperative Work, ser. CSCW &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/1718918.1718921</idno>
		<ptr target="http://doi.acm.org/10.1145/1718918.1718921" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dating deception: Gender, online dating, and exaggerated self-presentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guadagno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Okdie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kruse</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2011.11.010</idno>
		<ptr target="http://dx.doi.org/10.1016/j.chb.2011.11.010" />
	</analytic>
	<monogr>
		<title level="j">Comput. Hum. Behav</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="642" to="647" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Warrants and deception in computer mediated communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Warkentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cormier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM Cnference on Computer Supported Cooperative Work</title>
				<meeting>the 2010 ACM Cnference on Computer Supported Cooperative Work</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Explanations for the perpetration of and reactions to deception in a virtual community</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Joinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dietz-Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="289" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Guess who? an empirical study of gender deception and detection in computer-mediated communication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hollister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Society for Information Science and Technology</title>
				<meeting>the American Society for Information Science and Technology</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards a general rule for identifying deceptive opinion spam</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using deep linguistic features for finding deceptive opinion spam</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C12-2131" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Posters. Mumbai, India: The COLING 2012 Organizing Committee</title>
				<meeting>COLING 2012: Posters. Mumbai, India: The COLING 2012 Organizing Committee</meeting>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Seeing through deception: A computational approach to deceit detection in written communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Valencia-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cantos</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W12-0403" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Deception Detection</title>
				<meeting>the Workshop on Computational Approaches to Deception Detection<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-04">April 2012</date>
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic detection of deception in child-produced speech using syntactic complexity features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yancheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1093" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08">August 2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="944" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automated deception detection of 911 call transcripts</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Moffitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Security Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Which spoken language markers identify deception in high-stakes settings? evidence from earnings conference calls</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Mayew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Giboney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Elkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Moffitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spitzley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="157" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detecting deceptive discussions in conference calls</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Larcker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Zakolyukina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Accounting Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="495" to="540" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discussion of detecting deceptive discussions in conference calls</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bloomfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Accounting Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="541" to="552" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automating linguistics-based cues for detecting deception in textbased asynchronous computer-mediated communications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Twitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Group decision and negotiation</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="81" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A comparison of classification methods for predicting deception in computer-mediated communication</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="139" to="166" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An examination and validation of linguistic constructs for studying high-stakes deception</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Biros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Group Decision and Negotiation</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="117" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">His lips are moving: Pinocchio effect and other lexical indicators of political deceptions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Van Swol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discourse Processes</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Telling and detecting lies in a high-stake situation: the case of a convicted murderer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="203" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic deception detection in Italian court cases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fornaciari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="340" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Control and resistance in the psychology of lying</title>
		<author>
			<persName><forename type="first">M</forename><surname>Derksen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Psychology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="212" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Are you awerewolf? detecting deceptive roles and outcomes in a conversational role-playing game</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chittaranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2010-03">March 2010</date>
			<biblScope unit="page" from="5334" to="5337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Thermal analyzer enables improved lie detection in criminal-suspect interrogations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sumriddetchkajorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Somboonkaew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Newsroom: Defense &amp; Security</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A new theoretical perspective on deception detection: On the psychology of instrumental mindreading</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Granhag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hartwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology, Crime &amp; Law</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="200" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Telling Lies: Clues to Deceit in the Marketplace</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Politics and Marriage. Norton, W.W. and Company</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">EMOTIONS INSIDE OUT: 130 Years after Darwin&apos;s The Expression of the Emotions in Man and Animals</title>
		<author>
			<persName><forename type="first">E</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1000</biblScope>
			<biblScope unit="page" from="205" to="221" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Darwin, deception, and facial expression</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS), ser</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Series in Affective Science</title>
				<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Automatic recognition of facial actions in spontaneous expressions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lainscsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="22" to="35" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Facial expression analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Face Recognition</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="247" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The design and development of a lie detection system using facial micro-expressions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Owayjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kashour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alhaddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alsouki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 2nd International Conference on Advances in Computational Tools for Engineering Applications (ACTEA)</title>
				<imprint>
			<date type="published" when="2012-12">Dec 2012</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Electronic imaging &amp; signal processing automatic identification of facial clues to lies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE Newsroom</title>
		<imprint>
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Blob analysis of the head and hands: A method for deception detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Hawaii International Conference on System Sciences (HICSS&apos;05), ser. HICSS &apos;05. Washington</title>
				<meeting>the 38th Annual Hawaii International Conference on System Sciences (HICSS&apos;05), ser. HICSS &apos;05. Washington<address><addrLine>DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hmm-based deception recognition from visual cues</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
				<imprint>
			<date type="published" when="2005-07">2005. 2005. July 2005</date>
			<biblScope unit="page" from="824" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deception detection through automatic, unobtrusive analysis of nonverbal behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The impact of deception and suspicion on different hand movements</title>
		<author>
			<persName><forename type="first">L</forename><surname>Caso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maricchiolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonaiuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Nonverbal indicators of deception: How iconic gestures reveal thoughts that cannot be suppressed</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shovelton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semiotica</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="issue">182</biblScope>
			<biblScope unit="page" from="133" to="174" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Coding hand gestures: A reliable taxonomy and a multi-media support</title>
		<author>
			<persName><forename type="first">F</forename><surname>Maricchiolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gnisci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonaiuto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Behavioural Systems, ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Vinciarelli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Hoffmann</surname></persName>
		</editor>
		<editor>
			<persName><surname>Muller</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7403</biblScope>
			<biblScope unit="page" from="405" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The effect of deception on specific hand gestures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Legal and Criminological Psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="336" to="345" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Um ... they were wearing</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Detecting concealment of intent in transportation screening: A proof of concept</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Younger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="112" />
			<date type="published" when="2009-03">March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Automatic, multimodal evaluation of human interaction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Group Decision and Negotiation</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="367" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Establishing a foundation for automated human credibility screening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Twyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Proudfoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schuetzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Giboney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Intelligence and Security Informatics (ISI)</title>
				<imprint>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Towards multimodal deception detection -step 1: Building a collection of deceptive videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Multimodal Interaction, ser. ICMI &apos;12</title>
				<meeting>the 14th ACM International Conference on Multimodal Interaction, ser. ICMI &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A multimodal dataset for deception detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narvaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Language Resources and Evaluations (LREC 2014)</title>
				<meeting>the Conference on Language Resources and Evaluations (LREC 2014)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deception detection using a multimodal approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction, ser. ICMI &apos;14</title>
				<meeting>the 16th International Conference on Multimodal Interaction, ser. ICMI &apos;14<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Detecting deceptive behavior via integration of discriminative features from multiple modalities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1042" to="1055" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Openface: an open source facial behavior analysis toolkit</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Recent developments in opensmile, the munich open-source multimedia feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
				<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The truth and nothing but the truth: Multimodal analysis for deception detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabibu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)</title>
				<meeting>the 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Deception detection in videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subrahmanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04415</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Toward end-to-end deception detection in videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Data (Big Data</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1278" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">A deep learning approach for multimodal deception detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00344</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
