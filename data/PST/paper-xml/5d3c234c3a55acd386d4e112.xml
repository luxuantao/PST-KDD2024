<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative models for graph-based protein design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><surname>Ingraham</surname></persName>
							<email>ingraham@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vikas</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
							<email>vgarg@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<email>regina@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generative models for graph-based protein design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Engineered proteins offer the potential to solve many problems in biomedicine, energy, and materials science, but creating designs that succeed is difficult in practice. A significant aspect of this challenge is the complex coupling between protein sequence and 3D structure, with the task of finding a viable design often referred to as the inverse protein folding problem. In this work, we introduce conditional language models for protein sequences that directly condition on a graph specification of the target structure. Our approach efficiently captures the complex dependencies in proteins by focusing on those that are long-range in sequence but local in 3D space. Our framework improves in both speed and reliability over conventional and neural network-based approaches, and takes a step toward rapid and targeted biomolecular design with the aid of deep generative models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A central goal for computational protein design is to automate the invention of protein molecules with defined structural and functional properties. This field has seen tremendous progess in the past two decades <ref type="bibr" target="#b0">[1]</ref>, including the design of novel 3D folds <ref type="bibr" target="#b1">[2]</ref>, enzymes <ref type="bibr" target="#b2">[3]</ref>, and complexes <ref type="bibr" target="#b3">[4]</ref>. However, the current practice often requires multiple rounds of trial-and-error, with first designs frequently failing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Several of the challenges stem from the bottom-up nature of contemporary approaches that rely on both the accuracy of energy functions to describe protein physics as well as on the efficiency of sampling algorithms to explore the protein sequence and structure space.</p><p>Here, we explore an alternative, top-down framework for protein design that directly learns a conditional generative model for protein sequences given a specification of the target structure, which is represented as a graph over the residues (amino acids). Specifically, we augment the autoregressive self-attention of recent sequence models <ref type="bibr" target="#b6">[7]</ref> with graph-based descriptions of the 3D structure. By composing multiple layers of structured self-attention, our model can effectively capture higher-order, interaction-based dependencies between sequence and structure, in contrast to previous parameteric approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> that are limited to only the first-order effects.</p><p>The graph-structured conditioning of a sequence model affords several benefits, including favorable computational efficiency, inductive bias, and representational flexibility. We accomplish the first two by leveraging a well-evidenced finding in protein science, namely that long-range dependencies in sequence are generally short-range in 3D space <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. By making the graph and self-attention similarly sparse and localized in 3D space, we achieve computational scaling that is linear in sequence length. Additionally, graph structured inputs offer representational flexibility, as they accomodate both coarse, 'flexible backbone' (connectivity and topology) as well as fine-grained (precise atom locations) descriptions of structure.</p><p>We demonstrate the merits of our approach via a detailed empirical study. Specifically, we evaluate our model at structural generalization to sequences of protein folds that were outside of the training set. For fixed-backbone sequence design, our model achieves considerably improved statistical performance over a neural-network based model and also achieves higher accuracy and efficiency than Rosetta fixbb, a state-the-art program for protein design.</p><p>The rest of the paper is organized as follows. We first position our contributions with respect to the prior work in Section 1.1. We provide details on our methods, including structure representation, in Section 2. We introduce our Structured Transformer model in Section 2.2. The details of our experiments are laid out in Section 3, and the corresponding results that elucidate the merits of our approach are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Generative models for protein sequence and structure A number of works have explored the use of generative models for protein engineering and design <ref type="bibr" target="#b12">[13]</ref>. Recently <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref> proposed neural network-based models for sequences given 3D structure, where the amino acids are modeled independently of one another. <ref type="bibr" target="#b14">[15]</ref> introduced a generative model for protein sequences conditioned on a 1D, context-free grammar based specification of the fold topology. Multiple works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> have modeled the conditional distribution of single amino acids given surrounding structure and sequence context with convolutional neural networks. In contrast to these works, our model captures the joint distribution of the full protein sequence while grounding these dependencies in terms of long-range interactions arising from structure.</p><p>In parallel to the development of structure-based models, there has been considerable work on deep generative models for protein sequences in individual protein families <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. While useful, these methods presume the availability of a large number of sequences from a particular family, which are unavailable in the case of designing novel proteins that diverge significantly from natural sequences.</p><p>Several groups have obtained promising results using (unconditional) protein language models <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> to learn sequence representations that transfer well to supervised tasks. While serving different purposes, we emphasize that one advantage of conditional generative modeling is to facilitate adaptation to specific (and potentially novel) parts of structure space. Language models trained on hundreds of millions of evolutionary sequences are nevertheless 'semantically' bottlenecked by the much smaller number of evolutionary 3D folds (thousands) that these sequences represent. We propose evaluating protein language models with structure-based splitting of sequence data, and begin to see how unconditional language models may struggle to assign high likelihoods to sequences from out-of-training folds.</p><p>In a complementary line of research, several deep and differentiable parameterizations of protein structure <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref> have been recently proposed that could be used to generate, optimize, or validate 3D structures for input to sequence design.</p><p>Protein design and interaction graphs For classical approaches to computational protein design, which are based on joint modeling of structure and sequence, we refer the reader to a review of both methods and accomplishments in <ref type="bibr" target="#b0">[1]</ref>. Many of the major 'firsts' in protein design are due to Rosetta <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, a leading framework for protein design. More recently, there have been successes with non-parametric approaches to protein design <ref type="bibr" target="#b31">[32]</ref> that are based on finding substructural homologies between the target and diverse templates in large protein database. We will focus on comparisons to Rosetta, since it is based on a shared parametric energy function that captures the sequence-structure relationship.</p><p>Self-Attention Our model extends the Transformer <ref type="bibr" target="#b32">[33]</ref> to capture sparse, pairwise relational information between sequence elements. The dense variation of this problem was explored in <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b34">[35]</ref>. As noted in those works, incorporating general pairwise information incurs O(N 2 ) memory (and computational) cost for sequences of length N , which can be highly limiting for training on GPUs. We circumvent this cost by instead restricting the self-attention to the sparsity of the input graph. Given this graph-structured self-attention, our model may also be reasonably cast in the framework of message-passing or graph neural networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Our approach is similar to Graph Attention Networks <ref type="bibr" target="#b37">[38]</ref>, but augmented with edge features and an autoregressive decoder.  We cast protein design as language modeling conditioned on an input graph. In our architecture, am encoder develops sequence-independent, contextual embeddings of each residue in the 3D structure with multi-head self-attention <ref type="bibr" target="#b6">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive decoder then predicts amino acid s i given the full structure and previously decoded amino acids. (B) Each layer interleaves local neighborhood aggregation with position-wise feedforward sub-layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this work, we introduce a Structured Transformer model that draws inspiration from the selfattention based Transformer model <ref type="bibr" target="#b6">[7]</ref> and is augmented for scalable incorporation of relational information (Figure <ref type="figure" target="#fig_1">1</ref>). While general relational attention incurs quadratic memory and computation costs, we avert these by restricting the attention for each node i to the set N(i, k) of its k-nearest neighbors in 3D space. Since our architecture is multilayered, iterated local attention can derive progressively more global estimates of context for each node i. Second, unlike the standard Transformer, we also include edge features to embed the spatial and positional dependencies in deriving the attention. Thus, our model generalizes Transformer to spatially structured settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Representing structure as a graph</head><p>We represent protein structure in terms of an attributed graph G = (V, E) with node features V = {v 1 , . . . , v N } describing each residue (amino acid, which are the letters which compose a protein sequence) and edge features E = {e ij } i =j capturing relationships between them. This formulation can accommodate different variations on the macromolecular design problem, including both the 'rigid backbone' design where the precise coordinates of backbone atoms are fixed, as well as the 'flexible backbone' design where softer constraints such as blueprints of hydrogen-bonding connectivity <ref type="bibr" target="#b4">[5]</ref> or 1D architectures <ref type="bibr" target="#b14">[15]</ref> could define the structure of interest.</p><p>3D considerations For a rigid-body design problem, the structure for conditioning is a fixed set of backbone coordinates</p><formula xml:id="formula_0">X = {x i ∈ R 3 : 1 ≤ i ≤ N },</formula><p>where N is the number of positions <ref type="foot" target="#foot_1">1</ref> . We desire a graph representation of the coordinates G(X ) that has two properties:</p><p>• Invariance. The features are invariant to rotations and translations.</p><p>• Locally informative. The edge features incident to v i due to its neighbors N(i, k), i.e. {e ij } j∈N(i,k) , contain sufficient information to reconstruct all adjacent coordinates {x j } j∈N(i,k) up to rigid-body motion.</p><p>While invariance is motivated by standard symmetry considerations, the second property is motivated by limitations of current graph neural networks <ref type="bibr" target="#b35">[36]</ref>. In these networks, updates to node features v i depend only on the edge and node features adjacent to v i . However, typically, these features are insufficient to reconstruct the relative neighborhood positions {x j } j∈N(i,k) , so individual updates cannot fully depend on the 'local environment'. For example, pairwise distances D ij and D il are insufficient to determine if x j and x l are on the same or opposite sides of x i .</p><p>Relative spatial encodings We develop invariant and locally informative features by first augmenting the points x i with 'orientations' O i that define a local coordinate system at each point (Figure <ref type="figure" target="#fig_2">2</ref>). We define these in terms of the backbone geometry as</p><formula xml:id="formula_1">O i = [b i n i b i × n i ] ,</formula><p>where b i is the negative bisector of angle between the rays (x i−1 − x i ) and (x i+1 − x i ), and n i is a unit vector normal to that plane. Formally, we have</p><formula xml:id="formula_2">u i = x i − x i−1 ||x i − x i−1 || , b i = u i − u i+1 ||u i − u i+1 || , n i = u i × u i+1 ||u i × u i+1 || .</formula><p>Finally, we derive the spatial edge features e (s) ij from the rigid body transformation that relates reference frame (x i , O i ) to reference frame (x j , O j ). While this transformation has 6 degrees of freedom, we decompose it into features for distance, direction, and orientation as</p><formula xml:id="formula_3">e (s) ij = r (||x j − x i ||) , O T i x j − x i ||x j − x i || , q O T i O j .</formula><p>Here the first vector is a distance encoding r(•) lifted into a radial basis<ref type="foot" target="#foot_2">2</ref> , the second vector is a direction encoding that corresponds to the relative direction of x j in the reference frame of (x i , O i ), and the third vector is an orientation encoding q(•) of the quaternion representation of the spatial rotation matrix O T i O j . Quaternions represent 3D rotations as four-element vectors that can be efficiently and reasonably compared by inner products <ref type="bibr" target="#b38">[39]</ref>. <ref type="foot" target="#foot_3">3</ref>Relative positional encodings Taking a cue from the original Transformer model, we obtain positional embeddings e (p) ij that encode the role of local structure around node i. Specifically, we need to model the positioning of each neighbor j relative to the node under consideration i. Therefore, we obtain the position embedding as a sinusoidal function of the gap i − j. We retain the sign of the distance i − j because protein backbones are asymmetric. These relative distances contrast the absolute positional encodings of the original Transformer, and instead matches the relative encodings in <ref type="bibr" target="#b33">[34]</ref>.</p><p>Node and edge features Finally, we obtain an aggregate edge encoding vector e ij by concatenating the structural encodings e (s) ij and the positional encodings e (p) ij and then linearly transforming them to have the same dimension as the model. We only include edges in the k-nearest neighbors graph of X , with k = 30 for all experiments. This k is generous, as typical definitions of residue-residue contacts in proteins will result in &lt; 20 contacts per residue. For node features, we compute the three dihedral angles of the protein backbone (φ i , ψ i , ω i ) and embed these on the 3-torus as {sin, cos}×(φ i , ψ i , ω i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flexible backbone features</head><p>We also consider 'flexible backbone' descriptions of 3D structure based on topological binary edge features and coarse backbone geometry. We combine the relative positional encodings with two binary edge features: contacts that indicate when the distance between C α residues at i and j are less than 8 Angstroms and hydrogen bonds which are directed and defined by the electrostatic model of DSSP <ref type="bibr" target="#b40">[41]</ref>. For coarse node features, we compute virtual dihedral angles and bond angles between backbone C α residues, interpret them as spherical coordinates, and represent them as points on the unit sphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structured Transformer</head><p>Autoregressive decomposition We decompose the joint distribution of the sequence given structure p(s|x) autoregressively as</p><formula xml:id="formula_4">p(s|x) = i p(s i |x, s &lt;i ),</formula><p>where the conditional probability p(s i |x, s &lt;i ) of amino acid s i at position i is conditioned on both the input structure x and the preceding amino acids s &lt;i = {s 1 , . . . s i−1 }. <ref type="foot" target="#foot_4">4</ref> These conditionals are parameterized in terms of two sub-networks: an encoder that computes refined node embeddings from structure-based node features V(x) and edge features E(x) and a decoder that autoregressively predicts letter s i given the preceding sequence and structural embeddings from the encoder (Figure <ref type="figure" target="#fig_1">1</ref>).</p><p>Encoder Our encoder module is designed as follows. A transformation W h : R dv → R d produces initial embeddings h i = W h (v i ) from the node features v i pertaining to position i ∈ [N ] {1, 2, . . . , N }.</p><p>Each layer of the encoder implements a multi-head self-attention component, where head ∈ [L] can attend to a separate subspace of the embeddings via learned query, key and value transformations <ref type="bibr" target="#b6">[7]</ref>.</p><p>The queries are derived from the current embedding at node i while the keys and values from the relational information r ij = (h j , e ij ) at adjacent nodes j ∈ N (i, k). Specifically, W ( ) q maps h i to query embeddings q </p><formula xml:id="formula_5">( ) i , W</formula><formula xml:id="formula_6">a ( ) ij = exp(m ( ) ij ) j ∈N(i,k) exp(m ( ) ij )</formula><p>, where m</p><formula xml:id="formula_7">( ) ij = q ( ) i z ( ) ij √ d .</formula><p>The results of each attention head l are collected as the weighted sum h </p><formula xml:id="formula_8">( ) i = j∈N (i,k) a ( ) ij v ( ) ij ,</formula><formula xml:id="formula_9">∆h i = W o Concat h<label>(1)</label></formula><p>i , . . . , h (L) i</p><p>.</p><p>We update the embeddings with this residual, and alternate between these self-attention layers and position-wise feedforward layers as in the original Transformer <ref type="bibr" target="#b6">[7]</ref>. We stack multiple layers atop each other, and thereby obtain continually refined embeddings as we traverse the layers bottom up.</p><p>The encoder yields the embeddings produced by the topmost layer as its output.</p><p>Decoder Our decoder module has the same structure as the encoder but with augmented relational information r ij that allows access to the preceding sequence elements s &lt;i in a causally consistent manner. In contrast to the encoder, where the keys and values are based on the relational information r ij = (h j , e ij ), the decoder can additionally access sequence elements s j as</p><formula xml:id="formula_10">r (dec) ij = (h (dec) j , e ij , g(s j )) i &gt; j (h (enc) j , e ij , 0) i ≤ j .</formula><p>Here h</p><p>(dec) j</p><p>is the embedding of node j in the current layer of the decoder, h</p><p>is the embedding of node j in the final layer of the encoder, and g(s j ) is a sequence embedding of amino acid s j at node j. This concatenation and masking structure ensures that sequence information only flows to position i from positions j &lt; i, but still allows position i to attend to subsequent structural information unlike the standard Transformer decoder.</p><p>We now demonstrate the merits of our approach via a detailed empirical analysis. We begin with the experimental set up including our architecture, and description of the data used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training</head><p>Architecture In all experiments, we used three layers of self-attention and position-wise feedforward modules for the encoder and decoder with a hidden dimension of 128.</p><p>Optimization We trained models using the learning rate schedule and initialization of the original Transformer paper <ref type="bibr" target="#b6">[7]</ref>, a dropout rate of 10% <ref type="bibr" target="#b41">[42]</ref>, a label smoothing rate of 10%, and early stopping based on validation perplexity. The unconditional language models did not include dropout or label smoothing.</p><p>Dataset To evaluate the ability of our models to generalize across different protein folds, we collected a dataset based on the CATH hierarchical classification of protein structure <ref type="bibr" target="#b39">[40]</ref>. For all domains in the CATH 4.2 40% non-redundant set of proteins, we obtained full chains up to length 500 and then randomly assigned their CATH topology classifications (CAT codes) to train, validation and test sets at a targeted 80/10/10 split. Since each chain can contain multiple CAT codes, we first removed any redundant entries from train and then from validation. Finally, we removed any chains from the test set that had CAT overlap with train and removed chains from the validation set with CAT overlap to train or test. This resulted in a dataset of 18024 chains in the training set, 608 chains in the validation set, and 1120 chains in the test set. There is zero CAT overlap between these sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>A challenge in evaluating computational protein design methods is the degeneracy of the relationship between protein structure and sequence. Many protein sequences may reasonably design the same 3D structure <ref type="bibr" target="#b42">[43]</ref>, meaning that sequence similarity need not necessarily be high. At the same time, single mutations may cause a protein to break or misfold, meaning that high sequence similarity isn't sufficient for a correct design. To deal with this, we will focus on three kinds of evaluation: (i) likelihood-based, where we test the ability of the generative model to give high likelihood to held out sequences, (ii) native sequence recovery, where we evaluate generated sequences vs the native sequences of templates, and (iii) experimental comparison, where we compare the likelihoods of the model to high-throughput data from a de novo protein design experiment.</p><p>We find that our model is able to attain considerably improved statistical performance in its likelihoods while simultaneously providing more accurate and efficient sequence recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Statistical comparison to likelihood-based models</head><p>Protein perplexities What kind of perplexities might be useful? To provide context, we first present perplexities for some simple models of protein sequences in Table <ref type="table" target="#tab_0">1</ref>. The amino acid alphabet and its natural frequencies upper-bound perplexity at 20 and ∼17.8, respectively. Random protein sequences under these null models are unlikely to be functional without further selection <ref type="bibr" target="#b43">[44]</ref>. First order profiles of protein sequences such as those from the Pfam database <ref type="bibr" target="#b44">[45]</ref>, however, are widely used for protein engineering. We found the average perplexity per letter of profiles in Pfam 32 (ignoring alignment uncertainty) to be ∼11.6. This suggests that even models with high perplexities of this order have the potential to be useful models for the space of functional protein sequences.</p><p>The importance of structure We found that there was a significant gap between unconditional language models of protein sequences and models conditioned on structure. Remarkably, for a range of structure-independent language models, the typical test perplexities are ∼16-17 (Table <ref type="table" target="#tab_1">2</ref>), which were barely better than null letter frequencies (Table <ref type="table" target="#tab_0">1</ref>). We emphasize that the RNNs were not broken and could still learn the training set in these capacity ranges. All structure-based models had (unsurprisingly) considerably lower perplexities. In particular, our Structured Transformer model attained a perplexity of ∼7 on the full test set. It would seem that protein language models trained on one subset of 3D folds (in our cluster-splitting procedure) generalize poorly to predict the sequences of unseen folds, which is important to consider when training protein language models for protein engineering and design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvement over deep profile-based methods</head><p>We also compared to a recent method SPIN2 that predicts, using deep neural networks, protein sequence profiles given protein structures <ref type="bibr" target="#b7">[8]</ref>. Since SPIN2 is computationally intensive (minutes per protein for small proteins) and was trained on complete proteins rather than chains, we evaluated it on two subsets of the full test set: a a 'Small' subset of the test set containing chains up to length 100 and a 'Single chain' subset containing only those models where the single chain accounted for the entire protein record in the Protein Data Bank. Both subsets discarded any chains with structural gaps (chain break). We found that our Structured Transformer model significantly improved upon the perplexities of SPIN2 (Table <ref type="table" target="#tab_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph representations and attention mechanisms</head><p>The graph-based formulation of protein design can accommodate very different formulations of the problem depending on how structure is represented by a graph. We tested different approaches for representing the protein including both more 'rigid' design with precise geometric details, and 'flexible' topological design based on spatial contacts and hydrogen bonding (Table <ref type="table" target="#tab_2">3</ref>). For the best perplexities, we found that using local orientation information was indeed important above simple distance measures. At the same time, even the topological features were sufficient to obtain better perplexities than SPIN2 (Table <ref type="table" target="#tab_1">2</ref>), which uses precise atomic details.</p><p>In addition to varying the graph features, we also experimented with an alternative aggregation function from message passing neural networks <ref type="bibr" target="#b35">[36]</ref>. <ref type="foot" target="#foot_5">5</ref> We found that a simple aggregation function ∆h i = j MLP(h j , h j , e ij ) led to the best performance of all models, where MLP(•) is a two layer perceptron that preserves the hidden dimension of the model. We speculate that this is due to potential overfitting by the attention mechanism. Although all following experiments still use multi-head self-attention and full rigid backbone features, this suggests room future improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmarking protein redesign</head><p>Decoding strategies Generating protein sequence designs requires a sampling scheme for drawing high-likelihood sequences from the model. While beam-search or top-k sampling <ref type="bibr" target="#b45">[46]</ref> are commonly used heuristics for decoding, we found that simple biased sampling from the temperature adjusted distributions p (T ) (s|x) = i p(si|x,s&lt;i) 1/T a p(a|x,s&lt;i) 1/T was sufficient for obtaining sequences with higher likelihoods than native. We used a temperature of T = 0.1 selected from sequence recovery on validation. For conditional redesign of a subset of positions in a protein, we speculate that the likelihood calculation is sufficiently fast such that MCMC-based approaches such as Gibbs sampling may be feasible. Table <ref type="table">5</ref>: Structure-conditioned likelihoods correlate with mutation effects in de novo-designed miniproteins. Shown are Pearson correlation coefficients (R, higher is better) between the loglikelihoods of mutated sequences and high-throughput mutation effect data from a systematic design of miniproteins <ref type="bibr" target="#b5">[6]</ref>. Each design (column) includes 775 experimentally tested mutant protein sequences. Comparison to Rosetta To evaluate the performance of our model at generating realistic protein sequences, we performed two experiments that compare with Rosetta <ref type="bibr" target="#b29">[30]</ref>, a state-of-the-art framework for computational protein design. We found that our model was more accurate and significantly faster than Rosetta (Table <ref type="table" target="#tab_3">4</ref>). In the first, we used the latest version of Rosetta (3.10) to design sequences for our 'Single chain' test set with the fixbb fixed-backbone design protocol and default parameters (Table <ref type="table" target="#tab_3">4</ref>, a). In the second, we also compared to a prior benchmark from members of the Rosetta community <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> across 40 diverse proteins. For this set of proteins, we re-split our dataset to form new training and validation sets with no CAT overlap to the 40 templates for design. Although this reduced the size of the training set from ∼18,000 to ∼10,000 chains, we found our model to be both more accurate than and several orders of magnitude faster than Rosetta (Table <ref type="table" target="#tab_3">4</ref>, b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unsupervised anomaly detection for experimental protein design</head><p>While synthesis and testing of designed sequences is the gold standard of evaluating protein design methods we can measure what our structure-conditioned language model 'knows' about protein function by comparing the likelihoods it assigns to functional and non-functional mutant proteins from recent high-throughput design experiments as a kind of unsupervised anomaly detection <ref type="bibr" target="#b17">[18]</ref>. We compare to a recent high-throughput design and mutagenesis experiment in which several de novo designed mini-proteins were subject to systematic mutagenesis to all possible point mutations <ref type="bibr" target="#b5">[6]</ref>. We find that the log-likelihoods of our model non-trivially reflect mutational preferences of designed proteins (Table <ref type="table">5</ref>). Importantly, we see that the performance is not dependent on precise 3D geometric features (e.g. distances and orientations) but can also be realized with coarse information (e.g. contacts, hydrogen bonds, and coarse backbone angles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a new deep generative model to 'design' protein sequences given a graph specification of their structure. Our model augments the traditional sequence-level self-attention of Transformers <ref type="bibr" target="#b6">[7]</ref> with relational 3D structural encodings and is able to leverage the spatial locality of dependencies in molecular structures for efficient computation. When evaluated on unseen folds, the model achieves significantly improved perplexities over recent neural network-based generative models and more accurate and efficient sequence generation than the state-of-art program Rosetta.</p><p>Our framework suggests the possibility of being able to efficiently design and engineer protein sequences with structurally-guided deep generative models, and underscores the central role of modeling sparse long-range dependencies in biological sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A graph-based, autoregressive model for protein sequences given 3D structures. (A)We cast protein design as language modeling conditioned on an input graph. In our architecture, am encoder develops sequence-independent, contextual embeddings of each residue in the 3D structure with multi-head self-attention<ref type="bibr" target="#b6">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive decoder then predicts amino acid s i given the full structure and previously decoded amino acids. (B) Each layer interleaves local neighborhood aggregation with position-wise feedforward sub-layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Spatial features capture structural relationships across diverse folds. (A) For rigidbody protein design, we develop spatial relations based on the relative distances, directions, and orientations between between local reference frames (x i , O i ) and (x j , O j ) at different positions in the protein backbone. We achieve efficient scaling to large proteins by computing a k-Nearest Neighbors graph from Euclidean distances (right, top) and restrict all subsequent computation, such as relative directions (right, bottom), to this graph. (B) Example of topological variation in the dataset. Protein chains in train, test, and validation are split by the sub-chain CATH [40] topologies, which means that folds in each set will have distinct 2D patterns of connectivity.</figDesc><graphic url="image-42.png" coords="4,294.13,76.32,160.05,130.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>z</head><label></label><figDesc>maps pairs r ij to key embeddings z ( ) ij for j ∈ N(i, k), and W ( ) v maps the same pairs r ij to value embeddings v ( ) ij for each i ∈ [N ], ∈ [L]. Decoupling the mappings for keys and values allows each to depend on different subspaces of the representation.We compute the attention a as a function of their scaled inner product:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Null perplexities for common statistical models of proteins.</figDesc><table><row><cell>Null model</cell><cell>Perplexity</cell><cell>Conditioned on</cell></row><row><cell>Uniform</cell><cell>20.00</cell><cell>-</cell></row><row><cell>Natural frequencies</cell><cell>17.83</cell><cell>Random position in a natural protein</cell></row><row><cell>Pfam HMM profiles</cell><cell>11.64</cell><cell>Specific position in a specific protein family</cell></row><row><cell cols="3">and then concatenated and transformed to give the update</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Per-residue perplexities for protein language modeling (lower is better). The protein chains have been cluster-split by CATH topology, such that test includes only unseen 3D folds. While a structure-conditioned language model can generalize in this structure-split setting, unconditional language models struggle.</figDesc><table><row><cell>Test set</cell><cell cols="2">Short Single chain</cell><cell>All</cell></row><row><cell>Structure-conditioned models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Structured Transformer (ours)</cell><cell>8.54</cell><cell>9.03</cell><cell>6.85</cell></row><row><cell>SPIN2 [8]</cell><cell>12.11</cell><cell>12.61</cell><cell>-</cell></row><row><cell>Language models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM (h = 128)</cell><cell>16.06</cell><cell>16.38</cell><cell>17.13</cell></row><row><cell>LSTM (h = 256)</cell><cell>16.08</cell><cell>16.37</cell><cell>17.12</cell></row><row><cell>LSTM (h = 512)</cell><cell>15.98</cell><cell>16.38</cell><cell>17.13</cell></row><row><cell>Test set size</cell><cell>94</cell><cell>103</cell><cell>1120</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation of graph features and model components. Test perplexities (lower is better).</figDesc><table><row><cell>Node features</cell><cell>Edge features</cell><cell cols="3">Aggregation Short Single chain</cell><cell>All</cell></row><row><cell>Rigid backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dihedrals</cell><cell>Distances, Orientations</cell><cell>Attention</cell><cell>8.54</cell><cell>9.03</cell><cell>6.85</cell></row><row><cell>Dihedrals</cell><cell>Distances, Orientations</cell><cell>PairMLP</cell><cell>8.33</cell><cell>8.86</cell><cell>6.55</cell></row><row><cell>C α angles</cell><cell>Distances, Orientations</cell><cell>Attention</cell><cell>9.16</cell><cell>9.37</cell><cell>7.83</cell></row><row><cell>Dihedrals</cell><cell>Distances</cell><cell>Attention</cell><cell>9.11</cell><cell>9.63</cell><cell>7.87</cell></row><row><cell>Flexible backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C α angles</cell><cell>Contacts, Hydrogen bonds</cell><cell>Attention</cell><cell>11.71</cell><cell>11.81</cell><cell>11.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Improved reliability and speed compared to Rosetta. (a) Our model more accurately recovers native sequences than Rosetta fixbb (median recovery across 103 templates, 100 designs per) with greater speed (CPU: single core of Intel Xeon Gold 5115, GPU: NVIDIA RTX 2080). This set includes traditionally difficult templates based on NMR structures. (b) Evaluation with a prior benchmark of 40 structures, 100 designs per structure. Average of 4 trials ±2 standard deviations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ββαββ 37 ββαββ 1498 ββαββ 1702 ββαββ 1716 αββα 779</figDesc><table><row><cell>Rigid backbone</cell><cell>0.47</cell><cell>0.45</cell><cell>0.12</cell><cell>0.47</cell><cell>0.57</cell></row><row><cell>Flexible backbone</cell><cell>0.50</cell><cell>0.44</cell><cell>0.17</cell><cell>0.40</cell><cell>0.56</cell></row><row><cell>Design</cell><cell cols="5">αββα 223 αββα 726 αββα 872 ααα 134 ααα 138</cell></row><row><cell>Rigid backbone</cell><cell>0.36</cell><cell>0.11</cell><cell>0.21</cell><cell>0.24</cell><cell>0.33</cell></row><row><cell>Flexible backbone</cell><cell>0.33</cell><cell>0.21</cell><cell>0.23</cell><cell>0.36</cell><cell>0.41</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="33" xml:id="foot_0">33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">Here we consider a single representative coordinate per position when deriving edge features but may revisit multiple atom types per position for features such as backbone angles or hydrogen bonds.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">We used 16 Gaussian RBFs isotropically spaced from 0 to 20 Angstroms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">We represent quaternions in terms of their vector of real coefficients.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">We anticipate that alternative orderings for decoding the sequence may be favorable but leave this to future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">We thank one of our reviewers for this suggestion.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank members of the MIT MLPDS consortium, the MIT NLP group, and the reviewers for helpful feedback. This work was supported by the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The coming of age of de novo protein design</title>
		<author>
			<persName><forename type="first">Po-Ssu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Boyken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">537</biblScope>
			<biblScope unit="issue">7620</biblScope>
			<biblScope unit="page">320</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Design of a novel globular protein fold with atomic-level accuracy</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kuhlman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Dantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Gregory C Ireton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><forename type="middle">L</forename><surname>Varani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Stoddard</surname></persName>
		</author>
		<author>
			<persName><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">302</biblScope>
			<biblScope unit="issue">5649</biblScope>
			<biblScope unit="page" from="1364" to="1368" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computational design of an enzyme catalyst for a stereoselective bimolecular diels-alder reaction</title>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">B</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Zanghellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helena</forename><forename type="middle">M</forename><surname>Lovick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><forename type="middle">R</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>St Clair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><forename type="middle">L</forename><surname>Gallaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Hilvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><forename type="middle">L</forename><surname>Michael H Gelb</surname></persName>
		</author>
		<author>
			<persName><surname>Stoddard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">329</biblScope>
			<biblScope unit="issue">5989</biblScope>
			<biblScope unit="page" from="309" to="313" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate design of megadalton-scale two-component icosahedral protein complexes</title>
		<author>
			<persName><forename type="first">Shane</forename><surname>Jacob B Bale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sheffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chantz</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duilio</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">O</forename><surname>Cascio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamir</forename><surname>Yeates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">P</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">6297</biblScope>
			<biblScope unit="page" from="389" to="394" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Principles for designing ideal protein structures</title>
		<author>
			<persName><forename type="first">Nobuyasu</forename><surname>Koga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rie</forename><surname>Tatsumi-Koga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas B Acton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gaetano T Montelione</surname></persName>
		</author>
		<author>
			<persName><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">491</biblScope>
			<biblScope unit="issue">7423</biblScope>
			<biblScope unit="page">222</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global analysis of protein folding using massively parallel design, synthesis, and testing</title>
		<author>
			<persName><surname>Gabriel J Rocklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tamuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inna</forename><surname>Chidyausiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Goreshnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Houliston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Lemak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Vikram K Mulligan</surname></persName>
		</author>
		<author>
			<persName><surname>Chevalier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">6347</biblScope>
			<biblScope unit="page" from="168" to="175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spin2: Predicting sequence profiles from protein structures using deep neural networks</title>
		<author>
			<persName><forename type="first">O'</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiu</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhys</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldip</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdollah</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuedong</forename><surname>Dehzangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="629" to="633" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computational protein design with deep learning neural networks</title>
		<author>
			<persName><forename type="first">Jingxue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huali</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Zh</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6349</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Protein 3d structure computed from evolutionary sequence variation</title>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e28766</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Direct-coupling analysis of residue coevolution captures native contacts across many protein families</title>
		<author>
			<persName><forename type="first">Faruck</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Lunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bertolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Onuchic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName><surname>Weigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="E1293" to="E1301" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning generative models for protein fold families</title>
		<author>
			<persName><forename type="first">Sivaraman</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hetunandan</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Langmead</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1061" to="1078" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frances</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10775</idno>
		<title level="m">Machine learning in protein engineering</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">To improve protein sequence profile prediction through image captioning on pairwise residue distance map</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuedong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">628917</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Design of metalloproteins and novel protein folds using variational autoencoders</title>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Joe G Greener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16189</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spherical convolutions and their application in molecular modelling</title>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jes</forename><surname>Frellsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3433" to="3443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d steerable cnns: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10402" to="10413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep generative models of genetic variation capture the effects of mutations</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Adam J Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="816" to="822" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Variational auto-encoding of protein sequences</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Sinai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kelsic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Nowak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03346</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning protein constitutive motifs from sequence data</title>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Tubiana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simona</forename><surname>Cocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Monasson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08718</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accelerating protein design using autoregressive generative models</title>
		<author>
			<persName><forename type="first">Jung-Eun</forename><surname>Adam J Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Kollasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elana</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aashish</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Manglik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">757252</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-only deep representation learning</title>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">589333</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling the language of life-deep learning protein sequences</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Nachaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Matthes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">614313</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative modeling for protein structures</title>
		<author>
			<persName><forename type="first">Namrata</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Possu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7505" to="7516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fully differentiable full-atom protein backbone generation</title>
		<author>
			<persName><forename type="first">Namrata</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Ssu</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning protein structure with a differentiable simulator</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end differentiable learning of protein structure</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">265231</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rosetta3: an object-oriented software suite for the simulation and design of macromolecules</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Leaver-Fay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tyka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><forename type="middle">F</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Jacak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><forename type="middle">W</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Douglas Renfrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Sheffler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methods in enzymology</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">487</biblScope>
			<biblScope unit="page" from="545" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The rosetta all-atom energy function for macromolecular modeling and design</title>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">F</forename><surname>Alford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Leaver-Fay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeliazko</forename><forename type="middle">R</forename><surname>Jeliazkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J O'</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">P</forename><surname>Meara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hahnbeom</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><forename type="middle">V</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName><surname>Douglas Renfrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalli</forename><surname>Vikram K Mulligan</surname></persName>
		</author>
		<author>
			<persName><surname>Kappel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3031" to="3048" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A general-purpose protein design framework based on mining sequence-structure relationships in known protein structures</title>
		<author>
			<persName><forename type="first">Jianfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><forename type="middle">E</forename><surname>Panaitiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gevorg</forename><surname>Grigoryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">431635</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toward machine-guided design of proteins</title>
		<author>
			<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gleb</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierce</forename><forename type="middle">J</forename><surname>Ogden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">337154</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An improved relative self-attention mechanism for transformer with application to music generation</title>
		<author>
			<persName><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04281</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Metrics for 3d rotations: Comparison and analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="164" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cath-a hierarchic classification of protein domain structures</title>
		<author>
			<persName><forename type="first">Christine</forename><forename type="middle">A</forename><surname>Orengo</surname></persName>
		</author>
		<author>
			<persName><surname>Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><forename type="middle">M</forename><surname>Swindells</surname></persName>
		</author>
		<author>
			<persName><surname>Thornton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1093" to="1109" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Kabsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2577" to="2637" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Emergence of preferred structures in a simple model of protein folding</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Helling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ned</forename><surname>Wingreen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="issue">5275</biblScope>
			<biblScope unit="page" from="666" to="669" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Functional proteins from a random-sequence library</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName><surname>Szostak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="issue">6829</biblScope>
			<biblScope unit="page">715</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The pfam protein families database in 2019</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>El-Gebali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matloob</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorna</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Smart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D427" to="D432" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04833</idno>
		<title level="m">Hierarchical neural story generation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Computational protein design quantifies structural constraints on amino acid covariation</title>
		<author>
			<persName><forename type="first">Noah</forename><surname>Ollikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Kortemme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">e1003313</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A web resource for standardized benchmark datasets, metrics, and rosetta protocols for macromolecular modeling and design</title>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">A</forename><surname>Shane Ó Conchúir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">A</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Pache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kale</forename><surname>Ollikainen</surname></persName>
		</author>
		<author>
			<persName><surname>Kundert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J O'</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Meara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Kortemme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">e0130433</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
