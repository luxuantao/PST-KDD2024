<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CODET: CODE GENERATION WITH GENERATED TESTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-21">21 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
							<email>beichen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fengji</forename><surname>Zhang</surname></persName>
							<email>v-fengjzhang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
							<email>anhnguyen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daoguang</forename><surname>Zan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
							<email>zeqi.lin@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
							<email>jlou@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CODET: CODE GENERATION WITH GENERATED TESTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-21">21 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.10397v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given a programming problem, pre-trained language models such as Codex have demonstrated the ability to generate multiple different code solutions via sampling. However, selecting a correct or best solution from those samples still remains a challenge. While an easy way to verify the correctness of a code solution is through executing test cases, producing high-quality test cases is prohibitively expensive. In this paper, we explore the use of pre-trained language models to automatically generate test cases, calling our method CODET: CODE generation with generated Tests. CODET executes the code solutions using the generated test cases, and then chooses the best solution based on a dual execution agreement with both the generated test cases and other generated solutions. We evaluate CODET on five different pre-trained models with both HumanEval and MBPP benchmarks. Extensive experimental results demonstrate CODET can achieve significant, consistent, and surprising improvements over previous methods. For example, CODET improves the pass@1 on HumanEval to 65.8%, an increase of absolute 18.8% on the code-davinci-002 model, and an absolute 20+% improvement over previous state-of-the-art results. * The first three authors contributed equally. 1 https://github.com/features/copilot 2 Results on the HumanEval benchmark with code-cushman-001. More results can be found in Section 4.1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Thanks to the recent advances in pre-training techniques, many large language models have been pre-trained for code generation, e.g., Codex <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>, AlphaCode <ref type="bibr">(Li et al., 2022b)</ref>, IN-CODER <ref type="bibr">(Fried et al., 2022a)</ref>, CODEGEN <ref type="bibr" target="#b21">(Nijkamp et al., 2022) and</ref><ref type="bibr">PolyCoder Xu et al. (2022)</ref>, as well as bringing code generation into real-world applications such as Copilot 1 . While these advanced pre-trained models are able to generate many different solutions for a programming problem via sampling, it remains a challenge to select a single correct solution from multiple generated candidates. Taking the HumanEval benchmark <ref type="bibr" target="#b5">(Chen et al., 2021)</ref> as an example, Codex has a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% 2 . This huge gap makes it imperative to explore how to pick the correct or best solution from multiple candidates.</p><p>A simple way to verify if a solution is correct is to execute it and then check if it passes all corresponding test cases. Such an execution-guided approach has been extensively applied to many code-related tasks, such as code generation <ref type="bibr" target="#b5">(Chen et al., 2021;</ref><ref type="bibr">Li et al., 2022b;</ref><ref type="bibr" target="#b30">Shi et al., 2022)</ref>, code translation <ref type="bibr" target="#b27">(Roziere et al., 2021)</ref>, and program synthesis <ref type="bibr" target="#b6">(Chen et al., 2018;</ref><ref type="bibr">Ellis et al., 2019)</ref>. The challenge, however, is that preparing a sufficient number of high-quality test cases to cover all the corner cases is prohibitively expensive and inefficient. In real-world applications like Copilot, it is troublesome if users are required to provide test cases when they are using a code generation tool. To address these challenges, we explore approaches to automatically produce test cases for arbitrary programming problems and then use them to quickly verify any solution.</p><p>Although pre-trained models such as Codex have been used to generate code solutions, we start by designing an elaborate instruction as prompt, asking the same language model to automatically generate large amounts of test cases for each programming problem, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Second,  we execute each of the generated solutions on the generated test cases to associate each solution with all the test cases it can pass. Third, we apply a dual execution agreement on both the solutions and the test cases. We believe a solution could get support from both the test cases and the other solutions. The more test cases a solution can pass, the better the solution is. Meanwhile, if there is another test-driven sibling solution that could pass the exact same test cases as the current solution, it is likely that the two solutions have the same functionality, although with different implementations. We regard the sibling solutions as supporting each other where a larger number of sibling solutions can directly contribute to the correctness of a solution. Finally, we calculate a ranking score based on this dual execution agreement and produce the best solution. We call our method CODET: CODE generation with generated Test-driven dual execution agreement.</p><p>Although CODET is simple and efficient, without any need of either labelled data or additional rankers, its performance is surprisingly exceptional. We evaluate CODET on five different pretrained models: three OpenAI Codex models <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>, INCODER <ref type="bibr">(Fried et al., 2022b)</ref>, and CODEGEN <ref type="bibr" target="#b21">(Nijkamp et al., 2022)</ref>, as well as two established benchmarks: HumanEval <ref type="bibr" target="#b5">(Chen et al., 2021)</ref> and MBPP <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>. Extensive experimental results show CODET can effectively select the correct solution, boosting the pass@1 score significantly: HumanEval (33.5% ? 44.5% with code-cushman-001 and 47.0% ? 65.8% with code-davinci-002), and MBPP (45.9% ? 55.4% with code-cushman-001 and 58.1% ? 67.7% with code-davinci-002). Furthermore, combining code-davinci-002 and CODET outperforms previous state-of-the-art methods by a large margin, e.g., HumanEval: 42.7% (Inala et al., 2022) ? 65.8%. Our work will be publicly available at https: //github.com/microsoft/CodeT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head><p>The task of code generation is to solve a programming problem: generate code solution x based on context c. As shown in Figure <ref type="figure">2</ref>, context c contains natural language problem descriptions in the form of code comments, and a code snippet that includes statements such as imports and the function header. A code solution is a code snippet that solves the programming problem described in the context. Generally, we sample a set of code solutions, denoted as  on the context c using a pre-trained language model M, which can be formulated as x = M(c). Our goal is to select the best code solution x from the set of generated code solutions x, where x is the most likely solution to correctly solve the given programming problem. To this end, we propose CODET in the hope of unleashing the inherent power of the pre-trained language model M. Specifically, we use M to generate test cases for the programming problems (Section 2.1), and then select the best code solution x based on a dual execution agreement (Section 2.2).</p><formula xml:id="formula_0">x = {x 1 , x 2 , ? ? ?, x K },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TEST CASE GENERATION</head><p>We leverage the pre-trained language model M to generate both code solutions and test cases. When generating test cases, to tell the model that we want to generate test cases rather than code solutions, we add an instruction p as a prompt following the context c. As shown in Figure <ref type="figure">2</ref>, we construct instruction p using the following parts: a "pass" statement as a placeholder of the function body, a comment "check the correctness of [entry point]" to clarify the intention of generating test cases, and an "assert" to kick off the test case generation. The process of test case generation can be formulated as y = M(concat(c, p)), where y = {y 1 , y 2 , ? ? ?, y M } denotes a set of test cases and concat is the concatenation operation. It is worth noting that we remove all example input-output cases from the context c to avoid exposing real test cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DUAL EXECUTION AGREEMENT</head><p>In this subsection, we try to answer the question: given the code solutions x and the test cases y, how do we select the solution x that is most likely correct? First, we execute each of the generated code solutions on the generated test cases. Then, the most straightforward way is to score each code solution by the number of test cases it can pass. However, we found this naive method is not good enough<ref type="foot" target="#foot_0">3</ref> . A simple example is shown in Figure <ref type="figure" target="#fig_1">3</ref>. There are three code solutions and five test cases for the programming problem "return the square of a number". The highest scoring code solution is x 3 , which passes four test cases. x 3 , however, is obviously not the correct solution, since it returns the double of a number, not its square. As observed, although x 1 and x 2 are two different solutions, they are both correct with the same functionality of returning the square of a number. Hence, it is reasonable for them to group together. By adding up the scores of x 1 and x 2 , they will be selected based on a combined score of 6.</p><p>Based on this idea, we propose our approach CODET to perform what we call dual execution agreement. Formally, for each code solution x ? x, we execute it with all test cases in y. If two code solutions can pass a same set of test cases, then they are sibling solutions with the same functionality; thus, we can put them into the same cluster. In this way, all code solutions in x can be divided into several clusters, denoted as</p><formula xml:id="formula_1">x = {x 1 , x 2 , ? ? ?, x N }. Let W = {w ij</formula><p>} be an N ?M matrix to represent the execution results, where N is the number of code solution clusters and M is the number of test cases. If code solutions in cluster x i can pass the test case y j , then w ij = 1; otherwise, w ij = 0. The basic idea of the dual execution agreement is that a good code solution should be agreed upon by both the test cases and the other solutions: (1) The more test cases it can pass, the better the solution is. (2) A larger number of sibling solutions can directly contribute to the correctness of the solution. Hence, we define the score of each cluster x i to be:</p><formula xml:id="formula_2">f (x i ) = r i M j=1 w ij g 0 (y j ),<label>(1)</label></formula><p>where g 0 (y j ) = 1 M denotes the initial normalized score of test case y j and r i is the square root of the code solution number in the cluster x i . We use the square root to reduce the impact caused by code solutions due to the intuition that the number of code solutions is less important than the number of test cases. For example, there may be one code solution that can pass five test cases, whereas another five code solutions may pass only one test case. We intuitively consider that the former may be more likely correct. Finally, we get the best code solution x by selecting any code solution from the highest scoring cluster. In addition, if we want to obtain k code solutions, we can select one code solution from each of the top k highest scoring clusters.</p><p>In CODET, each test case contributes equally to the score of the code solution. So the question arises: do different test cases have different levels of importance? Intuitively, if a test case can be passed by more code solutions, the more likely the test case is to be correct. To this end, we further propose CODET-Iter to consider the importance of test cases in an iterative manner. Inspired by the work for bipartite graph ranking <ref type="bibr" target="#b16">(Kleinberg, 1999;</ref><ref type="bibr" target="#b14">He et al., 2016;</ref><ref type="bibr" target="#b37">Yang et al., 2020)</ref>, the scores of x i and y j can be calculated as:</p><formula xml:id="formula_3">f (x i ) =?r i M j=1 w ij g(y j ) + (1 -?)f 0 (x i ), g(y j ) =? N i=1 w ij f (x i ) + (1 -?)g 0 (y j ),</formula><p>(2) where f 0 (x i ) = r i / N n=1 r n is the initial normalized score of x i , and ? and ? are hyper-parameters to be set between [0, 1] to account for the importance of the initial scores. The scores of x i and y j are calculated iteratively using Equation <ref type="formula">2</ref>. For convergence, the scores of all code solution clusters are normalized after each iteration; the same applies to the test cases as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL SETUP</head><p>In this section, we introduce the experimental setup, including the pre-trained language models, the benchmarks for code generation, the evaluation metrics, and the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MODELS</head><p>Our main experiments are based on Codex <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>, which is a descendant of GPT-3 <ref type="bibr" target="#b4">(Brown et al., 2020)</ref>. Codex is proficient in understanding the provided context and generating functional programs, and it has been successfully applied to many programming tasks <ref type="bibr" target="#b8">(Drori et al., 2021;</ref><ref type="bibr" target="#b25">Pearce et al., 2021;</ref><ref type="bibr" target="#b28">Sarsa et al., 2022)</ref>. We use three Codex models with different sizes provided by OpenAI: code-cushman-001, code-davinci-001, and code-davinci-002. Furthermore, we include two publicly available pre-trained models: INCODER <ref type="bibr">(Fried et al., 2022a)</ref> and CODEGEN <ref type="bibr" target="#b21">(Nijkamp et al., 2022)</ref>. INCODER is a unified generative model that can perform leftto-right code generation and code infilling (editing) via the causal mask language modelling training objective <ref type="bibr" target="#b0">(Aghajanyan et al., 2022)</ref>. CODEGEN is a family of large-scale language models trained on natural language and programming data to perform conversational program synthesis. We take use of the INCODER 6.7B version (INCODER-6B) and the CODEGEN 16B Python mono-lingual version (CODEGEN-MONO-16B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BENCHMARKS</head><p>We conduct experiments on two public code generation benchmarks using zero-shot settings.</p><p>HumanEval is a code generation benchmark consisting of 164 hand-written Python programming problems, covering subjects of language comprehension, reasoning, algorithms, and simple mathematics <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>. As shown in Figure <ref type="figure">2</ref>, the context of each problem may include the natural language description in the form of a comment, a function header, and statements like imports. Each problem includes a canonical solution and several ground truth test cases. To be clear, the original context of each problem may include example input-output cases, which are removed in our experiments to avoid exposing real test cases.</p><p>MBPP (sanitized version) contains 427 crowd-sourced Python programming problems, ranging from the basic usage of standard library functions to problems that require nontrivial external knowledge <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>. Originally, each problem includes the natural language problem description, a function header, a canonical solution, and several ground truth test cases. We follow Hu-manEval to construct the context for MBPP, which contains a well-formed function header and its natural language description in the form of a multi-line comment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">METRICS</head><p>We use the metric pass@k for performance evaluation and take advantage of ground truth test cases to determine the functional correctness of code solutions. For each problem, k code solutions are produced as the final result. If any of the k code solutions pass all ground truth test cases, the problem is considered solved. Then pass@k is the percentage of solved problems. Following Chen et al. ( <ref type="formula">2021</ref>), pass@k can be formulated as:</p><formula xml:id="formula_4">pass@k := E Problems 1 - n-c k n k ,<label>(3)</label></formula><p>where n ? k is the sampling number and c ? n is the number of correct code solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">IMPLEMENTATION DETAILS</head><p>For experiments with Codex models, we set the top p to 0.95 and set the max generation length to 300. To get the results of baseline pass@1, the temperature is set to 0 and the sampling number n is set to 1. For other results, the temperature is set to 0.8 and the sampling number n is set to 100. That is, for baseline pass@10 and pass@100, we use sampling number n = 100. For CODET, we select the top k code solutions as mentioned in Section 2.2 and use n = k. The timeout of executing a test case is set to 0.1 seconds. The hyper-parameters ? and ? in CODET-Iter (Equation <ref type="formula">2</ref>) are both set to 0.9, and the iteration number is set to 3. For code solution post-processing, we follow <ref type="bibr" target="#b5">Chen et al. (2021)</ref> to truncate the generated content by five stop sequences: "\nclass", "\ndef", "\n#", "\nif", and "\nprint". For test case post-processing, we extract the first five assertions that conform to the Python syntax for each generated sample. A valid assertion should start with "assert" and contain the name of the corresponding entry point function.</p><p>For experiments with INCODER and CODEGEN models, we use the HuggingFace transformers library <ref type="bibr" target="#b35">(Wolf et al., 2019)</ref>. The setup and post-processing procedure are the same as in the Codex experiments, except that the baseline pass@1 results are obtained by picking the sample with the highest mean log-probability from n = 100 samples with a small temperature close to 0. To speed up our experiments, we run both models with half precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we evaluate CODET on five different pre-trained models and two benchmarks to verify its effectiveness, followed by a deep analysis on the quality of generated test case and different design choices in CODET. Table <ref type="table" target="#tab_2">1</ref>: Pass@k (%) on the HumanEval and MBPP benchmarks with Codex. The numbers in red indicate the absolute improvements of our methods over baseline on pass@1 and pass@10. For baseline pass@1, the temperature is set to 0 and sampling number is set to 1; for others, temperature is set to 0.8 and sampling number is set to 100. We do not show CODET pass@100, since it is the same as the baseline pass@100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MAIN RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>pass@1 pass@2 pass@10 code-cushman-001 29.9 -3.6 36.6 59.5 Table <ref type="table">2</ref>: Pass@k (%) on the HumanEval with simply counting the number of passed test cases. The numbers in red indicate the absolute improvements over baseline on pass@1 and pass@10.</p><p>to pass@1, it is clear that the former is significantly better than the latter, indicating the potential to select the best code solution from the 100 generated samples.</p><p>When we compare the CODET column with the Baseline column, CODET achieves an absolute improvement of about 10% over the baseline. For HumanEval, the improvements are consistently above 10%. Surprisingly, even for the strongest baseline, code-davinci-002, the improvement is 18.8%, boosting the pass@1 to 65.8% -a 20+% absolute improvement over the best previously reported results <ref type="bibr" target="#b15">(Inala et al., 2022)</ref>. We attribute this larger improvement to the higher quality of test cases generated by code-davinci-002, providing a deeper analysis in the following section.</p><p>We also report pass@2 and pass@10 of CODET to further show its superiority. The pass@2 results of CODET are close to the baseline pass@10 results. Meanwhile, the improvement on pass@10 is also consistently over 10% in the HumanEval benchmark. Since CODET is performed on the 100 generated code solutions, its pass@100 performance is the same as that of the baseline. For the MBPP benchmark, we continue to see consistent and significant improvements, although the magnitude of the improvements is slightly less than that of HumanEval. Using the code-davinci-002 as an example, the pass@1 improves by 9.6%.</p><p>In addition, we compare the performance between CODET-Iter and CODET. The results show that they are comparable without any significant difference. We conjecture it might be unnecessary to consider the importance of test cases in this way, or an obviously good test case with a high score can pass many different solutions without introducing differentiation to rank the code solutions. We will leave the further study of a more complicated iterative approach for future work.</p><p>As mentioned in section 2.2, a straightforward way to score a code solution is to simply count the number of test cases it can pass. Nevertheless, this method highly depends on the overall quality of generated test cases and completely elides the agreement between code solutions. We evaluate this method on the HumanEval benchmark using Codex and provide the results in Table <ref type="table">2</ref>. It clearly shows that its performance is significantly and consistently worse than CODET, with only codedavinci-002 gaining improvement on pass@1 over the baseline. This observation again demonstrates the importance and the rationality of CODET. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS OF INCODER AND CODEGEN</head><p>To further verify the effectiveness of CODET, we include the experimental results of INCODER-6B and CODEGEN-MONO-16B, as shown in Table <ref type="table" target="#tab_3">3</ref>. It is obvious CODET can significantly improve the pass@1, with absolute improvements in the range of 4.2% to 13.1%. INCODER-6B achieves the most improvement with a gain of 13.1% on the MBPP benchmark. Similar to the experimental results of Codex, the pass@2 results are close to the baseline pass@10, with exceptionally close CODET and CODET-Iter results. All the results demonstrate that CODET can boost the performance of various pre-trained language models consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ANALYSIS ON TEST CASES</head><p>The test cases are vital since the core idea of CODET is based on test-driven execution. Hence, in this subsection, we would like to analyze test cases by answering the following two research questions.</p><p>Q1. What is the quality of the generated test cases?</p><p>For test case generation, we generate 100 samples for each problem and extract the first five syntactically correct test cases for each sample, meaning each problem is equipped with up to 500 generated test cases. Table <ref type="table" target="#tab_4">4</ref> summarizes the average and median numbers of the test cases for each problem on HumanEval. Almost all models could generate considerable number of syntactically correct test cases, while CODEGEN generates plenty of unexpected noise.</p><p>We leverage the canonical solutions provided by the HumanEval benchmark to evaluate the correctness of generated test cases. Each test case is an assert statement, and we only consider it correct if its assert condition evaluates as true to the canonical solution. Figure <ref type="figure" target="#fig_3">4a</ref> summarizes the distribution of test case accuracy. The horizontal axis represents the test case accuracy value for each problem. The vertical axis represents the probability density of problems with the corresponding accuracy value. We can see that the test cases generated by code-davinci-002 are of high accuracy, while those generated by INCODER are relatively inaccurate.  Table <ref type="table">5</ref>: Pass@k (%) on the HumanEval and MBPP benchmarks with code-cushman-001, codedavinci-001, INCODER, and CODEGEN using the test cases generated by code-davinci-002. The numbers in orange indicate the absolute improvements of pass@k using code-davinci-002 test cases over that using their own generated test cases.</p><p>Besides accuracy, we also introduce the toxicity rate to evaluate the quality of test cases. We consider a test case to be "toxic" if there exists a generated code solution that passes it, but the canonical solution does not. Toxic test cases may hinder the execution agreement of CODET and lead to performance degradation. Figure <ref type="figure" target="#fig_3">4b</ref> summarizes the distribution of test case toxicity rate, from which we can see that the toxicity rate highly correlates to the test case accuracy with respect to different models. The proportion of toxic test cases for code-davinci-002 is much smaller than that for INCODER, which explains the minor performance improvement of INCODER on the HumanEval benchmark by CODET when compared to code-davinci-002.</p><p>Q2. Can better test cases further boost the performance of mediocre models?</p><p>As analyzed above, code-davinci-002 is the most capable model for generating high-quality test cases. Hence, we conduct an experiment to boost the code generation of the other four models (code-cushman-001, code-davinci-001, INCODER, and CODEGEN) using test cases generated by code-davinci-002. Table <ref type="table">5</ref> summarizes the performance gain shown by different models on the Hu-manEval and MBPP benchmarks. In general, compared to the results of using their own generated test cases, the results of using test cases generated by code-davinci-002 show significant improvements. For code-cushman-001 and code-davinci-001, the absolute improvements are in the range of 1.8% to 4.3% on CODET pass@1, while for INCODER and CODEGEN, the range is from 6.2% to 15.9%. This indicates that potentially correct code solutions generated by mediocre models can be further exploited by adopting better test cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ANALYSIS ON DESIGN CHOICE</head><p>Temperature The temperature hyper-parameter has great impact on the quality of generated code solutions and test cases when using top p sampling. We use a high temperature of 0.8 in our main experiments since CODET could benefit from a larger number of diverse samples. To investigate the sensitivity of CODET on the temperature, we perform an ablation study by using a range of temperatures to report the results of baseline pass@100 and CODET pass@1. Figure <ref type="figure">5</ref> shows the results of code-cushman-001 on the HumanEval benchmark at different temperature settings. We can find that a higher temperature does improve the baseline pass@100 and CODET pass@1, and CODET achieves a good performance when temperature is set to 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of code solutions</head><p>As mentioned in Section 2.2, we define r i to be the square root of the code solution number in the cluster x i , because we believe the number of passed test cases is more valuable than the size of code solution clusters. For validation, we perform an ablation study by comparing the performance of CODET with the "sqrt", "log" functions, and without any constraint ("linear") on the number of code solutions. Figure <ref type="figure">6</ref> shows the results of three Codex models on HumanEval. We can conclude that reducing the importance of code solutions improves the performance of CODET, indicating our design of r i is reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Code Generation with Large Models Recently, a number of large pre-trained language models have been proposed for code generation. Benefiting from billions of trainable parameters and massive publicly available source code, models could achieve surprisingly good performance. For instance, AlphaCode <ref type="bibr">(Li et al., 2022b)</ref>   <ref type="bibr" target="#b22">(Pacheco et al., 2007)</ref>, EvoSuite <ref type="bibr" target="#b11">(Fraser &amp; Arcuri, 2011)</ref>, MOSA <ref type="bibr" target="#b23">(Panichella et al., 2015)</ref>, DynaMOSA <ref type="bibr" target="#b24">(Panichella et al., 2017)</ref>, and MIO <ref type="bibr" target="#b1">(Arcuri, 2017)</ref>, were proposed to automatically generate test cases for statically typed programming languages like Java. The later proposed Pynguin <ref type="bibr" target="#b20">(Lukasczyk &amp; Fraser, 2022)</ref> could handle dynamically typed language like Python. Nevertheless, they are all search-based heuristics methods, which have limitations to the diversity and quantity of generated test cases. To combat these limitations, recently proposed approaches <ref type="bibr" target="#b31">(Tufano et al., 2020;</ref><ref type="bibr">Li et al., 2022b</ref>) leveraged pre-trained language models like BART <ref type="bibr" target="#b17">(Lewis et al., 2019)</ref> and T5 <ref type="bibr" target="#b26">(Raffel et al., 2020)</ref> fine-tuned on labelled data for test case generation. Unlike previous works that require heuristics rules or model training, we directly sample test cases from powerful code generation models like Codex in a zero-shot manner with elaborate prompts.</p><p>Code Selection from Multiple Samples Despite large models having achieved great performance in code generation, the models need to sample many times to find the correct answer, which brings the challenge to select the correct ones from multiple samples. Recently, several approaches were proposed to tackle this issue. In the domain of solving math word problems, <ref type="bibr" target="#b7">Cobbe et al. (2021)</ref> generated many candidate solutions and chose the one with highest rank by a trained verifier. <ref type="bibr" target="#b29">Shen et al. (2021)</ref> proposed to jointly train the generator and ranker through a multi-task framework. In the domain of general purpose code generation, Inala et al. ( <ref type="formula">2022</ref>) trained a fault-aware ranker to increase the pass@1 accuracy of code generation models. Besides training an additional verifier/ranker, <ref type="bibr" target="#b30">Shi et al. (2022)</ref> and <ref type="bibr">Li et al. (2022b)</ref> proposed to leverage the execution information by ranking the similarity of outputs based on the given inputs. As for the input data, <ref type="bibr" target="#b30">Shi et al. (2022)</ref> employed the test cases provided by the benchmark, while <ref type="bibr">Li et al. (2022b)</ref> trained an additional generator. The idea of ranking based on agreement also appears in the domain of reasoning <ref type="bibr" target="#b34">(Wang et al., 2022;</ref><ref type="bibr">Li et al., 2022a)</ref>. Unlike previous works that require either model training or pre-existing test cases to rank the generated code solutions, we let the large models generate test cases for themselves and then rank the solutions based on their test-driven dual execution agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a simple yet effective approach, called CODET, leveraging pre-trained language models to generate both the code solutions and the test cases. CODET executes the code solutions using the test cases and chooses the best solution based on the dual execution agreement. We demonstrate dual agreement with both the test cases and other solutions is critical to the success of CODET and perform thorough analysis on the quality of generated test cases and their impact to CODET. Meanwhile, experimental results clearly demonstrate the superiority of CODET, improving the pass@1 numbers significantly on both the HumanEval and the MBPP benchmarks. Furthermore, the combination of code-davinci-002 and CODET surprisingly improves the pass@1 on HumanEval to 65.8%, an absolute 20+% improvement over the previous sate-of-the-art results. In future work, we will explore how to combine the ideas of CODET with training a ranker. Another direction is to extend CODET to other code-related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The illustration of CODET. Both the code solutions and the test cases are generated by the pre-trained language model. The best code solution is then selected by a dual execution agreement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A simple example of the programming problem "return the square of a number".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The distributions of (a) test case accuracy and (b) toxicity rate for each programming problem on the HumanEval benchmark. Test cases of a model are of better quality if they have higher accuracy and lower toxicity rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure5: The baseline pass@100 and CODET pass@1 with code-cushman-001 at different temperature settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>based</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Case Generation</cell></row><row><cell cols="2">for idx, elem in enumerate(numbers): for idx2, elem2 in enumerate(numbers): if idx != idx2:</cell><cell cols="2">from typing import List def has_close_elements(numbers: List[float], Context ?</cell><cell>assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True</cell></row><row><cell cols="2">distance = abs(elem -elem2)</cell><cell>threshold: float) -&gt; bool:</cell><cell></cell></row><row><cell cols="2">if distance &lt; threshold:</cell><cell>"""</cell><cell></cell></row><row><cell>return False</cell><cell>return True</cell><cell cols="2">Check if in given list of numbers, are any two numbers closer to each other</cell><cell>Test Case ? 1</cell></row><row><cell></cell><cell></cell><cell>than given threshold.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>""" pass</cell><cell>Instruction ?</cell><cell>?</cell><cell>Test Case ? 2</cell></row><row><cell></cell><cell></cell><cell cols="2"># check the correctness of has_close_elements assert</cell><cell>Test Case ? ?</cell></row><row><cell cols="5">Figure 2: Code generation and test case generation: an example from the HumanEval benchmark.</cell></row><row><cell cols="4">Example input-output cases are removed from the context.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Methods</cell><cell></cell><cell>Baseline</cell><cell></cell><cell cols="2">CODET</cell><cell></cell><cell cols="2">CODET-Iter</cell><cell></cell></row><row><cell>k</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>1</cell><cell>2</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HumanEval</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">code-cushman-001 33.5 54.3 77.4 44.5 11.0 50.1 65.7 11.4 45.2 11.7 50.9 66.0 11.7</cell></row><row><cell>code-davinci-001</cell><cell cols="7">39.0 60.6 84.1 50.2 11.2 58.9 75.8 15.2 48.5 9.5</cell><cell cols="2">57.9 76.4 15.8</cell></row><row><cell>code-davinci-002</cell><cell cols="9">47.0 74.9 92.1 65.8 18.8 75.1 86.6 11.7 65.2 18.2 75.2 86.8 11.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MBPP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">code-cushman-001 45.9 66.9 79.9 55.4 9.5</cell><cell cols="2">61.7 72.7 5.8</cell><cell>54.9 9.0</cell><cell cols="2">61.1 72.7 5.8</cell></row><row><cell>code-davinci-001</cell><cell cols="6">51.8 72.8 84.1 61.9 10.1 69.1 79.3 6.5</cell><cell cols="3">62.1 10.3 69.4 79.6 6.8</cell></row><row><cell>code-davinci-002</cell><cell cols="4">58.1 76.7 84.5 67.7 9.6</cell><cell cols="2">74.6 81.5 4.8</cell><cell>67.9 9.8</cell><cell cols="2">73.7 80.5 3.8</cell></row></table><note><p><p><p>summarizes the experimental results of various Codex models on both the HumanEval and the MBPP benchmarks. Using HumanEval in the Baseline column as an example, we find our reproduced results of the 12B code-cushman-001 model are slightly better than that of the Codex-12B model reported in the original Codex paper</p>(Chen et al., 2021) (33.5% vs. 28</p>.6% in pass@1 and 77.4% vs. 72.3% in pass@100). We conjecture that code-cushman-001 is a better 12B pretrained model than the original Codex-12B model. On the other hand, if we compare the pass@100</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Pass@k (%) on the HumanEval and MBPP benchmarks with INCODER and CODEGEN. The numbers in red indicate the absolute improvements of our methods over baseline on pass@1 and pass@10. We also list the baseline results fromFried et al. (2022a)  and<ref type="bibr" target="#b21">Nijkamp et al. (2022)</ref> for reference (denoted by ?), where the settings of context are not exactly the same as ours.</figDesc><table><row><cell>Models</cell><cell cols="2">Average Median</cell></row><row><cell>code-cushman-001</cell><cell>410.7</cell><cell>429.0</cell></row><row><cell>code-davinci-001</cell><cell>381.9</cell><cell>388.0</cell></row><row><cell>code-davinci-002</cell><cell>391.1</cell><cell>402.0</cell></row><row><cell>INCODER</cell><cell>390.1</cell><cell>400.0</cell></row><row><cell>CODEGEN</cell><cell>55.6</cell><cell>42.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The average and median numbers of syntactically correct test cases for each problem generated by various models on the HumanEval benchmark.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>rate for each programming problem on the HumanEval benchmark. Test cases of a model are of better quality if they have higher accuracy and lower toxicity rate. CODEGEN-MONO-16B 47.7 11.0 54.9 10.2 71.0 11.7 47.1 9.6 54.3 9.4 70.7 11.2</figDesc><table><row><cell>Methods</cell><cell></cell><cell>CODET</cell><cell></cell><cell></cell><cell>CODET-Iter</cell><cell></cell></row><row><cell>k</cell><cell>1</cell><cell>2</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell cols="2">HumanEval</cell><cell></cell><cell></cell><cell></cell></row><row><cell>code-cushman-001</cell><cell>47.1 2.6</cell><cell>58.6 8.5</cell><cell>71.2 5.5</cell><cell cols="3">45.9 0.7 56.1 5.2 69.7 3.7</cell></row><row><cell>code-davinci-001</cell><cell>52.0 1.8</cell><cell>62.9 4.0</cell><cell>78.1 2.3</cell><cell cols="3">52.6 4.1 61.7 3.8 77.5 1.1</cell></row><row><cell>INCODER-6B</cell><cell>26.8 6.2</cell><cell>30.4 2.8</cell><cell>40.8 3.7</cell><cell cols="3">26.2 5.3 29.8 2.2 40.2 3.1</cell></row><row><cell></cell><cell></cell><cell>MBPP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>code-cushman-001</cell><cell>59.7 4.3</cell><cell>64.8 3.1</cell><cell>75.5 2.8</cell><cell cols="3">59.7 4.8 64.4 3.3 74.8 2.1</cell></row><row><cell>code-davinci-001</cell><cell>64.3 2.4</cell><cell>71.7 2.6</cell><cell>80.5 1.2</cell><cell cols="3">64.3 2.2 71.0 1.6 79.9 0.3</cell></row><row><cell>INCODER-6B</cell><cell cols="3">50.3 15.9 55.4 11.5 64.5 6.3</cell><cell cols="3">48.0 14 52.8 9.2 63.3 6.3</cell></row><row><cell cols="4">CODEGEN-MONO-16B 60.0 10.5 67.6 11.0 76.5 8.0</cell><cell cols="3">58.6 8.1 65.6 8.8 75.5 8.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>claimed to have outperformed half of the human competitors in real-world programming competitions, and Codex<ref type="bibr" target="#b5">(Chen et al., 2021)</ref> is empowering Copilot to provide real-time coding suggestions. In addition to the private-access AlphaCode and Codex models, there are also open-source code generation models like GPT-Neo<ref type="bibr" target="#b3">(Black et al., 2021)</ref>, GPT-J<ref type="bibr" target="#b33">(Wang &amp; Komatsuzaki, 2021)</ref>, CodeParrot(Tunstall et al., 2022), PolyCoder (Xu et al., 2022), CODEGEN<ref type="bibr" target="#b21">(Nijkamp et al., 2022)</ref>, and INCODER(Fried et al., 2022a). In our study, we take advantage of the Codex inference API provided by OpenAI as well as the two competitive open-source models CODEGEN and INCODER to perform zero-shot code generation.</figDesc><table><row><cell>Automatic Test Case Generation Automated test case generation for programming problems</cell></row><row><cell>can reduce the effort of writing test cases manually by developers. Early works including Ran-</cell></row><row><cell>doop</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Results can be found in Section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>4.1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>We would like to thank <rs type="person">Davis Mueller</rs> and <rs type="person">Jade Huang</rs> for proofreading the paper and providing insightful comments.</p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Generation</head><p>Code Solution ? 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Solution ? 2</head><p>Code Solution ? ? ?</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cm3: A causal masked multimodal model of the internet</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Candace</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Many independent objective (mio) algorithm for test suite generation</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Arcuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on search based software engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><surname>Gpt-Neo</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5297715" />
		<title level="m">Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Zenodo</title>
		<imprint>
			<date type="published" when="2021-03">March 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Execution-guided neural program synthesis</title>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level</title>
		<author>
			<persName><forename type="first">Iddo</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reece</forename><surname>Shuttleworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newman</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.15594</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armando</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">assess: Program synthesis with a repl</title>
		<author>
			<persName><forename type="first">Execute</forename><surname>Write</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EvoSuite: automatic test suite generation for object-oriented software</title>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Arcuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering</title>
		<meeting>the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="416" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<title level="m">Incoder: A generative model for code infilling and synthesis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05999</idno>
		<title level="m">Incoder: A generative model for code infilling and synthesis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Birank: Towards ranking on bipartite graphs</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jeevana</forename><surname>Priya Inala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Codas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Encarnaci?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shuvendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madanlal</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Musuvathi</surname></persName>
		</author>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03865</idno>
		<title level="m">Fault-aware neural code rankers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the advance of making language models better reasoners</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02336</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Competition-level code generation with alphacode</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><surname>Dal Lago</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07814</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pynguin: Automated unit test generation for python</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Lukasczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Fraser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05218</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Silvio Savarese, and Caiming Xiong. A conversational paradigm for program synthesis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feedback-directed random test generation</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shuvendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th International Conference on Software Engineering (ICSE&apos;07)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reformulating branch coverage as a many-objective optimization problem</title>
		<author>
			<persName><forename type="first">Annibale</forename><surname>Panichella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fitsum</forename><surname>Meshesha Kifetew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Tonella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 8th international conference on software testing, verification and validation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated test case generation as a many-objective optimisation problem with dynamic selection of the targets</title>
		<author>
			<persName><forename type="first">Annibale</forename><surname>Panichella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fitsum</forename><surname>Meshesha Kifetew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Tonella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="158" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Can openai codex and other large language models help us fix security bugs</title>
		<author>
			<persName><forename type="first">Hammond</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baleegh</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Karri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02125</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><surname>Baptiste Roziere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Charton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06773</idno>
		<title level="m">Leveraging automated unit tests for unsupervised code translation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic generation of programming exercises and code explanations with large language models</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Sarsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arto</forename><surname>Hellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Leinonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11861</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03034</idno>
		<title level="m">Generate &amp; rank: A multi-task framework for math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Natural language to code translation with execution</title>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida I</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.11454</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unit test case generation with transformers and focal context</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Sundaresan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05617</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Natural language processing with transformers</title>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<publisher>O&apos;Reilly Media, Inc</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A systematic evaluation of large language models of code</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Frank F Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><surname>Josua Hellendoorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Code Workshop</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Birank: Fast and flexible ranking on bipartite networks with r and python</title>
		<author>
			<persName><forename type="first">Kai-Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Yeol</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of open source software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">51</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
