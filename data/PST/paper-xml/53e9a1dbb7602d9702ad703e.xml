<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Approaching Sentiment Analysis by using semi-supervised learning of multi-dimensional classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-03-05">5 March 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Ortigosa-Herna ´ndez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Computer Science Faculty</orgName>
								<orgName type="laboratory">Intelligent Systems Group</orgName>
								<orgName type="institution">The University of the Basque Country UPV/EHU</orgName>
								<address>
									<addrLine>San Sebastia ´n</addrLine>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">Diego</forename><surname>Rodrı ´guez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Computer Science Faculty</orgName>
								<orgName type="laboratory">Intelligent Systems Group</orgName>
								<orgName type="institution">The University of the Basque Country UPV/EHU</orgName>
								<address>
									<addrLine>San Sebastia ´n</addrLine>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leandro</forename><surname>Alzate</surname></persName>
							<email>leandro.alzate@asomo.net</email>
							<affiliation key="aff1">
								<orgName type="department">Socialware &amp;</orgName>
								<address>
									<settlement>Bilbao</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Lucania</surname></persName>
							<email>manuel.lucania@socialware.eu</email>
							<affiliation key="aff1">
								<orgName type="department">Socialware &amp;</orgName>
								<address>
									<settlement>Bilbao</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">In</forename><surname>˜aki Inza</surname></persName>
							<email>inaki.inza@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Computer Science Faculty</orgName>
								<orgName type="laboratory">Intelligent Systems Group</orgName>
								<orgName type="institution">The University of the Basque Country UPV/EHU</orgName>
								<address>
									<addrLine>San Sebastia ´n</addrLine>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
							<email>ja.lozano@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Computer Science Faculty</orgName>
								<orgName type="laboratory">Intelligent Systems Group</orgName>
								<orgName type="institution">The University of the Basque Country UPV/EHU</orgName>
								<address>
									<addrLine>San Sebastia ´n</addrLine>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Approaching Sentiment Analysis by using semi-supervised learning of multi-dimensional classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-03-05">5 March 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">F4405818A62C0D3C2770D896E2C348E5</idno>
					<idno type="DOI">10.1016/j.neucom.2012.01.030</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Sentiment Analysis Multi-dimensional classification Multi-dimensional class Bayesian network classifiers Semi-supervised learning EM algorithm</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentiment Analysis is defined as the computational study of opinions, sentiments and emotions expressed in text. Within this broad field, most of the work has been focused on either Sentiment Polarity classification, where a text is classified as having positive or negative sentiment, or Subjectivity classification, in which a text is classified as being subjective or objective. However, in this paper, we consider instead a real-world problem in which the attitude of the author is characterised by three different (but related) target variables: Subjectivity, Sentiment Polarity, Will to Influence, unlike the two previously stated problems, where there is only a single variable to be predicted. For that reason, the (uni-dimensional) common approaches used in this area yield to suboptimal solutions to this problem. Somewhat similar happens with multi-label learning techniques which cannot directly tackle this problem. In order to bridge this gap, we propose, for the first time, the use of the novel multidimensional classification paradigm in the Sentiment Analysis domain. This methodology is able to join the different target variables in the same classification task so as to take advantage of the potential statistical relations between them. In addition, and in order to take advantage of the huge amount of unlabelled information available nowadays in this context, we propose the extension of the multidimensional classification framework to the semi-supervised domain. Experimental results for this problem show that our semi-supervised multi-dimensional approach outperforms the most common Sentiment Analysis approaches, concluding that our approach is beneficial to improve the recognition rates for this problem, and in extension, could be considered to solve future Sentiment Analysis problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sentiment Analysis (SA), which is also known as Opinion Mining, is a broad area defined as the computational study of opinions, sentiments and emotions expressed in text <ref type="bibr" target="#b32">[33]</ref>. It mainly originated to meet the need for organisations to automatically find the opinions or sentiments of the general public about their products and services, as expressed on the Internet. A fundamental methodology in many current SA applications and problems is the well-known pattern recognition field called classification <ref type="bibr" target="#b5">[6]</ref>.</p><p>Most of the work within this field has focused on the Sentiment Polarity classification, i.e. determining if an opinionated text has positive or negative sentiment <ref type="bibr" target="#b37">[38]</ref>. However, motivated by different real-world problems and applications, researchers have considered a wide range of closely related problems over a variety of different types of corpora <ref type="bibr" target="#b36">[37]</ref>. As an example of these problems, we can find the following: Subjectivity classification, which consists of determining if a text is subjective or objective <ref type="bibr" target="#b40">[41]</ref>, Authorship identification, which deals with the problem of identifying the author of a given text <ref type="bibr" target="#b1">[2]</ref> or Affect Analysis, which recognises emotions in a text <ref type="bibr" target="#b0">[1]</ref>.</p><p>In an analogous fashion, a real-world application within this field has recently been tackled in Socialware &amp; , 1 one of the most relevant companies in mobilised opinion analysis in Europe. The main goal of this application is to determine the attitude of the customers that write a post about a particular topic in a specific forum. The characterisation of these costumers is performed in this problem by measuring three different dimensions: the sentiment, the subjectivity and the potential influence of each post in the forum. By just relying on the previous work done in the SA domain, we can approach this problem by dividing it into three different subproblems (one per each dimension to be classified) and tackle them separately, i.e. study them in isolation by learning different classifiers to predict the value of each target variable as if they were independent.</p><p>However, some of the individual approaches explored in the literature for each subproblem could be adapted to the others. Moreover, a large number of papers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b55">56]</ref> proposed approaches for the problems of Sentiment Polarity classification and Subjectivity classification, and sometimes by even using the same corpus. This could indicate a certain degree of correlation between these subproblems, and consequently between their target variables, as noticed in <ref type="bibr" target="#b36">[37]</ref>. Nevertheless, this relation has never been directly demonstrated due to the fact that, to the best of our knowledge, no technique capable of dealing with several target variables has ever been used to embrace SA problems. In spite of that, it is relatively easy to notice the relation that exists between the sentiment and subjectivity (a neutral review probably indicates objectivity). So, why not learn a single classifier to classify the three dimensions simultaneously so as to make use of the statistical similarities between them? Finding a more predictive classifier by means of this thought would demonstrate that there is an actual relationship between these target variables. Also, in extension to the SA domain, why not join some previously cited problems in the same classification task in order to find more accurate multi-dimensional classifiers that take advantage of their closeness?</p><p>In order to embody this perception, we propose the use of the recently proposed multi-dimensional Bayesian network classification framework <ref type="bibr" target="#b52">[53]</ref> to deal with multiple class classification problems in the context of SA by solving a real-world application. This methodology performs a simultaneous classification by exploiting the relationships between the class variables to be predicted. Note that, under this framework, several target variables could be taken into account to enrich the SA problem and create market intelligence.</p><p>Most papers have already addressed the SA task by building classifiers that exclusively rely on labelled examples <ref type="bibr" target="#b32">[33]</ref>. However, in practice, obtaining enough labelled examples for a classifier may be costly and time consuming, an annotator has to read loads of text to create a reliable corpus. And this problem is accentuated when using multiple target variables. So, why not make use of the huge amount of unlabelled data available on the Internet to improve our solutions? Thus, the scarcity of labelled data also motivates us to deal with unlabelled examples in a semi-supervised framework when working with the exposed multi-dimensional view.</p><p>Motivated by the aforementioned comments, the following contributions are presented in this paper:</p><p>1. A novel and competitive methodology to solve the exposed real-world SA application. 2. An innovative perspective to manage the SA domain by dealing with several related problems in the same classification task. 3. The use of multi-dimensional class Bayesian network classifiers as supervised methodology to solve these multi-dimensional problems. This demonstrates that there is an actual correlation between sentiment and subjectivity as previously observed in several SA researches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56]</ref>. 4. A supervised filter learning algorithm for multi-dimensional J/K dependences Bayesian network classifiers. 5. The extension of multi-dimensional classification to the semisupervised framework by proposing a set of semi-supervised learning algorithms. This demonstrates that more predictive models can be found by making use of the unlabelled examples.</p><p>The rest of the paper is organised as follows. Section 2 describes the real multi-dimensional problem extracted from the SA domain which is solved in this paper, reviews the work related to SA and its problems, and motivates the use of multidimensional approaches in this context. The multi-dimensional supervised classification paradigm is defined in Section 3. Section 4 describes the multi-dimensional class Bayesian network classifiers. A group of algorithms to learn different types of multidimensional Bayesian classifiers in a supervised framework is introduced in Section 5. Section 6 not only introduces the idea of semi-supervised learning into the multi-dimensional classification, but also extends the supervised algorithms presented in Section 5 to the semi-supervised framework. Section 7 shows the experimental results of applying the proposed multi-dimensional classification algorithms using different feature sets to the real problem stated in Section 3. Finally, Section 8 sums up the paper with some conclusions and future work recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem statement and state-of-the-art review</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Sentiment Analysis domain</head><p>The concept of SA, motivated by different real-world applications and business-intelligence requirements, has recently been interpreted more broadly to include many different types of analysis of text, such as the treatment of opinion, sentiment or subjectivity <ref type="bibr" target="#b36">[37]</ref>.</p><p>Within this broad field, the most known problem is referred to as Sentiment Polarity classification, in which the problem of classifying documents by their overall sentiment is considered, i.e. determining whether a review is positive or negative <ref type="bibr" target="#b37">[38]</ref>. Several papers have expanded this original goal by, for instance, adding a neutral sentiment <ref type="bibr" target="#b55">[56]</ref> or considering a multi-point scale <ref type="bibr" target="#b47">[48]</ref> (e.g. one to five stars for a review) or using sentences or phrases instead of reviews as input of the sentiment classifier <ref type="bibr" target="#b55">[56]</ref>.</p><p>Work in Sentiment Polarity classification often assumes the incoming documents to be opinionated <ref type="bibr" target="#b36">[37]</ref>. For many applications, though, we may need to decide whether a given document contains subjective information or not. This is referred to as Subjectivity classification and has gained considerable attention in the research community <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref>. Due to its ability to distinguish subjective texts from the factual ones, it is also of great importance in SA. There are works in which a subjectivity classifier is used to filter the objective documents from a dataset before applying a sentiment classifier <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b34">35]</ref>. There are even works in which the need to predict both the Sentiment Polarity and the Subjectivity has been noticed <ref type="bibr" target="#b20">[21]</ref>. As stated in <ref type="bibr" target="#b36">[37]</ref>, ''the problem of distinguishing subjective versus objective instances has often proved to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification''. From this quotation we can infer that Sentiment Polarity classification depends on Subjectivity classification and that there is a need to improve the methodologies used in Subjectivity classification.</p><p>Other closely related problems can be found in the SA domain: the set of problems called Viewpoints and Perspectives <ref type="bibr" target="#b36">[37]</ref> which includes problems such as classifying political texts as liberal or conservative or placing texts along an ideological scale. Authorship identification deals with the problem of identifying the author of a given text <ref type="bibr" target="#b1">[2]</ref>. Affection Analysis consists of extracting different types of emotions or affects from texts <ref type="bibr" target="#b0">[1]</ref>. Sarcasm Recognition deals with the SA hard-nut problem of recognising sarcastic sentences <ref type="bibr" target="#b51">[52]</ref>. More problems and applications are discussed in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The ASOMO problem: a multi-dimensional perspective of SA</head><p>In this paper, we deal with a recent real-world problem studied in Socialware &amp; . This problem is extracted from its ASOMO service of mobilised opinion analysis and it has an underlying multi-dimensional nature that can be viewed as an augmentation of the classical problems of Sentiment Polarity classification and Subjectivity classification.</p><p>The main goal of this application is to determine the attitude of a customer when he writes a post about a particular topic in a specific forum through three different dimensions: Sentiment Polarity and Subjectivity (as widely used in the SA domain), and a third one called Will to Influence, which is frequently used in the framework of ASOMO. The latter is defined as the dimension that rates the desire of the opinion holder to cause a certain reaction in the potential readers of the text. While some people leave a post on a forum to tell of their experience, others seek to provoke a certain kind of reaction in the readers. In our application, this class variable has four possible values: none (declarative text), question (soft Will to Influence), complaint/recommendation (medium Will to Influence) and appellation (strong Will to Influence). We use two example cases in order to introduce the ASOMO problem:</p><p>A customer who has bought an iPhone does not know how to set up 3G on the phone, so he writes on a forum: ''How can I configure 3G on my iPhone?''.</p><p>Another customer is upset with the iPhone battery lifetime and writes in the same forum: ''If you want long battery life, then don't buy an iPhone''.</p><p>The attitude of both customers is very different. The first one has a doubt and writes to obtain a solution to his problem {neutral sentiment, objective and soft Will to Influence} while the second writes so as not to recommend the iPhone {negative sentiment, subjective and strong Will to Influence}. Fig. <ref type="figure" target="#fig_1">1</ref> shows one possible view of this problem and how we can translate the attitude of the author in three different class variables that are strongly correlated.</p><p>As previously mentioned, Will to Influence has four possible values: declarative text, soft, medium and strong will to influence. Sentiment Polarity, in this dataset, has five different labels as occurs in the 1-5 star ratings. In addition to the three classic values (positive, neutral and negative), it has the values ''very negative'' and ''very positive''. Note that in using this approach, the label ''neutral'' in Sentiment Polarity is ambiguous, as happens in the SA literature <ref type="bibr" target="#b36">[37]</ref>. So, it can be used as a label for the objective text (no opinion) or as a label for the sentiment that lies between positive and negative. As usual, Subjectivity has two values: objective and subjective. Fig. <ref type="figure" target="#fig_2">2</ref> shows not only the label distribution in the dataset for each different class variable (the three bar diagrams on the left), but also the joint label distribution of these three class variables over the labelled subset of the ASOMO dataset (the table on the right). Note that there are configurations of the joint label distribution that are equal to zero, this is because there are configurations of the class variables which are not possible, e.g. {strong Will to Influence, negative sentiment and objective}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">The ASOMO SA dataset</head><p>In order to deal with the previous problem, the <ref type="bibr">ASOMO</ref>   <ref type="formula">3</ref>) the subjective language used in the text. These broad factors have been helpful in detecting a list of 14 morphological features which characterise each analysed document. In order to engineer this list of features, each document is preprocessed using an open source morphological analyser <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. Firstly, spelling in the entire corpus is checked. Then, the analyser provides information related to the part-of-the-speech (PoS) tagging <ref type="bibr" target="#b37">[38]</ref>. Once the preprocessing task is performed, determining the values of the features is carried out by just looking for specific patterns in the corpus. In the following paragraphs, a detailed introduction of each factor is given, as well as a description of the features used in each factor.</p><p>The implication of the author: This factor covers the features that are related with the interaction between the author and the other customers in the forum. It consists of six different features that are described in Table <ref type="table" target="#tab_1">1</ref>. For each feature, we show its description and an example (with its translation into English) of the type of pattern that matches with the feature.</p><p>The position of authority of the opinion holder is mainly characterised by the purpose of the written post and it is related to the potential influence on the readers of the forum. The author could express advice, disapproval with a specific product, predictions, etc. Table <ref type="table" target="#tab_2">2</ref> shows the six features that are part of this major factor. Subjective language deals with the opinion of the author. In order to determine this factor, we consider only the adjective detected with the PoS recogniser, as commonly carried out in the state-of-the-art literature <ref type="bibr" target="#b26">[27]</ref>. Then, the adjectives are classified in polarity terms by means of a hand-annotated sentiment-lexicon. As a result of this task, we obtain two features: Positive Adjectives and Negative Adjectives, which are the number of positive and negative adjectives, respectively, in the text.  The 14 features (the ASOMO features) are normalised to be in the range [0,1] by dividing them by the maximal observed value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The need for multi-dimensional classification techniques</head><p>In order to solve the typical problems of the SA domain (those exposed in Section 2.1), there are two main types of techniques that can be distinguished: Symbolic and Machine Learning <ref type="bibr" target="#b19">[20]</ref>. The symbolic approach uses manually crafted rules and lexicons, whereas the machine learning approach uses supervised or semisupervised learning to construct a model from a training corpus. Due to the fact that the main proposal of this paper is to solve a real SA problem by means of a novel machine learning technique, this paper focuses on the latter. The machine learning approaches have gained interest because of ( <ref type="formula" target="#formula_1">1</ref>) their capability to model many features and, in doing so, capturing context, (2) their easier adaptability to changing input, and (3) the possibility to measure the degree of uncertainty by which a classification is made <ref type="bibr" target="#b19">[20]</ref>. Supervised methods that train from examples which have been manually classified by humans are the most popular.</p><p>Most of the work that has been carried out in tuning up these machine learning techniques (as also happens in text processing tasks) has been dedicated to addressing the problem of converting a piece of text into a feature vector (i.e. model features able to capture the context of the text) in order to improve the recognition rates. The most common approaches use the single lower cased words (unigrams) as features, which in several cases reports pretty good results as in <ref type="bibr" target="#b37">[38]</ref>. However, other common approaches can be found, such as n-grams <ref type="bibr" target="#b34">[35]</ref> or PoS information <ref type="bibr" target="#b37">[38]</ref>. A deeper study of such work is beyond the scope of this paper. The reader who is interested in feature engineering can consult <ref type="bibr" target="#b21">[22]</ref>, where there is an extensive body of work that addresses feature selection for machine learning approaches in general.</p><p>On the other hand, little research has been done on the induction of the classifiers. Most of the existing works learn either a naive Bayes <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b55">56]</ref> or a support vector machine (SVM) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref>, i.e. uni-dimensional classifiers able to predict a single target variable. For that reason, the classification models used in the SA literature seem inappropriate to model the three-dimensional problem exposed in this paper. However, there are several possibilities to adapt these uni-dimensional classifiers to multidimensional classification problems, and the ASOMO problem is no exception. Unfortunately, none of these approaches captures exactly the underlying characteristics of the problem <ref type="bibr" target="#b43">[44]</ref>:</p><p>One approach is to develop multiple classifiers, one for each class variable. However, this approach does not capture the real characteristics of the problem, because it does not model the correlations between the different class variables and so, it does not take advantage of the information that they may provide. It treats the class variables as if they were independent. In the case of the previously exposed problem, it would be splitting it into three different uni-dimensional problems, one per each class variable.</p><p>Another approach consists of constructing a single artificial class variable that models all possible combinations of classes. This class variable models the Cartesian product of all the class variables. The problem of this approach arises because this compound class variable can easily end up with an excessively high cardinality. This leads to computational problems because of the high number of parameters the model has to estimate. Furthermore, the model does not reflect the real structure of the classification problem either. By means of this approach, the ASOMO problem would be redefined as a unidimensional problem with a 40-label class variable.</p><p>The previous approaches are clearly insufficient for the resolution of problems where class variables have high cardinalities or large degrees of correlation among them. The first approach does not reflect the multi-dimensional nature of the problem because it does not take into account any correlation among the class variables. The second approach, however, does not consider the possible conditional independences between the classes and assumes models that are too complex. As can be seen in the experiments section, these deficiencies in capturing the real relationship between the class variables may cause a low performance, so new techniques are required to bridge the gap between the solutions offered by the learning algorithms used in the SA literature and the multi-dimensional underlying nature of the ASOMO problem.</p><p>Multi-label learning <ref type="bibr" target="#b50">[51]</ref>, which deals with problems with several labels per each instance, could also be viewed as a potential solution to this problem. However, as we show in the following section, the ASOMO problem cannot be directly tackled by the multi-label techniques. This problem is characterised for having several class variables, instead of several labels.</p><p>Within this framework, in order to yield more adequate models for problems with several target variables, multi-dimensional classification appears. It is able to use the correlations and conditional independencies between class variables in order to help in the classification task in both supervised and semisupervised learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-dimensional classification</head><p>In this section we present, in detail, the nature of the multidimensional supervised classification paradigm and how to define and evaluate a multi-dimensional classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-dimensional supervised classification problems</head><p>A typical (uni-dimensional) supervised classification problem consists of building a classifier from a labelled training dataset (see Table <ref type="table">3</ref>) in order to predict the value of a class variable C given a set of features X ¼ ðX 1 ,: :,X n Þ of an unseen unlabelled instance x ¼ ðx 1 ,: :,x n Þ.</p><p>If we suppose that ðX,CÞ is a random vector with a joint feature-class probability distribution pðx,cÞ then, a classifier c is a function that maps a vector of features X into a single class variable C c : f0, . . . ,r 1 À1g Â Á Á Á Â f0, . . . ,r n À1g/f0, . . . ,tÀ1g</p><p>x/c</p><p>where r i and t are the number of possible values of each feature X i ,ði ¼ 1, . . . ,nÞ and the class variable respectively. A generalisation of this problem to the simultaneous prediction of several class variables has recently been proposed in the research community <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref>. This generalisation is known as multi-dimensional supervised classification. Its purpose is to simultaneously predict the value of each class variable in the class variable vector c ¼ ðc 1 ,: :,c m Þ given the feature vector x ¼ ðx 1 ,: :,x n Þ of an unseen unlabelled instance. The training dataset, in this multi-dimensional framework, is expressed as shown in Table <ref type="table">4</ref>.</p><p>Thus, the classifier c becomes a function that maps a vector of features X into a vector of class variables</p><formula xml:id="formula_0">C c : f0, . . . ,r 1 À1g Â Á Á Á Â f0, . . . ,r n À1g/f0, . . . ,t 1 À1g Â Á Á Á Â f0, . . . ,t m À1g x/c</formula><p>where r i and t j are the cardinalities of each feature X i (for i ¼ 1, . . . ,n) and each class variable C j (for j ¼ 1, . . . ,m) respectively. Note that we consider all variables, both predictive features and class variables, as discrete random variables.</p><p>A classifier is learnt from a training set (see Table <ref type="table">4</ref>) with a classifier induction algorithm AðÁÞ. Given the induction algorithm AðÁÞ, which is assumed to be a deterministic function of the training set, the multi-dimensional classifier obtained from a training set D is denoted as c ¼ AðDÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Related areas</head><p>In this paper we deal with multi-dimensional classification problems, and they must not be confused with other classification tasks which have similar designations, e.g. multi-class <ref type="bibr" target="#b49">[50]</ref>:</p><p>problems with a single class variable that can take more than two values, multi-task <ref type="bibr" target="#b8">[9]</ref>: an inductive transfer approach, where a main task is predicted with the help of the prediction of some extra tasks, or multi-label classification <ref type="bibr" target="#b50">[51]</ref>: where an instance can be classified with several different labels.</p><p>Note, however, that a multi-label problem can be easily modelled as a multi-dimensional classification problem where each label or category is a binary class variable whose value is one when the instance is included in that category or zero otherwise. The opposite, which is redefining multi-dimensional problems as multi-label problems, seems very unnatural and has an important drawback: current multi-label methods cannot always handle the multi-dimensional nature of this kind of problems (illustrated in Example 1). This limitation gave rise to the development of multidimensional techniques, which nowadays, is differentiated to multi-label learning in the machine learning research community, see for instance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref>. These concerns can be demonstrated by setting up the following simple example:</p><p>Example 1. Suppose we consider a multi-dimensional problem where we want to determine the sex, the colour of the eyes and hair colour given several characteristics of a person (as represented in Table <ref type="table" target="#tab_3">5</ref>). The problem has five discrete predictive variables and three class variables: Sex, Eyes and Hair. Sex have two possible values: male and female, Eyes has three: blue, dark and green, and Hair has four possible labels: black, blonde, brown and ginger. Note that this kind of problem is similar to our application due to the fact that both have several class variables with more than two values.</p><p>If we wanted to tackle this problem by means of a multi-label algorithm, we would have to force it to fit in the multi-label framework. The most straightforward way to transfer it is to treat each value of each class variable as one independent label, i.e. treat each value Male, Female, Blue, Dark, etc. as a different label. In order to accomplish that the following conversion has to be done:</p><p>1. First, define each instance as a list of three labels (view the last column of Table <ref type="table" target="#tab_3">5</ref>), one per each class variable. 2. Second, deal with the main drawback that this approach has: there are several configuration of labels that are forbidden and/or senseless in the original multi-dimensional problem.</p><p>For that reason, several restrictions to the multi-label technique have to be added in order to reflect the true nature of the problem. These constraints are the following: (a) Fix the number of labels per each instance, e.g. forbid the instances classified as {Male} or {Male, Blue, Black, Ginger}. Each instance must have just three labels. (b) Ensure that each instance has just one label per each class variable of the original multi-dimensional problem. We cannot classify an instance as {Male, Female}.</p><p>To the best of our knowledge, adapting multi-label techniques by adding several restrictions to deal with this type of problems, as stated in the previous paragraphs, has not been proposed by</p><formula xml:id="formula_1">Table 3 A possible representation of a (uni-dimensional) labelled training dataset. X 1 X 2 y X n C x<label>ð1Þ</label></formula><formula xml:id="formula_2">1 x<label>ð1Þ</label></formula><formula xml:id="formula_3">2 y x<label>ð1Þ</label></formula><formula xml:id="formula_4">n c<label>ð1Þ</label></formula><p>x ð2Þ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Representation of a multi-dimensional labelled training dataset.  In analogous fashion, the exposed ASOMO problem cannot be solved by means of a multi-label technique. It has two class variables, Will to Influence and Sentiment polarity, with four and five values, respectively. For that reason, multi-label techniques cannot be applied to the application presented in this paper.</p><formula xml:id="formula_5">X 1 X 2 y X n C 1 C 2 y C m x<label>ð1Þ</label></formula><formula xml:id="formula_6">1 x<label>ð1Þ</label></formula><formula xml:id="formula_7">2 y x<label>ð1Þ</label></formula><formula xml:id="formula_8">n c<label>ð1Þ</label></formula><formula xml:id="formula_9">1 c<label>ð1Þ</label></formula><formula xml:id="formula_10">2 y c<label>ð1Þ</label></formula><formula xml:id="formula_11">m x<label>ð2Þ</label></formula><formula xml:id="formula_12">1 x<label>ð2Þ</label></formula><formula xml:id="formula_13">2 y x<label>ð2Þ</label></formula><formula xml:id="formula_14">n c<label>ð2Þ</label></formula><formula xml:id="formula_15">1 c<label>ð2Þ</label></formula><formula xml:id="formula_16">2 y c<label>ð2Þ</label></formula><formula xml:id="formula_17">m ^^^^^^^x<label>ðNÞ</label></formula><p>In addition to multi-label classification, other classification tasks in pattern recognition can also naturally be modelled as a multi-dimensional classification problem <ref type="bibr" target="#b4">[5]</ref>. For instance, structured prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>, where there are several class variables with a conditional structure among them, or hierarchical classification <ref type="bibr" target="#b18">[19]</ref>, where there is a hierarchical structure (two or more levels) among the class variables. Therefore, the multi-dimensional techniques can be applied to these subproblems as the set of multi-dimensional problems contains these well known subproblems, as shown in Fig. <ref type="figure" target="#fig_5">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-dimensional classification rule</head><p>In probabilistic classification, the induction multi-dimensional algorithm learns a probability distribution pðx,cÞ or pðc9xÞ from the training data and classifies a new unlabelled instance based on it. For that purpose, a classification rule must be defined.</p><p>In uni-dimensional supervised classification, the most common classification rule returns the most likely class value given the features</p><formula xml:id="formula_18">ĉ ¼ arg max c fpðc9x 1 , . . . ,x n Þg</formula><p>The multi-dimensional nature of the problem allows us to develop several classification rules that would make no sense in single-class classification because they take into account multiple class variables. Nevertheless, the previous one-dimensional classification rule can be easily generalised to the prediction of more than one class variable. In this case, the multi-dimensional classifier returns the most probable combination of class variables given the features. This rule is known as joint classification rule <ref type="bibr" target="#b43">[44]</ref> </p><formula xml:id="formula_19">ð ĉ1 , . . . , ĉm Þ ¼ arg max c 1 ,...,cm fpðc 1 , . . . ,c m 9x 1 , . . . ,x n Þg</formula><p>Although several other classification rules are proposed in <ref type="bibr" target="#b43">[44]</ref>, it is shown that the joint classification rule obtains better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-dimensional classification evaluation</head><p>Once a classifier is constructed, its associated error needs to be measured. The prediction error of a single-class classifier c is the probability of the incorrect classification of an unlabelled instance x and is denoted as EðcÞ EðcÞ ¼ pðcðXÞ a CÞ ¼ E X ½dðc,cðxÞÞ where dðx,yÞ is a loss function whose results are 1 if x a y and 0 if x¼y.</p><p>However, in multi-dimensional classification, the correctness of a classifier can be measured in two different ways:</p><p>Joint evaluation: This consists of evaluating the estimated values of all class variables simultaneously, that is, it only counts a success if all the classes are correctly predicted, and otherwise it counts an error</p><formula xml:id="formula_20">EðcÞ ¼ pðcðXÞ a CÞ ¼ E X ½dðc,cðxÞÞ</formula><p>This rule is the generalisation of the previous single-class evaluation measure to multi-dimensional classification.</p><p>Single evaluation: After a multi-dimensional learning process, this consists of separately checking if each class is correctly classified. For example, if we classify an instance x as ð ĉ1 ¼ 0, ĉ2 ¼ 1Þ and the real value is ðc 1 ¼ 0,c 2 ¼ 0Þ, we count ĉ1 as a success and ĉ2 as an error. This approach provides one performance measure for each class C j (for j ¼ 1, . . . ,m).</p><p>The output of this evaluation is a vector e of size m with the performance function of the multi-dimensional classifier for each of the class variables E j ðcÞ ¼ pðc j ðXÞ a C j Þ ¼ E X ½dðc j ,c j ðxÞÞ where c j ðxÞ is the estimation of the multi-dimensional classifier for the j-th class variable.</p><p>Ideally, we would like to exactly calculate the error of a classifier, but in most real world problems the feature-label probability distribution pðx,cÞ is unknown. So, the prediction error of a classifier c is also unknown; it cannot be computed exactly, and thus, must be estimated from data.</p><p>Several approaches to estimate the prediction error can be used. In this work, we use one of the most popular error estimation techniques: k-fold cross-validation (k-cv) <ref type="bibr" target="#b48">[49]</ref> in its repeated version. In k-cv the dataset is divided into k folds, a classifier is learnt using kÀ1 folds and an error value is calculated by testing the learnt classifier in the remaining fold. Finally, the k-cv estimation of the error is the average value of the errors made in each fold. The repeated r times k-cv consists of estimating the error as the average of r k-cv estimations with different random partitions into folds. This method considerably reduces the variance of the error estimation <ref type="bibr" target="#b44">[45]</ref>.</p><p>In multi-dimensional classification we could be interested in either learning the most accurate classifier for all class variables simultaneously (measured with a joint evaluation) or in finding the most accurate classifier for each single class variable (measured with single evaluations). In this paper, we are mainly interested in using the joint evaluation for evaluation. However, in our application to SA we also measure the performance of the algorithms with a single evaluation per each class variable in order to compare both types of evaluation and perform a deeper analysis of the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-dimensional Bayesian network classifiers</head><p>In this section, multi-dimensional class Bayesian network classifiers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b52">53]</ref>, which are able to deal with multiple class variables to be predicted, are presented as a recent generalisation of the classical Bayesian network classifiers <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bayesian network classifiers</head><p>Bayesian networks are powerful tools for knowledge representation and inference under uncertainty conditions <ref type="bibr" target="#b38">[39]</ref>. These formalisms have been extensively used as classifiers <ref type="bibr" target="#b30">[31]</ref> and have become a classical and well-known classification paradigm.</p><p>A Bayesian network is a pair B ¼ ðS,HÞ where S is a directed acyclic graph (DAG) whose vertices correspond to random variables and whose arcs represent conditional (in)dependence relations among variables, and where H is a set of parameters.</p><p>A Bayesian classifier is usually represented as a Bayesian network with a particular structure. The class variable is on the top of the graph and it is the parent of all predictive variables.</p><p>In spite of the popularity of Bayesian network classifiers, few works have taken into account their generalisation to multiple class variables <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref>. In multi-dimensional classification, we consider Bayesian networks over a finite set V ¼ fC 1 , . . . ,C m ,X 1 , . . . ,X n g where each class variable C j and each feature X i takes a finite set of values. H is formed by parameters p ijk and y ijk , where p ijk ¼ p</p><formula xml:id="formula_21">ðC i ¼ c k 9PaðC i Þ ¼ Paðc i Þ j Þ for each value c k that can take each class variable C i and for each value assignment Paðc i Þ j to the set of the parents of C i . Similarly, y ijk ¼ pðX i ¼ x k 9PaðX i Þ ¼ Paðx i Þ j Þ for each value</formula><p>x k that can take each feature X i and for each value assignment Paðx i Þ j to the set of the parents of X i .</p><p>Thus, the network B defines a joint probability distribution pðc 1 , . . . ,c m ,x 1 , . . . ,x n Þ which is given by</p><formula xml:id="formula_22">pðc 1 , . . . ,c m ,x 1 , . . . ,x n Þ ¼ Y m i ¼ 1 p ijk Y n i ¼ 1 y ijk</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Structure of multi-dimensional class Bayesian network classifiers</head><p>A multi-dimensional class Bayesian network classifier is a generalisation of the classical one-class variable Bayesian classifiers for domains with multiple class variables <ref type="bibr" target="#b52">[53]</ref>. It models the relationships between the variables by means of directed acyclic graphs (DAG) over the class variables and over the feature variables separately, and then connects the two sets of variables by means of a bi-partite directed graph. So, the DAG structure S ¼ ðV,AÞ has the set V of random variables partitioned into the sets V C ¼ fC 1 , . . . ,C m g, m 4 1, of class variables and the set V F ¼ fX 1 , . . . ,X n g ðn Z 1Þ of features. Moreover, the set of arcs A can be partitioned into three sets: A CF , A C and A F with the following properties: A F D V F Â V F is composed of the arcs between the feature variables, so we can define the feature subgraph of S induced by V F as S F ¼ ðV F ,A F Þ. Multi-dimensional tree-augmented Bayesian network classifier (MDTAN): Both the class subgraph and the feature subgraph are directed trees. It could be viewed as the multi-dimensional version of the (uni-dimensional) tree-augmented Bayesian network classifier (TAN) proposed in <ref type="bibr" target="#b23">[24]</ref>.</p><formula xml:id="formula_23">A CF DV C Â V F is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-dimensional J/K dependences Bayesian classifier (MD J/K):</head><p>This structure is the multi-dimensional generalisation of the well-known K-DB <ref type="bibr" target="#b46">[47]</ref> classifier. It allows each class variable C i to have a maximum of J dependences with other class variables C j , and each predictive variable X i to have, apart from the class variables, a maximum of K dependences with other predictive variables. In the following section, several algorithms are provided in order to learn from a given dataset the previous sub-families of multi-dimensional Bayesian network classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning multi-dimensional Bayesian network classifiers</head><p>As in the classic Bayesian network learning task, learning a multi-dimensional class Bayesian network classifier from a training dataset consists of estimating its structure and its parameters. These two subtasks are called structure learning and parameter learning respectively. Due to the fact that each previously introduced sub-family has different restrictions in its structure, a different learning algorithm is needed for each one.</p><p>In this section, we provide algorithms for learning MDnB <ref type="bibr" target="#b52">[53]</ref>, MDTAN <ref type="bibr" target="#b52">[53]</ref> and MD J/K classifiers from a given dataset. The MDnB and MD J/K learning algorithms use a filter approach, i.e. the learning task precedes the classification evaluation. However, as it is proposed in its original work <ref type="bibr" target="#b52">[53]</ref>, the MDTAN learning algorithm is formulated as a wrapper approach <ref type="bibr" target="#b24">[25]</ref>, i.e. it tries to find more accurate classifiers by taking advantage of the classification evaluation.</p><p>As Fig. <ref type="figure">5</ref> shows, in the MDnB classifier, each class variable is parent of all the features, and each feature has only all the class variables as parents. Conventionally, the class subgraph and the feature subgraph are empty and the feature selection subgraph is complete. This classifier assumes conditional independence between each pair of features given the entire set of class variables. Due to the fact that it has no structure learning (the structure is fixed for a determined number of class variables and features), learning a MDnB classifier consists of just estimating the parameters Y of the actual model by using a training dataset D. This is achieved by calculating the maximum likelihood estimator (MLE) <ref type="bibr" target="#b14">[15]</ref>.</p><p>Instead, learning a MDTAN classifier consists of learning both structure and parameters. A wrapper structure learning algorithm is proposed in <ref type="bibr" target="#b52">[53]</ref>. Its aim is to produce the MDTAN structure (see Fig. <ref type="figure">6</ref>) that maximises the accuracy from a given dataset. This algorithm has a main part called Feature subset selection algorithm, which follows a wrapper approach <ref type="bibr" target="#b24">[25]</ref> by performing a local search over the A CF structure. In order to obtain a MDTAN structure in each iteration, it generates a set of different A CF structures from a current A CF and learns its class subgraph and feature subgraph by using the following sub-algorithms:</p><p>1. A C structure learning algorithm is the algorithm that learns the structure between the class variables by building a maximum weighted spanning <ref type="bibr" target="#b28">[29]</ref> tree using mutual information. 2. A F structure learning algorithm learns the A F subgraph by using conditional mutual information, by means of the Chow and Liu algorithm <ref type="bibr" target="#b10">[11]</ref>.</p><p>After that, the accuracies of all the learnt models are computed. The iterative process continues by setting the A CF of the best classifier, in terms of estimated accuracy, as current A CF . This algorithm belongs to the hill-climbing family of optimisation algorithms, i.e. when no improvement is achieved by generating a new set of structures, the algorithm stops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Multi-dimensional J/K dependences Bayesian classifier</head><p>The MD J/K (see Fig. <ref type="figure" target="#fig_10">7</ref>), which is introduced in <ref type="bibr" target="#b43">[44]</ref>, is the generalisation of the K dependence structure <ref type="bibr" target="#b46">[47]</ref> to the multidimensional framework. It is able to move through the spectrum of allowable dependence in the multi-dimensional framework, from the MDnB to the full multi-dimensional Bayesian classifier. Note that from the setting J ¼ K ¼ 0, we can learn a MDnB, setting J ¼ K ¼ 1 a MDTAN structure is learned and so on. The full multidimensional Bayesian classifier, which is the classifier that has the three complete subgraphs, can be learnt by setting J ¼ ðmÀ1Þ and K ¼ ðnÀ1Þ, where m and n are the number of class variables and predictive features respectively.</p><p>Although the MD J/K structure has been proposed in the stateof-the-art literature, to the best of our knowledge, a specific MD J/ K learning algorithm for the multi-dimensional framework has not been defined by the research community. To bridge this gap, in this paper, we propose a filter algorithm in a supervised learning framework capable of learning this type of structure (see <ref type="bibr">Algorithm 1)</ref>.</p><p>In this algorithm, we do not directly use the mutual information as measured in the previous MDTAN learning algorithm <ref type="bibr" target="#b52">[53]</ref>. This is due to the fact that the mutual information is not normalised when the cardinalities of the variables are different, so we use an independence test to determine if a dependence between two variables is strong enough to be part of the model: It  is known <ref type="bibr" target="#b29">[30]</ref> that 2N ÎðX i ,X j Þ asymptotically follows a w 2 distribution with ðr i À1Þðr j À1Þ degrees of freedom, where N is the number of cases, if X i and X j are independent, i.e. Lim N-1 2 N ÎðX i ,X j Þ*w 2 ðr i À1Þðr j À1Þ .</p><p>Algorithm 1. A MD J/K structure learning algorithm using a filter approach.</p><p>1. Learn the A C structure 1. Calculate the p-value using the independence test for each pair of class variables, and rank them. 2. Remove the p-value higher than the threshold ð1Às a Þ ¼ 0:10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Use the ranking to add arcs between the class variables</head><p>fulfilling the conditions of no cycles between the class variables and no more than J-parents per class. 2. Learn the A CF structure 1. Calculate the p-value using the independence test for each pair C i and X j and rank them. 2. Remove the p-value higher than the threshold ð1Às a Þ ¼ 0:10. 3. Use the ranking to add arcs from the class variables to the features. 3. Learn the A F structure 1. Calculate the p-value using the conditional independence test for each pair X i and X j given Pa c ðX j Þ and rank them. 2. Remove the p-value higher than the threshold ð1Às a Þ ¼ 0:10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Use the ranking to add arcs between the class variables</head><p>fulfilling the conditions of no cycles between the features and no more than K-parents per feature.</p><p>Based on this result, a statistical hypothesis test can be carried out in a multi-dimensional Bayesian network classifier to check the robust dependences in A C . The null hypothesis H 0 is that the random variables C i and C j are independent. If the quantity 2N ÎðC i ,C j Þ surpasses a threshold s a for a given test size</p><formula xml:id="formula_24">a ¼ Z 1 sa w 2 ðt i À1Þðt j À1Þ ds</formula><p>where t i is the cardinality of C i and t j the cardinality of C j , the null hypothesis is rejected and a dependence between C i and C j is considered. Therefore the arc between these class variables is included in the model. The dependences in A CF are calculated using the same procedure, the null hypothesis H 0 is that ''The random variables C i and X j are independent''. So, if 2N ÎðC i ,X j Þ surpasses the threshold s a , then the null hypothesis is rejected and an arc is included in the model. This test was also used on single-class Bayesian network classifiers to check the dependences among the class variables and the features <ref type="bibr" target="#b6">[7]</ref>.</p><p>Using this approach, the structures A C and A CF are learnt in steps 1 and 2 of Algorithm 1, respectively.</p><p>In order to calculate the structure F , we need to use the conditional mutual information between a feature X i and a feature X j given its class parents Pa c ðX j Þ to determine if the relation between both predictive features should be included in the model. For that purpose, we use the generalisation of the previous result to the case of conditional mutual information as defined in <ref type="bibr" target="#b29">[30]</ref> Lim N-1 2N ÎðX i ,X j 9Pa c ðX j ÞÞ*w 2 ðr i À1Þðr j À1Þð9PacðX j Þ9Þ where r i is the cardinality of X i , r j the cardinality of X j and 9Pa c ðX j Þ9 the cardinality of the class parents of X j .</p><p>Analogously to the hypothesis test previously described, based on these results we can perform the following conditional independence test: the null hypothesis assumes that the random variables X i and X j are conditionally independent given Pa c ðX j Þ. So, if the quantity 2N ÎðC i ,C j 9Pa c ðX j ÞÞ surpasses a threshold s a for a given test size a ¼ Z 1 sa w 2 ðt i À1Þðt j À1Þð9PacðX j Þ9Þ ds the null hypothesis is rejected and the random variables X i and X j are considered dependent given Pa c ðX j Þ. Therefore, the arc is included in the model. The structure A CF is learnt using this hypothesis test in step 3 of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Semi-supervised multi-dimensional classification</head><p>In this section, we proceed with the extension of the previous multi-dimensional learning algorithms to the semi-supervised learning framework. When large amounts of labelled data are available, one can apply familiar and powerful machine learning techniques such as the previous multi-dimensional Bayesian network algorithms in order to learn accurate classifiers. However, when there is a scarcity of such labelled data and a huge amount of unlabelled data, as happens in the SA domain, one can wonder if it is possible to learn competitive classifiers from unlabelled data.</p><p>In this context, where the training dataset consists of labelled and unlabelled data, the semi-supervised learning approach [10,57,58] appears as a promising alternative. It is motivated from the fact that in many real world problems, obtaining unlabelled data is relatively easy, e.g. collecting posts from different blogs, while labelling is expensive and/or labor intensive, due to the fact that the tasks of labelling the training dataset is usually carried out by human beings. Thus, it is highly desirable to have learning algorithms that are able to incorporate a large number of unlabelled data with a small number of labelled data when learning classifiers.</p><p>In the semi-supervised learning framework, the training dataset D, as shown in Table <ref type="table" target="#tab_3">5</ref>, is divided into two parts: the subset of instances D L for which labels are provided, and the subset D U , where the labels are not known. Therefore, we have a dataset of N instances, where there are L labelled examples and ðNÀLÞ unlabelled examples. Normally, ðNÀLÞ b L, i.e. the unlabelled subset tends to have a very large amount of instances whilst the labelled subset tends to have a small size.</p><p>Therefore, the aim of a semi-supervised learning algorithm is to build more accurate classifiers using both labelled and unlabelled data, rather than using exclusively labelled examples as happens in supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Learning multi-dimensional Bayesian network classifiers in the semi-supervised framework</head><p>In this section, we propose the extension of multi-dimensional Bayesian network classifiers to the semi-supervised learning framework by using the EM algorithm <ref type="bibr" target="#b17">[18]</ref>. Although this method was proposed in <ref type="bibr" target="#b17">[18]</ref> and was deeply analysed in <ref type="bibr" target="#b33">[34]</ref>, it had been used much earlier, e.g. <ref type="bibr" target="#b25">[26]</ref>, and it is still widely used in many recent semi-supervised learning algorithms, e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>The aim of the EM algorithm as typically used in semisupervised learning is to find the parameters of the model that maximise the likelihood of the data, using both labelled and unlabelled instances. The iterative process, which ensures that the likelihood is maximised in each step, works as follows: in the Kth iteration the algorithm alternates between completing the unlabelled instances by using the parameters H ðKÞ (E-step) and updating the parameters of the model H ðK þ 1Þ using MLE with the whole dataset (M-step), i.e. the labelled data and the unlabelled instances that have been previously classified in the E-Step. Note that the structure remains fixed in the whole iterative process.</p><p>Although good results have been achieved with the EM algorithm in uni-dimensional classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>, we are concerned about the restriction of only maximising the parameters of a fixed structure in our extension of the EM algorithm to the multi-dimensional domain, where there are several class variables to be predicted. As stated in <ref type="bibr" target="#b11">[12]</ref>, if the correct structure of the real distribution of the data is obtained, unlabelled data improve the classifier, otherwise, unlabelled data can actually degrade performance. For this reason, it seems more appropriate to perform a structural search in order to find the real model. Thus, we perform several changes to the EM algorithm in order to avoid fixing the structure of the model during the iterative process. The proposal is shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2. Our version of the EM Algorithm.</head><p>Input A training dataset with both labelled and unlabelled data (Table <ref type="table">6</ref>) and an model c ðK ¼ 0Þ with a fixed structure and with an initial set of parameters H ðK ¼ 0Þ . 1: while the model c ðKÞ does not converge do In this version of the EM algorithm, we want to find the model, both structure and parameters, that maximises the likelihood of the whole dataset. So, in this version, the iterative process is performed as follows: in the Kth iteration, the algorithm alternates between completing the unlabelled instance by the previously learnt model c ðKÞ (E-step) and learning a new model c ðK þ 1Þ by using a learning algorithm with the whole dataset, both labelled and completed instances (M-step). In the semi-supervised learning research community, the input initial parameter c ðK ¼ 0Þ of the EM Algorithm is usually learnt from the labelled subset D L . Hence, we will continue to use this modus operandi in this version of the algorithm. Note that our version of the EM algorithm is closer to the Bayesian structural EM algorithm proposed in <ref type="bibr" target="#b22">[23]</ref> rather than the original formulation of the algorithm <ref type="bibr" target="#b17">[18]</ref>. However, in the case of the MDnB classifier, it is just a parametric search since it has a fixed structure.</p><p>Using Algorithm 2, all the supervised learning approaches proposed in the previous section can be straightforwardly used in this semi-supervised scenario. The learning algorithm is used in the M-step, where it learns a model using labelled and unlabelled data that have been previously labelled in the E-step. So, applying our adaptation of the EM Algorithm, we have extended the multidimensional Bayesian network classifiers to the semi-supervised learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Artificial experimentation</head><p>Before solving the ASOMO problem, we have tested our proposed semi-supervised algorithms over a set of designed artificial datasets as commonly carried out in the machine learning research community. This has been done due to the fact that, unfortunately, we cannot apply our proposals in the baseline datasets of the SA domain. The multi-dimensional classification paradigm has recently been proposed in the research community <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref>, and, to the best of our knowledge, there are no benchmark multi-dimensional datasets in this domain to test our proposals as they consider just one target variable at a time (usually sentiment polarity). So, in order to bridge this gap, we provide a detailed report of these artificial experiments on several synthetic datasets in the following website. 3  The major conclusions extracted from this experimentation can be summarised as follows:</p><p>1. As happens in the uni-dimensional framework <ref type="bibr" target="#b11">[12]</ref>, when using the real structure to semi-supervisely learnt multidimensional classifiers, the unlabelled data always help. 2. There is a tendency to achieve better classifiers in terms of joint accuracy in the semi-supervised framework when the used multi-dimensional algorithm can reach the generative structure. 3. In the uni-dimensional approaches, performance degradation occurs in the semi-supervised framework. This is probably due to the fact that the uni-dimensional approaches are not able to match the actual multi-dimensional structure of the problems. 4. Although there are small differences between the uni-dimensional and the multi-dimensional approaches in the supervised framework (only the MD J/K reports statistical differences), in the semi-supervised framework these differences grow larger (except for the case of the MDTAN learning algorithm, the qrest of the multi-dimensional approaches report statistical differences). 5. In the semi-supervised framework, clearly the multi-dimensional classifiers outperform the uni-dimensional techniques, with the exception of the MDTAN classifier. 6. The MDnB learning algorithm <ref type="bibr" target="#b52">[53]</ref> is very specific, it obtains very good results when dealing with problems with an underlying MDnB structure, but when the generative models are more complex, its rigid structure makes the algorithm lead to very suboptimal solutions. 7. The MDTAN algorithm <ref type="bibr" target="#b52">[53]</ref> also shows very poor performances in the semi-supervised framework. 8. The MD J/K learning algorithms have great flexibility to capture different types of complex structures, which results in an improvement in terms of joint accuracy in the semisupervised framework.</p><p>Table <ref type="table">6</ref> A formal representation of a multi-dimensional semi-supervised training dataset. In brief, these artificial experiments show that not only the multi-dimensional approaches statistically outperform the unidimensional approaches in the supervised framework when dealing with multi-dimensional problems, but also more accurate classifiers can be found using the semi-supervised learning framework.</p><formula xml:id="formula_25">X 1 X 2 y X n C 1 C 2 y C m D L x<label>ð1Þ</label></formula><formula xml:id="formula_26">1 x<label>ð1Þ</label></formula><formula xml:id="formula_27">2 y x<label>ð1Þ</label></formula><formula xml:id="formula_28">n c<label>ð1Þ</label></formula><formula xml:id="formula_29">1 c<label>ð1Þ</label></formula><formula xml:id="formula_30">2 y c<label>ð1Þ</label></formula><formula xml:id="formula_31">m x<label>ð2Þ</label></formula><formula xml:id="formula_32">1 x<label>ð2Þ</label></formula><formula xml:id="formula_33">2 y x<label>ð2Þ</label></formula><formula xml:id="formula_34">n c<label>ð2Þ</label></formula><formula xml:id="formula_35">1 c<label>ð2Þ</label></formula><formula xml:id="formula_36">2 y c<label>ð2Þ</label></formula><formula xml:id="formula_37">m ^^^^^^^x ðLÞ 1 x ðLÞ 2 y x ðLÞ n c<label>ðLÞ</label></formula><formula xml:id="formula_38">1 c<label>ðLÞ</label></formula><formula xml:id="formula_39">2 y c ðLÞ m D U x ðL þ 1Þ 1 x ðL þ 1Þ 2 y x ðL þ 1Þ n ? ? y ? x ðL þ 2Þ 1 x ðL þ 2Þ 2 y x ðL þ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Solving the ASOMO SA problem</head><p>Two different series of experiments with the ASOMO corpus were performed: the first series (Section 7.2.1) shows the comparison between the ASOMO features and three different state-ofthe-art feature sets broadly used in the SA domain. The second series (Section 7.2.2) is devoted to demonstrate that the addition of unlabelled instances could achieve better results in this very application. There, a multi-dimensional classification solution for the ASOMO problem is proposed and analysed. By means of these experiments in a real SA problem, we would like to shed some light on the truthfulness of the following hypotheses:</p><p>1. The choice of the feature set is a key matter when dealing with the exposed problem, and in extension, with the SA problems. 2. The uni-dimensional models obtained with the common approaches of the SA domain yield suboptimal solutions to the ASOMO problem. 3. The explicit use of the relationships between the class variables in this real-world problem can be beneficial to improve their recognition rates, i.e. multi-dimensional techniques are able to outperform the most common uni-dimensional techniques. 4. When there is a scarcity of labelled data, multi-dimensional techniques can work with unlabelled data in order to improve the classification rates in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1.">Comparison between different feature sets in both uni-dimensional and multi-dimensional learning scenarios</head><p>By means of this experiment, we evaluate the new feature set proposed in Section 2 (the ASOMO features) with the most commonly used in the related literature. If we only use them to test the multi-dimensional Bayesian network classifiers in this real dataset, it is difficult to assess their efficacy without using other viable baseline methods. For this reason, we also use three different commonly used feature sets in order to perform a benchmark comparison: unigrams, unigramsþ bigrams and PoS. In order to avoid computation burden, we limited consideration to (1) the 357 unigrams that appear at least five times in our 150-post corpus, (2) the 563 unigrams and bigrams occurring at least five times, and (3) the PoS to 766 features. No stemming or stoplists were used. These benchmark feature sets have been constructed using the TagHelper tool <ref type="bibr" target="#b45">[46]</ref>. Finally, since the ASOMO features are continuous, they were discretised into three values using equal frequency discretisation in order to apply the algorithms proposed in this paper.</p><p>To assess the efficacy and efficiency of the exposed multidimensional Bayesian network classifiers in this problem, we conducted a comparison in the supervised framework between the presented multi-dimensional classifiers and the two different uni-dimensional attempts to adapt single-class classifiers to multi-dimensional problems: (1) develop multiple uni-dimensional classifiers and (2) construct a Cartesian product class variable. In order to perform such comparison, we use naive Bayes classifiers in the uni-dimensional attempts (one per each class variable in the first uni-dimensional approach, and another one for the compound class of the second) and a MDnB classifier in the multi-dimensional solution. The parameters of both types of models are calculated by MLE corrected with Laplace smoothing and the forbidden configurations of the vector of class variables have been taken into account in the learning process. Note that this comparison is performed in the supervised framework, so the 2392 unlabelled posts were not used in this experiment.</p><p>The classification accuracies, which have been estimated via 20 runs of 5-fold non-stratified cross validation (20 Â 5cv) <ref type="bibr" target="#b44">[45]</ref>, are shown in Table <ref type="table" target="#tab_5">7</ref>. The results of using the four different feature sets (unigrams, unigrams þ bigrams, PoS, and the ASOMO features) in conjunction with the three different learning approaches (multiple uni-dimensional, Cartesian class variable, and multi-dimensional classifiers) in a supervised framework are shown. The first eight rows of Table <ref type="table" target="#tab_5">7</ref> correspond to the two unidimensional approaches while the last four correspond to the multi-dimensional approach. In addition to the estimation of the joint accuracy, the accuracies per each class variable are also shown.</p><p>For each column, the best estimated accuracy rate is highlighted in bold for each single class variable and the joint accuracy. Moreover, statistical hypothesis tests (Student's t-test with a ¼ 0:05) have been performed in order to determine if there are significant statistical differences between the tested accuracies. For each column, the symbol 'y' is used to mark the accuracies that are statistically outperformed by the highlighted best estimated accuracy (in bold). The symbol 'z' corresponds to the accuracies that, despite not being significantly worse, show a p-value on the significant borderline ð0:05 o pÀvalue r0:1Þ. Besides, the CPU time spent on learning a classification model with the 150 posts (for each approach and feature set) is shown in the results. Based on the results of Table <ref type="table" target="#tab_5">7</ref>, different conclusions can be extracted:</p><p>1. With respect to the feature set comparison, the ASOMO feature set significantly outperforms the n-grams, not only in terms of joint accuracy, but also in terms of accuracy of two (out of the three) dimensions, i.e. in Will to Influence and in Subjectivity. Also, as stated in <ref type="bibr" target="#b37">[38]</ref>, n-grams information is the most effective feature set in the task of Sentiment Polarity classification. However, the unigramsþbigrams feature set outperforms unigrams instead of the opposite as reported in <ref type="bibr" target="#b37">[38]</ref>.</p><p>Finally, the PoS approach obtains very low performance. 2. According to the comparison between the uni-dimensional and multi-dimensional approaches, the best joint accuracy is given by the MDnB model when using the ASOMO features. It significantly outperforms both the multi-dimensional approaches with the state-of-the-art feature sets, and the uni-dimensional approaches. Looking at the class variables in isolation, the unidimensional approaches only outperform the multi-dimensional approach in the case of the Will to Influence target dimension. 3. An analysis of the CPU computational times show that, as expected, learning one classifier per each class variable is the most time-consuming. Next, multi-dimensional Bayesian network classifiers are found. The least-time consuming approach is the uni-dimensional learning process that uses a compound class. Moreover, the number of features is also an important issue with respect to the computational time. The ASOMO feature set, which has only 14 attributes, is, by far, the least time-consuming of the four different types of feature set.</p><p>However, in this problem, errors are not simply present or absent, their magnitude can be computed, e.g. it is not the same to misclassify a negative post as having a very negative sentiment or misclassify it with a very positive sentiment. For that reason, in addition to the accuracy term, we also estimate the numeric error of each classifier. Note that the values of the three class variables can be trivially translated into ordinal values without changing their meaning. Therefore, using this approach, the previous example could be exposed as: it is not the same misclassify a post, which has its sentiment equal to 2, with a 1 or misclassify it with a 5.</p><p>In order to estimate the numeric error, we use the mean absolute error (MAE) term <ref type="bibr" target="#b54">[55]</ref>, which is a measure broadly used in evaluating numeric prediction. It is defined as the measure that averages the magnitude of the individual errors without taking their sign into account. It is given by the following formula:</p><formula xml:id="formula_40">MAEE j ðcÞ ¼ P N i ¼ 1 9c j ðx ðiÞ ÞÀc ðiÞ j 9 N</formula><p>where c j ðx ðiÞ Þ is the value of the class variable C j resulting from the classification of the instance x ðiÞ using the classifier c j and c ðiÞ j is the actual class value in that instance. N is the number of instances in the test set. Note that the resulting error varies between 0 and ð9C j 9À1Þ, where 9C j 9 is the cardinality of the class variable C j .</p><p>In a similar way to the accuracy, we also compute a joint measure for simultaneously characterising this error in all the class variables. Due to this, we estimate the joint MAE (JMAE) for each learning algorithm. It is the sum of the normalised value of the MAE term in each class variable</p><formula xml:id="formula_41">JMAEEðcÞ ¼ X m j ¼ 1 1 9C j 9À1 MAEE j<label>ðcÞ</label></formula><p>Note that the JMAE term varies between 0 and m, being m the number of class variables.</p><p>The MAE values of the exposed experimentation setup, which have also been estimated via 20 runs of 5-fold non-stratified cross validation <ref type="bibr" target="#b44">[45]</ref>, are shown in Table <ref type="table" target="#tab_6">8</ref>. It has the same shape as in Table <ref type="table" target="#tab_5">7</ref>, i.e. each row represents each learning algorithm with a specific feature set, and each column represents each class variable and the JMAE value. The best estimated error per classifier is also highlighted in bold and Student's t-tests ða ¼ 0:05Þ have been performed in order to study the significance of estimated differences. Table <ref type="table" target="#tab_6">8</ref> reports conclusions similar to the ones extracted with the accuracy term:</p><p>1. The feature set comparison reports the same conclusions to those obtained with the accuracy: the ASOMO feature set significantly outperforms the n-grams and PoS, not only in terms of joint accuracy, but also in terms of two (out of the three) dimensions. 2. The best joint accuracy is given by the multi-dimensional approach which uses the ASOMO feature set and it significantly outperforms both the multi-dimensional approaches with the state-of-the-art feature sets, and the uni-dimensional approaches. With respect to the class variables in isolation, the only case in which the uni-dimensional approaches outperform the multi-dimensional approach is in the Sentiment Polarity target dimension.</p><p>In brief and regarding the feature set, for this specific problem, we strongly recommend the use of the ASOMO features, not only because of their performance, but also for their lower learning times. The results also show that the multi-dimensional classification approach to SA is a novel attractive point of view that needs to be taken into account due to the fact that it could lead to better results in terms of accuracy as well as in MAE. In addition, learning a multi-dimensional classifier is faster than learning With the knowledge that the ASOMO features can lead us to better classification rates in this problem, we evaluated their performance in both supervised and semi-supervised frameworks. With this experiment we want to determine if the use of unlabelled examples when learning a classifier can lead to better solutions to the ASOMO problem. In order to do so, the following experiment was performed: the ASOMO dataset has been used to learn three different (uni-dimensional) Bayesian network classifiers and three different sub-families of multi-dimensional classifiers in both frameworks.</p><p>For uni-dimensional classification, naive Bayes classifier (nB), tree-augmented Bayesian network classifier (TAN) <ref type="bibr" target="#b23">[24]</ref> and a 2-dependence Bayesian classifier (2 DB) <ref type="bibr" target="#b46">[47]</ref> have been chosen. The uni-dimensional approach selected for these experiments is that which consists of splitting the problem into three different uni-dimensional problems (this is because it is more common in the state-of-the-art solutions to solve different problems rather than create an artificial class variable by means of the Cartesian product). From the multi-dimensional aspect, MDnB, MDTAN, MD1/1 and MD2/K (with K¼2,3,4) structures have been selected. MD1/1 is included as an algorithm able to learn MDTAN structures due to the poor performance shown by the MDTAN learning algorithm <ref type="bibr" target="#b52">[53]</ref> in the artificial experiments. Although both multidimensional learning approaches learn MDTAN structures, each learning algorithm follows a different path to come to that end.</p><p>While the MD 1/1 uses a filter approach, the MDTAN learning algorithm follows a wrapper scheme.</p><p>The supervised learning procedure only uses the labelled dataset (consisting of 150 documents), whilst the semi-supervised approach uses the 2532 posts (2392 unlabelled). Our multidimensional extension of the EM algorithm is used in the latter approach and it terminates after finding a local likelihood maxima or after 250 unsuccessful trials.</p><p>Finally, the performance of each model has been estimated via 20 runs of 5-fold non-stratified cross validation. Due to fact that, in semisupervised learning, the labels of the unlabelled subset of instances are unknown, only the labelled subset is divided into five folds to estimate the performance of the proposed approaches. So, in each iteration of the cross validation, a classifier is learnt with four labelled folds and the whole unlabelled subset, and then it is tested in the remaining labelled fold. This modified cross validation is illustrated in Fig. <ref type="figure" target="#fig_12">8</ref> for the case of three folds. As done in the previous experiments, we use the accuracy and the MAE terms as evaluation measures.</p><p>Table <ref type="table" target="#tab_7">9</ref> shows the results of applying the different unidimensional and multi-dimensional algorithms over the ASOMO dataset in terms of accuracy and Table <ref type="table" target="#tab_1">10</ref> in terms of MAE. Both tables can be described as follows: for each classifier (row), the joint performance and the single evaluation measure (for each class variable) are shown. In order to simultaneously compare uni-dimensional with respect to multi-dimensional approaches, and supervised with respect to semi-supervised learning, the results are highlighted as follows:</p><p>1. In order to compare the supervised and the semi-supervised frameworks, for each type of classifier and accuracy measure (class variable and joint performance), we have highlighted the best single value and joint performance in bold (analysed per row). Note that, in the case of the accuracy term (Table <ref type="table" target="#tab_7">9</ref>), the highlighted values are the greatest values, while in the case of the MAE (Table <ref type="table" target="#tab_1">10</ref>) they are the lowest. Pairwise statistical hypothesis tests (Student's t-test with a ¼ 0:05) have been performed in order to determine if there are significant statistical differences between the values of the tested techniques. We use the symbol 'y' to mark the values that are statistically outperformed by the highlighted best estimated measure (in bold). 2. To compare the performance between the uni-dimensional and the multi-dimensional approaches, the best accuracy per class variable, as well as the joint performance, has been highlighted in italics. For each column, statistical hypothesis tests (Student's t-test with a ¼ 0:05) have been performed in order to determine if there are significant statistical differences. The symbol 'z' is used to mark the values that are statistically worse than the best estimated value (in italics).</p><p>Several conclusions can be extracted from the supervised and semi-supervised comparison in Tables <ref type="table" target="#tab_7">9</ref> and<ref type="table" target="#tab_1">10</ref> (analysed per row):</p><p>1. The uni-dimensional models tend to perform worse when they are learnt using the semi-supervised learning framework. This could be due to the fact that incorrect models tend to lead to performance degradation in the semi-supervised framework due to the fact that they are not able to match the underlying generative structure <ref type="bibr" target="#b11">[12]</ref>. This phenomenon occurs in both evaluation measures. 2. As occurs in the artificial experiments, the MDTAN approach <ref type="bibr" target="#b52">[53]</ref> tends to behave more similarly to the uni-dimensional approaches rather than to the multi-dimensional approaches. 3. With respect to the accuracy measure, in the multi-dimensional scenario, the Will to Influence class variable tends to degrade its performance in the semi-supervised scenario, whilst Sentiment Polarity and Subjectivity tend to achieve better single accuracies. In the uni-dimensional approach, the opposite happens. 4. The MAE results show that, unlike what happens in the unidimensional framework where the semi-supervised degradation is significant, in the multi-dimensional scenario similar results are reported for supervised and semi-supervised learning. However, there are cases in which the semi-supervised learning algorithms obtain better results and in one case there is a significant statistical gain. 5. The MDnB method in its semi-supervised framework is the best solution for the ASOMO problem. In terms of accuracy, it obtains statistically significant better results in joint accuracy and in the Sentiment Polarity target variable, as well as better results in the other two variables. With respect to the MAE, MDnB obtains statistically significant better results in joint accuracy and in the Subjectivity dimension, as well as better results in the other dimensions.</p><p>Regarding the comparison of the uni-dimensional and multidimensional approaches (for each column), the following comments can be extracted:</p><p>1. With the exception of Will to Influence, the single class variables tend to achieve better accuracies in the multidimensional approaches. 2. The MAE terms show similar results to those found with the accuracy evaluation measure. However, in this case, the exception is provided by the estimated MAE obtained in the Sentiment Polarity dimension in the semi-supervised version of the uni-dimensional TAN algorithm. 3. The multi-dimensional classification approach statistically outperforms the uni-dimensional framework in terms of joint accuracy. 4. MDnB is also the best technique in terms of global performance, i.e. in accuracy and in MAE metrics.</p><p>One explanation for the surprising success of the MDnB could be the use of the knowledge of the experts in engineering the features, as stated in <ref type="bibr" target="#b27">[28]</ref>: when applying rational criteria in determining the predictive features of a problem, the resulting features are usually probabilistically independent given the class variables. This characteristic favours the learning scheme provided by the MDnB algorithm. Moreover, this is crucial in the success obtained in the semi-supervised framework due to the fact that it matches the actual underlying domain structure <ref type="bibr" target="#b11">[12]</ref>.</p><p>The MDnB assumes conditional independence between the three class variables. In spite of this assumption, we cannot talk about independence between the class variables as happens when the problem is approached by several uni-dimensional classifiers. Each class variable in this model uses the information of the remaining class variables when it carries out the classification task. It also simultaneously uses all the class variables to learn the parameters of the structure. The success of this algorithm can shed some light on the relation between the three variables used in this problem: the multi-dimensional framework achieves better results than the uni-dimensional counterparts. Therefore, it seems that there is a certain relation between them. Furthermore, it can be seen that simultaneously using the information of these class variables in the same classification task by means of a multi-dimensional technique conceives better predictive classifiers.</p><p>In conclusion, we show that the proposed semi-supervised multi-dimensional formulation designs a novel perspective for this kind of SA problems, opening new ways to deal with this domain. In addition, it can also be seen that the explicit use of the different class variables in the same classification task has successfully solved the ASOMO problem, where the MDnB in a semi-supervised framework is the best solution.</p><p>In conclusion, we show that the proposed semi-supervised multi-dimensional formulation designs a novel perspective for this kind of SA problems, opening new ways to deal with this domain. In addition, it can also be seen that the explicit use of the different class variables in the same classification task has successfully solved the ASOMO problem, where the MDnB in a semi-supervised framework is the best solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions and future work</head><p>In this paper, we solve a real-world multi-dimensional SA problem. This real problem consists of characterising the attitude of a customer when he writes a post about a particular topic in a specific forum through three differently related dimensions: Will to Influence, Polarity and Subjectivity.</p><p>Due to the fact that it has three different target variables, the SA (uni-dimensional) state-of-the-art classification techniques seem inappropriate. They do not match the underlying multidimensional nature of this problem. The problem also cannot be directly tackled by multi-label techniques. For that reason, we propose the use of multi-dimensional Bayesian network classifiers as a novel methodological tool which joins the different target variables in the same classification task in order to exploit the potential relationships between them. Within this methodology, in this paper, we have proposed a new filter algorithm to learn multi-dimensional J/K dependences Bayesian network structures in order to explore a wider range of structures while dealing with this application.</p><p>Moreover, in order to avoid the arduous and time-consuming task of labelling examples in this field, we extend, by means of the EM algorithm, these multi-dimensional techniques to semisupervised learning framework so as to make use of the huge amount of unlabelled data available on the Internet.</p><p>Experimental results of applying the proposed battery of multi-dimensional learning algorithms to a corpus consisting of 2542 posts (150 manually labelled and 2392 unlabelled) show that: (1) the uni-dimensional approaches cannot capture the multi-dimensional underlying nature of this problem, (2) engineering a suitable feature set is a key factor for obtaining better solutions, (3) more accurate classifiers can be found using the multi-dimensional approaches which perform a simultaneous classification task, (4) the use of large amounts of unlabelled data in a semi-supervised framework can be beneficial to improve the recognition rates, and (5) the MDnB classifier in a semi-supervised framework is the best solution for this problem because it matches the actual underlying domain structure <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>The proposed multi-dimensional methodology can be improved or extended in several ways. For instance, in the ASOMO multidimensional problem, the values of all class variables are missing in each sample of the unlabelled subset. However, by means of the EM algorithm, the learning algorithms can be easily generalised to the situation where not all the class variables are missing in all the samples of the unlabelled data subset.</p><p>Besides, we are concerned about the scalability of the multidimensional Bayesian network classifiers in the semi-supervised framework. The computational burden is not a problem when dealing with these datasets, but it could happen when the number of variables increases. This could open a line in researching feature subset selection techniques for multi-dimensional classification.</p><p>Regarding the application of multi-dimensional classification to the SA domain, this work can be extended in a number of different ways:</p><p>The proposed multi-dimensional Bayesian network classifiers can be directly applied to Affect Analysis. This area is concerned with the analysis of text containing emotions and it is associated with SA <ref type="bibr" target="#b0">[1]</ref>. However, Affect Analysis tries to extract a large number of potential emotions, e.g. happiness, sadness, anger, hate, violence, excitement, fear, etc, instead of just looking at the polarity of the text. Additionally, in the case of Affect Analysis, the emotions are not mutually exclusive and certain emotions may be correlated. So, this can easily be viewed as a multi-label classification problem, a type of problem in which multi-dimensional Bayesian network classifiers have reported good results in the recent past <ref type="bibr" target="#b4">[5]</ref>.</p><p>Within SA, the same corpus can be used to deal with different target dimensions. This could open different research lines in adding more target variables in the same classification task so as to take advantage of these existing relationships, engineering a suitable feature set for working with several dimensions, etc. For instance, in the works where the need to predict both the Sentiment Polarity and the Subjectivity has been noticed <ref type="bibr" target="#b20">[21]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>dataset was collected by Socialware &amp; . This corpus was extracted from a single online discussion forum in which different customers of a specific company had left their comments, doubts and views about a single product. The forum consists of 2542 posts written in Spanish, with an average of 94774 words per post. One hundred and fifty of these documents have been manually labelled by an expert in Socialware &amp; according to the exposed three different dimensions and 2392 are left as unlabelled instances. As a result of the extensive work carried out by Socialware &amp; on manually dealing with the ASOMO problem in the recent past, high levels of experience and understanding of determining the major factors that characterise the attitude of the customers have been gained. These factors are the following: (1) the implication of the author with the other customers in the forum, (2) the position of authority of the customer, and (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The vision of Socialware &amp; of the problem of determining the attitude of the writers in a forum.</figDesc><graphic coords="3,134.60,613.91,316.54,103.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of the labels of the three class variables over the labelled subset. The marginal distributions of Will to Influence, Sentiment Polarity and Subjectivity are represented as bar diagrams (left) and the joint distribution is represented in a table (right).</figDesc><graphic coords="4,86.59,58.64,432.00,432.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Well known multi-dimensional classification subproblems, such as multilabel and multi-task learning, and structure prediction, contained in the set of multi-dimensional problems.</figDesc><graphic coords="7,62.12,58.61,192.29,191.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>composed of the arcs between the class variables and the feature variables, so we can define the feature selection subgraph of S as S CF ¼ ðV,A CF Þ. This subgraph represents the selection of features that seems relevant for classification given the class variables.A C DV C Â V C iscomposed of the arcs between the class variables, so we can define the class subgraph of S induced by V C as S C ¼ ðV C ,A C Þ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4</head><label>4</label><figDesc>Fig.4shows a multi-dimensional class Bayesian network classifier with 3 class variables and 5 features, and its partition into the three subgraphs.Depending on the structure of the three subgraphs, the following sub-families 2 of multi-dimensional class network classifiers are proposed in the state-of-the-art literature:Multi-dimensional naive Bayes classifier (MDnB): The class subgraph and the feature subgraph are empty and the feature selection subgraph is complete.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A multi-dimensional Bayesian classifier and its division. (a) Complete graph, (b) feature selection subgraph, (c) class subgraph and (d) feature subgraph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. An example of a multi-dimensional naive Bayes structure.</figDesc><graphic coords="9,35.66,482.81,245.58,100.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. An example of a multi-dimensional 2/3-dependence Bayesian network structure.</figDesc><graphic coords="9,305.12,552.05,244.55,170.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 :</head><label>2</label><figDesc>E-STEP Use the current model c ðKÞ to estimate the probability of each configuration of class variables for each unlabelled instance. 3: M-STEP Learn a new model c ðK þ 1Þ with structure and parameters, given the estimated probabilities in the E-STEP. 4: end while Output classifier c, that takes an unlabelled instance and predicts the class variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Applying 3-fold cross validation to a dataset with labelled and unlabelled instances.</figDesc><graphic coords="14,86.59,435.30,432.00,294.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Subset of features related to the implication of the author with other customers.</figDesc><table><row><cell>Feature</cell><cell>Description</cell><cell>Example</cell><cell>Translation</cell></row><row><cell>First persons</cell><cell>Number of verbs in the fist person</cell><cell>Contrate ´y</cell><cell>I hired y</cell></row><row><cell>Second persons</cell><cell>Number of verbs in the second person</cell><cell>Tienes y</cell><cell>You have y</cell></row><row><cell>Third persons</cell><cell>Number of verbs in the third person</cell><cell>Sabe y</cell><cell>He knows y</cell></row><row><cell>Relational forms</cell><cell>Number of phatic expressions, i.e. expressions</cell><cell>(1) Hola</cell><cell>(1) Hello</cell></row><row><cell></cell><cell>whose only function is to perform a social task</cell><cell>(2) Gracias de antemano</cell><cell>(2) Thanks in advance</cell></row><row><cell>Agreement expressions</cell><cell>Number of expressions that show</cell><cell>(1) Estoy de acuerdo contigo</cell><cell>(1) I agree with you</cell></row><row><cell></cell><cell>agreement or disagreement</cell><cell>(2) No tienes razo ´n</cell><cell>(2) You're wrong</cell></row><row><cell>Request</cell><cell>Number of sentences that express</cell><cell>(1) Me gustarı ´a saber y</cell><cell>(1) I'd like to know y</cell></row><row><cell></cell><cell>a certain degree of request</cell><cell>(2) Alguien podrı ´a y</cell><cell>(2) I would appreciate it if someone could y</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Subset of features related to the position of authority of the customer.</figDesc><table><row><cell>Feature</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>An example of a multi-dimensional problem and the most straightforward way to transfer it to the multi-label domain.</figDesc><table><row><cell>Eyes</cell><cell>Hair</cell><cell>List of labels</cell></row></table><note><p><p>inst. X 1 X 2 X 3 X 4 X 5 Sex B B D A C Female Blue</p>Ginger {Female, Blue, Ginger} the research community. Therefore, it is not possible to fit most of the multi-dimensional problems (those that have at least one class variable with more than two values) into the current multilabel framework.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Estimated accuracy values on the ASOMO dataset using three different types of feature sets in both uni and multi-dimensional scenarios (20 Â 5cv).</figDesc><table><row><cell>Approach</cell><cell>Classifier</cell><cell>Feature set</cell><cell>#feat.</cell><cell>Will to influence Acc</cell><cell>Sentiment P. Acc</cell><cell>Subjectivity Acc</cell><cell>Joint Acc</cell><cell>Time (ms)</cell></row><row><cell>Uni-dimen.</cell><cell>Multiple classif.</cell><cell>Unigrams</cell><cell>357</cell><cell>y51:83 7 2:79</cell><cell>33.007 1.82</cell><cell>y78:90 7 2:16</cell><cell>y11:70 7 1:68</cell><cell>1011</cell></row><row><cell></cell><cell></cell><cell>Uni.þ bigrams</cell><cell>563</cell><cell>y47:17 7 2:47</cell><cell>32.90 71.96</cell><cell>y79:47 7 2:75</cell><cell>y9:807 1:47</cell><cell>1470</cell></row><row><cell></cell><cell></cell><cell>PoS</cell><cell>766</cell><cell>y47:57 7 2:52</cell><cell>y30:80 7 2:75</cell><cell>y66:33 7 2:96</cell><cell>y9:43 7 1:65</cell><cell>1755</cell></row><row><cell></cell><cell></cell><cell>ASOMO feat.</cell><cell>14</cell><cell>55:07 7 1:32</cell><cell>y26:07 7 2:09</cell><cell>y82:17 7 1:35</cell><cell>y10:37 7 2:30</cell><cell>110</cell></row><row><cell></cell><cell>Cartesian class</cell><cell>Unigrams</cell><cell>357</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>y9:907 2:35</cell><cell>32</cell></row><row><cell></cell><cell></cell><cell>Uni.þ bigrams</cell><cell>563</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>y8:87 7 2:05</cell><cell>74</cell></row><row><cell></cell><cell></cell><cell>PoS</cell><cell>766</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>y10:33 7 2:11</cell><cell>106</cell></row><row><cell></cell><cell></cell><cell>ASOMO feat.</cell><cell>14</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>z13:70 7 2:60</cell><cell>17</cell></row><row><cell>Multi-dimen.</cell><cell>MDnB</cell><cell>Unigrams</cell><cell>357</cell><cell>y41:63 7 3:74</cell><cell>y31:00 7 2:29</cell><cell>y75:30 7 2:20</cell><cell>y10:23 7 1:92</cell><cell>47</cell></row><row><cell></cell><cell></cell><cell>Uni.þ bigrams</cell><cell>563</cell><cell>y38:03 7 3:33</cell><cell>33:40 7 2:37</cell><cell>y74:27 7 2:71</cell><cell>y9:63 7 2:05</cell><cell>107</cell></row><row><cell></cell><cell></cell><cell>PoS</cell><cell>766</cell><cell>y39:50 7 3:32</cell><cell>y30:53 7 2:22</cell><cell>y76:40 7 2:47</cell><cell>y9:86 7 1:93</cell><cell>122</cell></row><row><cell></cell><cell></cell><cell>ASOMO feat.</cell><cell>14</cell><cell>y53:23 7 1:62</cell><cell>y30:87 7 2:52</cell><cell>83:53 7 0:69</cell><cell>14:97 7 1:94</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>Estimated mean absolute error rates on the ASOMO dataset using three different types of feature sets in both uni and multi-dimensional scenarios (20 Â 5cv). Although the reported times are not a problem in the current supervised learning settings, in the semi-supervised framework, where the computation time increases dramatically with the number of instances, the learning process could be intractable. 7.2.2. Experiments with the ASOMO corpus in the supervised and semi-supervised learning frameworks</figDesc><table><row><cell>Approach</cell><cell>Classifier</cell><cell>Feature set</cell><cell>#feat.</cell><cell>Will to influence MAE</cell><cell>Sentiment P. MAE</cell><cell>Subjectivity MAE</cell><cell>JMAE</cell></row><row><cell>Uni-dimen.</cell><cell>Multiple classif.</cell><cell>Unigrams</cell><cell>357</cell><cell>y0:632 7 0:040</cell><cell>y1:048 7 0:042</cell><cell>y0:211 7 0:017</cell><cell>y0:684 7 0:025</cell></row><row><cell></cell><cell></cell><cell>Uni.þ bigrams</cell><cell>563</cell><cell>y0:700 7 0:043</cell><cell>0:956 7 0:043</cell><cell>y0:196 7 0:014</cell><cell>y0:669 7 0:023</cell></row><row><cell></cell><cell></cell><cell>PoS</cell><cell>766</cell><cell>y0:878 7 0:063</cell><cell>y1:147 7 0:052</cell><cell>y0:339 7 0:043</cell><cell>y0:918 7 0:054</cell></row><row><cell></cell><cell></cell><cell>ASOMO feat.</cell><cell>14</cell><cell>0.563 70.014</cell><cell>y1:036 7 0:036</cell><cell>y0:173 7 0:011</cell><cell>y0:620 7 0:014</cell></row><row><cell></cell><cell>Cartesian class</cell><cell>Unigrams</cell><cell>357</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>y0:650 7 0:026</cell></row><row><cell></cell><cell></cell><cell>Uni.þ bigrams</cell><cell>563</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>y0:680 7 0:030</cell></row><row><cell></cell><cell></cell><cell>PoS</cell><cell>766</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>y0:757 7 0:040</cell></row><row><cell></cell><cell></cell><cell>ASOMO feat.</cell><cell>14</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>y0:640 7 0:060</cell></row><row><cell>Multi-dimen.</cell><cell>MDnB</cell><cell>Unigrams</cell><cell>357</cell><cell>y0:788 7 0:049</cell><cell>y1:104 7 0:039</cell><cell>y0:247 7 0:026</cell><cell>y0:789 7 0:031</cell></row><row><cell></cell><cell></cell><cell>Uni.þ bigrams</cell><cell>563</cell><cell>y0:852 7 0:052</cell><cell>y1:107 7 0:042</cell><cell>y0:263 7 0:040</cell><cell>y0:824 7 0:049</cell></row><row><cell></cell><cell></cell><cell>PoS</cell><cell>766</cell><cell>y0:728 7 0:044</cell><cell>y1:037 7 0:040</cell><cell>y0:240 7 0:020</cell><cell>y0:742 7 0:029</cell></row><row><cell></cell><cell></cell><cell>ASOMO feat.</cell><cell>14</cell><cell>0.5597 0.029</cell><cell>y1:019 7 0:027</cell><cell>0.1677 0.009</cell><cell>0.6087 0.016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9</head><label>9</label><figDesc>Accuracies on the ASOMO dataset with the ASOMO features in the supervised and the semi-supervised learning frameworks (20 Â 5cv).</figDesc><table><row><cell>Classif.</cell><cell cols="2">Labelled data (supervised learning)</cell><cell></cell><cell></cell><cell>Labelled þunlabelled data (semi-supervised learning)</cell></row><row><cell></cell><cell>W. influence Acc</cell><cell>Sentiment P. Acc</cell><cell>Subjectivity Acc</cell><cell>Joint Acc</cell><cell>W. influence Acc</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J. Ortigosa-Herna ´ndez et al. / Neurocomputing 92 (2012) 98-115</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In<ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43]</ref>, instead of multi-dimensional, the term fully is used in order to name the classifiers.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been partially supported by the Saiotek, Etortek and Research Groups 2007-2012 (IT-242-07) programs (Basque Government), TIN2008-06815-C02-01, TIN2010-14931, Consolider Ingenio 2010-CSD2007-00018 projects and MEC-FPU Grant AP2008-00766 (Spanish Ministry of Science and Innovation), and COMBIOMED network in computational bio-medicine (Carlos III Health Institute).</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentiment P. Acc Subjectivity Acc Joint Acc nB yz55:07 7 1:32 z26:07 7 2:09 z82:17 7 1:35 z10:37 7 2:30 56:70 7 2:85 yz21:97 7 3:11 yz58:07 7 3:78 yz7:43 7 2:27 TAN z52:93 7 3:23 yz26:30 7 2:16 z80:10 7 1:46 z9:47 7 1:29 yz48:77 7 3:44 z29:00 7 3:69 yz75:10 7 3:42 z9:87 7 1:79 2DB 57:13 7 1:85 z27:43 7 2:82 z82:30 7 1:57 z13:17 7 1:59 yz54:13 7 3:37 yz24:807 2:95 yz61:30 7 4:83 yz8:607 2:08 MDnB z53:23 7 1:62 y30.877 2.52 83.537 0.69 y14:97 7 1:94 z54:00 7 2:08 32:27 7 1:41 83:63 7 0:55 16:83 7 1:14 MDTAN z52:60 7 3:48 yz27:13 7 2:43 z82:57 7 1:72 z12:80 7 2:38 yz31:10 7 4:37 z29:43 7 2:48 z82:60 7 1:37 yz8:97 7 1:70 MD 1/1 56:67 7 2:93 29:97 7 3:75 78:17 7 2:59 15:63 7 2:00 yz53:27 7 2:78 z29:17 7 2:54 z77:90 7 2:20 z15:33 7 1:37 MD 2/2 56:60 7 3:63 z28:93 7 2:36 z77:47 7 2:24 15.17 71.99 yz53:30 7 2:08 z28:77 7 2:37 z76:93 7 2:97 z15:50 7 0:94 MD 2/3 56:70 7 2:87 29.87 7 3.20 z77:03 7 1:41 15.9072.67 yz52:77 7 1:53 z30:77 7 2:25 z77:90 7 2:20 16.63 71.32 MD 2/4 56.97 72.11 z28:53 7 3:13 z76:87 7 2:39 15.577 2.41 yz52:47 7 2:05 29.307 3.02 z75:27 7 3:24 z15:43 7 1:28 0:173 7 0:011 z0:620 7 0:014 yz0:613 7 0:040 yz1:209 7 0:069 yz0:421 7 0:036 yz0:986 7 0:041 TAN z0:614 7 0:041 yz1:044 7 0:043 z0:204 7 0:015 z0:670 7 0:029 yz0:664 7 0:055 0:991 7 0:041 yz0:225 7 0:038 yz0:718 7 0:039 2DB z0:553 7 0:028 1:035 7 0:053 0:171 7 0:009 0:614 7 0:018 yz0:636 7 0:041 yz1:114 7 0:082 yz0:387 7 0:048 yz0:885 7 0:056 MDnB z0:559 7 0:029 1.0197 0.027 y0.1677 0.009 y0.60870.016 0:549 7 0:019 1:002 7 0:028 0:612 7 0:005 0:596 7 0:007 MDTAN z0:567 7 0:045 z1:101 7 0:052 z0:180 7 0:015 z0:644 7 0:026 yz0:878 7 0:075 z1:102 7 0:028 z0:172 7 0:011 yz0:750 7 0:027 MD 1/1 0:529 7 0:034 z1:056 7 0:032 z0:219 7 0:022 z0:659 7 0:029 y0:556 7 0:031 z1:061 7 0:054 z0:216 7 0:018 z0:666 7 0:015 MD 2/2 0:525 7 0:033 z1:057 7 0:054 z0:222 7 0:021 z0:660 7 0:034 y0:560 7 0:040 z1:075 7 0:050 z0:227 7 0:023 yz0:682 7 0:022 MD 2/3 0:531 7 0:032 z1:061 7 0:032 z0:237 7 0:034 z0:679 7 0:037 y0.5497 0.016 z1:048 7 0:049 z0:223 7 0:033 z0:678 7 0:033 MD 2/4 0:529 7 0:041 z1:069 7 0:058 z0:227 7 0:020 z0:670 7 0:031 yz0:579 7 0:038 z1:047 7 0:038 z0:225 7 0:023 z0:680 7 0:021</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Affect analysis of web forums and blogs using correlation ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thoms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge Data Eng</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1168" to="1180" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stylistic text classification using functional lexical features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Whitelaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><surname>Levitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="802" to="822" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freeling 1.3: Syntactic and semantic services in an open-source NLP library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atserias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Comelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Padro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Padro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC&apos;06)</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation (LREC&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Baklr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<title level="m">Predicting Structured Data</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Larran ˜aga, Multi-dimensional classification with Bayesian networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bielza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approx. Reason</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="705" to="727" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning Bayesian Networks from Data with Factorisation and Classification Purposes. Applications in Biomedicine</title>
		<author>
			<persName><forename type="first">R</forename><surname>Blanco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of the Basque Country</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An open-source suite of language analyzers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Padro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Padro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="239" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semi-Supervised</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximating discrete probability distributions with dependence trees</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Member</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semisupervised Learning of Classifiers with Application to Human-Computer Interaction</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The effect of unlabeled data on generative classifiers, with application to model selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE 93 Conference on Geometric Methods in Computer Vision</title>
		<meeting>the SPIE 93 Conference on Geometric Methods in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semisupervised learning of classifiers: theory, algorithms and their application to human-computer interaction</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Cirelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1553" to="1567" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Bayesian method for the induction of probabilistic networks from data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herskovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="309" to="347" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning as search optimization: approximate large margin methods for structured prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inference and learning multi-dimensional Bayesian network classifiers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>De Waal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Van Der Gaag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lect. Notes Artif. Intell.</title>
		<imprint>
			<biblScope unit="volume">4724</biblScope>
			<biblScope unit="page" from="501" to="511" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical classification of web content</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A machine learning approach to sentiment analysis in multilingual web text</title>
		<author>
			<persName><forename type="first">E</forename><surname>Boiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="526" to="558" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sentiwordnet: a publicly available lexical resource for opinion mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Language Resources and Evaluation (LREC06)</title>
		<meeting>the Fifth Conference on Language Resources and Evaluation (LREC06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An extensive empirical study of feature selection metrics for text classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1289" to="1305" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Bayesian structural EM algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 14th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian network classifiers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="131" to="163" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Irrelevant features and the subset selection problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Eleventh International Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation from incomplete data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="174" to="194" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effects of adjective orientation and gradability on sentence subjectivity</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Computational linguistics</title>
		<meeting>the 18th Conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Wrappers for Performance Enhancement and Oblivious Decision Graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the shortest spanning subtree of a graph and the traveling salesman problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Am. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="50" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<title level="m">Information Theory and Statistics</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An analysis of Bayesian classifiers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Iba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th National Conference on Artificial Intelligence</title>
		<meeting>the 10th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pen ˜a, I. Inza, Special issue on probabilistic graphical models for classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Larran ˜aga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Handbook of Natural Language Processing</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Indurkhya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</editor>
		<imprint>
			<publisher>Chapman &amp; Hall</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Sentiment analysis and subjectivity. second ed.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The EM Algorithm and Extensions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M N</forename><surname>Arifin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL Conference</title>
		<meeting>the COLING/ACL Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using EM</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="134" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Thumbs up? Sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automated heart wall motion abnormality detection from ultrasound images using Bayesian networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Steck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poldermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;07: Proceedings of the 20th International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="519" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning extraction patterns for subjective expressions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;03)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning subjective nouns using extraction pattern bootstrapping</title>
		<author>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning</title>
		<meeting>the Seventh Conference on Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-objective learning of multi-dimensional Bayesian classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Rodrı ´guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Conference on Hybrid Intelligent Systems (HIS&apos;08)</title>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
			<biblScope unit="page" from="501" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning Bayesian Network Classifiers for Multidimensional Supervised Classification Problems by Means of a Multi-objective Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Rodrı ´guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
		<idno>EHU-KZAA-TR-3-2010</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>San Sebastia ´n, Spain</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Artificial Intelligence, University of the Basque Country</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sensitivity analysis of k-fold crossvalidation in prediction error estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="574" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Analyzing collaborative learning processes automatically: exploiting the advances of computational linguistics in computer-supported collaborative learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer-Supported Collab. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning limited dependence Bayesian classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining (KDD-96</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="335" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiple aspect ranking using the good grief algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Human Language Technology/North American Chapter of the ACL Conference (HLT-NAACL)</title>
		<meeting>the Joint Human Language Technology/North American Chapter of the ACL Conference (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="300" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-validatory choice and assessment of statistical predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="147" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Using two-class classifiers for multiclass classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 16th International Conference on Pattern Recognition</title>
		<meeting>16th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="124" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi label classification: an overview</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Data Warehousing Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ICWSM a great catchy name: semisupervised recognition of sarcastic sentences in product reviews</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Davidiv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Weblogs and Social Media-ICWSM10</title>
		<meeting>the International AAAI Conference on Weblogs and Social Media-ICWSM10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-dimensional Bayesian network classifiers</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Van Der Gaag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>De Waal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third European Workshop in Probabilistic Graphical Models</title>
		<meeting>the Third European Workshop in Probabilistic Graphical Models</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning subjective language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Ling</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="308" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Morgan Kaufmann Series in Data Management Systems. second ed.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards answering opinion questions: separating facts from opinions and identifying the polarity of opinion sentences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Semi-supervised Learning Literature Survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno>1530</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Madison</publisher>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Sciences, University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Introduction to Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">he received a FPU fellowship from the Spanish Ministry of Education. Since then, he has been a PhD student in The University of the Basque Country and a member of the Intelligent Systems Group research team. His research interests are multi-dimensional classification, semisupervised learning, and sentiment and affect analysis</title>
		<ptr target="http://www.sc.ehu.es/ccwbayes/members/jonathan" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Jonathan Ortigosa-Herna ´ndez received an MSc degree in Computer Science from The University of the Basque Country, Donostia-San Sebastia ´n, Spain in 2008, and a BSc in Informatics from Coventry University, United Kingdom, in the same year</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
