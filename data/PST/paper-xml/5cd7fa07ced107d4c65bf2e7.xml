<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
							<email>haom@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
							<email>wang.chi@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<email>yuxdong@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
							<email>lijian83@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
							<email>kuansanw@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University Yuxiao Dong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond Kuansan Wang</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Redmond Jie Tang †</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Center for Information Science and Technol-ogy (BNRist)</orgName>
								<orgName type="institution">Microsoft Research. † Also with Beijing National Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<address>
									<postCode>2019</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3308558.3313446</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of large-scale network embedding, which aims to learn latent representations for network mining applications. Previous research shows that 1) popular network embedding benchmarks, such as DeepWalk, are in essence implicitly factorizing a matrix with a closed form, and 2) the explicit factorization of such matrix generates more powerful embeddings than existing methods. However, directly constructing and factorizing this matrix-which is dense-is prohibitively expensive in terms of both time and space, making it not scalable for large networks.</p><p>In this work, we present the algorithm of large-scale network embedding as sparse matrix factorization (NetSMF). NetSMF leverages theories from spectral sparsification to efficiently sparsify the aforementioned dense matrix, enabling significantly improved efficiency in embedding learning. The sparsified matrix is spectrally close to the original dense one with a theoretically bounded approximation error, which helps maintain the representation power of the learned embeddings. We conduct experiments on networks of various scales and types. Results show that among both popular benchmarks and factorization based methods, NetSMF is the only method that achieves both high efficiency and effectiveness. We show that NetSMF requires only 24 hours to generate effective embeddings for a large-scale academic collaboration network with tens of millions of nodes, while it would cost DeepWalk months and is computationally infeasible for the dense matrix factorization solution. The source code of NetSMF is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have witnessed the emergence of network embedding, which offers a revolutionary paradigm for modeling graphs and networks <ref type="bibr" target="#b15">[16]</ref>. The goal of network embedding is to automatically learn latent representations for objects in networks, such as vertices and edges. Significant lines of research have shown that the latent representations are capable of capturing the structural properties of networks, facilitating various downstream network applications, such as vertex classification and link prediction <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Over the course of its development, the DeepWalk <ref type="bibr" target="#b26">[27]</ref>, LINE <ref type="bibr" target="#b32">[33]</ref>, and node2vec <ref type="bibr" target="#b13">[14]</ref> models have been commonly considered as powerful benchmark solutions for evaluating network embedding research. The advantage of LINE lies in its scalability for large-scale networks as it only models the first-and second-order proximities. That is to say, its embeddings lose the multi-hop dependencies in networks. DeepWalk and node2vec, on the other hand, leverage random walks on graphs and skip-gram <ref type="bibr" target="#b23">[24]</ref> with large context sizes to model nodes further away (i.e., global structures). Consequently, it is computationally more expensive for DeepWalk and node2vec to handle large-scale networks. For example, with the default parameter settings <ref type="bibr" target="#b26">[27]</ref>, DeepWalk requires months to embed an academic collaboration network of 67 million vertices and 895 million edges 2 . The node2vec model, which performs high-order random walks, takes more time than DeepWalk to learn embeddings.</p><p>More recently, a study shows that both the DeepWalk and LINE methods can be viewed as implicit factorization of a closed-form matrix <ref type="bibr" target="#b27">[28]</ref>. Building upon this theoretical foundation, the NetMF method was instead proposed to explicitly factorize this matrix, 2 With the default DeepWalk parameters (walk length: 40 and #walk per node: 80), 214+ billion nodes (67M×40×80) with a vocabulary size of 67 million are fed into skip-gram. As a reference, Mikolov et al. reported that training on Google News of 6 billion words and a vocabulary size of only 1 million cost 2.5 days with 125 CPU cores <ref type="bibr" target="#b23">[24]</ref>.  Unfortunately, it turns out that the matrix to be factorized is an n × n dense one with n being the number of vertices in the network, making it prohibitively expensive to directly construct and factorize for large-scale networks.</p><p>In light of these limitations of existing methods (See the summary in Table <ref type="table" target="#tab_0">1</ref>), we propose to study representation learning for large-scale networks with the goal of achieving efficiency, capturing global structural contexts, and having theoretical guarantees. Our idea is to find a sparse matrix that is spectrally close to the dense NetMF matrix implicitly factorized by DeepWalk. The sparsified matrix requires a lower cost for both construction and factorization. Meanwhile, making it spectrally close to the original NetMF matrix can guarantee that the spectral information of the network is maintained, and the embeddings learned from the sparse matrix is as powerful as those learned from the dense NetMF matrix.</p><p>In this work, we present the solution to network embedding learning as sparse matrix factorization (NetSMF). NetSMF comprises three steps. First, it leverages the spectral graph sparsification technique <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> to find a sparsifier for a network's random-walk matrix-polynomial. Second, it uses this sparsifier to construct a matrix with significantly fewer non-zeros than, but spectrally close to, the original NetMF matrix. Finally, it performs randomized singular value decomposition to efficiently factorize the sparsified NetSMF matrix, yielding the embeddings for the network.</p><p>With this design, NetSMF offers both efficiency and effectiveness with guarantees, as the approximation error of the sparsified matrix is theoretically bounded. We conduct experiments in five networks, which are representative of different scales and types. Experimental results show that for million-scale or larger networks, NetSMF achieves orders of magnitude speedup over NetMF, while maintaining competitive performance for the vertex classification task. In other words, both NetSMF and NetMF outperform wellrecognized network embedding benchmarks (i.e., DeepWalk, LINE, and node2vec), but NetSMF addresses the computation challenge faced by NetMF.</p><p>To summarize, we introduce the idea of network embedding as sparse matrix factorization and present the NetSMF algorithm, which makes the following contributions to network embedding:</p><p>Efficiency. NetSMF reaches significantly lower time and space complexity than NetMF. Remarkably, NetSMF is able to generate embeddings for a large-scale academic network of 67 million vertices and 895 million edges on a single server in 24 hours, while it would cost months for DeepWalk and node2vec, and is computationally infeasible for NetMF on the same hardware. </p><formula xml:id="formula_0">)) L L's sparsifier M 1 T T r =1 (D −1 A) r D −1 M M 's sparsifier trunc_log • vol(G) b M NetMF matrix trunc_log • vol(G) b M NetMF matrix sparisifier M number of non-zeros in L ϵ approximation factor [x ] set {1, 2, • • • , x } for positive integer x<label>4</label></formula><p>Effectiveness. NetSMF is capable of learning embeddings that maintain the same representation power as the dense matrix factorization solution, making it consistently outperform DeepWalk and node2vec by up to 34% and LINE by up to 100% for the multi-label vertex classification task in networks.</p><p>Theoretical Guarantee. NetSMF's efficiency and effectiveness are theoretically backed up. The sparse NetSMF matrix is spectrally close to the exact NetMF matrix, and the approximation error can be bounded, maintaining the representation power of its sparsely learned embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Commonly, the problem of network embedding is formalized as follows: Given an undirected and weighted network G = (V , E, A) with V as the vertex set of n vertices, E as the edge set of m edges, and A as the adjacency matrix, the goal is to learn a function V → R d that maps each vertex to a d-dimensional (d ≪ n) vector that captures its structural properties, e.g., community structures. The vector representation of each vertex can be fed into downstream applications such as link prediction and vertex classification.</p><p>One of the pioneering work on network embedding is the Deep-Walk model <ref type="bibr" target="#b26">[27]</ref>, which has been consistently considered as a powerful benchmark over the past years <ref type="bibr" target="#b15">[16]</ref>. In brief, DeepWalk is coupled with two steps. First, it generates several vertex sequences by random walks over a network; Second, it applies the skip-gram model <ref type="bibr" target="#b24">[25]</ref> on the generated vertex sequences to learn the latent representations for each vertex. Commonly, skip-gram is parameterized with the context window size T and the number of negative samples b. Recently, a theoretical study <ref type="bibr" target="#b27">[28]</ref> reveals that DeepWalk essentially factorizes a matrix derived from the random walk process. More formally, it proves that when the length of random walks goes to infinity, DeepWalk implicitly and asymptotically factorizes the following matrix:</p><formula xml:id="formula_1">log • vol(G) b M ,<label>(1)</label></formula><p>where vol (G) = i j A i j denotes the volume of the graph, and</p><formula xml:id="formula_2">M = 1 T T r =1 (D −1 A) r D −1 ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">D = diag (d 1 , • • • , d n )</formula><p>is the degree matrix with d i = j A i j as the generalized degree of the i-th vertex. Note that log • (•) represents the element-wise matrix logarithm <ref type="bibr" target="#b17">[18]</ref>, which is different from the matrix logarithm. In other words, the matrix in Eq. ( <ref type="formula" target="#formula_1">1</ref>) can be characterized as the result of applying element-wise matrix logarithm (i.e., log</p><formula xml:id="formula_4">• ) to matrix vol (G)</formula><p>b M. The matrix in Eq. ( <ref type="formula" target="#formula_1">1</ref>) offers an alternative view of the skip-gram based network embedding methods. Further, Qiu et al. provide an explicit matrix factorization approach named NetMF to learn the embeddings <ref type="bibr" target="#b27">[28]</ref>. It shows that the accuracy for vertex classification based on the embeddings from NetMF outperforms that based on DeepWalk and LINE. Note that the matrix in Eq. ( <ref type="formula" target="#formula_1">1</ref>) would be ill-defined if there exist a pair of vertices unreachable in T hops, because log(0) = −∞. So following Levy and Goldberg <ref type="bibr" target="#b21">[22]</ref>, NetMF uses the logarithm truncated at point one, that is, trunc_log(x) = max(0, log(x)). Thus, NetMF targets to factorize the matrix</p><formula xml:id="formula_5">trunc_log • vol(G) b M .<label>(3)</label></formula><p>In the rest of this work, we refer to the matrix in Eq. (3) as the NetMF matrix.</p><p>However, there exist a couple of challenges when leveraging the NetMF matrix in practice. First, almost every pair of vertices within distance r ≤ T correspond to a non-zero entry in the NetMF matrix. Recall that many social and information networks exhibit the smallworld property where most vertices can be reached from each other in a small number of steps. For example, as of the year 2012, 92% of the reachable pairs in Facebook are at distance five or less <ref type="bibr" target="#b1">[2]</ref>. As a consequence, even if setting a moderate context window size (e.g., the default setting T = 10 in DeepWalk), the NetMF matrix in Eq. ( <ref type="formula" target="#formula_5">3</ref>) would be a dense matrix with O(n 2 ) number of non-zeros. The exact construction and factorization of such a matrix is impractical for large-scale networks. More concretely, computing the matrix power in Eq. (2) involves dense matrix multiplication which costs O(n 3 ) time; factorizing a n × n dense matrix is also time consuming. To reduce the construction cost, NetMF approximates M with its top eigen pairs. However, the approximated matrix is still dense, making this strategy unable to handle large networks.</p><p>In this work, we aim to address the efficiency and scalability limitation of NetMF, while maintaining its superiority in effectiveness. We list necessary notations and their descriptions in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NETWORK EMBEDDING AS SPARSE MATRIX FACTORIZATION (NetSMF)</head><p>In this section, we develop network embedding as sparse matrix factorization (NetSMF). We present the NetSMF method to construct and factorize a sparse matrix that approximates the dense NetMF matrix. The main technique we leverage is random-walk matrixpolynomial (molynomial) sparsification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Random-Walk Molynomial Sparsification</head><p>We first introduce the definition of spectral similarity and the theorem of random-walk molynomial sparsification. We define G and G are (1 + ϵ)-spectrally similar if</p><formula xml:id="formula_6">∀x ∈ R n , (1 − ϵ ) • x ⊤ Lx ≤ x ⊤ Lx ≤ (1 + ϵ ) • x ⊤ Lx .</formula><p>Theorem 1. (Spectral Sparsifiers of Random-Walk Molynomials <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>) For random-walk molynomial</p><formula xml:id="formula_7">L = D − T r =1 α r D D −1 A r ,<label>(4)</label></formula><p>where T r =1 α r = 1 and α r non-negative, one can construct, in time O(T 2 mϵ −2 log 2 n), a (1 + ϵ)-spectral sparsifier, L, with O(n log nϵ −2 ) non-zeros. For unweighted graphs, the time complexity can be reduced to O(T 2 mϵ −2 log n).</p><p>To achieve a sparsifier L with O(ϵ −2 n log n) non-zeros, the sparsification algorithm consists of two steps: The first step obtains an initial sparsifier for L with O(Tmϵ −2 log n) non-zeros. The second step then applies the standard spectral sparsification algorithm <ref type="bibr" target="#b29">[30]</ref> to further reduce the number of non-zeros to O(ϵ −2 n log n). In this work, we only adopt the first step because a sparsifier with O(Tmϵ −2 log n) non-zeros is sparse enough for our task. Thus we skip the second step that involves additional computations. From now on, when referring to the random-walk molynomial sparsification algorithm in this work, we mean its first step only.</p><p>One can immediately observe that, if we set α r = 1 T , r ∈ [T ], the matrix L in Eq. ( <ref type="formula" target="#formula_0">4</ref>) has a strong connection with the desired matrix M in Eq. <ref type="bibr" target="#b1">(2)</ref>. Formally, we have the following equation</p><formula xml:id="formula_8">M = D −1 (D − L) D −1 .<label>(5)</label></formula><p>Thm. 1 can help us construct a sparsifier L for matrix L. Then we define M ≜ D −1 (D − L)D −1 by replacing L in Eq. ( <ref type="formula" target="#formula_8">5</ref>) with its sparsifier L. One can observe that matrix M is still a sparse one with the same order of magnitude of non-zeros as L. Consequently, instead of factorizing the dense NetMF matrix in Eq. ( <ref type="formula" target="#formula_5">3</ref>), we can factorize its sparse alternative, i.e.,</p><formula xml:id="formula_9">trunc_log • vol(G) b M .<label>(6)</label></formula><p>In the rest of this work, the matrix in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is referred to as the NetMF matrix sparsifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The NetSMF Algorithm</head><p>In this section, we formally describe the NetSMF algorithm, which consists of three steps: random-walk molynomial sparsification, NetMF sparsifier construction, and truncated singular value decomposition.</p><p>Step 1: Random-Walk Molynomial Sparsification. To achieve the sparsifier L, we adopt the algorithm in Cheng et al. <ref type="bibr" target="#b7">[8]</ref>. The algorithm starts from creating a network G that has the same vertex set as G and an empty edge set (Alg. 1, Line 1). Next, the algorithm constructs a sparsifier with O(M) non-zeros by repeating the Path-Sampling algorithm for M times. In each iteration, it picks an edge </p><formula xml:id="formula_10">G 9 Compute M = D −1 D − L D −1 10 U d , Σ d , V d ← RandomizedSVD(trunc_log • vol(G ) b M , d ) 11 return U d √ Σ d as network embeddings</formula><p>Algorithm 2: PathSampling algorithm as described in <ref type="bibr" target="#b7">[8]</ref>.</p><formula xml:id="formula_11">1 Procedure PathSampling(e = (u, v), r ) 2 Uniformly pick an integer k ∈ [r ] 3 Perform (k − 1)-step random walk from u to u 0 4 Perform (r − k )-step random walk from v to u r<label>5</label></formula><p>Keep track of Z (p) along the length-r path p between u 0 and u r according to Eq. ( <ref type="formula" target="#formula_13">7</ref>)</p><formula xml:id="formula_12">6 return u 0 , u r , Z<label>(p)</label></formula><p>e ∈ E and an integer r ∈ [T ] uniformly (Alg. 1, Line 3-4). Then, the algorithm uniformly draws an integer k ∈ [r ] and performs (k − 1)-step and (r − k)-step random walks starting from the two endpoints of edge e respectively (Alg. 2, Line 3-4). The above process samples a length-r path p = (u 0 , u 1 , • • • , u r ). At the same time, the algorithm keeps track of Z (p), which is defined by</p><formula xml:id="formula_13">Z (p) = r i =1 2 A u i −1 ,u i ,<label>(7)</label></formula><p>and then adds a new edge (u 0 , u r ) with weight 2rm M Z (p) to G (Alg. 1, Line 6). 3 Parallel edges in G will be merged into one single edge, with their weights summed up together. Finally, the algorithm computes the Laplacian of G, which is the sparsifier L as we desired (Alg. 1, Line 8). This step gives us a sparsifier with O(M) non-zeros.</p><p>Step 2: Construct a NetMF Matrix Sparsifier. As we have discussed at the end of Section 3.1, after constructing a sparsifier L, we can plug it into Eq. ( <ref type="formula" target="#formula_8">5</ref>) to obtain a NetMF matrix sparsifier as shown in Eq. ( <ref type="formula" target="#formula_9">6</ref>) (Alg. 1, Line 9-10). This step does not change the order of magnitude of non-zeros in the sparsifier. 3 Details about how the edge weight is derived can be found in Thm. 4 in Appendix.</p><p>Algorithm 3: Randomized SVD on NetMF Matrix Sparsifier /* In this work, the matrix to be factorized (Eq. ( <ref type="formula" target="#formula_9">6</ref>)) is an n × n symmetric sparse matrix. We store this sparse matrix in a row-major way and make use of its symmetry to simplify the computation. */ </p><formula xml:id="formula_14">1 Procedure RandomizedSVD(A, d ) 2 Sampling Gaussian random matrix O // O ∈ R n×d 3 Compute sample matrix Y = A ⊤ O = AO // Y ∈ R n×d 4 Orthonormalize Y 5 Compute B = AY // B ∈ R n×d 6 Sample another Gaussian random matrix P // P ∈ R d ×d 7 Compute sample matrix of Z = BP // Z ∈ R n×d 8 Orthonormalize Z 9 Compute C = Z ⊤ B // C ∈ R d ×d 10 Run Jacobi SVD on C = U ΣV ⊤ 11 return Z U , Σ, Y V /* Result matrices are of shape n × d, d × d, n × d resp. */</formula><formula xml:id="formula_15">O (M + n + m) Step 2 O (M ) O (M + n) Step 3 O (Md + nd 2 + d 3 ) O (M + nd )</formula><p>Step 3: Truncated Singular Value Decomposition. The final step is to perform truncated singular value decomposition (SVD) on the constructed NetMF matrix sparsifier (Eq. ( <ref type="formula" target="#formula_9">6</ref>)). However, even the sparsifier only has O(M) number of non-zeros, performing exact SVD is still time consuming. In this work, we leverage a modern randomized matrix approximation technique-Randomized SVDdeveloped by Halko et al. <ref type="bibr" target="#b14">[15]</ref>. Due to space constraint, we cannot include many details. Briefly speaking, the algorithm projects the original matrix to a low-dimensional space through a Gaussian random matrix. One only needs to perform traditional SVD (e.g. Jacobi SVD) on a d × d small matrix. We list the pseudocode algorithm in Alg. 3. Another advantage of SVD is that we can determine the dimensionality of embeddings by using, for example, Cattell's Scree test <ref type="bibr" target="#b4">[5]</ref>. In the test, we plot the singular values and select a rank d such that there is a clear drop in the magnitudes or the singular values start to even out. More details will be discussed in Section 4.</p><p>Complexity Analysis. Now we analyze the time and space complexity of NetSMF, as summarized in Table <ref type="table" target="#tab_2">3</ref>. As for step 1, we call the PathSampling algorithm for M times, during each of which it performs O(T ) steps of random walks over the network. For unweighted networks, sampling a neighbor requires O(1) time, while for weighted networks, one can use roulette wheel selection to choose a neighbor in O(log n). It taks O(M) space to store G, while the additional O(n + m) space comes from the storage of the input network. As for step 2, it takes O(M) time to perform the transformation in Eq. ( <ref type="formula" target="#formula_8">5</ref>) and the element-wise truncated logarithm in Eq. ( <ref type="formula" target="#formula_9">6</ref>). The additional O(n) space is spent in storing the degree matrix. As for step 3, O(Md) time is required to compute the product of a row-major sparse matrix and a dense matrix (Alg. Connection to NetMF. The major difference between NetMF and NetSMF lies in the approximation strategy of the NetMF matrix in Eq. ( <ref type="formula" target="#formula_5">3</ref>). As we mentioned in Section 2, NetMF approximates it with a dense matrix, which brings new space and computation challenges. In this work, NetSMF aims to find a sparse approximator to the NetMF matrix by leveraging theories and techniques from spectral graph sparsification.</p><p>Example. We provide a running example to help understand the NetSMF algorithm. Suppose we want to learn embeddings for a network with n = 10 6 vertices, m = 10 7 edges, context window size T = 10, and approximation factor ϵ = 0.1. The NetSMF method calls the PathSampling algorithm for M = Tmϵ −2 log n ≈ 1.4×10 11 times and provides us with a NetMF matrix sparsifier with at most 1.4 × 10 11 non-zeros (Notice that the reducer in Step 1 and trunc_log • in Step 2 will further sparsify the matrix, making 1.4 × 10 11 an upper bound). The density of the sparsifier is at most M n 2 ≈ 14%. Then, when computing the sparse-dense matrix product in randomized SVD (Alg. 3, Lines 3 and 5), the sparseness of the factorized matrix can greatly accelerate the calculation. In comparison, NetMF must construct a dense matrix with n 2 = 10 12 non-zeros, which is an order of magnitude larger in terms of density. Also, the density of the sparsifier in NetSMF can be further reduced by using a larger ϵ, while NetMF does not have this flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approximation Error Analysis</head><p>In this section, we analyze the approximation error of the sparsification. We assume that we choose an approximation factor ϵ &lt; 0.5. We first see how the constructed M approximates M and then compare the NetMF matrix (Eq. ( <ref type="formula" target="#formula_5">3</ref>)) against the NetMF matrix sparsifier (Eq. ( <ref type="formula" target="#formula_9">6</ref>)). We use σ i to denote the i-th descending-order singular value of a matrix. We also assume the vertices' degrees are sorted in ascending order, that is,</p><formula xml:id="formula_16">d min = d 1 ≤ d 2 • • • ≤ d n . Theorem 2. The singular value of M − M satisfies σ i ( M − M) ≤ 4ϵ √ d i d min , ∀i ∈ [n].</formula><p>Theorem 3. Let ∥•∥ F be the matrix Frobenius norm. Then</p><formula xml:id="formula_17">trunc_log • vol(G) b M − trunc_log • vol(G) b M F ≤ 4ϵ vol(G) b √ d min n i =1 1 d i .</formula><p>Proof. See Appendix. □ Discussion on the Approximation Error. The above bound is achieved without making assumptions about the input network. If we introduce some assumptions, say a bounded lowest degree d min or a specific random graph model (e.g., Planted Partition Model or Extended Planted Partition Model), it is promising to explore tighter bounds by leveraging theorems in literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parallelization</head><p>Each step of NetSMF can be parallelized, enabling it to scale to very large networks. The parallelization design of NetSMF is introduced in Figure <ref type="figure" target="#fig_1">1</ref>. Below we discuss the parallelization of each step in detail. At the first step, the paths in the PathSampling algorithm are sampled independently with each other. Thus we can launch multiple PathSampling workers simultaneously. Each worker handles a subset of the samples. Herein, we require that each worker is able to access the network data G = (V , E, A) efficiently. There are many options to meet this requirement. The easiest one is to load a copy of the network data to each worker's memory. When the network is extremely large (e.g., trillion scale) or workers have memory constraints, the graph engine should be designed to expose efficient graph query APIs to support graph operations such as random walks. At the end of this step, a reducer is designed to merge parallel edges and sum up their weights. If this step is implemented in a big data system such as Spark <ref type="bibr" target="#b44">[45]</ref>, the reduction step can be simply achieved by running a reduceByKey(_+_)<ref type="foot" target="#foot_0">4</ref> function. After the reduction, the sparsifier L is organized as a collection of triplets, a.k.a, COOrdinate format, with each indicating an entry of the sparsifier. The second step is the most straightforward step to scale up. When processing a triplet (u, v, w), we can simply query the degree of vertices u and v and perform the transformation defined in Eq. ( <ref type="formula" target="#formula_8">5</ref>) as well as the truncated logarithm in Eq. ( <ref type="formula" target="#formula_9">6</ref>), which can be well parallelized. For the last step, we organize the sparsifier into row-major format. This format allows efficient multiplication between a sparse and a dense matrix (Alg. 3, Line 3 and 5). Other dense matrix operators (e.g., Gaussian random matrix generation, Gram-Schmidt orthogonalization and Jacobi SVD) can be easily accelerated by using multi-threading or common linear algebra libraries. In this work, we adopt a single-machine shared-memory implementation. We use OpenMP <ref type="bibr" target="#b9">[10]</ref> to parallelize NetSMF in our implementation <ref type="foot" target="#foot_1">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed NetSMF method on the multi-label vertex classification task, which has been commonly used to evaluate previous network embedding techniques <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>. We introduce our datasets and baselines in Section 4.1 and Section 4.2. We report experimental results and parameter analysis in Section 4.3 and Section 4.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We employ five datasets for the prediction task, four of which are in relatively small scale but have been widely used in network embedding literature, including BlogCatalog, PPI, Flickr, and YouTube. The remaining one is a large-scale academic co-authorship network, which is at least two orders of magnitude larger than the largest one (YouTube) used in most network embedding studies. The statistics of these datasets are listed in Table <ref type="table" target="#tab_4">4</ref>. The input comes from a graph engine which stores the network data and provides efficient APIs to graph queries. In Step 1, the system launches several PathSampling workers. Each worker handles a subset of samples. Then, a reducer is designed to aggregate the output of the PathSampling algorithm. In Step 2, the system distributes data to several sparsifier constructors to perform the transformation defined in Eq. ( <ref type="formula" target="#formula_8">5</ref>) and the truncated element-wise matrix logarithm in Eq. ( <ref type="formula" target="#formula_9">6</ref>). In the final step, the system applies truncated randomized SVD on the constructed sparsifier and dumps the resulted embeddings to storage.  BlogCatalog <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref> is a network of social relationships of online bloggers. The vertex labels represent the interests of the bloggers.</p><p>Protein-Protein Interactions (PPI) <ref type="bibr" target="#b30">[31]</ref> is a subgraph of the PPI network for Homo Sapiens. The vertex labels are obtained from the hallmark gene sets and represent biological states.</p><p>Flickr <ref type="bibr" target="#b34">[35]</ref> is the user contact network in Flickr. The labels represent the interest groups of the users.</p><p>YouTube <ref type="bibr" target="#b35">[36]</ref> is a video-sharing website that allows users to upload, view, rate, share, add to their favorites, report, comment on videos. The users are labeled by the video genres they liked.</p><p>Open Academic Graph (OAG) <ref type="foot" target="#foot_2">6</ref> is an academic graph indexed by Microsoft Academic <ref type="bibr" target="#b28">[29]</ref> and AMiner.org <ref type="bibr" target="#b33">[34]</ref>. We construct an undirected co-authorship network from OAG, which contains 67,768,244 authors and 895,368,962 collaboration edges. The vertex labels are defined to be the top-level fields of study of each author, such as computer science, physics and psychology. In total, there are 19 distinct fields (labels) and authors may publish in more than one field, making the associated vertices have multiple labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We compare NetSMF with NetMF <ref type="bibr" target="#b27">[28]</ref>, LINE <ref type="bibr" target="#b32">[33]</ref>, DeepWalk <ref type="bibr" target="#b26">[27]</ref>, and node2vec <ref type="bibr" target="#b13">[14]</ref>. For NetSMF, NetMF, DeepWalk, and node2vec that allow multi-hop structural dependencies, the context window size T is set be 10, which is also the default setting used in both DeepWalk and node2vec. Across all datasets, we set the embedding dimension d to be 128. We follow the common practice for the other hyper-parameter settings, which are introduced below.</p><p>LINE. We use LINE with the second order proximity (i.e., LINE (2nd) <ref type="bibr" target="#b32">[33]</ref>). We use the default setting of LINE's hyper-parameters: the number of edge samples to be 10 billion and the negative sample size to be 5.</p><p>DeepWalk. We present DeepWalk's results with the authors' preferred parameters, that is, walk length to be 40, the number of walks from each vertex to be 80, and the number of negative samples in skip-gram to be 5.</p><p>node2vec. For the return parameter p and in-out parameter q in node2vec, we adopt the default setting that was used by its authors if available. Otherwise, we grid search p, q ∈ {0.25, 0.5, 1, 2, 4}. For a fair comparison, we use the same walk length and the number of walks per vertex as DeepWalk.</p><p>NetMF. In NetMF, the hyper-parameter h indicates the number of eigen pairs used to approximate the NetMF matrix. We choose h = 256 for the BlogCatalog, PPI and Flickr datasets.</p><p>NetSMF. In NetSMF, we set the number of samples M = 10 3 ×T ×m for the PPI, Flickr, and YouTube datasets, M = 10 4 × T × m for BlogCatalog, and M = 10 × T × m for OAG in order to achieve desired performance. For both NetMF and NetSMF, we have b = 1.</p><p>Prediction Setting. We follow the same experiment and evaluation procedures that were performed in DeepWalk <ref type="bibr" target="#b26">[27]</ref>. First, we randomly sample a portion of labeled vertices for training and use the remaining for testing. For the BlogCatalog and PPI datasets, the training ratio varies from 10% to 90%. For Flickr, YouTube and OAG, the training ratio varies from 1% to 10%. We use the one-vs-rest logistic regression model implemented by LIBLINEAR <ref type="bibr" target="#b12">[13]</ref> for the multi-label vertex classification task. In the test phase, the onevs-rest model yields a ranking of labels rather than an exact label assignment. To avoid the thresholding effect, we take the assumption that was made in DeepWalk, LINE, and node2vec, that is, the number of labels for vertices in the test data is given <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref>. We repeat the prediction procedure ten times and evaluate the average performance in terms of both Micro-F1 and Macro-F1 scores <ref type="bibr" target="#b40">[41]</ref>.</p><p>All the experiments are performed on a server with Intel Xeon E7-8890 CPU (64 cores), 1.7TB memory, and 2TB SSD hard drive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>We summarize the prediction performance in Figure <ref type="figure" target="#fig_5">2</ref>. To compare the efficiency of different algorithms, we also list the running time of each algorithm across all datasets, if available, in Table <ref type="table" target="#tab_5">5</ref>.</p><p>NetSMF vs. NetMF. We first focus on the comparison between NetSMF and NetMF, since the goal of NetSMF is to address the efficiency and scalability issues of NetMF while maintaining its superiority in effectiveness. From Table <ref type="table" target="#tab_5">5</ref>, we observe that for YouTube and OAG, both of which contain more than one million vertices, NetMF fails to complete because of the excessive space and memory consumption, while NetSMF is able to finish in four hours and one day, respectively. For the moderate-size network Flickr, both methods are able to complete within one week, though NetSMF is 2.5× faster (i.e., 48 mins vs. 2 hours). For small-scale networks, NetMF is faster than NetSMF in BlogCatalog and is comparable to NetSMF in PPI in terms of running time. This is because when the input networks contain only thousands of vertices, the advantage of sparse matrix construction and factorization over its dense alternative could be marginalized by other components of the workflow.</p><p>In terms of prediction performance, Figure <ref type="figure" target="#fig_5">2</ref> suggests NetSMF and NetMF yield consistently the best results among all compared methods, empirically demonstrating the power of the matrix factorization framework for network embedding. In BlogCatalog, NetSMF has slightly worse performance than NetMF (on average less than 3.1% worse regarding both Micro-and Macro-F1). In PPI, the two leading methods' performance are relatively indistinguishable in terms of both metrics. In Flickr, NetSMF achieves significantly better Macro-F1 than NetMF (by 3.6% on average), and also higher Micro-F1 (by 5.3% on average). Recall that NetMF uses a dense approximation of the matrix to factorize. These results show that the sparse spectral approximation used by NetSMF does not necessarily yield worse performance than the dense approximation used by NetMF.</p><p>Overall, not only NetSMF improves the scalability, and the running time of NetMF by orders of magnitude for large-scale networks, it also has competitive, and sometimes better, performance. This demonstrates the effectiveness of our spectral sparsification based approximation algorithm.</p><p>NetSMF vs. DeepWalk, LINE &amp; node2vec. We also compare NetSMF against common graph embedding benchmarks-DeepWalk, LINE, and node2vec. For the OAG dataset, DeepWalk and node2vec fail to finish the computation within one week, while NetSMF requires only 24 hours. Based on the publicly reported running time of skip-gram <ref type="bibr" target="#b23">[24]</ref>, we estimate that DeepWalk and node2vec may require months to generate embeddings for the OAG dataset. In BlogCatalog, DeepWalk and NetSMF require similar computing time, while in Flickr, YouTube, and PPI, NetSMF is 2.75×, 5.9×, and 24× faster than DeepWalk, respectively. In all the datasets, NetSMF achieves 4-24× speedup over node2vec.</p><p>Moreover, the performance of NetSMF is significantly better than DeepWalk in BlogCatalog, PPI, and Flickr, by 7-34% in terms of Micro-F1 and 5-25% in terms of Macro-F1. In YouTube, NetSMF achieves comparable results to DeepWalk. Compared with node2vec, NetSMF achieves comparable performance in BlogCatalog and YouTube, and significantly better performance in PPI and Flickr. In summary, NetSMF consistently outperforms DeepWalk and node2vec in terms of both efficiency and effectiveness.</p><p>LINE has the best efficiency among all the five methods and together with NetSMF, they are the only methods that can generate embeddings for OAG within one week (and both finish in one day). However, it also has the worst prediction performance and consistently loses to others by a large margin across all datasets. For example, NetSMF beats LINE by 21% and 39% in Flickr, and by 30% and 100% in OAG in terms of Micro-F1 and Macro-F1, respectively.</p><p>In summary, LINE achieves efficiency at the cost of ignoring multi-hop dependencies in networks, which are supported by all the other four methods-DeepWalk, node2vec, NetMF, and NetSMF, demonstrating the importance of multi-hop dependencies for learning network representations.</p><p>More importantly, among these four methods, DeepWalk achieves neither efficiency nor effectiveness superiority; node2vec achieves relatively good performance at the cost of efficiency; NetMF achieves effectiveness at the expense of significantly increased time and space costs; NetSMF is the only method that achieves both high efficiency and effectiveness, empowering it to learn effective embeddings for billion-scale networks (e.g., the OAG network with 0.9 billion edges) in one day on one modern server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Analysis</head><p>In this section, we discuss how the hyper-parameters influence the performance and efficiency of NetSMF. We report all the parameter analyses on the Flickr dataset with training ratio set to be 10%.</p><p>How to Set the Embedding Dimension d. As mentioned in Section 3.1, SVD allows us to determine a "good" embedding dimension without supervised information. There are many methods available such as captured energy and Cattell's Scree test <ref type="bibr" target="#b4">[5]</ref>. Here we propose to use Cattell's Scree test. Cattell's Scree test plots the singular values and selects a rank d such that there is a clear drop in the magnitudes or the singular values start to even out. In Flickr, if we sort the singular values in decreasing order, we can observe that the singular values approach 0 when the rank increases to around 100, as shown in Figure <ref type="figure" target="#fig_7">3</ref> where k is chosen from 1, 10, 100, 200, 500, 1000, 2000 and investigate how the number of non-zeros influence the quality of learned embeddings. As shown in Figure <ref type="figure" target="#fig_7">3(c)</ref>, when increasing the number of non-zeros, NetSMF tends to have better prediction performance because the original matrix is being approximated more accurately. On the other hand, although increasing M has a positive effect on the prediction performance, its marginal benefit diminishes gradually. One can observe that setting M = 1000 × T × m (the second-to-the-right data point on each line in Figure <ref type="figure" target="#fig_7">3(c)</ref>) is a good choice that balances NetSMF's efficiency and effectiveness.</p><p>The Number of Threads. In this work, we use a single-machine shared memory implementation with multi-threading acceleration. We report the running time of NetSMF when setting the number of threads to be 1, 10, 20, 30, 60, respectively. As shown in Figure <ref type="figure" target="#fig_7">3(d)</ref>, NetSMF takes 12 hours to embed the Flickr network with one thread and 48 minutes to run with 30 threads, achieving a 15× speedup ratio (with ideal being 30×). This relatively good sub-linear speedup supports NetSMF to scale up to very large-scale networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>In this section, we review the related work of network embedding, large-scale embedding algorithms, and spectral graph sparsification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Network Embedding</head><p>Network embedding has been extensively studied over the past years <ref type="bibr" target="#b15">[16]</ref>. The success of network embedding has driven a lot of downstream network applications, such as recommendation systems <ref type="bibr" target="#b43">[44]</ref>. Briefly, recent work about network embedding can be categorized into three genres: (1) Skip-gram based methods that are inspired by word2vec <ref type="bibr" target="#b23">[24]</ref>, such as LINE <ref type="bibr" target="#b32">[33]</ref>, DeepWalk <ref type="bibr" target="#b26">[27]</ref>, node2vec <ref type="bibr" target="#b13">[14]</ref>, metapath2vec <ref type="bibr" target="#b11">[12]</ref>, and VERSE <ref type="bibr" target="#b39">[40]</ref>; (2) Deep learning based methods such as <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44]</ref>; (3) Matrix factorization based methods such as GraRep <ref type="bibr" target="#b3">[4]</ref> and NetMF <ref type="bibr" target="#b27">[28]</ref>. Among them, NetMF bridges the first and the third categories by unifying a collection of skip-gram based network embedding methods into a matrix factorization framework. In this work, we leverage the merit of NetMF and address its limitation in efficiency. Among literature, PinSage is notably a network embedding framework for billionscale networks <ref type="bibr" target="#b43">[44]</ref>. The difference between NetSMF and PinSage lies in the following aspect. The goal of NetSMF is to pre-train general network embeddings in an unsupervised manner, while PinSage is a supervised graph convolutional method with both the objective of recommender systems and existing node features incorporated. That being said, the embeddings learned by NetSMF can be consumed by PinSage for downstream network applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Large-Scale Embedding Learning</head><p>Studies have attempted to optimize embedding algorithms for large datasets from different perspectives. Some focus on improving skipgram model, while others consider it as matrix factorization.</p><p>Distributed Skip-Gram Model. Inspired by word2vec <ref type="bibr" target="#b24">[25]</ref>, most of the modern embedding learning algorithms are based on the skip-gram model. There is a sequence of work trying to accelerate the skip-gram model in a distributed system. For example, Ji et al. <ref type="bibr" target="#b18">[19]</ref> replicate the embedding matrix on multiple workers and synchronize them periodically; Ordentlich et al. <ref type="bibr" target="#b25">[26]</ref> distribute the columns (dimensions) of the embedding matrix to multiple executors and synchronize them with a parameter server <ref type="bibr" target="#b22">[23]</ref>. Negative sampling is a key step in skip-gram, which requires to draw samples from a noisy distribution. Stergiou et al. <ref type="bibr" target="#b31">[32]</ref> focus on the optimization of negative sampling by replacing the roulette wheel selection with a hierarchical sampling algorithm based on the alias method. More recently, Wang et al. <ref type="bibr" target="#b42">[43]</ref> propose a billion-scale network embedding framework by heuristically partitioning the input graph to small subgraphs, and processing them separately in parallel. However, the performance of their framework highly relies on the quality of graph partition. The drawback for partition-based embedding learning is that the embeddings learned in different subgraphs do not share the same latent space, making it impossible to compare nodes across subgraphs.</p><p>Efficient Matrix Factorization. Factorizing the NetMF matrix, either implicitly (e.g., LINE <ref type="bibr" target="#b32">[33]</ref> and DeepWalk <ref type="bibr" target="#b26">[27]</ref>) or explicitly (e.g., NetMF <ref type="bibr" target="#b27">[28]</ref>), encounters two issues. First, the denseness of this matrix makes computation expensive even for a moderate context window size (e.g., T = 10). Second, the non-linear transformation, i.e., element-wise matrix logarithm, is hard to approximate. LINE <ref type="bibr" target="#b32">[33]</ref> solves this problem by setting T = 1. With such simplification, it achieves good scalability at the cost of prediction performance. NetSMF addresses these issues by efficiently sparsifying the dense NetMF matrix with a theoretically-bounded approximation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Spectral Graph Sparsification</head><p>Spectral graph sparsification has been studied for decades in graph theory <ref type="bibr" target="#b37">[38]</ref>. The task of graph sparsification is to approximate a "dense" graph by a "sparse" one that can be effectively used in place of the dense one <ref type="bibr" target="#b37">[38]</ref>, which arises in many applications such as scientific computing <ref type="bibr" target="#b16">[17]</ref>, machine learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> and data mining <ref type="bibr" target="#b45">[46]</ref>. Our NetSMF model is the first work that incorporates spectral sparsification algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> into network embedding, which offers a powerful and efficient way to approximate and analyze the random-walk matrix-polynomial in the NetMF matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we study network embedding with the goal of achieving both efficiency and effectiveness. To address the scalability challenges faced by the NetMF model, we propose to study large-scale network embedding as sparse matrix factorization. We present the NetSMF algorithm, which achieves a sparsification of the (dense) NetMF matrix. Both the construction and factorization of the sparsified matrix are fast enough to support very large-scale network embedding learning. For example, it empowers NetSMF to efficiently embed the Open Academic Graph in 24 hours, whose size is computationally intractable for the dense matrix factorization solution (NetMF). Theoretically, the sparsified matrix is spectrally close to the original NetMF matrix with an approximation bound. Empirically, our extensive experimental results show that the sparsely learned embeddings by NetSMF are as effective as those from the factorization of the NetMF matrix, leaving it outperform the common network embedding benchmarks-DeepWalk, LINE, and node2vec. In other words, among both matrix factorization based methods (NetMF and NetSMF) and common skip-gram based benchmarks (DeepWalk, LINE, and node2vec), NetSMF is the only model that achieves both efficiency and performance superiority.</p><p>Future Work. NetSMF brings an efficient, effective, and guaranteed solution to network embedding learning. There are multiple tangible research fronts we can pursue. First, our current singlemachine implementation limits the number of samples we can take for large networks. We plan to develop a multi-machine solution in the future to further scale NetSMF. Second, building upon NetSMF, we would like to efficiently and accurately learn embeddings for large-scale directed <ref type="bibr" target="#b8">[9]</ref>, dynamic <ref type="bibr" target="#b19">[20]</ref>, and/or heterogeneous networks. Third, as the advantage of matrix factorization methods demonstrated, we are also interested in exploring the other matrix definitions that may be effective in capturing different structural properties in networks. Last, it would be also interesting to bridge matrix factorization based network embedding methods with graph convolutional networks.</p><p>Theorem 3. Let ∥•∥ F be the matrix Frobenius norm. Then</p><formula xml:id="formula_18">trunc_log • vol(G) b M − trunc_log • vol(G) b M F ≤ 4ϵ vol(G) b √ d min n i =1 1 d i .</formula><p>Proof. It is easy to observe that trunc_log • is 1-Lipchitz w.r.t. Frobenius norm. So we have</p><formula xml:id="formula_19">trunc_log • vol(G) b M − trunc_log • vol(G) b M F ≤ vol(G) b M − vol(G) b M F = vol(G) b M − M F = vol(G) b i ∈[n] σ 2 i ( M − M ) ≤ 4ϵ vol(G) b √ d min n i =1 1 d i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□</head><p>We finally explain the remaining question in Step 1 of NetSMF: After sampling a length-r path p = (u 0 , • • • , u r ), why does the algorithm add a new edge to the sparsifier with weight rm MZ (p) ? Our proof relies on two lemmas from <ref type="bibr" target="#b7">[8]</ref>. For unweighted networks, this weight can be simplified to m M , since Z (p) = 2r for unweighted networks. □</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>√</head><label></label><figDesc>achieving more effective embeddings than DeepWalk and LINE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 1 .</head><label>1</label><figDesc>(Spectral Similarity of Networks) Suppose G = (V , E, A) and G = (V , E, A) are two weighted undirected networks. Let L = D G − A and L = D G − A be their Laplacian matrices, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :2 for i ← 1 to M do 3 4 Uniformly pick an integer r ∈ [T ] 5 u 6</head><label>13456</label><figDesc>NetSMF Input : A social network G = (V , E, A) which we want to learn network embedding; The number of non-zeros M in the sparsifier; The dimension of embedding d . Output : An embedding matrix of size n × d , each row corresponding to a vertex. 1 G ← (V , ∅, A = 0) /* Create an empty network with E = ∅ and A = 0. */ Uniformly pick an edge e = (u, v) ∈ E ′ , v ′ , Z ← PathSampling(e, r) Add an edge u ′ , v ′ , 2r m M Z to G /* Parallel edges will be merged into one edge, with their weights summed up together. */ 7 end 8 Compute L to be the unnormalized graph Laplacian of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The System Design of NetSMF. The input comes from a graph engine which stores the network data and provides efficient APIs to graph queries. In Step 1, the system launches several PathSampling workers. Each worker handles a subset of samples. Then, a reducer is designed to aggregate the output of the PathSampling algorithm. In Step 2, the system distributes data to several sparsifier constructors to perform the transformation defined in Eq. (5) and the truncated element-wise matrix logarithm in Eq. (6). In the final step, the system applies truncated randomized SVD on the constructed sparsifier and dumps the resulted embeddings to storage.</figDesc><graphic url="image-1.png" coords="6,62.78,92.73,486.45,229.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Predictive performance w.r.t. the ratio of training data. The x-axis represents the ratio of labeled data (%), and the y-axis in the top and bottom rows denote the Micro-F1 and Macro-F1 scores (%) respectively. For methods which fail to finish computation in one week or cannot handle the computation, their results are not available and thus not plotted in this figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b). In our experiments, by varying d form 2 4 to 2 8 , we reach the best performance at d = 128, as shown in Figure 3(a), demonstrating the ability of our matrix factorization based NetSMF for automatically determining the embedding dimension. The Number of Non-Zeros M. In theory, M = O(Tmϵ −2 log n) is required to guarantee the approximation error (See Section 3.1). Without loss of generality, we empirically set M to be k × T × m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parameter analysis: (a) Prediction performance v.s. embedding dimension d; (b) Cattel's Scree Test on singular values. (c) Prediction performance v.s. the number of non-zeros M; (d) Running time v.s. the number of threads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Lemma 5 . 1 i</head><label>51</label><figDesc>(Lemma 3.3 in<ref type="bibr" target="#b7">[8]</ref>) Given the path length r , the probability for the PathSampling algorithm to sample a path p is τ (p) = w (p)Z (p) 2rm , where Z (p) is defined in Eq.<ref type="bibr" target="#b6">(7)</ref> andw (p) = r i =1 A u i −1 ,u i r −=1 D u i . Lemma 6. (Theorem 2.2 in<ref type="bibr" target="#b7">[8]</ref>) After sampling a length-r path p = (u 0 , u 1 , • • • , u r ), the weight corresponding to the new edge (u 0 , u r ) added to the sparsifier should bew (p) τ (p)M .Theorem 4. After sampling a length-r path p = (u 0 , u 1 , • • • , u r ) using the PathSampling algorithm (Alg. 2). The weight of the new edge added to the sparsifier L is 2rm MZ (p) .Proof. The proof is to plug the definition of Z (p), w(p), and τ (p) from Lemma 5 into Lemma 6, that is,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The comparison between NetSMF and other popular network embedding algorithms.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Notations.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>G</cell><cell>input network</cell></row><row><cell>V</cell><cell>vertex set of G with |V |=n</cell></row><row><cell>E</cell><cell>edge set of G with |E | = m</cell></row><row><cell>A</cell><cell>adjacency matrix of G</cell></row><row><cell>D</cell><cell>degree matrix of G</cell></row><row><cell>vol (G)</cell><cell>volume of G</cell></row><row><cell>b</cell><cell>number of negative samples</cell></row><row><cell>T</cell><cell>context window size</cell></row><row><cell>d</cell><cell>embedding dimension</cell></row><row><cell>L</cell><cell>random-walk molynomial of G (Eq. (</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Time and Space Complexity of NetSMF.</figDesc><table /><note>Time SpaceStep 1 O (MT log n) for weighted networks O (MT ) for unweighted networks</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>3, Lines 3 and 5); O(nd 2 ) time is spent in Gram-Schmidt orthogonalization (Alg. 3, Lines 4 and 8); O(d 3 ) time is spent in Jacobi SVD (Alg. 3, Line 10).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Statistics of Datasets.</figDesc><table><row><cell cols="2">Dataset BlogCatalog</cell><cell>PPI</cell><cell>Flickr</cell><cell>YouTube</cell><cell>OAG</cell></row><row><cell>|V |</cell><cell>10,312</cell><cell>3,890</cell><cell>80,513</cell><cell>1,138,499</cell><cell>67,768,244</cell></row><row><cell>|E |</cell><cell>333,983</cell><cell cols="4">76,584 5,899,882 2,990,443 895,368,962</cell></row><row><cell>#labels</cell><cell>39</cell><cell>50</cell><cell>195</cell><cell>47</cell><cell>19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Efficiency comparison. The running time includes filesystem IO and computation time. "-" indicates that the corresponding algorithm fails to complete within one week. "×" indicates that the corresponding algorithm is unable to handle the computation due to excessive space and memory consumption.</figDesc><table><row><cell></cell><cell>L I N E</cell><cell>D e e p W a l k</cell><cell>n o d e 2 v e c</cell><cell>N e t M F</cell><cell>N e t S M F</cell></row><row><cell>BlogCatalog</cell><cell>40 mins</cell><cell>12 mins</cell><cell>56 mins</cell><cell>2 mins</cell><cell>13 mins</cell></row><row><cell>PPI</cell><cell>41 mins</cell><cell>4 mins</cell><cell>4 mins</cell><cell>16 secs</cell><cell>10 secs</cell></row><row><cell>Flickr</cell><cell>42 mins</cell><cell cols="3">2.2 hours 21 hours 2 hours</cell><cell>48 mins</cell></row><row><cell>YouTube</cell><cell>46 mins</cell><cell>1 day</cell><cell>4 days</cell><cell>×</cell><cell>4.1 hours</cell></row><row><cell>OAG</cell><cell>2.6 hours</cell><cell>-</cell><cell>-</cell><cell>×</cell><cell>24 hours</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">https://spark.apache.org/docs/latest/rdd-programming-guide.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1">Code is publicly available at https://github.com/xptree/NetSMF</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2">www.openacademic.ai/oag/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Dehua Cheng and Youwei Zhuo from USC for helpful discussions. Jian Li is supported in part by the National Basic Research Program of China Grant 2015CB358700, the National Natural Science Foundation of China Grant 61822203, 61772297, 61632016, 61761146003, and a grant from Microsoft Research Asia. Jie Tang is the corresponding author.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We first prove Thm. 2 and Thm. 3 in Section 3.3. The following lemmas will be useful in our proof.</p><p>Lemma 1. ( <ref type="bibr" target="#b38">[39]</ref>) Singular values of a real symmetric matrix are the absolute values of its eigenvalues.</p><p>x ⊤ Ax . <ref type="bibr">Lemma 3. ([18]</ref>) Let B, C be two n × n symmetric matrices. Then for the decreasingly-ordered singular values σ of B, C and BC,</p><p>Then all the singular values of L−L are smaller than 2ϵ, i.e., ∀i</p><p>Proof. Notice that</p><p>which is a normalized graph Laplacian whose eigenvalues lie in the interval [0, 2), i.e., for i ∈ [n], λ i (L) ∈ [0, 2) <ref type="bibr" target="#b41">[42]</ref>. Since L is a (1 + ϵ)-spectral sparsifier of L, we know that for ∀x ∈ R n ,</p><p>Let x = D −1/2 y which is bijective, we have</p><p>The last inequality is because we assume ϵ &lt; 0.5. Then, by Courant-Fisher Theorem (Lemma 2), we can immediately get, ∀i ∈ [n],</p><p>Then, by Lemma 1,</p><p>Given the above lemmas, we can see how the constructed M approximates M and how the constructed NetMF matrix sparsifier (Eq. ( <ref type="formula">6</ref>)) approximates the NetMF matrix (Eq. ( <ref type="formula">3</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2. The singular value of</head><p>. Apply Lemma 3 twice and use the result from Lemma 4, we have</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Social Identity Approach to Identify Familiar Strangers in a Social Network</title>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arunabha</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xufei</forename><surname>Wang</surname></persName>
		</author>
		<idno>ICWSM &apos;09</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Four degrees of separation</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastiano</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WebSci &apos;12</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved large-scale graph learning through ridge spectral sparsification</title>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Calandriello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Koutis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno>ICML &apos;18. 687âĂŞ696</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GraRep: Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The scree test for the number of factors</title>
		<author>
			<persName><surname>Raymond B Cattell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate behavioral research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="276" />
			<date type="published" when="1966">1966. 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral clustering of graphs with general degrees in the extended planted partition model</title>
		<author>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Tsiatas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT &apos;12</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="35" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient sampling for Gaussian graphical models via spectral sparsification</title>
		<author>
			<persName><forename type="first">Dehua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT &apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="364" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Dehua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03496</idno>
		<title level="m">Spectral sparsification of random-walk matrix polynomials</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Faster algorithms for computing the stationary distribution, simulating random walks, and more</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Michael B Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kelner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Sidford</surname></persName>
		</author>
		<author>
			<persName><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS &apos;16</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">OpenMP: an industry standard API for shared-memory programming</title>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Menon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE computational science and engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectral analysis of random graphs with skewed degree distributions</title>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS &apos;04</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-08">2008. Aug (2008</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;16</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Per-Gunnar Martinsson</surname></persName>
		</author>
		<author>
			<persName><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="217" to="288" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs: Methods and Applications</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data(base) Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On p th roots of stochastic matrices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijing</forename><surname>Higham</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="page" from="448" to="463" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511840371</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511840371" />
		<title level="m">Topics in Matrix Analysis</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04661</idno>
		<title level="m">Parallelizing word2vec in shared and distributed memory</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single pass spectral sparsification in dynamic streams</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kapralov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="456" to="477" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>ICLR &apos;17</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural Word Embedding as Implicit Matrix Factorization</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS &apos;14</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling Distributed Machine Learning with the Parameter Server</title>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><forename type="middle">Woo</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bor-Yiing</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;14</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="583" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop &apos;13</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos; 13</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Network-efficient distributed word2vec training system for large vocabularies</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Ordentlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Cnudde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;16</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1139" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;14</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;18</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An overview of microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph sparsification by effective resistances</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1913" to="1926" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The BioGRID interaction database: 2011 update</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby-Joe</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chatr-Aryamontri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrie</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Oughtred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Michael S Livstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Van Auken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="D698" to="D704" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed Negative Sampling for Word Embeddings</title>
		<author>
			<persName><forename type="first">Stergios</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zygimantas</forename><surname>Straznickas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolina</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI &apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2569" to="2575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;08</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;09</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalable learning of collective behavior based on sparse social dimensions</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;09</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large scale multi-label classification via metalabeler</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suju</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">K</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;09</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable algorithms for data and network analysis</title>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="274" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Numerical linear algebra</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lloyd N Trefethen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Bau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">50</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">VERSE: Versatile Graph Embeddings from Similarity Measures</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mining multi-label data</title>
		<author>
			<persName><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data mining and knowledge discovery handbook</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;18</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Spark: Cluster computing with working sets</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Michael J Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><surname>Stoica</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">gSparsify: Graph Motif Based Sparsification for Graph Clustering</title>
		<author>
			<persName><forename type="first">Peixiang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
