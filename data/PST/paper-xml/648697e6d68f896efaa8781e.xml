<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantifying the Knowledge in GNNs for Reliable Distillation into MLPs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-09">9 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haitao</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yufei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Quantifying the Knowledge in GNNs for Reliable Distillation into MLPs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-09">9 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.05628v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To bridge the gaps between topology-aware Graph Neural Networks (GNNs) and inference-efficient Multi-Layer Perceptron (MLPs), GLNN (Zhang  et al., 2021)  proposes to distill knowledge from a well-trained teacher GNN into a student MLP. Despite their great progress, comparatively little work has been done to explore the reliability of different knowledge points (nodes) in GNNs, especially their roles played during distillation.</p><p>In this paper, we first quantify the knowledge reliability in GNN by measuring the invariance of their information entropy to noise perturbations, from which we observe that different knowledge points (1) show different distillation speeds (temporally); (2) are differentially distributed in the graph (spatially). To achieve reliable distillation, we propose an effective approach, namely Knowledge-inspired Reliable Distillation (KRD), that models the probability of each node being an informative and reliable knowledge point, based on which we sample a set of additional reliable knowledge points as supervision for training student MLPs. Extensive experiments show that KRD improves over the vanilla MLPs by 12.62% and outperforms its corresponding teacher GNNs by 2.16% averaged over 7 datasets and 3 GNN architectures. Codes are publicly available at: https://github.com/LirongWu/RKD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed the great success of Graph Neural Networks (GNNs) <ref type="bibr" target="#b6">(Hamilton et al., 2017;</ref><ref type="bibr">Wu et al., 2023a;</ref><ref type="bibr" target="#b19">Veli?kovi? et al., 2017;</ref><ref type="bibr" target="#b13">Liu et al., 2020;</ref><ref type="bibr" target="#b27">Wu et al., 2020;</ref><ref type="bibr" target="#b35">Zhou et al., 2020;</ref><ref type="bibr">Wu et al., 2021b;</ref><ref type="bibr">a)</ref> in handling graph-related tasks. Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for such academic-industrial gap is the neighborhoodfetching latency incurred by data dependency in GNNs <ref type="bibr" target="#b9">(Jia et al., 2020;</ref><ref type="bibr" target="#b33">Zhang et al., 2021)</ref>, which makes it hard to deploy for latency-sensitive applications. Conversely, Multi-Layer Perceptrons (MLPs) involve no data dependence between data pairs and infer much faster than GNNs, but their performance is less competitive. Motivated by these complementary strengths and weaknesses, one solution to reduce their gaps is to perform GNN-to-MLP knowledge distillation <ref type="bibr" target="#b30">(Yang et al., 2021;</ref><ref type="bibr" target="#b33">Zhang et al., 2021;</ref><ref type="bibr" target="#b5">Gou et al., 2021)</ref>, which extracts the knowledge from a well-trained teacher GNN and then distills the knowledge into a student MLP.</p><p>Despite the great progress, most previous works have simply treated all knowledge points (nodes) in GNNs as equally important, and few efforts are made to explore the reliability of different knowledge points in GNNs and the diversity of the roles they play in the distillation process. From the motivational experiment in Fig. <ref type="figure" target="#fig_0">1</ref>, we can make two important observations about knowledge points: (1) More is better: the performance of distilled MLPs can be improved as the number of knowledge points N KP increases; and (2) Reliable is better: the performance variances (e.g., standard deviation and best/worst performance gap) of different knowledge combinations are enlarged as N KP decreases. The above two observations suggest that different knowledge points may play different roles in the distillation process and that distilled MLPs can consistently benefit from more reliable knowledge points, while those uninformative and unreliable knowledge points may contribute little to the distillation.</p><p>Present Work. In this paper, we identify a potential underconfidence problem for GNN-to-MLP distillation, i.e., the distilled MLPs may not be able to make predictions as confidently as teacher GNNs. Furthermore, we conduct extensive theoretical and experimental analysis on this problem and find that it is mainly caused by the lack of reliable supervision from teacher GNNs. To provide more supervision for reliable distillation into student MLPs, we propose to quantify the knowledge in GNNs by measuring the invariance of their information entropy to noise perturbations, from which we find that different knowledge points (1) show different distillation speeds (temporally); (2) are differentially distributed in the graph (spatially). Finally, we propose an effective approach, namely Knowledge-inspired Reliable Distillation (KRD), for filtering out unreliable knowledge points and making full use of those with informative knowledge. The proposed KRD framework models the probability of each node being an information-reliable knowledge point, based on which we sample a set of additional reliable knowledge points as supervision for training student MLPs.</p><p>Our main contributions can be summarized as follows:</p><p>? We are the first to identify a potential under-confidence problem for GNN-to-MLP distillation, and more importantly, we described in detail what it represents, how it arises, what impact it has, and how to deal with it.</p><p>? We propose a perturbation invariance-based metric to quantify the reliability of knowledge in GNNs and analyze the roles played by different knowledge nodes temporally and spatially in the distillation process.</p><p>? We propose a Knowledge-inspired Reliable Distillation (KRD) framework based on the quantified GNN knowledge to make full use of those reliable knowledge points as additional supervision for training MLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>GNN-to-GNN Knowledge Distillation. Despite the great progress, most existing GNNs share the de facto design that relies on message passing to aggregate features from neighborhoods, which may be one major source of latency in GNN inference. To address this problem, there are previous works that attempt to distill knowledge from large teacher GNNs to smaller student GNNs, termed as GNN-to-GNN distillation <ref type="bibr" target="#b12">(Lassance et al., 2020;</ref><ref type="bibr">Zhang et al., 2020a;</ref><ref type="bibr" target="#b16">Ren et al., 2021;</ref><ref type="bibr" target="#b10">Joshi et al., 2021;</ref><ref type="bibr">Wu et al., 2022a;</ref><ref type="bibr">b)</ref>. For example, the student model in RDD <ref type="bibr">(Zhang et al., 2020b)</ref> and TinyGNN <ref type="bibr" target="#b28">(Yan et al., 2020</ref>) is a GNN with fewer parameters but not necessarily fewer layers than the teacher GNN. Besides, LSP <ref type="bibr">(Yang et al., 2020b)</ref> transfers the topological structure (rather than feature) knowledge from a pre-trained teacher GNN to a shallower student GNN. In addition, GNN-SD <ref type="bibr" target="#b2">(Chen et al., 2020)</ref> directly distills knowledge across different GNN layers, mainly aiming to solve the over-smoothing problem but with unobvious performance improvement at shallow layers. Moreover, FreeKD <ref type="bibr" target="#b3">(Feng et al., 2022</ref>) studies a free-direction knowledge distillation architecture, with the purpose of dynamically exchanging knowledge between two shallower GNNs. Note that both teacher and student models in the above works are GNNs, making it still suffer from neighborhood-fetching latency.</p><p>GNN-to-MLP Knowledge Distillation. To enjoy the topology awareness of GNNs and inference-efficient of MLPs, the other branch of graph knowledge distillation is to directly distill from teacher GNNs to lightweight student MLPs, termed as GNN-to-MLP distillation. For example, CPF <ref type="bibr" target="#b30">(Yang et al., 2021)</ref> directly improves the performance of student MLPs by adopting deeper/wider network architectures and incorporating label propagation in MLPs, both of which burden the inference latency. Instead, GLNN <ref type="bibr" target="#b33">(Zhang et al., 2021)</ref> distills knowledge from teacher GNNs to vanilla MLPs without other computing-consuming operations; while the performance of their distilled MLPs can be indirectly improved by employing more powerful GNNs, they still cannot match GNN-to-GNN distillation in terms of classification performance. To further improve GLNN, RKD-MLP <ref type="bibr" target="#b0">(Anonymous, 2023)</ref> adopts a meta-policy to filter out unreliable soft labels, but this is essentially a downsampling-style strategy that will further reduce the already limited supervision. In contrast, this paper aims to provide more reliable supervision for training student MLPs, which can be considered as an up-sampling-style strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Notions and Problem Statement. Let G = (A, X) be a graph with the node set V and edge set E, where V is the set of N nodes with features</p><formula xml:id="formula_0">X = [x 1 , x 2 , ? ? ? , x N ] ? R N ?d .</formula><p>The graph structure is denoted by an adjacency matrix A ? [0, 1] N ?N with A i,j = 1 if e i,j ? E and A i,j = 0 if e i,j / ? E. Considering a semi-supervised node classification task where only a subset of node V L with labels Y L are known, we denote the labeled set as D L = (V L , Y L ) and unlabeled set as</p><formula xml:id="formula_1">D U = (V U , Y U ), where V U = V\V L .</formula><p>The node classification aims to learn a mapping ? : V ? Y so that it can be used to infer the ground-truth label y i ? Y U .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Neural Networks (GNNs).</head><p>A general GNN framework consists of two key operations for each node v i : (1) AGGREGATE: aggregating messages from neighborhood N i ; (2) UPDATE: updating node representations. For an L-layer GNN, the formulation of the l-th layer is as</p><formula xml:id="formula_2">m (l) i = AGGREGATE (l) h (l-1) j : vj ? Ni h (l) i = UPDATE (l) h (l-1) i , m (l) i (1)</formula><p>where 1 ? l ? L, h (0) i = x i is the input node feature, and h inference, the vanilla MLPs are used as the student model by default in this paper. For a L-layer MLP, the l-th layer is composed of a linear transformation, an activation function ReLu(?), and a dropout function Dropout(?), as follows</p><formula xml:id="formula_3">z (l) i = Dropout ReLu z (l-1) i W (l-1)<label>(2)</label></formula><p>where z (0) i = x i is the input feature, and {W (l) } L-1 l=0 are weight matrices with the hidden dimension F . In this paper, the network architecture of MLPs, such as the layer number L and layer size F , is set the same as that of teacher GNNs.</p><p>GNN-to-MLP Knowledge Distillation. The knowledge distillation is first introduced in <ref type="bibr" target="#b7">(Hinton et al., 2015)</ref> to mainly handle image data. However, recent works on GNNto-MLP distillation <ref type="bibr" target="#b30">(Yang et al., 2021;</ref><ref type="bibr" target="#b33">Zhang et al., 2021)</ref> extend it to the graph domain by imposing KL-divergence constraint D KL (?, ?) between the label distributions generated by teacher GNNs and student MLPs, as follows</p><formula xml:id="formula_4">LKD = 1 |V| i?V DKL ? z (L) i , ? h (L) i<label>(3)</label></formula><p>where ?(?) = softmax(?), and all nodes (knowledge points) in the set V are indiscriminately used as supervisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>4.1. What Gets in the Way of Better Distillation?</p><p>Potential Under-confident Problem. The GNN-to-MLP distillation can be achieved by directly optimizing the objective function L KD defined in Eq. (3). However, such a straightforward distillation completely ignores the differences between knowledge points in GNNs and may suffer from a potential under-confident problem, i.e., the distilled MLP may fail to make predictions as confidently as teacher GNNs. To illustrate this problem, we report in Fig. <ref type="figure" target="#fig_1">2</ref>(a) the confidences of teacher GCNs and student MLPs for those correct predictions by the UMAP <ref type="bibr" target="#b15">(McInnes et al., 2018)</ref> algorithm on the Cora dataset. It can be seen that there exists a significant distribution shift between the confidence distribution of teacher GCNs and student MLPs, which confirms the existence of the under-confident problem. The direct hazard of such an under-confident problem is that it may push those samples located near the class boundaries into incorrect predictions, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>(a) and Fig. <ref type="figure" target="#fig_2">3(c</ref>), which hinders the performance of student MLPs.</p><p>To go deeper into the under-confident problem and explore what exactly stands in the way of better GNN-to-MLP distillation, we conducted extensive theoretical and experimental analysis and found that one of the main causes could be due to the lack of reliable supervision from teacher GNNs.</p><p>Theoretical Analysis. The main strength of teacher GNNs over student MLPs is their excellent topology-awareness capability, which is mainly enabled by message passing.</p><p>There have been a number of works exploring the roles of message passing in GNNs. For example, <ref type="bibr">(Yang et al., 2020a)</ref> have proved that message passing (architecture design) in GNNs is equivalent to performing Laplacian smoothing (supervision design) on node embeddings in MLPs. In essence, message-passing-based GNNs implicitly take the objective of Dirichlet energy minimization <ref type="bibr" target="#b1">(Belkin &amp; Niyogi, 2001)</ref> as graph-based regularization, which is defined as follows</p><formula xml:id="formula_5">L reg = Tr Y ? ?Y = i j?Ni Y i ? d i - Y j d j 2 2<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">? = I -D -1 2 AD -1 2 is the normalized Laplacian operator, D is the degree matrix with D i,i = d i = j A i,j , and Y = softmax H (L) is the label distribution matrix.</formula><p>Apart from the supervision of cross-entropy on the labeled set, message passing in GNNs implicitly provides a special kind of self-supervision, which imposes regularization constraints on the label distributions between neighboring nodes. We conjecture that it is exactly such additional selfsupervision that enables GNNs to make highly confident predictions. In contrast, student MLPs are trained in a way that cannot capture the fine-grained dependencies between neighboring nodes; instead, they only learn the overall contextual information about their neighborhood from teacher GNNs, resulting in undesirable under-confident predictions. Experimental Analysis. To see why the (distilled) student MLPs tend to make low-confidence predictions, we conducted an in-depth statistical analysis on two types of special samples. (1) The distribution of "False Negative" samples (predicted correctly by GNNs but incorrectly by MLPs) w.r.t the information entropy of teacher's predictions is reported in Fig. <ref type="figure" target="#fig_1">2(b)</ref>, from which we observe that most of the "False Negative" samples are distributed in the region of higher entropy. (2) For those "True Positive" samples (predicted correctly by both GNNs and MLPs), the scatter of confidence and information entropy from student MLPs and teacher GNNs is plotted in Fig. <ref type="figure" target="#fig_1">2(c</ref>), which shows that GNN knowledge with high uncertainty (low reliability) may undermine the capability of student MLPs to make sufficiently confident predictions. Based on these two observations, it is reasonable to hypothesize that one cause of the underconfident problem suffered by student MLPs is be the lack of sufficiently reliable supervision from teacher GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">How to Quantify the Knowledge in GNNs?</head><p>Based on the above experimental and theoretical analysis, a key issue in GNN-to-MLP distillation may be to provide more and reliable supervision for training student MLPs. Next, we first describe how to quantify the reliability of knowledge in GNNs, and then propose how to sample more reliable supervision through a knowledge-inspired manner.</p><p>Knowledge Quantification. Given a graph G = (A, X) and a pre-trained teacher GNN f ? (?, ?), we propose to quantify the reliability of a knowledge point (node) v i ? V in GNNs by measuring the invariance of its information entropy to noise perturbations, which is defined as follows</p><formula xml:id="formula_7">?i = 1 ? 2 E X ? ?N (X,?(?)) H(Y ? i ) -H(Yi) 2 ,</formula><p>where</p><formula xml:id="formula_8">Y ? = f ? (A, X ? ) and Y = f ? (A, X)<label>(5)</label></formula><p>where ? is the variance of Gaussian noise and H(?) denote the information entropy. The smaller the metric ? i is, the higher the reliability of knowledge point v i is. The quantification of GNN knowledge defined in Eq. ( <ref type="formula" target="#formula_8">5</ref>) has the following three strengths: (1) It measures the robustness of knowledge in teacher GNNs to noise perturbations, and thus more truly reflects the reliability of different knowl-edge points, which is very important for reliable distillation.</p><p>(2) The message passing is what makes GNNs special over MLPs, so the key to quantify GNN knowledge is to measure its topology-awareness capability. Compared with nodewise information entropy, Eq. ( <ref type="formula" target="#formula_8">5</ref>) not only reflects the node uncertainty, but also takes into account the contextual information from the neighborhood.</p><p>(3) As will be analyzed next, the knowledge quantified by Eq. ( <ref type="formula" target="#formula_8">5</ref>) shows the roles played by different knowledge points spatially and temporally.</p><p>Spatial Distribution of Knowledge Points. To explore the spatial distribution of different knowledge points in the graph, we first visualize the embeddings of teacher GNNs and student MLPs in Fig. <ref type="figure" target="#fig_2">3</ref>(a) and Fig. <ref type="figure" target="#fig_2">3</ref>(c), and then we mark the knowledge points with the reliability ranked in the top 20% and bottom 10% as green and orange in Fig. <ref type="figure" target="#fig_2">3</ref>  Temporal Distribution of Knowledge Points. To see the distillation speed of different knowledge points, we explore which knowledge points the student MLPs will be fitted to first during the training process. We considered those knowledge points that are correctly predicted by student MLPs and ranked in the top 50% of reliability, among which we calculate the percentage of points with the top 20% of reliability in Fig. <ref type="figure" target="#fig_3">4</ref>. It can be seen that student MLPs will quickly fit to those highly reliable knowledge points first as the training proceeds, and then gradually learn from those relatively less reliable knowledge points. This indicates that different knowledge points play different roles in the distillation process, which inspires us to sample some reliable knowledge points from teacher GNNs in a dynamic manner to provide more additional supervision for training MLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Knowledge-inspired Reliable Distillation</head><p>In this subsection, we first model the probability of each node being an informative and reliable knowledge point based on the knowledge quantification defined by Eq. ( <ref type="formula" target="#formula_8">5</ref>). Next, we propose a knowledge-based sampling strategy to make full use of those reliable knowledge points as additional supervision for more reliable distillation into MLPs. A high-level overview of the proposed Knowledge-inspired Reliable Distillation (KRD) framework is shown in Fig. <ref type="figure" target="#fig_4">5</ref>.  Sampling Probability Modeling. We aim to estimate the sampling probability of a knowledge point based on its quantified reliability. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, we plot the histograms of "True Positive" sample density w.r.t the reliability metric ? on two datasets (see Appendix D for more results), where the density has been min/max normalized. We model the sampling probability s i of node v i based on the metric ? i by a learnable power distribution (with power ?), as follows:</p><formula xml:id="formula_9">p(s i | ? i , ?) = 1 -( ? i ? M ) ? , ?v i ? V<label>(6)</label></formula><p>where ? M = argmax j ? j . When the ground-truth labels are available, an optimal power ? opt can be directly fitted from histograms. However, the ground-truth labels are often unknown in practice, so we propose to combine the student MLPs g ? (t) (?) with the pre-trained teacher GNNs f ?pre (?) to model p ? (t) | f ?pre (A, X), g ? (t) (A, X) at t-th epoch, which can be implementated by the following four steps:</p><p>(1) initializing the power ? (0) = 1.0;</p><p>(2) constructing a histogram of sample density (predicted to be the same by both teacher GNNs and student MLPs) w.r.t the knowledge reliability metric ?;</p><p>(3) inferring a new power ?</p><formula xml:id="formula_10">(t)</formula><p>new by fitting the histogram; (4) updating power ? (t-1) in a dynamic momentum manner, which can be formulated as follows</p><formula xml:id="formula_11">? (t) ? ?? (t-1) + (1 -?) * ? (t) new (7)</formula><p>where ? is the momentum updating rate. We provide the fitted curves with fixed and learnable powers in Fig. <ref type="figure" target="#fig_6">6</ref>, which shows that the fitted distributions of learnable powers are more in line with the histogram. Moreover, we also include the results of fitting by Gaussian and exponential distributions as comparisons, but it shows that they do not work better. A quantitative comparison of different distribution fitting schemes has been provided in Table . 2, and the fitted results on more datasets are available in Appendix D.  Knowledge-based Sampling. Next, we describe how to sample a set of reliable knowledge points as additional supervision for training student MLPs. Given any target node v i , we first sample some highly reliable knowledge points v j ? N i from its neighborhood according to the sampling probability p(s j | ? i , ? (t) ). Then, we take sampled knowledge points as multiple teachers and distill their knowledge into student MLPs as additional supervision through a multiteacher distillation objective, which is defined as follows</p><formula xml:id="formula_12">LKRD = E i E j?N i j?p(s j |? i ,? (t) ) DKL ?(z (L) j /? ), ?(h (L) i /? ) (8)</formula><p>where ? is the distillation temperature coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training Strategy</head><p>The pseudo-code of the KRD framework is summarized in Algorithm 1. To achieve GNN-to-MLP knowledge distillation, we first pre-train the teacher GNNs with the classification loss</p><formula xml:id="formula_13">L label = 1 |V L | i?V L CE y i , ?(h (L) i )</formula><p>, where CE(?) denotes the cross-entropy loss. Finally, the total objective function to distill reliable knowledge from the teacher GNNs into the student MLPs is defined as follows</p><formula xml:id="formula_14">L total = ? |V L | i?V L H y i , ?(z (L) i ) + 1-? L KD +L KRD</formula><p>where ? is the weight to balance the influence of the classification loss and knowledge distillation losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Time Complexity Analysis</head><p>It is noteworthy that the main computational burden introduced in this paper comes from additional reliable supervision as defined in Eq. ( <ref type="formula">8</ref>). However, we sample reliable Estimate the sampling probability by Eq. ( <ref type="formula" target="#formula_9">6</ref>); 7:</p><p>Sample reliable knowledge points and calculate the multi-teacher distillation loss L KRD by Eq. ( <ref type="formula">8</ref>); 8:</p><p>Calculate the total loss L total and update the parameters of student MLPs {W l } L-1 l=0 by back propagation. 9:</p><p>Momentum updating the power ? (t) by Eq. ( <ref type="formula">7</ref>). 10: end for 11: Predicted labels y i ? Y U for those unlabeled nodes V U . 12: return Predicted labels Y U and Parameters {W l } L-1 l=0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate KRD on seven real-world datasets by answering the following six questions. Q1: How effective is KRD in the transductive and inductive settings? Is KRD applicable to different teacher GNNs? Q2: How does KRD compare to other leading baselines on graph knowledge distillation? Q3: What happens if we model the sampling probability using other distribution functions? Q4: How does KRD perform by applying other heuristic knowledge sampling approach? Q5: Can KRD improve the predictive confidence of distilled MLPs? Q6: How do the two key hyperparameters ? and ? influence the performance of KRD?</p><p>Dataset. The effectiveness of the KRD framework is evaluated on seven real-world datasets, including Cora <ref type="bibr" target="#b17">(Sen et al., 2008)</ref>, Citeseer <ref type="bibr" target="#b4">(Giles et al., 1998</ref><ref type="bibr">), Pubmed (McCallum et al., 2000)</ref>, Coauthor-CS, Coauthor-Physics, Amazon-Photo <ref type="bibr" target="#b18">(Shchur et al., 2018)</ref>, and ogbn-arxiv <ref type="bibr" target="#b8">(Hu et al., 2020)</ref>.</p><p>A statistical overview of datasets is placed in Appendix A.</p><p>Besides, each set of experiments is run five times with different random seeds, and the average accuracy and standard deviation are reported. Due to space limitations, we defer the implementation details and hyperparameter settings for each dataset to Appendix C and supplementary materials.</p><p>Baselines. Three basic components in knowledge distillation are (1) teacher model, (2) student model, and (3) distillation objective. As a model-agnostic framework, KRD can be combined with any teacher GNN architecture. In this paper, we consider three types of teacher GNNs, including GCN <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016)</ref>, GraphSAGE <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>, and GAT <ref type="bibr" target="#b19">(Veli?kovi? et al., 2017)</ref>. Besides, we adopt pure MLPs (with the same layer number L and size F as teacher GNNs) as the student model for a fair comparison. The focus of this paper is to provide more reliable selfsupervision for GNN-to-MLP distillation. Thus, we only take GLNN <ref type="bibr" target="#b33">(Zhang et al., 2021)</ref> as an important benchmark to demonstrate the necessity and effectiveness of additional supervision. Besides, we also compare KRD with some stateof-the-art graph distillation baselines in Table . 2, including CPF <ref type="bibr" target="#b30">(Yang et al., 2021)</ref>, RKD-MLP <ref type="bibr" target="#b0">(Anonymous, 2023)</ref>, FF-G2M <ref type="bibr">(Wu et al., 2023b)</ref>, RDD <ref type="bibr">(Zhang et al., 2020b)</ref>, TinyGNN <ref type="bibr" target="#b28">(Yan et al., 2020)</ref>, LSP <ref type="bibr">(Yang et al., 2020b)</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Classification Performance Comparison (Q1)</head><p>The reliable knowledge of three teahcer GNNs is distilled into student MLPs in the transductive and inductive settings.</p><p>The experimental results on seven datasets are reported in Table . 1, from which we can make three observations:</p><p>(1) Compared to the vanilla MLPs and intuitive KD baseline -GLNN, KRD performs significantly better than them in all cases, regardless of the datasets, teacher GNNs and evaluation settings. For example, KRD outperforms GLNN by 2.03% (GCN), 1.92% (SAGE), and 2.03% (GAT) averaged over seven datasets in the transductive setting, respectively. The superior performance of KRD demonstrates the effectiveness of providing more reliable self-supervision for GNN-to-MLP distillation.</p><p>(2) The performance gain of KRD over GLNN is higher on the large-scale ogbn-arxiv dataset. We speculate that this is because the reliability of different knowledge points probably differ more in largescale datasets, making those reliable knowledge points play a more important role.</p><p>(3) It can be seen that KRD works much better in the transductive setting than in the inductive one, since there are more node features that can be used for training in the transductive setting, providing more reliable knowledge points to serve as additional self-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparision with Representative Baselines (Q2)</head><p>To answer Q2, we compare KRD with several representative graph knowledge distillation baselines, including both GNNto-GNN and GNN-to-MLP distillation. As can be seen from the results reported in Table <ref type="table" target="#tab_3">2</ref>, KRD outperforms all other GNN-to-MLP baselines by a wide margin. More importantly, we are the first work to demonstrate the promising potential of distilled MLPs to surpass distilled GNNs. Even when compared with those state-of-the-art GNN-to-GNN distillation methods, KRD still shows competitive performance, ranking in the top two on 6 out of 7 datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation on Distribution Fitting Function (Q3)</head><p>To evaluate the effectiveness of different distribution fitting functions and the momentum updating defined in Eq. ( <ref type="formula">7</ref>), we compare the learnable power distribution defined in Eq. ( <ref type="formula" target="#formula_9">6</ref>) with the other four schemes: (A) exponential distribution</p><formula xml:id="formula_15">p(s i | ? i , ?) = ? exp -?? ? i ? M with learnable rate ?; (B) Gaussian distribution p(s i | ? i , ?) = N (0, ?)</formula><p>with learnable variance ?; (C) power distribution with fixed power ? = 1; and (D) power distribution with fixed power ? = 3.</p><p>From the results reported in Table . 3, it can be seen that ( <ref type="formula">1</ref>) when modeling the sampling probability with power distribution, the learnable power is consistently better than the fixed power on all datasets, and (2) the exponential, Gaussian and power distributions perform differently on different datasets, but the power distribution can achieve better overall performance than the other two distributions.  To explore whether providing additional reliable supervision can improve the predictive confidence of distilled MLPs, we compare the confidence distribution of KRD with that of GLNN in Fig. <ref type="figure" target="#fig_8">7</ref> on four datasets. It can be seen that the predictive confidence of student MLPs in GLNN (optimized with only the distillation term defined by Eq. ( <ref type="formula" target="#formula_4">3</ref>) is indeed not very high. Instead, KRD provides additional reliable self-supervision defined in Eq. ( <ref type="formula">8</ref>), which helps to greatly improve the predictive confidence of student MLPs. 5.6. Evaluation on Hyperparameter Sensitivity (Q6)</p><p>We provide sensitivity analysis for two hyperparameters, loss weights ? and momentum updating rate ? in Fig. <ref type="figure" target="#fig_9">8</ref>(a) and <ref type="bibr">Fig. 8(b)</ref>, from which we observe that (1) setting the loss weight ? too large weakens the contribution of the distillation term, leading to poor performance; (2) too large or small ? are both detrimental to modeling sampling probability and extracting informative knowledge. In practice, ? = 0.9, 0.99 often yields pretty good performance. In practice, we can determine ? and ? by selecting the model with the highest accuracy on the validation set by the grid search. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we identified a potential under-confidence problem for GNN-to-MLP distillation, and more impor- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Statistics</head><p>Seven publicly available real-world graph datasets have been used to evaluate the proposed KRD framework. An overview summary of the statistical characteristics of these datasets is given in Table . A1. For the three small-scale datasets, namely Cora, Citeseer, and Pubmed, we follow the data splitting strategy in <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016)</ref>. For the three largescale datasets, including Coauthor-CS, Coauthor-Physics, and Amazon-Photo, we follow <ref type="bibr" target="#b33">(Zhang et al., 2021;</ref><ref type="bibr" target="#b30">Yang et al., 2021)</ref> to randomly split the data into train/val/test sets, and each random seed corresponds to a different data splitting. For the ogbn-arxiv dataset, we use the public data splits provided by the authors <ref type="bibr" target="#b8">(Hu et al., 2020)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The following hyperparameters are set the same for all datasets: Epoch E = 500, noise variance ? = 1.0, and momentum rate ? = 0.99 (0.9 for ogb-arxiv).</p><p>The other dataset-specific hyperparameters are determined by an AutoML toolkit NNI with the hyperparameter search spaces as: hidden dimension F = {128, 256, 512, 1024, 2048}, layer number L = {2, 3}, distillation temperature ? = {0.8, 0.9, 1.0, 1.1, 1.2}, loss weight ? = {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}, learning rate lr = {0.001, 0.005, 0.01}, and weight decay decay = {0.0, 0.0005, 0.001}. For a fairer comparison, the model with the highest validation accuracy is selected for testing. Besides, the best hyperparameter choices of each setting are available in the supplementary. Moreover, the experiments on both baselines and our approach are implemented based on the standard implementation in the DGL library <ref type="bibr" target="#b20">(Wang et al., 2019)</ref> using the PyTorch 1.6.0 with Intel(R) Xeon(R) Gold 6240R @ 2.40GHz CPU and NVIDIA V100 GPU.</p><p>Transductive vs. Inductive. We evaluate our model under two evaluation settings: transductive and inductive. Their main difference is whether to use the test data for training. Specifically, we partition node features and labels into three disjoint sets, i.e., X = X L ? X U obs ? X U ind , and Y = Y L ? Y U obs ?Y U ind . Concretely, the input and output of two settings are: (1) Transductive: training on X and Y L and testing on (X U , Y U ). ( <ref type="formula" target="#formula_3">2</ref>) Inductive: training on X L ? X U obs and Y L and testing on (X U ind , Y U ind ) <ref type="bibr" target="#b0">(Anonymous, 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Results on Spatial Distribution</head><p>The embeddings of teacher GNNs and student MLPs on the Cora dataset are visualized in Fig. <ref type="figure" target="#fig_0">A1</ref>(a)(c). Then, we mark the knowledge points with the reliability ranked in the top 20% and bottom 10% as green and orange in Fig. <ref type="figure" target="#fig_0">A1</ref>(b)(d), respectively. It can be seen that most reliable knowledge points are distributed around the class centers, while those unreliable ones are distributed at the class boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Results on Fitted Distributions</head><p>We report histograms of "True Positive" sample density w.r.t the reliability metric ? as well as the fitted distributions in Fig. <ref type="figure" target="#fig_1">A2</ref>. It can be seen that the fitted distributions of the sampling probability closely matches the true histograms. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Mean, standard deviation, and minimum/maximum classification accuracy of student MLPs trained with different combinations of (randomly sampled) GNN knowledge points on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>iFigure 2 .</head><label>2</label><figDesc>Figure 2. (a) Histograms of the confidence distributions of teacher GCNs and students MLP for those correct predictions on the Cora dataset. (b) Distribution of "False Negative" samples w.r.t the information entropy of teacher's predictions on the Cora dataset. (c) Scatter curve of confidence (student MLP) and information entropy (teacher GCN) for those "True Positive" samples on the Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a)(c) Visualizations of the embeddings of teacher GNNs and student MLPs for two classes on Cora. (b)(d) Spatial distribution of knowledge points with the reliability ranked in the top 20% and bottom 10%, which are marked in green and orange, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Percentage of highly reliable knowledge points on Cora to show the distillation speeds of different knowledge points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. A high-level overview of the proposed KRD framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Histograms of "True Positive" sample density w.r.t the reliability metric ?, as well as five distribution fitting schemes for modeling the sampling probability on the Cora and Citeseer datasets, where the density has been min/max normalized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>knowledge points in the neighborhood instead of the entire set of nodes V, which has reduced the time complexity from O(|V 2 |F ) to less than O(|E|F ). The training time complexity of the KRD framework mainly comes from two parts: (1) GNN training O(|V|dF + |E|F ) and (2) knowledge distillation O(|E|F ), where d and F are the dimensions of input and hidden spaces. The total time complexity O(|V|dF +|E|F ) is linear w.r.t the number of nodes |V| and edges |E|, which is in the same order as GCNs and GLNN. Algorithm 1 Algorithm for KRD framework (Transductive) Input: Graph G = (V, E), Node Features: X, # Epoch: E. Output: Predicted labels Y U , MLP parameters {W l } L-1 l=0 . 1: Randomly initialize the parameters of GNNs and MLPs. 2: Pre-train the teacher GNNs until convergence by L label . 3: Quantify the reliability of knowledge points by Eq. (5). 4: for t ? {1, 2, ? ? ? , E + 1} do 5:Calculate the node representations from teacher GNNs and student MLPs by Eq. (1) and Eq. (2); 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Confidence distribution of the distilled MLPs in GLNN and KRD on four datasets, where GCN is adopted as the teacher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Hyperparameter sensitivity analysis on ? and ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A1 .Figure A2 .</head><label>A1A2</label><figDesc>Figure A1. (a)(c) Visualizations of the embeddings of teacher GNNs and student MLPs for two classes on Cora. (b)(d) Spatial distribution of knowledge points with the reliability ranked in the top 20% and bottom 10%, which are marked in green and orange, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Classificatiom accuracy ? std (%) on seven real-world datasets in both transductive and inductive settings, where three different GNN architectures (GCN, GraphSAGE, and GAT) have been considered as the teacher models. The best metrics are marked by bold. ?0.88 71.52 ?0.54 80.32 ?0.38 91.34 ?1.46 92.00 ?0.57 92.82 ?0.93 68.30 ?0.19 -KRD (ours) 84.60 ?0.76 73.68 ?0.68 81.60 ?0.33 92.12 ?1.50 93.93 ?0.40 94.18 ?0.58 ?1.04 70.78 ?0.60 79.88 ?0.85 90.06 ?1.38 90.90 ?0.37 91.97 ?0.58 71.08 ?0.19 -GLNN 81.78 ?0.75 70.96 ?0.86 80.48 ?0.47 91.22 ?1.45 92.44 ?0.41 92.70 ?0.56 68.56 ?0.22 -KRD (ours) 84.12 ?0.39 73.06 ?0.59 82.02 ?0.56 92.13 ?1.48 94.35 ?0.29 94.19 ?0.50 ?1.26 60.16 ?0.87 73.26 ?0.83 79.02 ?1.42 87.90 ?0.58 89.10 ?0.90 54.46 ?0.52 -GCN -79.30 ?0.49 71.46 ?0.36 78.10 ?0.51 89.32 ?1.63 90.07 ?0.60 92.05 ?0.78 70.88 ?0.35 -GLNN 71.24 ?0.55 70.76 ?0.30 80.16 ?0.73 89.92 ?1.34 92.08 ?0.98 92.89 ?0.88 60.92 ?0.31 -KRD (ours) 73.78 ?0.55 71.80 ?0.41 81.48 ?0.29 90.37 ?1.79 93.15 ?0.43 93.86 ?0.55 ?0.47 70.24 ?0.62 79.40 ?0.48 89.76 ?1.51 89.96 ?0.56 91.79 ?0.69 71.13 ?0.32 -GLNN 71.82 ?0.35 70.26 ?0.71 80.46 ?0.34 89.94 ?1.70 92.06 ?0.69 92.97 ?0.94 60.46 ?0.26 -KRD (ours) 73.48 ?0.43 70.94 ?0.49 81.36 ?0.51 90.37 ?1.79 92.96 ?0.44 93.91 ?0.63 ?0.63 69.58 ?0.43 79.02 ?0.43 90.54 ?1.73 90.50 ?0.97 91.99 ?1.08 70.65 ?0.23 -GLNN 71.10 ?0.86 70.20 ?0.69 81.28 ?0.58 90.57 ?1.59 92.15 ?0.75 93.17 ?0.92 60.38 ?0.30 -KRD (ours) 72.48 ?0.53 70.64 ?0.38 82.00 ?0.65 91.10 ?1.69 93.23 ?0.48 94.02 ?0.73</figDesc><table><row><cell>Teacher</cell><cell>Student</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Photo</cell><cell>CS</cell><cell cols="3">Physics ogbn-arxiv Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Transductive Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLPs</cell><cell>-</cell><cell cols="6">59.58 ?0.97 60.32 ?0.61 73.40 ?0.68 78.65 ?1.68 87.82 ?0.64 88.81 ?1.08</cell><cell>54.63 ?0.84</cell><cell>-</cell></row><row><cell></cell><cell>-</cell><cell cols="6">81.70 ?0.96 71.64 ?0.34 79.48 ?0.21 90.63 ?1.53 90.00 ?0.58 92.45 ?0.53</cell><cell>71.20 ?0.17</cell><cell>-</cell></row><row><cell>GCN</cell><cell cols="7">GLNN KRD (ours) 84.42 ?0.57 74.86 ?0.58 81.98 ?0.41 92.21 ?1.44 94.08 ?0.34 94.30 ?0.46 82.20 ?0.73 71.72 ?0.30 80.16 ?0.20 91.42 ?1.61 92.22 ?0.72 93.11 ?0.39</cell><cell>67.76 ?0.23 70.92 ?0.21</cell><cell>--</cell></row><row><cell></cell><cell>Improv.</cell><cell>2.22</cell><cell>3.14</cell><cell>1.82</cell><cell>0.79</cell><cell>1.86</cell><cell>1.19</cell><cell>3.16</cell><cell>2.03</cell></row><row><cell></cell><cell>-</cell><cell cols="6">82.02 ?0.94 71.76 ?0.49 79.36 ?0.45 90.56 ?1.69 89.29 ?0.77 91.97 ?0.91</cell><cell>71.06 ?0.27</cell><cell>-</cell></row><row><cell>GraphSAGE</cell><cell>GLNN</cell><cell cols="7">81.86 71.50 ?0.25</cell><cell>-</cell></row><row><cell></cell><cell>Improv.</cell><cell>2.74</cell><cell>2.16</cell><cell>1.28</cell><cell>0.78</cell><cell>1.93</cell><cell>1.36</cell><cell>3.20</cell><cell>1.92</cell></row><row><cell>GAT</cell><cell>-</cell><cell cols="7">81.66 71.45 ?0.26</cell><cell>-</cell></row><row><cell></cell><cell>Improv.</cell><cell>2.34</cell><cell>2.10</cell><cell>1.54</cell><cell>0.91</cell><cell>1.91</cell><cell>1.49</cell><cell>2.89</cell><cell>1.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Inductive Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLPs</cell><cell>-</cell><cell cols="7">59.20 62.85 ?0.32</cell><cell>-</cell></row><row><cell></cell><cell>Improv.</cell><cell>2.54</cell><cell>1.04</cell><cell>1.32</cell><cell>0.45</cell><cell>1.07</cell><cell>0.97</cell><cell>2.93</cell><cell>1.47</cell></row><row><cell>GraphSAGE</cell><cell>-</cell><cell cols="7">79.56 62.56 ?0.33</cell><cell>-</cell></row><row><cell></cell><cell>Improv.</cell><cell>1.66</cell><cell>0.68</cell><cell>0.90</cell><cell>0.43</cell><cell>0.90</cell><cell>0.94</cell><cell>2.10</cell><cell>1.09</cell></row><row><cell>GAT</cell><cell>-</cell><cell cols="7">79.96 62.16 ?0.24</cell><cell>-</cell></row><row><cell></cell><cell>Improv.</cell><cell>1.38</cell><cell>0.44</cell><cell>0.72</cell><cell>0.53</cell><cell>1.08</cell><cell>0.85</cell><cell>1.78</cell><cell>0.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison with leading graph distillation algorithms, where bold and underline denote the best and second metrics. The experiments are conducted by adopting GCN as the teacher in the transductive setting (same forTable. 3, Table. 4, Fig. 7, and Fig. 8). ?0.97 60.32 ?0.61 73.40 ?0.68 78.65 ?1.68 87.82 ?0.64 88.81 ?1.08 54.63 ?0.84 12.0 Vanilla GCNs 81.70 ?0.96 71.64 ?0.34 79.48 ?0.21 90.63 ?1.53 90.00 ?0.58 92.45 ?0.53</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Photo</cell><cell>CS</cell><cell cols="2">Physics ogbn-arxiv Avg. Rank</cell></row><row><cell>Vanilla</cell><cell>MLPs</cell><cell cols="7">59.58 71.20 ?0.17</cell><cell>10.1</cell></row><row><cell></cell><cell>LSP</cell><cell cols="6">82.70 ?0.43 72.68 ?0.62 80.86 ?0.50 91.74 ?1.42 92.56 ?0.45 92.85 ?0.46</cell><cell>71.57 ?0.25</cell><cell>7.4</cell></row><row><cell></cell><cell>GNN-SD</cell><cell cols="6">82.54 ?0.36 72.34 ?0.55 80.52 ?0.37 91.83 ?1.58 91.92 ?0.51 93.22 ?0.66</cell><cell>70.90 ?0.23</cell><cell>8.3</cell></row><row><cell>GNN-to-GNN</cell><cell>TinyGNN</cell><cell cols="6">83.10 ?0.53 73.24 ?0.72 81.20 ?0.44 92.03 ?1.49 93.78 ?0.38 93.70 ?0.56</cell><cell>72.18 ?0.27</cell><cell>4.7</cell></row><row><cell></cell><cell>RDD</cell><cell cols="6">83.68 ?0.40 73.64 ?0.50 81.74 ?0.44 92.18 ?1.45 94.20 ?0.48 94.14 ?0.39</cell><cell>72.34 ?0.17</cell><cell>2.1</cell></row><row><cell></cell><cell>FreeKD</cell><cell cols="6">83.84 ?0.47 73.92 ?0.47 81.48 ?0.38 92.38 ?1.54 93.65 ?0.43 93.87 ?0.48</cell><cell>72.50 ?0.29</cell><cell>2.9</cell></row><row><cell></cell><cell>GLNN</cell><cell cols="6">82.20 ?0.73 71.72 ?0.30 80.16 ?0.20 91.42 ?1.61 92.22 ?0.72 93.11 ?0.39</cell><cell>67.76 ?0.23</cell><cell>9.7</cell></row><row><cell>GNN-to-MLP</cell><cell>CPF RKD-MLP</cell><cell cols="6">83.56 ?0.48 72.98 ?0.60 81.54 ?0.47 91.70 ?1.50 93.42 ?0.48 93.47 ?0.41 82.68 ?0.45 73.42 ?0.45 81.32 ?0.32 91.28 ?1.48 93.16 ?0.64 93.26 ?0.37</cell><cell>69.05 ?0.18 69.87 ?0.25</cell><cell>6.4 7.3</cell></row><row><cell></cell><cell>FF-G2M</cell><cell cols="6">84.06 ?0.43 73.85 ?0.51 81.62 ?0.37 91.84 ?1.42 93.35 ?0.55 93.59 ?0.43</cell><cell>69.64 ?0.26</cell><cell>4.9</cell></row><row><cell></cell><cell>KRD (ours)</cell><cell cols="6">84.42 ?0.57 74.86 ?0.58 81.98 ?0.41 92.21 ?1.44 94.08 ?0.34 94.30 ?0.46</cell><cell>70.92 ?0.21</cell><cell>2.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison of different distribution fitting functions and the momentum updating of Eq. (7), where bold and underline denote the best and second metrics on each dataset.</figDesc><table><row><cell>Methods</cell><cell cols="4">Cora Citeseer Pubmed Photo</cell><cell>CS</cell><cell>Physics</cell></row><row><cell>Exponential</cell><cell>83.30</cell><cell>73.84</cell><cell>81.10</cell><cell>92.12</cell><cell>93.80</cell><cell>93.63</cell></row><row><cell>Gaussian</cell><cell>84.12</cell><cell>74.52</cell><cell>81.56</cell><cell>92.10</cell><cell>94.15</cell><cell>94.08</cell></row><row><cell cols="2">Power (fixed ?=1) 83.84</cell><cell>74.18</cell><cell>81.44</cell><cell>92.04</cell><cell>93.93</cell><cell>93.93</cell></row><row><cell cols="2">Power (fixed ?=3) 83.54</cell><cell>74.32</cell><cell>81.34</cell><cell>91.95</cell><cell>94.01</cell><cell>93.75</cell></row><row><cell>Power (learnable)</cell><cell>84.42</cell><cell>74.86</cell><cell>81.98</cell><cell>92.21</cell><cell>94.08</cell><cell>94.30</cell></row><row><cell cols="7">5.4. Evaluation on Knowledge Sampling Strategy (Q4)</cell></row><row><cell cols="7">To explore how different sampling strategies influence the</cell></row><row><cell cols="7">performance of distillation, we compare our knowledge-</cell></row><row><cell cols="7">inspired sampling with other three schemes: (A) Non-</cell></row><row><cell cols="7">sampling: directly takes all nodes in the neighborhood as</cell></row><row><cell cols="7">additional supervision and distills their knowledge into the</cell></row><row><cell cols="7">student MLPs; (B) Random Sampling: randomly sampling</cell></row><row><cell cols="7">knowledge points with 50% probability in the neighborhood</cell></row><row><cell cols="7">for distillation; (C) Entropy-based Sampling: performing</cell></row><row><cell cols="7">min/max normalization on the information entropy of each</cell></row><row><cell cols="7">knowledge point to [0-1], and then sampling by taking en-</cell></row><row><cell cols="7">tropy as sampling probability. Besides, we also include the</cell></row><row><cell cols="7">performance of vanilla GCN and GLNN as a comparison.</cell></row><row><cell cols="7">We can observe from Table. 3 that (1) Both non-sampling</cell></row><row><cell cols="7">and random sampling help to significantly improve the per-</cell></row><row><cell cols="7">formance of GLNN, again demonstrating the importance of</cell></row><row><cell cols="7">providing additional supervision for training student MLPs.</cell></row><row><cell cols="7">(2) Entropy-and knowledge-based sampling performs much</cell></row><row><cell cols="7">better than non-sampling and random sampling, suggest-</cell></row><row><cell cols="7">ing that different knowledge plays different roles during</cell></row><row><cell cols="7">distillation. (3) Compared with entropy-based sampling,</cell></row><row><cell cols="7">knowledge-based sampling fully takes into account the con-</cell></row><row><cell cols="7">textual information of the neighborhood as explained in</cell></row><row><cell cols="7">Sec. 4.2, and thus shows better overall performance.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison of different sampling strategies, where the best/second metrics are marked in bold and underline.</figDesc><table><row><cell>Methods</cell><cell cols="4">Cora Citeseer Pubmed Photo</cell><cell>CS</cell><cell>Physics</cell></row><row><cell>Vanilla GCN</cell><cell>81.70</cell><cell>71.64</cell><cell>79.48</cell><cell>90.63</cell><cell>90.00</cell><cell>92.45</cell></row><row><cell>GLNN</cell><cell>82.54</cell><cell>71.92</cell><cell>80.16</cell><cell>90.48</cell><cell>91.48</cell><cell>92.81</cell></row><row><cell>Non-sampling</cell><cell>83.26</cell><cell>73.58</cell><cell>80.74</cell><cell>91.45</cell><cell>93.04</cell><cell>93.42</cell></row><row><cell>Random</cell><cell>82.42</cell><cell>73.10</cell><cell>81.08</cell><cell>91.28</cell><cell>92.57</cell><cell>93.74</cell></row><row><cell>Entropy-based</cell><cell>83.64</cell><cell>73.74</cell><cell>81.32</cell><cell>91.58</cell><cell>93.35</cell><cell>93.63</cell></row><row><cell cols="2">Knowledge-based 84.42</cell><cell>74.86</cell><cell>81.98</cell><cell>92.21</cell><cell>94.08</cell><cell>94.30</cell></row><row><cell cols="7">5.5. Evaluation on Confidence Distribution (Q5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A1 .</head><label>A1</label><figDesc>Statistical information of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Cora Citeseer Pubmed Photo</cell><cell>CS</cell><cell cols="2">Physics ogbn-arxiv</cell></row><row><cell># Nodes</cell><cell>2708</cell><cell>3327</cell><cell>19717</cell><cell>7650</cell><cell>18333</cell><cell>34493</cell><cell>169343</cell></row><row><cell># Edges</cell><cell>5278</cell><cell>4614</cell><cell>44324</cell><cell cols="2">119081 81894</cell><cell>247962</cell><cell>1166243</cell></row><row><cell cols="2"># Features 1433</cell><cell>3703</cell><cell>500</cell><cell>745</cell><cell>6805</cell><cell>8415</cell><cell>128</cell></row><row><cell># Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>8</cell><cell>15</cell><cell>5</cell><cell>40</cell></row><row><cell cols="2">Label Rate 5.2%</cell><cell>3.6%</cell><cell>0.3%</cell><cell>2.1%</cell><cell>1.6%</cell><cell>0.3%</cell><cell>53.7%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China. Correspondence to: Stan Z. Li &lt;stan.zq.li@westlake.edu.cn&gt;, Lirong Wu &lt;wulirong@westlake.edu.cn&gt;.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Double wins: Boosting accuracy and efficiency of graph neural networks by reliable knowledge distillation</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">Submitted to The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>id=NGIFt6BNvLe. under review</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nips</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On self-distilling graph neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02255</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freekd: Freedirection knowledge distillation for graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An automatic citation indexing system</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><surname>Citeseer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third ACM conference on Digital libraries</title>
		<meeting>the third ACM conference on Digital libraries</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1789" to="1819" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Redundancy-free computation for graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On representation knowledge distillation for graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Foo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.04964</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep geometric knowledge distillation with graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bontonou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Hacene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8484" to="8488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName><surname>Umap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<title level="m">Uniform manifold approximation and projection for dimension reduction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-task selfdistillation for graph-based semi-supervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01174</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Graphmixup: Improving class-imbalanced node classification on graphs by self-supervised context prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11133</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Selfsupervised on graphs: Contrastive, generative, or predictive</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07342</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge distillation improves graph structure augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Teaching yourself: Graph self-distillation on neighborhood for node classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02097</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond homophily and homogeneity assumption: Relation-based frequency adaptive graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Extracting low-/high-frequency knowledge from graph neural networks and injecting it into mlps: An effective gnn-tomlp distillation framework</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning efficient graph neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><surname>Tinygnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1848" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abdelzaher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13663</idno>
		<title level="m">Revisiting over-smoothing in deep gcns</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extract the knowledge of graph neural networks and go beyond it: An effective knowledge distillation framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1227" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distilling knowledge from graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7074" to="7083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Iterative graph self-distillation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12609</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph-less neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08727</idno>
	</analytic>
	<monogr>
		<title level="m">Teaching old mlps new tricks via distillation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reliable data distillation on graph convolutional network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2020 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1399" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
