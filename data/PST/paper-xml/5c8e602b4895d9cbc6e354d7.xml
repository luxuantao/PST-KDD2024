<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Decompose and Disentangle Representations for Video Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
							<email>bingbin@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
							<email>dahuang@cs.stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<email>feifeili@cs.stanford.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<email>jniebles@cs.stanford.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Decompose and Disentangle Representations for Video Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3F9D44BCE9669C463102A556DFA6979D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our goal is to predict future video frames given a sequence of input frames. Despite large amounts of video data, this remains a challenging task because of the highdimensionality of video frames. We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the high-dimensional video that we aim to predict into components, and (ii) disentangle each component to have low-dimensional temporal dynamics that are easier to predict. Crucially, with an appropriately specified generative model of video frames, our DDPAE is able to learn both the latent decomposition and disentanglement without explicit supervision. For the Moving MNIST dataset, we show that DDPAE is able to recover the underlying components (individual digits) and disentanglement (appearance and location) as we intuitively would do. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset involving complex interactions between multiple objects to predict the video frame directly from the pixels and recover physical states without explicit supervision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our goal is to build intelligent systems that are capable of visually predicting and forecasting what will happen in video sequences. Visual prediction is a core problem in computer vision that has been studied in several contexts, including activity prediction and early recognition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>, human pose and trajectory forecasting <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, and future frame prediction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>. In particular, the ability to visually hallucinate future frames has enabled applications in robotics <ref type="bibr" target="#b7">[8]</ref> and healthcare <ref type="bibr" target="#b25">[26]</ref>. However, despite the availability of a large amount of video data, visual frame prediction remains a challenging task because of the high-dimensionality of video frames.</p><p>Our key insight into this high-dimensional, continuous sequence prediction problem is to decompose it into sub-problems that can be more easily predicted. Consider the example of predicting digit movements of Moving MNIST in Figure <ref type="figure" target="#fig_0">1</ref>: the transformation that converts an entire frame containing two digits into the next frame is high-dimensional and non-linear. Directly learning such transformation is challenging. On the other hand, if we decompose and understand this video correctly, the underlying dynamics that we must predict are simply the x, y coordinates of each individual digit, which are low-dimensional and easy to model and predict in this case (constant velocity translation).</p><p>The main technical challenge is thus: How do we decompose the high-dimensional video sequence into sub-problems with lower-dimensional temporal dynamics? While the decomposition is seemingly obvious in the example from Figure <ref type="figure" target="#fig_0">1</ref>, it is unclear how we can extend this to arbitrary videos. More importantly, how do we discover the decomposition automatically? It is infeasible or even impossible to hand-craft the decomposition for predicting each type of video. While there have been previous works that similarly aim to reduce the complexity of frame prediction by human pose <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref> and patch-based model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref>, they either require domain-specific external supervision <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref> or do not achieve a significant level of dimension reduction using heuristics <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><note type="other">Decompose</note><p>We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the video we aim to predict into components, and (ii) disentangle each component into low-dimensional temporal dynamics that are easy to predict. With appropriately specified generative model on future frames, DDPAE is able to learn both the video decomposition and the component disentanglement that are effective for video prediction without any explicit supervision on these latent variables. By training a structural generative model of future frames like DDPAE, the aim is not only to obtain good future frame predictions, but also to learn to produce good decomposition and understanding of videos that significantly reduce the complexity of visual frame prediction.</p><p>We evaluate DDPAE on two datasets: Moving MNIST <ref type="bibr" target="#b30">[31]</ref> and Bouncing Balls <ref type="bibr" target="#b2">[3]</ref>. Moving MNIST has been widely used for evaluating video prediction models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>. We show that DDPAE is able to learn to decompose videos in the Moving MNIST dataset into individual digits, and further disentangles each component into the digit's appearance and its spatial location which is much easier to predict (Figure <ref type="figure" target="#fig_0">1</ref>). This significantly reduces the complexity of frame prediction and leads to strong quantitative and qualitative improvements over the baselines that aim to predict the video as a whole <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset, which has been used mainly for approaches that have access to full physical states (location, velocity, mass) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>. We show that DDPAE is able to achieve reliable prediction of such complex systems directly from pixels, and recover physical properties without explicitly modeling the physical states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video Prediction. The task of video prediction has received increasing attention in the community. Early works include prediction on small image patches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>. Recent common approaches for full frame prediction predict the feature representations that generate future frames <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> in a sequence-to-sequence framework <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>, which has been extended to incorporate spatio-temporal recurrence <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>. Instead of directly generating the pixels, transformation-based models focus on predicting the difference/transformation between frames and lead to sharper results <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. We also aim to predict the transformation, but only for the temporal dynamics of the decomposed and disentangled representation, which is much easier to predict than whole-frame transformation.</p><p>Visual Representation Decomposition. Decomposing the video that we aim to predict into components plays an important role to the success of our method. The idea of visual representation decomposition has also been applied in different contexts, including representation learning <ref type="bibr" target="#b26">[27]</ref>, physics modeling <ref type="bibr" target="#b2">[3]</ref>, and scene understanding <ref type="bibr" target="#b6">[7]</ref>. In particular, some previous works use methods such as Expectation Maximization to perform perceptual grouping and discover individual objects in videos <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>A highly related work is Attend-Infer-Repeat (AIR) by Eslami et al. <ref type="bibr" target="#b6">[7]</ref>, which decomposes images in a variational auto-encoder framework. Our work goes beyond the image and extends to the temporal dimension, where the model automatically learns the decomposition that is best suited for predicting the future frames. Concurrent to our work, Kosiorek et al. <ref type="bibr" target="#b18">[19]</ref> proposed the Sequential Attend-Infer-Repeat (SQAIR), which extends the AIR model and is very similar to our work. Disentangled Representation. To learn meaningful decomposition, our DDPAE enforces the components to be disentangled into a representation with low-dimensional temporal dynamics. The idea of disentangled representation has already been explored <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> for video. Denton et al. <ref type="bibr" target="#b5">[6]</ref> proposed DRNet, where representations are disentangled into content and pose, and the poses are penalized for encoding semantic information with the use of a discrimination loss. Similarly, MCNet <ref type="bibr" target="#b36">[37]</ref> disentangles motion from content using image differences and shared a single content vector in prediction. Note that some videos are hard to directly disentangle. Our work addresses this by decomposing the video so that each component can actually be disentangled.</p><p>Variational Auto-Encoder (VAE). Our DDPAE is based on the VAE <ref type="bibr" target="#b16">[17]</ref>, which provides one solution to the multiple future problem <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref>. VAEs have been used for image and video generation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>. Our key contribution is to make the model structural, where the latent representation is decomposed and more importantly disentangled. Our network models both motion and content probabilistically, and is regularized by learning transformations in a way similar to <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Our goal is to predict K future frames given T input frames. Our core insight is to combine structured probabilistic models and deep networks to (i) decompose the high-dimensional video into components, and (ii) disentangle each component into low-dimensional temporal dynamics that are easy to predict. First, we take a Bayesian perspective and propose the Decompositional Disentangled Predictive Auto-Encoder (DDPAE) as our formulation in Section 3.1. Next, we discuss our deep parameterization of each of the components in DDPAE in Section 3.2. Finally, we show how we learn the DDPAE by optimizing the evidence lower bound in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decompositional Disentangled Predictive Auto-Encoder</head><p>Formally, given an input video x 1:T of length T , our goal is to predict future K frames x1:K = x (T +1):(T +K) . For simplicity, in this paper we denote any variable z1:K to be the prediction sequence of z from time step T + 1 to T + K, i.e. z1:K = z (T +1):(T +K) . We assume that each video frame x t is generated from a corresponding latent representation z t . In this case, we can formulate the video frame prediction p(x 1:K |x 1:T ) as:</p><formula xml:id="formula_0">p(x1:K |x1:T ) = p(x1:K |z1:K )p(z1:K |z1:T )p(z1:T |x1:T ) dz1:K dz1:T ,<label>(1)</label></formula><p>where p(x 1:K |z 1:K ) is the frame decoder for generating frames based on latent representations, p(z 1:K |z 1:T ) is the prediction model that captures the dynamics of the latent representations, and p(z 1:T |x 1:T ) is the temporal encoder that infers the latent representations given the input video x 1:T . From a Bayesian perspective, we model these three as probability distributions.</p><p>Our core insight is to decompose the video prediction problem in Eq. ( <ref type="formula" target="#formula_0">1</ref>) into sub-problems that are easier to predict. In a simplified case, where each of the components can be predicted independently (e.g., digits in Figure <ref type="figure" target="#fig_0">1</ref>), we can use the following decomposition:</p><formula xml:id="formula_1">x1:K = N i=1 xi 1:K , x1:T = N i=1 x i 1:T ,<label>(2)</label></formula><formula xml:id="formula_2">p(x i 1:K |x i 1:T ) = p(x i 1:K |z i 1:K )p(z i 1:K |z i 1:T )p(z i 1:T |x i 1:T ) dz i 1:K dz i 1:T ,<label>(3)</label></formula><p>where we decompose the input x 1:T into {x i 1:T } and independently predict the future frames {x i 1:K }, which will be combined as the final prediction x1:K . We will use this independence assumption for the sake of explanation, but we will show later how this can easily be extended to the case where the components are interdependent, which is crucial for capturing interactions between components.</p><p>The key technical challenge is thus: How do we learn the decomposition? How do we enforce that each component is actually easier to predict? One can imagine a trivial decomposition, where</p><p>x 1 1:T = x 1:T and x i 1:T = 0 for i &gt; 1. This does not simplify the prediction at all, but only keeps the same complexity at a single component. We address this challenge by enforcing the latent representations of each component (z i 1:K and z i 1:T ) to have low-dimensional temporal dynamics. In other words, the temporal signal to be predicted in each component should be low-dimensional. More specifically, we achieve this by leveraging the disentangled representation <ref type="bibr" target="#b5">[6]</ref>: a latent representation z i t is disentangled to the concatenation of (i) a time-invariant content vector z i t,C , and (ii) a timedependent (low-dimensional) pose vector z i t,P . The content vector captures the information that is shared across all frames of the component. For example, in the first component of Figure <ref type="figure" target="#fig_0">1</ref>, the content vector models the appearance of the digit "9". Formally, we assume the content vector is the same for all frames in both the input and the prediction:</p><formula xml:id="formula_3">z i t,C = zi t,C = z i C .</formula><p>On the other hand, the pose vector z i t,P is low-dimensional, which captures the location of the digit in Figure <ref type="figure" target="#fig_0">1</ref>. This allows us to disentangle the prediction of decomposed latent representations as follows:</p><formula xml:id="formula_4">p(z i 1:K |z i 1:T ) = p(z i 1:K,P |z i 1:T,P ), zi t = [z i C , zi t,P ], z i t = [z i C , z i t,P ],<label>(4)</label></formula><p>where the prediction p(z i 1:K |z i 1:T ) is reduced to just predicting the low-dimensional pose vectors p(z i 1:K,P |z i 1:T,P ). This is possible since we share the content vector between the input and the prediction. This disentangled representation allows the prediction of each component to focus on the low-dimensional varying pose vectors, and significantly simplifies the prediction task.</p><p>Eq. ( <ref type="formula" target="#formula_1">2</ref>)-( <ref type="formula" target="#formula_4">4</ref>) thus define the proposed Decompositional Disentangled Predictive Auto-Encoder (DDPAE). Note that both the decomposition and the disentanglement are learned automatically without explicit supervision. Our formulation encourages the model to decompose the video into components with low-dimensional temporal dynamics in the disentangled representation. By training this structural generative model of future frames, the hope is to learn to produce good decomposition and disentangled representations of the video that reduce the complexity of frame prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Implementation</head><p>We have formulated how we decompose the video prediction problem into sub-problems of disentangled representations that are easier to predict in our DDPAE framework. In this section, we discuss our implementation of each of the component of our model in Eq. ( <ref type="formula" target="#formula_1">2</ref>)-( <ref type="formula" target="#formula_4">4</ref>), starting from the generation p(x i 1:K |z i 1:K ), inference p(z i 1:T |x i 1:T ), and finally prediction p(z i 1:K,P |z i 1:T,P ). Frame Generation Model. In Eq. (3), p(x i 1:K |z i 1:K ) is frame generation model. We assume conditional independence between the frames: p(x i 1:</p><formula xml:id="formula_5">K |z i 1:K ) = K j=1 p(x i j |z i j )</formula><p>. This model is used for both input reconstruction p(x i t |z i t ) and prediction p(x i t |z i t ). Our frame generation model is flexible and can vary based on the domain. For 2D scenes, we follow work in scene understanding <ref type="bibr" target="#b6">[7]</ref> and use an attention-based generative model. Note that our latent representation is disentangled:</p><formula xml:id="formula_6">zi t = [z i C , zi t,P ]</formula><p>, where zi C = z i C is the fixed content vector (e.g., the latent representation of the digit), and zi t,P is the pose vector (e.g., the location and scale of the digit). As shown in Figure <ref type="figure" target="#fig_2">2</ref>(c), we generate the image xi t as follows: First, the content vector is decoded to a rectified image ȳi t using deconvolution layers. Next, the pose vector is used to parameterize an inverse spatial transformer T -1 z <ref type="bibr" target="#b13">[14]</ref> to warp ȳi t to the generated frame xi t . The pose vector in this example is a 3-dimensional continuous variable, which significantly simplifies the prediction problem compared to predicting the full frame.</p><p>Inference. In Eq. (3), our prediction requires the inference of the latent representations, p(z i 1:T |x i 1:T ). Given our generation model p(x i t |z i t ), the true posterior distribution is intractable. Thus, the standard practice is to employ a variational approximation q(z i 1:T |x i 1:T ) to the true posterior <ref type="bibr" target="#b16">[17]</ref>. Since our latent representations are decomposed and disentangled, we explain our model q in the following two sections: Video Decomposition and Disentangled Representation.</p><p>Video Decomposition. The next question is: How do we get the decomposed x i 1:T from x 1:T ? Eq. (3) assumes that the decomposition is given. Our key observation is that even if we decompose the input x 1:T to {x i 1:T } in a separate step, the decomposed video would only be used to infer its respective latent representation through variational approximation. In this case, we can combine the video decomposition with the variational approximation as q(z i 1:T |x 1:T ), which directly infers the latent representations of each component. We implement q(z i 1:T |x 1:T ) using an RNN with 2-dimensional recurrence, where one recurrence is for the temporal modeling (1 : T ) and the other is used to capture  the dependencies between components. For instance, in the video in Figure <ref type="figure" target="#fig_0">1</ref>, the component of digit "6" needs to know that "9" is already modeled by the first component. Figure <ref type="figure" target="#fig_2">2</ref>(a) shows our 2-dimensional recurrence (our input RNN) in both the time steps and the components.</p><p>Disentangled Representation. While the 2D recurrence model can directly infer the latent representations, it is not guaranteed to output disentangled representation. We thus design a structural inference model to disentangle the representation. In contrast to frame generation, where the goal is to generate different frames conditioning on the same content vector, the goal here in inference is to revert the process and obtain a single shared content vector z i C for different frames, and hence force the variations between frames to be encoded in the pose vectors z i 1:T,P . Thus, we apply the inverse of the structural model in our generation process (see Figure <ref type="figure" target="#fig_2">2(d)</ref>). For 2D scenes, this means applying the spatial transformer parameterized by z i t,P to extract the rectified image y i t from the frame x t . We then use a CNN to encode each y i t into a latent representation. Instead of training with similarity regularization <ref type="bibr" target="#b5">[6]</ref>, we use another RNN on top of the raw output as pooling to obtain a single content vector z i C for each component. Figure <ref type="figure" target="#fig_2">2</ref>(d) shows the process of inferring z i C from z i 1:T,P and x 1:T . Since the same z i C is used for each time step in prediction, this forces the decomposition of our model to separate the components with different motions to get good prediction of the sequence.</p><p>Pose Prediction. The final component is the pose vector prediction p(z i 1:K,P |z i 1:T,P ). Since z i C is fixed in prediction, we only need to predict the pose vectors. Inspired by <ref type="bibr" target="#b15">[16]</ref>, instead of directly inferring z i t,P , we introduce a set of transition variables β i t to reparametrize the pose vectors. Given z i t-1,P and β i t , the transition to z i t,P is deterministic with linear combination: z i t,P = f (z i t-1,P , β i t ). This allows us to use a meaningful prior for β t . Therefore, as shown in Figure <ref type="figure" target="#fig_2">2</ref>(a) and (b), given an input sequence x 1:T , for each component our model infers an initial pose vector z i 0,P and the transition variables β i 1:T , from which we can iteratively obtain z i t,P at each time step. We use a seq2seq <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref> based model to predict βi 1:K (Figure <ref type="figure" target="#fig_2">2(b)</ref>). With this RNN-based model, the dependencies between poses of components can be captured by passing the hidden states across components. This allows the model to learn and predict interactions between components, such as collisions between objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>Our DDPAE framework is based on VAEs <ref type="bibr" target="#b16">[17]</ref>, and thus we can use the same variational techniques to optimize our model. For VAE, the assumption is that each data point x is generated from a latent random variable z with p θ (x|z), where z is sampled from a prior p θ (z). In our case, the output video x1:K is generated from the latent representations z1:N 1:K of N components, where zi t is the disentangled representation [z i C , zi t,P ] (Eq. ( <ref type="formula" target="#formula_4">4</ref>)) of the ith component, and zi 1:K,P is parameterized by the initial pose z i 0,P and the transition variables β i 1:(T +K) . Therefore, in our model, we treat z 1:N 0,P , β 1:N 1:(T +K) ,   <ref type="bibr" target="#b30">[31]</ref> 341.2 -Brabandere et al. <ref type="bibr" target="#b4">[5]</ref> 285.2 -Patraucean et al. <ref type="bibr" target="#b24">[25]</ref> 262.6 -Ghosh et al. <ref type="bibr" target="#b9">[10]</ref> 241.8 167.9 Kalchbrenner et al. <ref type="bibr" target="#b14">[15]</ref> 87. and z 1:N C as the underlying random latent variables that generate data x1:K . We denote z as the combined set of random variables in our model. z is inferred from the input frames, z ∼ q φ (z|x 1:T ), where q φ is our inference model explained in Section 3.2, parameterized by φ. The output frames x1:K are generated by x1:K ∼ p θ (x 1:K |z), where p θ is our frame generation model parameterized by θ. Moreover, we assume that the prior distribution to be p(z) = N (µ, diag(σ 2 )). We jointly optimize θ and φ by maximizing the evidence lower bound (ELBO):</p><formula xml:id="formula_7">log p θ (x1:K ) ≥ Eq[log p θ (x1:K , z) -log q φ (z|x1:T )] = Eq[log p θ (x1:K |z) -KL(q φ (z|x1:T )||p(z)) (5)</formula><p>The first term corresponds to the prediction error, and the second term serves as regularization of the latent variables z. With the reparametrization trick, the entire model is differentiable, and the parameters θ and φ can be jointly optimized by standard backpropagation technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our goal is to predict a sequence of future frames given a sequence of input frames. The key contribution of our DDPAE is to both decompose and disentangle the video representation to simplify the challenging frame prediction task. First, we evaluate the importance of both the decomposition and disentanglement of the video representation for frame prediction on the widely used Moving MNIST dataset <ref type="bibr" target="#b30">[31]</ref>. Next, we evaluate how DDPAE can be applied to videos involving more complex interactions between components on the Bouncing Balls dataset <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>. Finally, we evaluate how DDPAE can generalize and adapt to the cases where the optimal number of components is not known a priori, which is important for applying DDPAE to new domains of videos.</p><p>Code for DDPAE and the experiments are available at https://github.com/jthsieh/ DDPAE-video-prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluating Decompositional Disentangled Video Representation</head><p>The key element of DDPAE is learning the decompositional-disentangled representations. We evaluate the importance of both decomposition and disentanglement using the Moving MNIST dataset. Since the digits in the videos follow independent low-dimensional trajectories, our framework significantly simplifies the prediction task from the original high-dimensional pixel prediction. We show that DDPAE is able to learn the decomposition and disentanglement automatically without explicit supervision, which plays an important role in the accurate prediction of DDPAE.</p><p>We compare two state-of-the-art video prediction methods without decomposition as baselines: MCNet <ref type="bibr" target="#b36">[37]</ref> and DRNet <ref type="bibr" target="#b5">[6]</ref>. Both models perform video prediction using disentangled representations, similar to our model with only one component. We use the code provided by the authors of the two papers. For reference, we also list the results of existing work on Moving MNIST, where they use more complicated models such as convolutional LSTM or PixelCNN decoders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Dataset. Moving MNIST is a synthetic dataset consisting of two digits moving independently in a 64 × 64 frame. It has been used in many previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref>. For training, each   sequence is generated on-the-fly by sampling MNIST digits and generating trajectories with randomly sampled velocity and angle. The test set is a fixed dataset downloaded from <ref type="bibr" target="#b30">[31]</ref> consisting of 10,000 sequences of 20 frames, with 10 as input and 10 to predict.</p><p>Evaluation Metric. We follow <ref type="bibr" target="#b30">[31]</ref> and use the binary cross-entropy (BCE) as the evaluation metric. We also report the mean squared error (MSE) as an additional metric from <ref type="bibr" target="#b9">[10]</ref>.</p><p>Results. Table <ref type="table" target="#tab_0">1</ref> shows the quantitative results. DDPAE significantly outperforms the baselines without decomposition (MCNet, DRNet) or without disentanglement. For MCNet and DRNet, the latent representations need to contain complicated information of the digits' combined content and motion, and moreover, the decoder has a much harder task of generating two digits at the same time. In fact, <ref type="bibr" target="#b5">[6]</ref> specifically stated that DRNet is unable to get good results when the two digits have the same color. In addition, our baseline without disentanglement produces blurry results due to the difficulty of predicting representations.</p><p>Our model, on the other hand, greatly simplifies the inference of the latent variables and the decoder by both decomposition and disentanglement, resulting in better prediction. This is also shown in the qualitative results in Figure <ref type="figure" target="#fig_3">3</ref>, where DDPAE successfully separates the two digits into two components and only needs to predict the low-dimensional pose vectors. Note that DDPAE can also handle occlusion. Compared to existing works, DDPAE achieves the best result except BCE compared to VPN <ref type="bibr" target="#b14">[15]</ref>, which can be the result of its more sophisticated image generation process using PixelCNN. The main contribution of DDPAE is in the decomposition and disentanglement, which is in principle applicable to other existing models like VPN.</p><p>It is worth noting that the ordering of the components is learned automatically by the model. We obtain the final output by adding the components, which is a permutation-invariant operation. The model can learn to generate components in any order, as long as the final frames are correct. This phenomenon is also observed in many fields, including tracking and object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluating Interdependent Components</head><p>Previously in Eq. (3), we assume the components to be independent, i.e., the pose of each component is separately predicted without information of other components. The independence assumption is not true in most scenarios, as components in a video may interact with each other. Therefore, it is important for us to generalize to interdependent components. In Section 3.2, we explain how our model adds dependencies between components in the prediction RNN. We now evaluate the importance of it in more complex videos. We evaluate the interdependency on the Bouncing Balls dataset <ref type="bibr" target="#b2">[3]</ref>. Bouncing Balls is ideal for evaluating this because (i) it is widely used for methods with access to physical states <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref> and (ii) it involves physical interactions between components. One contribution of our DDPAE framework is the ability to achieve complex physics system predictions directly from the pixels, without any physics information and assumptions.  Dataset. We simulate sequences of 4 balls bouncing in an image with the physics engine code used in <ref type="bibr" target="#b2">[3]</ref>. The balls are allowed to bounce off walls and collide with each other. Following the prediction task setting in <ref type="bibr" target="#b2">[3]</ref>, the balls have the same mass and the maximum velocity is 60 pixels/second (roughly 6 pixels/frame). The size of the original videos are 800 pixels, so we re-scale the videos to 128 × 128. We generated a fixed training set of 50,000 sequences and a test set of 2,000 sequences.</p><p>Evaluation Metric. The primary goal of this experiment is to evaluate the importance of modeling the dependencies between components. Therefore, following <ref type="bibr" target="#b2">[3]</ref>, we evaluate the predicted velocities of the balls. Since our model outputs the spatial transformer of each component at every time step, we can calculate the position p i t of the attention region directly and thus the translation between frames. We normalize the positions to be [0, 1], and define the velocity to be v i t = p i t+1 -p i t-1 . At every time step, we calculate the relative error in magnitude and the cosine similarity between the predicted and ground truth velocities, which corresponds to the speed and direction respectively. The final results are averaged over all instances in the test set. Note that the correspondence between components and balls is not known, so we first match each component to a ball by minimum distance.</p><p>Results. Figure <ref type="figure" target="#fig_4">4</ref> shows results of our model on Bouncing Balls. Each component captures a single ball correctly. Note that during prediction, a collision occurs between the two balls in the upper right corner in the ground truth video. Our model successfully predicts the colliding balls to bounce off of each other instead of overlapping each other. On the other hand, our baseline model predicts the balls' motion independently and fails to identify the collision, and thus the two balls overlap each other in the predicted video. This shows that DDPAE is able to capture the important dependencies between components when predicting the pose vectors. It is worth noting that predicting the trajectory after collision is a fundamentally challenging problem for our model since it highly depends on the collision surface of the balls, which is very hard to predict accurately. Figure <ref type="figure" target="#fig_6">5</ref> shows the relative error in magnitude and cosine similarity between the predicted and ground truth velocities, at each time step during prediction. The accuracy of the predicted velocities decreases with time as expected. We compare our model against the baseline model without interdependent components. Figure <ref type="figure" target="#fig_6">5</ref> shows that our model outperforms the baseline for both metrics. The dependency allows our model to capture the interactions between balls, and hence generates more accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluating Generalization to Unknown Number of Components</head><p>In the previous experiments, the number of objects in the video is known and fixed, and thus we set the number of components in DDPAE to be the same. However, videos may contain an unknown and variable number of objects. We evaluate the robustness of our model in these scenarios with the Moving MNIST dataset. We set the number of components to be 3 for all experiments, and the number of digits to be a subset of {1, 2, 3}. Similar to previous experiments, we generate the training sequences on-the-fly and evaluate on a fixed test set.</p><p>Figure <ref type="figure" target="#fig_8">6</ref> (a) shows results of our model trained on 1 to 3 digits. The two test sequences have 1 and 3 digits respectively. For sequences with 1 digit, our model learns to set two redundant components to empty, while for sequences with 3 digits, it correctly separates the 3 digits into 3 components. We observe similar results when we train our model with 2 digits. Figure <ref type="figure" target="#fig_8">6</ref> (b) shows that our model learns to set the extra component to be empty.</p><p>Next, we train our model with sequences containing 1 or 3 digits, but test with sequences of 2 digits. In this case, the number of digits is unseen during training. Figure <ref type="figure" target="#fig_8">6</ref> (c) shows that our model is able to produce correct results as well. Interestingly, two of the components generate the exact same outputs. This is reasonable since we do not set any constraints between components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a video prediction framework that explicitly decomposes and disentangles the video representation and reduces the complexity of future frame prediction. We show that, with an appropriately specified structural model, DDPAE is able to learn both the video decomposition and disentanglement that are effective for video prediction without any explicit supervision on these latent variables. This leads to strong quantitative and qualitative improvements on the Moving MNIST dataset. We further show that DDPAE is able to achieve reliable prediction directly from the pixel on the Bouncing Balls dataset involving complex object interaction, and recover physical properties without explicit modeling the physical states.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our key insight is to decompose the video into several components. The prediction of each individual component is easier than directly predicting the whole image sequence. It is important to note that the decomposition is learned automatically without explicit supervision.</figDesc><graphic coords="2,99.72,81.29,473.50,93.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our model implementation. (a) We use 2D recurrence to implement q(z i 1:T |x 1:T ) to model both the temporal and dependency between components. (b) The prediction RNN is used only to predict the pose vector. (c) Our frame generation model generates different image with the same content using inverse spatial transformer. (d) A single content vector z i C is obtained for each component from input x 1:T and pose vectors z i 1:T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DDPAE separates the two digits and obtains good results even when the digits overlap. The bounding boxes of the two components are drawn manually.</figDesc><graphic coords="6,-20.57,144.23,357.28,70.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Our model prediction on Bouncing Balls. Note that our model correctly predicts the collision between the two balls in the upper right corner, whereas the baseline model does not.</figDesc><graphic coords="7,-26.23,130.67,386.12,115.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy of velocity with time. Top: Relative error in magnitude. Bottom: Cosine similarity.</figDesc><graphic coords="7,-26.23,34.16,386.12,115.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of DDPAE trained on variable number of digits. Only the predicted frames are shown. Our model is able to correctly handle redundant components.</figDesc><graphic coords="8,184.28,158.69,294.10,72.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on Moving MNIST (Bold for the best and underline for the second best). Our results significantly outperforms the baselines.</figDesc><table><row><cell>Model</cell><cell>BCE</cell><cell>MSE</cell></row><row><cell>Shi et al. [43]</cell><cell>367.2</cell><cell>-</cell></row><row><cell>Srivastava et al.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially funded by Panasonic and Oppo. We thank our anonymous reviewers, John Emmons, Kuan Fang, Michelle Guo, and Jingwei Ji for their helpful feedback and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social LSTM: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning visual predictive models of physics for playing billiards</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Contextual rnn-gans for abstract reasoning diagram generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tagger: Deep unsupervised perceptual grouping</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural expectation maximization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep variational bayes filters: Unsupervised learning of state space models from raw data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soelch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequential attend, infer, repeat: Generative modelling of moving objects</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A hierarchical representation for future action prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Folded recurrent neural networks for future video prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06309</idno>
		<title level="m">Spatio-temporal video autoencoder with differentiable memory</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Barnoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Katyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00062</idno>
		<title level="m">Visual robot task planning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object-centric representation learning from unlabeled videos</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<title level="m">Video (language) modeling: a baseline for generative models of natural videos</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">One-shot generalization in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05106</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Generating notifications for missing actions: Don&apos;t forget to turn the lights off! In ICCV</title>
		<author>
			<persName><forename type="first">B</forename><surname>Soran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hybrid vae: Improving deep generative models using partial observations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11566</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04993</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Transformation-based models of video sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08435</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relational neural expectation maximization: Unsupervised discovery of objects and their interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05831</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generating the future with adversarial transformers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
