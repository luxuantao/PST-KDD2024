<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<email>yuxdong@microso.com</email>
						</author>
						<author>
							<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
							<email>nchawla@nd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
							<email>ananthram.swami.civ@mail.mil</email>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Malik</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microso Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Army Research Laboratory Adelphi</orgName>
								<address>
									<postCode>20783</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3097983.3098036</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems →Social networks</term>
					<term>•Computing methodologies →Unsupervised learning</term>
					<term>Learning latent representations</term>
					<term>Knowledge representation and reasoning</term>
					<term>Network Embedding</term>
					<term>Heterogeneous Representation Learning</term>
					<term>Latent Representations</term>
					<term>Feature Learning</term>
					<term>Heterogeneous Information Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of representation learning in heterogeneous networks. Its unique challenges come from the existence of multiple types of nodes and links, which limit the feasibility of the conventional network embedding techniques. We develop two scalable representation learning models, namely metapath2vec and metapath2vec++. e metapath2vec model formalizes meta-pathbased random walks to construct the heterogeneous neighborhood of a node and then leverages a heterogeneous skip-gram model to perform node embeddings. e metapath2vec++ model further enables the simultaneous modeling of structural and semantic correlations in heterogeneous networks. Extensive experiments show that metapath2vec and metapath2vec++ are able to not only outperform state-of-the-art embedding models in various heterogeneous network mining tasks, such as node classi cation, clustering, and similarity search, but also discern the structural and semantic correlations between diverse network objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural network-based learning models can represent latent embeddings that capture the internal relations of rich, complex data across various modalities, such as image, audio, and language <ref type="bibr" target="#b14">[15]</ref>. Social  and information networks are similarly rich and complex data that encode the dynamics and types of human interactions, and are similarly amenable to representation learning using neural networks. In particular, by mapping the way that people choose friends and maintain connections as a "social language," recent advances in natural language processing (NLP) <ref type="bibr" target="#b2">[3]</ref> can be naturally applied to network representation learning, most notably the group of NLP models known as word2vec <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. A number of recent research publications have proposed word2vec-based network representation learning frameworks, such as DeepWalk <ref type="bibr" target="#b21">[22]</ref>, LINE <ref type="bibr" target="#b29">[30]</ref>, and node2vec <ref type="bibr" target="#b7">[8]</ref>. Instead of handcra ed network feature design, these representation learning methods enable the automatic discovery of useful and meaningful (latent) features from the "raw networks. "</p><p>However, these work has thus far focused on representation learning for homogeneous networks-representative of singular type of nodes and relationships. Yet a large number of social and information networks are heterogeneous in nature, involving diversity of node types and/or relationships between nodes <ref type="bibr" target="#b24">[25]</ref>. ese heterogeneous networks present unique challenges that cannot be handled by representation learning models that are speci cally designed for homogeneous networks. Take, for example, a heterogeneous academic network: How do we e ectively preserve the concept of "word-context" among multiple types of nodes, e.g., authors, papers, venues, organizations, etc.? Can random walks, such those used in DeepWalk and node2vec, be applied to networks Table <ref type="table">1</ref>: Case study of similarity search in the heterogeneous DBIS data used in <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>PathSim <ref type="bibr" target="#b25">[26]</ref> DeepWalk / node2vec <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref> LINE (1st+2nd) <ref type="bibr" target="#b29">[30]</ref> PTE <ref type="bibr" target="#b28">[29]</ref> metapath2vec metapath2vec++</p><p>Input meta-paths heterogeneous random walk paths heterogeneous edges heterogeneous edges probabilistic meta-paths probabilistic meta-paths of multiple types of nodes? Can we directly apply homogeneous network-oriented embedding architectures (e.g., skip-gram) to heterogeneous networks? By solving these challenges, the latent heterogeneous network embeddings can be further applied to various network mining tasks, such as node classi cation <ref type="bibr" target="#b12">[13]</ref>, clustering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, and similarity search <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">35]</ref>. In contrast to conventional meta-path-based methods <ref type="bibr" target="#b24">[25]</ref>, the advantage of latent-space representation learning lies in its ability to model similarities between nodes without connected meta-paths. For example, if authors have never published papers in the same venue-imagine one publishes 10 papers all in NIPS and the other has 10 publications all in ICML; their "APCPA"-based Path-Sim similarity <ref type="bibr" target="#b25">[26]</ref> would be zero-this will be naturally overcome by network representation learning. Contributions. We formalize the heterogeneous network representation learning problem, where the objective is to simultaneously learn the low-dimensional and latent embeddings for multiple types of nodes. We present the metapath2vec and its extension meta-path2vec++ frameworks. e goal of metapath2vec is to maximize the likelihood of preserving both the structures and semantics of a given heterogeneous network. In metapath2vec, we rst propose meta-path <ref type="bibr" target="#b24">[25]</ref> based random walks in heterogeneous networks to generate heterogeneous neighborhoods with network semantics for various types of nodes. Second, we extend the skip-gram model <ref type="bibr" target="#b17">[18]</ref> to facilitate the modeling of geographically and semantically close nodes. Finally, we develop a heterogeneous negative sampling-based method, referred to as metapath2vec++, that enables the accurate and e cient prediction of a node's heterogeneous neighborhood.</p><p>e proposed metapath2vec and metapath2vec++ models are different from conventional network embedding models, which focus on homogeneous networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. Speci cally, conventional models su er from the identical treatment of di erent types of nodes and relations, leading to the production of indistinguishable representations for heterogeneous nodes-as evident through our evaluation. Further, the metapath2vec and metapath2vec++ models also di er from the Predictive Text Embedding (PTE) model <ref type="bibr" target="#b28">[29]</ref> in several ways. First, PTE is a semi-supervised learning model that incorporates label information for text data. Second, the heterogeneity in PTE comes from the text network wherein a link connects two words, a word and its document, and a word and its label. Essentially, the raw input of PTE is words and its output is the embedding of each word, rather than multiple types of objects.</p><p>We summarize the di erences of these methods in Table <ref type="table">1</ref>, which lists their input to learning algorithms, as well as the top-ve similarity search results in the DBIS network for the same two queries used in <ref type="bibr" target="#b25">[26]</ref> (see Section 4 for details). By modeling the heterogeneous neighborhood and further leveraging the heterogeneous negative sampling technique, metapath2vec++ is able to achieve the best top-ve similar results for both types of queries. Figure <ref type="figure" target="#fig_1">1</ref> shows the visualization of the 2D projections of the learned embeddings for 16 CS conferences and corresponding high-pro le researchers in each eld. Remarkably, we nd that metapath2vec++ is capable of automatically organizing these two types of nodes and implicitly learning the internal relationships between them, suggested by the similar directions and distances of the arrows connecting each pair. For example, it learns J. Dean → OSDI and C. D. Manning → ACL. metapath2vec is also able to group each author-conference pair closely, such as R. E. Tarjan and FOCS. All of these properties are not discoverable from conventional network embedding models.</p><p>To summarize, our work makes the following contributions:</p><p>(1) Formalizes the problem of heterogeneous network representation learning and identi es its unique challenges resulting from network heterogeneity. (2) Develops e ective and e cient network embedding frameworks, metapath2vec &amp; metapath2vec++, for preserving both structural and semantic correlations of heterogeneous networks. <ref type="bibr" target="#b2">(3)</ref> rough extensive experiments, demonstrates the e cacy and scalability of the presented methods in various heterogeneous network mining tasks, such as node classi cation (achieving relative improvements of 35-319% over benchmarks) and node clustering (achieving relative gains of 13-16% over baselines). (4) Demonstrates the automatic discovery of internal semantic relationships between di erent types of nodes in heterogeneous networks by metapath2vec &amp; metapath2vec++, not discoverable by existing work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><p>We formalize the representation learning problem in heterogeneous networks, which was rst brie y introduced in <ref type="bibr" target="#b20">[21]</ref>. In speci c, we leverage the de nition of heterogeneous networks in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> and present the learning problem with its inputs and outputs.</p><p>De nition 2.1. A Heterogeneous Network is de ned as a graph G = (V , E,T ) in which each node and each link e are associated with their mapping functions ϕ ( ) : V → T V and φ(e) : E → T E , respectively. T V and T E denote the sets of object and relation types, where</p><formula xml:id="formula_0">|T V | + |T E | &gt; 2.</formula><p>For example, one can represent the academic network in Figure <ref type="figure">2</ref>(a) with authors (A), papers (P), venues (V), organizations (O) as nodes, wherein edges indicate the coauthor (A-A), publish (A-P, P-V), a liation (O-A) relationships. By considering a heterogeneous network as input, we formalize the problem of heterogeneous network representation learning as follows. P 1. Heterogeneous Network Representation Learning: Given a heterogeneous network G, the task is to learn the d-</p><formula xml:id="formula_1">dimensional latent representations X ∈ R |V |×d , d</formula><p>|V | that are able to capture the structural and semantic relations among them. e output of the problem is the low-dimensional matrix X, with the th row-a d-dimensional vector X -corresponding to the representation of node . Notice that, although there are di erent types of nodes in V , their representations are mapped into the same latent space. e learned node representations can bene t various heterogeneous network mining tasks. For example, the embedding vector of each node can be used as the feature input of node classi cation, clustering, and similarity search tasks.</p><p>e main challenge of this problem comes from the network heterogeneity, wherein it is di cult to directly apply homogeneous language and network embedding methods. e premise of network embedding models is to preserve the proximity between a node and its neighborhood (context) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. In a heterogeneous environment, how do we de ne and model this 'node-neighborhood' concept? Furthermore, how do we optimize the embedding models that e ectively maintain the structures and semantics of multiple types of nodes and relations?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE METAPATH2VEC FRAMEWORK</head><p>We present a general framework, metapath2vec, which is capable of learning desirable node representations in heterogeneous networks. e objective of metapath2vec is to maximize the network probability in consideration of multiple types of nodes and edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Homogeneous Network Embedding</head><p>We, rst, brie y introduce the word2vec model and its application to homogeneous network embedding tasks. Given a text corpus, Mikolov et al. proposed word2vec to learn the distributed representations of words in a corpus <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Inspired by it, DeepWalk <ref type="bibr" target="#b21">[22]</ref> and node2vec <ref type="bibr" target="#b7">[8]</ref> aim to map the word-context concept in a text corpus into a network. Both methods leverage random walks to achieve this and utilize the skip-gram model to learn the representation of a node that facilitates the prediction of its structural context-local neighborhoods-in a homogeneous network. Usually, given a network G = (V , E), the objective is to maximize the network probability in terms of local structures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>, that is:</p><formula xml:id="formula_2">arg max θ ∈V c ∈N ( ) p(c | ; θ )<label>(1)</label></formula><p>where N ( ) is the neighborhood of node in the network G, which can be de ned in di erent ways such as 's one-hop neighbors, and p(c | ; θ ) de nes the conditional probability of having a context node c given a node .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Heterogeneous Network Embedding: metapath2vec</head><p>To model the heterogeneous neighborhood of a node, metapath2vec introduces the heterogeneous skip-gram model. </p><p>where N t ( ) denotes 's neighborhood with the t t h type of nodes and p(c t | ; θ ) is commonly de ned as a so max function <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>, that is:</p><formula xml:id="formula_4">p(c t | ; θ ) = e Xc t •X u ∈V e Xu •X</formula><p>, where X is the t h row of X, representing the embedding vector for node . For illustration, consider the academic network in Figure <ref type="figure">2</ref>(a), the neighborhood of one author node a 4 can be structurally close to other authors (e.g., a 2 , a 3 &amp; a 5 ), venues (e.g., ACL &amp; KDD), organizations (CMU &amp; MIT), as well as papers (e.g., p 2 &amp; p 3 ).</p><p>To achieve e cient optimization, Mikolov et al. introduced negative sampling <ref type="bibr" target="#b17">[18]</ref>, in which a relatively small set of words (nodes) are sampled from the corpus (network) for the construction of somax. We leverage the same technique for metapath2vec. Given a negative sample size M, Eq. 2 is updated as follows</p><formula xml:id="formula_5">: log σ (X c t •X )+ M m=1 E u m ∼P (u ) [log σ (−X u m •X )], where σ (x ) = 1</formula><p>1+e −x and P (u) is the pre-de ned distribution from which a negative node u m is drew from for M times. metapath2vec builds the the node frequency distribution by viewing di erent types of nodes homogeneously and draw (negative) nodes regardless of their types. Meta-Path-Based Random Walks. How to e ectively transform the structure of a network into skip-gram? In DeepWalk <ref type="bibr" target="#b21">[22]</ref> and node2vec <ref type="bibr" target="#b7">[8]</ref>, this is achieved by incorporating the node paths traversed by random walkers over a network into the neighborhood function.</p><p>Naturally, we can put random walkers in a heterogeneous network to generate paths of multiple types of nodes. At step i, the transition probability p( i+1 | i ) is denoted as the normalized probability distributed over the neighbors of i by ignoring their node types.</p><p>e generated paths can be then used as the input of node2vec and DeepWalk. However, Sun et al. demonstrated that heterogeneous random walks are biased to highly visible types of nodes-those with a dominant number of paths-and concentrated nodes-those with a governing percentage of paths pointing to a small set of nodes <ref type="bibr" target="#b25">[26]</ref>.</p><p>In light of these issues, we design meta-path-based random walks to generate paths that are able to capture both the semantic and structural correlations between di erent types of nodes, facilitating the transformation of heterogeneous network structures into metapath2vec's skip-gram.</p><p>Formally, a meta-path scheme P is de ned as a path that is</p><formula xml:id="formula_6">denoted in the form of V 1 R 1 − − → V 2 R 2 − − → • • • V t R t − − → V t +1 • • • R l −1 − −−− → V l , wherein R = R 1 • R 2 • • • • • R l −1 de</formula><p>nes the composite relations between node types V 1 and V l <ref type="bibr" target="#b24">[25]</ref>. Take Figure <ref type="figure">2</ref>(a) as an example, a meta-path "APA" represents the coauthor relationships on a paper (P) between two authors (A), and "APVPA" represents two authors (A) publish papers (P) in the same venue (V). Previous work has shown that many data mining tasks in heterogeneous information networks can bene t from the modeling of meta-paths <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>.   </p><formula xml:id="formula_7">V = V V ∪ V A ∪ V O ∪ V P . k t speci</formula><formula xml:id="formula_8">= k V + k A + k O + k P .</formula><p>Here we show how to use meta-paths to guide heterogeneous random walkers. Given a heterogeneous network G = (V , E,T ) and a meta-path scheme</p><formula xml:id="formula_9">P: V 1 R 1 − − → V 2 R 2 − − → • • • V t R t − − → V t +1 • • • R l −1 − −−− → V l ,</formula><p>the transition probability at step i is de ned as follows:</p><formula xml:id="formula_10">p( i+1 | i t , P) =          1 |N t +1 ( i t ) | ( i+1 , i t ) ∈ E, ϕ ( i+1 ) = t+1 0 ( i+1 , i t ) ∈ E, ϕ ( i+1 ) t+1 0 ( i+1 , i t ) E<label>(3)</label></formula><p>where i t ∈ V t and N t +1 ( i t ) denote the V t +1 type of neighborhood of node i t . In other words, i+1 ∈ V t +1 , that is, the ow of the walker is conditioned on the pre-de ned meta-path P. In addition, meta-paths are commonly used in a symmetric way, that is, its rst node type V 1 is the same with the last one V l <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>, facilitating its recursive guidance for random walkers, i.e.,</p><formula xml:id="formula_11">p( i+1 | i t ) = p( i+1 | i 1 ), if t = l<label>(4)</label></formula><p>e meta-path-based random walk strategy ensures that the semantic relationships between di erent types of nodes can be properly incorporated into skip-gram. For example, in a traditional random walk procedure, in Figure <ref type="figure">2</ref>(a), the next step of a walker on node a 4 transitioned from node CMU can be all types of nodes surrounding it-a 2 , a 3 , a 5 , p 2 , p 3 , and CMU. However, under the meta-path scheme 'OAPVPAO', for example, the walker is biased towards paper nodes (P) given its previous step on an organization node CMU (O), following the semantics of this path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">metapath2vec++</head><p>metapath2vec distinguishes the context nodes of node conditioned on their types when constructing its neighborhood function N t ( ) in Eq. 2. However, it ignores the node type information in so max.</p><p>In other words, in order to infer the speci c type of context c t in N t ( ) given a node , metapath2vec actually encourages all types of negative samples, including nodes of the same type t as well as the other types in the heterogeneous network. Heterogeneous negative sampling. We further propose the metapath2vec++ framework, in which the so max function is normalized with respect to the node type of the context c t . Speci cally, p(c t | ; θ ) is adjusted to the speci c node type t, that is,</p><formula xml:id="formula_12">p(c t | ; θ ) = e X c t •X u t ∈V t e X u t •X (5)</formula><p>where V t is the node set of type t in the network. In doing so, metapath2vec++ speci es one set of multinomial distributions for each type of neighborhood in the output layer of the skip-gram model. Recall that in metapath2vec and node2vec / DeepWalk, the dimension of the output multinomial distributions is equal to the number of nodes in the network. However, in metapath2vec++'s skip-gram, the multinomial distribution dimension for type t nodes is determined by the number of t-type nodes. A clear illustration can be seen in Figure <ref type="figure">2</ref>(c). For example, given the target node a 4 in the input layer, metapath2vec++ outputs four sets of multinomial distributions, each corresponding to one type of neighbors-venues V , authors A, organizations O, and papers P.</p><p>Inspired by PTE <ref type="bibr" target="#b28">[29]</ref>, the sampling distribution is also speci ed by the node type of the neighbor c t that is targeted to predict, i.e., P t (•). erefore, we have the following objective:</p><formula xml:id="formula_13">O(X) = log σ (X c t • X ) + M m=1 E u m t ∼P t (u t ) [log σ (−X u m t • X )]<label>(6)</label></formula><p>Input: e heterogeneous information network G = (V , E,T ), a meta-path scheme P, #walks per node w, walk length l, embedding dimension d, neighborhood size k Output: e latent node embeddings X ∈ R |V |×d initialize X ;</p><formula xml:id="formula_14">for i = 1 → w do for ∈ V do MP = MetaPathRandomWalk(G, P, , l) ; X = HeterogeneousSkipGram(X, k, MP) ; end end return X ; MetaPathRandomWalk(G, P, , l) MP[1] = ; for i = 1 → l−1 do</formula><p>draw u according to Eq. 3 ;</p><formula xml:id="formula_15">MP[i+1] = u ; end return MP ; HeterogeneousSkipGram(X, k, MP) for i = 1 → l do = MP[i] ; for j = max(0, i-k) → min(i+k, l) &amp; j i do c t = MP[j] ; X new = X old − η • ∂ O(X)</formula><p>∂X (Eq. 7) ; end end ALGORITHM 1: e metapath2vec++ Algorithm.</p><p>whose gradients are derived as follows:</p><formula xml:id="formula_16">∂O(X) ∂X u m t = (σ (X u m t • X − I c t [u m t ]))X (7) ∂O(X) ∂X = M m=0 (σ (X u m t • X − I c t [u m t ]))X u m t where I c t [u m t</formula><p>] is an indicator function to indicate whether u m t is the neighborhood context node c t and when m = 0, u 0 t = c t . e model is optimized by using stochastic gradient descent algorithm.</p><p>e pseudo code of metapath2vec++ is listed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we demonstrate the e cacy and e ciency of the presented metapath2vec and metapath2vec++ frameworks for heterogeneous network representation learning. Data. We use two heterogeneous networks, including the AMiner Computer Science (CS) dataset <ref type="bibr" target="#b31">[31]</ref> and the Database and Information Systems (DBIS) dataset <ref type="bibr" target="#b25">[26]</ref>. Both datasets and code are publicly available 1 . is AMiner CS dataset consists of 9,323,739 1 e network data, learned latent representations, labeled ground truth data, and source code can be found at h ps://ericdongyx.github.io/metapath2vec/m2v.html computer scientists and 3,194,405 papers from 3,883 computer science venues-both conferences and journals-held until 2016. We construct a heterogeneous collaboration network, in which there are three types of nodes: authors, papers, and venues. e links represent di erent types of relationships among three sets of nodessuch as collaboration relationships on a paper. e DBIS dataset was constructed and used by Sun et al. <ref type="bibr" target="#b25">[26]</ref>. It covers 464 venues, their top-5000 authors, and corresponding 72,902 publications. We also construct the heterogeneous collaboration networks from DBIS wherein a link may connect two authors, one author and one paper, as well as one paper and one venue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We compare metapath2vec and metapath2vec++ with several recent network representation learning methods:</p><p>(1) DeepWalk <ref type="bibr" target="#b21">[22]</ref> / node2vec <ref type="bibr" target="#b7">[8]</ref>: With the same random walk path input (p=1 &amp; q=1 in node2vec), we nd that the choice between hierarchical so max (DeepWalk) and negative sampling (node2vec) techniques does not yield signi cant di erences. erefore we use p=1 and q=1 <ref type="bibr" target="#b7">[8]</ref> in node2vec for comparison. (2) LINE <ref type="bibr" target="#b29">[30]</ref>: We use the advanced version of LINE by considering both the 1st-and 2nd-order of node proximity; (3) PTE <ref type="bibr" target="#b28">[29]</ref>: We construct three bipartite heterogeneous networks (author-author, author-venue, venue-venue) and restrain it as an unsupervised embedding method; (4) Spectral Clustering <ref type="bibr" target="#b33">[33]</ref> / Graph Factorization <ref type="bibr" target="#b1">[2]</ref>: With the same treatment to these methods in node2vec <ref type="bibr" target="#b7">[8]</ref>, we exclude them from our comparison, as previous studies have demonstrated that they are outperformed by DeepWalk and LINE.</p><p>For all embedding methods, we use the same parameters listed below. In addition, we also vary each of them and x the others for examining the parameter sensitivity of the proposed methods. For metapath2vec and metapath2vec++, we also need to specify the meta-path scheme to guide random walks. We surveyed most of the meta-path-based work and found that the most commonly and e ectively used meta-path schemes in heterogeneous academic networks are "APA" and "APVPA" <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. Notice that "APA" denotes the coauthor semantic, that is, the traditional (homogeneous) collaboration links / relationships. "APVPA" represents the heterogeneous semantic of authors publishing papers at the same venues. Our empirical results also show that this simple meta-path scheme "APVPA" can lead to node embeddings that can be generalized to diverse heterogeneous academic mining tasks, suggesting its applicability to potential applications for academic search services.</p><p>We evaluate the quality of the latent representations learned by di erent methods over three classical heterogeneous network mining tasks, including multi-class node classi cation <ref type="bibr" target="#b12">[13]</ref>, node clustering <ref type="bibr" target="#b26">[27]</ref>, and similarity search <ref type="bibr" target="#b25">[26]</ref>. In addition, we also use the embedding projector in TensorFlow <ref type="bibr" target="#b0">[1]</ref> to visualize the node embeddings learned from the heterogeneous academic networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Class Classi cation</head><p>For the classi cation task, we use third-party labels to determine the class of each node. First, we match the eight categories<ref type="foot" target="#foot_0">2</ref> of venues in Google Scholar<ref type="foot" target="#foot_1">3</ref> with those in AMiner data. Among all of the 160 venues (20 per category × 8 categories), 133 of them are successfully matched and labeled correspondingly (Most of unmatched venues are pre-print venues, such as arXiv). Second, for each author who published in these 133 venues, his / her label is assigned to the category with the majority of his / her publications, and a tie is resolved by random selection among the possible categories; 246,678 authors are labeled with research category. Note that the node representations are learned from the full dataset. e embeddings of above labeled nodes are then used as the input to a logistic regression classi er. In the classi cation experiments, we vary the size of the training set from 5% to 90% and the remaining nodes for testing. We repeat each prediction experiment ten times and report the average performance in terms of both Macro-F1 and Micro-F1 scores.</p><p>Results. Tables <ref type="table" target="#tab_4">2 and 3</ref> list the eight-class classi cation results. Overall, the proposed metapath2vec and metapath2vec++ models consistently and signi cantly outperform all baselines in terms of both metrics. When predicting for the venue category, the advantage of both metapath2vec and metapath2vec++ are particular strong given a small size of training data. Given 5% of nodes as training data, for example, metapath2vec and metapath2vec++ achieve 0.08-0.23 (relatively 35-319%) improvements in terms of Macro-F1 and 0.13-0.26 (relatively 39-145%) gains in terms of Micro-F1 over DeepWalk / node2vec, LINE, and PTE. When predicting for authors' categories, the performance of each method is relatively stable when varying the train-test split. e constant gain achieved by the proposed methods is around 2-3% over LINE and PTE, and ∼20% over DeepWalk / node2vec.</p><p>In summary, metapath2vec and metapath2vec++ learn signicantly be er heterogeneous node embeddings than current stateof-the-art methods, as measured by multi-class classi cation performance. e advantage of the proposed methods lies in their proper consideration and accommodation of the network heterogeneity challenge-the existence of multiple types of nodes and relations. Parameter sensitivity. In skip-gram-based representation learning models, there exist several common parameters (see Section 4.1). We conduct a sensitivity analysis of metapath2vec++ to these parameters. Figure <ref type="figure">3</ref> shows the classi cation results as a function   e increase of author classi cation performance converges as w and l reach around 1000 and 100, respectively. Similarly, Figures <ref type="figure">3(c</ref>) and 3(d) suggest that the number of embedding dimensions d and neighborhood size k are again of relatively li le relevance to the predictive task for venues, and k on the other hand is positively crucial to determine the class of a venue. However, the descending lines as the increase of k for author classi cations imply that a smaller neighborhood size actually produces the best embeddings for separating authors.</p><p>is nding di ers from those in a homogeneous environment <ref type="bibr" target="#b7">[8]</ref>, wherein the neighborhood size generally shows a positive e ect on node classi cation.</p><p>According to the analysis, metapath2vec++ is not strictly sensitive to these parameters and is able to reach high performance under a cost-e ective parameter choice (the smaller, the more efcient). In addition, our results also indicate that those common parameters show di erent functions for heterogeneous network embedding with those in homogeneous network cases, demonstrating the request of di erent ideas and solutions for heterogeneous network representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Node Clustering</head><p>We illustrate how the latent representations learned by embedding methods can help the node clustering task in heterogeneous networks. We employ the same eight-category author and venue nodes used in the classi cation task above. e learned embeddings by each method is input to a clustering model. Here we leverage the k-means algorithm to cluster the data and evaluate the clustering results in terms of normalized mutual information (NMI) <ref type="bibr" target="#b25">[26]</ref>. In addition, we also report metapath2vec++'s sensitivity with respect to di erent parameter choices. All clustering experiments are conducted 10 times and the average performance is reported. Results. Table <ref type="table" target="#tab_5">4</ref> shows the node clustering results as measured by NMI in the AMiner CS data. Overall, the table demonstrates that metapath2vec and metapath2vec++ outperform all the comparative methods. When clustering for venues, the task is trivial as evident from the high NMI scores produced by most of the methods: metapath2vec, metapath2vec++, LINE, and PTE. Nevertheless, the proposed two methods outperform LINE and PTE by 2-3%. e author clustering task is more challenging than the venue case, and the gain obtained by metapath2vec and metapath2vec++ over the best baselines (LINE and PTE) is more signi cant-around 13-16%.</p><p>In summary, metapath2vec and metapath2vec++ generate more appropriate embeddings for di erent types of nodes in the network than comparative baselines, suggesting their ability to capture and incorporate the underlying structural and semantic relationships between various types of nodes in heterogeneous networks. Parameter sensitivity. Following the same experimental procedure in classi cation, we study the parameter sensitivity of meta-path2vec++ as measured by the clustering performance. Figure <ref type="figure">4</ref> shows the clustering performance as a function of each of the four parameters when xing the other three. From Figures <ref type="figure">4(a</ref>) and 4(b), we can observe that the balance between computational cost (a small w and l in x-axis) and e cacy (a high NMI in -axis) can be achieved at around w = 800∼1000 and l = 100 for the clustering of both authors and venues. Further, di erent from the positive e ect of increasing w and l on author clustering, d and k are negatively correlated with the author clustering performance, as observed from Figures <ref type="figure">4(c</ref>) and 4(d). Similarly, the venue clustering performance also shows an descending trend with an increasing d, while on the other hand, we observe a rst-increasing and then-decreasing NMI line when k is increased. Both gures together imply that d = 128 and k = 7 are capable of embedding heterogeneous nodes into latent space for promising clustering outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study: Similarity Search</head><p>We conduct two case studies to demonstrate the e cacy of our methods. We select 16 top CS conferences from the corresponding sub-elds in the AMiner CS data and another 5 from the DBIS data. is results in a total of 21 query nodes. We use cosine similarity to determine the distance (similarity) between the query node and the remaining others.</p><p>Table <ref type="table" target="#tab_6">5</ref> lists the top ten similar results for querying the 16 leading conferences in corresponding computer science sub-elds. One can observe that for the query "ACL", for example, metapath2vec++ returns venues with the same focus-natural language processing, such as EMNLP (1 st ), NAACL (2 nd ), Computational Linguistics (3 r d ), CoNLL (4 th ), COLING (5 t h ), and so on. Similar performance can be also achieved when querying the other conferences from various elds. More surprisingly, we nd that in most cases, the top three results cover venues with similar prestige to the query one, such as STOC to FOCS in theory, OSDI to SOSP in system, HPCA to ISCA in architecture, CCS to S&amp;P in security, CSCW to CHI in human-computer interaction, EMNLP to ACL in NLP, ICML to NIPS in machine learning, WSDM to WWW in Web, AAAI to IJCAI in arti cial intelligence, PVLDB to SIGMOD in database, etc. Similar results can also be observed in Tables <ref type="table" target="#tab_7">6 and 1</ref>, which show the similarity search results for the DBIS network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study: Visualization</head><p>We employ the TensorFlow embedding projector to further visualize the low-dimensional node representations learned by embedding As to metapath2vec, instead of separating the two types of nodes into two columns, it is capable of grouping each pair of one venue and its corresponding author closely, such as R. E. Tarjan and FOCS, H. Jensen and SIGGRAPH, H. Ishli and CHI, R. Agrawal and SIG-MOD, etc. Together, both models arrange nodes from similar elds close to each other and dissimilar ones distant from each other, such as the "Core CS" cluster of systems (OSDI), networking (SIGCOMM), security (S&amp;P), and architecture (ISCA), as well as the "Big AI" cluster of data mining (KDD), information retrieval (SIGIR), arti cial intelligence (AI), machine learning (NIPS), NLP (ACL), and vision (CVPR). ese groupings are also re ected by their corresponding author nodes.</p><p>Second, Figure <ref type="figure">5</ref> visualizes the latent vectors-learned by meta-path2vec++-of 48 venues used in similarity search of Section 4.4,  three each from 16 sub-elds. We can see that conferences from the same domain are geographically grouped to each other and each group is well separated from others, further demonstrating the embedding ability of metapath2vec++. In addition, similar to the observation in Figure <ref type="figure" target="#fig_1">1</ref>, we can also notice that the heterogeneous embeddings are able to unveil the similarities across di erent domains, including the "Core CS" sub-eld cluster at the bo om right and the "Big AI" sub-eld cluster at the top right. us, Figures <ref type="figure" target="#fig_1">1 and 5</ref> intuitively demonstrate metapath2vec++'s novel capability to discover, model, and capture the underlying structural and semantic relationships between multiple types of nodes in heterogeneous networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Scalability</head><p>In the era of big (network) data, it is necessary to demonstrate the scalability of the proposed network embedding models. e metapath2vec and metapath2vec++ methods can be parallelized by using the same mechanism as word2vec and node2vec <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. All codes are implemented in C and C++ and our experiments are conducted in a computing server with ad 12 (48) core 2.3 GHz Intel Xeon CPUs E7-4850. We run experiments on the AMiner CS data with the default parameters with di erent number of threads, i.e., 1, 2, 4, 8, 16, 24, 32, 40, each of them utilizing one CPU core.</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows the speedup of metapath2vec &amp; metapath2vec++ over the single-threaded case. Optimal speedup performance is denoted by the dashed = x line, which represents perfect distribution and execution of computation across all CPU cores. In general, we nd that both methods achieve acceptable sublinear speedups as both lines are close to the optimal line. In speci c, they can reach 11-12× speedup with 16 cores and 24-32× speedup with 40 cores used. By using 40 cores, metapath2vec++'s learning process costs only 9 minutes for embedding the full AMiner CS network, which is composed of over 9 million authors with 3 million papers published in more than 3800 venues. Overall, the proposed metapath2vec and metapath2vec++ models are e cient and scalable for large-scale heterogeneous networks with millions of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">34]</ref>, such as the application of factorization models for recommendation systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, node classi cation <ref type="bibr" target="#b32">[32]</ref>, relational mining <ref type="bibr" target="#b18">[19]</ref>, and role discovery <ref type="bibr" target="#b8">[9]</ref>. is rich line of research focuses on factorizing the matrix/tensor format (e.g., the adjacency matrix) of a network, generating latent-dimension features for nodes or edges in this network. However, the computational cost of decomposing a large-scale matrix/tensor is usually very expensive, and also su ers from its statistical performance drawback <ref type="bibr" target="#b7">[8]</ref>, making it neither practical nor e ective for addressing tasks in big networks.</p><p>With the advent of deep learning techniques, signi cant e ort has been devoted to designing neural network-based representation learning models. For example, Mikolov et al. proposed the word2vec framework-a two-layer neural network-to learn the distributed representations of words in natural language <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Building on word2vec, Perozzi et al. suggested that the "context" of a node can be denoted by their co-occurrence in a random walk path <ref type="bibr" target="#b21">[22]</ref>. Formally, they put random walkers over networks to record their walking paths, each of which is composed of a chain of nodes that could be considered as a "sentence" of words in a text corpus. More recently, in order to diversify the neighborhood of a node, Grover &amp; Leskovec presented biased random walkers-a mixture of breadth-rst and width-rst search procedures-over networks to produce paths of nodes <ref type="bibr" target="#b7">[8]</ref>. With node paths generated, both works leveraged the skip-gram architecture in word2vec to model the structural correlations between nodes in a path. In addition, several other methods have been proposed for learning representations in networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. In particular, to learn network embeddings, Tang et al. decomposed a node's context into rst-order (friends) and second-order (friends' friends) proximity <ref type="bibr" target="#b29">[30]</ref>, which was further developed into a semi-supervised model PTE for embedding text data <ref type="bibr" target="#b28">[29]</ref>.</p><p>Our work furthers this direction of investigation by designing the metapath2vec and metapath2vec++ models to capture heterogeneous structural and semantic correlations exhibited from largescale networks with multiple types of nodes, which can not be handled by previous models, and applying these models to a variety of network mining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we formally de ne the representation learning problem in heterogeneous networks in which there exist diverse types of nodes and links. To address the network heterogeneity challenge, we propose the metapath2vec and metapath2vec++ methods. We develop the meta-path-guided random walk strategy in a heterogeneous network, which is capable of capturing both the structural and semantic correlations of di erently typed nodes and relations. To leverage this method, we formalize the heterogeneous neighborhood function of a node, enabling the skip-gram-based maximization of the network probability in the context of multiple types of nodes. Finally, we achieve e ective and e cient optimization by presenting a heterogeneous negative sampling technique. Extensive experiments demonstrate that the latent feature representations learned by metapath2vec and metapath2vec++ are able to improve various heterogeneous network mining tasks, such as similarity search, node classi cation, and clustering. Our results can be naturally applied to real-world applications in heterogeneous academic networks, such as author, venue, and paper search in academic search services.</p><p>Future work includes various optimizations and improvements. For example, 1) the metapath2vec and metapath2vec++ models, as is also the case with DeepWalk and node2vec, face the challenge of large intermediate output data when sampling a network into a huge pile of paths, and thus identifying and optimizing the sampling space is an important direction; 2) as is also the case with all metapath-based heterogeneous network mining methods, metapath2vec and metapath2vec++ can be further improved by the automatic learning of meaningful meta-paths; 3) extending the models to incorporate the dynamics of evolving heterogeneous networks; and 4) generalizing the models for di erent genres of heterogeneous networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 2D PCA projections of the 128D embeddings of 16 top CS conferences and corresponding high-pro le authors.</figDesc><graphic url="image-4.png" coords="1,459.63,298.18,84.98,82.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>walks per node w: 1000; (2) e walk length l: 100; (3) e vector dimension d: 128 (LINE: 128 for each order); (4) e neighborhood size k: 7; (5) e size of negative samples: 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>#walks per node w 1 Figure 3 :</head><label>13</label><figDesc>Figure 3: Parameter sensitivity in multi-class node classi cation. 50% as training data and the remaining as test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) and 3(b) the number of walks w rooting from each node and the length l of each walk are positive to the author classi cation performance, while they are surprisingly inconsequential for inferring venue nodes' categories as measured by Macro-F1 and Micro-F1 scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Parameter sensitivity in clustering.</figDesc><graphic url="image-5.png" coords="8,79.34,231.68,191.39,159.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Scalability of metapath2vec and metapath2vec++.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Multi-class venue node classi cation results in AMiner data.</figDesc><table><row><cell>Metric</cell><cell>Method</cell><cell>5%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell></cell><cell cols="11">DeepWalk/node2vec 0.0723 0.1396 0.1905 0.2795 0.3427 0.3911 0.4424 0.4774 0.4955 0.4457</cell></row><row><cell></cell><cell>LINE (1st+2nd)</cell><cell cols="10">0.2245 0.4629 0.7011 0.8473 0.8953 0.9203 0.9308 0.9466 0.9410 0.9466</cell></row><row><cell>Macro-F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PTE</cell><cell cols="10">0.1702 0.3388 0.6535 0.8304 0.8936 0.9210 0.9352 0.9505 0.9525 0.9489</cell></row><row><cell></cell><cell>metapath2vec</cell><cell cols="10">0.3033 0.5247 0.8033 0.8971 0.9406 0.9532 0.9529 0.9701 0.9683 0.9670</cell></row><row><cell></cell><cell>metapath2vec++</cell><cell cols="10">0.3090 0.5444 0.8049 0.8995 0.9468 0.9580 0.9561 0.9675 0.9533 0.9503</cell></row><row><cell></cell><cell cols="11">DeepWalk/node2vec 0.1701 0.2142 0.2486 0.3266 0.3788 0.4090 0.4630 0.4975 0.5259 0.5286</cell></row><row><cell></cell><cell>LINE (1st+2nd)</cell><cell cols="10">0.3000 0.5167 0.7159 0.8457 0.8950 0.9209 0.9333 0.9500 0.9556 0.9571</cell></row><row><cell>Micro-F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PTE</cell><cell cols="10">0.2512 0.4267 0.6879 0.8372 0.8950 0.9239 0.9352 0.9550 0.9667 0.9571</cell></row><row><cell></cell><cell>metapath2vec</cell><cell cols="10">0.4173 0.5975 0.8327 0.9011 0.9400 0.9522 0.9537 0.9725 0.9815 0.9857</cell></row><row><cell></cell><cell>metapath2vec++</cell><cell cols="10">0.4331 0.6192 0.8336 0.9032 0.9463 0.9582 0.9574 0.9700 0.9741 0.9786</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Multi-class author node classi cation results in AMiner data.</figDesc><table><row><cell>Metric</cell><cell>Method</cell><cell>5%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell></cell><cell cols="11">DeepWalk/node2vec 0.7153 0.7222 0.7256 0.7270 0.7273 0.7274 0.7273 0.7271 0.7275 0.7275</cell></row><row><cell></cell><cell>LINE (1st+2nd)</cell><cell cols="10">0.8849 0.8886 0.8911 0.8921 0.8926 0.8929 0.8934 0.8936 0.8938 0.8934</cell></row><row><cell>Macro-F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PTE</cell><cell cols="10">0.8898 0.8940 0.897 0.8982 0.8987 0.8990 0.8997 0.8999 0.9002 0.9005</cell></row><row><cell></cell><cell>metapath2vec</cell><cell cols="10">0.9216 0.9262 0.9292 0.9303 0.9309 0.9314 0.9315 0.9316 0.9319 0.9320</cell></row><row><cell></cell><cell>metapath2vec++</cell><cell cols="10">0.9107 0.9156 0.9186 0.9199 0.9204 0.9207 0.9207 0.9208 0.9211 0.9212</cell></row><row><cell></cell><cell cols="11">DeepWalk/node2vec 0.7312 0.7372 0.7402 0.7414 0.7418 0.7420 0.7419 0.7420 0.7425 0.7425</cell></row><row><cell></cell><cell>LINE (1st+2nd)</cell><cell cols="10">0.8936 0.8969 0.8993 0.9002 0.9007 0.9010 0.9015 0.9016 0.9018 0.9017</cell></row><row><cell>Micro-F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PTE</cell><cell cols="10">0.8986 0.9023 0.9051 0.9061 0.9066 0.9068 0.9075 0.9077 0.9079 0.9082</cell></row><row><cell></cell><cell>metapath2vec</cell><cell cols="10">0.9279 0.9319 0.9346 0.9356 0.9361 0.9365 0.9365 0.9365 0.9367 0.9369</cell></row><row><cell></cell><cell>metapath2vec++</cell><cell cols="10">0.9173 0.9217 0.9243 0.9254 0.9259 0.9261 0.9261 0.9262 0.9264 0.9266</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Node clustering results (NMI) in AMiner data.</figDesc><table><row><cell>methods</cell><cell>venue author</cell></row><row><cell cols="2">DeepWalk/node2vec 0.1952 0.2941</cell></row><row><cell>LINE (1st+2nd)</cell><cell>0.8967 0.6423</cell></row><row><cell>PTE</cell><cell>0.9060 0.6483</cell></row><row><cell>metapath2vec</cell><cell>0.9274 0.7470</cell></row><row><cell>metapath2vec++</cell><cell>0.9261 0.7354</cell></row></table><note>of one chosen parameter when the others are controlled for. In general, we nd that in Figures3(a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Case study of similarity search in AMiner Data</figDesc><table><row><cell>Rank</cell><cell>ACL</cell><cell>NIPS</cell><cell>IJCAI</cell><cell>CVPR</cell><cell>FOCS</cell><cell>SOSP</cell><cell>ISCA</cell><cell>S&amp;P</cell><cell>ICSE</cell><cell cols="2">SIGGRAPH SIGCOMM</cell><cell>CHI</cell><cell>KDD</cell><cell>SIGMOD</cell><cell>SIGIR</cell><cell>WWW</cell></row><row><cell>0</cell><cell>ACL</cell><cell>NIPS</cell><cell>IJCAI</cell><cell>CVPR</cell><cell>FOCS</cell><cell>SOSP</cell><cell>ISCA</cell><cell>S&amp;P</cell><cell>ICSE</cell><cell cols="2">SIGGRAPH SIGCOMM</cell><cell>CHI</cell><cell>KDD</cell><cell>SIGMOD</cell><cell>SIGIR</cell><cell>WWW</cell></row><row><cell>1</cell><cell>EMNLP</cell><cell>ICML</cell><cell>AAAI</cell><cell>ECCV</cell><cell>STOC</cell><cell>TOCS</cell><cell>HPCA</cell><cell>CCS</cell><cell>TOSEM</cell><cell>TOG</cell><cell>CCR</cell><cell>CSCW</cell><cell>SDM</cell><cell>PVLDB</cell><cell>ECIR</cell><cell>WSDM</cell></row><row><cell>2</cell><cell cols="2">NAACL AISTATS</cell><cell>AI</cell><cell>ICCV</cell><cell>SICOMP</cell><cell>OSDI</cell><cell>MICRO</cell><cell>NDSS</cell><cell>FSE</cell><cell>SI3D</cell><cell>HotNets</cell><cell>TOCHI</cell><cell>TKDD</cell><cell>ICDE</cell><cell>CIKM</cell><cell>CIKM</cell></row><row><cell>3</cell><cell>CL</cell><cell>JMLR</cell><cell>JAIR</cell><cell>IJCV</cell><cell>SODA</cell><cell>HotOS</cell><cell cols="2">ASPLOS USENIX S</cell><cell>ASE</cell><cell>RT</cell><cell>NSDI</cell><cell>UIST</cell><cell>ICDM</cell><cell>DE Bull</cell><cell>IR J</cell><cell>TWEB</cell></row><row><cell>4</cell><cell>CoNLL</cell><cell>NC</cell><cell>ECAI</cell><cell>ACCV</cell><cell>A-R</cell><cell>SIGOPS E</cell><cell>PACT</cell><cell>ACSAC</cell><cell>ISSTA</cell><cell>CGF</cell><cell>CoNEXT</cell><cell>DIS</cell><cell>DMKD</cell><cell>VLDBJ</cell><cell cols="2">TREC ICWSM</cell></row><row><cell>5</cell><cell>COLING</cell><cell>MLJ</cell><cell>KR</cell><cell>CVIU</cell><cell>TALG</cell><cell>ATC</cell><cell>ICS</cell><cell>JCS</cell><cell>E SE</cell><cell>NPAR</cell><cell>IMC</cell><cell>HCI</cell><cell>KDD E</cell><cell>EDBT</cell><cell>SIGIR F</cell><cell>HT</cell></row><row><cell>6</cell><cell>IJCNLP</cell><cell>COLT</cell><cell>AI Mag</cell><cell>BMVC</cell><cell>ICALP</cell><cell>NSDI</cell><cell cols="2">HiPEAC ESORICS</cell><cell>MSR</cell><cell>Vis</cell><cell>TON</cell><cell cols="2">MobileHCI WSDM</cell><cell>TODS</cell><cell>ICTIR</cell><cell>SIGIR</cell></row><row><cell>7</cell><cell>NLE</cell><cell>UAI</cell><cell>ICAPS</cell><cell>ICPR</cell><cell>ECCC</cell><cell>OSR</cell><cell>PPOPP</cell><cell>TISS</cell><cell>ESEM</cell><cell>JGT</cell><cell cols="3">INFOCOM INTERACT CIKM</cell><cell>CIDR</cell><cell>WSDM</cell><cell>KDD</cell></row><row><cell>8</cell><cell>ANLP</cell><cell>KDD</cell><cell>CI</cell><cell>EMMCVPR</cell><cell>TOC</cell><cell>ASPLOS</cell><cell>ICCD</cell><cell>ASIACCS</cell><cell>A SE</cell><cell>VisComp</cell><cell>PAM</cell><cell>GROUP</cell><cell cols="2">PKDD SIGMOD R</cell><cell>TOIS</cell><cell>TIT</cell></row><row><cell>9</cell><cell>LREC</cell><cell>CVPR</cell><cell>AIPS</cell><cell>T on IP</cell><cell>JAlG</cell><cell>EuroSys</cell><cell>CGO</cell><cell>RAID</cell><cell>ICPC</cell><cell>GI</cell><cell>MobiCom</cell><cell>NordiCHI</cell><cell>ICML</cell><cell>WebDB</cell><cell>IPM</cell><cell>WISE</cell></row><row><cell>10</cell><cell>EACL</cell><cell>ECML</cell><cell>UAI</cell><cell>WACV</cell><cell>ITCS</cell><cell cols="2">SIGCOMM ISLPED</cell><cell>CSFW</cell><cell>WICSA</cell><cell>CG</cell><cell>IPTPS</cell><cell cols="2">UbiComp PAKDD</cell><cell>PODS</cell><cell>AIRS</cell><cell>WebSci</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Case study of similarity search in DBIS Data</figDesc><table><row><cell>Rank</cell><cell>KDD</cell><cell cols="2">SIGMOD SIGIR</cell><cell>WWW</cell><cell>WSDM</cell></row><row><cell>0</cell><cell>KDD</cell><cell cols="2">SIGMOD SIGIR</cell><cell>WWW</cell><cell>WSDM</cell></row><row><cell>1</cell><cell>SDM</cell><cell>PVLDB</cell><cell>TREC</cell><cell>CIKM</cell><cell>WWW</cell></row><row><cell>2</cell><cell>ICDM</cell><cell>ICDE</cell><cell>CIKM</cell><cell>SIGIR</cell><cell>SIGIR</cell></row><row><cell>3</cell><cell>DMKD</cell><cell>TODS</cell><cell>IPM</cell><cell>KDD</cell><cell>KDD</cell></row><row><cell>4</cell><cell>KDD E</cell><cell>VLDBJ</cell><cell>IRJ</cell><cell>ICDE</cell><cell>AIRWeb</cell></row><row><cell>5</cell><cell>PKDD</cell><cell>PODS</cell><cell>ECIR</cell><cell>TKDE</cell><cell>CIKM</cell></row><row><cell>6</cell><cell>PAKDD</cell><cell>EDBT</cell><cell>TOIS</cell><cell>VLDB</cell><cell>WebDB</cell></row><row><cell>7</cell><cell>TKDE</cell><cell>CIDR</cell><cell>WWW</cell><cell>TOTT</cell><cell>ICDM</cell></row><row><cell>8</cell><cell>CIKM</cell><cell>TKDE</cell><cell cols="3">JASIST SIGMOD VLDB</cell></row><row><cell>9</cell><cell>ICDE</cell><cell>ICDT</cell><cell>JASIS</cell><cell>WebDB</cell><cell>VLDBJ</cell></row><row><cell>10</cell><cell>TKDD</cell><cell cols="2">DE Bull SIGIR F</cell><cell>WISE</cell><cell>SDM</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">1. Computational Linguistics, 2. Computer Graphics,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">. Computer Networks &amp; Wireless Communication,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">. Computer Vision &amp; Pa ern Recognition,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">. Computing Systems,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">. Databases &amp; Information Systems,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">. Human Computer Interaction, and</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">. eoretical Computer Science.<ref type="bibr" target="#b2">3</ref> h ps://scholar.google.com/citations?view op=top venues&amp;hl=en&amp;vq=eng. Accessed on February, 2017.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We would like to thank Reid Johnson for discussions and suggestions. is work is supported by the Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053 and the National Science Foundation (NSF) grants CNS-1629914 and IIS-1447795.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><surname>Rey Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma</forename><surname>Hieu Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;16</title>
				<imprint>
			<publisher>Geo rey Irving</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>and others</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed Large-scale Natural Graph Factorization</title>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;13</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Heterogeneous Network Embedding via Deep Architectures</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identi cation</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CoupledLP: Link Prediction in Coupled Networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">word2vec Explained: deriving Mikolov et al. &apos;s negative-sampling word-embedding method</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno>CoRR abs/1402.3722</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Node2Vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;16</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rolx: structural role extraction &amp; mining in large graphs</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;12</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1231" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent space approaches to social network analysis</title>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Peter D Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Ra Ery</surname></persName>
		</author>
		<author>
			<persName><surname>Handcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1090" to="1098" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Label Informed A ributed Network Embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;17. na</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta structure: Computing relevance in large heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Mamoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;16</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ranking-based classi cation of heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Danilevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;11</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1298" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative ltering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;08</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geo</forename><surname>Rey Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;11</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">E cient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><surname>Rey Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<ptr target="//arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS &apos;13</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Leveraging relational autocorrelation with latent group models</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th international workshop on Multi-relational mining</title>
				<meeting>the 4th international workshop on Multi-relational mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asymmetric Transitivity Preserving Graph Embedding</title>
		<author>
			<persName><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;16</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning for network analysis: Problems, approaches and challenges</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><surname>Ramanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Military Communications Conference</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="588" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;14</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Label noise reduction in entity typing by heterogeneous partial-label embedding</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;16</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Rong</surname></persName>
		</author>
		<idno>CoRR abs/1411.2738</idno>
		<ptr target="//arxiv.org/abs/1411.2738" />
		<title level="m">word2vec Parameter Learning Explained</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mining Heterogeneous Information Networks: Principles and Methodologies</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB &apos;11</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Integrating Meta-path Selection with User-guided Object Clustering in Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;12</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1348" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ranking-based Clustering of Heterogeneous Information Networks with Star Network Schema</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yintao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;09</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PTE: Predictive Text Embedding rough Large-scale Heterogeneous Text Networks</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><surname>Mei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ar-netMiner: Extraction and Mining of Academic Social Networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;08</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;09</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Leveraging social media networks for classi cation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DMKD</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="447" to="478" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: A general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Panther: Fast top-k similarity search on large networks</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1445" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
