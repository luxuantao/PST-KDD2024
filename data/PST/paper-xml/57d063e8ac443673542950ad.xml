<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMAGE SENTIMENT ANALYSIS USING LATENT CORRELATIONS AMONG VISUAL, TEXTUAL, AND SENTIMENT VIEWS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Marie</forename><surname>Katsurai</surname></persName>
							<email>katsurai@mm.doshisha.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems Design</orgName>
								<orgName type="institution">Doshisha University Kyoto</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shin&apos;ichi Satoh Digital Content and Media Sciences Research Division</orgName>
								<orgName type="institution">National Institute of Informatics Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IMAGE SENTIMENT ANALYSIS USING LATENT CORRELATIONS AMONG VISUAL, TEXTUAL, AND SENTIMENT VIEWS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image sentiment analysis</term>
					<term>multi-view embedding</term>
					<term>canonical correlation analysis</term>
					<term>SentiWordNet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As Internet users increasingly post images to express their daily sentiment and emotions, the analysis of sentiments in user-generated images is of increasing importance for developing several applications. Most conventional methods of image sentiment analysis focus on the design of visual features, and the use of text associated to the images has not been sufficiently investigated. This paper proposes a novel approach that exploits latent correlations among multiple views: visual and textual views, and a sentiment view constructed using SentiWordNet. In the proposed method, we find a latent embedding space in which correlations among the three views are maximized. The projected features in the latent space are used to train a sentiment classifier, which considers the complementary information from different views. Results of experiments conducted on Flickr and Instagram images show that our approach achieves better sentiment classification accuracy than methods that use a single modality only and the state-of-the art method that jointly uses multiple modalities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the popularity of image capturing devices and social media platforms, we have seen a dramatic increase in our ability to collect digital images in various situations and share them on the Web. Two pertinent examples that are currently popular are Flickr, which hosted over 10 billion photos in 2015 <ref type="bibr" target="#b8">[1]</ref>, and Instagram, which has grown to have more than 400 million monthly active users <ref type="bibr" target="#b9">[2]</ref>. These images uploaded by Internet users can be considered to reflect visual aspects of their daily lives. Such ever-growing user-generated images have potential as a new information source to analyze users' opinions and sentiment, which enables several applications including opinion mining about social events, product marketing, and affective human-machine interaction <ref type="bibr" target="#b10">[3]</ref>. Thus, automatic inference of the sentiment implied in the images has received increasing research attention in recent years <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" target="#b13">[6]</ref><ref type="bibr" target="#b14">[7]</ref>.</p><p>Conventional methods of image sentiment analysis have aimed to design effective visual features for training sentiment polarity classifiers <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" target="#b13">[6]</ref>. However, due to the affective gap between lowlevel visual features and high-level concepts of human sentiments, This research has been partly funded by Harris Science Research Institute of Doshisha University.</p><p>it is difficult to directly associate the visual features with sentiment labels. On the other hand, studies about image annotation, not particularly focusing on sentiment analysis, have reported that the collaborative use of textual features around training images (e.g., tags and descriptions) can improve the image content recognition <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9]</ref>. Inspired from these studies, to bridge images and sentiment, we should investigate how to introduce additional views obtained from textual information to the feature space for training a sentiment classifier.</p><p>In this paper, we present a novel image sentiment analysis method that uses latent correlations among visual, textual, and sentiment views of training images. In the proposed method, we first extract features from pairs of images and text to construct visual and textual views. To highlight the sentiment information in the text, we introduce an external sentiment knowledge base, Senti-WordNet <ref type="bibr" target="#b17">[10]</ref>, which forms the sentiment view. Then, using a framework of multi-view canonical correlation analysis (CCA) <ref type="bibr" target="#b18">[11]</ref>, we calculate a latent embedding space in which correlations among the three views are maximized. Specifically, to capture the nonlinear relationship between features, we introduce explicit feature maps <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b20">13]</ref> to CCA. Finally, using the features that are projected to the latent embedding space, we train a sentiment classifier. Because the latent space learns the alignments of multiple views, our method corresponds to effectively exploiting the textual information of the training images even if a testing image only has a visual view. Our experiments were conducted on a collection of images from Flickr and Instagram, to which sentiment labels were assigned via crowdsourcing. Results of the experiments show that our three-view approach outperforms the conventional methods.</p><p>In summary, the main contributions of this paper are twofold: (i) most conventional methods use only visual features of training images, while we propose a novel image sentiment classification method that can exploit visual, textual, and sentiment views of the training images; and (ii) with experiments designed via crowdsourcing, we show that the complementary use of multiple views of the images can classify image sentiment better than the conventional methods do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The idea of associating low-level visual features with sentiments has been investigated based on psychology and art theory using relatively small and controlled datasets <ref type="bibr" target="#b21">[14,</ref><ref type="bibr" target="#b22">15]</ref>, while recent works have started to analyze the sentiments of unconstrained real-world images on social media <ref type="bibr" target="#b11">[4]</ref><ref type="bibr" target="#b12">[5]</ref><ref type="bibr" target="#b13">[6]</ref><ref type="bibr" target="#b14">[7]</ref>. Typically, the goal is to determine the sentiment polarity of images, i.e., positive or negative. To train a sen-timent polarity classifier, color histogram and SIFT-based features of images are used in <ref type="bibr" target="#b11">[4]</ref>. In <ref type="bibr" target="#b12">[5]</ref>, emotion-related adjective-noun pairs were selected for image sentiment analysis, and their classifiers, called SentiBank, were trained based on low-level visual features. The detector response of SentiBank was used to form a midlevel representation of an image. Similarly, attribute features including facial expression were used as mid-level features in <ref type="bibr" target="#b13">[6]</ref>. These conventional methods focus on how to design visual representation for sentiment analysis, and other available views of the data (e.g., tag concurrence) are discarded in training classifiers. Recently, Wang et al. <ref type="bibr" target="#b14">[7]</ref> exploited both visual content and textual information for sentiment-based image clustering in a nonnegative matrix factorization framework. However, the method in <ref type="bibr" target="#b14">[7]</ref> has severe sensitivity to the initialization, and the experiments in this paper demonstrate that our method outperforms the conventional method.</p><p>The use of correlations among visual and textual features associated to images has improved several image annotation and crossmodal retrieval tasks <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9,</ref><ref type="bibr" target="#b23">[16]</ref><ref type="bibr" target="#b24">[17]</ref><ref type="bibr" target="#b25">[18]</ref><ref type="bibr" target="#b26">[19]</ref><ref type="bibr" target="#b27">[20]</ref>, but its effectiveness has not been fully demonstrated in image sentiment analysis. Thus, this paper aims to use the latent correlations among multiple views for better sentiment analysis. Canonical correlation analysis (CCA) <ref type="bibr" target="#b28">[21]</ref> is one of the techniques typically used to learn the alignments of multiple views, but it only models the linear relationship between random variables. Several nonlinear extensions such as kernel CCA <ref type="bibr" target="#b18">[11]</ref> and Deep CCA <ref type="bibr" target="#b29">[22]</ref> have been proposed to reveal nonlinear relationship between the variables. However, these methods are intractable for large-scale datasets due to their high computational complexity and memory use. In contrast, recent advances of explicit feature maps <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b20">13]</ref> can convert nonlinear problems to linear problems, which can be solved by linear frameworks with a low computation cost <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b30">23]</ref>. Following these studies, we introduce the explicit feature maps to CCA in the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMAGE SENTIMENT ANALYSIS USING LATENT CORRELATIONS AMONG MULTIPLE VIEWS</head><p>This section presents a novel image sentiment analysis method that uses latent correlations among multiple views. An overview of the proposed method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. As shown, we first extract features from each view (See 3.1). Then, after learning the multiview embedding space (See 3.2), the latent embedding space is used to train an image sentiment polarity classifier (See 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Design of views for learning a latent embedding space</head><p>Our image sentiment analysis approach exploits three types of features: visual, textual, and sentiment views. This subsection describes the details of feature extraction from each view.</p><p>Visual features: Following the feature design used in recent visual classification methods <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b25">18,</ref><ref type="bibr" target="#b26">19]</ref>, we represent image appearance using a combination of different visual descriptors: a 3× 256 dimensional histogram extracted from RGB color channels, a 512 dimensional GIST descriptor, a Bag-of-Words quantized descriptor using a 1, 000 word dictionary with a 2-layer spatial pyramid and max pooling. We also extract the following mid-level features: 2,000-dimensional attribute features <ref type="bibr" target="#b31">[24]</ref> and 1,200-dimensional SentiBank outputs <ref type="bibr" target="#b12">[5]</ref>.</p><p>For GIST features, attribute features, and SentiBank features, we use the random Fourier feature mapping <ref type="bibr" target="#b19">[12]</ref> to approximate the Gaussian kernel. All other histogram-based features were mapped using the exact Bhattacharyya kernel map- ping <ref type="bibr" target="#b20">[13]</ref>. Finally, similar to <ref type="bibr" target="#b16">[9]</ref>, we reduce each kernelmapped feature to 500 dimensions using PCA and the final concatenated feature results in a 2,500-dimensional vector. Textual features: The second consists of textual features, which are extracted from text associated to images. We first construct a vocabulary from a training dataset and represent the textual features of an image using a traditional bagof-words approach, which counts how many times a word appears in text around the image. Following <ref type="bibr" target="#b15">[8,</ref><ref type="bibr" target="#b16">9]</ref>, we use the linear kernel for the textual features, which counts the number of words shared between two images. Since this representation is highly sparse, we exploit SVD for large and sparse matrices <ref type="bibr" target="#b32">[25]</ref> to reduce the dimensions of the textual feature matrix. In this paper, we experimentally set the dimension of final textual representation to 1, 500. Sentiment features: The third view aims to characterize the sentiment aspect of the associate text. For this, we use an external knowledge base, called SentiWordNet <ref type="bibr" target="#b17">[10]</ref>. It is based on the well-known English lexical dictionary WordNet <ref type="bibr" target="#b33">[26]</ref>, and has been utilized in text-based opinion mining tasks <ref type="bibr" target="#b34">[27]</ref>.</p><p>In SentiWordNet, three types of sentiment scores, "positivity," "negativity," or "objectivity," are assigned to each Word-Net synset. We use these scores to construct a vocabulary of sentiment-related words. Specifically, we select words whose sentiment scores of either positive or negative are larger than a pre-defined threshold. Then, based on the constructed vocabulary, we calculate the sentiment features of an image in the bag-of-words approach. Finally, we apply the SVD to the feature matrix to reduce its dimensionality. The resulting feature is represented as a 20-dimensional vector.</p><p>We will use v, t, s to denote the indexes of the visual, textual, and sentiment views, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Finding Latent Correlations Among Multiple Views</head><p>This subsection describes how to find latent correlations among multiple views using a framework of the generalization of canonical correlation analysis <ref type="bibr" target="#b18">[11]</ref>. Let X i (i ∈ {v, t, s}) denote the feature matrix of the i-th view, and the similarity between two feature vectors x, x in the i-th view is defined by a kernel function K i such that K i (x, x ) = ϕ i (x)ϕ i (x ). We want to find projection matrices W i which maps the i-th view into the latent embedding space. The canonical correlation problem can be transformed into a distance problem such that the distances in the resulting space between each pair of views for the same image are minimized <ref type="bibr" target="#b18">[11]</ref>. The objective function to learn the latent space is as follows: min Wv,Wt,Ws i, j∈{v,t,s}</p><formula xml:id="formula_0">ϕ i (X i )W i − ϕ j (X j )W j 2 F subject to W T i Σ ii W i = I, w T ik Σ i j w jl = 0, i, j ∈ {v, t, s}, i j k, l = 1, • • • , d, k l. (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where Σ i j is a covariance matrix between ϕ i (X i ) and ϕ j (X j ), and w ik represents the k-th column of the matrix W i . In the conventional kernel CCA <ref type="bibr" target="#b18">[11]</ref>, kernel trick is used in Eq. (1). To reduce the computation complexity, one can use explicit feature maps <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b20">13]</ref>. Let φ(x) denote an explicit feature mapping such that K i (x, x ) = φ(x) φ(x). Instead of using the kernel trick, the mapping φ(x) can substituted to the objective function <ref type="bibr" target="#b16">[9]</ref>. Solving the following generalized eigenvalue problem provides the solution of Eq. ( <ref type="formula" target="#formula_0">1</ref>):</p><formula xml:id="formula_2">⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ S 11 S 12 S 13 S 21 S 22 S 23 S 31 S 32 S 33 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ w 1 w 2 w 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ = λ ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ S 11 0 0 0 S 22 0 0 0 S 33 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ w 1 w 2 w 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ ,<label>(2)</label></formula><p>where S i j = φi (X i ) φj (X j ) is the covariance matrix between the i-th and j-th views, and w i is a column of W i . This multi-view formulation has recently proven to be effective for cross-modal retrieval and image annotation <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. In the following subsection, we describe how to use the latent space learned from multiple views for image sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sentiment polarity classification using latent correlations among multiple views</head><p>Using the projection matrices W i , the features of the i-th view of the training images can be represented in the latent space as follows:</p><formula xml:id="formula_3">P i = φi (X i )W i D p ,<label>(3)</label></formula><p>where D is a diagonal matrix whose elements are the eigenvalues of each dimension in the embedding space. p is a weighting parameter, which is set to 4 as in <ref type="bibr" target="#b16">[9,</ref><ref type="bibr" target="#b26">19]</ref>. Using Eq. ( <ref type="formula" target="#formula_3">3</ref>) for each view, we represent the final feature matrix of training images as the concatenation of P v , P t , and P s . If we consider the case in which text of testing images is unavailable, we concatenate the projection P v to the original feature, following the conventional cross-modal retrieval method <ref type="bibr" target="#b25">[18]</ref>. Based on the new feature representation of the training dataset with sentiment labels, we learn a sentiment polarity classifier.</p><p>In this paper, we exploit a linear SVM, which is also used in the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>. Note that although this paper focuses on binary classification as well as the conventional methods <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, our method can be easily extended to multi-class sentiment classification (e.g., positive, negative, and neural). Given a testing image, we also extract features from available views (either or both of visual and textual views) and classify the features projected to the embedding space. To conduct experiments, we collected a set of images from Flickr and Instagram as follows.</p><p>• Flickr dataset. From Flickr, we first downloaded a set of image IDs provided by <ref type="bibr" target="#b35">[28]</ref>. Some images were unavailable, and limiting the number of images for each Flickr user to 70, we obtained 105, 587 images. The most frequent words are "view," "black," "photo," "canon," "nikon," and "film." • Instagram dataset. This dataset was constructed by ourselves from Instagram. Using each of the emotional words listed in SentiWordNet as a query keyword, we crawl a set of images. The total number of images was 120, 000. This dataset contains more images that reflect users' daily lives than Flickr dataset. The most frequent words are "love," "like," "life," "day," and "new." In this experiment, we extracted textual and sentiment features from tags and descriptions associated to images.</p><p>To evaluate the performance of image sentiment classification, we prepared sentiment labels of images via crowdsourcing. Conventional methods exploited pseudo sentiment labels using the automatic annotation algorithm based on image tags <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b14">7]</ref>, but it is unreliable due to the noisy tags or lack of tags. To the best of our knowledge, this paper is the first to provide sentiment polarity labels to large-scale image datasets by crowdsourcing-based human annotations. Specifically, we chose CrowdFlower<ref type="foot" target="#foot_1">1</ref> as a platform, and presented each image for subjective evaluation. For each image, three workers were asked to provide a sentiment score. They could choose on a discrete five-point scale labeled with "highly positive," "positive," "neutral," "negative," and "highly negative." The final construction of the ground truth exploited the majority votes of polarity for each image. Table <ref type="table" target="#tab_0">1</ref> shows the details of the number of positive and negative images in each dataset. Since this experiment targets on the binary classification problem following the previous works <ref type="bibr" target="#b11">[4,</ref><ref type="bibr" target="#b12">5]</ref>, we discarded the images labeled by "neutral" and the images resulting in disagreement among workers. Note that our method can be extended to the multi-class classification problem, which will be performed in our future work. The datasets with sentiment labels is available on the Web<ref type="foot" target="#foot_2">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We compare the performance of our multi-view embedding-based approach with the following conventional methods, which exploit either visual or textual view: a low-level visual feature-based method <ref type="bibr" target="#b11">[4]</ref> (denoted as Low), a mid-level visual feature-based method <ref type="bibr" target="#b12">[5]</ref> (denoted as SentiBank), a method that concatenates low-level visual features with the mid-level features (denoted as Low&amp;SentiBank), and a textual feature-based method <ref type="bibr" target="#b17">[10]</ref> (denoted as SentiStrength<ref type="foot" target="#foot_3">3</ref> ). Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same Table <ref type="table">2</ref>. Average and standard deviation of the classification accuracy of image sentiment polarity for 10 runs in each dataset. Note that for Low <ref type="bibr" target="#b11">[4]</ref>, we use the same visual feature set as those described in Sec. 3.1, except for SentiBank outputs.</p><p>Method Flickr dataset Instagram dataset Random 49.78 ± 1.05% 50.06 ± 1.09% Low <ref type="bibr" target="#b11">[4]</ref> 69.44 ± 0.85% 67.16 ± 1.28% SentiBank <ref type="bibr" target="#b12">[5]</ref> 70.01 ± 0.63% 67.26 ± 1.12% Low&amp;SentiBank 70.54 ± 1.00% 68.03 ± 1.36% SentiStrength <ref type="bibr" target="#b36">[29]</ref> 59.30 ± 0.87% 62.78 ± 0.91% USEA <ref type="bibr" target="#b14">[7]</ref> 51.87 ± visual feature set as those described in Sec. 3.1, except for Sen-tiBank outputs. By comparing these methods in terms of using a single view of the testing data, we investigate the effectiveness of our multi-view embedding approach. For each method, we used Liblinear <ref type="foot" target="#foot_4">4</ref> to train a linear SVM, and the soft margin parameter C of the linear SVM was determined by cross validation. We also compare our method with the state-of-the-art method that exploits visual and textual features of the testing data <ref type="bibr" target="#b14">[7]</ref> (denoted as USEA). For reference, the random classification results are shown as Random.</p><p>To validate the contribution of the latent correlations among multiple views, we split up different views with and without embedding. The views used for calculating latent correlations are denoted by LC, and the features projected from images for classification are shown by P. LC(V+T) will refer to the two-view embedding based on visual and tag features; LC(V+T+S) to the three-view embedding based on visual, textual, and sentiment features; P(V) to the projection of only visual features of the images; and P(V+T) to the projection of visual and textual features of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance evaluation and discussion</head><p>Each dataset was randomly separated into a training set and a test set for 10 runs. In the Flickr dataset, for each sentiment polarity at each run, we randomly sampled 11,346 images and 1,260 images for training and testing, respectively. In the Instagram dataset, for each sentiment polarity at each run, we randomly sampled 8,802 images and 978 images for training and testing, respectively. As a performance evaluation metric, we calculated the average and standard deviation of classification accuracy over all runs. The results are shown in Table <ref type="table">2</ref>. As shown, our method using the three views of the training and testing images obtained the best average classification accuracy. This result validates the effectiveness of the complementary use of the multiple views for image sentiment analysis. Even in the case in which the textual and sentiment views of a testing image are unavailable due to the lack of associated text (i.e., LC(V+T+S)+P(V)), our approach presents a better representation of visual features because the latent space learns the alignments of multiple views.</p><p>Examples of classification results by our three-view embeddingbased method are shown in Fig. <ref type="figure" target="#fig_1">2</ref>, in which the red border indicates a misclassified image. We found some difficult cases that cannot be accurately classified by the proposed method. For example, current visual features do not characterize facial expression, letters and drawings in the images. Thus, the design of better features of each view will be performed in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we present a novel image sentiment analysis method that uses the latent correlations among multiple views of training images. In the proposed method, we first extract features from visual, textual, and sentiment views. Then, to project the features from these views, we follow the framework of multi-view CCA using explicit feature mappings. Finally, in the embedding space, a sentiment polarity classifier is trained based on the projected features. To validate the effectiveness of the proposed method, we constructed image datasets via crowdsourcing. Experiments conducted on the datasets show that our multi-view embedding space is more effective for classifying image sentiment polarity than methods that use a single modality only and the state-of-the art method that jointly uses multiple modalities.</p><p>The features used in our framework should be investigated for further performance improvement. We will introduce additional views or features such as facial expressions <ref type="bibr" target="#b13">[6]</ref>. In addition, we will introduce the deep learning-based features <ref type="bibr" target="#b37">[30,</ref><ref type="bibr" target="#b38">31]</ref>, which have significantly improved many computer vision tasks, into the proposed framework. Furthermore, we will tackle the multi-class sentiment classification such as positive, negative, and neutral.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of images classified as positive or negative in Flickr dataset. Image captions correspond to Flickr user IDs. The red border indicates a misclassified image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The number of positive and negative images in each dataset.</figDesc><table><row><cell></cell><cell cols="2">Positive Negative</cell></row><row><cell>Flickr dataset</cell><cell>48,139</cell><cell>12,606</cell></row><row><cell>Instagram dataset</cell><cell>33,076</cell><cell>9,780</cell></row><row><cell cols="2">4. EXPERIMENTS</cell><cell></cell></row><row><cell>4.1. Dataset construction</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 30,2022 at 13:55:57 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">http://www.crowdflower.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">http://mm.doshisha.ac.jp/senti/CrossSentiment.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">http://sentistrength.wlv.ac.uk/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">http://www.csie.ntu.edu.tw/˜cjlin/liblinear/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">@</forename><surname>N00</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">@N00 10508943@N00 68009656@N00</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">@N01 92132559@N00 15790252@N06 11334344@N00 27597792@N03 78745957@N00 48600091327@N01</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m">@N00 65802182@N00 58812071@N00 63022030@N00 29146273@N03 53454935@N00 64232630@N00 (a) Images classified as positive</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m">@N08 81813159@N00 29555476@N06 95288238@N00 27403767@N00 79753359@N00 40526588@N00</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m">@N01 24128704@N08 17715663@N00 61495861@N00 98816109@N00 70346960@N00</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m">@N02 83555001@N00 20473443@N07 82967074@N00</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Images classified as negative</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Find every photo with flickrs new unified search experience</title>
		<ptr target="http://blog.flickr.net/en/2015/05/07/flickr-unified-search/" />
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
	<note>Last accessed: 09/24/2015</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Celebrating a community of 400 million</title>
		<author>
			<persName><forename type="first">Instagram</forename><surname>Blog</surname></persName>
		</author>
		<idno>09/24/2015</idno>
		<ptr target="http://blog.instagram.com/post/129662501137/150922-400million" />
		<imprint>
			<date type="published" when="2015-09">Sep 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">New avenues in opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyzing and predicting sentiment of images on the social web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Minack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia (MM)</title>
				<meeting>Int. Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="715" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Largescale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia (MM)</title>
				<meeting>Int. Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentribute: Image sentiment analysis from a mid-level perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM)</title>
				<meeting>Int. Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised sentiment analysis for social media images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intelligence (IJCAI)</title>
				<meeting>Int. Joint Conf. Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal semisupervised learning for image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Int. Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="902" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A multi-view embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="233" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SentiWordNet: A publicly available lexical resource for opinion mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Language Resources and Evaluation (LREC)</title>
				<meeting>Int. Conf. Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004-12">Dec 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale image categorization with explicit data embedding</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Int. Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="2297" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emotional valence categorization using holistic image features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Herbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing (ICIP)</title>
				<meeting>Int. Conf. Image essing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2008-10">Oct 2008</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia (MM)</title>
				<meeting>Int. Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new approach to cross-modal multimedia retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia (MM)</title>
				<meeting>Int. Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust structured subspace learning for data representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2085" to="2098" />
			<date type="published" when="2015-10">Oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving image-sentence embeddings using large weakly annotated photo collections</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV 2014</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8692</biblScope>
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV 2014</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8690</biblScope>
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A cross-modal approach for extracting semantic relationships between concepts using tagged images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Katsurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haseyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1059" to="1074" />
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936-12">Dec. 1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
				<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Randomized nonlinear component analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
				<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Designing category-level attributes for discriminative visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Int. Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Lanczos bidiagonalization with partial reorthogonalization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Larsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Aarhus University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 537</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995-11">Nov. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using objective words in SentiWord-Net to improve word-of-mouth sentiment classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47" to="54" />
			<date type="published" when="2013-03">March 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How do your friends on social media disclose your emotions?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intelligence (AAAI)</title>
				<meeting>AAAI Conf. Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="306" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sentiment in short strength detection informal text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2544" to="2558" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia (MM)</title>
				<meeting>Int. Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DeViSE: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
				<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
