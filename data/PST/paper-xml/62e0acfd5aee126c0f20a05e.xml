<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Deep-Learning Compilation Bugs with NNSmith</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-26">26 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
							<email>jiawei6@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jinkun</forename><surname>Lin</surname></persName>
							<email>jinkun.lin@nyu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Ruffy</surname></persName>
							<email>fruffy@nyu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Tan</surname></persName>
							<email>c.tan@northeastern.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
							<email>jinyang@cs.nyu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lingming</forename><surname>Zhang</surname></persName>
							<email>lingming@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
							<email>apanda@cs.nyu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Illinois at Urbana-Champaign Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Finding Deep-Learning Compilation Bugs with NNSmith</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-26">26 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.13066v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep-learning (DL) compilers such as TVM and TensorRT are increasingly used to optimize deep neural network (DNN) models to meet performance, resource utilization and other requirements. Bugs in these compilers can produce optimized models whose semantics differ from the original models, and produce incorrect results impacting the correctness of down stream applications. However, finding bugs in these compilers is challenging due to their complexity. In this work, we propose a new fuzz testing approach for finding bugs in deep-learning compilers. Our core approach uses (i) light-weight operator specifications to generate diverse yet valid DNN models allowing us to exercise a large part of the compiler's transformation logic; (ii) a gradient-based search process for finding model inputs that avoid any floating-point exceptional values during model execution, reducing the chance of missed bugs or false alarms; and (iii) differential testing to identify bugs. We implemented this approach in NNSmith which has found 65 new bugs in the last seven months for TVM, TensorRT, ONNXRuntime, and PyTorch. Of these 52 have been confirmed and 44 have been fixed by project maintainers. * Co-first authors. 1 As deep-learning models use floating-point operations, a correctly compiled executable model can have close but not identical results as those of the input model. We do not regard this case as a bug.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning (DL) compilers such as TVM <ref type="bibr" target="#b10">[11]</ref>, TensorRT <ref type="bibr" target="#b43">[44]</ref>, and TensorFlow XLA <ref type="bibr" target="#b0">[1]</ref> are increasingly being used to deploy deep neural network (DNN) models in many different applications. These compilers optimize DL models to meet desired performance, energy, and resource requirements, allowing their use by interactive or safety-critical applications deployed on a variety of devices. However, as compiler implementations are complex, we must be vigilant about detecting bugs in these systems. Compiler bugs can result in crashes or generating an incorrect executable that produces different results than those intended by the user-specified input model 1 .</p><p>In this paper, we develop techniques to automatically find bugs in deep-learning compilers. Similar to prior work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b56">57]</ref>, we adopt a fuzzing and differential testing based approach: we generate random models, compile them using the compiler being tested, and then compare results obtained from the compiled model with those from a reference implementation. This basic approach faces two main challenges, which are not adequately addressed by prior work. First, how to generate structurally diverse and valid models? Deep-learning compilers express a model as a computation graph of tensor operators. For better test coverage, we must ensure model diversity, which requires us to generate graphs by combining operators in different ways. However, it is often invalid to connect two arbitrary operators together; invalid models are rejected and will not be compiled. For example, a compiler will reject any computation graph containing a MatMul (matrix multiplication) operator for which the number of rows in the first input differs from the columns for the second. Therefore, for test efficiency, our graph generation method must also ensure the validity of generated models. Second, given a compiled model, what weights/inputs should we use to run it for differential testing? Naively testing generated models with random or default weights/inputs can easily lead to floating point (FP) exceptional values, i.e., ? ?? s or infinities (??? s). In such cases, we cannot compare the compiled model with its reference implementation. Therefore, to enable equivalence checking, we must be able to generate computational inputs that can avoid FP exceptional values during model execution.</p><p>We address these two challenges to build NNSmith, a tester for deep-learning compilers including TVM <ref type="bibr" target="#b10">[11]</ref>, ONNXRuntime <ref type="bibr" target="#b37">[38]</ref>, and TensorRT <ref type="bibr" target="#b43">[44]</ref>. NNSmith adopts a three-step approach for finding bugs: (i) first, it automatically generates an arbitrary but valid computation graph expressing some model ? ? ; (ii) it then uses the compiler being tested to produce a compiled model ? ? from ? ? , and a reference backend to produce an executable model ? ? ; and (iii) finally it generates random inputs which it passes to ? ? and ? ? , and compares their outputs.</p><p>NNSmith addresses the challenges of generating models and their inputs as follows. Generating diverse and valid computation graphs: The computation graph expressing a deep-learning model consists of tensor operators with attributes attached to both the operators and graph edges. Operator attributes specify parameters such as kernel sizes that impact the operator's semantics, while edge attributes are used to specify input and output tensor types 2 . Before proceeding with the actual compilation steps, deep-learning compilers check the validity of the input computation graph, e.g., whether an operator's output tensor type matches the expected input tensor type of its downstream operators and whether an operator's attributes are valid. In order to produce valid graphs, NNSmith aims to capture and ensure the type matching constraints of a computation graph during its generation. To do so, NNSmith requires that users provide operator specifications, which specify constraints that must be satisfied by an operator's input tensors/attributes and guarantee about its output type, which it uses to check the validity of generated graphs. During NNSmith's incremental graph generation, it inserts one candidate operator at a time by solving for the satisfiability of its type matching constraints given the existing graph. NNSmith uses an existing SMT solver <ref type="bibr" target="#b39">[40]</ref> for constraint solving. Executing compiled models without FP exceptional values. In order to meaningfully compare the outputs of a compiled computation graph with those from a reference implementation, NNSmith aims to select computation inputs (aka model weights and inputs) that do not result in ? ?? s or ??? s during execution. Instead of random search, NNSmith uses gradient-guided search to efficiently find viable model inputs/weights for 98% of the generated models with negligible overhead.</p><p>In addition to addressing the two main challenges above, we designed NNSmith so it can be easily extended to add support for new operators or to work with other deep-learning compilers. We do so by providing users with a framework for writing operator specifications that are needed to ensure graph validity, and by providing a library of common patterns. In our experience, using this framework and library, users can write new operator specifications in a few lines of code. We evaluated the efficacy of our approach by using NNSmith to identify bugs in TVM, ONNXRuntime, TensorRT, and PyTorch.</p><p>Over the last seven months, NNSmith found 65 new bugs in these frameworks. Developers have confirmed 52 and fixed 44 of these bugs. Our coverage evaluation also shows that NNSmith outperforms the state-of-the-art fuzzer by 1.8? for ONNXRuntime and 1.08? for TVM in total branch coverage, as well as 32.7? and 10.8? respectively in unique branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The DNN Computation Graph</head><p>DL frameworks represent a model's underlying computation as a directed graph of tensor operators. In this work, we focus on DNN inference, where the graph captures the forward NN computation that given inputs generates predicted labels or outputs. For example, the model in Figure <ref type="figure">1</ref> is invoked by specifying its inputs (i.e., input variables %x0 and %x1) and the model weights (i.e., input variable 2 A tensor's type defines its shape and its elements' data type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv2d</head><p>Add Reshape <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b1">2]</ref>   In what follows, we use the term tensor type to refer to the shape and element type of a tensor. In the DNN computation graph, each edge is marked with the tensor type that corresponds to the output of the edge's upstream operator, as shown in Figure <ref type="figure">1</ref>. When instantiating an operator, model developers must specify certain additional attributes that dictate its output tensor type. For example, on line 4 in Figure <ref type="figure">1</ref>, the Reshape operator takes %v1 as an input tensor and <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b1">2]</ref> as an attribute indicating the output shape. Because each operator expects its input tensors to be of certain types, it is often invalid to connect two arbitrary operators together by an edge: e.g., the reshape operator on line 4 is valid if and only if its upstream operator's output (%v1) has 7688 elements (62?62?2). This is akin to a "type checking error" in traditional programs. We say that a DNN computation graph is valid if and only if all operators in the graph are valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DL Compilers</head><p>State-of-the-art DL compilers turn a user-specified model, expressed as a DNN computation graph, into an executable implementation. As shown in Figure <ref type="figure">2</ref>, DL compilers process an input DNN model in two stages during its compilation.</p><p>First, DL compilers need to convert an input computation graph into their own internal formats. For interoperability, DL training frameworks typically export trained models to a standardized and commonly supported format such as ONNX <ref type="bibr" target="#b1">[2]</ref>. DL compilers take ONNX models as input and convert them to a compiler-specific Intermediate Representation (IR) that makes it easier to perform compiler optimization.</p><p>Second, DL compilers invoke various transformation passes which rewrite their input IR into a more efficient version. These passes include: graph-optimization passes that simplify the graph (e.g., constant folding) or fuse operators (merge Add and Softmax into BiasSoftmax) <ref type="bibr" target="#b44">[45]</ref>; low-level passes that optimize computation using arithmetic simplification and loop tiling/fusion, to reduce computational overheads. Bugs can exist during both conversion and transformation, with the latter ones likelier to be harder to identify and debug. To comprehensively detect both kinds of bugs, we need to test using models with a diverse graph structure and tensor operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenges in Finding DL Compiler Bugs</head><p>Differential testing and fuzzing <ref type="bibr" target="#b36">[37]</ref> presents a promising approach to finding DL compiler bugs. As shown in the right part of Figure <ref type="figure">2</ref>, this approach requires synthesizing random models for compilation, then running the compiled models with random inputs, and finally comparing the generated results with those from a reference implementation.</p><p>There are several challenges facing the basic approach of fuzzing and differential testing, which are not addressed by prior work <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59]</ref>. Next, we illustrate these challenges using concrete examples. Challenge #1: Generating graphs with diverse patterns. Finding DL compiler bugs requires generating input graphs that contain a variety of operators and connections. Some prior fuzzers <ref type="bibr" target="#b58">[59]</ref> test only using single-operators and thus are too limiting. LEMON <ref type="bibr" target="#b56">[57]</ref> and GraphFuzzer <ref type="bibr" target="#b32">[33]</ref> generate multi-operator computation graphs, but they are restricted to certain types of operators and connections in order to avoid "type check" errors on the generated graphs (detailed in ?6.1). Such restrictions limit graph diversity, and compromise test coverage. Listing 1 shows an example model (M0) generated by NNSmith which has triggered a layout analysis bug in TVM. LEMON cannot generate this model because M0 contains non-shape-preserving operators (e.g., Conv2d) and connections (e.g., broadcasting) which are not supported by LEMON for ensuring graph validity. GraphFuzzer uses a different strategy to guarantee graph validity. Specifically, GraphFuzzer tries to "fix" mismatched tensor shapes in generated graphs through slicing and padding, as illustrated by line 6 in model M1. Unfortunately, doing so biases the generated graphs to include many slicing/padding nodes. In our example, the slice operation in M1 would silence the layout bug found by M0. Challenge #2: Exploring diverse attributes for operators and edges. When generating graphs, it is tempting to ignore the need to explore the operator/edge attribute space and rely on some default values. For example, M2 of Listing 1 uses trivial attributes (e.g., always 1) to initialize operator Ones(1,1,1) (line 13). Unfortunately, the bug found by M0 will not be triggered by M2. Since exploring different attribute values result in diverse output tensor types on edges, it further complicates the task to ensure the validity of generated graphs.</p><p>Challenge #3: Running compiled models to produce numerically valid output. Using arbitrary inputs and model weights to test a compiled model can result in FP exceptional values (i.e., ? ?? s and ??? s) during execution. Such cases occur when the given inputs to some operator are outside of its expected domain, e.g., feeding Sqrt negative values results in ? ?? s, and feeding Pow large base or exponents results in ??? s. Larger graphs are particularly prone to encountering FP exceptional values. For example, we have found that ? ?? /??? occurs in 56.8% of 20-node models generated by NNSmith when using PyTorch's default weight initializer. Previous testing frameworks did not consider these issues, and consequently up to 41% of their bug reports can be false-alarms because of the undefined/nondeterministic behaviors arising from the ? ?? /??? <ref type="bibr" target="#b15">[16]</ref>.</p><p>Clearly we should not compare the output of a compiled model to those of the reference implementation if the results themselves contain ? ?? /??? . What about those scenarios with "normal" final results (aka without any ? ?? /??? ) where some internal operator has produced FP exceptional values during graph execution? For example, operator ArgMax can output a normal FP value even though one of its upstream operators gives it ? ?? as input. It is a subtle requirement that we must also exclude these results from differential testing or risk incurring false positives in bug detection. This is because when handling FP exceptional values, otherwise semantically equivalent operators could produce different results. Therefore, to be able to test effectively, we must generate model inputs/weights that avoid FP exceptional values for all operators in the graph. Only then we refer to the model's output as numerically valid. Otherwise, we might miss detecting bugs. As an example, Listing 1's model M3 can trigger a semantic bug. However, this bug is not exposed because the execution results in ??? values which are not used for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NNSmith's Design</head><p>Overview of the approach. Figure <ref type="figure" target="#fig_2">3</ref> shows an overview of NN-Smith's workflow. NNSmith generates valid random models to be compiled and executed. To ensure graph validity, NNSmith captures the "type checking" constraints of a graph as operator specifications ( ?3.1), and uses an SMT solver to generate valid operator attributes during graph generation ( ?3.2). To run a compiled model, NNSmith uses a gradient guided search procedure to find benign weights/inputs so that no FP exceptional values are produced at any step of the execution ( ?3.3). Finally, NNSmith compares the results obtained from multiple deep learning libraries and compilers to those from a reference implementation to identify bugs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling DNN Operators</head><p>NNSmith generates random DNN models expressed as computational graphs by connecting together different operators. We aim to generate valid graphs that "type check", i.e., graphs where each operator's attributes and input tensor type meet requirements imposed by the compiler.</p><p>In order to generate valid graphs, we require users to provide operator specifications that explicitly state the compiler's requirements for each operator and guarantees about its output. An operator's specification codifies rules for checking validity and depends on its inputs and attributes: For example, the 2-D convolution operator (Conv2d) has several attributes, including a kernel, and takes an image as input. A model that uses a Conv2d operator is valid if the input image is a rank-4 tensor that is larger than the kernel's size. While our implementation includes specifications for common operators (detailed in ?4), we designed NNSmith to make it easy for users to write specification for additional operators. NNSmith specifications are written using symbolic integers and abstract tensors. An abstract tensor is specified with its data type, rank and shape. In our implementation we use concrete values to specify an abstract tensor's data type and rank, and use symbolic integers to specify its shape. As we will see later in ?3.2, NNSmith uses an SMT solver to assign concrete integers to each symbolic integer during graph generation. NNSmith operator specifications provide input and output types (specified using abstract tensors), constraints on inputs and attributes, as well as transfer rules for each operator. Listing 2 shows the operator specification for a 2-D pooling operator (Pool2d), and we describe each of part below: Inputs and outputs. An operator's attributes are inferred from the inputs to its __init__ function. The class variables input_type and output_type describe the input and output tensor types respectively (Lines 3 and 5). Programmers specify a list of tuples, each tuple says what data types can be used for an input (or provided as output). In the listing, the Pool2d operator accepts a single rank 4 tensor of 32-bit or 64-bit floats.</p><p>Constraints. The operator's requires function (Line 10) returns constraints that its inputs and attributes must satisfy as a list of logical predicates. For example, among other constraints, the Pool2d operator requires that the kernel size should be greater than 0 (Line 12). Type transfer function. The operator uses a type transfer function (Line 16) to specify how its output tensor relates to its inputs. For example, on Line 21, Pool2d's type transfer function relates the shape of the operator's output tensor to its kernel size (self.kw and self.kh) and its input shapes. Observe that the constraints output by the type transfer function are the input constraints on a downstream operator. These constraints are used to to combine constraints from connected operators in a computation graph, and thus allow NNSmith to generate valid models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Generation</head><p>Given a set of operator specifications, NNSmith generates models that are topologically diverse and whose operators use diverse attributes. Below we first detail our approach for generating diverse model topologies and then present our binning based approach to assigning diverse attributes. Generating computation graphs. Our model generation algorithm is designed to ensure that the computation graphs it generates are fully connected, as is the case with most real-world models. Additionally, it is also designed so that it can generate a rich variety of models, including ones similar to existing multi-modal and multitask models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51]</ref> that can accept multiple inputs and/or produce multiple outputs.</p><p>NNSmith generates connected computation graph by extending an existing graph while maintaining connectivity. It does so by starting with a graph that contains a single placeholder node, and extending it by either (a) adding a new node whose input edges are connected to the output of an existing node (we refer to this as a forward insertion) or (b) replacing an existing placeholder node with an operator node whose input edges are connected to one or more placeholder nodes (we refer to this as backward insertion). In both cases, the node added by NNSmith is picked at random from the set of symbolic operator specification (op) it is provided. Placeholder nodes have one output, and at the end of the graph generation process they are replaced by input nodes or by weights (which are constant inputs). Algorithm 1 shows our graph generation algorithm. We detail the steps taken when inserting a randomly selected operator (op) into an existing compute graph below:</p><p>1. Type matching: To insert op, NNSmith must first find a feasible insertion point in the current graph. When using forward insertion, this means finding an output edge in the graph whose constraints (as provided by the operator that node represents) satisfy op's input constraints. Similarly, when using backward insertion, this means finding a placeholder node whose output is connected to node(s) whose input constraints are satisfied by op's output constraints. To do so we need to check constraint satisfaction, and we use a SMT solver for this. Rather than invoking an SMT solver for all possible insertion points, we use a simple type matching heuristic to filter out nodes that are obviously infeasible because of incompatible data types or ranks. For example, when using forward insertion for Where(cond, T, F), type matching (Lines 7) will filter out any output edges which are not boolean. 2. Constraint solving: Next, NNSmith generates constraints for any feasible insertion points that have not been filtered out by its type matching heuristic, and uses an SMT solver to check their satisfiability. NNSmith caches constraints for the current model (in ?.solver) to reduce constraint generation overheads, and uses incremental solving <ref type="bibr" target="#b4">[5]</ref> to reduce time taken for checking constraints (Line 5). 3. Node insertion: As we stated previously, we use one of two approaches to insert nodes into the graph: forward insertion and backward insertion:</p><p>? Forward insertion (Line 6) selects one group of plausible tensors (?) as the inputs of op (Line 8) and inserts op as their consumer (Line 10) if the insertion constraints are satisfiable (Line 9). Attribute binning. In addition to graph topological diversity, attribute diversity is also crucial as discussed in ?2.3. We use the model generated by an SMT solver (Z3 <ref type="bibr" target="#b39">[40]</ref> in our implementation) when checking satisfiability for the graph's constraints to determine attributes. However, we found that when producing models for integer constraints, most SMT solvers pick boundary values, e.g., when given a constraint that requires tensor dimensions to be at least 1, all returned models have a dimension of 1. This limits attribute diversity and prevents us from finding bugs in practice. For example, we found that naively using the model returned by Z3 led to us usually using a batch size of 1, and that prevented us from finding some bugs ( ?2.3). We address this problem by adding binning constraints that confine each symbolic integer to a randomly chosen range. Adding binning constraints to the graph's constraints can produce an unsatisfiable constraint system, leading to a situation where we can find no attributes for a valid graph. We avoid this situation by adding constraints only after a graph has been generated ( ?3.2) and only when doing so does not impact satisfiability. Specifically, we group all (positive) integers exponentially into ? bins with the ?-th bin representing integers ? [2 ? = 1,...,? -1, and the last bin [2 ?-1 ,?). To sample a range [?,? ], we select a bin and sample two integers (? and ? ) from the selected bin. <ref type="foot" target="#foot_0">3</ref>We use bins with exponential ranges because, in practice, systems are more sensitive to changes in smaller values, e.g., changing a variable from 0 to 1 generally has larger effect on the output than changes from 30 to 31. Other fuzzers, including AFL <ref type="bibr" target="#b66">[66]</ref>, use a similar binning-based strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Improving Numeric Validity with Gradients</head><p>Next, NNSmith generates inputs and weights that can be used to test the generated models. We initially considered using randomly selected numbers for this, however we found that the generated graphs produce FP exceptional values, including NaN (not a number) and Inf (infinite number). For example, when generating 20-operator graphs, FP exceptional values occur in 56.8% of generated graphs if we use random weight and inputs. This is because some operators, which we refer to as vulnerable operators <ref type="bibr" target="#b62">[63]</ref>, produce real (e.g., ? ? returns ? ?? if ? &lt; 0) or stable (e.g., ? ? returns ??? for large ? and ?) results only for a subset of their input domain. If a vulnerable operator's input lies outside of this domain, the operator outputs an FP exceptional value, which propagates through the model and impacts the model's output, preventing us from comparing model outputs during differential testing. Table <ref type="table" target="#tab_3">1</ref> lists examples of vulnerable operators we encountered in our evaluation.</p><p>One way to address this problem is to use additional heuristics to extend and fix vulnerable operators. For example, changing Div(x, y) to Div(x, |y|+?) renders the Div operator safe. However, this requires changing inputs to the operator, which limits graph diversity as discussed in ?2.3. We thus propose an alternate approach, where we use a gradient-search algorithm to find inputs that ensure that the model's output is numerically valid. Our approach is inspired by GRIST <ref type="bibr" target="#b62">[63]</ref>, though that work has a totally opposite goal: it aims to find inputs that result in FP exceptional values.</p><formula xml:id="formula_0">Operator Domain Violation Loss functions Asin(X) |? | ? 1 NaN L (|? |-1 ? 0) Div(X, Y) |? | &gt; 0 NaN L (|? | &gt; 0) Pow(X, Y) ? &gt; 0 ? log(? ) ? 40 NaN /Inf L (? &gt; 0) L (? log(? ) -40 ? 0) Log2(X) ? &gt; 0 NaN L (? &gt; 0)</formula><p>At a high-level, our approach first associates a loss function with each operator; then, starting with random inputs, it iteratively refines these inputs so that no operator in the graph produces an FP exceptional value. In each iteration, NNSmith identifies the first operator in the model that produces an FP exceptional value. It then uses the loss function associated with the operators to compute new model inputs and uses these for the next iteration. The algorithm terminates when no FP exceptional values are found. We provide details below: Loss functions for avoiding FP exceptional values. NNSmith requires that each vulnerable operator is associated with a set of loss functions, and our input search algorithm (Algorithm 2) uses these loss functions to update the model's inputs to avoid FP exceptional values. We allow users to specify the loss function for each operator, and here we describe the approach we adopted to produce loss functions.</p><p>As we noted above, vulnerable operators produce valid outputs (i.e., outputs that are not FP exceptional values) when inputs are drawn from a particular domain, and this domain can be expressed (or approximated) by the conjunction of a few (usually one or two) inequality predicates on the operator's input. We refer to this conjunction of inequality predicates as the operator's tensor inequalities. For example, the ???? (? ) operator takes a tensor ? as input, and is numerically valid if and only if ? ? 0 (i.e., all elements of ? are positive). Similarly, the ??? (?,? ) operator's numerically valid domain can be under-approximated as ? ? 0?? log(? ) ? 40, which requires that all elements of ? be positive to avoid ? ?? s (since ? might contain fractional elements) and bounds ? log(? ) to avoid outputs that are too large (and would be represented by infinity) <ref type="foot" target="#foot_1">4</ref> . We associate a loss function with each predicate in an operator's tensor inequality. We do so by first rewriting each predicate so that it is either of the form ? (? ) &lt; 0 or ? (? ) ? 0, and then use the formulas in Table <ref type="table" target="#tab_4">2</ref> to convert this cannonical form to a scalar loss. We show examples of the loss functions produced in this manner in Table <ref type="table" target="#tab_3">1</ref>. When an operator produces invalid outputs, the search algorithm picks which loss function to use by finding a predicate that is violated by the operator's current input and using the loss function associated with it. For simplicity, our design assumes that a loss function is positive if and only if its associated predicate is violated by the operator's input, allowing us to use any positive loss function associated with the operator (Line 8) without evaluating its associated predicate. Proxy derivative. Given a vulnerable operator's loss, NNSmith uses gradient propagation to compute changes to the model inputs and weights. Doing so requires computing gradients (derivatives) for each operator in the graph (Line 9). However, some operators are either undifferentiable for some inputs (e.g., Floor, Ceil, and other operators cannot be differentiated at integers) or have zero gradient in some region (e.g., ReLU has gradient 0 for all negative inputs), and this prevents backward propagation. For these functions, we use Proxy Derivative Functions <ref type="bibr" target="#b3">[4]</ref> instead of actual derivatives during gradient propagation.</p><p>Given an operator ? whose gradient is 0 in region U we use</p><formula xml:id="formula_1">?? (?)</formula><p>?? (? ? U) =? as the derivative. We set ?'s sign based on the overall trend of the function, e.g., we use a positive ? for ReLU because it is monotonic. Similar to LeakyReLU <ref type="bibr" target="#b61">[62]</ref>, we choose a small magnitude for ?, thus avoiding large discrepancies between the proxy and the actual derivative. On the other hand, if the operator ? cannot be differentiated in the region ? , we use the closest left-derivative instead. Search process. The overall input search algorithm (Algorithm 2) proceeds as follows: Given a model ? and time budget ? , we first randomly initialize inputs and weights ??,? ? (Line 2) used by the first iteration of the search algorithm (Line 3). In each iteration, we find the first operator (in topological order, Line 4) that produces an FP exceptional value (Line 7). We use its loss function as an optimization objective (Line 8) to tune ??,? ?. If the gradient is neither zero nor a FP exceptional values then we move on to the next iteration (Line 14), otherwise we restart the search with a different initial value (Line 11 and 13). The algorithm throws an exception (Line 16) if it does not terminate within the time budget.</p><p>Because loss functions can vary by orders-of-magnitude across operators, we use Adam <ref type="bibr" target="#b22">[23]</ref>, an adaptive learning rate scheduling algorithm, to set the learning rate. We also reset the learning rate whenever we switch the loss functions used for optimization (as would be the case when an iteration finds a different operator). While this design can lead to a scenario where optimizing for one operator leads to another producing invalid outputs and vice-versa, we found that this to be rare in practice (it occurred less than 1% of the time). We found that the most common reason for the search algorithm failing was that the model has no valid inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>NNSmith is implemented in 5157 lines of Python code. Consistent with Algorithm 1, NNSmith outputs a symbolic graph and its SMT solution for being valid with the help of the Z3 <ref type="bibr" target="#b39">[40]</ref> solver. We then concretize the symbolic graph by invoking the materialized PyTorch functors in the topological order, and export the model to the deployment-friendly ONNX <ref type="bibr" target="#b1">[2]</ref> format using PyTorch's exporter. We also use PyTorch to implement our algorithm for finding model inputs/weights that result in numerically valid output ( ?3.3).</p><p>Since DL compilers vary in operator and data type support, we infer the set of operators supported by the compiler being tested by trying to compile single-operator models with different data types. We use this information when generating graphs, so as to avoid "Not-Implemented" errors.</p><p>Our implementation uses PyTorch as a reference backend, and we compare the optimized model's output to PyTorch's output. If they disagree, we further compare it with results from the model compiled in "O0" mode to narrow down the cause of the bug (i.e., PyTorch or the compiler).</p><p>We wrote operator specifications in NNSmith using information obtained from framework documentation <ref type="bibr" target="#b49">[50]</ref> and source code <ref type="bibr" target="#b45">[46]</ref>. To simplify this task, we implemented several meta types including, unary/binary, reduce and broadcast that further reduce the amount of code needed to specify an operator. Using these, we found that we could implement 59 (out of 73) operator specification within 4 lines of code. Furthermore, even for the most complex specification, which was for Conv2d, the requires function has 9 inequalities and the type_transfer function is only 7 lines of code (formatted by PEP8 <ref type="bibr" target="#b51">[52]</ref>) that can be quickly implemented in a few minutes. Furthermore, these specifications can be written once and then shared by all compilers that can accept ONNX models as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Metrics. We mainly target the following metrics for evaluation: ? Code coverage: Following prior fuzzing work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b58">59]</ref>, we trace source-level branch coverage for both the entire systems and their pass-only components, measuring 1) total coverage counts all hit branches; and 2) unique coverage counts unique branches ("hard" branches) that other baselines cannot cover. ? Bug counting: Following prior work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, we use the number of independent patches as the number of detected bugs, except that we directly count the number of bug reports for closed-source systems (i.e., TensorRT) and unfixed ones.</p><p>Baselines. We compare NNSmith with both the state-of-the-art general DNN model generators (LEMON and GraphFuzzer) and fuzzer specifically designed for TVM (i.e., Tzer).</p><p>? LEMON <ref type="bibr" target="#b56">[57]</ref> is a mutation-based model generator that mutates pre-trained Keras <ref type="bibr" target="#b16">[17]</ref> models <ref type="bibr" target="#b57">[58]</ref>. We convert Keras models into ONNX, to reuse the same differential testing and evaluation framework of NNSmith for fair comparison;</p><p>? GraphFuzzer <ref type="bibr" target="#b32">[33]</ref> generates models by randomly connecting nodes from a block corpus. While LEMON is limited to shape-preserving unary operators, GraphFuzzer also supports non-unary operators by aligning input tensor shapes with slicing/padding and uses specific attributes to create shape-preserving instances for a few nonshape-preserving operators such as Conv2d. As its implementation is not open-sourced, for a fair comparison, we reimplemented its main design, e.g., stitching operators via padding/slicing, by replacing NNSmith's specification-based node insertion. ? Tzer <ref type="bibr" target="#b28">[29]</ref> is a coverage-guided and mutation-based fuzzer targeting TVM's low-level IR. As DNNs generated by NNSmith can also be lowered to low-level IR, we compare Tzer with NNSmith to see if NNSmith can well cover low-level optimizations as Tzer.</p><p>Systems under test. NNSmith finds bugs in the following commonly used compilers:</p><p>? ONNXRuntime <ref type="bibr" target="#b44">[45]</ref> (by Microsoft) is a graph-optimized DNN library for ONNX models, with over 130 source files on various graph optimizations. Like many runtime-based frameworks (e.g., PyTorch), though ONNXRuntime enables optimizations, the optimized graph will still be directly mapped into pre-compiled kernel functions (i.e., no code generation). To evaluate pass-only coverage, we only instrument files under onnxruntime/core/optimizer; ? TVM <ref type="bibr" target="#b10">[11]</ref> is an end-to-end compiler for deploying DNNs on various platforms. In addition to 61 graph-level passes, TVM also performs up to 58 low-level optimizations to generate highly optimized target code. As a front end, ONNX models will be converted into TVM's graph-level IR to perform further optimization. TVM also has a much higher coverage upper limit (i.e., 116k) than ON-NXRuntime (i.e., 65k) given its higher capability/complexity. For pass-only instrumentation, we consider files in all transfroms folders. ? TensorRT <ref type="bibr" target="#b42">[43]</ref> is a compiler and runtime highly optimized for NVIDIA GPUs and has been used by more than 350k developers across 27.5k companies. Since TensorRT is closed-sourced, we exclude it for coverage evaluation.</p><p>Experimental configuration. The testbed hardware configurations include: 1) Intel 10700k CPU (16 threads); 2) 64 GB memory (3200 Mhz); and 3) 2TB NVMe SSD. The operating system is Ubuntu 20.04 and targeted DL systems are compiled by Clang 14 under release mode. Except that we performed bug findings on various latest compiler versions over the last seven months, the default software versions used in evaluation are: ONNXRuntime v1.12 (c556f5), TVM v0.8 (9ab3a1), TensorRT v8.4 and PyTorch v1.13 (dev20220615). When evaluating NNSmith, for Algorithm 1 we choose between forward and backward at every insertion randomly with equal probability. For the binning approach we use ? = 6 bins ( ?3.2) to ensure a decent amount of attribute diversity while keeping the models small for fuzzing efficiency. For the gradient search, the initial learning rate is set to be 0.5, ? in the tensor inequality loss function is set to 10 -10 . While LEMON does not explicitly control the graph sizes (since it mutates existing models), we set the default generated graph size of NNSmith and GraphFuzzer to be 10. For coverage evaluation, we run fuzzers for 4 hours by default (following Tzer <ref type="bibr" target="#b28">[29]</ref>) as we observe that code coverage curves generally converge before that point (e.g., as shown in Figure <ref type="figure" target="#fig_5">4</ref>).       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">End-to-end Coverage Efficiency</head><p>We first compare NNSmith with our graph-level baselines (i.e., GraphFuzzer and LEMON) in terms of code coverage on TVM and ONNXRuntime (since TensorRT is closed-sourced). Figure <ref type="figure" target="#fig_5">4</ref> shows the coverage growth (y axis) over four hours (x axis). As is shown in the Figure, NNSmith beats the 2nd-best baseline (i.e., GraphFuzzer) by 1.8? on ONNXRuntime and by 1.08? on TVM. NNSmith also achieves a decent percentage of total coverage, i.e., 17.9% on ON-NXRuntime and 18.6% on TVM <ref type="foot" target="#foot_2">5</ref> . Figure <ref type="figure" target="#fig_7">5</ref> further shows the number of generated test cases (x axis) within 4 hours and their accumulated total coverage (y axis, consistent to Figure <ref type="figure" target="#fig_5">4</ref>). We can observe that with fewer test cases generated within the same time limit (mainly due to the overhead incurred by constraint solving), NNSmith can still achieve higher coverage than the 2nd-best baseline (i.e., Graph-Fuzzer), indicating that NNSmith can generate higher-quality test cases. It is also worth noting that LEMON is the slowest technique (e.g., up to 103? slower than NNSmith). The reason is that LEMON mutates real-world models which can be very costly to run. We also have similar observations on the pass-only coverage. For example, as shown in Figure <ref type="figure" target="#fig_9">6</ref>, NNSmith outperforms GraphFuzzer by 1.85? on ONNXRuntime and 1.09? on TVM, showing its effectiveness for testing compiler transformation passes. Another interesting observation is that NNSmith's coverage improvement on TVM is relatively smaller than that on ONNXRuntime (1.08? v.s. 1.8?). This can be inferred by the difference in their fundamental designs. While ONNXRuntime implements over 130 optimization files targeting various specific graph patterns, TVM's graph-level optimization is more general. For example, TVM's operator fusion does not check specific operator types, but high-level operator properties such as injective, reduce, etc. Therefore, TVM's coverage is less sensitive to the diversity of generated graph patterns.</p><p>To show the unique coverage for each studied technique, Figure <ref type="figure" target="#fig_10">7</ref> further breaks down the coverage sets of different fuzzers through Venn diagrams <ref type="bibr" target="#b60">[61]</ref>. It shows that NNSmith can achieve much higher unique coverage than the 2nd-best baseline (i.e., LEMON), e.g., 32.7? higher on ONNXRuntime and 10.8? higher on TVM. Despite that GraphFuzzer beats LEMON in total coverage, LEMON contrastingly outperforms GraphFuzzer in unique coverage. This is because LEMON has a different design from NNSmith and GraphFuzzer: it mutates existing real-world models rather than generating new models from scratch, creating different model patterns. Please note that we omitted the unique coverage distribution analysis for pass-only files as it follows a similar pattern as Figure <ref type="figure" target="#fig_10">7</ref>.   Figure <ref type="figure" target="#fig_13">8</ref> also compares NNSmith against Tzer on TVM (as Tzer is specifically designed for TVM). On all TVM files, NNSmith as a general graph-level fuzzer, can outperform state-of-the-art IR-level TVM-specific fuzzer by 1.4? in total coverage and 13? in unique coverage. Interestingly, while other graph-level baselines can at most exclusively cover 117 branches (i.e., LEMON in Figure <ref type="figure" target="#fig_10">7b</ref>), Tzer has an unique coverage of 461. This is because Tzer directly manipulates low-level IR and some low-level operations are not exposed at the graph level. Moreover, in terms of pass-only coverage, NN-Smith outperforms Tzer even more, e.g., by 123? in unique coverage, demonstrating the superiority of graph-level fuzzing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>Attribute binning. Figure <ref type="figure">9</ref> evaluates the effectiveness of attribute binning from the perspective of redundancy. Note that for implementation convenience we use the type system from TVM's Relay IR (parsed from ONNX models) to distinguish operators.It shows that within 4 hours, our binning approach achieves 2.07? unique operator instances, which are distinguished by input types and operator attributes.</p><p>Turning to system coverage, as shown in Figure <ref type="figure">10</ref>, attribute binning improves the unique branch coverage by 2.2? for ONNXRuntime (Figure <ref type="figure">10a</ref>) and 1.8? for TVM (Figure <ref type="figure">10b</ref>). The total coverage improvement is relatively subtle (up to 2.3%) as the binning approach aims at covering the hard-to-hit branches whose proportion is expected to be minor. For example, simply importing TVM's libraries with "import tvm" can hit 4015 branches but those branches are unlikely to have bugs.   Gradient guidance. Figure <ref type="figure" target="#fig_16">11</ref> evaluates the effectiveness of three input/weight searching methods: 1) Sampling: randomly initializing test case values; 2) Gradient (Proxy Deriv.): searching values via the full gradient-based approach; and 3) Gradient: method two without proxy derivatives. The experiment is conducted on three model groups, each of which contains 512 models of 10, 20 and 30 nodes respectively. Every model has at least one vulnerable operator. The Sampling baseline randomly samples values from the range of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref> which is empirically obtained selecting the best one from various tested ranges. For fairness, all methods run on the same groups of models with the same initial weights/inputs generated by the Sampling baseline. We assign different per-model searching timeouts (i.e., ? ?8ms where ? ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>) to each method and observe the ratio of models with numeric-valid inputs/weights (y-axis) over group-wide average searching time (x-axis). Figure <ref type="figure" target="#fig_16">11</ref> shows that our full gradient search improves the numerical validity of Sampling by 1.16-1.34? as the node size/difficulty grows. Also, the proxy derivative mechanism consistently helps our gradient search achieve higher success rate within shorter amount of time. We also observe that searching time is negligible compared with model generation time, e.g., generating a 10-node model costs 83ms on average while our gradient-based searching only takes 3.5ms (4.2%) to achieve a success rate of 98%.   To date, NNSmith has uncovered 65 new bugs as shown in Table <ref type="table" target="#tab_7">3</ref>, where 52 have been confirmed and 44 have been fixed. Others are awaiting developer responses. Interestingly, in addition to compiler bugs, since NNSmith generates models through PyTorch ONNX exporter ( ?4), it also found 10 conversion bugs in PyTorch as a byproduct. Among the bugs we found, 15 are semantic bugs (result inconsistencies with PyTorch) and 50 are crash bugs (segmentation faults or exceptions). In total, there are 36 transformation bugs in ONNXRuntime <ref type="bibr" target="#b8">(9)</ref>, TVM <ref type="bibr" target="#b22">(23)</ref> and TensorRT (4), accounting for the majority of the detected bugs <ref type="foot" target="#foot_4">6</ref> . We found that most of these were optimization bugs: of the 20 fixed transformation bugs we found, 19 are optimization bugs (and the remaining one is an unclassified bug in TensorRT whose code is not available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Bug Study</head><p>Of the 65 bugs we found, 43 bugs cannot be triggered using the algorithms implemented by LEMON or GraphFuzzer. Of these 25 are transformation bugs and 14 are conversion bugs. LEMON's algorithms can trigger at most 16 of all bugs we found, while Graph-Fuzzer's algorithms can trigger at most 22 of these. The core difference is that these prior approaches limit how non-shape preserving operators are connected in the graph, thus limiting graph diversity. In addition to this theoretical analysis, we also evaluated all tools by running them for four hours under the same setting (e.g., all on the default compiler versions as shown in ? 5.1), NNSmith triggers 38 unique crashes (by error messages) for ONNXRuntime and 13 for TVM, while LEMON triggers none and GraphFuzzer only triggers 1 crash for each of ONNXRuntime and TVM. For instance, the only ONNXRuntime bug detected by GraphFuzzer is the wrong fusion to a double-precision ReLU-Clip connection (element-wise and thus shape-preserving).</p><p>We next describe transformation and conversion bugs we found by illustrating prominent bug patterns with examples. We use ? to denote bugs exclusively found by NNSmith.</p><p>Transformation bugs. Wrong expression simplification: We found 5 such bugs in ONNXRuntime (4) and TVM (1). One bug ? happens in FuseMatMulScale when ONNXRuntime optimizes (? ? ??)@(? ? ??) to (? ? ?? ? ) ? (?@?) for scalars? ? ,? ? and matrices?,? where @ denotes MatMul. However, when ? is a 1?1 matrix, ONNXRuntime can mistake matrix ? as a scalar and rewrite it into (? ? ??) ? (?@? ? ), which is illegal as MatMul does not accept scalar inputs, causing a compiler exception. Prior work cannot use the non-shape-preserving MatMul operator, thus missing such bugs. Wrong expression simplification can also lead to semantic bugs, which may lead to wrong decisions in downstream AI applications, introducing security threats in critical scenarios (e.g., self-driving). For example, TVM has a buggy arithmetic optimization pass that switches the order of division and multiplication when rewriting ? ? mod ? ? ? ?? mod?, simplifying it to (? mod?) mod? incorrectly.</p><p>Wrong layout analysis: Memory layout optimizations in TVM first rewrite layouts of the most beneficial operators (e.g., Conv2d) to efficient ones and then let remaining operators adapt changed layouts. We found 7 layout transformation bugs ? in TVM, related to non-shape-preserving operators including broadcasting, reduce and slicing, which cannot be handled by prior work. For example, TVM can rewrite NCHW Conv2d to the SIMD-friendly N ? 4 HW4c layout (NCHW4c for short), by packing every 4 elements on C to the new sub-dimension (4c). However, using this optimization when the Conv2d is followed by a Slice operator whose stride for C is greater than one causes TVM to crash. GraphFuzzer cannot find this bug because to ensure shape alignment it always uses a stride of 1.</p><p>Integer type mismatch: Like traditional compilers (e.g., LLVM <ref type="bibr" target="#b23">[24]</ref>), DL compilers leverage IRs to simplify optimization. IR type mismatch can happen if one pass makes wrong assumption for the IR being transformed. This is especially a pain for TVM: we found 8 bugs ? stopping the compilation due to int32-int64 mismatch and one core TVM developer also admits that "TVM has a pretty fragile system of using i32 vs i64; I personally experienced it a few times before... ".int64 is often introduced by shape-related operators (e.g., shape attributes of Reshape and BroadcastTo), which are not supported by prior work as they cannot handle those complicated shape constraints. Since our first bug report on such issues, there have been 12 fixes (7 from us and 5 from followers) within 5 months to resolve similar issues, one of which even blocked models in production. Interestingly, a bug we found also helped the developers find another bug that had previously been diagnosed as the outcome of a flaky test <ref type="bibr" target="#b31">[32]</ref>. Conversion bugs. Wrong scalar handling: We found 6 crash bugs ? triggered when TVM imports reduce-like operators with a scalar input. Since these operators are not shape-preserving, prior work cannot trigger such bugs. Similarly in PyTorch, when exporting Log2 with a scalar input, the exporter mistakenly sets its output to a rank-1 tensor instead of a scalar, causing a semantic issue. A few days after our report, developers identified 37 other similar bugs. Concurrently, NNSmith also identified a subset of these bugs, but in our evaluation we only treat the first bug (Log2) as one found by NNSmith.</p><p>Wrong broadcasting: Given a 3-way broadcasting Where(? 1?1 ,? 3?1 ,? 2 ), a TVM bug ? causes the lower-ranked tensor ? 2 being ignored during shape inference, resulting in the wrongly inferred shape 3?1, which should be 3?2. This incurs a compiler failure in later phases.</p><p>Another TVM bug ? causes an import failure to MatMul with singlerank broadcasting (one input is a vector) and notably, one month after our bug report, real-world TVM users also encountered such issues and pushed for its fix, showing that NNSmith can synthesize real-world model patterns. Prior work cannot detect them since their design are incompatible with broadcasting operations.</p><p>Data type mismatch: Operators' data type supports vary by ONNX versions, which are often mishandled. For example, PyTorch can mistakenly (and silently) export Clip whose data type is int32 which is not supported by ONNX version 11. Such ill-formed models will be rejected by most compilers; however, it can also be mistakenly compiled by TensorRT, producing unexpected model outputs (i.e., semantic bugs in TensorRT), due to the wrongly interpreted attributes. False alarms. As we discussed in the introduction, floating point semantics <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref> mean that even correct optimizations can lead to scenarios where an optimized model's output differs from the reference output. Consequently, we check output equivalence by checking that the distance between model outputs,when scaled by their overall magnitude is small. However, in some cases valid optimizations can lead to a large relative change in outputs and produce false alarms. For example, optimizing a model where a Sigmoid operator provides the input to a Floor operator, can result in a scenario where the optimized output differs from the reference output by 1, causing NNSmith to falsely report a bug.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Since the first proposal of fuzzing <ref type="bibr" target="#b38">[39]</ref>, various techniques have been proposed for fuzzing systems of different application domains <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b70">70]</ref>. In this section, we mainly talk about the most closely related work in DL system fuzzing and compiler fuzzing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">DL System Fuzzing</head><p>In recent years, a number of techniques have been proposed to test DL libraries and compilers. As one of the first techniques in this direction, CRADLE <ref type="bibr" target="#b35">[36]</ref> directly runs existing DNN models on different DL libraries to detect potential inconsistencies via differential testing.Later on, AUDEE <ref type="bibr" target="#b18">[19]</ref> and LEMON <ref type="bibr" target="#b56">[57]</ref> further extend CRADLE by applying search-based mutation strategies on the DNN models and their inputs to cover more library code. While AUDEE mainly focuses on mutating layer parameters and weight/input tensors, LEMON further applies more advanced mutation rules, including layer deletions/additions. Meanwhile, to ensure correctness of generated models, LEMON <ref type="bibr" target="#b56">[57]</ref> only mutates type-preserving operators (or blocks of operators) from the real-world models, to avoid handling type constraints.However, there are many non-shape-preserving operator types, e.g., even the commonly used Conv2d cannot be completely handled by LEMON. More recently, GraphFuzzer <ref type="bibr" target="#b32">[33]</ref> allows a slightly larger operator search space using padding/slicing to align unmatched tensor shapes and also specifically controls the attributes of shape-changing operator types to create shape-preserving instances (e.g., Conv2d with kernel size/stride of 1). However, this design still substantially limits model diversity (as demonstrated in ?2.3). The very recent (and concurrent) Muffin work <ref type="bibr" target="#b17">[18]</ref> shares a similar limitation as GraphFuzzer: it uses "reshaping" layers to align tensor shapes during model generation; in addition, Muffin focuses on finding gradient computation bugs in DL libraries rather than DL compiler bugs. In this work, we aim to support more diverse/valid model generation for DL compiler fuzzing via a fundamentally different design powered by symbolic constraint solving <ref type="bibr" target="#b9">[10]</ref> and gradient-driven search.</p><p>To complete DL system testing at the model/graph level, researchers have also proposed DL system fuzzing techniques focusing on directly generating or manipulating the low-level model IRs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48]</ref>. TVMFuzz <ref type="bibr" target="#b47">[48]</ref> aims to automatically generate arbitrary low-level IRs based on a set of predefined grammar rules for fuzzing the popular TVM compiler <ref type="bibr" target="#b10">[11]</ref>. The more recent Tzer work <ref type="bibr" target="#b28">[29]</ref> leverages coverage feedback to perform joint mutation of both the low-level IR and optimization passes for TVM. While Tzer has shown promising results over TVMFuzz, the low-level IR mutation adopted by Tzer can hardly test the graph-level optimizations widely adopted by various DL compilers (as shown in ?5.2).</p><p>In recent years, researchers have also investigated techniques to fuzz each DL system API in isolation. Meanwhile, DL APIs are usually exposed in Python, a dynamically typed language, making it hard even to determine their argument types for test generation. Therefore, prior techniques, such as Predoo <ref type="bibr" target="#b68">[68]</ref>, require users to manually set up the function arguments, and can only be evaluated on a limited number of APIs. More recently, FreeFuzz <ref type="bibr" target="#b58">[59]</ref> aims to address this challenge via dynamically tracing API executions from various sources (including library documents, developer tests, and real-world models), and further mutates the traced inputs for each API to test DL libraries. While such API-level testing techniques are adequate for testing first-generation DL libraries ( ?2.1), they can hardly find bugs in graph-level optimizations (e.g., 86% of the transformation bugs detected by NNSmith require multiple operators to trigger).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Compiler Fuzzing</head><p>As one of the most widely studied compiler fuzzing approaches in the literature <ref type="bibr" target="#b35">[36]</ref>, grammar-based techniques (such as Csmith <ref type="bibr" target="#b64">[64]</ref>, jsfunfuzz <ref type="bibr" target="#b52">[53]</ref>, and LangFuzz <ref type="bibr" target="#b20">[21]</ref>) aim to generate syntactically valid input programs acceptable by the underlying compilers. While effective, it is hard for grammar-based techniques to ensure the semantic correctness of the generated programs to cover deep code paths, and highly specialized analyses have to be employed for specific languages. Therefore, various mutation-based techniques <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b67">67]</ref> have also been proposed for fuzzing compilers via mutating existing seed input programs.Moreover, given the advances in DL, researchers have also proposed learning-based techniques for compiler fuzzing. DeepSmith <ref type="bibr" target="#b12">[13]</ref> and DeepFuzz <ref type="bibr" target="#b30">[31]</ref> directly leverage recurrent neural networks (RNNs) to generate test programs from scratch, while Montage <ref type="bibr" target="#b26">[27]</ref> performs mutation-based fuzzing, and replaces code snippets of the seed programs with new code fragments generated by RNNs. More recently, researchers have also leveraged the advanced pre-trained language models (e.g., GPT <ref type="bibr" target="#b8">[9]</ref>) for more powerful test program generation for compiler fuzzing <ref type="bibr" target="#b65">[65]</ref>. Such existing compiler fuzzing techniques can be potentially applied to the low-level IRs (C-like) for fuzzing DL compilers <ref type="bibr" target="#b28">[29]</ref>. However, they can be hardly directly applied for graph-level DL compiler fuzzing, and our study has also shown the superiority of NNSmith over state-of-the-art IR-level DL compiler fuzzer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>NNSmith is a tool for generating diverse and valid test cases for deep learning compilers. It creates abstract operator models to ensure the validity of the generated models, and further utilizes incremental graph generation and attribute binning to ensure its diversity. To avoid false alarms and bug escapes, NNSmith leverages gradient search to find inputs that do not introduce NaN/Inf in the computation. NNSmith is easily extensible to support new operators with few lines of code. Lastly, NNSmith is implemented to generate models in the popular format ONNX and is readily applicable to any systems with ONNX support. To date NNSmith has found 65 new bugs in TVM, TensorRT, ONNXRuntime, and PyTorch, 52 of which have been confirmed or fixed, demonstrating its effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Sample DNN graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of NNSmith.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 : 2 ?? 3 OUTER:while time budget not exhausted do 4 for 5 ? 6 ? 8 L</head><label>2234568</label><figDesc>Gradient-guided value search. 1 Function GradSearch(DNN ?, learning rate ?): ,? ? ? randomly initialized inputs and weights operator ? ? in topologicalSort(?) do ? ? input to ? ? ? ? ? ? (? ? ) 7 if ? NaN/Inf ?? ? then ? first positive loss functions of ? ? 9 ?? ,? ? ? ?? ,? ?-? ? ?? ,? ? L (? ? ) 10 if ?? ,? ? not changed then // Zero gradients 11 ?? ,? ? ? randomly initialized values 12 else if ? NaN/Inf ? ?? ,? ? then 13 Replace NaN/Inf with random values 14 continue OUTER // Go to Line 3 15 return ?? ,? ? 16 raise failed to find viable ?? ,? ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Total branch coverage over time (all files).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Total branch over test cases (all files).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Total branch coverage over time (pass files).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Venn diagram of overall coverage (total coverage shown in parenthesis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Pass-only files.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: NNSmith vs. Tzer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>w h e r e s t r id e d s li c e r e s iz e 1 d c o n c a t e n a t e t r a n s p o s e b r o a d c a s t t o b i n . e le m .-w is e r e s iz e 3 d r e s h a p e m a x p o o l2 d c o n v 2 d a v g p o o l2 d p a d r e d u c e r e s iz e 2 d s o f t m a x u n a r y e le m .-w is e b r o a d c a s t t o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Normalized unique operator instances tested.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Effectiveness of gradient-based search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Listing 1: DNN patterns that can expose compiler bugs.</figDesc><table><row><cell>def M0 () :</cell><cell># M0 triggers a compiler crash bug !</cell></row><row><cell>A = Conv2d (...)</cell><cell># shape : (1 ,2 ,1 ,48)</cell></row><row><cell>B = Ones (1 ,1 ,48)</cell><cell># shape : ( 1 ,1 ,48)</cell></row><row><cell>return A + B</cell><cell></cell></row><row><cell>def M1 () :</cell><cell># bug NOT triggered !</cell></row><row><cell>A = Conv2d (...)</cell><cell># shape : (1 ,2 ,1 ,48)</cell></row><row><cell cols="2">B = Ones (1 ,2 ,1 ,49) # different shape : (1 ,2 ,1 ,49)</cell></row><row><cell cols="2">return A + B [: ,: ,: ,:48] # slice to match (1 ,2 ,1 ,48)</cell></row><row><cell>def M2 () :</cell><cell># bug NOT triggered !</cell></row><row><cell>A = Conv2d (...)</cell><cell># shape : (1 ,2 ,1 ,48)</cell></row><row><cell>B = Ones (1,1,1)</cell><cell># trivial shape : (1 ,1 ,1)</cell></row><row><cell>return A + B</cell><cell></cell></row><row><cell>def M3 () :</cell><cell># M3 can trigger a semantic bug</cell></row><row><cell cols="2">Y = Conv2d ( Conv2d (...) , ...) # bug lies here</cell></row><row><cell cols="2">Y = Pow(Y, BIG_NUM) # bug not exposed due to Infs</cell></row><row><cell>return Y</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>?-1 , 2 ? ] for all Algorithm 1: Computation graph generation. :Graph ?; operator to try op; global constraints ?. 1 Function Solve(?, ?, op, op's inputs ?): {?? ? [0, |? | ],? ? .shape =? ? .shape}, op, ?) then</figDesc><table><row><cell cols="2">Input 2 ? ?? ???.requires(?)</cell></row><row><cell>3</cell><cell>for ?? ???.type_transfer(?) do</cell></row><row><cell>4</cell><cell>? ?? ? {?.shape 0 ? 1,???,?.shape ?.????-1 ? 1}</cell></row><row><cell>5</cell><cell>return ?.solver.try_add_constraints(?)</cell></row><row><cell cols="2">6 Function ForwardInsert(?, op):</cell></row><row><cell>7</cell><cell>? ? TypeMatch(? .intermediates(), op.input_type)</cell></row><row><cell>8</cell><cell>? ? randomly choose one input combination from ?</cell></row><row><cell>9</cell><cell>if Solve(?, ?, op, ?) then</cell></row><row><cell>10</cell><cell>?.insert_consumer(v, op)</cell></row><row><cell cols="2">11 Function BackwardInsert(?, op):</cell></row><row><cell>12</cell><cell>? ? TypeMatch(? .placeholders(), op.output_type)</cell></row><row><cell>13</cell><cell>? ? randomly choose one set of placeholders from ?</cell></row><row><cell>14</cell><cell>// Also see infer_input_type in Line 23 of Listing 2</cell></row><row><cell>15</cell><cell>? ? new placeholders w.r.t. op.infer_input_type(v)</cell></row></table><note><p>16 ? ? op.type_transfer(?) 17 if Solve(?, 18 ?.insert_consumer(p, op) 19 ?.replace_placeholder(v, op)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Representative vulnerable operators.</figDesc><table><row><cell>Tensor Ineq.</cell><cell>Loss function L</cell></row><row><cell>? (? ) ? 0</cell><cell>? ?? max(? (?),0)</cell></row><row><cell>? (? ) &lt; 0</cell><cell>? ?? max(? (?) +?,0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Tensor inequality to loss function conversions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Bug distribution.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>If the last bin is chosen we use ? = 2 ?-1 and ? = ?. For all other bins ?, values are chosen by sampling ? ?? (? -1,?) and using ?2 ? ?.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We constrain the logarithm instead of directly using the power function to ensure that the loss function does not generate a FP exceptional value.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Note that it is unlikely for NNSmith to achieve perfect overall coverage as there are many other irrelevant components related to debugging, auto-tuning[12,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>69], etc. For instance, existing Linux kernel fuzzers<ref type="bibr" target="#b21">[22]</ref> can only achieve 0.8-10.5% coverage.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We classify bugs first based on code inspection (when possible); otherwise, we classify a bug as transformation bug if its individual operators cannot reproduce the issue separately.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX symposium on operating systems design and implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/onnx/onnx" />
		<title level="m">Open neural network exchange</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>ArXiv, abs/1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Nikolaj</forename><surname>Bj?rner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Nachmanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Wintersteiger</surname></persName>
		</author>
		<title level="m">International Summer School on Engineering Trustworthy Software Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="148" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fuzzing: Challenges and reflections</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Boehme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Cadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Roychoudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Softw</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Directed greybox fuzzing</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>B?hme</surname></persName>
		</author>
		<author>
			<persName><surname>Van-Thuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manh-Dung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Roychoudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2329" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the reliability of coverage-based fuzzer benchmarking</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>B?hme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?szl?</forename><surname>Szekeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Metzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th IEEE/ACM International Conference on Software Engineering, ser. ICSE</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Symbolic execution for software testing: three decades later</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Cadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compiler fuzzing through deep learning</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Leather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis</title>
		<meeting>the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="95" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language fuzzing using constraint logic programming</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Dewey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hardekopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM/IEEE international conference on Automated software engineering</title>
		<meeting>the 29th ACM/IEEE international conference on Automated software engineering</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="725" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated testing of graphics shader compilers</title>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Alastair F Donaldson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Evrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Lascu</surname></persName>
		</author>
		<author>
			<persName><surname>Thomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2017">2017</date>
			<pubPlace>1(OOPSLA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Found result inconsistency in graph-based fuzz testing</title>
		<ptr target="https://github.com/gbftdlie/Graph-based-fuzz-testing/blob/master/BugDetails_DCF.md" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Keras</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Jiazhen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuchuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Muffin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08734</idno>
		<title level="m">Testing deep learning libraries via neural architecture fuzzing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audee: Automated testing for deep learning frameworks</title>
		<author>
			<persName><forename type="first">Qianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="486" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fuzzing with code fragments</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st USENIX Security Symposium (USENIX Security 12)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="445" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hfl: Hybrid fuzzing on the linux kernel</title>
		<author>
			<persName><forename type="first">Kyungtae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><forename type="middle">Hwan</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Insik</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byoungyoung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NDSS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">LLVM: An infrastructure for multi-stage optimization</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lattner</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compiler validation via equivalence modulo inputs</title>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Vu Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Afshari</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="216" to="226" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finding deep compiler bugs via guided stochastic program mutation</title>
		<author>
			<persName><forename type="first">Chengnian</forename><surname>Vu Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="386" to="399" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Montage: A neural network language model-guided javascript engine fuzzer</title>
		<author>
			<persName><forename type="first">Suyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyungseok</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Kil Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sooel</forename><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th USENIX Security Symposium (USENIX Security 20)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2613" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fuzzing: a survey</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybersecurity</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coverageguided tensor compiler fuzzing with joint ir-pass mutation</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinlin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Program. Lang</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022-04">apr 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pmfuzz: Test case generation for persistent memory programs</title>
		<author>
			<persName><forename type="first">Sihang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyash</forename><surname>Mahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2021</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2021<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="487" to="502" />
		</imprint>
	</monogr>
	<note>Association for Computing</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic generation of syntax valid c programs for fuzz testing</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupesh</forename><surname>Prajapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Deepfuzz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1044" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An empirical analysis of flaky tests</title>
		<author>
			<persName><forename type="first">Qingzhou</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farah</forename><surname>Hariri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lamyaa</forename><surname>Eloussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Marinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSOFT international symposium on foundations of software engineering</title>
		<meeting>the 22nd ACM SIGSOFT international symposium on foundations of software engineering</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph-based fuzz testing for deep learning inference engines</title>
		<author>
			<persName><forename type="first">Weisi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyue</forename><surname>Run</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunrong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="288" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">C11tester: A fuzzer for c/c++ atomics</title>
		<author>
			<persName><forename type="first">Weiyu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Demsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2021)</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2021)</meeting>
		<imprint>
			<date type="published" when="2021-04">April 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The art, science, and engineering of fuzzing: A survey</title>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Man?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyungseok</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choongwoo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><forename type="middle">Kil</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Egele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maverick</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compiler fuzzing: How much does it matter?</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Marcozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><forename type="middle">F</forename><surname>Donaldson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Cadar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Differential testing for software</title>
		<author>
			<persName><surname>William M Mckeeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Technical Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="107" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Onnx runtime: cross-platform, high performance ml inferencing and training accelerator</title>
		<ptr target="https://onnxruntime.ai/" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An empirical study of the reliability of unix utilities</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Barton P Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Fredriksen</surname></persName>
		</author>
		<author>
			<persName><surname>So</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="32" to="44" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Z3: An efficient smt solver</title>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moura</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaj</forename><surname>Bj?rner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Tools and Algorithms for the Construction and Analysis of Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hippocrates: Healing persistent memory bugs without doing any harm</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Kasikci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="401" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://nvidianews.nvidia.com/news/nvidia-inference-breakthrough-makes-conversational-ai-smarter-more-interactive-from-cloud-to-edge" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">NVIDIA. Nvidia tensorrt</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graph optimizations in onnx runtime</title>
		<author>
			<persName><surname>Onnxruntime</surname></persName>
		</author>
		<ptr target="https://onnxruntime.ai/docs/performance/graph-optimizations.html" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Symbolic shape inference in onnxruntime</title>
		<author>
			<persName><surname>Onnxruntime</surname></persName>
		</author>
		<ptr target="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/symbolic_shape_infer.py" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Numerical computing with IEEE floating point arithmetic</title>
		<author>
			<persName><surname>Michael L Overton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Pankratz</surname></persName>
		</author>
		<author>
			<persName><surname>Tvmfuzz</surname></persName>
		</author>
		<ptr target="https://github.com/dpankratz/TVMFuzz" />
		<title level="m">Fuzzing tensor-level intermediate representation in tvm</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">On properties of floating point arithmetics: Numerical stability and the cost of accurate computations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><surname>Priest</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<author>
			<persName><surname>Conv</surname></persName>
		</author>
		<ptr target="https://pytorch.org/docs/1.11/generated/torch.nn.Conv2d.html" />
		<imprint>
			<date type="published" when="2021">2d -pytorch 1.11.0 documentation, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pep 8 -style guide for python code</title>
		<author>
			<persName><forename type="first">Guido</forename><surname>Rossum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Warsaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Coghlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
		<ptr target="https://github.com/MozillaSecurity/funfuzz" />
	</analytic>
	<monogr>
		<title level="j">Mozilla Security. jsfunfuzz</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Finding compiler bugs via live code mutation</title>
		<author>
			<persName><forename type="first">Chengnian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications</title>
		<meeting>the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="849" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Finding missed optimizations through the lens of dead code elimination</title>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Rigger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="697" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Trippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kang G Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garret</forename><surname>Chernyakhovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><surname>Hicks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02308</idno>
		<title level="m">Fuzzing hardware like software</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep learning library testing via effective model generation</title>
		<author>
			<persName><forename type="first">Zan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="788" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The implementation repository of LEMON: Deep Learning Library Testing via Effective Model Generation</title>
		<author>
			<persName><forename type="first">Zan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdi</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/Jacob-yen/LEMON" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Free lunch for testing: Fuzzing deep-learning libraries from open source</title>
		<author>
			<persName><forename type="first">Anjiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinlin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingming</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.06589</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Patches as better bug reports</title>
		<author>
			<persName><forename type="first">Westley</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international conference on Generative programming and component engineering</title>
		<meeting>the 5th international conference on Generative programming and component engineering</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m">Wikipedia contributors. Venn diagram -Wikipedia</title>
		<imprint>
			<date type="published" when="2022-07">2022. July-2022</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Exposing numerical bugs in deep learning via gradient back-propagation</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zan</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><surname>Esec/Fse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="627" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Finding and understanding bugs in c compilers</title>
		<author>
			<persName><forename type="first">Xuejun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Regehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ACM SIGPLAN conference on Programming language design and implementation</title>
		<meeting>the 32nd ACM SIGPLAN conference on Programming language design and implementation</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="283" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Automated conformance testing for javascript engines via deep compiler fuzzing</title>
		<author>
			<persName><forename type="first">Guixin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Hwei Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhong</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation</title>
		<meeting>the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="435" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">American fuzzing lop (afl)</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Zalewski</surname></persName>
		</author>
		<ptr target="https://lcamtuf.coredump.cx/afl/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Skeletal program enumeration for rigorous compiler testing</title>
		<author>
			<persName><forename type="first">Qirun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengnian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="347" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Predoo: precision testing of deep learning operators</title>
		<author>
			<persName><forename type="first">Xufan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunrong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis</title>
		<meeting>the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="400" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Ansor: Generating {High-Performance} tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">{TCP-Fuzz}: Detecting memory and semantic bugs in {TCP} stacks with fuzzing</title>
		<author>
			<persName><forename type="first">Yong-Hao</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Ju</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jielong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 USENIX Annual Technical Conference (USENIX ATC 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="489" to="502" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
