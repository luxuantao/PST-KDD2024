<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Simulate Complex Physics with Graph Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02-21">21 Feb 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
							<email>&lt;peterbattaglia@google.com&gt;.</email>
						</author>
						<title level="a" type="main">Learning to Simulate Complex Physics with Graph Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02-21">21 Feb 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2002.09405v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Here we present a general framework for learning simulation, and provide a single model implementation that yields state-of-the-art performance across a variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework-which we term "Graph Network-based Simulators" (GNS)-represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework is the most accurate general-purpose learned physics simulator to date, and holds promise for solving a wide range of complex forward and inverse problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Realistic simulators of complex physics are invaluable to many scientific and engineering disciplines, however traditional simulators can be very expensive to create and use. Building a simulator can entail years of engineering effort, and often must trade off generality for accuracy in a narrow range of settings. High-quality simulators require substantial computational resources, which makes scaling up prohibitive. Even the best are often inaccurate due to insufficient knowledge of, or difficulty in approximating, the underlying physics and parameters. An attractive alternative to traditional simulators is to use machine learning to train simulators directly from observed data, however the large state spaces and complex dynamics have been difficult for standard end-to-end learning approaches to overcome. Here we present a general framework for learning simulation from data-"Graph Network-based Simulators" (GNS). Our framework imposes strong inductive biases, where rich physical states are represented by graphs of interacting particles, and complex dynamics are approximated by learned message-passing among nodes.</p><p>We implemented our GNS framework in a single deep learning architecture, and found it could learn to accurately simulate a wide range of physical systems in which fluids, rigid solids, and deformable materials interact with one another. Our model also generalized well to much larger systems and longer time scales than those on which it was trained. While previous learning simulation approaches <ref type="bibr" target="#b17">(Li et al., 2018;</ref><ref type="bibr" target="#b33">Ummenhofer et al., 2020)</ref> have been highly specialized for particular tasks, we found our single GNS model performed well across dozens of experiments and was generally robust to hyperparameter choices. Our analyses showed that performance was determined by a handful of key factors: its ability to compute long-range interactions, inductive biases for spatial invariance, and training procedures which mitigate the accumulation of error over long simulated trajectories. d ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n E 7 3 T 9 e R J L f c X y u Q Q c L Q 7 L / N S k o = " &gt; A A A C C H i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w V Z J S 0 G X B j c s K 9 g F t K J P J p B k 6 m Q k z N 0 I J / Q G X b v U j 3 I l b / 8 J v 8 C e c t l n Y 1 g M X D u f c y 7 3 3 B K n g B l z 3 2 y l t b e / s 7 p X 3 K w e H R 8 c n 1 d O z r l G Z p q x D l V C 6 H x D D B J e s A x w E 6 6 e a k S Q Q r B d M 7 u Z + 7 4 l p w 5 V 8 h G n K / I S M J Y 8 4 J W C l f j g a Q s y A j K o 1 t + 4 u g D e J V 5 A a K t A e V X + G o a J Z w i R Q Q Y w Z e G 4 K f k 4 0 c C r Y r D L M D E s J n Z A x G 1 g q S c K M n y / u n e E r q 4 Q 4 U t q W B L x Q / 0 7 k J D F m m g S 2 M y E Q m 3 V v L v 7 n D T K I b v 2 c y z Q D J u l y U Z Q J D A r P n 8 c h 1 4 y C m F p C q O b 2 V k x j o g k F G 9 H q F j F W t i F O G v Y Z m 4 6 3 n s U m 6 T b q X r P e f G j W W s 0 i p z K 6 Q J f o G n n o B r X Q P W q j D q J I o B f 0 i t 6 c Z + f d + X A + l 6 0 l p 5 g 5 R y t w v n 4 B C N O a Y w = = &lt; / l a t e x i t &gt; Update &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d x / D L B + 2 Y g K I z z 8 J 9 3 z 5 l 1 3 N 0 K U = " &gt; A A A C B n i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K k k p 6 L H g x W M F 0 x b a U D a b T b t 2 N x t 2 J 0 I J v X v 0 q j / C m 3 j 1 b / g b / B N u 2 x x s 6 4 O B x 3 s z z M w L U 8 E N u O 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 7 b R m W a M p 8 q o X Q 3 J I Y J n j A f O A j W T T U j M h S s E 4 5 v Z 3 7 n i W n D V f I A k 5 Q F k g w T H n N K w E p t P 4 0 I s E G l 6 t b c O f A 6 8 Q p S R Q V a g 8 p P P 1 I 0 k y w B K o g x P c 9 N I c i J B k 4 F m 5 b 7 m W E p o W M y Z D 1 L E y K Z C f L 5 t V N 8 a Z U I x 0 r b S g D P 1 b 8 T O Z H G T G R o O y W B k V n 1 Z u J / X i + D + C b I e Z J m w B K 6 W B R n A o P C s 9 d x x D W j I C a W E K q 5 v R X T E d G E g g 1 o e Y s Y K t s w k n X 7 j E 3 H W 8 1 i n b T r N a 9 R a 9 z X q 8 1 G k V M J n a M L d I U 8 d I 2 a 6 A 6 1 k I 8 o e k Q v 6 B W 9 O c / O u / P h f C 5 a N 5 x i 5 g w t w f n 6 B W X + m X s = &lt; / l a t e x i t &gt;</p><p>x i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o + q + 3 R y I 9 + o 0 o 6 Y l / g G V 2 k 9 7 X 3 I = " &gt; A A A C D H i c b V D L S s N A F L 3 x W e u r 6 t J N s A i u S l I K u i y 4 c V n B P q A J Z T K d t E P n E W Y m Y g n 9 B Z d u 9 S P c i V v / w W / w J 5 y 0 W d j W A w O H c + 7 l n j l R w q g 2 n v f t b G x u b e / s l v b K + w e H R 8 e V k 9 O O l q n C p I 0 l k 6 o X I U 0 Y F a R t q G G k l y i C e M R I N 5 r c 5 n 7 3 k S h N p X g w 0 4 S E H I 0 E j S l G x k p B w J E Z R 3 H 2 N B v Q Q a X q 1 b w 5 3 H X i F 6 Q K B V q D y k 8 w l D j l R B j M k N Z 9 3 0 t M m C F l K G Z k V g 5 S T R K E J 2 h E + p Y K x I k O s 3 n m m X t p l a E b S 2 W f M O 5 c / b u R I a 7 1 l E d 2 M s + o V 7 1 c / M / r p y a + C T M q k t Q Q g R e H 4 p S 5 R r p 5 A e 6 Q K o I N m 1 q C s K I 2 q 4 v H S C F s b E 3 L V 9 h I 2 o E x r 9 v P 2 H b 8 1 S 7 W S a d e 8 x u 1 x n 2 j 2 m w U P Z X g H C 7 g C n y 4 h i b c Q Q v a g C G B F 3 i F N + f Z e X c + n M / F 6 I Z T 7 J z B E p y v X 5 t r n F w = &lt; / l a t e x i t &gt; v 0 i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M M 0 D t + b H 5 t 8 X i a 8 T 3 E 2 2 + f 6 7 f L c = " &gt; A A A C D n i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l Z l S 0 G X B j c s K 9 g H t t G T S T B u a Z I Y k U y h D / 8 G l W / 0 I d + L W X / A b / A k z 7 S x s 6 4 H A 4 Z x 7 u S c n i D n T x n W / n a 3 t n d 2 9 / c J B 8 f D o + O S 0 d H b e 0 l G i C G 2 S i E e q E 2 B N O Z O 0 a Z j h t B M r i k X A a T u Y 3 G d + e 0 q V Z p F 8 M r O Y + g K P J A s Z w c Z K / Z 7 A Z h y E 6 X T e d w d s U C q 7 F X c B t E m 8 n J Q h R 2 N Q + u k N I 5 I I K g 3 h W O u u 5 8 b G T 7 E y j H A 6 L / Y S T W N M J n h E u 5 Z K L K j 2 0 0 X q O b q 2 y h C F k b J P G r R Q / 2 6 k W G g 9 E 4 G d z F L q d S 8 T / / O 6 i Q n v / J T J O D F U k u W h M O H I R C i r A A 2 Z o s T w m S W Y K G a z I j L G C h N j i 1 q 9 w k e R H R i L q v 2 M b c d b 7 2 K T t K o V r 1 a p P d b K 9 V r e U w E u 4 Q p u w I N b q M M D N K A J B B S 8 w C u 8 O c / O u / P h f C 5 H t 5 x 8 5 w J W 4 H z 9 A t F q n P w = &lt; / l a t e x i t &gt; e 0 i,j A r e r G f r 3 f q w P u e j a 1 a x c w w W Y H 3 9 A l / E n t I = &lt; / l a t e x i t &gt; v 0 j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j y E 9</p><formula xml:id="formula_0">F V A N U 3 a c f S W 6 H Z 2 A 8 C Y d f c M = " &gt; A A A C D n i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K j N l Q J c F N y 4 r 2 A e 0 0 5 J J M 2 1 s H k O S K Z S h / + D S r X 6 E O 3 H r L / g N / o S Z d h a 2 9 U D g c M 6 9 3 J M T x o x q 4 7 r f T m F r e 2 d 3 r 7 h f O j g 8 O j 4 p n 5 6 1 t E w U J k 0 s m V S d E G n C q C B N Q w 0 j n V g R x E N G 2 u H k L v P b U 6 I 0 l e L R z G I S c D Q S N K I Y G S v 1 e x y Z c R i l 0 3 n f H T w N y h W 3 6 i 4 A N 4 m X k w r I 0 R i U f 3 p D i R N O h M E M a d 3 1 3 N g E K V K G Y k b m p V 6 i S Y z w B I 1 I 1 1 K B O N F B u k g 9 h 1 d W G c J I K v u E g Q v 1 7 0 a K u N Y z H t r J L K V e 9 z L x P 6 + b m O g 2 S K m I E 0 M E X h 6 K E g a N h F k F c E g V w Y b N L E F Y U Z s V 4 j F S C B t b 1 O o V N p J 2 Y M x r 9 j O 2 H W + 9 i 0 3 S q l U 9 v + o / + J W 6 n / d U B B f g E l w D D 9 y A O r g H D d A E G C j w A l 7 B m / P s v D s f z u d y t O D k O + d g B c 7 X L 9 M H n P 0 = &lt; / l a t e x i t &gt; v m i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 y x e F d M R m e S u W h e w s Y D A J n 3 m C c E = " &gt; A A A C D n i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l Z l S 0 G X B j c s K 9 g H t t G T S T B u a Z I Y k U y h D / 8 G l W / 0 I d + L W X / A b / A k z 7 S x s 6 4 H A 4 Z x 7 u S c n i D n T x n W / n a 3 t n d 2 9 / c J B 8 f D o + O S 0 d H b e 0 l G i C G 2 S i E e q E 2 B N O Z O 0 a Z j h t B M r i k X A a T u Y 3 G d + e 0 q V Z p F 8 M r O Y + g K P J A s Z w c Z K / Z 7 A Z h y E 6 X T e F w M 2 K J X d i r s A 2 i R e T s q Q o z E o / f S G E U k E l Y Z w r H X X c 2 P j p 1 g Z R j i d F 3 u J p j E m E z y i X U s l F l T 7 6 S L 1 H F 1 b Z Y j C S N k n D V q o f z d S L L S e i c B O Z i n 1 u p e J / 3 n d x I R 3 f s p k n B g q y f J Q m H B k I p R V g I Z M U W L 4 z B J M F L N Z E R l j h Y m x R a 1 e 4 a P I D o x F 1 X 7 G t u O t d 7 F J W t W K V 6 v U H m v l e i 3 v q Q C X c A U 3 4 M E t 1 O E B G t A E A g p e 4 B X e n G f n 3 f l w P p e j W 0 6 + c w E r c L 5 + A T R c n T k = &lt; / l a t e x i t &gt; e m i,j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f W B X W Q O 7 s B W / E 8 k h Y h l G 6 6 O e a x s = " &gt; A A A C F H i c b V C 7 T s M w F H V 4 l v I K Z W S x q J A Y U J V U k W C s x M J Y J P q Q 2 h I 5 r t O a 2 n F k O 4 g q y m 8 w s s J H s C F W d r 6 B n 8 B p M 9 C W I 1 k 6 O u d e 3 e M T x I w q 7 T j f 1 t r 6 x u b W d m m n v L u 3 f 3 B o H 1 X a S i Q S k x Y W T M h u g B R h N C I t T T U j 3 V g S x A N G O s H k O v c 7 j 0 Q q K q I 7 P Y 3 J g K N R R E O K k T a S b 1 f 6 H O l x E K Y k u + d + S i 8 e M t + u O j V n B r h K 3 I J U Q Y G m b / / 0 h w I n n E Q a M 6 R U z 3 V i P U i R 1 B Q z k p X 7 i S I x w h M 0 I j 1 D I 8 S J G q S z 7 B k 8 M 8 o Q h k K a F 2 k 4 U / 9 u p I g r N e W B m c y T q m U v F / / z e o k O r w Y p j e J E k w j P D 4 U J g 1 r A v A g 4 p J J g z a a G I C y p y Q r x G E m E t a l r 8 Q o b C T M w 5 n X z G d O O u 9 z F K m n X a 6 5 X 8 2 6 9 a s M r e i q B E 3 A K z o E L L k E D 3 I A m a A E M n s A L e A V v 1 r P 1 b n 1 Y n / P R N a v Y O Q Y L s L 5 + A c O b n w 8 = &lt; / l a t e x i t &gt; e m+1 i,j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z P b D p b / F s B 6 W 1 1 n L R s K T K 1 r g J 8 I = " &gt; A A A C G H i c b V D L S s N A F J 3 U V 6 2 v q O D G z W A R B K U k J a D L g h u X F e w D 2 h g m 0 0 k 7 d i Y J M x O h x P y I S 7 f 6 E e 7 E r T u / w Z 9 w 0 m Z h W w 8 M H M 6 5 l 3 v m + D G j U l n W t 1 F a W V 1 b 3 y h v V r a 2 d 3 b 3 z P 2 D t o w S g U k L R y w S X R 9 J w m h I W o o q R r q x I I j 7 j H T 8 8 X X u d x 6 J k D Q K 7 9 Q k J i 5 H w 5 A G F C O l J c 8 8 6 n O k R n 6 Q k u w + 5 e d 2 5 q X 0 4 i H z z K p V s 6 a A y 8 Q u S B U U a H r m T 3 8 Q 4 Y S T U G G G p O z Z V q z c F A l F M S N Z p Z 9 I E i M 8 R k P S 0 z R E n E g 3 n e b P 4 K l W B j C I h H 6 h g l P 1 7 0 a K u J Q T 7 u v J P K 1 c 9 H L x P 6 + X q O D K T W k Y J 4 q E e H Y o S B h U E c z L g A M q C F Z s o g n C g u q s E I + Q Q F j p y u a v s G G k B 0 a 8 r j + j 2 7 E X u 1 g m 7 X r N d m r O r V N t O E V P Z X A M T s A Z s M E l a I A b 0 A Q t g M E T e A G v 4 M 1 4 N t 6 N D + N z N l o y i p 1 D M A f j 6 x e b J q C L &lt; / l a t e x i t &gt; v m+1 i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I U z o d + b 9 A 2 x f m F N 7 R E G 4 x b j x Q 6 Y = " &gt; A A A C F H i c b V D L S s N A F J 3 4 r P U V 6 9 J N s A i C U J I S 0 G X B j c s K 9 g F t L J P p p B 0 6 j z A z K Z a Q 3 3 D p V j / C n b h 1 7 z f 4 E 0 7 a L G z r g Y H D O f d y z 5 w w p k R p 1 / 2 2 N j a 3 t n d 2 S 3 v l / Y P D o 2 P 7 p N J W I p E I t 5 C g Q n Z D q D A l H L c 0 0 R R 3 Y 4 k h C y n u h J P b 3 O 9 M s V R E 8 A c 9 i 3 H A 4 I i T i C C o j T S w K 3 0 G 9 T i M 0 m n 2 m L I r L x u Q g V 1 1 a + 4 c z j r x C l I F B Z o D + 6 c / F C h h m G t E o V I 9 z 4 1 1 k E K p C a I 4 K / c T h W O I J n C E e 4 Z y y L A K 0 n n 2 z L k w y t C J h D S P a 2 e u / t 1 I I V N q x k I z m S d V q 1 4 u / u f 1 E h 3 d B C n h c a I x R 4 t D U U I d L Z y 8 C G d I J E a a z g y B S B K T 1 U F j K C H S p q 7 l K 3 Q k z M C Y 1 c 1 n T D v e a h f r p F 2 v e X 7 N v / e r D b / o q Q T O w D m 4 B B 6 4 B g 1 w B 5 q g B R B 4 A i / g F b x Z z 9 a 7 9 W F 9 L k Y 3 r G L n F C z B + v o F g V i e 5 g = = &lt; / l a t e x i t &gt; (b) (c) (e) (d) (a) X t0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F F N U w H H Z R t x X a C X R Y f U e p t o s C z E = " &gt; A A A C B 3 i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h d 0 Q 0 G P A i 8 c I 5 g H J G m Y n s 8 m Q e S w z s 0 J Y 8 g E e v e p H e B O v f o b f 4 E 8 4 S f Z g E g s a i q p u u r u i h D N j f f / b 2 9 j c 2 t 7 Z L e w V 9 w 8 O j 4 5 L J 6 c t o 1 J N a J M o r n Q n w o Z y J m n T M s t p J 9 E U i 4 j T d j S + n f n t J 6 o N U / L B T h I a C j y U L G Y E W y e 1 O 4 + Z 7 f v T f q n s V / w 5 0 D o J c l K G H I 1 + 6 a c 3 U C Q V V F r C s T H d w E 9 s m G F t G e F 0 W u y l h i a Y j P G Q d h 2 V W F A T Z v N z p + j S K Q M U K + 1 K W j R X / 0 5 k W B g z E Z H r F N i O z K o 3 E / / z u q m N b 8 K M y S S 1 V J L F o j j l y C o 0 + x 0 N m K b E 8 o k j m G j m b k V k h D U m 1 i W 0 v I U P l W s Y i a p 7 x q U T r G a x T l r V S l C r 1 O 5 r 5 X o t z 6 k A 5 3 A B V x D A N d T h D h r Q B A J j e I F X e P O e v X f v w / t c t G 5 4 + c w Z L M H 7 + g U L 2 5 n V &lt; / l a t e x i t &gt; XtK &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J K q d N d D D w 0 s y Q m r x V h W 8 0 9 5 c H k 4 = " &gt; A A A C E X i c b V D L S s N A F J 3 4 r P X R q E s 3 w S K 4 K k k J 6 L L g R n B T w T 6 g j W E y m b R D J 5 M w c y O U k K 9 w 6 V Y / w p 2 4 9 Q v 8 B n / C a Z u F b T 0 w c D j n X O 6 d E 6 S c K b D t b 2 N j c 2 t 7 Z 7 e y V 9 0 / O D y q m c c n X Z V k k t A O S X g i + w F W l D N B O 8 C A 0 3 4 q K Y 4 D T n v B 5 G b m 9 5 6 o V C w R D z B N q R f j k W A R I x i 0 5 J u 1 I T A e 0 r x f P O b g 3 x W + W b c b 9 h z W O n F K U k c l 2 r 7 5 M w w T k s V U A O F Y q Y F j p + D l W A I j n B b V Y a Z o i s k E j + h A U 4 F j q r x 8 f n h h X W g l t K J E 6 i f A m q t / J 3 I c K z W N A 5 2 M M Y z V q j c T / / M G G U T X X s 5 E m g E V Z L E o y r g F i T V r w Q q Z p A T 4 V B N M J N O 3 W m S M J S a g u 1 r e w k e J D o z j p v 6 M b s d Z 7 W K d d J s N x 2 2 4 9 2 6 9 5 Z Y 9 V d A Z O k e X y E F X q I V u U R t 1 E E E Z e k G v 6 M 1 4 N t 6 N D + N z E d</formula><p>0 w y p l T t A T j 6 x d 5 x 5 3 X &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Construct graph</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K g      </p><formula xml:id="formula_1">Z z / W 1 U 1 Z S T b i Y j G x 8 y v a x j G k E = " &gt; A A A C E X i c b V D L S g M x F M 3 U V 6 2 P j r p 0 E y y C q z J T C r o s d O O y g n 1 A O 5 R M m m l D M 8 m Q 3 B H K 0 K 9 w 6 V Y / w p 2 4 9 Q v 8 B n / C t J 2 F b T 0 Q O J x z L v f m h I n g B j z v 2 y n s 7 O 7 t H x Q P S 0 f H J 6 d l 9 + y 8 Y 1 S q K W t T J Z T u h c Q w w S V r A w f B e o l m J A 4 F 6 4 b T 5 s L v P j F t u J K P M E t Y E J O x 5 B G n B K w 0 d M t N J Q 3 o l A I e a 5 J M h m 7 F q 3 p L 4 G 3 i 5 6 S C c r S G 7 s 9 g p G g a M w l U E G P 6 v p d A k B E N n A o 2 L w 1 S w x J C p 2 T M + p Z K E j M T Z M v D 5 / j a K i M c K W 2 f B L x U / 0 5 k J D Z m F o c 2 G R O Y m E 1 v I f 7 n 9 V O I 7 o K M y y Q F J u l q U Z Q K D A o v W s A j r h k F M b O E U M 3 t r Z h O i C Y U b F f r W 8 R Y 2 c A k r t n P 2 H b 8 z S 6 2 S a d W 9 e v V + k O t 0 q j n P R X R J b p C N 8 h H t 6 i B 7 l E L t R F F K X p B r + j N e</formula><formula xml:id="formula_2">C v u H x w e H Z d O T l t G p Z r Q J l F c 6 U 6 E D e V M 0 q Z l l t N O o i k W E a f t a H w 7 0 9 t P V B u m 5 I O d J D Q U e C h Z z A i 2 j g o b 2 B g k q D F 4 S E 2 / V P Y r / r z Q O g h y U I a 8 G v 3 S T 2 + g S C q o t I S 7 S d 3 A T 2 y Y Y W 0 Z 4 X R a 7 K W G J p i M 3 f C u g x K 7 T W E 2 P 3 q K L h 0 z Q L H S 7 k m L 5 u z f j g w L Y y Y i c k 6 B 7 c i s a j P y P 6 2 b 2 v g m z J h M U k s l W S y K U 4 6 s Q r M E 0 I B p S i y f O I C J Z u 5 W R E Z Y Y 2 J d T s t b + F A 5 w 0 h U 3 W d c O s F q F u u g V a 0 E t U r t v l q u 1 / K c C n A O F 3 A F A V x</formula><formula xml:id="formula_3">V O O B E G M 6 R 1 z / d i E 2 R I G Y o Z m Z b 7 i S Y x w h M 0 I j 1 L b R T R Q T Y 7 f w o v r D K E k V T 2 C Q N n 6 t + N D H G t U x 7 a S Y 7 M W C 9 7 u f i f 1 0 t M d B 1 k V M S J I Q L P g 6 K E Q S N h 3 g U c U k W w Y a k l C C t q b 4 V 4 j P I 6 b G O L K W w k 7 c C Y 1 + x n b D v + c</formula><formula xml:id="formula_4">I I D Q a G 5 n 1 t U x 4 Z A V 5 f D M a G d 5 H c X f 8 l 1 u Z R U S p X L c v G k M u l p g W 2 z X X b A E n b E T t g Z u 2 B V J t g j e 2 Y v 7 D V 6 i t 6 i 9 + h j H J 2 K J j N b 7 A e i w R</formula><formula xml:id="formula_5">x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J b 3 Z p q g H 9 G R 5 C F n 1 F i p + T A o V 9 y q u w B Z J 1 5 O K p C j M S h / 9 Y c x S y O U h g m q d c 9 z E + N n V B n O B M 5 K / V R j Q t m E j r B n q a Q R a j 9 b H D o j F 1 Y Z k j B W t q Q h C / X 3 R E Y j r a d R Y D s j a s Z 6 1 Z u L / 3 m 9 1 I Q 3 f s Z l k h q U b L k o T A U x M Z l / T Y Z c I T N i a g l l i t t b C R t T R Z m x 2 Z R s C N 7 q y + u k f V X 1 a t V</formula><formula xml:id="formula_6">+ w h N q n t 8 C T w J Q s 4 F d d W r O A n S d u A W p k g K t g f 3 V D x K e R R A j l 0 z r n u u k 6 O V M o e A S Z p V + p i F l f M y G 0 D M 0 Z h F o L 1 9 c P q M X R g l o m C h T M d K F + n s i Z 5 H W 0 8 g 3 n R H D k V 7 1 5 u J / X i / D 8 N r L R Z x m C D F f L g o z S T G h 8 x h o I B R w l F N D G F f C 3 E r 5 i C n G 0 Y R V M S G 4 q y + v k 0 6 9 5 j Z q j f t 6 t d k o 4 i i T M 3 J O L o l L r k i T 3 J E W a R N O J u S Z v J I 3 K 7 d e</formula><formula xml:id="formula_7">8 G H u / N M D M v S A T X 6 L r f 1 t b 2 z u 7 e f u m g f H h 0 f H J q n 1 U 6 W q a K Q Z t J I V U v o B o E j 6 G N H A X 0 E g U 0 C g R 0 g + n d w u 8 + g d J c x o 8 4 T 8 C P 6 D j m I W c U j T S 0 K w O E G W q W t Z R k o L V U + d C u u j V 3 C W e T e A W p k g K t o f 0 1 G E m W R h A j E 1 T r v u c m 6 G d U I W c C 8 v I g 1 Z B Q N q V j 6 B s a 0 w i 0 n y 1 v z 5 0 r o 4 y c U C p T M T p L 9 f d E R i O t 5 1 F g O i O K E 7 3 u L c T / v H 6 K 4 a 2 f 8 T h J E W K 2 W h S m w k H p L I J w R l w B Q z E 3 h D L F z a 0 O m 1 B F G Z q 4 y i Y E b / 3 l T d K p 1 7 x G r f F Q r z Y b R R w l c k E u y T X x y A 1 p k n v S I m 3 C y I w 8 k 1 f y Z u X W i / V u f a x a t 6 x i 5 p z 8 g f X 5 A w C D l Q U = &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y c Y N d + q c 6 F A G c x N R G O e D B 4 K 1 c 2 s = " &gt; A A A C A X i c b V D J S g N B E O 2 J W 4 x b 1 K O X x i B 4 C j N h Q I 8 B L x 4 T M A s k Q + j p 1 C R N e h m 6 e 4 Q w 5 O T R q 3 6 E N / H q l / g N / o S d 5 a C J D w o e 7 1 V R V S 9 O O T P W 9 7 + 8 w t b 2 z u 5 e c b 9 0 c H h 0 f F I + P W s b l W k K L a q 4 0 t 2 Y G O B M Q s s y y 6 G b a i A i 5 t C J J 3 d z v / M I 2 j A l H + w 0 h U i Q k W Q J o 8 Q 6 q d k d l C t + 1 V 8 A b 5 J g R S p o h c a g / N 0 f K p o J k J Z y Y k w v 8 F M b 5 U R b R j n M S v 3 M Q E r o h I y g 5 6 g k A k y U L w 6 d 4 S u n D H G i t C t p 8 U L 9 P Z E T Y c x U x K 5 T E D s 2 6 9 5 c / M / r Z T a 5 j X I m 0 8 y C p M t F S c a x V X j + N R 4 y D d T y q S O E a u Z u x X R M N K H W Z f N 3 C x 8 p 1 z A W N f e M S y d Y z 2 K T t G v V I K y G z b B S D 1 c 5 F d E F u k T X K E A 3 q I 7 u U Q O 1 E E W A n t E L e v W e v D f v 3 f t Y t h a 8 1 c w 5 + g P v 8 w d b 0 5 d A &lt; / l a t e x i t &gt; GN 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G T G U b 6 T D i X 9 p 7 6 V U L 7 J p 1 W F R j 4 g = " &gt; A A A C D X i c b V D L S g M x F M 3 4 r P V V d e k m W A R X Z a Y U d F l w o S u p Y B / Q j i W T Z t r Q J D M m d 8 Q y 9 B t c u t W P c C d u / Q a / w Z 8 w b W d h W w 8 E D u e c y 7 0 5 Q S y 4 A d f 9 d l Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h w 0 S J p q x O I x H p V k A M E 1 y x O n A Q r B V r R m Q g W D M Y X k 7 8 5 i P T h k f q D k Y x 8 y X p K x 5 y S s B K f g f Y E 2 i Z X t 2 M 7 7 1 u o e i W 3 C n w M v E y U k Q Z a t 3 C T 6 c X 0 U Q y B V Q Q Y 9 q e G 4 O f E g 2 c C j b O d x L D Y k K H p M / a l i o i m f H T 6 d F j f G q V H g 4 j b Z 8 C P F X / T q R E G j O S g U 1 K A g O z 6 E 3 E / 7 x 2 A u G F n 3 I V J 8 A U n S 0 K E 4 E h w p M G c I 9 r R k G M L C F U c 3 s r p g O i C Q X b 0 / w W 0 Y 9 s Y C D L 9 j O 2 H W + x i 2 X S K J e 8 S q l y W y l W K 1 l P O X S M T t A Z 8 t A 5 q q J r V E N 1 R N E D e k G v 6 M 1 5 d t 6 d D + d z F l 1 x s p k j N A f n 6 x f p d Z x 8 &lt; / l a t e x i t &gt; GN M &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d 6 1 b y I F x V c G Y O m j / S q U + D m A c D W 4 = " &gt; A A A C D X i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K j N l Q J c F F 7 p R K t g H t G P J p J k 2 N J k Z k z t i G f o N L t 3 q R 7 g T t 3 6 D 3 + B P m L a z s K 0 H A o d z z u X e H D 8 W X I N t f 1 u 5 l d W 1 9 Y 3 8 Z m F r e 2 d 3 r 7 h / 0 N B R o i i r 0 0 h E q u U T z Q Q P W R 0 4 C N a K F S P S F 6 z p D y 8 m f v O R K c 2 j 8 A 5 G M f M k 6 Y c 8 4 J S A k b w O s C d Q M r 2 8 G d 9 f d 4 s l u 2 x P g Z e J k 5 E S y l D r F n 8 6 v Y g m k o V A B d G 6 7 d g x e C l R w K l g 4 0 I n 0 S w m d E j 6 r G 1 o S C T T X j o 9 e o x P j N L D Q a T M C w F P 1 b 8 T K Z F a j 6 R v k p L A Q C 9 6 E / E / r 5 1 A c O 6 l P I w T Y C G d L Q o S g S H C k w Z w j y t G Q Y w M I V R x c y u m A 6 I I B d P T / B b R j 0 x g I C v m M 6 Y d Z 7 G L Z d K o l B 2 3 7 N 6 6 p a q b 9 Z R H R + g Y n S I H n a E q u k I 1 V E c U P a A X 9 I r e r G f r 3 f q w P m f R n J X N H K I 5 W F + / F r C c m A = = &lt; / l a t e x i t &gt; G M</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T q m c r S j b P 6 i h d a y   </p><formula xml:id="formula_8">I c K 6 6 n O C 8 H Q o = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w V Z I S 0 G X B h W 6 E i v Y B b S y T 6 S Q d O o 8 w M x F K 6 N K l W / 0 I d + L W D / E b / A m n b R a 2 9 c C F w z n 3 c u 8 9 Y c K o N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 t U Y d L E k k n V C Z E m j A r S N N Q w 0 k k U Q T x k p B 2 O r q Z + + 4 k o T a V 4 M O O E B B z F g k Y U I 2 O l + + v H 2 3 6 5 4 l b d G e A q 8 X J S A T k a / f J P b y B x y o k w m C G t u 5 6 b m C B D y l D M y K T U S z V J E B 6 h m H Q t F Y g T H W S z U y f w z C o D G E l l S x g 4 U / 9 O Z I h r P e a h 7 e T I D P W y N x X / 8 7 q p i S 6 D j I o k N U T g + a I o Z d B I O P 0 b D q g i 2 L C x J Q g r a m + F e I g U w s a m s 7 i F x d I 2 D H n N P m P T 8 Z a z W C W t W t X z q / 6 d X 6 n 7 e U 5 F c A J O w T n w w A W o g x v Q A E 2 A Q Q x e w C t 4 c 5 6 d d + f D + Z y 3 F p x 8 5 h g s w P n 6 B Z 8 8 l + 4 = &lt; / l a t e x i t &gt; G 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L g + G b l 8 y p r 9 T X T U r Q Y 5 G S 4 s M N v A = " &gt; A A A C A 3 i c b V D L S g N B E O z 1 G e M r 6 t H L Y h A 8 h d 0 Q 0 G P A g x 4 j m g c k a 5 i d z G 6 G z G O Z m R X C k q N H r / o R 3 s S r H + I 3 + B N O k j 2 Y x I K G o q q b 7 q 4 w Y V Q b z / t 2 1 t Y 3 N r e 2 C z v F 3 b 3 9 g 8 P S 0 X F L y 1 R h 0 s S S S d U J k S a M C t I 0 1 D D S S R R B P G S k H Y 6 u p 3 7 7 i S h N p X g w 4 4 Q E H M W C R h Q j Y 6 X 7 m 0 e v X y p 7 F W 8 G d 5 X 4 O S l D j k a / 9 N M b S J x y I g x m S O u u 7 y U m y J A y F D M y K f Z S T R K E R y g m X U s F 4 k Q H 2 e z U i X t u l Y E b S W V L G H e m / p 3 I E N d 6 z E P b y Z E Z 6 m V v K v 7 n d V M T X Q U Z F U l q i M D z R V H K X C P d 6 d / u g C q C D R t b g r C i 9 l Y X D 5 F C 2 N h 0 F r e w W N q G I a / a Z 2 w 6 / n I W q 6 R V r f i 1 S u 2 u V q 7 X 8 p w K c A p n c A E + X E I d b q E B T c A Q w w u 8 w p v z 7 L w 7 H 8 7 n v H X N y W d O Y A H O 1 y 9 w c 5 f R &lt; / l a t e x i t &gt; G 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x I u o i r 5 r y k l H s A D 7 M a D i T C C c f g g = " &gt; A A A C A 3 i c b V D L S g N B E O z 1 G e M r 6 t H L Y h A 8 h d 0 Q 0 G P A g x 4 j m g c k a 5 i d z G 6 G z G O Z m R X C k q N H r / o R 3 s S r H + I 3 + B N O k j 2 Y x I K G o q q b 7 q 4 w Y V Q b z / t 2 1 t Y 3 N r e 2 C z v F 3 b 3 9 g 8 P S 0 X F L y 1 R h 0 s S S S d U J k S a M C t I 0 1 D D S S R R B P G S k H Y 6 u p 3 7 7 i S h N p X g w 4 4 Q E H M W C R h Q j Y 6 X 7 m 0 e / X y p 7 F W 8 G d 5 X 4 O S l D j k a / 9 N M b S J x y I g x m S O u u 7 y U m y J A y F D M y K f Z S T R K E R y g m X U s F 4 k Q H 2 e z U i X t u l Y E b S W V L G H e m / p 3 I E N d 6 z E P b y Z E Z 6 m V v K v 7 n d V M T X Q U Z F U l q i M D z R V H K X C P d 6 d / u g C q C D R t b g r C i 9 l Y X D 5 F C 2 N h 0 F r e w W N q G I a / a Z 2 w 6 / n I W q 6 R V r f i 1 S u 2 u V q 7 X 8 p w K c A p n c A E + X E I d b q E B T c A Q w w u 8 w p v z 7 L w 7 H 8 7 n v H X N y W d O Y A H O 1 y 9 y E J f S &lt; / l a t e x i t &gt; G M 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G x n 1 c L 5 f S M i g v y / b y 6 W J k X f 4 d V c = " &gt; A A A C B 3 i c b V D L S g N B E O y N r x h f U Y 9 e F o P g x b A b F v Q Y 8 K A X I Y J 5 Q L K G 2 c k k G T K P Z W Z W C E s + w K N X / Q h v 4 t X P 8 B v 8 C S f J H k x i Q U N R 1 U 1 3 V x Q z q o 3 n f T u 5 t f W N z a 3 8 d m F n d 2 / / o H h 4 1 N A y U Z j U s W R S t S K k C a O C 1 A 0 1 j L R i R R C P G G l G o + u p 3 3 w i S l M p H s w 4 J i F H A 0 H 7 F C N j p e b N Y 3 p 3 4 U + 6 x Z J X 9 m Z w V 4 m f k R J k q H W L P 5 2 e x A k n w m C G t G 7 7 X m z C F C l D M S O T Q i f R J E Z 4 h A a k b a l A n O g w n Z 0 7 c c + s 0 n P 7 U t k S x p 2 p f y d S x L U e 8 8 h 2 c m S G e t m b i v 9 5 7 c T 0 r 8 K U i j g x R O D 5 o n 7 C X C P d 6 e 9 u j y q C D R t b g r C i 9 l Y X D 5 F C 2 N i E F r e w g b Q N Q 1 6 x z 9 h 0 / O U s V k m j U v a D c n A f l K p B l l M e T u A U z s G H S 6 j C L d S g D h h G 8 A K v 8 O Y 8 O + / O h / M 5 b 8 0 5 2 c w x L M D 5 + g V h K Z l s &lt; / l a t e x i t &gt; … v M i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 8 p t r x A W I f v / j W S N 0 N E G x Q a x f v 4 = " &gt; A A A C D n i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K j N l Q J c F N 2 6 E C v Y B 7 b R k 0 k w b m s e Q Z A p l 6 D + 4 d K s f 4 U 7 c + g t + g z 9 h p p 2 F b T 0 Q O J x z L / f k h D G j 2 r j u t 1 P Y 2 t 7 Z 3 S v u l w 4 O j 4 5 P y q d n L S 0 T h U k T S y Z V J 0 S a M C p I 0 1 D D S C d W B P G Q k X Y 4 u c v 8 9 p Q o T a V 4 M r O Y B B y N B I 0 o R s Z K / R 5 H Z h x G 6 X T e f x j Q Q b n i V t 0 F 4 C b x c l I B O R q D 8 k 9 v K H H C i T C Y I a 2 7 n h u b I E X K U M z I v N R L N I k R n q A R 6 V o q E C c 6 S B e p 5 / D K K k M Y S W W f M H C h / t 1 I E d d 6 x k M 7 m a X U 6 1 4 m / u d 1 E x P d B i k V c W K I w M t D U c K g k T C r A A 6 p I t i w m S U I K 2 q z Q j x G C m F j i 1 q 9 w k b S D o x 5 z X 7 G t u O t d 7 F J W r W q 5 1 f 9 R 7 9 S 9 / O e i u A C X I J r 4 I E b U A f 3 o A G a A A M F X s A r e H O e n X f n w / l c j h a c f O c c r M D 5 + g U A f J 0 Z &lt; / l a t e x i t &gt; y i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 0 E 0 7 B Y 1 k z R m p g S d 9 O C q S i q w S k g = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q C u i y 4 c V n B P q A J Z T K d t E N n J m F m I o T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 y J U s 6 0 c d 1 v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p u n J z 2 d J I p Q r s k 4 Y k a R F h T z i T t G m Y 4 H a S K Y h F x 2 o 9 m d 6 X f f 6 J K s 0 Q + m j y l o c A T y W J G s L F S E A h s p l F c 5 P M R G z W a b s t d A K 0 T r y J N q N A Z N b 6 C c U I y Q a U h H G s 9 9 N z U h A V W h h F O 5 / U g 0 z T F Z I Y n d G i p x I L q s F h k n q N L q 4 x R n C j 7 p E E L 9 f d G g Y X W u Y j s Z J l R r 3 q l + J 8 3 z E x 8 G x Z M p p m h k i w P x R l H J k F l A W j M F C W G 5 5 Z g o p j N i s g U K 0 y M r a l u S / B W v 7 x O e t c t z 2 / 5 D 3 6 z 7 V d 1 1 O A c L u A K P L i B N t x D B 7 p A I I V n e I U 3 J 3 N e n H f n Y z m 6 4 V Q 7 Z / A H z u c P f t e R 8 g = = &lt; / l a t e x i t &gt;</formula><formula xml:id="formula_9">A F A p a K F B C N 9 X A 4 l B C J 5 z c z v 3 O E 2 g j E v W A 0 x S C m I 2 U i A R n a K W u G f R x D M g G 1 Z p b d x e g 6 8 Q r S I 0 U a A 6 q X / 1 h w r M Y F H L J j O l 5 b o p B z j Q K L m F W 6 W c G U s Y n b A Q 9 S x W L w Q T 5 4 t 4 Z v b D K k E a J t q W Q L t T f E z m L j Z n G o e 2 M G Y 7 N q j c X / / N 6 G U Y 3 Q S 5 U m i E o v l w U Z Z J i Q u f P 0 6 H Q w F F O L W F c C 3 s r 5 W O m G U c b U c W G 4 K 2 + v E 7 a V 3 X P r / v 3 f q 3 h F 3 G U y R</formula><formula xml:id="formula_10">A F A p a K F B C N 9 X A 4 l B C J 5 z c z v 3 O E 2 g j E v W A 0 x S C m I 2 U i A R n a K W u G f R x D M g G 1 Z p b d x e g 6 8 Q r S I 0 U a A 6 q X / 1 h w r M Y F H L J j O l 5 b o p B z j Q K L m F W 6 W c G U s Y n b A Q 9 S x W L w Q T 5 4 t 4 Z v b D K k E a J t q W Q L t T f E z m L j Z n G o e 2 M G Y 7 N q j c X / / N 6 G U Y 3 Q S 5 U m i E o v l w U Z Z J i Q u f P 0 6 H Q w F F O L W F c C 3 s r 5 W O m G U c b U c W G 4 K 2 + v E 7 a V 3 X P r / v 3 f q 3 h F 3 G U y R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our approach focuses on particle-based simulation, which is used widely across science and engineering, e.g., computational fluid dynamics, computer graphics. States are represented as a set of particles, which encode mass, material, movement, etc. within local regions of space. Dynamics are computed on the basis of particles' interactions within their local neighborhoods. One popular particle-based method for simulating fluids is "smoothed particle hydrodynamics" (SPH) <ref type="bibr" target="#b21">(Monaghan, 1992)</ref>, which evaluates pressure and viscosity forces around each particle, and updates particles' velocities and positions accordingly. Other techniques, such as "position-based dynamics" (PBD) <ref type="bibr" target="#b23">(Müller et al., 2007)</ref> and "material point method" (MPM) <ref type="bibr" target="#b28">(Sulsky et al., 1995)</ref>, are more suitable for interacting, deformable materials. In PBD, incompressibility and collision dynamics involve resolving pairwise distance constraints between particles, and directly predicting their position changes. Several differentiable particle-based simulators have recently appeared, e.g., DiffTaichi <ref type="bibr" target="#b12">(Hu et al., 2019)</ref>, PhiFlow <ref type="bibr" target="#b10">(Holl et al., 2020)</ref>, and Jax-MD <ref type="bibr" target="#b27">(Schoenholz &amp; Cubuk, 2019)</ref>, which can backpropagate gradients through the architecture.</p><p>Learning simulations from data <ref type="bibr" target="#b8">(Grzeszczuk et al., 1998)</ref> has been an important area of study with applications in physics and graphics. Compared to engineered simulators, a learned simulator can be far more efficient for predicting complex phenomenon <ref type="bibr" target="#b9">(He et al., 2019)</ref>; e.g., <ref type="bibr" target="#b16">(Ladickỳ et al., 2015;</ref><ref type="bibr" target="#b35">Wiewel et al., 2019)</ref> learn parts of a fluid simulator for faster prediction.</p><p>Graph networks (GN) <ref type="bibr" target="#b2">(Battaglia et al., 2018</ref>)-a type of graph neural network <ref type="bibr" target="#b26">(Scarselli et al., 2008)</ref>-have recently proven effective at learning forward dynamics in various settings that involve interactions between many entities. A GN maps an input graph to an output graph with the same structure but potentially different node, edge, and graph-level attributes, and can be trained to learn a form of messagepassing <ref type="bibr" target="#b6">(Gilmer et al., 2017)</ref>, where latent information is propagated between nodes via the edges. GNs and their variants, e.g., "interaction networks", can learn to simulate rigid body, mass-spring, n-body, and robotic control systems <ref type="bibr" target="#b1">(Battaglia et al., 2016;</ref><ref type="bibr" target="#b4">Chang et al., 2016;</ref><ref type="bibr" target="#b24">Sanchez-Gonzalez et al., 2018;</ref><ref type="bibr" target="#b22">Mrowca et al., 2018;</ref><ref type="bibr" target="#b18">Li et al., 2019;</ref><ref type="bibr" target="#b25">Sanchez-Gonzalez et al., 2019)</ref>, as well as non-physical systems, such as multi-agent dynamics <ref type="bibr" target="#b30">(Tacchetti et al., 2018;</ref><ref type="bibr" target="#b29">Sun et al., 2019)</ref>, algorithm execution <ref type="bibr" target="#b34">(Velikovi et al., 2020)</ref>, and other dynamic graph settings <ref type="bibr" target="#b32">(Trivedi et al., 2019;</ref><ref type="bibr">2017;</ref><ref type="bibr" target="#b36">Yan et al., 2018;</ref><ref type="bibr" target="#b20">Manessi et al., 2020)</ref>.</p><p>Our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General learnable simulation</head><p>We assume X t ∈ X is the state of the world at time t. Applying physical dynamics over K timesteps yields a trajectory of states, X t 0:K = (X t0 , . . . , X t K ). A simulator, s : X → X , models the dynamics by mapping preceding states to causally consequent future states. We denote a simulated "rollout" trajectory as, </p><formula xml:id="formula_11">Xt 0:K = (X t0 , Xt1 , . . . , Xt K ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Simulation as message-passing on a graph</head><p>Our learnable simulation approach adopts a particle-based representation of the physical system (see Section 2), i.e., X = (x 0 , . . . , x N ), where each of the N particles' x i represents its state. Physical dynamics are approximated by interactions among the particles, e.g., exchanging energy and momentum among their neighbors. The way particleparticle interactions are modeled determines the quality and generality of a simulation method-i.e., the types of effects and materials it can simulate, in which scenarios the method performs well or poorly, etc. We are interested in learning these interactions, which should, in principle, allow learning the dynamics of any system that can be expressed as particle dynamics. So it is crucial that different θ values allow d θ to span a wide range of particle-particle interaction functions.</p><p>Particle-based simulation can be viewed as message-passing on a graph. The nodes correspond to particles, and the edges correspond to pairwise relations among particles, over which interactions are computed. We can understand methods like SPH in this framework-the messages passed between nodes could correspond to, e.g., evaluating pressure using the density kernel.</p><p>We capitalize on the correspondence between particle-based simulators and message-passing on graphs to define a general-purpose d θ based on GNs. Our d θ has three steps-ENCODER, PROCESSOR, DECODER <ref type="bibr" target="#b2">(Battaglia et al., 2018)</ref> (see Figure <ref type="figure" target="#fig_13">2(b)</ref>).</p><p>ENCODER definition. The ENCODER : X → G embeds the particle-based state representation, X, as a latent graph, G 0 = ENCODER(X), where G = (V, E, u), v i ∈ V , and e i,j ∈ E (see Figure <ref type="figure" target="#fig_13">2(b,c</ref>)). The node embeddings,</p><formula xml:id="formula_12">v i = ε v (x i )</formula><p>, are learned functions of the particles' states.</p><p>Directed edges are added to create paths between particle nodes which have some potential interaction. The edge embeddings, e i,j = ε e (r i,j ), are learned functions of the pairwise properties of the corresponding particles, r i,j , e.g., displacement between their positions, spring constant, etc. The graph-level embedding, u, can represent global properties such as gravity and magnetic fields (though in our implementation we simply appended those as input node features-see Section 4.2 below).</p><p>PROCESSOR definition. The PROCESSOR : G → G computes interactions among nodes via M steps of learned message-passing, to generate a sequence of updated latent graphs, G = (G 1 , ..., G M ), where</p><formula xml:id="formula_13">G m+1 = GN m+1 (G m ) (see Figure 2(b,d)).</formula><p>It returns the final graph, G M = PROCESSOR(G 0 ). Message-passing allows information to propagate and constraints to be respected: the number of message-passing steps required will likely scale with the complexity of the interactions.</p><p>DECODER definition. The DECODER : G → Y extracts dynamics information from the nodes of the final latent graph,</p><formula xml:id="formula_14">y i = δ v (v M i ) (see Figure 2(b,e)).</formula><p>Learning δ v should cause the Y representations to reflect relevant dynamics information, such as acceleration, in order to be semantically meaningful to the update procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental methods</head><p>Model code and datasets will be released on publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Physical domains</head><p>We explored how our GNS learns to simulate in datasets which contained three diverse, complex physical materials: water as a barely damped fluid, chaotic in nature; sand as a granular material with complex frictional behavior; and "goop" as a viscous, plastically deformable material. These materials have very different behavior, and in most simulators, require implementing separate material models or even entirely different simulation algorithms.</p><p>For one domain, we use <ref type="bibr" target="#b17">Li et al. (2018)</ref>'s BOXBATH, which simulates a container of water and a cube floating inside, all represented as particles, using the PBD engine FleX <ref type="bibr" target="#b19">(Macklin et al., 2014)</ref>.</p><p>We also created WATER-3D, a high-resolution 3D water scenario with randomized water position, initial velocity and volume, comparable to <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref>'s containers of water. We used SPlisHSPlasH <ref type="bibr" target="#b3">(Bender &amp; Koschier, 2015)</ref>, a SPH-based fluid simulator with strict volume preservation to generate this dataset.</p><p>For most of our domains, we use the Taichi-MPM engine <ref type="bibr" target="#b11">(Hu et al., 2018)</ref> to simulate a variety of challenging 2D and 3D scenarios. We chose MPM for the simulator because it can simulate a very wide range of materials, and also has some different properties than PBD and SPH, e.g., particles may become compressed over time.</p><p>Our datasets typically contained 1000 train, 100 validation and 100 test trajectories, each simulated for 300-2000 timesteps (tailored to the average duration for the various materials to come to a stable equilibrium). A detailed listing of all our datasets can be found in the Supplementary Materials B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">GNS implementation details</head><p>We implemented the above model using standard deep learning building blocks and nearest neighbor algorithms.</p><p>Input and output representations. Each particle's input state vector represents position, a sequence of C = 5 previous velocities 1 , and features that capture static material properties (e.g., water, sand, goop, rigid, boundary particle),</p><formula xml:id="formula_15">x t k i = [p t k i , ṗt k−C+1 i , . . . , ṗt k i , f i ],</formula><p>respectively. The global properties of the system, g, include external forces and global material properties, when applicable. The prediction targets for supervised learning are the per-particle average acceleration, pi . Note that in our datasets, we only require p i vectors: the ṗi and pi are computed from p i using finite differences. For full details of these input and target features, see Supplementary Material Section B.</p><p>ENCODER details. The ENCODER constructs the graph structure G 0 by assigning a node to each particle and adding edges between particles within a "connectivity radius", R, which reflected local interactions of particles, and which was kept constant for all simulations of the same resolution. For generating rollouts, on each timestep the graph's edges were recomputed by a nearest neighbor algorithm, to reflect the current particle positions.</p><p>The ENCODER implements ε v and ε e as multilayer perceptrons (MLP), which encode node features and edge features into the latent vectors, v i and e i,j , of size 128.</p><p>We tested two ENCODER variants, distinguished by whether 1 C is a hyperparameter which we explore in our experiments. they use absolute versus relative positional information. For the absolute variant, the input to ε v was the x i described above, with the globals features concatenated to it. The input to ε e , i.e., r i,j , did not actually carry any information and was discarded, with the e 0 i in G 0 set to a trainable fixed bias vector. The relative ENCODER variant was designed to impose an inductive bias of invariance to absolute spatial location. The ε v was forced to ignore p i information within x i by masking it out. The ε e was provided with the relative positional displacement, and its magnitude<ref type="foot" target="#foot_0">2</ref> , r i,j = [(p i − p j ), p i − p j ]. Both variants concatenated the global properties g onto each x i before passing it to ε u . PROCESSOR details. Our processor uses a stack of M GNs (where M is a hyperparameter) with identical structure, MLPs as internal edge and node update functions, and either shared or unshared parameters (as analyzed in Results Section 5.4). We use GNs without global features or global updates (similar to an interaction network)<ref type="foot" target="#foot_1">3</ref> , and with a residual connections between the input and output latent node and edge attributes. DECODER details. Our decoder's learned function, δ v , is an MLP. After the DECODER, the future position and velocity are updated using an Euler integrator, so the y i corresponds to accelerations, pi , with 2D or 3D dimension, depending on the physical domain. As mentioned above, the supervised training targets were simply these, pi vectors.</p><p>Neural network parameterizations. All MLPs have two hidden layers (with ReLU activations), followed by a nonactivated output layer, each layer with size of 128. All MLPs (except the output decoder) are followed by a Lay-erNorm <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> layer, which we generally found improved training stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training</head><p>Software. We implemented our models using TensorFlow 1, Sonnet 1, and the "Graph Nets" library (2018).</p><p>Training noise. Modeling a complex and chaotic simulation system requires the model to mitigate error accumulation over long rollouts. Because we train our models on ground-truth one-step data, they are never presented with input data corrupted by this sort of accumulated noise. This means that when we generate a rollout by feeding the model with its own noisy, previous predictions as input, the fact that its inputs are outside the training distribution may lead it to make more substantial errors, and thus rapidly accumulate further error. We use a simple approach to make the model more robust to noisy inputs: during training we corrupt the input positions and velocities of the model with random-walk noise N (0, σ v = 0.0003), so the training distribution is more similar to the distribution generated during rollouts. See Supplementary Materials B for full details.</p><p>Normalization. We normalize all input and target vectors elementwise to zero mean and unit variance, using statistics computed online during training. Preliminary experiments showed that normalization led to faster training, though converged performance was not noticeably improved.</p><p>Loss function and optimization procedures. We randomly sampled particle state pairs (x t k i , x</p><formula xml:id="formula_16">t k+1 i</formula><p>) from our training trajectories, calculated target accelerations pt k i , and computed the L 2 loss on the predicted per-particle accelerations, i.e., L(x t k i , x</p><formula xml:id="formula_17">t k+1 i ; θ) = d θ (x t k i ) − pt k i 2</formula><p>. We optimized the model parameters θ over this loss with the Adam optimizer <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2014)</ref>, using a minibatch size of 2. We performed a maximum of 20M gradient update steps, with exponential learning rate decay from 10 −<ref type="foot" target="#foot_2">4</ref> to 10 −6 . While models can train in significantly less steps, we avoid aggressive learning rates to reduce variance across datasets and make comparisons across settings more fair.</p><p>We evaluated our models regularly during training by producing full-length rollouts on 5 held-out validation trajectories, and recorded the associated model parameters for best rollout MSE. We stopped training when we observed negligible decrease in MSE, which, on GPU/TPU hardware, was typically within a few hours for smaller, simpler datasets, and up to a week for the larger, more complex datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation</head><p>To report quantitative results, we evaluated our models after training converged by computing one-step and rollout metrics on held-out test trajectories, drawn from the same distribution of initial conditions used for training. We used particle-wise MSE as our main metric between ground truth and predicted data, both for rollout and one-step predictions, averaging across time, particle and spatial axes. We also investigated distributional metrics including optimal transport (OT) <ref type="bibr" target="#b35">(Villani, 2003)</ref> (approximated by the Sinkhorn Algorithm <ref type="bibr" target="#b5">(Cuturi, 2013)</ref>), and Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b7">(Gretton et al., 2012)</ref>. For the generalization experiments we also evaluate our models on a number of initial conditions drawn from distributions different than those seen during training, including, different number of particles, different object shapes, different number of objects, different initial positions and velocities and longer trajectories. See Supplementary Materials B for full details on metrics and evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Our main findings are that our GNS model can learn accurate, high-resolution, long-term simulations of different fluids, deformables, and rigid solids, and it can generalize well beyond training to much longer, larger, and challenging settings. In Section 5.5 below, we compare our GNS model to two recent, related approaches, and find our approach was simpler, more generally applicable, and more accurate.</p><p>To challenge the robustness of our architecture, we used a single set of model hyperparameters for training across all of our experiments. Our GNS architecture used the relative ENCODER variant, 10 steps of message-passing, with unshared GN parameters in the PROCESSOR. We applied noise with a scale of 3 • 10 −4 to the input states during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Simulating complex materials</head><p>Our GNS model was very effective at learning to simulate different complex materials. Table <ref type="table" target="#tab_2">1</ref> shows the one-step and rollout accuracy, as MSE, for all experiments. For intuition about what these numbers mean, the edge length of the container was approximately 1.0, and Figure <ref type="figure">3(a-c</ref>) shows rendered images of the rollouts of our model, compared to ground truth 4 . Visually, the model's rollouts are quite plausible. Though specific model-generated trajectories can be distinguished from ground truth when compared side-by-Figure <ref type="figure">3</ref>. We can simulate many materials, from goop (a) over water (b) to sand (c), and their interaction with rigid obstacles (d)-and even train a single model on multiple materials and their interaction (e). We applied pre-trained models on several out-of-distribution tasks, involving high-res turbulence (f), multi-material interactions with unseen objects (g), and generalizing on significantly larger domains (h).</p><p>side, it is difficult to visually classify individual videos as generated from our model versus the ground truth simulator.</p><p>Our GNS model scales to large amounts of particles and very long rollouts. With up to 19k particles in our 3D domains-substantially greater than demonstrated in previous methods-GNS can operate at resolutions high enough for practical prediction tasks and high-quality 3D renderings (e.g., Figure <ref type="figure" target="#fig_0">1</ref>). And although our models were trained to make one-step predictions, the long-term trajectories remain plausible even over thousands of rollout timesteps.</p><p>The GNS model could also learn how the materials respond to unpredictable external forces. In the FLUIDSHAKE domain, a container filled with water is being moved side-toside, causing splashes and irregular waves.</p><p>Our model could also simulate fluid interacting with complicated static obstacles, as demonstrated by our WATER-RAMPS and SANDRAMPS domains in which water or sand pour over 1-5 obstacles. Figure <ref type="figure">3</ref> <ref type="bibr">(d)</ref> depicts comparisons between our model and ground truth, and Table <ref type="table" target="#tab_2">1</ref> shows quantitative performance measures.</p><p>We also trained our model on continuously varying material parameters. In the CONTINUOUS domain, we varied the friction angle of a granular material, to yield behavior similar to a liquid (0 • ), sand (45 • ), or gravel (&gt; 60 • ). Our results and videos show that our model can account for these continuous variations, and even interpolate between them: a model trained with the region [30 • , 55 • ] held out in training can accurately predict within that range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multiple interacting materials</head><p>So far we have reported results of training identical GNS architectures separately on different systems and materials.</p><p>However, we found we could go a step further and train a single architecture with a single set of parameters to simulate all of our different materials, interacting with each other in a single system.</p><p>In our MULTIMATERIAL domain, the different materials could interact with each other in complex ways, which means the model had to effectively learn the product space of different interactions (e.g., water-water, sand-sand, watersand, etc.). The behavior of these systems was often much richer than the single-material domains: the stiffer materials, such as sand and goop, could form temporary semi-rigid obstacles, which the water would then flow around. Figure <ref type="figure">3</ref>(e) and this video shows renderings of such rollouts. Visually, our model's performance in MULTIMATERIAL is comparable to its performance when trained on those materials individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Generalization</head><p>We found that the GNS generalizes well even beyond its training distributions, which suggests it learns a more general-purpose understanding of the materials and physical processes experienced during training.</p><p>To examine its capacity for generalization, we trained a GNS architecture on WATERRAMPS, whose initial conditions involved a square region of water in a container, with 1-5 ramps of random orientation and location. After training, we tested the model on several very different settings. In one generalization condition, rather than all water being present in the initial timestep, we created an "inflow" that continuously added water particles to the scene during the rollout, as shown in Figure <ref type="figure">3</ref>(f). When unrolled for 2500 time steps, the scene contained 28k particles-an order of magnitude more than the 2.5k particles used in training- and the model was able to predict complex, highly chaotic dynamics not experienced during training, as can be seen in this video. The predicted dynamics were visually similar to the ground truth sequence.</p><p>Because we used relative displacements between particles as input to our model, in principle the model should handle much scenes with much larger spatial extent at test time.</p><p>We evaluated this on a much larger domain, with several inflows over a complicated arrangement of slides and ramps (see Figure <ref type="figure">3</ref>(h), video here). The test domain's spatial width × height were 8.0 × 4.0, which was 32x larger than the training domain's area; at the end of the rollout, the number of particles was 85k, which was 34x more than during training; we unrolled the model for 5000 steps, which was 8x longer than the training trajectories. We conducted a similar experiment with sand on the SANDRAMPS domain, testing model generalization to hourglass-shaped ramps.</p><p>As a final, extreme test of generalization, we applied a model trained on MULTIMATERIAL to a custom test domain with inflows of various materials and shapes (Figure <ref type="figure">3</ref>(g)). The model learned about frictional behavior between different materials (sand on sticky goop, versus slippery floor), and that the model generalized well to unseen shapes, such as hippo-shaped chunks of goop and water, falling from midair, as can be observed in this video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Key architectural choices</head><p>We performed a comprehensive analysis of our GNS's architectural choices to discover what influenced performance most heavily. We analyzed a number of hyperparameter choices-e.g., number of MLP layers, linear encoder and decoder functions, global latent state in the PROCESSORbut found these had minimal impact on performance (see Supplementary Materials C for details).</p><p>While our GNS model was generally robust to architectural and hyperparameter settings, we also identified several factors which had more substantial impact:</p><p>1. the number of message-passing steps, 2. shared vs. unshared PROCESSOR GN parameters, 3. the connectivity radius, 4. the scale of noise added to the inputs during training, 5. relative vs. absolute ENCODER. We varied these choices systematically for each axis, fixing all other axes with the default architecture's choices, and report their impact on model performance in the GOOP domain (Figure <ref type="figure" target="#fig_11">4</ref>).</p><p>For (1), Figure <ref type="figure" target="#fig_11">4(a,b</ref>) shows that a greater numbers of message-passing steps M yielded improved performance in both one-step and rollout accuracy. This is likely because increasing M allows computing longer-range, and more complex, interactions among particles. Because computation time scales linearly with M , in practice it is advisable to use the smallest M that still provides desired performance.</p><p>For (2), Figure <ref type="figure" target="#fig_11">4(c,d</ref>) shows that models with unshared GN parameters in the PROCESSOR yield better accuracy, especially for rollouts. Shared parameters imposes a strong inductive bias that makes the PROCESSOR analogous to a recurrent model, while unshared parameters are more analogous to a deep architecture, which incurs M times more parameters. In practice, we found marginal difference in computational costs or overfitting, so we conclude that using unshared parameters has little downside. For (3), Figure <ref type="figure" target="#fig_11">4</ref>(e,f) shows that greater connectivity R values yield lower error. Similar to increasing M , larger neighborhoods allow longer-range communication among nodes. Since the number of edges increases with R, more computation and memory is required, so in practice the minimal R that gives desired performance should be used.</p><p>For (4), we observed that rollout accuracy is best for an intermediate noise scale (see Figure <ref type="figure" target="#fig_11">4(g,h</ref>)), consistent with our motivation for using it (see Section 4.3). We also note that one-step accuracy decreases with increasing noise scale. This is not surprising: adding noise makes the training distribution less similar to the uncorrupted distribution used for one-step evaluation.</p><p>For (5), Figure <ref type="figure" target="#fig_11">4(i,j</ref>) shows that the relative ENCODER is clearly better than the absolute version. This is likely because the underlying physical processes that are being learned are invariant to spatial position, and the relative EN-CODER's inductive bias is consistent with this invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparisons to previous models</head><p>We compared our approach to two recent papers which explored learned fluid simulators using particle-based approaches. <ref type="bibr" target="#b17">Li et al. (2018)</ref>'s DPI studied four datasets of fluid, deformable, and solid simulations, and presented four different, distinct architectures, which were similar to Sanchez-Gonzalez et al. ( <ref type="formula">2018</ref>)'s, with additional features such as as hierarchical latent nodes. When training our GNS model on DPI's BOXBATH domain, we found it could learn to simulate the rigid solid box floating in water, faithfully maintaining the stiff relative displacements among rigid particles, as shown Figure <ref type="figure" target="#fig_11">4</ref>(k) and this video. Our GNS model did not require any modification-the box particles' material type was simply a feature in the input vector-while DPI required a specialized hierarchical mechanism and forced all box particles to preserve their relative displacements with each other. Presumably the relative ENCODER and training noise alleviated the need for such mechanisms. <ref type="formula">2020</ref>)'s CConv propagates information across particles<ref type="foot" target="#foot_3">5</ref> , and uses particle update functions and training procedures which are carefully tailored to modeling fluid dynamics (e.g., an SPH-like local kernel, different sub-networks for fluid and boundary particles, a loss function that weights slow particles with few neighbors more heavily). <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref> reported CConv outperformed DPI, so we quantitatively compared our GNS model to CConv. We implemented CConv as described in its paper, plus two additional versions which borrowed our noise and multiple input states, and performed hyperparameter sweeps over various CConv parameters. Figure <ref type="figure" target="#fig_11">4(m)</ref> shows that across all six domains we tested, our GNS model with default hyperparameters has better rollout accuracy than the best CConv model (among the different versions and hyperparameters) for that domain. In this comparison video, we observe than CConv performs well for domains like water, which it was built for, but struggles with some of our more complex materials. Similarly, in a CConv rollout of the BOXBATH DOMAIN the rigid box loses its shape (Figure <ref type="figure" target="#fig_11">4</ref>(l)), while our method preserves it. See Supplementary Materials D for full details of our DPI and CConv comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ummenhofer et al. (</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a powerful, general-purpose machine learning framework for learning to simulate complex systems, based on particle-based representations of physics and learned message-passing on graphs. Our experimental results show our single GNS architecture can learn to simulate the dynamics of fluids, rigid solids, and deformable materials, interacting with one another, using tens of thousands of particles over thousands time steps. We find our model is simpler, more accurate, and has better generalization than previous approaches.</p><p>While here we focus on mesh-free particle methods, our GNS approach may also be applicable to data represented using meshes, such as finite-element methods. There are also natural ways to incorporate stronger, generic physical knowledge into our framework, such as Hamiltonian mechanics <ref type="bibr" target="#b25">(Sanchez-Gonzalez et al., 2019)</ref> and rich, architecturally imposed symmetries. To realize advantages over traditional simulators, future work should explore how to parameterize and implement GNS computations more efficiently, and exploit the ever-improving parallel compute hardware. Learned, differentiable simulators will be valuable for solving inverse problems, by not strictly optimizing for forward prediction, but for inverse objectives as well.</p><p>More broadly, this work is a key advance toward more sophisticated generative models, and furnishes the modern AI toolkit with a greater capacity for physical reasoning. P(X t 0:K ) represents a distribution over state trajectories, starting from the initial conditions X t0 , over K timesteps. The Xt 1:K s θ ,X t 0 indicates the simulated rollout generated by s θ given X t0 . The objective function L, considers the whole trajectory generated by s θ . In this work, we specifically train our GNS model on a one-step loss function, L 1-step , with</p><formula xml:id="formula_18">θ * 1-step ← arg θ min E P(X t k:k+1 ) L 1-step (X t k+1 , s θ (X t k )) .</formula><p>This imposes a stronger inductive bias that physical dynamics are Markovian, and should operate the same at any time during a trajectory.</p><p>In fact, we note that optimizing for whole trajectories may not actually not be ideal, as it can allow the simulator to learn biases which may not be hold generally. In particular, an L which considers the whole trajectory means θ * does not necessarily equal the θ * 1-step that would optimize L 1-step . This is because optimizing a capacity-limited simulator model for whole trajectories might benefit from producing greater one-step errors at certain times, in order to allow for better overall performance in the long term. For example, imagine simulating an undamped pendulum system, where the initial velocity of the bob is always zero. The physics dictate that in the future, whenever the bob returns to its initial position, it must always have zero velocity. If s θ cannot learn to approximate this system exactly, and makes mistakes on intermediate timesteps, this means that when the bob returns to its initial position it might not have zero velocity. Such errors could accumulate over time, and causes large loss under an L which considers whole trajectories. The training process could overcome this by selecting θ * which, for example, subtly encodes the initial position in the small decimal places of its predictions, which the simulator could then exploit by snapping the bob back to zero velocity when it reaches that initial position. The resulting s θ * may be more accurate over long trajectories, but not generalize as well to situations where the initial velocity is not zero. This corresponds to using the predictions, in part, as a sort of memory buffer, analogous to a recurrent neural network.</p><p>Of course, a simulator with a memory mechanism can potentially offer advantages, such as being better able to recognize and respect certain symmetries, e.g., conservation of energy and momentum. An interesting area for future work is exploring different approaches for training learnable simulators, and allowing them to store information over rollout timesteps, especially as a function for how the predictions will be used, which may favor different trade-offs between accuracy over time, what aspects of the predictions are most important to get right, generalization, etc. Input and output representations. We define the input "velocity" as average velocity between the current and previous timestep, which is calculated from the difference in position, ṗt k ≡ p t k − p t k−1 (omitting constant ∆t for simplicity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Supplementary experimental methods</head><p>Similarly, we use as target the average acceleration, calculated between the next and current timestep, pt k ≡ ṗt k+1 − ṗt k .</p><p>Target accelerations are thus calculated as, pt k = p t k+1 − 2p t k + p t k−1 .</p><p>We express the material type (water, sand, goop, rigid, boundary particle) as a particle feature, a i , represented with a learned embedding vector of size 16. For datasets with fixed flat orthogonal walls, instead of adding boundary particles, we add a feature to each node indicating the vector distance to each wall. Crucially, to maintain spatial translation invariance, we clip this distance to the connectivity radius R, achieving a similar effect to that of the boundary particles. In FLUIDSHAKE, particle positions were provided in the coordinate frame of the container, and the container position, velocity and acceleration were provided as 6 global features. In CONTINUOUS a single global scalar was used to indicate the friction angle of the material.</p><p>Building the graph. We construct the graph by, for each particle, finding all neighboring particles within the connectivity radius. We use a standard k-d tree algorithm for this search. The connectivity radius was chosen, such that the number of neighbors in roughly in the range of 10 − 20. We however did not find it necessary to fine-tune this parameter: All 2D scenes of the same resolution share R = 0.015, only the high-res 2D and 3D scenes, which had substantially different particle densities, required choosing a different radius. Note that for these datasets, the radius was simply chosen once based on particle neighborhood size adn total number of edges, and was not fine-tuned as a hyperparameter.</p><p>Neural network parametrizations. We also trained models where we replaced the deep encoder and decoder MLPs by simple linear layers without activations, and observed similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Training</head><p>Noise injection. Because our models take as input a sequence of states (positions and velocities), we draw independent samples ∼ N (0, σ v = 0.0003), for each input state, particle and spatial dimension, before each training step. We accumulate them across time as a random walk, and use this to perturb the stack of input velocities. Based on the updated velocities, we then adjust the position features, such that ṗt k ≡ p t k − p t k−1 is maintained, for consistency. We also experimented with other types of noise accumulation, as detailed in Section C.</p><p>Another way to address differences in training and test input distributions is to, during training, provide the model with its own predictions by rolling out short sequences. <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref>, for example, train with two-step predictions. However computing additional model predictions are more expensive, and in our experience may not generalize to longer sequences as well as noise injection.</p><p>Normalization. To normalize our inputs and targets, we compute the dataset statistics of during training. Instead of using moving averages, which could shift in cycles during training, we instead build exact mean and variance for all of the input and target particle features up seen up to the current training step l, by accumulating the sum, the sum of the squares and the total particle count. The statistics are computed after noise is applied to the inputs.</p><p>Loss function and optimization procedures. We load the training trajectories sequentially, and use them to generate input and target pairs (from a 1000-step long trajectory we generate 995 pairs, as we condition on 5 past states), and sample input-target pairs from a shuffle buffer of size 10k. Rigid obstacles, such as the ramps in WATER-RAMPS, are represented as boundary particles. Those particles are treated identical to regular particles, but they are masked out of the loss. Similarly, in the ground truth simulations, particles sometimes clip through the boundary and disappear; we also mask those particles out of the loss.</p><p>Due to normalization of predictions and targets, our prediction loss is normalized, too. This allows us to choose a scale-free learning rate, across all datasets. To optimize the loss, we use the Adam optimizer <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2014</ref>) (a form of stochastic gradient descent), with minibatches of size 2 averaging the loss for all particles in the batch. We performed a maximum of 20M gradient update steps, with an exponentially decaying learning rate, α(j), where on the j-th gradient update step, α(j) = α final + (α start − α final ) • 0.1 (j•5•10 6 ) , with α start = 10 −4 and α final = 10 −6 . While models can train in significantly less steps, we avoid aggressive learning rates to reduce variance across datasets and make comparisons across settings more fair.</p><p>We train our models using second generation TPUs and V100 GPUs interchangeably. For our datasets, we found that training time per example with a single TPU chip or a single V100 GPU was about the same. TPUs allowed for faster training through fast batch parallelism (each element of a batch on a separate TPU) and model parallelism (a single training distributed over multiple chips) <ref type="bibr" target="#b15">(Kumar et al., 2019)</ref>. Furthermore, since TPUs require fixed size tensors, instead of just padding each device's training example to a maximum number of nodes/edges, we set the fixed size to correspond to the largest graph in the dataset, and build a larger minibatch using multiple training examples whenever they would fit within the fixed sizes. This yielded an effective batch size between 1 (large examples) and 3 examples (small examples) per device and is equivalent to setting a mini batch size in terms of total number of particles per batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Distributional evaluation metrics</head><p>An MSE metric of zero indicates that the model perfectly predicts where each particle has traveled. However, if the model's predicted positions for particles A and B exactly match true positions of particles B and A, respectively, the MSE could be high, even though the predicted and true distributions of particles match. So we also explored two metrics that are invariant under particle permutations, by measuring differences between the distributions of particles: optimal transport (OT) <ref type="bibr" target="#b35">(Villani, 2003)</ref> using 2D or 3D Wasserstein distance and approximated by the Sinkhorn Algorithm <ref type="bibr" target="#b5">(Cuturi, 2013)</ref>, and maximum mean discrepancy (MMD) <ref type="bibr" target="#b7">(Gretton et al., 2012)</ref> with a Gaussian kernel bandwidth of σ = 0.1. These distributional metrics may be more appropriate when the goal is to predict what regions of the space will be occupied by the simulated material, or when the particles are sampled from some continuous representation of the state and there are no "true" particles to compare predictions to. We will analyze some of our results using those metrics in Section C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supplementary results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Architectural choices with minor impact on performance</head><p>We provided additional ablation scans on the GOOP dataset in We enable the global mechanisms of the GNs. This includes both, explicit input global features (instead of appending them to the nodes) and a global latent state that updates after every message passing step using aggregated information from all edges and nodes. We do not find significant differences when doing this, however we speculate that generalization performance for systems with more particles than used during training would be affected.</p><p>Use edges latent. <ref type="bibr">(Figure C.2k,</ref><ref type="bibr">m)</ref> We disabled the updates to the latent state of the edges that is performed at each edge message passing iteration, but found no significant differences in performance.</p><p>Add gravity acceleration. <ref type="bibr">(Figure C.2m,</ref><ref type="bibr">n)</ref> We attempted adding gravity accelerations to the model outputs in the update procedure, so the model would not need to learn to predict a bias acceleration due to gravity, but found no significant performance differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Noise-related training parameters</head><p>We provide some variations related to how we add noise to the input data on the GOOP dataset in We experimented with 4 different modes for adding noise to the inputs. only last adds noise only to the velocity of the most recent state in the input sequence of states. correlated draws a single per-particle and per-dimension set of noise samples, and applies the same noise to the velocities of all input states in the sequence. uncorrelated draws independent noise for the velocity of each input state. random walk draws noise for each input state, adding it to the noise of the previous state in sequence as in a random random walk, as an attempt to simulate accumulation of error in a rollout. In all cases the input states positions are adjusted to maintain ṗt k ≡ p t k − p t k−1 . To facilitate the comparison, the variance of the generated noise is adjusted so the variance of the velocity noise at the last step is constant. We found the best rollout performance for random walk noise type.</p><p>Noise Std. (Figure C.2b,f) This is described in the main text, included here for completeness.</p><p>Reconnect graph after noise. <ref type="bibr">(Figure C.2c,</ref><ref type="bibr">g)</ref> We found that the performance did not change regardless of whether we recalculated the connectivity of the graph after applying noise to the positions or not.</p><p>Fraction of noise to correct. <ref type="bibr">(Figure C.2d,</ref><ref type="bibr">h)</ref> In the process of corrupting the input position and velocity features with noise, we do not adjust the target accelerations, that is, we do not ask the model to predict the accelerations that would correct the noise in the input positions. For this variation we modify the target average accelerations to force the model to predict accelerations that would also correct for 10%, 30% or 100% of the the noise in the inputs. This it implicitly what happens when the loss is defined directly on the positions, regardless of whether the inputs are perturbed with noise <ref type="bibr" target="#b24">(Sanchez-Gonzalez et al., 2018)</ref> or the inputs have noise due to model error <ref type="bibr">(Ummenhofer et al., 2020). Figure C.2d,h</ref> show that asking the model to correct for a large fraction of the noise leads to worse performance. We speculate that this is because physically valid distributions are very complex and smooth in these datasets, and unlike in the work by <ref type="bibr" target="#b24">Sanchez-Gonzalez et al. (2018)</ref>, once noise is applied, it is not clear which is the route the model should take to bring the state back to a valid point in the distribution, resulting in large variance during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Distributional evaluation metrics</head><p>Generally we find that MSE and the distributional metrics lead to generally similar conclusions in our analyses (see <ref type="bibr">Figure C.3</ref>), though we notice that differences in the distributional metrics' values for qualitatively "good" and "bad" rollouts can be more prominent, and match more closely with our subjective visual impressions.   trajectories in the evaluation datasets (colored curves). MSE tends to grow as function of time due to the chaotic character of the systems. Optimal transport and MMD, which are invariant to particle permutations, tend to decrease for WATER and SAND towards the end of the rollout as they equilibrate towards a single consistent group of particles, due to the lower friction/viscosity. For GOOP, which can remain in separate clusters in a chaotic manner, the Optimal Transport error can still be very high. type of edge update function.</p><p>CConv relates to CNNs (with stride of 1) and GNs in two ways. First, in CNNs, CConv, and GNs, each element (e.g., pixel, feature vector, particle) is updated as a function of its neighbors. In CNNs the neighborhood is fixed and defined by the kernel's dimensions, while in CConv and GNs the neighborhood varies and is defined by connected edges (in CConv the edges connect to nearest neighbors).</p><p>Second, CNNs, CConv, and GNs all apply a function to element i's neighbors, j ∈ N (i), pool the results from within the neighborhood, and update element i's representation. In a CNN, this is computed as, f i = σ b + j∈N (i) W (τ i,j )f j , where W (τ i,j ) is a matrix whose parameters depend on the displacement between the grid coordinates of i and j, τ i,j = x j − x i (and b is a bias vector, and σ is a non-linear activation). Because there are a finite set of τ i,j values, one for each coordinate in the kernel's grid, there are a finite set of W (τ i,j ) parameterizations.</p><p>CConv uses a similar formula, except the particles' continuous coordinates mean a choice must be made about how to parameterize W (τ i,j ). Like in CNNs, CConv uses a finite set of distinct weight matrices, Ŵ ( τi,j ), associated with the discrete coordinates, τi,j , on the kernel's grid. For the continuous input τ i,j , the nearest Ŵ (τ i,j ) are interpolated by the fractional component of τ i,j . In 1D this would be linear interpolation, W (τ i,j ) = (1 − d) Ŵ ( τ i,j ) + d Ŵ ( τ i,j )), where d = τ i,j − τ i,j . In 3D, this is trilinear interpolation.</p><p>A GN can implement CNN and CConv computations by representing τ i,j using edge attributes, e i,j , and an edge update function which uses independent parameters for each τ i,j , i.e., e i,j = φ e (e i,j , v i , v j ) = φ e τi,j (v j ). Beyond their displacement-specific edge update function, CNNs and CConv are very similar to how graph convolutional networks (GCN) <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016)</ref> work. The full CConv update as described in <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref> is,</p><formula xml:id="formula_19">f i = 1 ψ(xi)</formula><p>j∈N (xi,R) a (x j , x i ) f j g (Λ (x j − x i )). In particular, it indexes into the weight matrices via a polar-to-Cartesian coordinate transform, Λ, to induce a more radially homogeneous parameterization. It also uses a weighted sum over the particles in a neighborhood, where the weights, a(x j , x i ), are proportional to the distance between particles. And it includes a normalization, ψ(x i ), for neighborhood size, they set it to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance comparisons.</head><p>We implemented the CConv model, loss and training procedure as described by <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref>. For simplicity, we only tested the CConv model on datasets with flat walls, rather than those with irregular geometry. This way we could omit the initial convolution with the boundary particles and instead give the fluid particles additional simple features indicating the vector distance to each wall, clipped by the radius of connectivity, as in our model. This has the same spatial constraints as CConv with boundary particles in the wall, and should be as or more informative than boundary particles for square containers. Also, for environments with multiple materials, we appended a particle type learned embedding to the input node features.</p><p>To be consistent with <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref>, we used their batch size of 16, learning rate decay of 10 −3 to 10 −5 for 50k iterations, and connectivity radius of 4.5x the particle radius. We were able to replicate their results on PBD/FLeX and SPH simulator datasets similar to the datasets presented in their paper. To allow a fair comparison when evaluating on our MPM datasets, we performed additional hyperparameter sweeps over connectivity particle radius, learning rate, and number of training iterations using our GOOP dataset. We used the best-fitting parameters on all datasets, analogous to how we selected hyperparameters for our GNS model.</p><p>We also implemented variations of CConv which used noise to corrupt the inputs during training (instead of using 2-step loss), as we did with GNS. We found that the noise improved CConv's rollout performance on most datasets. In our comparisons, we always report performance for the best-performing CConv variant.</p><p>Our qualitative results show CConv can learn to simulate sand reasonably well. But it struggled to accurately simulate solids with more complex fine grained dynamics. In the BOXBATH domain, CConv simulated the fluid well, but struggled to keep the box's shape intact. In the GOOP domain, CConv struggled to keep pieces of goop together and handle the rest state, while in MULTIMATERIAL it exhibited local "explosions", where regions of particles suddenly burst outward (see video). Generally CConv's performance is strongest for simulating water-like fluids, which is primarily what it was applied to in the original paper. However it still did not match our GNS model, and for other materials, and interactions between different materials, it was clearly not as strong. This is not particularly surprising, given that our GNS is a more general model, and our neural network implementation has higher capacity on several axes, e.g., more message-passing steps,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Rollouts of our GNS model. It learns to simulate rich materials at resolutions sufficient for high-quality rendering [video].</figDesc><graphic url="image-1.png" coords="1,307.80,183.63,233.27,211.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E A 5 8 G T T U FU P e O o L M C o 5 c R Q 5 l A T s = " &gt; A A A C F H i c b V C 7 T s M w F H V 4 l v I K Z W S x q J A Y U J V U k W C s x M J Y J P q Q 2 h I 5 r t O a 2 n F k O 4 g q y m 8 w s s J H s C F W d r 6 B n 8 B p M 9 C W I 1 k 6 O u d e 3 e M T x I w q 7 T j f 1 t r 6 x u b W d m m n v L u 3 f 3 B o H 1 X a S i Q S k x Y W T M h u g B R h N C I t T T U j 3 V g S x A N G O s H k O v c 7 j 0 Q q K q I 7 P Y 3 J g K N R R E O K k T a S b 1 f 6 H O l x E K Y k u 3 f 8 l F 4 8 Z L 5 d d W r O D H C V u A W p g g J N 3 / 7 p D w V O O I k 0 Z k i p n u v E e p A i q S l m J C v 3 E 0 V i h C d o R H q G R o g T N U h n 2 T N 4 Z p Q h D I U 0 L 9 J w p v 7 d S B F X a s o D M 5 k n V c t e L v 7 n 9 R I d X g 1 S G s W J J h G e H w o T B r W A e R F w S C X B m k 0 N Q V h S k x X i M Z I I a 1 P X 4 h U 2 E m Z g z O v m M 6 Y d d 7 m L V d K u 1 1 y v 5 t 1 6 1 Y Z X 9 F Q C J + A U n A M X X I I G u A F N 0 A I Y P I E X 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>X b e n Q / n c x U t O P n M B V q D 8 / U L G H u d m g = = &lt; / l a t e x i t &gt; Pass messages &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 t a c S Z Q O x 3 e L p A B d Y E b e V l X f 9 U U = " &gt; A A A C D X i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l Z l S 0 G X B j c s K 9 g H t U D J p p g 3 N Y 0 w y Q h n 6 D S 7 d 6 k e 4 E 7 d + g 9 / g T 5 i 2 s 7 C t F w K H c 8 5 9 5 E Q J Z 8 b 6 / r e 3 s b m 1 v b N b 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>D H e 6 g A U 0 g 8 A g v 8 A p v 3 r P 3 7 n 1 4 n w v r h p f 3 n M F S e V + / 0 I O c b Q = = &lt; / l a t e x i t &gt; Extract dynamics info &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z K Y S F s M / d f 4 n T I 3 c K P X E j + e V B e A = " &gt; A A A C F 3 i c b V D L S g M x F M 3 4 r P U 1 K q 7 c B I v g q s y U g i 4 L I r i s Y B / Q D i W T Z t r Q P I Y k I w 5 D P 8 S l W / 0 I d + L W p d / g T 5 h p Z 2 F b D w Q O 5 9 z L u T l h z K g 2 n v f t r K 1 v b G 5 t l 3 b K u 3 v 7 B 4 f u 0 X F b y 0 R h 0 s K S S d U N k S a M C t I y 1 D D S j R V B P G S k E 0 5 u c r / z S J S m U j y Y N C Y B R y N B I 4 q R s d L A P b 1 9 M g p h A 4 e p Q J x i D a m I 5 M C t e F V v B r h K / I J U Q I H m w P 3 p D y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>h e r p F 2 r + v V q / b 5 W a d S L n k r g D J y D S + C D K 9 A A d 6 A J W g C D D L y A V / D m P D v v z o f z O R 9 d c 4 q d E 7 A A 5 + s X m i y f / g = = &lt; / l a t e x i t &gt; Learned simulator, s ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 W S v 3 E Z Z l e 0 2 B J I l G h j 8 / b k r o Y M = " &gt; A A A C H 3 i c b V D L S g N B E J z 1 / T b q U Y T B K H i Q s B s C e h S 8 e P C g Y F R I Q u i d d J L B e S w z v U I I n v w S j 1 7 1 I 7 y J 1 3 y D P + H k c f B V M F B U V d P T l W Z K e o r j Q T Q 1 P T M 7 N 7 + w u L S 8 s r q 2 X t j Y v P Y 2 d w K r w i r r b l P w q K T B K k l S e J s 5 B J 0 q v E n v T o f + z T 0 6 L 6 2 5 o l 6 G D Q 0 d I 9 t S A A W p W d g 5 R 3 A G W 9 x L n S s g 6 w 7 5 n m / W q Y s E e 8 1 C M S 7 F I / C / J J m Q I p v g o l n 4 r L e s y D U a E g q 8 r y V x R o 0 + O J J C 4 c N S P f e Y g b i D D t Y C N a D R N / q j M x 7 4 f l B a v G 1 d e I b 4 S P 0 + 0 Q f t f U + n I a m B u v 6 3 N x T / 8 2 o 5 t Y 8 b f W m y n N C I 8 a J 2 r j h Z P u y E t 6 R D Q a o X C A g n w 1 + 5 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>c A X a L C &lt; / l a t e x i t &gt; + + Y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C 1 x y l l p m Z M U Y K + X N x 5 + 4 V u B P s q 8 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e C F 4 8 t 2 A 9 p Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>a s 1 a p 1 / I 4 i n A G 5 3 A J H l x D H e 6 g A S 1 g g P A M r / D m P D o v z r v z s W w t O P n M K f y B 8 / k D t U 2 M 1 g = = &lt; / l a t e x i t &gt; Encoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 + X k K 0 1 4 v L 9 0 v l 0 I j 8 X d 5 r h J 1 O 0 = " &gt; A A A B + X i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 B I v g q S S l o M e C C B 4 r 2 F Z o Q 9 l s J u 3 S z S b s T o o l 9 J 9 4 8 a C IV / + J N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S q 4 R t f 9 t k o b m 1 v b O + X d y t 7 + w e G R f X z S 0 U m m G L R Z I h L 1 G F A N g k t o I 0 c B j 6 k C G g c C u s H 4 Z u 5 3 J 6 A 0 T + Q D T l P w Y z q U P O K M o p E G t t 1 H e E L N 8 l v J k h D U b G B X 3 Z q 7 g L N O v I J U S Y H W w P 7 q h w n L Y p D I B N W 6 5 7 k p + j l V y J m A W a W f a U g p G 9 M h 9 A y V N A b t 5 4 v L Z 8 6 F U U I n S p Q p i c 5 C / T 2 R 0 1 j r a R y Y z p j i S K 9 6 c / E / r 5 d h d O 3 n X K Y Z g m T L R V E m H E y c e Q x O y B U w F F N D K F P c 3 O q w E V W U o Q m r Y k L w V l 9 e J 5 1 6 z W v U G v f 1 a r N R x F E m Z + S c X B K P X J E m u S M t 0 i a M T M g z e S V v V m 6 9 W O / W x 7 K 1 Z B U z p + Q P r M 8 f I 5 6 T 8 Q = = &lt;/ l a t e x i t &gt; Decoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M H O 1 R h I v S d Z t p + J f F q B 8 9 6 6 d j G s = " &gt; A A A B + X i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 L B b B U 0 l K Q Y 8 F P X i s Y F u h D W W z m b R L N x / s T o o l 9 J 9 4 8 a C I V / + J N / + N 2 z Y H b X 0 w 8 H h v h p l 5 f i q F R s f 5 t k o b m 1 v b O + X d y t 7 + w e G R f X z S 0 U m m O L R 5 I h P 1 6 D M N U s T Q R o E S H l M F L P I l d P 3 x z d z v T k B p k c Q P O E 3 B i 9 g w F q H g D I 0 0 s O 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>r H f r Y 9 l a s o q Z U / I H 1 u c P F D m T 5 w = = &lt; / l a t e x i t &gt; Processor &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 2 O u 2 I N 2 X B t / d + R Y y z 8 H 4 c C 7 D N 0 = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 4 W e t X r E c v w S J 4 K k k p 6 L H g x W M F + w F t K J v t p F 2 6 y Y b d i b S E / B U v H h T x 6 h / x 5 r 9 x 2 + a g r Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " / e F K t Y h N q p N 7 e D 7 T 8 u y S d b f G q n E = " &gt; A A A B 7 3 i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 k k o M e C F 4 8 V 7 A e 0 o W y 2 k 3 b p Z h N 3 J 0 I J / R N e P C j i 1 b / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 5 p m y T T H F o 8 k Y n u h s y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>k 5 J 5 f E I 9 e k Q e 5 I k 7 Q I J 5 I 8 k 1 f y 5 j w 6 L 8 6 7 8 7 F s L T n F z C n 5 A + f z B y + q k A c = &lt; / l a t e x i t &gt; s ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / e F K t Y h N q p N 7 e D 7 T 8 u y S d b f G q n E = " &gt; A A A B 7 3 i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 k k o M e C F 4 8 V 7 A e 0 o W y 2 k 3 b p Z h N 3 J 0 I J / R N e P C j i 1 b / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 5 p m y T T H F o 8 k Y n u h s y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 2. (a) Our GNS predicts future states represented as particles using its learned dynamics model, d θ , and a fixed update procedure. (b) The d θ uses an "encode-process-decode" scheme, which computes dynamics information, Y , from input state, X. (c) The ENCODER constructs latent graph, G 0 , from the input state, X. (d) The PROCESSOR performs M rounds of learned message-passing over the latent graphs, G 0 , . . . , G M . (e) The DECODER extracts dynamics information, Y , from the final latent graph, G M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (left) Effect of different ablations (grey) against our model (red) on the one-step error (a,c,e,g,i) and the rollout error (b,d,f,h,j). Bars show the median seed performance averaged across the entire GOOP test dataset. Error bars display lower and higher quartiles, and are shown for the default parameters. (right) Comparison of average performance of our GNS model to CConv. (k,l) Qualitative comparison between GNS (k) and CConv (l) in BOXBATH after 50 rollout steps (video link). (m) Quantitative comparison of our GNS model (red) to the CConv model (grey) across the test set . For our model, we trained one or more seeds using the same set of hyper-parameters and show results for all seeds. For the CConv model we ran several variations including different radius sizes, noise levels, and number of unroll steps during training, and show the result for the best seed. Errors bars show the standard error of the mean across all of the trajectories in the test set (95% confidence level).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Figure C.1, which show that the model is robust to typical hyperparameter changes. Number of input steps C. (Figure C.2a,b) We observe a significant improvement from conditioning the model on just the previous velocity (C = 1) to the two most recent velocities (C = 2), but find similar performance for larger values of C. All our models use C = 5. Number of MLP hidden layers. (Figure C.2c,d) Except for the case of zero hidden layers (linear layer), the performance does not chance much as a function of the MLP depth. Use MLP encoder &amp; decoder. (Figure C.2e,f) We replaced the MLPs used in the encoder and decoder by linear layers (single matrix multiplication followed by a bias), and observed no significant changes in performance. Include self-edges. (Figure C.2g,h) The model performs similarly regardless of whether self-edges are included in the message passing process or not. Use global latent. (Figure C.2i,j)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure C. 2 .</head><label>2</label><figDesc>Noise type.(Figure C.2a,e)  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Figure C.4 shows the rollout errors as a function of the key architectural choices from Figure 4 using these distributional metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Figure C.1. Additional ablations (grey) on the GOOP dataset compared to our default model (red). Error bars display lower and higher quartiles, and are shown for the default parameters. The same vertical limits from Figure 4 are reused for easier qualitative scale comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Figure C.3. Rollout error of one of our models as a function of time for water, sand and goop, using MSE, Optimal transport and MMD as metrics. Figures show curves for 3 trajectories in the evaluation datasets (colored curves). MSE tends to grow as function of time due to the chaotic character of the systems. Optimal transport and MMD, which are invariant to particle permutations, tend to decrease for WATER and SAND towards the end of the rollout as they equilibrate towards a single consistent group of particles, due to the lower friction/viscosity. For GOOP, which can remain in separate clusters in a chaotic manner, the Optimal Transport error can still be very high.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Figure C.4. Effect of different ablations on Optimal Transport (top) and Maximum Mean Discrepancy (bottom) rollout errors. Bars show the median seed performance averaged across the entire test dataset. Error bars display lower and higher quartiles, and are shown for the default parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-6.png" coords="6,55.44,67.07,485.91,160.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>which is computed iteratively by, Xt k+1 = s( Xt k ) for each timestep. Simulators compute dynamics information that reflects how the current state is changing, and use it to update the current state to a predicted future state (see Figure2(a)). An example is a numerical differential equation solver: the equations compute dynamics information, i.e., time derivatives, and the integrator is the update mechanism.</figDesc><table /><note>A learnable simulator, s θ , computes the dynamics information with a parameterized function approximator, d θ : X → Y, whose parameters, θ, can be optimized for some training objective. The Y ∈ Y represents the dynamics information, whose semantics are determined by the update mechanism. The update mechanism can be seen as a function which takes the Xt k , and uses d θ to predict the next state, Xt k+1 = Update( Xt k , d θ ). Here we assume a simple update mechanism-an Euler integrator-and Y that represents accelerations. However, more sophisticated update procedures which call d θ more than once can also be used, such as higher-order integrators (e.g.,<ref type="bibr" target="#b25">Sanchez-Gonzalez et al. (2019)</ref>).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>List of maximum number of particles N , sequence length K, and quantitative model accuracy (MSE) on the held-out test set. All domain names are also hyperlinks to the video website. Note, since K varies across datasets, the errors are not directly comparable to one another.</figDesc><table><row><cell>Experimental domain</cell><cell>N</cell><cell>K</cell><cell>1-step (×10 −9 )</cell><cell>Rollout (×10 −3 )</cell></row><row><cell>WATER-3D (SPH)</cell><cell>14k</cell><cell>800</cell><cell>8.66</cell><cell>10.1</cell></row><row><cell>SAND-3D</cell><cell>19k</cell><cell>400</cell><cell>1.42</cell><cell>0.554</cell></row><row><cell>GOOP-3D</cell><cell>15k</cell><cell>300</cell><cell>1.32</cell><cell>0.618</cell></row><row><cell>WATER-3D-S (SPH)</cell><cell>6k</cell><cell>800</cell><cell>9.66</cell><cell>9.52</cell></row><row><cell>BOXBATH (PBD)</cell><cell>1k</cell><cell>150</cell><cell>54.5</cell><cell>4.2</cell></row><row><cell>WATER</cell><cell>2k</cell><cell>1000</cell><cell>2.83</cell><cell>20.6</cell></row><row><cell>SAND</cell><cell>2k</cell><cell>320</cell><cell>6.23</cell><cell>2.37</cell></row><row><cell>GOOP</cell><cell>2k</cell><cell>400</cell><cell>2.95</cell><cell>1.96</cell></row><row><cell>MULTIMATERIAL</cell><cell>2k</cell><cell>1000</cell><cell>1.81</cell><cell>16.9</cell></row><row><cell>FLUIDSHAKE</cell><cell cols="2">1.4k 2000</cell><cell>2.17</cell><cell>21.1</cell></row><row><cell>WATERDROP</cell><cell>2k</cell><cell>1000</cell><cell>1.54</cell><cell>7.09</cell></row><row><cell>WATERDROP-XL</cell><cell>8k</cell><cell>1000</cell><cell>1.23</cell><cell>14.9</cell></row><row><cell>WATERRAMPS</cell><cell>2.5k</cell><cell>600</cell><cell>4.91</cell><cell>11.6</cell></row><row><cell>SANDRAMPS</cell><cell>3.5k</cell><cell>400</cell><cell>2.77</cell><cell>2.07</cell></row><row><cell>RANDOMFLOOR</cell><cell>3.5k</cell><cell>600</cell><cell>2.77</cell><cell>6.72</cell></row><row><cell>CONTINUOUS</cell><cell>5k</cell><cell>400</cell><cell>2.06</cell><cell>1.06</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Similarly, relative velocities could be used to enforce invariance to inertial frames of reference.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">  3  In preliminary experiments we also attempted using a PRO-CESSOR with a full GN and a global latent state, for which the global features g are encoded with a separate ε g MLP.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">All rollout videos can be found here: https://sites. google.com/view/learning-to-simulate</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">The authors state CConv does not use an explicit graph representation, however we believe their particle update scheme can be interpreted as a special type of message-passing on a graph. See Supplementary Materials D.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">We also ran experiments with C=5 and did not find any meaningful difference in performance. The results in Table1and the corresponding example video are run with C=5 for consistency with our other experiments</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Victor Bapst and Jessica Hamrick for valuable feedback on the work and manuscript, and we thank Benjamin Ummenhofer for advice on implementing the continuous convolution baseline model.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Example failure cases</head><p>In this video, we show two of the failure cases we sometimes observe with the GNS model. In the BOXBATH domain we found that our model could accurately predict the motion of a rigid block, and maintain its shape, without requiring explicit mechanisms to enforce solidity constraints or providing the rest shape to the network. However, we did observe limits to this capability in a harder version of BOXBATH, which we called FLUIDSHAKE-BOX, where the container is vigorously shaken side to side, over a rollout of 1500 timesteps. Towards the end of the trajectory, we observe that the solid block starts to deform. We speculate the reason for this is that GNS has to keep track of the block's original shape, which can be difficult to achieve over long trajectories given an input of only 5 initial frames.</p><p>In the second example, a bad seed of our model trained on the GOOP domain predicts a blob of goop stuck to the wall instead of falling down. We note that in the training data, the blobs do sometimes stick to the wall, though it tends to be closer to the floor and with different velocities. We speculate that the intricacies of static friction and adhesion may be hard to learn-to learn this behaviour more robustly, the model may need more exposure to fall versus sticking phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Supplementary baseline comparisons D.1. Continuous convolution (CConv)</head><p>Recently <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref> presented Continuous Convolution (CConv) as a method for particle-based fluid simulation. We show that CConv can also be understood in our framework, and compare CConv to our approach on several tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretation.</head><p>While <ref type="bibr" target="#b33">Ummenhofer et al. (2020)</ref> state that "Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors.", we find we can express CConv (which itself is a generalization of CNNs) as a GN <ref type="bibr" target="#b2">(Battaglia et al., 2018)</ref> with a specific pairwise interaction functions, more flexible function approximators (MLPs with multiple internal layers versus single linear/non-linear layers in CConv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. DPI</head><p>We trained our model on the <ref type="bibr" target="#b17">Li et al. (2018)</ref>'s BOXBATH dataset, and directly compared the qualitative behavior to the authors' demonstration video in this comparison. To provide a fair comparison to DPI, we show a model conditioned on just the previous velocity (C=1) in the above comparison video 6 . While DPI requires a specialized hierarchical mechanism and forced all box particles to preserve their relative displacements with each other, our GNS model faithfully represents the the ground truth trajectories of both water and solid particles without any special treatment. The particles making up the box and water are simply marked as a two different materials in the input features, similar to our other experiments with sand, water and goop. We also found that our model seems to also be more accurate when predicting the fluid particles over the long rollout, and it is able to perfectly reproduce the layering effect for fluid particles at the bottom of the box that exists in the ground truth data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Divergence-free smoothed particle hydrodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koschier</surname></persName>
		</author>
		<idno type="DOI">10.1145/2786784.2786796</idno>
		<ptr target="http://dx.doi.org/10.1145/2786784.2786796" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</title>
				<meeting>the 2015 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00341</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sinkhorn distances: Lightspeed computation of optimal transportation distances</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neuroanimator: Fast neural network emulation and control of physics-based models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual conference on Computer graphics and interactive techniques</title>
				<meeting>the 25th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="9" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to predict the cosmological structure formation</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="13825" to="13832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning to control pdes with differentiable physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Holl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07457</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A moving least squares material point method with displacement discontinuity and two-way rigid body coupling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pradhana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><surname>Difftaichi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00935</idno>
		<title level="m">Differentiable programming for physical simulation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bitorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09756</idno>
		<title level="m">Scale mlperf-0.6 models on google tpu-v3 pods</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-driven fluid simulations using regression forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladickỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01566</idno>
		<title level="m">Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Propagation networks for model-based control under partial observation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1205" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unified particle physics for real-time applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Macklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chentanez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Manessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">107000</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smoothed particle hydrodynamics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Monaghan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of astronomy and astrophysics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="543" to="574" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flexible neural representation for physics prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mrowca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8799" to="8810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Position based dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heidelberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hennix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01242</idno>
		<title level="m">Graph networks as learnable physics engines for inference and control</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Hamiltonian graph networks with ode integrators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12790</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Jax</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04232</idno>
		<title level="m">md: End-to-end differentiable, hardware accelerated, molecular dynamics in pure python</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Application of a particle-in-cell method to solid mechanics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Schreyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer physics communications</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="236" to="252" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stochastic prediction of multi-agent interactions from partial observations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09641</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11044</idno>
		<title level="m">Relational forward models for multi-agent learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Know-evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning representations over dynamic graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><surname>Dyrep</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyePrhR5KX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lagrangian fluid simulation with continuous convolutions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prantl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thürey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1lDoJSYDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural execution of graph algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velikovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkgKO0EtvS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Latent space physics: Towards learning the temporal evolution of fluid flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiewel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003">2003. 2019</date>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
	<note>Topics in optimal transportation</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
