<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PrivGene: Differentially Private Model Fitting Using Genetic Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
							<email>xkxiao@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
							<email>yin.yang@adsc.com.sg</email>
							<affiliation key="aff1">
								<orgName type="department">Advanced Digital Sciences Center</orgName>
								<orgName type="institution">Nanyang Technological University Illinois at Singapore Pte. Ltd</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenjie</forename><surname>Zhang</surname></persName>
							<email>zhenjie@adsc.com.sg</email>
							<affiliation key="aff1">
								<orgName type="department">Advanced Digital Sciences Center</orgName>
								<orgName type="institution">Nanyang Technological University Illinois at Singapore Pte. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marianne</forename><surname>Winslett</surname></persName>
							<email>winslett@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Advanced Digital Sciences Center</orgName>
								<orgName type="institution">Nanyang Technological University Illinois at Singapore Pte. Ltd</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PrivGene: Differentially Private Model Fitting Using Genetic Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DD03CFA93955608A403D64B22738B01C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Management]: Database applications-statistical databases Differential privacy</term>
					<term>genetic algorithms</term>
					<term>model fitting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ε-differential privacy is rapidly emerging as the state-of-the-art scheme for protecting individuals' privacy in published analysis results over sensitive data. The main idea is to perform random perturbations on the analysis results, such that any individual's presence in the data has negligible impact on the randomized results. This paper focuses on analysis tasks that involve model fitting, i.e., finding the parameters of a statistical model that best fit the dataset. For such tasks, the quality of the differentially private results depends upon both the effectiveness of the model fitting algorithm, and the amount of perturbations required to satisfy the privacy guarantees. Most previous studies start from a state-of-theart, non-private model fitting algorithm, and develop a differentially private version. Unfortunately, many model fitting algorithms require intensive perturbations to satisfy ε-differential privacy, leading to poor overall result quality.</p><p>Motivated by this, we propose PrivGene, a general-purpose differentially private model fitting solution based on genetic algorithms (GA). PrivGene needs significantly less perturbations than previous methods, and it achieves higher overall result quality, even for model fitting tasks where GA is not the first choice without privacy considerations. Further, PrivGene performs the random perturbations using a novel technique called the enhanced exponential mechanism, which improves over the exponential mechanism <ref type="bibr" target="#b25">[26]</ref> by exploiting the special properties of model fitting tasks. As case studies, we apply PrivGene to three common analysis tasks involving model fitting: logistic regression, SVM classification, and kmeans clustering. Extensive experiments using real data confirm the high result quality of PrivGene, and its superiority over existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Releasing sensitive information while preserving individuals' privacy has been an active research subject for decades. The recently proposed notion of ε-differential privacy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> is rapidly emerging as the state-of-the-art scheme for this purpose, due to its strong privacy guarantees, and robustness against adversaries with background knowledge. ε-differential privacy publishes randomly perturbed analysis results performed on a sensitive dataset, rather than the dataset itself. The randomized results must ensure that it is hard for the adversary to infer any attribute value of any individual record in the dataset, even if the adversary knows the exact details of all remaining records. This paper focuses on enforcing ε-differential privacy on analysis tasks involving model fitting. Given a statistical model M , a sensitive database D, and a fitting function f (D, ω) that measures how well M with parameter vector ω fits dataset D, model fitting aims to find the best parameter vector ω * for M that maximizes f (D, ω * ). Model fitting is used in a broad class of analysis tasks, including classification (which searches for the best way to distinguish different classes of objects), regression (which finds a mathematical model that describes the data best), and clustering (which computes the optimal assignment of objects to groups). As a real world scenario, consider an electronic medical records database, where it is highly beneficial to publish statistical analysis results, provided that individuals' information is kept private. Since classification, regression, and clustering are common tools <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref> for analyzing medical data, privacy-preserving methods for such analysis are highly valuable.</p><p>As we review in Section 2.2, existing studies on differentially private model fitting mostly develop the private version of a stateof-the-art, non-private model fitting algorithm. This methodology incurs two serious drawbacks. First, each such method is limited to a narrow range of applications that use a specific type of model fitting algorithms. Second and more importantly, the differentially private version of a good non-private algorithm does not necessarily yield high-quality results, because such an algorithm may need large amounts of random perturbations to satisfy ε-differential privacy, which dominates overall result inaccuracy. Many existing model fitting algorithms suffer from this problem due to their inherent complexity, which we elaborate further in Section 2.2. Consequently, the result quality of their differentially private versions often lags far behind the non-private versions. Finally, although generic solutions exist that can in theory render any analysis results differentially private, the quality of their results tends to be rather poor, if usable at all.</p><p>Motivated by this, we propose PrivGene, a novel framework for differentially private model fitting based on genetic algorithms (GA) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. Similar to conventional GA, PrivGene starts with a set of seed parameter vectors, and iteratively improves them by emulating natural evolutions. In each iteration, PrivGene recombines and modifies existing parameter vectors through crossover and mutation operations, and subsequently filters them through selection of the top ones that best fit the given dataset. Among these operations, crossover and mutation are performed independently of the sensitive data, and, thus, do not have privacy issues at all. Only the selection step involves the data, and requires random perturbations to satisfy ε-differential privacy. PrivGene performs differentially private selections with high accuracy through a novel enhanced exponential mechanism (EEM), which improves over the exponential mechanism <ref type="bibr" target="#b25">[26]</ref> by exploiting the special properties of the selection operations.</p><p>We define a broad class of model fitting tasks that can be solved by PrivGene, and demonstrate its use on three common analysis tools, namely logistic regression, SVM classification and k-means clustering. Although GA is usually not the first choice to solve these problems in the non-private setting, PrivGene outperforms all existing private solutions in terms of overall result quality, due to the former's low accuracy loss during perturbations. Extensive experiments using 6 real datasets confirm the high accuracy achieved by PrivGene and its superiority over previous methods.</p><p>The contributions of this paper are summarized as follows: (i) We propose PrivGene, a novel framework for model fitting under ε-differential privacy, which applies to a broad class of analysis tasks. (ii) We design EEM, a novel mechanism for differentially private parameter vector selection for PrivGene. EEM significantly improves over the exponential mechanism by utilizing the special properties of PrivGene. (iii) We apply PrivGene to three common analysis tasks: logistic regression, SVM classification and k-means clustering. (iv) We demonstrate through extensive experiments that PrivGene significantly outperform existing algorithms for enforcing differential privacy on these types of analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>In this section, we introduce the basic concepts of differential privacy in Section 2.1, and then present an overview of existing solutions for model fitting under differential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Differential Privacy</head><p>As stated in Section 1, ε-differential privacy guarantees the hardness for inferring any attribute value of any record in the dataset, provided that the adversary knows all attribute values of all remaining records. This hardness is controlled by the parameter ε called the privacy budget. Lower privacy budget indicates stronger privacy protection, but also noisier results. Formally, let D be any dataset in the application domain, and D be its neighbor database obtained by replacing a record of D with a new one. Meanwhile, let A be a randomized algorithm, and O be any possible output of A. Then, A satisfies ε-differential privacy, if and only if the following inequality holds <ref type="bibr" target="#b9">[10]</ref>.</p><formula xml:id="formula_0">Pr [A(D) = O] ≤ e ε • Pr A(D ) = O .</formula><p>(1)</p><p>According to the above inequality, a deterministic algorithm cannot possibly satisfy ε-differential privacy for any value of ε. Hence, a deterministic analysis algorithm must be randomly perturbed to satisfy the privacy requirement. Two fundamental approaches for this purpose are the Laplace mechanism <ref type="bibr" target="#b9">[10]</ref> and the exponential mechanism <ref type="bibr" target="#b25">[26]</ref>. The former is limited to analysis tasks that return numeric results, whereas the latter mainly targets tasks with categorical outputs. For simplicity, we present these mechanisms for analysis tasks with a single output -multiple outputs can be handled by utilizing the composability property of differential privacy <ref type="bibr" target="#b8">[9]</ref>. In particular, composability indicates that when a set of (say, m) random algorithms satisfy differential privacy with parameters ε1, ε2, . . . , εm, respectively, the set of algorithms as a whole satisfies i εi -differential privacy. Given an analysis task with a numeric output F and a privacy budget ε, the Laplace mechanism injects into F random Laplace noise <ref type="bibr" target="#b8">[9]</ref> of scale ΔF /ε, where ΔF is called the sensitivity of F. In particular, ΔF is defined as:</p><formula xml:id="formula_1">ΔF = max D,D F(D) -F(D ),<label>(2)</label></formula><p>where D, D are two arbitrary neighbor databases.</p><p>For an analysis task with a categorical output (e.g., an ID), injecting random noise no longer yields meaning results. The exponential mechanism tackles this problem by performing random perturbations during the selection of the output. Specifically, given the set Ω of all possible output values, the user assigns each possible output value ω ∈ Ω a quality score Q(ω); higher scores correspond to better values. Then, the exponential mechanism computes the probability for selecting each possible output value in the domain, and selects one randomly based on these probabilities. In particular, the probability for choosing a value ω satisfies:</p><formula xml:id="formula_2">Pr [ω is selected] ∝ exp ε • Q(ω) 2 maxω∈Ω Δ Q(ω) ,<label>(3)</label></formula><p>where Δ Q(ω) is the sensitivity of Q(ω) as a function of the input database, defined similarly to Equation 2, i.e., the maximum possible difference of Q(ω) for any two neighbor databases. Note that the exponential mechanism does not make any assumptions on the quality function Q(ω). However, as we show later in Section 4, the effectiveness of the exponential mechanism can be significantly improved, by exploiting special properties of Q(ω).</p><p>Based on the Laplace mechanism and the exponential mechanism, a plethora of differentially private methods have been proposed for various analysis tasks. A large number of existing solutions focus on simple aggregate queries such as COUNT and SUM. For instance, Xiao et al. <ref type="bibr" target="#b31">[32]</ref> investigate the problem of range-count answering under differential privacy, and propose Privlet, an effective synopsis based on the Laplace mechanism for answering arbitrary range-counts with a fixed privacy budget. Hay et al. <ref type="bibr" target="#b14">[15]</ref> also study count queries, and propose a post-processing method for improving the accuracy of differentially private synopsis, based on consistency conditions of the application domain. Cormode et al. <ref type="bibr" target="#b3">[4]</ref> design differentially private data structures for accurately answering multi-dimensional range-count queries. Li et al <ref type="bibr" target="#b22">[23]</ref> and Yuan et al. <ref type="bibr" target="#b33">[34]</ref> propose optimized solutions for answering a batch of linear counting queries under differential privacy. Besides simple counting queries, there is also work on publishing data structures consisting of multiple counts. For example, Xu et al. <ref type="bibr" target="#b32">[33]</ref> propose effective methods for publishing differentially private histograms. Ding et al. <ref type="bibr">[8]</ref> publish data cubes with both privacy and consistency guarantees. None of these methods, however, applies to our problem, as model fitting involves a complex optimization solving process, which is inherently more challenging than COUNT/SUM queries and their derivatives.</p><p>Differential privacy has also been applied to complex data min-  <ref type="bibr" target="#b17">[18]</ref> design solutions for record matching under differential privacy. These problems are orthogonal to ours. Finally, Kifer and Machanavajjhala point out the limitations of differential privacy <ref type="bibr" target="#b18">[19]</ref>, and explore alternative privacy definitions <ref type="bibr" target="#b19">[20]</ref>. This work focuses on the original definition of differential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Differentially Private Model Fitting</head><p>A large class of analysis tasks, including regression, classification, and clustering, require model fitting to determine the best parameter set. Existing approaches to differentially private model fitting generally follow the methodology of developing the differentially private version of a commonly used algorithm in the nonprivate setting. The main challenge faced by this methodology is that the sensitivity of such algorithms (and, consequently, the scale of perturbations) is usually prohibitively high, to the extent that direct use of the Laplace mechanism or the exponential mechanism simply returns noise. For instance, Zhang et al. <ref type="bibr" target="#b34">[35]</ref> investigate linear and logistic regressions under differential privacy. In the nonprivate case, the optimal parameter set of linear regression can be solved trivially, with a small number of matrix operations <ref type="bibr" target="#b34">[35]</ref>. Yet, as shown in <ref type="bibr" target="#b34">[35]</ref>, even this simple solver incurs prohibitively high sensitivity. Popular model fitting algorithms for logistic regression, such as iteratively re-weighted least squares <ref type="bibr" target="#b16">[17]</ref>, are much more complex, and incur prohibitive high sensitivity, too.</p><p>The solution proposed in <ref type="bibr" target="#b34">[35]</ref>, called functional mechanism (FM), injects random noise into the fitting function f , which is the only part of the model fitting task that involves the sensitive data. The model fitting task with the noisy f is then published, and subsequently solved by a standard algorithm. The idea is that when the noisy f is close to the original one, hopefully (but there is no guarantee) the former leads to parameters of comparable quality as the latter. The fitting function of linear regression is simple, and applying FM to it is relatively straightforward. The fitting function of logistic regression, however, still incurs prohibitively high sensitivity. <ref type="bibr" target="#b34">[35]</ref> tackles this problem by applying FM to a truncated version of the fitting function consisting of the first few terms of its Taylor expansion. This approach imposes considerable information loss. Further, it is limited to fitting functions with a closed-form Tayler expansion. As we show in Section 5.2 and 5.3, the fitting functions of SVM classification and k-means clustering cannot be handled this way as neither of them is differentiable.</p><p>Similar to FM, Chaudhuri et al. <ref type="bibr" target="#b2">[3]</ref> solves a restricted class of empirical risk minimization problems under differential privacy, by injecting noise into the fitting function f . The methods in <ref type="bibr" target="#b2">[3]</ref> re-lies on rather strong assumptions about f , e.g., f must be strongly concave and doubly differentiable. <ref type="bibr" target="#b2">[3]</ref> demonstrates two specific applications of their methods: logistic regression with a non-zero regularization term (logistic regression with zero regularization, however, fails the strongly concave condition), and SVM classification with certain special loss functions such as Huber loss (the more commonly used hinge loss function is not differentiable <ref type="bibr" target="#b28">[29]</ref>). Even for these two applications, however, the effectiveness of their methods is relatively unstable, and highly sensitive to the choice of the regularization factor. The parameter tuning algorithm in <ref type="bibr" target="#b2">[3]</ref> for selecting an appropriate regularization factor consumes a considerable portion of the privacy budget, reducing the overall accuracy of their methods.</p><p>Rubinstein et al. <ref type="bibr" target="#b28">[29]</ref> investigate differentially private kernel SVMs. Their main focus lies in tackling different types of kernels, including those that involves infinite dimensionality. However, the solutions in <ref type="bibr" target="#b28">[29]</ref> still assume a standard SVM solver, which incurs high sensitivity and large amounts of noise, as we show in the experiments. Finally, Kifer et al. <ref type="bibr" target="#b20">[21]</ref> improve both the accuracy and the applicability of <ref type="bibr" target="#b2">[3]</ref>, by using a relaxed privacy definition. This is orthogonal to our work since we focus on the stronger εdifferential privacy definition.</p><p>GUPT <ref type="bibr" target="#b26">[27]</ref> is a general-purpose differentially private analysis system based on the sample-and-aggregate framework <ref type="bibr" target="#b30">[31]</ref>, which applies to all analysis tasks whose results are not affected by the number of records in the dataset (e.g., AVG is supported, but COUNT and SUM are not). The main idea is to partition the dataset into smaller blocks, and run the analysis algorithm on each of the blocks without privacy considerations. Then, GUPT computes a differentially private average on the results from different blocks. The main strength of GUPT is its general applicability and ease of use. These come at a price of low result quality, however, as the differentially private averaging step can still incur high sensitivity for larger output domains. Overall, all existing solutions are limited to the implicit assumption that a model fitting task should be solved using the same algorithm as in the non-private case. Priv-Gene, presented next, lifts this restriction, and achieves high result quality for a broad class of model fitting tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRIVGENE</head><p>Let D ∈ T n be a sensitive database containing n tuples sampled from domain T , and f (D, ω) be the fitting function that measures how well a parameter vector ω fits the data D. Our goal is to find the best parameter vector ω * that maximizes f (D, ω * ). Unlike many existing solutions reviewed in Section 2, PrivGene does not rely on any restrictive assumptions on f , except that the sensitivity of f should be bounded and reasonably small. Table <ref type="table" target="#tab_0">1</ref> summarizes frequent notations used throughout the paper.</p><p>Algorithm 1 shows the general framework of PrivGene, which involves three main inputs: D (the sensitive database), f (the fitting function), ε (the privacy budget), as well as three more system parameters: m (the size of the candidate set), m (the size of the selected set), and r (the number of iterations). The choice of m, m , and r is discussed towards the end of this section. PrivGene initializes the candidate set Ω with m random parameter vectors (Line 1 in Algorithm 1), and refines them with r iterations. In each iteration (Lines 3-9), PrivGene chooses m best parameter vectors from Ω in a differentially private manner, and places them into the selected set Ω (Line 3). Then, the algorithm generates new parameter vectors by performing crossover and mutation operations over existing ones in Ω , and forms a new candidate set with the new vectors (Lines 6-9). Finally, after the last iteration, the best parame- </p><formula xml:id="formula_3">Algorithm 1 PrivGene (D, f , ε, m, m ,</formula><formula xml:id="formula_4">for i = 1 to r -1 do 3: Compute Ω = DP _Select(D, f, Ω, m , ε/r) 4:</formula><p>Set new candidate set Ω to empty 5:</p><p>for j = 1 to m/2 do 6:</p><p>Randomly choose two vectors ter vector is selected (again with perturbations to satisfy differential privacy), and returned as the final result (Lines 10-11).</p><formula xml:id="formula_5">ω 1 , ω 2 ∈ Ω 7: Compute (v 1 , v 2 ) = Crossover(ω 1 , ω 2 ) 8: Call M utate(v 1 ) and M utate(v 2 ) 9: Add v 1 , v 2 to</formula><p>There are three major components in PrivGene: Crossover, M utate, and DP _Select. Crossover and Mutate operate entirely on the parameter vectors, and do not involve the sensitive data at all. Hence, algorithms in the traditional, non-private setting apply to them. In the following we describe Crossover and Mutate in our specific implementation; we emphasize that details of these two functions are not the main contribution of this paper, and PrivGene works with any algorithms for them as long as they do not involve the dataset D. In particular, our realization of Crossover takes as input two parent vectors ω 1 and ω 2 , and recombines their elements to obtain two new vectors v 1 and v 2 . Let d be the dimensionality of a parameter vector. We perform the recombination by (i) choosing a random number d &lt; d, and subsequently (ii</p><formula xml:id="formula_6">) computing v 1 = (ω 1 1 , . . . , ω 1 d , ω 2 d +1 , . . . , ω 2 d ), and v 2 = (ω 2 1 , . . . , ω 2 d , ω 1 d +1 , . . . , ω 1 d ),</formula><p>where ωi means the i-th value in vector ω. The Mutate operation on a parameter vector ω is implemented by (i) choosing a random d &lt; d, and (ii) adding random noise to ω d . In our implementation, the noise's absolute value equals 5% of the domain of ω d in the first iteration of PrivGene, Algorithm 2 DP_Select (D, f , Ω, m , εs): returns Ω Input: D, f : sensitive dataset and its fitting function Ω: candidate set of parameter vectors m : number of parameter vectors to select from Ω εs: total amount of privacy budget used for selecting Ω Output: Ω : set of selected parameter vectors 1: Initialize Ω to empty 2: For each ω ∈ Ω, compute f (D, ω) 3: for i = 1 to m do 4:</p><p>Use privacy budget εs/m to apply the exponential mechanism (Section 2.1) or the enhanced exponential machanism (Section 4) to select the parameter vector ω * from Ω that aims to maximize f (D, ω * ) 5:</p><p>Remove ω * from Ω, and add ω * to Ω 6: end for 7: return Ω and decreases by 5% after each iteration; the sign of the noise is also random. The parameter vectors after the Crossover and Mutate operations are called the offsprings. Figure <ref type="figure">1</ref> illustrates an example of crossover and mutation in a 4-means clustering problem. Tuples in the database are projected to a 2-dimensional space, denoted by circles. On the left side of the figure, there are two parent center sets, with centers denoted by solid squares and triangles respectively. After crossover, these two parent center sets exchange a pair of centers, generating two intermediate center sets shown in the middle of the figure. Another mutation operation follows, by randomly moving one of the centers to a new location in the space. This leads to the final output of new offspring center sets on the right side of the figure. Note that crossover and mutation do not necessarily enhance the quality of input solutions. In Figure <ref type="figure">1</ref>, intermediate 1 and offspring 1 benefit from these genetic operations while intermediate 2 and offspring 2 have lower quality than their parents. Hence, it is necessary to perform an effective selection step, presented next.</p><p>The selection of top-quality parameter vectors is more complicated since it requires access to the sensitive dataset D, and thus, must be performed in a randomized manner to satisfy differential privacy. Algorithm 2 shows the DP _Select algorithm for this purpose, which takes 5 inputs: the data D, the fitting function f , the candidate set Ω, the number m of vectors to select from Ω, and the amount of privacy budget εs for this operation. DP_Select divides εs into m equal shares, and uses each share to select one parameter vector ω * under differential privacy, with the goal of maximizing the f (D, ω * ), i.e., ideally ω * should be the best fit for the data among all parameter vectors in Ω. The selection of ω * is performed using either the exponential mechanism (EM) described in Section 2.1 or the enhanced exponential mechanism (EEM), which we elaborate in Section 4.</p><p>The correctness of PrivGene directly follows the composability property of differential privacy, described in Section 2.1. Specifically, PrivGene performs DP_Select (the only step that involves the sensitive data) r times, each with privacy budget ε/r, which consumes ε overall. Inside DP_Select, its privacy budget (i.e., εs = ε/r) is used for m invocations of EM or EEM, each with budget εs/m = ε/(r •m ). Hence, we reach the following lemma.</p><p>LEMMA 1. PrivGene satisfies ε-differential privacy.</p><p>Parameter Settings. Next, we clarify the selection of parameters m, m , and r, which are the size of the candidate set, the number of parameter vectors selected in each iteration, and the total number of iterations, respectively. Existing studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30]</ref> on robust parameters for GA have suggested that without domain-specific information, m = 200 and m = 10 generally lead to good results. We adopt the same setting in PrivGene, except that we set m = 1 when the enhanced exponential mechanism is adopted (for selection of top-quality parameter vectors). We will explain the reason in Section 4.</p><p>On the other hand, the choice of r for PrivGene is more sutble. In the non-private setting, GA is usually run until convergence, or for a large number of iterations, in order to find high-quality solutions. However, in PrivGene, a larger value of r also has the negative effect of reducing the privacy budget εs = ε/r used in each invocation of DP_Select (to select m top-quality parameter vectors), leading to more noisy selections. Hence, the choice of r should balance the extensiveness of the search and the noisiness in the selections. This balancing is affected by three parameters: the total privacy budget ε, the total number of records n in the dataset D, and the number of top-quality parameter vectors to be selected in each iteration of PrivGene. Intuitively, a higher amount of total budget ε as well as a larger number of data samples reduce the randomness in the selection process, which allows using a larger r. Hence, we heuristically set r to c • (n • ε)/m , where c is a constant determined by the experiments. We will choose an appropriate value for c in the experimental section.</p><p>Finally, the performance of PrivGene is also affected by how the initial candidate set Ω is generated (see Line 1 in Algorithm 1). Intuitively, if Ω contains a parameter vector ω that is reasonably good, then, ω has a high chance to be chosen by DP_Select in the first iteration, and subsequently improved in later iterations into a near-optimal solution. Conversely, if all parameter vectors in Ω are of poor quality, PrivGene may not be able to refine them into good solutions before depleting its privacy budget. As we discuss in Section 5, for certain model fitting tasks (e.g., logistic regression and SVM classification), it is possible to heuristically insert into Ω some parameter vectors that provide relatively good initial solutions without using any privacy budget. When this cannot be done (e.g., for k-means clustering), we simply initiates Ω with random parameter vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ENHANCED EXPONENTIAL MECHA-NISM</head><p>This section focuses on EEM, an enhanced version of the exponential mechanism (described in Section 2.1) for PrivGene's settings. EEM aims at applications whose fitting function f can be expressed in the following form:</p><formula xml:id="formula_7">f (D, ω) = h(ω) + t∈D q(t, ω). (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>In the above equation, h(ω) is a function whose result is independent of the sensitive data D; in other words, releasing the output of h does not violate personal privacy. The tuple fitting function q(t, ω) measures how well the model fits a tuple t in the data. As we show later in Section 5, a broad class of model fitting tasks can be expressed in the form of Equation <ref type="formula" target="#formula_7">4</ref>. It is worth mentioning that when the above assumption does not hold, EEM gracefully reduces to the exponential mechanism, as we show soon. EEM follows the same general idea as the exponential mechanism, which consists of three steps: (i) assign each possible output value ω a fitting score f (D, ω); (ii) compute a probability for each possible output ω; and (iii) select an output value randomly based on the probabilities calculated in Step (ii). The main difference between EEM and the exponential mechanism is in Step (ii), i.e., probability computation. This is a vital step that determines the result quality of the whole mechanism. In general, the higher the impact of f on the probability of ω to be selected, the better the results, but also the higher amount of released private information. To see this, consider two extreme cases. First, without privacy considerations, we can simply assign probability 1 to the output value ω * with the highest fitting score f (D, ω * ), and 0 to all other possible outputs. Clearly, this assignment always returns the result of the highest fitting score. Second, when f (D, ω) has no impact at all on the probability of ω, the probabilities for all output values are identical, leading to no leakage of private information, but completely random results. The goal of EEM, thus, is to maximize the impact of f (D, ω) in the probability assignment, while satisfying differential privacy requirements.</p><p>Recall from Section 2.1 that, to achieve ε-differential privacy with the exponential mechanism, the probability of an output value</p><formula xml:id="formula_9">ω is proportional to exp (ε • f (D, ω)/Δ), where Δ ≥ 2 max ω∈Ω,D,D f (D, ω) -f (D , ω), (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>and D, D are two arbitrary neighbor databases.</p><p>Observe that the impact of f (D, ω) is negatively correlated to Δ, which we call the dampening factor. As an extreme case, when the dampening factor far exceeds ε•f (D, ω), the fraction ε•f (D, ω)/Δ approaches 0, and, consequently, each output value ω is assigned an almost identical probability. The major difference between EEM and the exponential mechanism is that the former uses a different dampening factor that is no larger than the latter does. Specifically, when Equation 4 holds, EEM computes the probability of selecting output value ω with the following dampening factor,</p><formula xml:id="formula_11">Δ ≥ min 2 max t,t ∈T ,ω∈Ω q(t, ω) -q(t , ω),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">max</head><p>t∈T ,ω,ω ∈Ω q(t, ω)q(t, ω ) . (6)</p><p>Otherwise, EEM simply follows the exponential mechanism by setting the dampening factor as inequality 5.</p><p>To prove the correctness of EEM with the assumption in Equation 4, we first introduce the following two lemmas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEMMA 2. EEM satisfies ε-differential privacy, if</head><formula xml:id="formula_12">Δ ≥ 2 max t,t ∈T ,ω∈Ω q(t, ω) -q(t , ω). (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>PROOF. Let D, D ∈ T n be any neighbor databases and t and t denote the differing tuples in D and D respectively. Given any ω ∈ Ω, we have</p><formula xml:id="formula_14">max D,D f (D, ω) -f (D , ω) = max D,D q(t, ω) -q(t , ω) = max t,t ∈T q(t, ω) -q(t , ω).</formula><p>Thus, the dampening factor in the lemma satisfies Equation <ref type="formula" target="#formula_9">5</ref>as</p><formula xml:id="formula_15">Δ ≥ 2 max t,t ∈T ,ω∈Ω q(t, ω) -q(t , ω) = 2 max ω∈Ω,D,D f (D, ω) -f (D , ω).</formula><p>The proof is complete.</p><formula xml:id="formula_16">LEMMA 3. EEM satisfies ε-differential privacy, if Δ ≥ 2 max t∈T ,ω,ω ∈Ω q(t, ω) -q(t, ω ).<label>(8)</label></formula><p>PROOF. Let D and D be any neighbor databases and t and t denote the differing tuples in D and D respectively. For any output ω ∈ Ω of enhanced exponential mechanism E , we have</p><formula xml:id="formula_17">Pr (E(D) = ω) Pr (E(D ) = ω) = exp (ε • f (D, ω)/Δ) ω ∈Ω exp (ε • f (D, ω )/Δ) exp (ε • f (D , ω)/Δ) ω ∈Ω exp (ε • f (D , ω )/Δ) ≤ exp (ε • (f (D, ω) -f (D , ω)) /Δ) min ω ∈Ω exp (ε • (f (D, ω ) -f (D , ω )) /Δ) .</formula><p>With the assumption given in Equation <ref type="formula" target="#formula_7">4</ref>, the above inequality can be rewritten as:</p><formula xml:id="formula_18">Pr (E(D) = ω) Pr (E(D ) = ω) ≤ exp (ε • (q (t, ω) -q (t , ω)) /Δ) min ω ∈Ω exp (ε • (q (t, ω ) -q (t , ω )) /Δ) = max ω ∈Ω exp ε • q (t, ω) -q t , ω -q t, ω -q t , ω /Δ = max ω ∈Ω exp ε • q (t, ω) -q t, ω + q t , ω -q t , ω /Δ .</formula><p>On the other hand,</p><formula xml:id="formula_19">Δ ≥ 2 max t∈T ,ω,ω ∈Ω q(t, ω) -q(t, ω ) ≥ max t∈T ,ω,ω ∈Ω q(t, ω) -q(t, ω ) + max t ∈T ,ω,ω ∈Ω q(t , ω ) -q(t , ω) ≥ max t,t ∈T ,ω,ω ∈Ω q(t, ω) -q(t, ω ) + q(t , ω ) -q(t , ω).</formula><p>Thus, Pr (E (D) = ω) is no more than to e ε • Pr (E (D ) = ω), which completes the proof.</p><p>Combing the results of Lemma 2 and Lemma 3, we prove the correctness of EEM, formally stated in Theorem 1. THEOREM 1. EEM satisfies ε-differential privacy with dampening factor in Inequality 6.</p><p>Let Δ1 denote the right hand side of Inequality 7 and Δ2 denote that in Inequality 8. According to proofs of Lemmas 2 and 3, Δ1 is exactly the dampening factor used in the exponential mechanism while Δ2 is a new dampening factor designed specifically for Priv-Gene. The dampening factor of EEM is the smaller of the two, which is no larger than Δ1. Further, when used in PrivGene, Δ2 is usually smaller than Δ1. The intuition is that as more iterations are performed, the quality of the parameter vectors in the candidate set becomes increasingly close to each other, since it converges to (possibly local) optimal. This means that it is likely that the maximum value of q(t, ω)q(t, ω ) gradually decreases with the number of iterations performed, leading to decreasing Δ2. Δ1, on the other hand, is not significantly affected by this phenomenon. Hence, the gap between Δ1 and Δ2 expands with the number of iterations, as confirmed by our experiments. The following example illustrates that Δ2 can be considerably smaller than Δ1 with candidates of similar fitting quality. EXAMPLE 1. Consider that we have a database D that contains one-dimensional tuples in the integer domain T = [0, 10], and candidate set Ω = {6, 7, 8}. The fitting function f (D, w) is defined as -t∈D (tω) 2 , i.e., we aim to identify the ω that best approximates the mean of the tuples. To enforce ε-differential privacy using EEM, the dampening factor Δ ≥ min {Δ1, Δ2}, where Δ1 = 128 under the worst case t = 8, t = 0, ω = 8 and Δ2 = 56 under the worst case t = 0, ω = 6, ω = 8.</p><p>In general, the benefit of Δ2 is more pronounced when the candidate set does not contain two parameter vectors that differ significantly from each other. Therefore, we set m = 1 for Priv-Gene+EEM, i.e., each iteration selects exactly one parameter vector ω from the candidate set to generate offsprings for the next iteration. This ensures that no two offsprings would have significant differences (as they are all mutated from ω). As such, when those offsprings are given to the tuple fitting function as inputs, the outputs of the function would be similar, leading to a small value of Δ2, and thus, high accuracy of EEM. In particular, as we show in Section 5, when we set m = 1 for for logistic regression and SVM classification, Δ2 is bounded by a multiple of the mutation scale, which leads to a dramatic accuracy boost compared to EEM with other values of m as well as EM. Hence, in these applications, it is strongly preferred to use EEM and set m = 1 in PrivGene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">APPLICATIONS</head><p>This section applies PrivGene to three common model fitting tasks: logistic regression, SVM classification, and k-means clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Logistic Regression</head><p>Let D be a database containing n tuples from a domain T , such that each tuple has d attributes X1, X2, . . . , X d-1 , Y , and attribute Y has a binary domain {0, 1}. For each t = (x, y) = (x1, x2, . . . , x d-1 , y), we assume without loss of generality 1 that</p><formula xml:id="formula_20">|x k | ≤ 1 for k ∈ {1, 2, . . . , d -1}, i.e., T = [-1, 1] d-1 × {0, 1}.</formula><p>A logistic regression model built on D is parameterized by a vector α and a constant β (called the bias), as formalized in Definition 1.</p><p>DEFINITION 1 (LOGISTIC REGRESSION). Logistic regression on D predicts ŷ = 1 given x = (x1, x2, . . . , xd-1 ) with probability</p><formula xml:id="formula_21">P r {ŷ = 1 | x} = 1/ 1 + exp(-x T α * -β * ) ,</formula><p>where α * is a vector of d -1 real numbers and β * is a real number, such that</p><formula xml:id="formula_22">(α * , β * ) = arg max α,β t∈D y x T α + β -log 1 + exp x T α + β .</formula><p>To apply PrivGene to logistic regression, each parameter vector in PrivGene has d elements: the first d-1 elements represent α and the last one represents β. In what follows, we focus on deriving the dampening factor Δ used in EEM for selecting the top parameter vectors.</p><p>First, given the fitting function in Definition 1, the tuple fitting function for t = (x, y) can be expressed as</p><formula xml:id="formula_23">q(t, ω) = y x T α + β -log 1 + exp x T α + β . (9)</formula><p>To derive Δ, it suffices to derive Δ1 and Δ2, i.e., the right hand sides of Inequalities 7 and 8, respectively.</p><p>Consider Δ1. Recall that</p><formula xml:id="formula_24">Δ1 = 2 max t,t ∈T ,ω∈Ω q(t, ω) -q(t , ω) = 2 max ω∈Ω max t∈T q(t, ω) -min t∈T q(t, ω) . (<label>10</label></formula><formula xml:id="formula_25">)</formula><p>1 This assumption can be easily enforced by changing each x k to</p><formula xml:id="formula_26">x k -0.5(max k -min k ) (max k -min k )</formula><p>, where min k and max k denote the minimum and maximum values in the domain of X k .</p><p>By Equation <ref type="formula">9</ref>, we have q(t, ω) ≤ 0 for any t ∈ T . Therefore, maxt∈T q(t, ω) = 0. To derive mint∈T q(t, ω), we differentiate two cases: x T α + β ≥ 0 and x T α + β &lt; 0.</p><p>When x T α + β ≥ 0, by Equation <ref type="formula">9</ref>, we have</p><formula xml:id="formula_27">min t∈T q(t, ω) = min t∈T y x T α + β -log 1 + exp x T α + β = min t∈T -log 1 + exp x T α + β ≥ min t∈T -x T α + β + 1 = - d-1 k=1 |α k | + β + 1 ≥ - d k=1 |ω k | + 1 .</formula><p>On the other hand, when x T α + β &lt; 0, we have</p><formula xml:id="formula_28">min t∈T q(t, ω) = min t∈T y x T α + β -log 1 + exp x T α + β ≥ min t∈T y x T α + β -1 = - d-1 k=1 |α k | + β -1 ≥ - d k=1 |ω k | + 1 .</formula><p>Combining the above inequalities, we have</p><formula xml:id="formula_29">Δ1 ≤ 2 max ω∈Ω d k=1 |ω k | + 1 . (<label>11</label></formula><formula xml:id="formula_30">)</formula><p>Now consider Δ2. Recall that</p><formula xml:id="formula_31">Δ2 = 2 max t∈T ,ω,ω ∈Ω q(t, ω) -q(t, ω ) = 2 max ω,ω ∈Ω max t∈T q(t, ω) -q(t, ω ) . (<label>12</label></formula><formula xml:id="formula_32">)</formula><p>By Equation <ref type="formula">9</ref>,</p><formula xml:id="formula_33">q(t, ω) -q(t, ω ) = y x T α + β -x T α + β -log 1 + exp x T α + β 1 + exp (x T α + β ) . If x T α + β ≥ x T α + β , then max t∈T q(t, ω) -q(t, ω ) ≤ max t∈T y x T α + β -x T α + β = max t∈T x T α -α + β -β ≤ d k=1 ω k -ω k .</formula><p>On the other hand, if</p><formula xml:id="formula_34">x T α + β &lt; x T α + β , then max t∈T q(t,ω) -q(t, ω ) ≤ max t∈T -log 1 + exp x T α + β 1 + exp (x T α + β ) ≤ max t∈T -log exp x T α + β exp (x T α + β ) = max t∈T x T α + β -x T α + β ≤ d k=1 ω k -ω k .</formula><p>Based on the above inequalities, we have</p><formula xml:id="formula_35">Δ2 ≤ 2 max ω,ω ∈Ω d k=1 ω k -ω k . (<label>13</label></formula><formula xml:id="formula_36">)</formula><p>Combining Inequalities 6, 11, and 13, we have</p><formula xml:id="formula_37">Δ ≥ min 2 max ω∈Ω d k=1 |ω k | + 1 , 2 max ω,ω ∈Ω d k=1 ω k -ω k .</formula><p>Observe that when m = 1 (i.e., only one parameter vector is selected in each iteration), after the first iteration, all candidate parameter vectors are generated from the same parent. Hence, the L1 distance between any two of them is bounded by twice the mutation scale. Therefore, Δ2 is bounded by 4 times the mutation scale. For any m = 1, this bound no longer holds, and Δ2 can be as large as twice the size of the parameter vector domain. Hence, setting m = 1 provides a significant accuracy boost to EEM. The accuracy of EM, however, is not dramatically affected by m , since Δ1 can always be as large as twice the domain size for any m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SVM Classification</head><p>Support vector machine (SVM) <ref type="bibr" target="#b4">[5]</ref> is a popular tool for classification, which predicts the labels of new observations based on existing observations with labels. For simplicity, we focus on SVM with linear kernels <ref type="bibr" target="#b28">[29]</ref>; our solution can be extended to SVMs with non-linear kernels similarly as in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Let D ∈ T n be a database containing n tuples sampled from a</p><formula xml:id="formula_38">d-dimensional domain T = [-1, 1] d-1 × {-1, 1}. We denote the i-th (i ∈ [1, d -1]</formula><p>) dimension of T as Xi, and the last dimension of T as Y . For ease of exposition, we use x to denote a vector in [-1, 1] d-1 , and use t = (x, y) to denote a tuple in D. A linear SVM classifier on D is defined as follows.</p><p>DEFINITION 2 (SVM CLASSIFICATION). Given x = (x1, x2, . . . , xd-1 ), a linear SVM classifier on D predicts the Y value associated with x as:</p><formula xml:id="formula_39">ŷ = 1, if xT α * + β * &gt; 0 -1, otherwise</formula><p>where α * is a vector of d -1 real numbers and β * is a real number, such that</p><formula xml:id="formula_40">(α * , β * ) = arg max α,β - ⎛ ⎝ 1 2 ||α|| 2 + C (x,y)∈D ξ(x, y) ⎞ ⎠ ,</formula><p>where ξ(x, y) = max 1y(x T α + β), 0 , referred to as the hinge-loss function.</p><p>To solve SVM classification with PrivGene, we define each parameter vectors as a d-dimensional vector, such that the first i (i ∈ [1, d -1]) dimensions correspond to α and the last dimension correspond to β. In addition, the tuple fitting function is defined as</p><formula xml:id="formula_41">q(t, ω) = -C max 1 -y(x T α + β), 0 , (<label>14</label></formula><formula xml:id="formula_42">)</formula><p>which is in accordance with Definition 2. As for the damping factor Δ for EEM, we derive it based on an analysis of Δ1 and Δ2, as formulated in Equations 10 and 12. First, let us consider Δ1. By Equation <ref type="formula" target="#formula_24">10</ref>, we can calculate an upperbound for Δ1 based on the maximum and minimum possible values of q(t, ω) for t ∈ T . By Equation <ref type="formula" target="#formula_41">14</ref>, maxt∈T q(t, ω) = 0 trivially holds. Meanwhile,</p><formula xml:id="formula_43">min t∈T q(t, ω) = min t∈T -C max 1 -y(x T α + β), 0 = -C max t∈T 1 + x T α + β ≥ -C d k=1 |ω k | + 1 .</formula><p>Therefore, we have an upperbound of Δ1 as follows:</p><formula xml:id="formula_44">Δ1 ≤ 2C max ω∈Ω d k=1 |ω k | + 1 . (<label>15</label></formula><formula xml:id="formula_45">)</formula><p>Next, we derive an upperbound of Δ2. This, by Equation <ref type="formula" target="#formula_31">12</ref>, can be achieved by upperbouding q(t, ω)q(t, ω ) for any pair of parameter vectors ω, ω and any tuple t. Observe that, for any t = (x, y), we have</p><formula xml:id="formula_46">q(t, ω) -q(t, ω ) = C max 1 -y(x T α + β ), 0 -C max 1 -y(x T α + β), 0 = C • 1 -y(x T α + β ) + 1 -y(x T α + β ) 2 -C • 1 -y(x T α + β) + 1 -y(x T α + β) 2 ≤ C • 1 2 y x T α + β -x T α + β + C • 1 2 y x T α + β -x T α + β ≤ C y x T α + β -x T α + β .</formula><p>Meanwhile,</p><formula xml:id="formula_47">max t∈T C y x T α + β -x T α + β = max t∈T C y x T α -α + β -β = C d k=1 ω k -ω k .</formula><p>Therefore, we have the following upperbound for Δ2:</p><formula xml:id="formula_48">Δ2 ≤ 2C max ω,ω ∈Ω d k=1 ω k -ω k . (<label>16</label></formula><formula xml:id="formula_49">)</formula><p>By combining the upperbounds for Δ1 and Δ2, we obtain the following dampening factor Δ:</p><formula xml:id="formula_50">Δ ≥ 2C • min max ω∈Ω d k=1 |ω k | + 1 , max ω,ω ∈Ω d k=1 ω k -ω k .</formula><p>Similar to the case of logistic regression, when (and only when) m = 1, Δ2 is bounded by 2C times the mutation scale. Hence, EEM with m = 1 leads to high accuracy for PrivGene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">k-means Clustering</head><p>Let k be a positive integer, and D be a private database containing n tuples from a domain T = [-1, 1] d . Given k and D, a kmeans clustering <ref type="bibr" target="#b24">[25]</ref> on D identifies k elements c1, c2, . . . , c k ∈ T (referred to as centers), such that each tuple in D has a small distance to at least one center. The formal definition is as follows:</p><formula xml:id="formula_51">DEFINITION 3 (k-MEANS CLUSTERING). A k-means clus- tering on D returns a set C = {c1, c2, . . . , c k }, such that ci ∈ T (i ∈ [1, d]) and C * = arg max C - t∈D dist(t, C), (<label>17</label></formula><formula xml:id="formula_52">)</formula><p>where dist(t, C) = minc∈C ||t -c|| 2 2 . To adopt PrivGene for k-mean clustering, we define each parameter vector ω as a k •d-dimensional vector from [-1, 1] kd , such that the (k • i + j)-th element in ω represents the j-th coordinate of the i-th center ci. For convenience, we abuse notation and define ωi as a d-dimensional vector whose j-th element equals the (k • i + j)-th element in ω, i.e., ωi represents ci. Then, the tuple fitting function for any tuple t ∈ D is defined as</p><formula xml:id="formula_53">q (t, ω) = -min i∈[1,d] ||t -ωi|| 2 2 .</formula><p>To derive the dampening factor Δ for EEM, we need to calculate an upperbound for Δ1 (in Equation <ref type="formula" target="#formula_24">10</ref>) and Δ2 (in Equation <ref type="formula" target="#formula_31">12</ref>). For Δ1, we have</p><formula xml:id="formula_54">Δ1 ≤ max t,t ∈T ,ω∈Ω q(t, ω) -q(t , ω) ≤ max t∈T ,ω∈Ω -q(t, ω) = max C∈Ω max t∈T min c∈C ||t -c|| 2 2 ≤ max C∈Ω min c∈C max t∈T ||t -c|| 2 2 . (<label>18</label></formula><formula xml:id="formula_55">)</formula><p>On the other hand, we find it difficult to derive a non-trivial upperbound for Δ2, as it is hard to quantify the maximum value of q (t, ω)q (t, ω ) for any t ∈ T and any ω, ω ∈ Ω. To explain, observe that for each tuple t ∈ T , the value of q (t, ω) is decided by the center closest to t (among all d centers represented by ω). Assume without loss of generality that, among the centers represented by ω, the i-th center is closet to t. Suppose that we replace ω with ω . In that case, the closet center to t might change from the i-th center ci to the j-th center cj (i = j), in which case q (t, ω ) is decided by the cj instead of ci. Therefore, if we are to derive the maximum value of q (t, ω)q (t, ω ), then we must take into account all possible changes in the center closest to t, which leads to highly complicated analysis.</p><p>Due to the difficulty in upperbouding Δ2, we use only Δ1 to derive Δ, resulting in Δ ≥ min{Δ1, Δ2} = Δ1. In that case, EEM is degenerated to the exponential mechanism (EM), since Δ1 is exactly the dampening factor used by the latter. In other words, our solution for k-means clustering adopts EM instead of EEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Choosing Initial Candidate Set</head><p>As discussed in Section 3, the performance of PrivGene can be improved if the initial candidate set Ω contains at least one reasonably good parameter vector. Towards this end, for logistic regression and SVM classification, we heuristically generate m = 200 parameter vectors in Ω as follows. First, we insert into Ω 180 vectors that are randomly selected from the solution space. After that, we add another 10 vectors to Ω, such that the first d -1 elements in each vector equal 0, while the last element is a random positive number. We denote this set of 10 vectors as Ω + . Finally, we add 10 vectors (denoted by Ω -) to Ω with a random negative number in the last dimension, and 0 in all other dimensions. Essentially, each parameter vector in Ω + (resp. Ω -) gives a naive logistic or SVM model that predicts Y = 1 (resp. Y = 1) for any given tuple. Although such naive models are not accurate in general, they can sometimes serve as a good starting point for Priv-Gene. To explain, let us consider a dataset D where 70% of the tuples have the same Y values (regardless of whether they all have Y = 1 or Y = 1). Then, at least 10 parameter vectors in Ω provide a model that can correctly predict the Y value of 70% of the tuples, resulting in 70% predication accuracy. This, intuitively, gives Priv-Gene a good initial set of solutions. In general, Ω + and Ω -tend to improve the performance of PrivGene for logitic regression and SVM classification, especially when a large majority of the tuples in the given dataset have the same values on Y . Note that the generation of Ω + and Ω -incur no privacy cost, as they are independent of the input data.</p><p>For k-means clustering, however, there is no obvious way to generate parameter vectors that correspond to reasonably good solutions. In that case, we resort to populating Ω with 200 vectors randomly sampled from the solution space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>This section experimentally evaluates PrivGene on six real datasets: (i) Adult <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>, which includes information of 48, 842 individuals extracted from the 1994 US Census database, (ii) Banking <ref type="bibr" target="#b11">[12]</ref>, a marketing dataset from a banking institution on 45, 211 individuals, (iii) US [1], containing 40, 000 US census records, (iv) BR [1], consisting of 38, 000 Brazilian census records, (v) Lifesci, a life sciences dataset, available at http://komarix.org/ac/ds/, (vi) Image, an image dataset with 34, 112 RGB vectors retrieved from http://cs.joensuu.fi/sipu/datasets/. Adult, Banking, BR and US are used for logistic regression and SVM classification; Lifesci and Image are used in the k-means clustering experiments.</p><p>Adult, Banking, US, and BR contain both continuous and categorical attributes. Following common practice in regression and classification, we transform each categorical attribute with l possible values into l binary attributes. We also normalize the values of each attribute to the range [-1, 1]. After these preprocessing steps, the dimensionalities of datasets Adult, Banking, US, and BR become 124, 33, 58 and 53, respectively. For k-means clustering, we choose k = 3, 5 for Lifesci and k = 10, 15 for Image. Table <ref type="table" target="#tab_2">2</ref> summarizes the properties of each dataset.</p><p>As shown in Table <ref type="table" target="#tab_2">2</ref>, in each regression or classification task, we label a tuple with Y = 1 iff. it belongs to a specific class, e.g., yearly income over 50k in Adult. In particular, in logistic regression, we predict a tuple t to be in the Y = 1 class, whenever t has over 50% probability to have Y = 1 under the logistic model. We measure the performance of a logistic model or an SVM classifier by its misclassification rate, i.e., the fraction of tuples in the testing dataset that are incorrectly classified. The performance of a k-means clustering method is evaluated by its average intra-cluster variance (also used in <ref type="bibr" target="#b26">[27]</ref>), defined as</p><formula xml:id="formula_56">1 |D| t∈D minc∈C ||t -c|| 2 2</formula><p>, where D is the testing data and C is the set of k cluster centers.</p><p>We compare PrivGene against six competitors, namely, GUPT <ref type="bibr" target="#b26">[27]</ref>, Functional Mechanism (FM) <ref type="bibr" target="#b34">[35]</ref>, PrivateERM <ref type="bibr" target="#b2">[3]</ref>, PrivateSVM <ref type="bibr" target="#b28">[29]</ref>, NoPrivacy, and Majority. As explained in Section 2, GUPT is a generic method that can handle all three model fitting tasks in our experiments. FM and PrivateSVM are limited to logistic regression and SVM classification, respectively. Priva-teERM is limited to specific classes of logistic regression (i.e., with a non-zero regularization term as explained in Section 2) and SVM classification (using the Huber loss function rather than the more popular hinge loss). We include it in our evaluations anyway, with a very small (10 -12 ) regularization factor in logistic regression, and Huber loss in SVM classification. The remaining SVM classification solutions (i.e., PrivGene, PrivateSVM and NoPrivacy) employ hinge loss. Note that the effectiveness of PrivateERM is sensitive to the regularization factor. In our experiments, we use fixed values of the factor that lead to relatively good performance; using the auto-tuning algorithm in <ref type="bibr" target="#b2">[3]</ref> leads to strictly and significantly worse results than what we report. These settings are in favor of PrivateERM.</p><p>NoPrivacy directly releases the best parameters without any privacy consideration. Finally, Majority is a naive differentially private classification method: it first counts the number of tuples in the training data with Y = 1, and then adds Laplace noise to the count to ensure -differential privacy; if the noisy count is larger than n/2 (n is the number of tuples in the training data), Majority always outputs Y = 1; otherwise, it always outputs Y = 1. Clearly, NoPrivacy provides the best possible accuracy for any private method, whereas Majority indicates an upper-bound for the misclassification rate in logistic regression and SVM classification.</p><p>Unless specified otherwise, we use the default parameter settings for each method as in previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. GUPT requires an explicitly defined search space for parameter vectors. For regression and classification, we set the search space to [-5, 5] d , where d is the number of elements in each parameter vector ω. For k-means clustering, we naturally set the search space to the domain of a data tuple. In addition, SVM classification requires a regularization parameter c. For NoPrivacy, we test a range of c's and select the best one. For all other methods, we arbitrarily set C = 10 without tuning, so as to avoid leaking private information. Finally, in every regression/classification experiment, we evaluate each method by repeating 5-fold cross-validation 500 times, and report the average result. In each k-means clustering experiment, we run every method 500 times and report the average result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effect of Number of Iterations</head><p>PrivGene has several internal parameters, which are all fixed to values suggested in the genetic algorithms literature, except for the number of iterations r. As discussed in Section 3, we adopt a heuristic r = c • (n • ε)/m , where c is a parameter to be tuned, n is the number of tuples in the input data, and m = 1 (resp. m = 10) when EEM (resp. EM) is incorporated with PrivGene. To choose an appropriate value for c, we conduct experiments as follows. First, we run PrivGene+EEM for logistic regression and SVM classification on Adult and Banking, based on which we select a fixed value for c, without looking at the other datasets or the task of k-means clustering. After that, we use the selected c for all experiments. This ensures (i) that our choice of c does not reveal private information on datasets US and BR, and (ii) that PrivGene can compete against other methods without relying on manual tuning of parameters. Meanwhile, since our parameter tuning is performed based on Adult and Banking, the experimental results on the other four datasets are more important.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the misclassification rate of PrivGene with varying values of c, as a ratio of the misclassification rate when c = 0.5 × 10 -3 . Observe that the misclassification rate of Priv-Gene tends to be high when c is either excessively small or excessively large. This is consistent with our analysis in Section 3 that (i) a small c prevents PrivGene from converging to the optimal so-Adult   </p><note type="other">, Logistic Adult, SVM Banking, Logistic Banking, SVM</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparisons with Existing Solutions</head><p>This section compares PrivGene with existing solutions on three model fitting tasks: logistic regression, SVM classification, and kmeans clustering. Note that previous work <ref type="bibr" target="#b26">[27]</ref> evaluates GUPT on exactly the same three tasks. Logistic Regression. Figure <ref type="figure" target="#fig_2">3</ref> shows the misclassfication rate of each algorithm, with varying privacy budget ε. The error of No-Privacy remains unchanged for all values of ε since it does not enforce ε-differential privacy at all. Clearly, both PrivGene and PrivateERM outperform FM and GUPT in all settings. Comparing PrivGene and PrivateERM, the former achieves consistently better accuracy than the latter, except when the privacy budget ε becomes extremely small (i.e., 0.05), in which case both methods obtain comparable (and relatively low) accuracy. For reasonably large ε, both PrivGene and PrivateERM achieve accuracy close to NoPrivacy. This result reassures that differential privacy is indeed a practical technique for model fitting on sensitive data, provided that an appropriate solution is used for the task.</p><p>Regarding Majority, its accuracy is not sensitive to the value of ε, because (i) there are large gaps between the number of records in the majority and minority classes on the Adult and Banking datasets, and (ii) the two classes have similar number of records on US and BR; consequently, Majority's accuracy is close to that of a wild guess (i.e., 50%). PrivGene outperforms Majority, except when ε is very small (i.e., &lt; 0.01), or when there is a dominating majority class (e.g., on Banking, since few people subscribe to a term deposit), where the Majority's accuracy is already close to that of NoPrivacy, leaving little space for improvement. SVM Classification. This set of experiments compares PrivGene with existing methods on SVM classification. Figure <ref type="figure">4</ref> illustrates the average misclassification rate of SVM classifiers trained by different algorithms. For all 4 datasets, PrivGene and PrivateERM are highly competitive, whereas GUPT and PrivateSVM report closeto-random class labels. Meanwhile, the accuracy of the former two is again close to that of NoPrivacy in all settings, and higher than Majority in most cases, which confirms their practical usefulness. PrivGene slightly outperforms PrivateERM on Adult and US, and the reverse is true on BR. This is because PrivGene's accuracy is slight better on Adult and US than on BR, which is also evident in the set of logistic regression experiments. The reason is that the parameter c is tuned based on Adult and Banking is sub-optimal on BR. Nevertheless, the accuracy loss due to this sub-optimal c is small. Concerning Banking, the difference in accuracy of Priv-Gene, Majority and PrivateERM is negligible, due to the presence of a dominating majority class.</p><p>k-means Clustering. Figure <ref type="figure">5</ref> exhibits the average intra-cluster variance of each algorithm. Note that the accuracy results for Image are shown in log scale. PrivGene is again the clear winner in all settings, and the performance gap between PrivGene and GUPT increases rapidly with decreasing ε. At ε = 0.1, the accuracy of PrivGene is two orders of magnitude better than that of GUPT. Both PrivGene and GUPT are more sensitive to ε compared to logistic regression and SVM classification tasks, which suggests that differentially private k-means clustering is a more difficult problem.</p><p>In summary, PrivGene outperforms both the general solution GUPT and specialized solutions FM (for logistic regression) and PrivateSVM (for SVM classification). The only method that can   compete with PrivGene is PrivateERM, which, however, relies on rather strong assumptions of the model fitting task as well as a good selection of the regularization factor. The accuracy of PrivGene is often close to that of NoPrivacy, and is generally robust against data dimensionality and (to a lesser degree) the amount of privacy budget ε. These results suggest that PrivGene is the method of choice for all three model fitting tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Improvement of EEM</head><p>Having established the superiority of PrivGene over existing solutions, we next investigate the intrinsic characteristics of Priv-Gene. In particular, we demonstrate the effectiveness of using EEM in comparison with vanilla exponential mechanism. Figure <ref type="figure" target="#fig_3">6a</ref>    classification, respectively, using the US dataset with ε fixed to its maximal value 1. Using a different value for ε leads to similar conclusions. There is a gap between the misclassification rate of PrivGene with EEM and with exponential mechanism, and the gap increases rapidly with the number of iterations. When Priv-Gene finishes all iterations, EEM wins by over 10% in terms of misclassification rate for logistic regression, and 7% for SVM classification. Figure <ref type="figure" target="#fig_6">7</ref> demonstrates the results from the BR dataset with same settings as US. Again, EEM is highly effective for PrivGene compared to the original exponential mechanism. In particular, the use of EEM reduces the misclassification rate by up to 8% for logistic regression, and up to 6% for SVM classification. Considering that the accuracy of PrivGene is already close to that of NoPrivacy, improvement in PrivGene's misclassification rate is difficult; hence, the use of EEM is strongly recommended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS AND FUTURE WORK</head><p>This paper presents PrivGene, a general framework for differentially private model fitting. Unlike most existing solutions that apply the differentially private version of a popular algorithm in the non-private setting to a restricted class of problems, PrivGene is based on the happy marriage of differential privacy with genetic algorithms, which achieves high accuracy for a broad class of model fitting problems. In addition, we propose EEM, an improved version of the exponential mechanism for PrivGene. We show that PrivGene outperforms existing solutions on three common model fitting tasks: logistic regression, SVM classification, and k-means clustering, and the accuracy of PrivGene is often close to the baseline approach without privacy considerations. Regarding future work, we plan to investigate the possibility of applying PrivGene to problems besides model fitting, e.g., dimensionality reduction and frequent pattern mining. Additionally, we are also interested in generalizing EEM to improve the accuracy of other differentially private algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>12 :Figure 1 :</head><label>121</label><figDesc>Figure 1: An example of crossover and mutation on 4-means clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of iterations in Logistic regression and SVM classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Logistic regression on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Lifesci, k = 3 (b) Lifesci, k = 5 (c) Image, k = 10 (d) Image, k = 15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: k-means clustering on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Improvement of EEM on the BR dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : List of common notations</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Notation Description</cell></row><row><cell>D, n</cell><cell>Sensitive database and its cardinality</cell></row><row><cell>T</cell><cell>Domain of a tuple in D</cell></row><row><cell>ω, d</cell><cell>Parameter vector for a model, and its dimen-</cell></row><row><cell></cell><cell>sionality</cell></row><row><cell cols="2">f (D, ω) Fitting function that quantifies how well a</cell></row><row><cell></cell><cell>model, parameterized by ω, fits D</cell></row><row><cell>q (t, ω)</cell><cell>Tuple fitting function that describes the quality</cell></row><row><cell></cell><cell>of a model, parameterized by ω, fits tuple t</cell></row><row><cell>Ω, m</cell><cell>Candidate set of parameter vectors, and its car-</cell></row><row><cell></cell><cell>dinality</cell></row><row><cell>Ω , m</cell><cell>Selected set of parameter vectors, and its car-</cell></row><row><cell></cell><cell>dinality</cell></row><row><cell>r</cell><cell>Number of iterations in PrivGene</cell></row><row><cell cols="2">ing tasks. For instance, Li et al. [24] study privacy-preserving fre-</cell></row><row><cell cols="2">quent item mining. Friedman et al. [13] investigate private decision</cell></row><row><cell cols="2">trees. Inan et al.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Datasets properties.</head><label>2</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell cols="3">Number of tuples Dimensionality Task</cell></row><row><cell>Adult</cell><cell>48, 842</cell><cell>124</cell><cell>Predict if a person makes over 50k USD per year</cell></row><row><cell>Banking</cell><cell>45, 211</cell><cell>33</cell><cell>Predict if a client subscribes a term deposit</cell></row><row><cell>US</cell><cell>40, 000</cell><cell>58</cell><cell>Predict if a person makes over 25k USD per year</cell></row><row><cell>BR</cell><cell>38, 000</cell><cell>53</cell><cell>Predict if a person makes over 300 USD per month</cell></row><row><cell>Lifesci</cell><cell>26, 733</cell><cell>10</cell><cell>k-means clustering with k equals 3 and 5</cell></row><row><cell>Image</cell><cell>34, 112</cell><cell>3</cell><cell>k-means clustering with k equals 10 and 15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>and 6b illustrate the performance of the exponential mechanism (with m = 1 in order to facilitate the comparison) and EEM in terms of misclassification rate in the tasks of logistic regression and SVM</figDesc><table><row><cell></cell><cell></cell><cell>EEM</cell><cell></cell><cell></cell><cell></cell><cell>EM</cell><cell></cell></row><row><cell>45%</cell><cell cols="2">misclassification rate</cell><cell></cell><cell>45%</cell><cell cols="2">misclassification rate</cell><cell></cell></row><row><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>35%</cell><cell></cell><cell></cell><cell></cell><cell>35%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>25%</cell><cell></cell><cell></cell><cell></cell><cell>25%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>20%</cell><cell>1</cell><cell>10 number of iterations 20 30</cell><cell>38</cell><cell>20%</cell><cell>1</cell><cell>10 number of iterations 20 30</cell><cell>38</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the Nanyang Technological University under SUG Grant M58020016, and by A*STAR under SERC Grant 102-158-0074. The authors would like to thank the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Differentially private empirical risk minimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1069" to="1109" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Differentially private spatial decompositions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995-09">Sept. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adapting operator probabilities in genetic algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third international conference on Genetic algorithms</title>
		<meeting>the third international conference on Genetic algorithms<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Handbook of Genetic Algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Van Nostrand Reinhold</publisher>
			<pubPlace>New York, New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Differentially private data cubes: optimizing noise sources and consistency</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<title level="m">Differential privacy. In ICALP</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TCC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Wakefield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Briggs</surname></persName>
		</author>
		<title level="m">Spatial Epidemiology; Methods and applications</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data mining with differential privacy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="493" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Genetic Algorithms in Search, Optimization and Machine Learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Boosting the accuracy of differentially private histograms through consistency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1021" to="1032" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Applied Logistic Regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hosmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lemeshow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Series in Probability and Statistics: Texts and References Section</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Private record matching using differential privacy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kantarcioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghinita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="123" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No free lunch in data privacy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="193" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A rigorous and customizable framework for privacy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Private convex optimization for empirical risk minimization with applications to high-dimensional regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="25" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Essentials of medical statistics</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Kirkwood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Blackwell Scientific Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimizing linear counting queries under differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="123" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Privbasis: Frequent itemset mining with differential privacy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qardaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1340" to="1351" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theor</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="2006-09">Sept. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mechanism design via differential privacy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gupt: privacy preserving data analysis made easy</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="349" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Statistical Evaluation of Medical Tests for Classification and Prediction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pepe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Oxford Statistical Science Series</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning in a large function space: Privacy-preserving mechanisms for svm learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I P</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="100" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A study of control parameters affecting online performance of genetic algorithms for function optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Eshelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third international conference on Genetic algorithms</title>
		<meeting>the third international conference on Genetic algorithms</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Privacy-preserving statistical estimation with optimal convergence rate</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Differential privacy via wavelet transforms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Differentially private histogram publication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Low-rank mechanism: Optimizing batch queries under differential privacy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1352" to="1363" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Functional mechanism: Regression analysis under differential privacy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1364" to="1375" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
