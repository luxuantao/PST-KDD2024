<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Energy Pattern for Texture Classification Using Self-Adaptive Quantization Thresholds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-12-20">December 20, 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<email>zhangjun@life.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Life Sciences and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jimin</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Life Sciences and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Heng</forename><surname>Zhao</surname></persName>
							<email>zhaoheng@life.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Life Sciences and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Local Energy Pattern for Texture Classification Using Self-Adaptive Quantization Thresholds</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-12-20">December 20, 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">33F9470C4DEEB25E07F2EDF5BF2F1DC3</idno>
					<idno type="DOI">10.1109/TIP.2012.2214045</idno>
					<note type="submission">received October 2, 2011; revised July 24, 2012; accepted August 1, 2012. Date of publication August 17, 2012; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dynamic texture</term>
					<term>local energy pattern (LEP)</term>
					<term>self-adaptive quantization thresholds</term>
					<term>steerable filter</term>
					<term>texture representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Local energy pattern, a statistical histogram-based representation, is proposed for texture classification. First, we use normalized local-oriented energies to generate local feature vectors, which describe the local structures distinctively and are less sensitive to imaging conditions. Then, each local feature vector is quantized by self-adaptive quantization thresholds determined in the learning stage using histogram specification, and the quantized local feature vector is transformed to a number by N-nary coding, which helps to preserve more structure information during vector quantization. Finally, the frequency histogram is used as the representation feature. The performance is benchmarked by material categorization on KTH-TIPS and KTH-TIPS2-a databases. Our method is compared with typical statistical approaches, such as basic image features, local binary pattern (LBP), local ternary pattern, completed LBP, Weber local descriptor, and VZ algorithms (VZ-MR8 and VZ-Joint). The results show that our method is superior to other methods on the KTH-TIPS2-a database, and achieving competitive performance on the KTH-TIPS database. Furthermore, we extend the representation from static image to dynamic texture, and achieve favorable recognition results on the University of California at Los Angeles (UCLA) dynamic texture database.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T EXTURE classification, unlike other forms of classifica- tions where the objects being categorized have a definite structure, most textures have large stochastic variations which make them difficult to model <ref type="bibr" target="#b0">[1]</ref>. Therefore, local statistical representation is a long lasting approach for texture representation. In order to obtain the statistical representation, each local structure should be labeled distinctively. Specifically, two fundamental problems to label the local structures are:</p><p>1) How to generate local feature vectors to describe local structures with higher distinguishing ability, lower redundancy and imaging conditions invariance.</p><p>2) How to quantize the local feature vectors distinctively to preserve more structure information. For the first item, the joint distribution of intensity values over compact neighborhoods is often used as local feature vectors to describe the local structures. The joint distribution is mostly the gradient of neighborhoods, the responses of filter banks or even the intensity values themselves. The imaging conditions include illumination (brightness and contrast), rotation and scale variation. Sometimes the local feature vectors are normalized to achieve some degree of robustness to illumination variation. The selection of dominant orientation, combination of histogram bins or other techniques are used to achieve rotation invariance, and the multi-scale analysis is used to make the representation more robust to different scales. The second item is about the vector quantization method which is important for the labeling step. The local binary pattern (LBP) operator used binary coding to generate a decimal number to label each local structure <ref type="bibr" target="#b1">[2]</ref>. Similar to the vector quantization approach used in data compression, the texton dictionarybased methods used nearest texton in the texton dictionary to label the local structure <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref>. In this paper, we propose normalized local oriented energies to generate local feature vectors which are invariant to the imaging conditions to some extent. Motivated by LBP operation using binary coding for quantization, we propose to use N-nary coding which reduces the quantization loss and thus preserves more local structure information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>The representative methods of statistical representation, to name a few, include co-occurrence matrices <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, Markov random fields <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, local binary pattern (LBP) <ref type="bibr" target="#b1">[2]</ref> and its extensions <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, texton dictionary-based methods <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref> and Weber local descriptor (WLD) <ref type="bibr" target="#b13">[14]</ref>.</p><p>Among these methods, LBP-based and texton dictionarybased methods have the similar procedures which can be concluded in three steps. First, the neighborhood property or responses of filter bank are used to generate local feature vectors for describing local structures of an image (every pixel corresponds to a local feature vector). Second, each local feature vector is transformed into a unique code number via vector quantization (this step could be treated as the partition of the local feature space). Finally, the frequency histogram of the code numbers is used to form the representation feature of the corresponding image.</p><p>Although the procedures of different methods are similar, there are still a number of open questions regarding how to generate local feature vectors and how to transform the local feature vectors into code numbers. For example, the basic method of LBP <ref type="bibr" target="#b1">[2]</ref> describes local structures with 8 gradients (the oriented gradients are the differences of neighboring pixels, which can be treated as the responses of special filters as well <ref type="bibr" target="#b14">[15]</ref>), so the local feature vectors are 8-element vectors. The elements in each vector are thresholded at zero to produce two states of 0 and 1, and then the elements of each quantized vector are transformed into a decimal number via binary coding (the decimal number is the one we called the code number). The frequency histogram of the code numbers is the representation feature. Some further operations were implemented to make the representation possess the properties of scale and rotation invariance <ref type="bibr" target="#b1">[2]</ref>. The LBP approach works fast due to its simple operation and has achieved impressive classification results on representative texture databases <ref type="bibr" target="#b1">[2]</ref>. Although the method of LBP achieves great success, there remain some problems. The local feature vectors are quantized by thresholding at zero, that is, only the sign information of the vectors is preserved to generate the decimal numbers. This causes too much information loss. Tan and Triggs <ref type="bibr" target="#b11">[12]</ref> proposed local ternary pattern (LTP) to quantize the local feature vectors into three levels to alleviate the information loss. But the thresholds for quantization are determined by experience. Guo et al. <ref type="bibr" target="#b12">[13]</ref> developed completed LBP (CLBP) scheme to retain more structure information as well. The CLBP contains three operators, CLBP_Center, CLBP_Sign, and CLBP_Magnitude, which are defined to extract the image local gray level, the sign and magnitude local features vectors respectively.</p><p>The process of constructing LBP features is datasetindependent, as the representation of each image does not rely on the training images. The texton dictionary-based methods are dataset-dependent since the process of quantizing local feature vectors to code numbers relies on the texton dictionary obtained from the training images. Therefore, texton dictionary-based approaches <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> need a learning stage to generate the texton dictionary, but the dictionary is mostly defined by a once-and-for-all clustering of local feature vectors from the training dataset <ref type="bibr" target="#b15">[16]</ref>. As a typical texton-based method, the algorithm of VZ-MR8 <ref type="bibr" target="#b3">[4]</ref> employs a maximum response 8 (MR8) filter bank to generate local feature vectors, and unsupervised clustering is adopted to acquire texton dictionary (since the vector quantization technique is often used in data compression, it is also called the codebook). Then, the textons in the dictionary are labeled as different code numbers. To represent an image, each local feature vector is labeled by its nearest neighbor texton in the texton dictionary, and the frequency histogram of the code numbers is regarded as the representation feature. Varma and Zisserman <ref type="bibr" target="#b5">[6]</ref> proposed another texton dictionary-based algorithm by using the image local patch directly to replace the response of filter bank and achieved competitive classification performance. However, no matter what the local feature vector is, there is a principal problem with schemes using unsupervised clustering to generate the texton dictionary. It is the inevitable computationally intensive of performing a nearest neighbor calculation to assign each new descriptor response-at every point in an image-to a texton <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contribution</head><p>In this work, we propose an approach named the local energy pattern (LEP), which can be considered as an extension of LBP. The main contribution is two-fold, corresponding to the two fundamental problems of statistical texture representation. First, is the generation of the local feature vector. The normalized local oriented energies which are obtained by rectifying the responses of the 2D Gaussian-like second derivative filters are used to generate the local feature vectors. These local feature vectors are invariant to brightness, contrast and rotation to some extent. Second, is the local feature vector quantization. We propose a N-nary coding quantization scheme instead of the binary coding method in the LBP operator. The N-nary coding method could preserve more structure information compared with traditional binary coding. Moreover, it still keeps the advantage that it does not require the cost of performing the nearest neighbor computation like the codebook quantization scheme. During the N-nary coding, we used histogram specification <ref type="bibr" target="#b16">[17]</ref> to train the quantization thresholds. In paper <ref type="bibr" target="#b17">[18]</ref>, the feature space was quantized by adding together feature distributions for every single model image in a total distribution. The operation has similar results to the operation of histogram specification method to train quantization thresholds. However, the two operations are somewhat used in different backgrounds and purposes. The histogram specification well solves our problem which makes it possible to adopt the N-nary coding for the vector quantization. Speaking of the usage of histogram specification for quantization alone, the operation shares a similar idea with that of <ref type="bibr" target="#b17">[18]</ref>. However, the quantization is one step for the N-nary coding, which does not influence the establishment of our contribution. The concept of N-nary coding, in contrast to the binary coding (vector quantization method in LBP), is our second contribution.</p><p>The static texture images have two dimensions and the dynamic textures (DTs) are textures with motion which can be considered as having three dimensions. The question that naturally emerges is whether the statistical representation approach of the static texture image can be extended to a dynamic texture image sequence. The answer is positive. Zhao et al. have improved LBP to a volume LBP (VLBP) for dynamic texture recognition with an application to facial expression recognition <ref type="bibr" target="#b18">[19]</ref>. The VLBP uses volume oriented gradients to generate local feature vectors describing the local volume structures, and the following steps are just like that for LBP. For our method, if the 2D Gaussian-like second derivative filters are replaced by 3D Gaussian-like second derivative filters to generate local feature vectors, it could capture the local texture and motion simultaneously. Some tough problems in dynamic texture recognition, such as different image sizes and number of frames, can be simply solved by the normalization of the representation histogram.</p><p>The remaining sections are organized as follows. Section II gives the basic algorithm of the proposed representation method and improvement which makes it robust to scale and rotation changes. In Section III, experiments on material categorization are conducted and the effect of the parameters are analyzed. In Section IV, we extend the proposed representation method from static image to dynamic texture image sequence, and also give the performance evaluation on dynamic texture database. Finally, the conclusion and discussion are given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. IMAGE REPRESENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivation</head><p>As an extension of LBP, we propose to use N-nary coding to preserve more information for local structures. However, we have to face two inevitably tough problems:</p><p>1) The length (P) of the local feature vector should not be too long and should still describe the local structures distinctively. Because the range of the code number is [0, N P ] and the quantization state N is usually greater than two, the dimensionality problem will happen to the representation if P is large. 2) A reasonable way to quantize the elements of the local feature vector into N states should be defined. It should preserve more information and still have a good distinguishing ability after quantization. To address the first problem, the oriented filters (instead of using local differences in the LBP operator) are adopted to generate the local feature vectors, which makes it possible to use fewer filter orientations to obtain more structure information. The responses of filters are then rectified to the normalized local oriented energies which are more stable to noise and imaging conditions. We describe this in more detail in Section II-B.1 and II-B.2. To address the second problem, we use the idea of histogram specification <ref type="bibr" target="#b16">[17]</ref> to learn some self-adaptive thresholds for vector quantization. The learning stage need a training set, but the labels of the images are not necessary. Section II-B.3 gives the detailed description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Basic Algorithm</head><p>Studies on description of local structure based on filters have a long history. The modeling of biological texture perception based on local orientations played a significant role <ref type="bibr" target="#b19">[20]</ref>. For example, Coggins and Jain suggested using ring filters and wedge-shaped orientation filters for feature extraction <ref type="bibr" target="#b20">[21]</ref>. Leung and Malik proposed an LM filter bank to construct 3D textons which are invariant to viewpoint and illumination <ref type="bibr" target="#b2">[3]</ref>. Varma and Zisserman described local structures using the response of maximum response 8 (MR8) filter bank <ref type="bibr" target="#b3">[4]</ref>. Some other descriptors were based on Gabor filters <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Besides, Schmid presented isotropic Gabor-like filters to extract local features <ref type="bibr" target="#b24">[25]</ref>. The performance evaluation for some other descriptors has been reported in <ref type="bibr" target="#b25">[26]</ref>.</p><p>In this paper, we generate the local features from the steerable filters <ref type="bibr" target="#b26">[27]</ref>. Using steerable filters for feature representation has been studied for a long time and has been demonstrated as a powerful tool for texture analysis. Simoncelli and Farid presented steerable wedge filters based on steerable filter for local orientation analysis <ref type="bibr" target="#b27">[28]</ref>. Winder and Brown rectified the outputs of steerable quadrature filters for object detection <ref type="bibr" target="#b28">[29]</ref>. Mellor et al. adopted the polarseparable steerable filters to construct a Hessian matrix, and the eigenvectors and eigenvalues were rectified to the local phases and local energies <ref type="bibr" target="#b29">[30]</ref>. Crosier and Griffin proposed the basic image features (BIF) based on steerable filters which achieved very high classification performance <ref type="bibr" target="#b15">[16]</ref>. Derpanis and Wildes used steerable filters to generate the global oriented energies for the dynamic texture recognition <ref type="bibr" target="#b30">[31]</ref>.</p><p>1) Steerable Filters: The descriptions for local structures of an image are realized by using a set of oriented second derivatives of Gaussian-like filters. An important thing is that these steerable filters we used are x-y separable, which helps to obtain the filter responses more efficiently <ref type="bibr" target="#b26">[27]</ref>. The second derivative of Gaussian filter is a well bandpass one and is often used for image analysis. It can be rotated to an arbitrary orientation by linear combination of the basis filters, which has been used in motion, texture and orientation analysis <ref type="bibr" target="#b26">[27]</ref>.</p><p>Assume the Gaussian-like function G is written as:</p><formula xml:id="formula_0">G(x, y) = e -(x 2 +y 2 ) σ 2 (1)</formula><p>where σ 2 is the variance of the Gaussian-like function. The oriented Gaussian-like second derivative G 2θ is defined as:</p><formula xml:id="formula_1">G 2θ = cos 2 (θ )G x x -2 cos(θ ) sin(θ )G xy + sin 2 (θ )G yy (2)</formula><p>where θ is sampled at proper intervals with the bound of 0 ≤ θ ≤ π. Fig. <ref type="figure" target="#fig_0">1</ref> shows the responses of the Gaussian-like second derivative filter at different orientations. It shows that G 2θ is a well bandpass filter and has directional selectivity to some extent.</p><p>2) Local Feature Vectors: Many studies have directly used the responses of filters to generate local feature vectors <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>. But in this paper, in order to make the local feature vectors more robust to noise and imaging conditions, the responses of filters are rectified (nonlinearity, smoothing and normalizing) to generate the local feature vectors according to the standard procedure summarized in <ref type="bibr" target="#b23">[24]</ref>. The procedure has four steps: a) Filtering: The input image I is convolved with the oriented Gaussian-like second derivative filters, and the resultant image I θ p is</p><formula xml:id="formula_2">I θ p = G 2θ p * I (3)</formula><p>where * denotes convolution, θ p ( p = 0, . . . , P -1) denotes the orientations, and P is the total number of oriented filters (P also corresponds to the length of the local feature vector). b) Nonlinearity: The oriented energy of an image is defined as:</p><formula xml:id="formula_3">e θ p = I 2 θ p . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>According to <ref type="bibr" target="#b23">[24]</ref>, the nonlinearity here could be understood as the operation of transforming negative amplitudes to the corresponding positive amplitudes. c) Smoothing: In order to obtain more stable features to describe the local structure and be robust to noise, the energies are locally summed over a Gaussian weighted region according to <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b31">[32]</ref>, with the corresponding pixel at the center, to yield the local oriented energy:</p><formula xml:id="formula_5">E θ p = G w * e θ p (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where G w is the Gaussian window used for smoothing. According to Parseval's theorem <ref type="bibr" target="#b32">[33]</ref>, the sum of energies in the space domain is proportional to the sum of energies in the frequency domain. The aggregation could decrease sensitivity to the local phase and thereby achieve a measure of energy. Consequently, the local feature vector V is defined as:</p><formula xml:id="formula_7">V = {E θ 0 , E θ 1 , . . . , E θ P-1 }.<label>(6)</label></formula><p>d) Normalization: The normalization of local feature is one important step to reduce the effects of noise and imaging conditions. In this paper, we propose a way of normalization and describe as following. The normalized local feature vector is illustrated as:</p><formula xml:id="formula_8">V = {E norm 0 , E norm 1 , . . . , E norm P-1 }<label>(7)</label></formula><p>where E norm p is the normalized local oriented energy defined as:</p><formula xml:id="formula_9">E norm p = E θ p E all + A (8) E all = P-1 p = 0 E θ p (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>where A is the mean value of E all for an image, and it is easy to find out that 0 ≤ E norm p &lt; 1. The reason that the mean value A is added to the denominator during normalization is to avoid some confusions when describing the local structures. If A is omitted, some different local structures may have similar local feature vectors. For example, two different local oriented energies E 1 θ = {E θ 0 , . . . , E θ P-1 } = {1000, 2000, 1500, 500} and E 2 θ = {100, 200, 150, 50} (here P = 4) will have the same local feature vector V = {0.2, 0.4, 0.3, 0.1}. The first structure, which has large responses of oriented filters, should be a coarse region. On the contrary, the second structure should be a flat region. Thus, it is hard to say they have similar local structures. The addition of the mean value A (e.g., A = 1000) to the denominator could distinguish such structures clearly. After the proposed normalization, the two local feature vectors become V 1 = {0.1667, 0.3333, 0.2500, 0.0833} and V 2 = {0.0667, 0.1333, 0.1000, 0.0333}. From the definition of the local feature vector, the image contrast change in which each pixel value is multiplied by a constant will multiply local oriented energies E θ p by the square of the constant. This contrast change will be removed by our normalization step. A brightness change where a constant or slowly varying variable is added to each image pixel will not affect the local feature vector, as the brightness change happens mostly in the low frequency domain which may be excluded by the bandpass filters. Derpanis and Wildes have used oriented energy for the feature extraction <ref type="bibr" target="#b30">[31]</ref>. The feature is invariant to brightness and contrast variation as well. But their normalization way of reducing the contrast effect is different from the proposed method.</p><p>3) Vector Quantization: In order to transform the local feature vector to a code number, the elements of each local feature vector should be first quantized into N states. We need some quantization thresholds to ensure the quantized local feature vectors are distinct and preserve enough information. The learning stage is to obtain some self-adaptive thresholds.</p><p>The thresholds for N levels are set so that 1/N of the histogram area is within each bin (between two adjacent thresholds). The thresholds are learned for each database and they may be quite different from one database to another. Experience quantization thresholds can hardly fit all of the databases. The difference between two databases are clearly shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Similar to the way of quantizing feature space in <ref type="bibr" target="#b17">[18]</ref>, we adopt histogram specification <ref type="bibr" target="#b16">[17]</ref> to find the self-adaptive thresholds. Let r denote the possible values of elements in the local feature vectors (normalized local oriented energies E norm p ), so the range of r is [0, 1). If the PDF of r follows uniform distribution, it is natural to quantize r by a uniformlyspaced partition. Thus, what we need is finding a monotonous transformation function f (•) to make the PDF of f (r ) follow a uniform distribution. The function f (r ) is defined as:</p><formula xml:id="formula_11">f (r ) = r 0 p(ω)dω (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where p(•) is the PDF of r . Note that 0 ≤ r &lt; 1 and 0 ≤ f (r ) ≤ 1, it is easy to determine the thresholds to quantize f (r ) into N states according to N -1 thresholds </p><formula xml:id="formula_13">T = f -1 1 N , f -1 2 N , . . . , f -1 N -1 N . (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>The elements of each local feature vector could be quantized to N states by T in the range [0, 1). Here, we denote N states as {0, 1, . . . , N -1}. It is worth pointing out that the thresholds only need to be calculated once for each dataset (off-line operation and does not need the label for each image).</p><p>After the elements of local feature vectors are quantized, the vectors are transformed into code numbers by coding. We denote the quantized local feature vectors as V Q . Thus, the local energy pattern (LEP) is defined as:</p><formula xml:id="formula_15">L E P P,N = P-1 p=0 V Q ( p)N p . (<label>12</label></formula><formula xml:id="formula_16">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Histogram Representation:</head><p>The frequency histogram of L E P P,N is a global representation of the texture. In order to achieve invariance to the image size, the representation histogram H is L 1 normalized. The complete flow diagram is shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Rotation Invariance</head><p>Under some circumstances, the representation lacks rotation invariance which is a big problem. Rotation invariance can be achieved in one of two ways, either by extracting rotation invariant features or by appropriate training of the classifier to make it 'learn' invariant properties <ref type="bibr" target="#b33">[34]</ref>. In this paper, we use the first option and adopt two different methods to allow our representation feature to have the ability of rotation invariance. The first is based on choosing a dominant orientation of each local energy vector, and the second is based on combining representation histogram bins.</p><p>L E P do P,N denotes L E P P,N with the rotation invariance based on choosing a dominant orientation. The maximum of the local oriented energy E θ p in different orientations is relatively stable, whose orientation is considered as the dominant orientation. Specifically, every local structure has its own dominant orientation, and the elements of the local feature vector are cyclically shifted according to the local dominant orientation.</p><p>L E P hb P,N denotes L E P P,N as having rotation invariance based on combining histogram bins. This step is the same as the method for L B P ri <ref type="bibr" target="#b1">[2]</ref>. L E P P,N produces N P different output values, and some values correspond to the same local structure if we consider the local rotation. For example, when P = 4 N = 4, the code number 216 corresponds to the quantized local feature vector V 1 Q = {0, 2, 1, 3} and the code number 54 corresponds to the quantized local feature vector</p><formula xml:id="formula_17">V 2 Q = {2, 1, 3, 0}. If the local structure of V 1 Q is rotated by 45°, it will produce V 2</formula><p>Q . Therefore, similar to L B P ri <ref type="bibr" target="#b1">[2]</ref>, we define</p><formula xml:id="formula_18">L E P hb P,N = mi n{RO R(L E P P,N , i )|i = 0, 1, . . . , P -1}<label>(13)</label></formula><p>where RO R(L E P P,N , i ) performs a circular bit-wise right shift on the P-bit N-nary number x i times. The number of bins of the representation histogram will be reduced. For example, the number of bins for L E P 4,4 is 256, and the number of bins for L E P hb 4,4 is 70, the number of bins for L E P 6,3 is 729 and the number of bins for L E P hb 6,3 is 130.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multiscale and Scale Invariance Analysis</head><p>To make the representation more robust, the coarse and fine information of the local structures could be captured by multi-scale, multi-resolution representation or pyramid representation (mixed multi-scale and multi-resolution representation). Since there exists good correlation between the representations of a different resolution and scale, using one of the multi-schemes is enough for the representation. We adopt the multi-scale analysis, and the representations of different scales are concatenated together as the multi-scale representation. The multi-scale σ = {1, 2, 4, 8} to capture the fine and coarse information simultaneously.</p><p>After obtaining the representation features, the nearest neighbor (NN) classifier <ref type="bibr" target="#b34">[35]</ref> is used for the classification. In classification, the dissimilarities between samples and training images need to be calculated. There are many distance measurements available to calculate the dissimilarity of two features. For histogram features, measurement like histogram intersection <ref type="bibr" target="#b35">[36]</ref>, log-likelihood statistic <ref type="bibr" target="#b36">[37]</ref> and chi-square distance <ref type="bibr" target="#b36">[37]</ref> are commonly used. In this paper, we tested many measurements and found that the chi-square distance measurement achieves best performance. The chi-square distance is defined as:</p><formula xml:id="formula_19">D(ξ, η) = b (ξ b -η b ) 2 ξ b + η b (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where b refers to the bth bin in histogram ξ or η. However, the multi-scale representation does not have the ability of scale invariance. In order to overcome the shortcoming, we adopt a shifting scheme which is motivated by the scale-shifting method in <ref type="bibr" target="#b15">[16]</ref>. The distance is not calculated by concatenating the multi-scale representations but shifting representations along the scale, and the smallest result is selected as the distance between two representation features. However, the experimental results of multi-scale representation are not satisfied. That is because for the multi-scale representation with the shifting method, there always is a situation of using one scale representation to calculate the distance. The distance is not stable. In order to avoid such a circumstance, we expand to use the pyramid representation. In this way, there are at least 3 scale representations to calculate the distance, which guarantees more robust results compared with using multiscale representation alone. The pyramid analysis has been frequently used to achieve scale and resolution invariance <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> and approved having practical effect. For our method, the input image I 1 is first used as the original level, and we use L different scales σ = { σ 1 , . . . , σ L } to generate histograms H 1, j , j = {1, . . . , L} corresponding to L scales. To generate the next pyramid level, we subsample the image I 1 using bilinear interpolation with a pixel spacing of 1.5 in each direction, so we can obtain histograms H 2, j using L different scales. By repeating the subsampling operation, we get H i, j , where i = {1, . . . , O} denotes the pyramid level (O is the total number of resolution levels) and j = {1, . . . , L} denotes the scale level. Fig. <ref type="figure" target="#fig_3">4</ref> shows the shifting procedure. The two pyramid histograms are shifted relative to each other in resolution. Hence, we can get 2O -1 possible distance values via chi-square distance measurement. Each distance value is normalized by the involved histogram number, so that all of the distance values lie in the same range. The distance between two images is effectively taken to be the minimum of the distances of 2O -1 possible ways illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>In this section, the classification performance of the proposed approach was systematically evaluated in material categorization. The material databases we used were KTH-TIPS <ref type="bibr" target="#b39">[40]</ref> and KTH-TIPS2-a <ref type="bibr" target="#b40">[41]</ref>. If the image was a color image, it should be transformed into a gray image first. The parameters such as σ, L and O did not change along with the different databases. For the pyramid analysis, L = 3 and O = 3, and the three scales were σ = {1.5, 2.5, 3.5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation on Material Categorization</head><p>The KTH-TIPS database <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> and the KTH-TIPS2a database <ref type="bibr" target="#b40">[41]</ref> were used to test the performance of the proposed algorithm on material categorization. The KTH-TIPS database contains 10 texture classes of sandpaper, crumpled aluminium foil, styrofoam, sponge, corduroy, linen, cotton, brown bread, orange peel and cracker. Images were captured at nine scales spanning two octaves, viewed under three different illumination directions and three different poses, thus giving a total of 9 images per scale, and 81 images per material. The KTH-TIPS2-a provides a considerable extension to KTH-TIPS, containing 4 physical, planar samples of each of 11 materials. The images of each sample have various poses, illumination conditions and scales.  In the first experiment on the KTH-TIPS database, we followed the evaluation setup proposed by Zhang et al. <ref type="bibr" target="#b41">[42]</ref>. The results were reported as the mean percentage of images correctly classified over 100 random splits into training and testing data, and 40 images per class were used for training. In the second experiment on the KTH-TIPS2-a database, the evaluation setup followed Chen et al. <ref type="bibr" target="#b13">[14]</ref>. The images which were not 200 × 200 pixels in size were removed, so there were 4395 images in all. Three samples of each material were randomly selected during training, while testing was subsequently performed on the remaining samples. We repeated the experiment 100 times to obtain credible results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effects of Using Two Distance Measurements:</head><p>In this paragraph we discuss the performance of two different distance measurements: multi-scale features (MLEP) with concatenating scheme and pyramid features (PLEP) with shifting scheme. The scale of textures from the KTH-TIPS and KTH-TIPS2a databases vary gradually from coarse scale to fine scale in the same category (see Fig. <ref type="figure" target="#fig_4">5</ref>). Being invariant to scale variation, PLEP should achieve superior performance compared to MLEP. However, the results in Table <ref type="table" target="#tab_0">I</ref> can not illustrate the superiority of PLEP. This may due to the experimental design. For example, on the KTH-TIPS database, the scales of images vary "gradually" from coarse scale to fine scale, and the testing and training sets were randomly partition. Most of the textures in the testing set could find the nearest images at a very close scale. That is why the PLEP method can not achieve better classification performance. In order to support our speculation, we designed another experiment on the KTH-TIPS database. The front 40 images of each category were used for training, and the others for testing. So the texture scales in the training set are very different from the testing set. Table <ref type="table" target="#tab_1">II</ref> shows the results. The method of PLEP achieves much higher classification performance. Generally, the measurement of PLEP needs a higher computational cost and does not have superiority on the scale invariant database. Mostly, the MLEP could achieve satisfactory result with less computation cost. Therefore, for the database without scale variation, we only adopt the MLEP representation.</p><p>2) Results of Rotation Invariant Representation: In this paragraph, the performance of rotation invariant representation  is discussed. Table <ref type="table" target="#tab_1">III</ref> illustrates the results of M L E P, M L E P do and M L E P hb . These two rotation invariant representations even reduce the classification accuracy on the KTH-TIPS database, and slightly improves the classification performance on the KTH-TIPS2-a database. Apparently, if the images in the database are not rotated, the proposed rotation representation may reduce the accuracy compared with the original method. Although there are some images rotated in KTH-TIPS2-a, it is not a database special for rotation verification. We believe that the rotation invariant methods should be powerful in representing the rotated images.</p><p>In addition, an often overlooked point is that if a texture is perfectly homogeneous and isotropic, then any texture descriptors would be rotation invariant <ref type="bibr" target="#b42">[43]</ref>. In order to check the performance of our method for representing rotated images, we constructed a rotated dataset based on the Brodatz database <ref type="bibr" target="#b43">[44]</ref>. We chose 16 textures which were all anisotropic shown in Fig. <ref type="figure" target="#fig_5">6</ref>. Each texture has the size of 640 × 640 and represents a texture class. The textures were rotated by 0°, 30°, 60°, 30°, 90°, 120°and 150°respectively. The middle 450 × 450 pixels of each rotated texture were sampled to generate a new rotated texture. Then, each new rotated texture was partitioned into nine nonoverlapping sub-images with the size of 150 × 150 pixels. So there were 864 images in the dataset (54 for each texture class). The n sub-images of each category were used for training with the remaining for testing. The experiment was randomly repeated 100 times. Fig. <ref type="figure" target="#fig_6">7 (a)</ref> shows the classification accuracy with different training samples. The methods without rotation invariance obtain very low classification accuracies especially when training number is small. Both the proposed rotation invariant representations M L E P do 6,3 and M L E P hb 6,3 achieve remarkable results. In order to show the results clearly, Table <ref type="table" target="#tab_0">IV</ref> shows the results of only using one training sample. The results of two rotation invariant representations are higher than all the other methods. To obtain more convincing evaluation, we also give the classification results of using all the 111 texture classes of Brodatz database. Fig. <ref type="figure" target="#fig_6">7</ref> (b) shows that our two rotation invariant representations still achieve very high classification accuracies.</p><p>3) Effects of Filter Orientations (P) and Quantization States (N): Table <ref type="table">V</ref> shows that the classification accuracies increase with P and N. A larger P and N indeed makes the feature more accurate but bring some redundancy as well, so the improvement of performance is limited. Most important thing is that larger P and N would induce the dimensionality problem because the length of corresponding feature vectors for M L E P equals 4 × N P (the length for M L E P hb is less because of histogram combination). Mostly, M L E P 6,3 achieves satisfactory performance without too high feature dimensionality (2916 bins for M L E P 6,3 and M L E P do 6,3 , 520 bins for M L E P hb 6,3 ). 4) Local Feature Vector Generated by Other Filters: In this part, we tested the classification performance of the Gaussianlike second derivative filters replaced by other filters, such as Gaussian-like first derivative filters, Gabor filters <ref type="bibr" target="#b23">[24]</ref>, and ring and wedge (R/W) filters <ref type="bibr" target="#b20">[21]</ref> (note that R/W filters are frequency filters, the operation was done in frequency domain). The local feature vector generation and vector quantization scheme remain unchanged. For all sorts of filters, we used 6 filtering orientations, and 4 scales for the multi-scale analysis. Specifically, for Gaussian-like first derivative filters the scales are <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8)</ref>. For Gabor filters, the expression is <ref type="bibr" target="#b14">(15)</ref> where μ = 0.1, σ = (1, 2, 4, 8) and θ determines the orientation. For R/W filters, we chose 6 wedge-shaped orientation filters and 4 ring filters for multi-scale analysis. The classification results are shown in Table <ref type="table" target="#tab_0">VI</ref>. The Gaussianlike second derivative filters achieves better results compared with others. But the R/W filter bank also achieves competitive results, which illustrates that proposed system is universally applicable to generate representation feature to some extent.</p><formula xml:id="formula_21">F(σ, μ, θ) = cos(2π(μx cos(θ ) + μy sin(θ )))e -x 2 +y 2 2σ 2</formula><p>5) Comparisons With Other Representation Methods: As shown in Table <ref type="table" target="#tab_1">VII</ref>, we compared our method with others on the classification task of the KTH-TIPS and KTH-TIPS2-a databases: BIF <ref type="bibr" target="#b15">[16]</ref>, LBP <ref type="bibr" target="#b1">[2]</ref>, LTP <ref type="bibr" target="#b11">[12]</ref>, CLBP <ref type="bibr" target="#b12">[13]</ref>,  WLD <ref type="bibr" target="#b13">[14]</ref>, VZ-MR8 <ref type="bibr" target="#b3">[4]</ref> and VZ-Joint <ref type="bibr" target="#b5">[6]</ref>. Except for the filter generation source code of VZ-MR8 was from <ref type="bibr" target="#b44">[45]</ref>, the BIF, LBP, CLBP, WLD, and VZ-Joint were implemented by ourselves. We have verified the performance on the datasets which were involved in the original literatures (all biases were no more than one percent). The LTP program was modified from the original source. Specifically, we used multiscale L B P u2 (L B P u2  riu2  3,24 /M riu2 2,24 /C was adopted. For the method WLD, the parameters were according to <ref type="bibr" target="#b13">[14]</ref>: M = 6, T = 8, S = 20, and the W L D 8,1 + W L D 16,2 + W L D 24,3 was used for multiscale analysis. As for VZ-MR8 <ref type="bibr" target="#b3">[4]</ref>, 40 textons were found by clustering (via K-means) at all the 10 materials, giving a total dictionary of 40 × 10 = 400 textons for the KTH-TIPS database and 40 × 11 = 440 textons for the KTH-TIPS2-a database. For the method of VZ-Joint <ref type="bibr" target="#b5">[6]</ref>, the size of image patch was 7 × 7, and the texton number of each category was 40 as well. Moreover, for the VZ-algorithms, the images were normalized to have zero means and unit standard deviations according to the original paper <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The chisquare distance measurement was adopted when dissimilarities were calculated for the above features. Results of some other texture classification methods such as <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b45">[46]</ref> are also given in Table <ref type="table" target="#tab_1">VII</ref>. On the KTH-TIPS database, our approach achieves a comparable classification accuracy of 97.56%, which is higher than most of other methods. But the results of <ref type="bibr" target="#b45">[46]</ref> and BIF representation <ref type="bibr" target="#b15">[16]</ref> are higher than ours (method of <ref type="bibr" target="#b45">[46]</ref> with support vector machine (SVM) classifier achieves better results which are not shown here). On the KTH-TIPS2-a database, our method achieves a classification accuracy of 75.57%, which is much higher than other popular methods. Since the KTH-TIPS2-a is a very challenging database, the superior classification performance suggests that our method is more robust to the imaging conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXTENSION TO IMAGE SEQUENCE ANALYSIS</head><p>In this section, we extend our method to image sequence representation. Many approaches have emerged to characterize image sequence. The families of optical flow <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> are currently the most popular because optical flow estimation is a computationally efficient and natural way to characterize the local dynamics of a temporal texture <ref type="bibr" target="#b48">[49]</ref>. The typical characteristic of optical flow is that it reduces dynamic texture analysis to a sequence of instantaneous motion patterns viewed as static textures <ref type="bibr" target="#b48">[49]</ref>. The model-based methods <ref type="bibr" target="#b49">[50]</ref>- <ref type="bibr" target="#b53">[54]</ref> use the framework based on a system identification theory which estimates the parameters of a stable dynamic model. AR-based linear dynamic system (LDS) model <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> is the most popular model-based method for dynamic texture recognition. The methods based on local spatiotemporal filtering <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b54">[55]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation on Dynamic Texture Data</head><p>The UCLA database <ref type="bibr" target="#b50">[51]</ref> comprises 50 sets of 4 sequences of a dynamic texture scene, including boiling water, fire, plants and waterfalls and so on, for a total of 200 sequences. Each sequence is 75 frames long and each frame has the size of 110 × 160. In our experiment, the orientations are shown in Table <ref type="table" target="#tab_1">VIII</ref>. In order to capture the texture and motion information simultaneously during each filtering, the orientations of uniform sampling start from π/4. Unlike the representing static image, we set σ = {0.8, 1.0, 1.2}. The main experiment included two parts: viewpoint specific recognition <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b50">[51]</ref>  and shift-invariant recognition <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Meanwhile, the NN classifier is still used for the recognition.</p><p>In the viewpoint specific recognition experiment, we simply used the original dynamic texture data. A correct detection for a given dynamic texture is defined as having one of the three other dynamic textures in its category as its closest neighbor <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b50">[51]</ref>. In order to reduce the computational complexity, each sequence was carefully cropped to the size of 48 × 48 pixels to include key statistical and dynamical features <ref type="bibr" target="#b50">[51]</ref>. For our method, it has high efficiency in spite of such cropping issues. Table <ref type="table" target="#tab_0">IX</ref> shows the recognition result. The results are concluded as follows. Using AR-based LDS model leads to a recognition rate of 89.5% with the cropped input image <ref type="bibr" target="#b50">[51]</ref>. The oriented energy features is 92.5% <ref type="bibr" target="#b30">[31]</ref>, and even higher rate using SVM has been reported, 97.5% in <ref type="bibr" target="#b56">[57]</ref>. Our method achieves the rate of 96.5% with multiscale analysis. However, the videos in the same category were captured from the same camera viewpoint, so they are so similar to each other that even simply used the mean frame of each sequence in combination with an NN classifier yielding a 60% classification rate <ref type="bibr" target="#b55">[56]</ref>.</p><p>In order to verify the generalized performance of the proposed representation method, a more rigorous experiment was designed similar to <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b55">[56]</ref>, which is called shiftinvariant experiment. Woolfe and Fitzgibbon <ref type="bibr" target="#b55">[56]</ref> cropped the data into a pair of 48 × 48 subsequences to remove the effects of identical viewpoint, but the location of the crop windows were not reported. The crop windows for <ref type="bibr" target="#b30">[31]</ref> are available, thus we cropped the images sequences in the same way as <ref type="bibr" target="#b30">[31]</ref>. Each sequence was spatially partitioned into left and right halves (window pairs, no overlapping), with a few exceptions (the exceptions is specially cropped by <ref type="bibr" target="#b30">[31]</ref>). After the partition we had 400 sequences totally. Comparisons were only performed between left and right halves, that is, train using left halves and test using right halves, and vice versa. A correct detection for a given dynamic texture sequence is defined as having one of the four dynamic texture sequences from other side of same category as its nearest neighbor. In this experiment, the left halves and the right halves are independent to each other, and could be regarded as having no location relationships.</p><p>Due to the partition, as Table <ref type="table" target="#tab_0">IX</ref> shows, the result of LDSbased approach <ref type="bibr" target="#b50">[51]</ref> reduces from 89.5% to about 15% <ref type="bibr" target="#b55">[56]</ref>. The method based on specifically multivariate autoregressive models improved the rate to almost 20% <ref type="bibr" target="#b55">[56]</ref>. The reason for the performance of the methods <ref type="bibr" target="#b50">[51]</ref> and <ref type="bibr" target="#b55">[56]</ref> reduced severely may have relation to that such methods needs rigorous alignment. Although we did not implement the algorithm in <ref type="bibr" target="#b56">[57]</ref>, which is an improvement of AR-based LDS model such that it still needs rigorous alignment. The shift invariant experiment makes the images do not have corresponding relation of locations. Thus, it is not expected that <ref type="bibr" target="#b56">[57]</ref> achieved good recognition performance in the shift-invariant experiment. The statistical based methods do not require alignment, that is why VLBP and our method achieved better results, and our method achieved a result as high as 86.75%. However, both used 6 orientations for filtering, MV L E P <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b2">3)</ref>, <ref type="bibr" target="#b2">3</ref> and MV L E P (3,2),3 have different classification performance. It is possible due to the fact that the ϕ controls the angle to rotate in the xy plain, and φ controls the angle to rotate around axis t. If the texture information is more important than motion during the classification, the sampling of ϕ should be denser than φ. Otherwise, the sampling of φ should be denser than ϕ. For the shift-invariant recognition on the UCLA database, it seems that the motion plays more important role for the classification.</p><p>Since many of the scenes (50 in total) capture semantically equivalent categories <ref type="bibr" target="#b30">[31]</ref>, Derpanis and Wildes combined some different scenes with similar semantic content into the same category. The number of categories is reduced from 200 to 7. The overall classification accuracy of <ref type="bibr" target="#b30">[31]</ref> increased to 92.5%, while the result of our method (MV L E P <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b2">3)</ref>,3 ) becomes 98.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We develop a local descriptor which models texture image as histogram over statistical local feature vectors. By using normalized local oriented energies to generate the local feature vectors, our features are less sensitive to the imaging conditions than other features considered in this paper. The pyramid analysis makes the feature more robust to scale changes.</p><p>We also propose two approaches to make the representation being rotation invariance. Our major contributions are that we use the normalized oriented energies to generate local feature vectors to describe local structures, and use the N-nary coding for the vector quantization. The experiments on material categorization illustrate that our approach is superior to the some other popular methods. However, for most databases in our evaluation, we show that rotation representation of our method often do not yield the best performance. Apparently, if the images in the database were not rotated, the proposed rotation representation would reduce the accuracy compared to the original method. It should make sure that the local invariance properties do not exceed the level absolutely required for a given application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Responses of the G 2θ filter in (a) space domain and (b) frequency domain with θ = π/4, π/2, 3π/4, π.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. PDFs of local feature vector elements. (a) PDF on the KTH-TIPS material database with σ = 2.5 and P = 4. (b) PDF on the Brodatz database with σ = 2.5 and P = 4. The PDF is of all images in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Diagram of LEP image representation. The image is from the KTH-TIPS2-a material database and the standard deviation of Gaussian-like filter σ = 2.5. The filter orientation θ = π/4, π/2, 3π/4, and π , respectively, and the quantization level N equals 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Distance achieved between pyramid histograms by shifting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of corduroy from the KTH-TIPS database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Anisotropic textures from the Brodatz database.</figDesc><graphic coords="8,187.43,129.53,54.50,54.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Classification accuracy with different number of training samples on the Brodatz database. (a) Results of using 16 anisotropic textures. (b) Results of using all 111 textures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="3">CLASSIFICATION ACCURACY OF USING</cell></row><row><cell cols="3">DIFFERENT DISTANCE SCHEMES</cell></row><row><cell></cell><cell>M L E P 6,3</cell><cell>P L E P 6,3</cell></row><row><cell></cell><cell>(concatenating)</cell><cell>(shifting)</cell></row><row><cell>KTH-TIPS</cell><cell>96.41%</cell><cell>97.56%</cell></row><row><cell>KTH-TIPS2-a</cell><cell>73.54%</cell><cell>71.53%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CLASSIFICATION</head><label>II</label><figDesc>ACCURACY OF USING DIFFERENT DISTANCE MEASUREMENTS ON THE KTH-TIPS DATABASE. THE FIRST 40 IMAGES OF EACH CATEGORY WERE USED FOR TRAINING,</figDesc><table><row><cell cols="3">AND THE OTHERS WERE USED FOR TESTING</cell></row><row><cell></cell><cell>M L E P 6,3</cell><cell>P L E P 6,3</cell></row><row><cell></cell><cell>(concatenating)</cell><cell>(shifting)</cell></row><row><cell>KTH-TIPS</cell><cell>71.95%</cell><cell>89.49%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank G. Doretto for providing access to the UCLA dynamic texture database and X. Tan for providing access to the local ternary pattern (LTP) code.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported in part by the 973 Program under Grant 2011CB707702, the National Natural Science Foundation of China under Grant 81090272, Grant 60872154, Grant 60902083, and Grant 41031064, the Ocean Public Welfare Scientific Research Project, and the State Oceanic Administration People's Republic of China under Grant 201005017. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Chun-Shien Lu.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>is of fundamental descriptive power, as it captures the firstorder correlation structure of the data irrespective of its origin <ref type="bibr" target="#b30">[31]</ref>, and can capture motions and textures by oriented energies. Derpanis and Wildes summed these oriented energies as features for dynamic texture recognition and achieved great progress in shift-invariant recognition <ref type="bibr" target="#b30">[31]</ref>.</p><p>We develop volume local energy pattern (VLEP) for the dynamic texture representation. The classification experiments on the University of California at Los Angeles (UCLA) dynamic texture database <ref type="bibr" target="#b50">[51]</ref> was employed to check the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Representation Methods for Dynamic Texture</head><p>The representation procedure for dynamic texture image sequence is similar to that of static texture image, but the local feature vector should describe the texture and motion simultaneously. 3D steerable filters <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref> replace steerable 2D filters, and the smoothing is 3D as well. The conclusion is that the filters now are given by two angles (φ and ϕ) while for the 2D case one angle (θ ) is used. For our method, both angles (φ and ϕ) are sampled at proper intervals with the bounds of 0 ≤ φ ≤ π and 0 ≤ ϕ ≤ π. Since there are two variables to define the orientation, it is more flexible to choose the filtering orientation than 2D filtering. The orientations could be obtained by uniform sampling the two angles (φ and ϕ). Suppose the numbers of sampling orientation of angles (φ and ϕ) are P φ and P ϕ respectively. The corresponding representation denotes as V L E P ( P φ ,P ϕ ),N , where N represents the quantization level. Thus, the total number of filtering orientations is P φ × P ϕ . Furthermore, the multi-scale V L E P ( P φ ,P ϕ ),N denotes as MV L E P ( P φ ,P ϕ ),N . Please note that the uniform sampling of the two orientation angles in 3D filtering does not yield a uniform sampling in 3D directions on the sphere. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unifying statistical texture classification frameworks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1175" to="1183" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiresolution gray scale and rotation invariant texture analysis with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A statistical approach to texture classification from single images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D texture recognition using bidirectional feature histograms</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="60" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A statistical approach to material classification using image patch exemplars</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2032" to="2047" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Textural features for image classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973-11">Nov. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Polarograms: A new tool for image texture analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="223" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Kindermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Society</surname></persName>
		</author>
		<title level="m">Markov Random Fields and their Applications</title>
		<meeting><address><addrLine>Providence, RI</addrLine></address></meeting>
		<imprint>
			<publisher>AMS</publisher>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification of rotated and scaled textured images using Gaussian Markov random field models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="202" />
			<date type="published" when="1991-02">Feb. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dominant local binary patterns for texture classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W K</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1107" to="1118" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced local texture feature sets for face recognition under difficult lighting conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1635" to="1650" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A completed modeling of local binary pattern operator for texture classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1657" to="1663" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">WLD: A robust local image descriptor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1705" to="1720" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image description using joint distribution of filter bank responses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="368" to="376" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using basic image features for texture classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Crosier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="460" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>Upper Saddle River, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Theories of visual texture perception</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Visual Dysfunction</title>
		<imprint>
			<date type="published" when="1991-01">Jan. 1991</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="114" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A spatial filtering approach to texture analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Coggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="203" />
			<date type="published" when="1985-05">May 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Texture features and learning similarity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vision Pattern Recogn</title>
		<meeting>Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="1996-06">Jun. 1996</date>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rotation-invariant texture classification using a complete space-frequency model</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Haley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="269" />
			<date type="published" when="1999-02">Feb. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Filtering for texture classification: A comparative study</title>
		<author>
			<persName><forename type="first">T</forename><surname>Randen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Husøy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="310" />
			<date type="published" when="1999-04">Apr. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constructing models for content-based image retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2001-03">Mar. 2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The design and use of steerable filters</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="891" to="906" />
			<date type="published" when="1991-09">Sep. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Steerable wedge filters for local orientation analysis</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1377" to="1382" />
			<date type="published" when="1996-09">Sep. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning local image descriptors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Locally rotation, contrast, and scale invariant descriptors for texture analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition based on distributions of spacetime oriented structure</title>
		<author>
			<persName><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Vision Pattern Recogn. Conf</title>
		<meeting>IEEE Comput. Vision Pattern Recogn. Conf</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised texture segmentation using Gabor filters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Farrokhnia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1167" to="1186" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bracewell</surname></persName>
		</author>
		<title level="m">The Fourier Transform and its Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rotation invariant texture recognition using a steerable pyramid</title>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Pattern Recogn. Conf. B, Comput. Vision Image Process</title>
		<meeting>12th Int. Pattern Recogn. Conf. B, Comput. Vision Image ess</meeting>
		<imprint>
			<date type="published" when="1994-10">Oct. 1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="162" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Sokal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rohlf</surname></persName>
		</author>
		<author>
			<persName><surname>Biometry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>W. H. Freeman</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analysis/synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Ann. Conf</title>
		<meeting>22nd Ann. Conf</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the significance of real-world conditions for material classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Varlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="253" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The KTH-TIPS and KTH-TIPS2 Image Databases</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mallikarjuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Targhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eklundh</surname></persName>
		</author>
		<ptr target="http://www.nada.kth.se/cvap/databases/kth-tips/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Local features and kernels for classification of texture and object categories: A comprehensive study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vision Pattern Recogn. Workshop Conf</title>
		<meeting>Comput. Vision Pattern Recogn. Workshop Conf</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rotation invariant texture features and their use in automatic script identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="751" to="756" />
			<date type="published" when="1998-07">Jul. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Textures: A Photographic Album for Artists and Designers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brodatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>Dover</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://www.robots.ox.ac.uk/∼vgg/research/texclass/filters.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sorted random projections for robust texture classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Beauchemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="77" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Synergizing spatial and temporal texture</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1179" to="1191" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A brief survey of dynamic texture description and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Comput. Recogn. Syst</title>
		<imprint>
			<biblScope unit="page" from="17" to="26" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal texture modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Image Process. Int. Conf</title>
		<meeting>Image ess. Int. Conf</meeting>
		<imprint>
			<date type="published" when="1996-09">Sep. 1996</date>
			<biblScope unit="page" from="823" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Saisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vision Pattern Recogn. Conf</title>
		<meeting>Comput. Vision Pattern Recogn. Conf</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic textures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Modeling textured motion: Particle, wave and sketch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Vision 9th Int. Conf</title>
		<meeting>IEEE Comput. Vision 9th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2003-10">Oct. 2003</date>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Synthesizing dynamic texture with closed-loop linear dynamic system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vision</title>
		<meeting>Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="603" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Three-dimensional nth derivative of Gaussian separable steerable filters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gryn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2005-09">Sep. 2005</date>
			<biblScope unit="page" from="553" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Shift-invariant dynamic texture recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Woolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vision</title>
		<meeting>Eur. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="549" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Probabilistic kernels for the classification of auto-regressive visual processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Vision Pattern Recogn</title>
		<meeting>IEEE Comput. Vision Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="846" to="851" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
