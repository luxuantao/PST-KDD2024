<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparative Assessment of Content-Based Face Image Retrieval in Different Color Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peichung</forename><surname>Shih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New Jersey Institute of Technology</orgName>
								<address>
									<postCode>07102</postCode>
									<settlement>Newark</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chengjun</forename><surname>Liu</surname></persName>
							<email>liu@cs.njit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">New Jersey Institute of Technology</orgName>
								<address>
									<postCode>07102</postCode>
									<settlement>Newark</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparative Assessment of Content-Based Face Image Retrieval in Different Color Spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E582A269185998527A1EE136F6416A24</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Content-based face image retrieval is concerned with computer retrieval of face images (of a given subject) based on the geometric or statistical features automatically derived from these images. It is well known that color spaces provide powerful information for image indexing and retrieval by means of color invariants, color histogram, color texture, etc.. This paper assesses comparatively the performance of content-based face image retrieval in different color spaces using a standard algorithm, the Principal Component Analysis (PCA), which has become a popular algorithm in the face recognition community. In particular, we comparatively assess 12 color spaces (RGB,</p><p>, HSI, and rgb) by evaluating 7 color configurations for every single color space. A color configuration is defined by an individual or a combination of color component images. Take the RGB color space as an example, possible color configurations are R, G, B, RG, RB, GB, and RGB. Experimental results using 1,800 FERET R, G, B images corresponding to 200 subjects show that some color configurations, such as R in the RGB color space and V in the HSV color space, help improve face retrieval performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Content-based face image retrieval is concerned with computer retrieval of face images (of a given subject) based on the geometric or statistical features automatically derived from these images <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Efficient retrieval requires a robust feature extraction method that has the ability to learn meaningful low-dimensional patterns in spaces of very high dimensionality. Low-dimensional representations are also important when one considers the intrinsic computational aspect. The Principal Component Analysis (PCA) <ref type="bibr" target="#b2">[3]</ref> has been widely used to perform dimensionality reduction for face indexing and retrieval <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In particular, PCA is the method behind the Eigenfaces coding scheme <ref type="bibr" target="#b7">[8]</ref> whose primary goal is to project the similarity judgment for face recognition into a low-dimensional space. This space defines a feature space, or a "face space", which drastically reduces the dimensionality of the original space, and face detection and identification are carried out in this reduced face space.</p><p>It is well known that color spaces provide powerful information for image indexing and retrieval by means of color invariants, color histogram, color texture, etc.. Different color spaces, which are defined by means of transformations from the original RGB (red, green, blue) color space, display different color properties. The HSV (hue, saturation, value) color space and its variants, such as the HSI (hue, saturation, intensity) color space and the HLS (hue, lightness, saturation) color space, are often applied in locating and extracting facial features <ref type="bibr" target="#b8">[9]</ref>. The Y CbCr (luminance, Chrominance-blue, Chrominance-red) color space, the Y IQ (luminance, in-phase, quadrature) color space, and the Y UV color space have wide applications in color clustering and quantization for skin color regions <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The perceptually uniform color spaces, such as the CIE-U * V * W * color space, the CIE-L * u * v * color space, and the CIE-L * a * b * color space have general and ubiquitous applications <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>In this paper, we assess the performance of content-based face image retrieval in different color spaces using a standard algorithm, PCA <ref type="bibr" target="#b2">[3]</ref>. Specifically, we assess comparatively 12 color spaces (RGB,</p><formula xml:id="formula_0">HSV , Y UV , Y CbCr, XY Z, Y IQ, L * a * b * , U * V * W * , L * u * v * , I 1 I 2 I 3 ,</formula><p>HSI, and rgb) by evaluating 7 color configurations for every single color space. A color configuration is defined by an individual or a combination of color component images. Take the RGB color space as an example, possible color configurations are R, G, B, RG, RB, GB, and RGB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Color Spaces</head><p>This section details the 12 color spaces assessed in this paper. The rgb color space is defined by projecting the R, G, B values onto the</p><formula xml:id="formula_1">R = G = B = max{R, G, B} plane, such that r = R/(R + G + B), g = G/(R + G + B), and b = B/(R + G + B).</formula><p>The I 1 I 2 I 3 color space proposed by Ohta et al. <ref type="bibr" target="#b12">[13]</ref> applies a Karhunen-Loeve transformation to decorrelate the RGB components. The linear transformation based on Ohta's experimental model is defined as: <ref type="bibr" target="#b12">[13]</ref>.</p><formula xml:id="formula_2">I 1 = (R + G + B)/3, I 2 = (R -B)/2, and I 3 = (2G -R -B)/2</formula><p>The HSV and the HSI color spaces are motivated by the human vision system in the sense that human describes color by means of hue, saturation, and brightness. Let M AX = max(R, G, B), M IN = min(R, G, B), and δ = M AX -M IN , the HSV color space is defined as follows <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_3">V = M AX; S = δ/M AX; H =    60(G -B)/δ if M AX = R 60(B -R + 2δ)/δ if M AX = G 60(R -G + 4δ)/δ if M AX = B (1)</formula><p>The HSI color space is specified as follows <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_4">I = (R + G + B)/3; S = 1 -I * M IN ; H = θ if B ≤ G 360 -θ otherwise (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where</p><formula xml:id="formula_6">θ = cos -1 1 2 [(R -G) + (R -B)]/[(R -G) 2 + (R -B)(G -B)] 1 2</formula><p>. Note that in both Eq. 1 and Eq. 2, the R, G, B values are scaled to [0,1].</p><p>The Y UV and the Y IQ color spaces are commonly used in video for transmission efficiency. The Y IQ color space is adopted by the NTSC(National Television System Committee) video standard in reference to RGB N T SC, while the Y UV color space is used by the PAL (Phase Alternation by Line) and the SECAM (System Electronique Couleur Avec Memoire). The Y UV color space and the Y IQ color space are specified as follows:</p><formula xml:id="formula_7">  Y U V   =  </formula><p>0.2990 0.5870 0.1140 -0.1471 -0.2888 0.4359 0.6148 -0.5148 -0.1000  </p><formula xml:id="formula_8">    R G B     Y I Q   =   0.</formula><formula xml:id="formula_9">    R G B  <label>(4)</label></formula><formula xml:id="formula_10">    R G B  <label>(5)</label></formula><p>Note that the Y component defined here is consistent with the luminance defined in Eq. 3 or Eq. 4. In addition, a chromaticity diagram can be derived via the chromaticity coordinates x, y, which are specified by the X, Y, Z tristimulus. This CIE chromaticity diagram, however, is not perceptually uniform <ref type="bibr" target="#b15">[16]</ref>. To overcome such a shortcoming, the CIE uv chromaticity diagram was proposed <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_11">u = 4x/(-2x + 12y + 3) or 4X/(X + 15Y + 3Z) v = 6y/(-2x + 12y + 3) or 6Y/(X + 15Y + 3Z)<label>(6)</label></formula><p>Based on this uniform chromaticity scale (UCS), a CIE uniform color space U * V * W * was proposed. The W * component corresponds to luminance, while the U * , V * components correspond to chrominance <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_12">W * = 116( Y Yo ) 1 3 -16 if Y Yo &gt; 0.008856 903.3( Y Yo ) otherwise ; U * = 13W * (u-u o ); V * = 13W * (v-v o )<label>(7</label></formula><p>) where the u o and v o are derived from the reference white stimulus.</p><p>Although the CIE-uv diagram is perceptually uniform, it has its own deficiency in representing yellow-red colors as the area of yellow-red in the diagram is relatively small <ref type="bibr" target="#b16">[17]</ref>. To improve this deficiency, a new u v coordinate system is defined: u = u, v = (3/2)v. Based on this new coordinate system, two CIE uniform color spaces were defined, namely the CIE-L * u * v * color space and the CIE-L * a * b * color space <ref type="bibr" target="#b15">[16]</ref>. The CIE-L * u * v * color space is proposed to obsolete the U * V * W * color space by substituting W * , U * , V * , u, v in Eq. 7 for L * , u * , v * , u , v , respectively. The L * a * b * color space is modeled based on human vision system and is defined as follows: 16  116 otherwise.</p><formula xml:id="formula_13">L * = 116f ( Y Y o ) -16; a * = 500[f ( X X o ) -f ( Y Y o )]; b * = 200[f ( Y Y o ) -f ( Z Z o )] (8) where f (x) = x 1 3 if x &gt; 0.008856; f (x) = 7.787x +</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Principal Component Analysis and Classification Rule</head><p>PCA is a standard decorrelation technique and following its application one derives an orthogonal projection basis that directly leads to dimensionality reduction and feature extraction. Let X ∈ R N be a random vector representing an image, and Σ X ∈ R N ×N be the covariance matrix of X. The PCA procedure factorizes the Σ X into the form: Σ X = ΦΛΦ t , where Φ is an orthogonal eigenvector matrix and Λ a diagonal eigenvalue matrix with diagonal elements in decreasing order. An important property of PCA is its optimal signal reconstruction in the sense of minimum Mean Square Error (MSE) when only a subset of principal components is used to represent the original signal. Following this property, an immediate application of PCA is the dimensionality reduction by projecting a random vector X onto the eigenvectors: Y = P t X, where P ∈ R N ×m is a subset of eigenvector matrix Φ and m &lt; N. The lower dimensional vector Y ∈ R m captures the most expressive features of the original data X.</p><p>After dimensionality reduction, feature vectors are compared and classified by the nearest neighbor (to the mean) rule using a similarity (distance) measure δ: δ(Y, M k ) = min j δ(Y, M j ) -→ Y ∈ ω k , where Y is a testing feature vector and M 0 k , k = 1, 2, . . . , L is the mean of the training samples for class ω k . The testing feature vector, Y, is classified as belonging to the class of the closest mean, M k , using the similarity measure δ.</p><p>The similarity measures used in our experiments to evaluate the efficiency of different representation and recognition methods include the L 1 distance measure, δ L1 , the L 2 distance measure, δ L2 , the Mahalanobis distance measure, δ Md , and the cosine similarity measure, δ cos , which are defined as follows:</p><formula xml:id="formula_14">δ L1 (X , Y) = i |X i -Y i | δ L2 (X , Y) = (X -Y) t (X -Y) δ Md (X , Y) = (X -Y) t Σ -1 (X -Y) δ cos (X , Y) = -X t Y X Y (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>where Σ is the covariance matrix, and • denotes the norm operator. Note that the cosine similarity measure includes a minus sign because the nearest neighbor (to the mean) rule applies minimum (distance) measure rather than maximum similarity measure <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section assesses the performance of content-based face image retrieval in the 12 color spaces defined in Sect. 2. The 1,800 images from the FERET database <ref type="bibr" target="#b18">[19]</ref> are The performance of content-based face image retrieval using the PCA method on the intensity images derived by averaging the R, G, B color components. The similarity measures applied are the Mahalanobis distance measure (Mah.), the L1 distance measure (L1), the L2 distance measure (L2), and the cosine similarity measure (cos) used for our experiments. The images correspond to 200 subjects such that each subject has 9 images (3 sets of R, G, B images). As there are three sets of images for each subject, two sets are randomly chosen for training, while the remaining set (unseen during training) is used for testing. Note that all images are normalized to the size of 128 × 128 to extract facial regions that contain only faces, so that the performance of face retrieval is not affected by the factors not related to face, such as hair style.</p><p>To provide a baseline performance for comparison, our first set of experiments applies different similarity measures as defined in Sect. 3 on the intensity images derived by averaging the R, G, B color components. Fig. <ref type="figure">1</ref> shows the performance of contentbased face image retrieval using the PCA method as detailed in Sect. 3. The horizontal axis indicates the number of features used, and the vertical axis represents the correct retrieval rate, which is the accuracy rate for the top response being correct. Fig. <ref type="figure">1</ref> shows that the Mahalanobis distance measure performs the best, followed in order by the L 1 distance measure, the L 2 distance measure, and the cosine similarity measure. The experimental results provide a baseline face retrieval performance based on the intensity images, and suggest that one should use the Mahalanobis distance measure for the comparative assessment in different color spaces.</p><p>We now assess comparatively content-based face image retrieval in the 12 different color spaces as defined in Sect. 2. For each color space, we define 7 color configurations by means of an individual or a combination of color component images. Take the RGB color space as an example, possible color configurations are R, G, B, RG, RB, GB, and RGB. Note that when two or three color component images are used to define a color configuration, each color component image is first normalized to zero mean and unit variance, and then the normalized color component images are concatenated to form an augmented vector representing the color configuration.  <ref type="figure">8</ref>, respectively. Note that the Y UV and the Y CbCr color spaces (as well as the U * V * W * and the L * u * v * color spaces) have identical face retrieval performance due to their definitions (see Sect. 2). In particular, Fig. <ref type="figure">2</ref> shows that the R and the RG color configurations perform better than the intensity images. Fig. <ref type="figure">3</ref> shows that the V color configuration outperforms the intensity images. Fig. <ref type="figure">4</ref> shows that the Y and Y V color configurations in the Y UV color space or the Y and Y Cr color configurations in the Y CbCr color space have better face retrieval perfor- mance than the intensity images. Fig. <ref type="figure">5</ref> shows the X, Y , and XY color configurations perform better than the intensity images. Fig. <ref type="figure">6</ref> shows that the Y and Y I color configurations outperform the intensity images. Fig. <ref type="figure">7</ref> shows that the L * , L * a * , L * b * , and L * a * b * color configurations are better than the intensity images for face retrieval. Fig. <ref type="figure">8</ref> shows that the W * and U * W * color configurations in the U * V * W * color space or the L * and L * u * color configurations in the L * u * v * color space perform better than the intensity images.</p><p>The color configurations, which perform better than the intensity images for face retrieval, are summarized in Table <ref type="table">1</ref>. Note that those color configurations with better face  <ref type="table">1</ref> all share one common characteristic: they contain both chromatic and achromatic (intensity) components. The pure chromatic color configurations, however, all display worse (than the intensity images) face retrieval performance. Specifically, these pure chromatic color configurations include the HS in Fig. <ref type="figure">3</ref>, the U V and the CbCr in Fig. <ref type="figure">4</ref>, the IQ in Fig. <ref type="figure">6</ref>, the a * b * in Fig. <ref type="figure">7</ref>, and the U * V * and the u * v * in Fig. <ref type="figure">8</ref>. Note also that simply applying all the color components does not necessarily achieve the best face retrieval performance. The reason for this finding is that some color configurations, such as the B in the RGB color space (Fig. <ref type="figure">2</ref>), the Z in the XY Z color space (Fig. <ref type="figure">5</ref>), and the pure chromatic color configurations discussed above, all perform worse than the intensity images for content-based face image retrieval. We have experimented with the I 1 I 2 I 3 , HSI, and rgb color spaces as well, but the experimental results show that the color configurations in these color spaces do not improve face retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have assessed comparatively the performance of content-based face image retrieval in 12 color spaces using a standard algorithm, the PCA, which has become a popular algorithm in the face recognition community. In particular, we have comparatively assessed the RGB, HSV , Y UV , Y CbCr, XY Z, Y IQ, L * a * b * , U * V * W * , L * u * v * , I 1 I 2 I 3 , HSI, and rgb color spaces by evaluating 7 color configurations for every single color space. Experimental results using 1,800 FERET R, G, B images corresponding to 200 subjects show that some color configurations, such as R in the RGB color space and V in the HSV color space, help improve face retrieval performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where the R, G, B values are scaled to [0,1].The CIE (Commission Internationale de l' Éclairage) perceptually uniform color spaces, such as the U * V * W * , the L * u * v * , and the L * a * b * color spaces, are defined based on the XY Z tristimulus:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig.1. The performance of content-based face image retrieval using the PCA method on the intensity images derived by averaging the R, G, B color components. The similarity measures applied are the Mahalanobis distance measure (Mah.), the L1 distance measure (L1), the L2 distance measure (L2), and the cosine similarity measure (cos)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig.2. Content-based face image retrieval performance of the 7 color configurations in the RGB color space. Note that the performance curve of the intensity images (Gray) is also included for comparison (same in the following figures)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Content-based face image retrieval performance of the 7 color configurations in the Y UV color space. Note that the face retrieval results in the Y CbCr color space are the same when the Y , U , and V color components are replaced by their counterparts Y , Cb, and Cr, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Content-based face image retrieval performance of the 7 color configurations in the Y IQ color space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 . 1 .</head><label>81</label><figDesc>Fig. 8. Content-based face image retrieval performance of the 7 color configurations in the U * V * W * color space. Note that the face retrieval results in the L * u * v * color space are the same when the U * , V * , and W * color components are replaced by their counterparts u * , v * , and L * , respectively</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>T. Kanade, A. Jain, and N.K. Ratha (Eds.): AVBPA 2005, LNCS 3546, pp. 1039-1048, 2005. c Springer-Verlag Berlin Heidelberg 2005</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the TSWG R&amp;D Contract N41756-03-C-4026.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling focus of attention for meeting index based on multiple cues</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="928" to="938" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Pattern Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
	<note>second edn.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gabor-based kernel PCA with fractional power polynomial m odels for face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="572" to="581" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust coding schemes for indexing and retrieval from large face databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="132" to="137" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Symmetric shape-from-shading using self-ratio image. Int</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Computer Vision</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="55" to="75" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A direct lda algorithm for high-dimensional data -with application to face recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2067" to="V2070" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face detection using quantized skin color regions merging and wavelet packet analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tziritas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="264" to="277" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hand and face segmentation using motion and color cues in digital image sequences</title>
		<author>
			<persName><forename type="first">N</forename><surname>Habili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Multimedia and Expo</title>
		<meeting>IEEE International Conference on Multimedia and Expo<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face detection from color images using a fuzzy pattern matching method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="557" to="563" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparative performance of different skin chrominance models and chrominance space for the automatic detection of human faces in color images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Terrillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fukamachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. The Fourth International Conference on Face and Gesture Recognition</title>
		<meeting>The Fourth International Conference on Face and Gesture Recognition<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Knowledge-Based Interpretation of Outdoor Natural Color Scenes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Pitman Publishing</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Color gamut transform pairs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="12" to="19" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wyszecki</surname></persName>
		</author>
		<title level="m">Color in Business, Science and Industry</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Colour: Its Measurement, Computation and Application</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chamberlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chamberlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heyden &amp; Son</title>
		<imprint>
			<date type="published" when="1980">1980</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analysis of pca-based face recognition algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Evaluation Techniques in Computer Vision</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley-IEEE Computer Society</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The FERET database and evaluation procedure for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
