<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Precision-Recall-Gain Curves: PR Analysis Done Right</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
							<email>peter.flach@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems Laboratory</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meelis</forename><surname>Kull</surname></persName>
							<email>meelis.kull@bristol.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Intelligent Systems Laboratory</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Precision-Recall-Gain Curves: PR Analysis Done Right</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BC60B8BBF7A0653170BB31F2D66DE194</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Precision-Recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier's performance. Perhaps inspired by the many advantages of receiver operating characteristic (ROC) curves and the area under such curves for accuracybased performance assessment, many researchers have taken to report Precision-Recall (PR) curves and associated areas as performance metric. We demonstrate in this paper that this practice is fraught with difficulties, mainly because of incoherent scale assumptions -e.g., the area under a PR curve takes the arithmetic mean of precision values whereas the F β score applies the harmonic mean. We show how to fix this by plotting PR curves in a different coordinate system, and demonstrate that the new Precision-Recall-Gain curves inherit all key advantages of ROC curves. In particular, the area under Precision-Recall-Gain curves conveys an expected F 1 score on a harmonic scale, and the convex hull of a Precision-Recall-Gain curve allows us to calibrate the classifier's scores so as to determine, for each operating point on the convex hull, the interval of β values for which the point optimises F β . We demonstrate experimentally that the area under traditional PR curves can easily favour models with lower expected F 1 score than others, and so the use of Precision-Recall-Gain curves will result in better model selection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>In machine learning and related areas we often need to optimise multiple performance measures, such as per-class classification accuracies, precision and recall in information retrieval, etc. We then have the option to fix a particular way to trade off these performance measures: e.g., we can use overall classification accuracy which gives equal weight to correctly classified instances regardless of their class; or we can use the F 1 score which takes the harmonic mean of precision and recall. However, multi-objective optimisation suggests that to delay fixing a trade-off for as long as possible has practical benefits, such as the ability to adapt a model or set of models to changing operating contexts. The latter is essentially what receiver operating characteristic (ROC) curves do for binary classification. In an ROC plot we plot true positive rate (the proportion of correctly classified positives, also denoted tpr) on the y-axis against false positive rate (the proportion of incorrectly classified negatives, also denoted fpr) on the x-axis. A categorical classifier evaluated on a test set gives rise to a single ROC point, while a classifier which outputs scores (henceforth called a model) can generate a set of points (commonly referred to as the ROC curve) by varying the decision threshold (Figure <ref type="figure" target="#fig_0">1</ref> (left)).</p><p>ROC curves are widely used in machine learning and their main properties are well understood <ref type="bibr" target="#b2">[3]</ref>. These properties can be summarised as follows. Universal baselines: the major diagonal of an ROC plot depicts the line of random performance which can be achieved without training. More specifically, a random classifier assigning the positive class with probability p and the negative class with probability 1p has expected true positive rate of p and true negative rate of 1p, represented by the ROC point (p, p).</p><p>The upper-left (lower-right) triangle of ROC plots hence denotes better (worse) than random performance. Related baselines include the always-negative and always-positive classifier which occupy fixed points in ROC plots (the origin and the upper right-hand corner, respectively). These baselines are universal as they don't depend on the class distribution.</p><p>Linear interpolation: any point on a straight line between two points representing the performance of two classifiers (or thresholds) A and B can be achieved by making a suitably biased random choice between A and B <ref type="bibr" target="#b13">[14]</ref>. Effectively this creates an interpolated contingency table which is a linear combination of the contingency tables of A and B, and since all three tables involve the same numbers of positives and negatives it follows that the interpolated accuracy as well as true and false positive rates are also linear combinations of the corresponding quantities pertaining to A and B. The slope of the connecting line determines the trade-off between the classes under which any linear combination of A and B would yield equivalent performance. In particular, test set accuracy assuming uniform misclassification costs is represented by accuracy isometrics with slope (1π)/π, where π is the proportion of positives <ref type="bibr" target="#b4">[5]</ref>. Optimality: a point D dominates another point E if D's tpr and fpr are not worse than E's and at least one of them is strictly better. The set of non-dominated points -the Pareto front -establishes the set of classifiers or thresholds that are optimal under some trade-off between the classes. Due to linearity any interpolation between non-dominated points is both achievable and non-dominated, giving rise to the convex hull (ROCCH) which can be easily constructed both algorithmically and by visual inspection.</p><p>Area: the proportion of the unit square which falls under an ROC curve (AUROC) has a well-known meaning as a ranking performance measure: it estimates the probability that a randomly chosen positive is ranked higher by the model than a randomly chosen negative <ref type="bibr" target="#b6">[7]</ref>. More importantly in a classification context, there is a linear relationship between AUROC = 1 0 tpr d fpr and the expected accuracy acc = πtpr + (1π)(1fpr) averaged over all possible predicted positive rates rate = πtpr + (1π)fpr which can be established by a change of variable:</p><formula xml:id="formula_0">E [acc] = 1 0 acc d rate = π(1 -π)(2AUROC -1) + 1/2 [8]</formula><p>. Calibration: slopes of convex hull segments can be interpreted as empirical likelihood ratios associated with a particular interval of raw classifier scores. This gives rise to a non-parametric calibration procedure which is also called isotonic regression <ref type="bibr" target="#b18">[19]</ref> or pool adjacent violators <ref type="bibr" target="#b3">[4]</ref> and results in a calibration map which maps each segment of ROCCH with slope r to a calibrated score c = πr/(πr + (1π)) <ref type="bibr" target="#b5">[6]</ref>. Define a skew-sensitive version of accuracy as acc c 2cπtpr + 2(1c)(1π)(1fpr) (i.e., standard accuracy is acc c=1/2 ) then a per-fectly calibrated classifier outputs, for every instance, the value of c for which the instance is on the acc c decision boundary.</p><p>Alternative solutions for each of these exist. For example, parametric alternatives to ROCCH calibration exist based on the logistic function, e.g. Platt scaling <ref type="bibr" target="#b12">[13]</ref>; as do alternative ways to aggregate classification performance across different operating points, e.g. the Brier score <ref type="bibr" target="#b7">[8]</ref>. However, the power of ROC analysis derives from the combination of the above desirable properties, which helps to explain its popularity across the machine learning discipline.</p><p>This paper presents fundamental improvements in Precision-Recall analysis, inspired by ROC analysis, as follows. (i) We identify in Section 2 the problems with current practice in Precision-Recall curves by demonstrating that they fail to satisfy each of the above properties in some respect. (ii) We propose a principled way to remedy all these problems by means of a change of coordinates in Section 3. (iii) In particular, our improved Precision-Recall-Gain curves enclose an area that is directly related to expected F 1 score -on a harmonic scale -in a similar way as AUROC is related to expected accuracy. (iv) Furthermore, with Precision-Recall-Gain curves it is possible to calibrate a model for F β in the sense that the predicted score for any instance determines the value of β for which the instance is on the F β decision boundary. (v) We give experimental evidence in Section 4 that this matters by demonstrating that the area under traditional Precision-Recall curves can easily favour models with lower expected F 1 score than others.</p><p>Proofs of the formal results are found in the Supplementary Material; see also http://www.cs. bris.ac.uk/ ˜flach/PRGcurves/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Traditional Precision-Recall Analysis</head><p>Over-abundance of negative examples is a common phenomenon in many subfields of machine learning and data mining, including information retrieval, recommender systems and social network analysis. Indeed, most web pages are irrelevant for most queries, and most links are absent from most networks. Classification accuracy is not a sensible evaluation measure in such situations, as it over-values the always-negative classifier. Neither does adjusting the class imbalance through costsensitive versions of accuracy help, as this will not just downplay the benefit of true negatives but also the cost of false positives. A good solution in this case is to ignore true negatives altogether and use precision, defined as the proportion of true positives among the positive predictions, as performance metric instead of false positive rate. In this context, the true positive rate is usually renamed to recall. More formally, we define precision as prec = TP/(TP + FP) and recall as rec = TP/(TP + FN), where TP, FP and FN denote the number of true positives, false positives and false negatives, respectively.</p><p>Perhaps motivated by the appeal of ROC plots, many researchers have begun to produce Precision-Recall or PR plots with precision on the y-axis against recall on the x-axis. Figure <ref type="figure" target="#fig_0">1</ref> (right) shows the PR curve corresponding to the ROC curve on the left. Clearly there is a one-to-one correspondence between the two plots as both are based on the same contingency tables <ref type="bibr" target="#b1">[2]</ref>. In particular, precision associated with an ROC point is proportional to the angle between the line connecting the point with the origin and the x-axis. However, this is where the similarity ends as PR plots have none of the aforementioned desirable properties of ROC plots.</p><p>Non-universal baselines: a random classifier has precision π and hence baseline performance is a horizontal line which depends on the class distribution. The always-positive classifier is at the right-most end of this baseline (the always-negative classifier has undefined precision). Non-linear interpolation: the main reason for this is that precision in a linearly interpolated contingency table is only a linear combination of the original precision values if the two classifiers have the same predicted positive rate (which is impossible if the two contingency tables arise from different decision thresholds on the same model). <ref type="bibr" target="#b1">[2]</ref> discusses this further and also gives an interpolation formula. More generally, it isn't meaningful to take the arithmetic average of precision values. Non-convex Pareto front: the set of non-dominated operating points continues to be well-defined (see the red circles in Figure <ref type="figure" target="#fig_0">1</ref> (right)) but in the absence of linear interpolation this set isn't convex for PR curves, nor is it straightforward to determine by visual inspection.</p><p>Uninterpretable area: although many authors report the area under the PR curve (AUPR) it doesn't have a meaningful interpretation beyond the geometric one of expected precision when uniformly varying the recall (and even then the use of the arithmetic average cannot be justified). Furthermore, PR plots have unachievable regions at the lower right-hand side, the size of which depends on the class distribution <ref type="bibr" target="#b0">[1]</ref>. No calibration: although some results exist regarding the relationship between calibrated scores and F 1 score (more about this below) these are unrelated to the PR curve. To the best of our knowledge there is no published procedure to output scores that are calibrated for F βthat is, which give the value of β for which the instance is on the F β decision boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The F β measure</head><p>The standard way to combine precision and recall into a single performance measure is through the F 1 score <ref type="bibr" target="#b15">[16]</ref>. It is commonly defined as the harmonic mean of precision and recall:</p><formula xml:id="formula_1">F 1 2 1/prec + 1/rec = 2prec • rec prec + rec = TP TP + (FP + FN)/2<label>(1)</label></formula><p>The last form demonstrates that the harmonic mean is natural here as it corresponds to taking the arithmetic mean of the numbers of false positives and false negatives. Another way to understand the F 1 score is as the accuracy in a modified contingency table which copies the true positive count to the true negatives:</p><formula xml:id="formula_2">Predicted ⊕ Predicted Actual ⊕ TP FN Pos Actual FP TP Neg -(TN -TP) TP + FP Pos 2TP + FP + FN</formula><p>We can take a weighted harmonic mean which is commonly parametrised as follows:</p><formula xml:id="formula_3">F β 1 1 1+β 2 /prec + β 2 1+β 2 /rec = (1 + β 2 )TP (1 + β 2 )TP + FP + β 2 FN (2)</formula><p>There is a range of recent papers studying the F-score, several of which in last year's NIPS conference <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. Relevant results include the following: (i) non-decomposability of the F β score, meaning it is not an average over instances (it is a ratio of such averages, called a pseudo-linear function by <ref type="bibr" target="#b11">[12]</ref>); (ii) estimators exist that are consistent: i.e., they are unbiased in the limit <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>;</p><p>(iii) given a model, operating points that are optimal for F β can be achieved by thresholding the model's scores <ref type="bibr" target="#b17">[18]</ref>; (iv) a classifier yielding perfectly calibrated posterior probabilities has the property that the optimal threshold for F 1 is half the optimal F 1 at that point (first proved by <ref type="bibr" target="#b19">[20]</ref> and later by <ref type="bibr" target="#b9">[10]</ref>, while generalised to F β by <ref type="bibr" target="#b8">[9]</ref>). The latter results tell us that optimal thresholds for F β are lower than optimal thresholds for accuracy (or equal only in the case of the perfect model). They don't, however, tell us how to find such thresholds other than by tuning (and <ref type="bibr" target="#b11">[12]</ref> propose a method inspired by cost-sensitive classification). The analysis in the next section significantly extends these results by demonstrating how we can identify all F β -optimal thresholds for any β in a single calibration procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Precision-Recall-Gain Curves</head><p>In this section we demonstrate how Precision-Recall analysis can be adapted to inherit all the benefits of ROC analysis. While technically straightforward, the implications of our results are far-reaching. For example, even something as seemingly innocuous as reporting the arithmetic average of F 1 values over cross-validation folds is methodologically misguided: we will define the corresponding performance measure that can safely be averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline</head><p>A random classifier that predicts positive with probability p has F β score (1 + β 2 )pπ/(p + β 2 π). This is monotonically increasing in p ∈ [0, 1] hence reaches its maximum for p = 1, the always- positive classifier. Hence Precision-Recall analysis differs from classification accuracy in that the baseline to beat is the always-positive classifier rather than any random classifier. This baseline has prec = π and rec = 1, and it is easily seen that any model with prec &lt; π or rec &lt; π loses against this baseline. Hence it makes sense to consider only precision and recall values in the interval [π, 1]. Any real-valued variable x ∈ [min, max] can be rescaled by the mapping x → x-min max-min . However, the linear scale is inappropriate here and we should use a harmonic scale instead, hence map to</p><formula xml:id="formula_4">1/x -1/min 1/max -1/min = max • (x -min) (max -min) • x<label>(3)</label></formula><p>Taking max = 1 and min = π we arrive at the following definition. Definition 1 (Precision Gain and Recall Gain).</p><formula xml:id="formula_5">precG = prec -π (1 -π)prec = 1 - π 1 -π FP TP recG = rec -π (1 -π)rec = 1 - π 1 -π FN TP<label>(4)</label></formula><p>A Precision-Recall-Gain curve plots Precision Gain on the y-axis against Recall Gain on the x-axis in the unit square (i.e., negative gains are ignored).</p><p>An example PRG curve is given in Figure <ref type="figure" target="#fig_1">2</ref> (right). The always-positive classifier has recG = 1 and precG = 0 and hence gets plotted in the lower right-hand corner of Precision-Recall-Gain space, regardless of the class distribution. Since we show in the next section that F 1 isometrics have slope -1 in this space it follows that all classifiers with baseline F 1 performance end up on the minor diagonal in Precision-Recall-Gain space. In contrast, the corresponding F 1 isometric in PR space is hyperbolic (Figure <ref type="figure" target="#fig_1">2</ref> (left)) and its exact location depends on the class distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Linearity and optimality</head><p>One of the main benefits of PRG space is that it allows linear interpolation. This manifests itself in two ways: any point on a straight line between two endpoints is achievable by random choice between the endpoints (Theorem 1) and F β isometrics are straight lines with slope -β 2 (Theorem 2). Theorem 1. Let P 1 = (precG 1 , recG 1 ) and P 2 = (precG 2 , recG 2 ) be points in the Precision-Recall-Gain space representing the performance of Models 1 and 2 with contingency tables C 1 and C 2 .</p><p>Then a model with an interpolated contingency table</p><formula xml:id="formula_6">C * = λC 1 + (1 -λ )C 2 has precision gain precG * = µprecG 1 + (1 -µ)precG 2 and recall gain recG * = µrecG 1 + (1 -µ)recG 2 , where µ = λ T P 1 /(λ T P 1 + (1 -λ )T P 2 ). Theorem 2. precG + β 2 recG = (1 + β 2 )FG β , with FG β = F β -π (1-π)F β = 1 -π 1-π FP+β 2 FN (1+β 2 )TP .</formula><p>FG β is a linearised version of F β in the same way as precG and recG are linearised versions of precision and recall. FG β measures the gain in performance (on a linear scale) relative to a classifier with both precision and recall -and hence F β -equal to π. F 1 isometrics are indicated in Figure <ref type="figure" target="#fig_1">2 (right)</ref>. By increasing (decreasing) β 2 these lines of constant F β become steeper (flatter) and hence we are putting more emphasis on recall (precision).</p><p>With regard to optimality, we already knew that every classifier or threshold optimal for F β for some β 2 is optimal for acc c for some c. The reverse also holds, except for the ROC convex hull points below the baseline (e.g., the always-negative classifier). Due to linearity the PRG Pareto front is convex and easily constructed by visual inspection. We will see in Section 3.4 that these segments of the PRG convex hull can be used to obtain classifier scores specifically calibrated for F-scores, thereby pre-empting the need for any more threshold tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Area</head><p>Define the area under the Precision-Recall-Gain curve as AUPRG = 1 0 precG d recG. We will show how this area can be related to an expected FG 1 score when averaging over the operating points on the curve in a particular way. To this end we define ∆ = recG/π -precG/(1π), which expresses the extent to which recall exceeds precision (reweighting by π and 1π guarantees that ∆ is monotonically increasing when changing the threshold towards having more positive predictions, as shown in the proof of Theorem 3 in the Supplementary Material). Hence, -y 0 /(1-π) ≤ ∆ ≤ 1/π, where y 0 denotes the precision gain at the operating point where recall gain is zero. The following theorem shows that if the operating points are chosen such that ∆ is uniformly distributed in this range, then the expected FG 1 can be calculated from the area under the Precision-Recall-Gain curve (the Supplementary Material proves a more general result for expected FG β .) This justifies the use of AUPRG as a performance metric without fixing the classifier's operating point in advance. Theorem 3. Let the operating points of a model with area under the Precision-Recall-Gain curve AUPRG be chosen such that ∆ is uniformly distributed within [-y 0 /1π, 1/π]. Then the expected FG 1 score is equal to</p><formula xml:id="formula_7">E [FG 1 ] = AUPRG/2 + 1/4 -π(1 -y 0 2 )/4 1 -π(1 -y 0 )<label>(5)</label></formula><p>The expected reciprocal F 1 score can be calculated from the relationship</p><formula xml:id="formula_8">E [1/F 1 ] = (1 -(1 - π)E [FG 1 ]</formula><p>)/π which follows from the definition of FG β . In the special case where y 0 = 1 the expected FG 1 score is AUPRG/2 + 1/4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Calibration</head><p>Figure <ref type="figure" target="#fig_2">3</ref> (left) shows an ROC curve with empirically calibrated posterior probabilities obtained by isotonic regression <ref type="bibr" target="#b18">[19]</ref> or the ROC convex hull <ref type="bibr" target="#b3">[4]</ref>. Segments of the convex hull are labelled with the value of c for which the two endpoints have the same skew-sensitive accuracy acc c . Conversely, if a point connects two segments with c 1 &lt; c 2 then that point is optimal for any c such that c 1 &lt; c &lt; c 2 . The calibrated values c are derived from the ROC slope r by c = πr/(πr + (1π)) <ref type="bibr" target="#b5">[6]</ref>.</p><p>For example, the point on the convex hull two steps up from the origin optimises skew-sensitive accuracy acc c for 0.29 &lt; c &lt; 0.75 and hence also standard accuracy (c = 1/2). We are now in a position to calculate similarly calibrated scores for F-score. Theorem 4. Let two classifiers be such that prec 1 &gt; prec 2 and rec 1 &lt; rec 2 , then these two classifiers have the same F β score if and only if</p><formula xml:id="formula_9">β 2 = - 1/prec 1 -1/prec 2 1/rec 1 -1/rec 2<label>(6)</label></formula><p>In line with ROC calibration we convert these slopes into a calibrated score between 0 and 1:</p><formula xml:id="formula_10">d = 1 (β 2 + 1) = 1/rec 1 -1/rec 2 (1/rec 1 -1/rec 2 ) -(1/prec 1 -1/prec 2 )<label>(7)</label></formula><p>It is important to note that there is no model-independent relationship between ROC-calibrated scores and PRG-calibrated scores, so we cannot derive d from c. However, we can equip a model with two calibration maps, one for accuracy and the other for F-score. Figure <ref type="figure" target="#fig_2">3</ref> (right) shows the PRG curve for the running example with scores calibrated for F β . Score 0.76 corresponds to β 2 = (1 -0.76)/0.76 = 0.32 and score 0.49 corresponds to β 2 = 1.04, so the point closest to the Precision-Recall breakeven line optimises F β for 0.32 &lt; β 2 &lt; 1.04 and hence also F 1 (but note that the next point to the right on the convex hull is nearly as good for F 1 , on account of the connecting line segment having a calibrated score close to 1/2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Practical examples</head><p>The key message of this paper is that precision, recall and F-score are expressed on a harmonic scale and hence any kind of arithmetic average of these quantities is methodologically wrong. We now demonstrate that this matters in practice. In particular, we show that in some sense, AUPR and AUPRG are as different from each other as AUPR and AUROC. Using the OpenML platform <ref type="bibr" target="#b16">[17]</ref> we took all those binary classification tasks which have 10-fold cross-validated predictions using at least 30 models from different learning methods (these are called flows in OpenML). In each of the obtained 886 tasks (covering 426 different datasets) we applied the following procedure. First, we fetched the predicted scores of 30 randomly selected models from different flows and calculated areas under ROC, PRG and PR curves(with hyperbolic interpolation as recommended by <ref type="bibr" target="#b1">[2]</ref>), with minority class as positives. We then ranked the 30 models with respect to these measures. Figure <ref type="figure">4</ref> plots AUPRG-rank against AUPR-rank across all 25 980 models.</p><p>Figure <ref type="figure">4</ref> (left) demonstrates that AUPR and AUPRG often disagree in ranking the models. In particular, they disagree on the best method in 24% of the tasks and on the top three methods in 58% of the tasks (i.e., they agree on top, second and third method in 42% of the tasks). This amount of disagreement is comparable to the disagreement between AUPR and AUROC (29% and 65% disagreement for top 1 and top 3, respectively) and between AUPRG and AUROC (22% and 57%). Therefore, AUPR, AUPRG and AUROC are related quantities, but still all significantly different. The same conclusion is supported by the pairwise correlations between the ranks across all tasks: the correlation between AUPR-ranks and AUPRG-ranks is 0.95, between AUPR and AUROC it is 0.95, and between AUPRG and AUROC it is 0.96. Figure <ref type="figure">4</ref> (right) shows AUPRG vs AUPR in two datasets with relatively low and high rank correlations (0.944 and 0.991, selected as lower and upper quartiles among all tasks). In both datasets AUPR and AUPRG agree on the best model. However, in the white-clover dataset the second best is AdaBoost according to AUPRG and Logistic Regression according to AUPR. As seen in Figure <ref type="figure">5</ref>, this disagreement is caused by AUPR taking into account the poor performance of AdaBoost in the early part of the ranking; AUPRG ignores this part as it has negative recall gain.   </p><formula xml:id="formula_11">• • • • • • • • • • • • • • • • • • • • • • • • • • • • • 0.00<label>0</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding remarks</head><p>If a practitioner using PR-analysis and the F-score should take one methodological recommendation from this paper, it is to use the F-Gain score instead to make sure baselines are taken into account properly and averaging is done on the appropriate scale. If required the FG β score can be converted back to an F β score at the end. The second recommendation is to use Precision-Recall-Gain curves instead of PR curves, and the third to use AUPRG which is easier to calculate than AUPR due to linear interpolation, has a proper interpretation as an expected F-Gain score and allows performance assessment over a range of operating points. To assist practitioners we have made R, Matlab and Java code to calculate AUPRG and PRG curves available at http://www.cs.bris.ac.uk/ ˜flach/PRGcurves/. We are also working on closer integration of AUPRG as an evaluation metric in OpenML and performance visualisation platforms such as ViperCharts <ref type="bibr" target="#b14">[15]</ref>.</p><p>As future work we mention the interpretation of AUPRG as a measure of ranking performance: we are working on an interpretation which gives non-uniform weights to the positives and as such is related to Discounted Cumulative Gain. A second line of research involves the use of cost curves for the FG β score and associated threshold choice methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (left) ROC curve with non-dominated points (red circles) and convex hull (red dotted line). (right) Corresponding Precision-Recall curve with non-dominated points (red circles).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (left) Conventional PR curve with hyperbolic F 1 isometrics (dotted lines) and the baseline performance by the always-positive classifier (solid hyperbole). (right) Precision-Recall-Gain curve with minor diagonal as baseline, parallel F 1 isometrics and a convex Pareto front.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: (left) ROC curve with scores empirically calibrated for accuracy. The green dots correspond to a regular grid in Precision-Recall-Gain space. (right) Precision-Recall-Gain curve with scores calibrated for F β . The green dots correspond to a regular grid in ROC space, clearly indicating that ROC analysis over-emphasises the high-recall region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: (left) Comparison of AUPRG-ranks vs AUPR-ranks. Each cell shows how many models across 886 OpenML tasks have these ranks among the 30 models in the same task. (right) Comparison of AUPRG vs AUPR in OpenML tasks with IDs 3872 (white-clover) and 3896 (ada-agnostic), with 30 models in each task. Some models perform worse than random (AUPRG &lt; 0) and are not plotted. The models represented by the two encircled triangles are shown in detail in Figure 5.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the REFRAME project granted by the European Coordinated Research on Long-Term Challenges in Information and Communication Sciences &amp; Technologies ERA-Net (CHIST-ERA), and funded by the Engineering and Physical Sciences Research Council in the UK under grant EP/K018728/1. Discussions with Hendrik Blockeel helped to clarify the intuitions underlying this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unachievable region in precision-recall space and its effect on empirical evaluation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">349</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and ROC curves</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PAV and the ROC convex hull</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The geometry of ROC space: understanding machine learning metrics through ROC isometrics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ROC analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Sammut</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Webb</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="869" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple generalisation of the area under the ROC curve for multiple class classification problems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Till</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="171" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified view of performance metrics: Translating threshold choice into expected classification loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2813" to="2869" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Consistent binary classification with generalized performance metrics</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">O</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2744" to="2752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal thresholding of classifiers to maximize F1 measure</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Naryanaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8725</biblScope>
			<biblScope unit="page" from="225" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the statistical consistency of plug-in classifiers for non-decomposable performance measures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1493" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimizing F-measures by cost-sensitive classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Parambath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2123" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust classification for imprecise environments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="231" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vipercharts: Visual performance evaluation platform</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sluban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Nijssen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Železný</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8190</biblScope>
			<biblScope unit="page" from="650" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval. Butterworth-Heinemann</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Newton, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">OpenML: networked science in machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimizing F-measures: A tale of two approaches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M A</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning (ICML</title>
		<meeting>the Eighteenth International Conference on Machine Learning (ICML</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond Fano&apos;s inequality: bounds on the optimal F-score, BER, and cost-sensitive risk and their implications</title>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Edakunni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pocock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1033" to="1090" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
