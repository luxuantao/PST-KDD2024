<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Parsing and Recognition of Hand-Sketched Diagrams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Burak</forename><surname>Levent</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mechanical Engineering Department Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><surname>Kara</surname></persName>
							<email>lkara@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Mechanical Engineering Department Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Stahovich</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mechanical Engineering Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92521</postCode>
									<settlement>Riverside Riverside</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Parsing and Recognition of Hand-Sketched Diagrams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">91DCD176D1565EB0836A660C3E9F53C0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sketch understanding</term>
					<term>pen computing</term>
					<term>symbol recognition</term>
					<term>visual parsing</term>
					<term>sketch editing</term>
					<term>SimuSketch</term>
					<term>Simulink</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A long standing challenge in pen-based computer interaction is the ability to make sense of informal sketches. A main difficulty lies in reliably extracting and recognizing the intended set of visual objects from a continuous stream of pen strokes. Existing pen-based systems either avoid these issues altogether, thus resulting in the equivalent of a drawing program, or rely on algorithms that are too constraining to be effective for the average user. As one step toward alleviating these difficulties, we present an integrated sketch parsing and recognition approach designed to enable natural, fluid sketch-based computer interaction. Our approach is based on a hierarchical mark-group-recognize architecture. First, the stream of pen strokes is examined to identify certain delimiter patterns called "markers." These markers then anchor a spatial analysis which groups the uninterpreted strokes into distinct clusters, each representing a single visual object. Finally, a trainable shape recognizer, which is informed by the spatial analysis, is used to find the best interpretations of the clusters. Based on these concepts, we have built SimuSketch, a sketch-based interface for Matlab's Simulink software package. An evaluation of SimuSketch has indicated that even novice users can effectively utilize our system to solve real engineering problems without having to know much about the underlying recognition techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Pen-based computer interaction is becoming increasingly ubiquitous as evidenced by the growing interest in Tablet PC's, electronic whiteboards and PDA's. Many of these devices now come equipped with robust handwriting recognition, making them an attractive alternative to the keyboard and mouse for text entry. However, when it comes to graphical input, such as sketches and diagrams, such devices either leave the pen strokes uninterpreted, or offer only limited support in the form of stroke beautification or simple shape recognition.</p><p>We believe that among the many issues that remain to be solved, there are two particular challenges that hinder the development of robust sketch understanding systems. The first is ink parsing, the task of grouping a user's pen strokes into clusters representing intended symbols, without requiring the user to indicate when one symbol ends and the next one begins. However, this is a difficult problem as the strokes can be grouped in many different ways, and moreover, the number of stroke groups to consider increases exponentially with the number of strokes. The combinatorics thus clearly render approaches based on exhaustive search infeasible. To alleviate this difficulty, many of the current systems require the user to explicitly indicate the intended partitioning of the ink. This is often done by pressing a button on the stylus or more commonly by pausing between symbols <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. Alternatively, some systems avoid parsing by requiring each object to be drawn in a single pen stroke <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18]</ref>. However, such constraints usually result in a less than natural drawing environment.</p><p>The second issue is symbol recognition, the task of recognizing individual hand drawn figures such as geometric shapes, glyphs and symbols. While there has been significant recent progress in symbol recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref>, many recognizers are either hand-coded or require large sets of training data to reliably learn new symbol definitions. Such issues make it difficult to extend these systems to new domains with novel shapes and symbols. Additionally most symbol recognizers have been built as stand alone applications without addressing the issue of integration into high-level sketch understanding systems.</p><p>In this paper, we address the issue of parsing and recognition of hand-drawn sketches in the domain of network diagrams. The particular techniques we present are well-suited to signal flow diagrams, organizational charts, algorithmic flowcharts, and to various graphical models such as finite state machines, Markov models and Petri nets.</p><p>Our approach is based on a hierarchical mark-group-recognize architecture shown in Figure <ref type="figure" target="#fig_0">1</ref>. The first step focuses on the identification of "markers," delimiter symbols that are easily and reliably extracted from a continuous stream of input. In the domain of network diagrams, the arrows connecting the nodes of a network serve as useful markers. These markers are then used to efficiently cluster the remaining strokes into distinct groups corresponding to individual symbols. The key here is that stroke clustering is driven exclusively by the marker symbols identified in the first step, without need for search. Next, informed by the result of the clustering algorithm, our approach employs contextual knowledge to generate a set of candidate interpretations for each of the stroke groups. The groups are then evaluated using a symbol recognizer to determine which of these interpretations is correct. The key advantage of our recognizer is that it can learn new symbol definitions from single prototype examples thus allowing users to easily customize the system to their unique styles. The underlying image-based pattern recognition techniques allows our recognizer to be applicable to multiple-stroke symbols without restricting the order in which the strokes are drawn. In cases of misrecognitions, the last step involves error correction where the user rectifies the sketch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OVERVIEW</head><p>To provide a test bed for our work, we have created Simu-Sketch; a prototype sketch-based front-end to Matlab's Simulink package (Figure <ref type="figure" target="#fig_1">2</ref>). Simulink is used for analyzing feedback control systems and other similar dynamic systems. It has a typical drag and drop interface in which the user navigates through a nested symbol palette to find, select and drag the components one at a time onto an empty canvas. With SimuSketch, on the other hand, the user can construct functional Simulink models by simply sketching them on a computer screen. The sketch interface does not restrict the order in which the symbols must be drawn or the number of strokes used to draw a symbol. Furthermore, it does not require the user to indicate when one symbol ends and the next one begins, nor to complete one symbol before moving onto another. Hence, the user may come back to a previous location to add more strokes. The objects interpreted by SimuSketch are live from the moment they are recognized, thus enabling users to interact with them. For example users can edit the objects through dialog boxes or alter their sketch using traditional means such as selection and deletion. Once the user's model is recognized, a simulation can be run and viewed directly in SimuSketch. At the end, users can save their work either in their original sketchy form or in a format compatible with Matlab, thus allowing users to resume their work either in the SimuSketch or the conventional Matlab environments.</p><p>In the next section, we present a survey of previous research on sketch-based systems with an emphasis on parsing and recognition approaches. Further detail about interaction with SimuSketch and the underlying parsing and recognition techniques are detailed in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Inspired by the advances in speech recognition, some systems facilitate parsing by requiring visual objects to be drawn with a predefined sequence of pen strokes <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref>. While useful at reducing computational complexity, the strong temporal dependency in these methods forces the user to remember the correct order in which the strokes must be drawn. The nature of these approaches thus makes them more suitable to handwriting recognition rather than sketch recognition. Other approaches employ constrained search methods, where the idea is to generate a multitude of partial interpre-tations from the strokes and later support or refute these interpretations based on new evidence <ref type="bibr" target="#b13">[14]</ref>. Such approaches are often faced with the difficulty of non-optimal thresholds that either prematurely terminate a promising path, or retain a futile one for too long. Alvarado and Davis <ref type="bibr" target="#b2">[3]</ref>, on the other hand, propose an extension to this idea in the form of Probabilistic Relational Models but have not yet presented formal evaluations.</p><p>A number of techniques have been devised for parsing and recognition in visual scenes. Shilman et.al. <ref type="bibr" target="#b31">[32]</ref> present a statistical visual language model for ink parsing. During training, a number of spatial relationships between objects are used to construct the object models. During recognition, the models are matched against the users' strokes using a Bayesian framework. Their approach requires a description of a visual grammar which is currently encoded manually. The trainable parser, on the other hand, requires a large number of training examples. Costagliola and Deufemia <ref type="bibr" target="#b7">[8]</ref> present an approach based on LR parsing for the construction of visual language editors. They employ "extended positional grammars" to encode the attributes of the graphical objects and present a set of production/reduction rules for the grammar. Saund et.al. <ref type="bibr" target="#b29">[30]</ref> present a system that uses Gestalt principles to determine the salient objects represented in a line drawing. Their work only concerns the grouping of the strokes and does not employ recognition to verify whether the identified groups are in fact the intended ones. Jacobs <ref type="bibr" target="#b16">[17]</ref> describes a system to recognize objects with straightline perimeter representations. The system uses a number of heuristic rules to group edges that likely come from a single object, and then uses simple recognizers to identify the objects represented by the edges. However the rules rely on the presence of straight line segments and sharp corners, and thus are not well-suited to less structured patterns such as sketches.</p><p>A number of systems that support sketch-based interaction have been developed in recent years. For user interface design, Landay and Myers <ref type="bibr" target="#b20">[21]</ref> present an interactive sketching tool called SILK that allows designers to quickly sketch out a user interface and transform it into a fully operational system. Hong and Landay <ref type="bibr" target="#b14">[15]</ref> describe a program called SATIN designed to support the creation of pen-based applications. Lin et al <ref type="bibr" target="#b22">[23]</ref> describe a program called DENIM that helps web site designers in the early stages of the design process. All three programs use Rubine's single-stroke gesture recognizer <ref type="bibr" target="#b27">[28]</ref> as their main recognition tool and are thus not concerned with parsing. Alvarado and Davis <ref type="bibr" target="#b3">[4]</ref> describe a system that can interpret and simulate a variety of simple, hand drawn mechanical systems. The system uses a number of heuristics to construct a recognition graph containing the likely interpretations of the sketch. The best interpretation is chosen using a scoring scheme that uses both contextual information and user feedback. In their approach, each time a new stroke is entered, the entire recognition tree is updated. By contrast, we allow recognition to be controlled by the user. Also, their shape recognizers are sensitive to the results of segmentation (i.e., fitting line and arc segments to the raw ink) forcing the user to be cautious during sketching. Our approach does not rely on segmentation, thus allowing for more casual drawing styles.</p><p>Matsakis <ref type="bibr" target="#b24">[25]</ref> describes a system for recognizing handwritten mathematical expressions. The work presents an interesting idea based on minimum-spanning trees used for uncovering the spatial structure of the expressions. However the approach requires a large amount of training samples to learn new symbols and each training sample needs to be drawn using the same number of strokes in the same direction and order. Similarly, recognition is sensitive to the number of strokes and order. Kurtoglu and Stahovich <ref type="bibr" target="#b18">[19]</ref> describe a program that augments sketch understanding with qualitative physical reasoning to understand schematic sketches of physical devices. One key feature of their system is that it allows users to incorporate shapes from several different domains instead of limiting them to one particular domain.</p><p>In the field of shape recognition, some methods either rely on single stroke methods in which an entire symbol must be drawn in a single stroke <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref>, or constant drawing order methods in which two similarly shaped patterns are considered different unless the pen strokes leading to those shapes follow the same sequence <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>. Systems such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> allow for multiple stroke symbols however the recognizers are manually coded. While trainable, systems such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16]</ref> typically require a multitude of training examples. By contrast, we present a multiple stroke symbol recognizer that can learn definitions from single prototype examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTERACTION WITH SIMUSKETCH</head><p>SimuSketch is deployed on a 9 in x 12 in Wacom Cintiq digitizing tablet with a cordless stylus. The drawing surface of the tablet is an LCD display, which enables users to see virtual ink directly under the stylus. Data points are collected as time sequenced (x,y) coordinates sampled along the stylus' trajectory. As shown in Figure <ref type="figure" target="#fig_1">2</ref>-top, SimuSketch's interface consists of a drawing region and a toolbar that contains buttons for commonly used commands.</p><p>The user draws as he or she ordinarily would on paper. As the user is drawing, SimuSketch does not attempt to interpret the scene. Instead, it employs a recognize on demand (ROD) strategy in which the user taps the "Recognize" button in the toolbar whenever he wants the scene to be interpreted. This command invokes the sketch recognition engine which then parses the current sketch, recognizes the objects, and produces a Simulink model. As shown Figure <ref type="figure" target="#fig_1">2</ref>-top, the program demonstrates its understanding by displaying a faint bounding box around each object, along with a text label indicating what the object is. Recognized arrows are delineated with small colored points at each of their two ends.</p><p>The ROD strategy has a number of advantages over the systems that try to interpret the scene after each input stroke. First, as the users are not distracted by display of potentially premature interpretation results, they can focus exclusively on sketching. Second, as very little internal processing takes place after each stroke, the program is better able to keep up with users' pace<ref type="foot" target="#foot_0">1</ref> . Third, by delaying recognition in a user controlled manner, it allows the system to acquire more context that would help improve the recognition accuracy of earlier strokes. Note that ROD does not require the model to be entirely completed before it can be used. In fact, it encourages an iterative construction process in which the user draws a portion of the final model, asks SimuSketch to recognize it, tests the model, and continues with the rest of the model.</p><p>Once the sketch is recognized, the user can run a simulation of it by pressing the "Simulate" button. This command simply hands the model over to Simulink (which runs in the background) for processing. The results of the simulation can be viewed directly in the sketch interface by double tapping on the Scope blocks. As shown in the right part of Object Manipulation: SimuSketch offers a number of gestures for different tasks. To select an object or an arrow, the user either taps on it or circles it with the stylus; the selected item is highlighted in a translucent blue color indicating its selection. The circular selection gesture is differentiated from a drawing stroke based on its end points and the region it encircles. If the distance between the stroke's first and last points is less than 10% of the total stroke length (i.e., the stroke forms a nearly closed contour) and the stroke encircles one or more objects or arrows, the stroke is taken as a selection gesture. Once an object is selected, one of four things can happen depending on the subsequent input stroke. First, if the stroke is simply a quick tap in the blue region, a pop dialog message is dispatched which brings up a dialog box pertinent to the selected object. Second, if the stroke is not a tap but its initial contact point is still within the blue region, a move message is dispatched and the selected object(s) is moved to the lift point of the stroke. Third, if both the contact and lift points of the stroke are outside the blue region but the midpoint is in the blue region, a delete message is dispatched and the object is removed from the visual scene. A typical manifestation of this gesture is a stroke through the selected object. Finally, if the entirety of the stroke is outside the blue region, all selected objects are de-selected and the stroke is added to the raw sketch. An alternative to deselection is a tap in the white space.</p><p>Object Dialogs: For objects with variable parameters, selecting and tapping on the object brings up a dialog box for editing its parameters. The left part of Figure <ref type="figure" target="#fig_2">3</ref> shows an example. Interaction in these dialog boxes is also sketch-based in that users can cross out the old value with a delete gesture (a stroke through the number) and simply write in the new value. The program can recognize negative and/or decimal numbers using a digit recognizer we have developed.</p><p>Views: Once the user's sketch has been interpreted, the user has the option of viewing the model in its sketchy or cleaned up form. In the cleaned up view, the sketchy symbols are replaced by their iconic forms and the arrows are straightened out into line segments. Users can toggle between these two views by tapping the "Toggle view" button. Subjects in our user studies have indicated that the informality of the sketchy view gave a sense of freedom and creativity, while the cleaned up view gave a sense of completeness and definiteness. Despite these perceived differences, the cleaned up view is just as functional as the sketchy view in that it supports the same interaction mechanisms including sketching, object selection, object manipulation and editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNDER THE HOOD</head><p>In the following sections, we detail each of the steps of our multi-level parsing and recognition approach outlined in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary Recognition</head><p>One key to successful sketch understanding lies in the ability to establish the ground truths about the sketch early on, before costly mistakes take place. Our approach is based on the use of "marker symbols," symbols that are easy to recognize and that can guide the interpretation of the remainder of the sketch. This approach is similar in spirit to the construction of "islands of certainty" in the Hearsay-II speech understanding system <ref type="bibr" target="#b9">[10]</ref>.</p><p>There are several characteristics that distinguish a good marker symbol. First, a good marker occurs relatively frequently in the sketch, thus providing good resolution for separating the other symbols. Second, a good marker should have unique geometric and kinematic (e.g., pen speed) features that allow it to be reliably extracted from the input stream. Third, a good marker should help guide the interpretation of the other symbols in the sketch, for example, by narrowing down the set of possible interpretations. In the domain of data flow diagrams such as Simulink, we have found arrows to be suitable markers. SimuSketch thus begins by recognizing the arrows.</p><p>Our observational tests on a small set of users during the design stages of our system indicated that, despite some exceptions, arrows were usually drawn as either a single pen stroke or two consecutive strokes, one for the shaft and one for the head. We thus developed two types of arrow recognizers to account for these two main styles. To simplify our analysis, we require that both types of arrows be drawn from tail to head. Here we only describe the single-stroke arrow recognizer, as the two-stroke recognizer is a minor extension of  Once these points are determined, a series of geometric tests is performed to determine whether or not the stroke really is an arrow. We require the four angles ABC, BCD, RAB and RAD to all be less than 90 • , and the length of line segments BC and DC to be less than 20% of the total stroke length. These geometric tests were designed empirically by collecting a corpus of positive and negative examples of arrows from several users, and experimenting with different levels of specificity and thresholds until the best classification performance was obtained. With the resulting recognizer, a variety of arrow shapes with different arrowhead styles can be recognized as shown in Figure <ref type="figure" target="#fig_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stroke Clustering</head><p>Once the arrows have been recognized, the next step is to group the remaining strokes into different clusters, representing different symbols. The key idea behind stroke clustering is that strokes are deemed to belong to the same symbol only when they are spatially proximate. The challenge is reli- ably determining when two pen strokes should be considered close together. Here, we rely on the arrows to help make this determination. In data flow diagrams, each arrow typically connects a source object at its tail to a target object at its head. Hence, different clusters can be identified by grouping together all the strokes that are near the end of a given arrow.</p><p>In effect, two strokes are considered spatially proximate if the nearest arrow is the same for each. Based on this observation, we developed the following procedure for identifying symbol clusters:</p><p>Step-1 Assign each non-arrow stroke to the nearest arrow: Stroke clustering begins by assigning each non-arrow stroke to the nearest arrow (Figure <ref type="figure" target="#fig_6">6a</ref>). The distance between a stroke and an arrow is defined to be the Euclidean distance between the median point of the stroke and either the head or tail of the arrow, whichever is closer. The head is taken to be the apex, which is shown as point C in Figure <ref type="figure" target="#fig_4">4</ref>.</p><p>Step-2 Combine strokes into clusters: Strokes assigned to the same arrow end in Step-1 are grouped to form a stroke cluster. These clusters will form the basis of the symbols. Figure <ref type="figure" target="#fig_6">6b</ref> shows the results of this step.</p><p>Step-3 Merge overlapping clusters: Next, clusters with partially or fully overlapping bounding boxes are merged. The bounding box of a cluster is the minimum sized rectangle that fully encloses the constituent strokes. As shown in Figure <ref type="figure" target="#fig_6">6c</ref>, this process combines strokes that are part of the same symbol but which were initially assigned to different arrows in Step-1. If bounding boxes of different symbols overlap, this process could erroneously merge the symbols. However, in our experience we have found that users rarely draw in such a way that this happens. Thus, at the completion of this step, each cluster is assumed to be a distinct symbol.</p><note type="other">Sine Wave Sum Switch</note><p>Step-4 Connect empty arrowhead/tails to the nearest cluster:</p><p>Step-1 guarantees that each non-arrow stroke is attached to the nearest arrow end. However, some of the arrow ends might remain devoid of any strokes. In this step, empty arrow ends are linked to the nearest stroke cluster (Figure <ref type="figure" target="#fig_6">6d</ref>). This step helps to ensure the intended connectivity of the diagram by ensuring that each arrow has a cluster at its tail and head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating Symbol Candidates</head><p>After identifying the stroke clusters, the next step is to recognize the symbols they represent. Our approach combines contextual knowledge with shape recognition to achieve accuracy and efficiency. In particular, we examine the number of input and output arrows associated with each stroke cluster to help constrain its possible interpretations. For example, function generators such as the Sine Wave can have only output terminals and therefore must have only outgoing arrows. Likewise, certain symbols can have only input terminals, such as the Scope block, or may have an arbitrary number of input and output terminals such as the Sum block.</p><p>By examining the number of input and output arrows for a given cluster, SimuSketch identifies a set of candidate symbols for the cluster. This reduces the amount of work the subsequent shape recognizer must do and additionally helps increase accuracy by reducing the possibilities for confusion. For example, while the Sum block and the Clock look quite similar (the two circular symbols in Figure <ref type="figure" target="#fig_6">6</ref>), context dictates that a Sum block must have at least two incoming arrows while the Clock must have none. With this additional knowledge, the shape recognizer would never consider the Sum block and the Clock as two competing candidates during shape recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol Recognition</head><p>We have developed a novel image-based symbol recognizer that can recognize shapes independent of their position, size and orientation<ref type="foot" target="#foot_1">2</ref> . However, it is sensitive to non-uniform scaling, and thus we can distinguish between, say, a square and a rectangle. A distinguishing feature of this recognizer is that it is used for recognizing both the Simulink objects, and the digits in the objects' dialog boxes.</p><p>Input symbols are internally described as 24x24 quantized bitmap images which we call "templates". Figure <ref type="figure" target="#fig_7">7</ref> shows example symbol templates. This representation has a number of desirable characteristics. First, segmentation -the process of decomposing the symbol into constituent primitives such as lines and curves -is eliminated entirely. Second, the representation is well suited for recognizing "sketchy" symbols such as those with heavy overtracing, missing or extra segments and different line styles (solid, dashed, etc.). Lastly, this recognizer puts no restrictions on the number of strokes, or the order in which the strokes are drawn.</p><p>Unlike many traditional methods, our shape recognizer requires only a single prototype example to learn a new symbol definition. Using the "Train New" button in the interface, the user can create a new symbol definition by simply drawing a shape and assigning a name to it. With this approach, users can seamlessly train new symbols or overwrite existing ones on the fly, without having to depart the main application. This feature makes it easy for users to extend and customize their symbol libraries<ref type="foot" target="#foot_2">3</ref> .</p><p>Our recognizer uses an ensemble of four different classifiers to evaluate the match between an unknown symbol and a candidate definition symbol. The classifiers we use are extensions of the following methods: (1) Hausdorff distance <ref type="bibr" target="#b28">[29]</ref>, (2) Modified Hausdorff distance <ref type="bibr" target="#b8">[9]</ref>, (3) Tanimoto coefficient <ref type="bibr" target="#b10">[11]</ref> and (4) Yule coefficient <ref type="bibr" target="#b32">[33]</ref>. The Hausdorff methods reveal the dissimilarity between two templates by measuring the distance between the maximally distant pixels in the two point sets. The Tanimoto coefficient on the other hand reveals the similarity between two templates by measuring the amount of overlapping black pixels. The Yule coefficient is also a similarity measure except it considers the matching white pixels in addition to the matching black pixels. The motivation for using a multiple classifier scheme lies in the pragmatic evidence that, although individual classifiers may not perform perfectly, they usually rank the true definition highly, and tend to misclassify differently <ref type="bibr" target="#b0">[1]</ref>. Hence, by advocating definitions ranked highly by all four classifiers while suppressing those that are not, we can sift the true class more reliably.</p><p>During recognition, each classifier outputs a list of symbol definitions ranked according to their similarity to the unknown. Results of the individual classifiers are then synthesized by first transforming the similarity measures into dissimilarity measures, then normalizing the classifiers' output into a unified scale (to make them compatible), and finally combining the modified outputs of the classifiers. The definition symbol with the best combined score is chosen as the symbol's interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Correction</head><p>Our system provides several means to correct recognition errors when they occur. Our techniques have strong parallels with the mediation techniques presented in <ref type="bibr" target="#b23">[24]</ref>. When an object is misrecognized, the user can repeat the object by se-lecting, deleting and redrawing it. A more direct way is by choosing the correct interpretation from a choice list, which is revealed by bringing the stylus near the misrecognized object and pressing one of the buttons on its side. This list contains only the candidate symbols previously determined using contextual information, and is ranked according to the results of the shape recognizer. Hence, the list is typically short with the correct interpretation usually occurring near the top. Finally, if an arrow goes undetected, and hence becomes part of an object, the user can dictate the correct interpretation by drawing a small circle on or near the stroke. This gesture, which we call the 'o' gesture, explicitly forces the stroke in question to be an arrow. The 'o' gesture is distinguished from a regular drawing stroke based on its absolute size and its two end points. If the gesture fits in a 30 x 30 square on a 1024 x 768 screen, and the stroke forms a closed contour (similar to a selection gesture) without encircling any object, the stroke is interpreted as an 'o' gesture. Once a misrecognized arrow is corrected, SimuSketch automatically rectifies the portion of the sketch that was affected by the missed arrow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USER STUDIES</head><p>We conducted two user studies to evaluate our system. The first study focused on the performance of our symbol recognizer and was conducted with a simple interface designed for this study. The second investigated users' reactions to Simu-Sketch as a pen-based interaction system, and the evaluation was more observational compared to the first study.</p><p>Evaluation of the Symbol Recognizer: Our evaluation of the symbol recognizer consisted of two experiments. In the first experiment, we used a set of 20 graphic symbols shown in Figure <ref type="figure" target="#fig_8">8</ref>. Five users participated in this experiment each of whom were asked to provide three sets of the symbols using the digitizing tablet. In the second experiment, we used digit recognition as our test bed. Nine users participated in the second study and each was asked to provide six sets of digits from "0" to "9". Both experiments were conducted in a user-dependent setting in which the recognizer was evaluated using the user's own training symbols. The last set from each user was used for training while the previous ones were used for testing. Each session involved only data collection; the data was processed at a later time. This approach was chosen to prevent users from adjusting their writing style based on our program's output.</p><p>When the top-one classification performance is considered, the recognition rate from the graphic symbol study was 87%. However when top-two classification performance is considered, i.e., the rate at which the correct class is either the highest or second highest ranked class, the accuracy was 97.5%. We consider the top-two classification performance to be of considerable importance, as it provides a measure of how frequently the correct class will appear in the list of alternatives suggested by our program during error correction.</p><p>For the digit recognition study, the top-one accuracy was 93.8% and the top-two accuracy was 98.0%. State-of-theart hand drawn digit recognition systems achieve recognition rates above 96-97% in user-independent settings <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. We achieve about 94% accuracy in a user-dependent setting. Nevertheless, we consider our approach to be quite attractive given that it works from a single training example. To have a point of comparison, LeCun's neural network recognizer <ref type="bibr" target="#b21">[22]</ref> for handwritten digits, one of the best in its class, uses a total of 60,000 digits for training purposes. As one would expect, if the problem is to recognize digits only, it is better to use a dedicated digit recognizer. However, if the problem involves user defined symbols, such as those shown in Figure <ref type="figure" target="#fig_8">8</ref>, our approach has distinct advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of SimuSketch:</head><p>The focus of this study was to assess the performance of SimuSketch. Among the various aspects that we investigated, we were particularly interested in SimuSketch's ease of use, its parsing and recognition accuracy, users' adaptability to the system, their success at recovering from recognition errors, and their short and long term view of SimuSketch as a practical front-end to Simulink. A total of 14 graduate and undergraduate students -12 engineering and 2 computer science majors -participated in the studies. Nearly half of the users either regularly used Simulink or had previously used it once or twice, while the other half had never used Simulink before. 10 users had no prior experience with the digitizing tablet or the stylus, while 4 users had once used the hardware in a previous study. However, none of the users had previously used SimuSketch, nor had seen it in use by others. Each session lasted approximately 30 to 40 minutes. For those who were not familiar with Simulink, we first described what Simulink is and gave a brief demonstration on its interface. Next, we introduced SimuSketch. Using simple examples, we demonstrated the means for creating a sketch, selecting, deleting and moving objects, editing object properties, correcting recognition errors, running simulations, training new symbols and switching between views. During this period, we elaborated on SimuSketch's arrow recognizer as our experience with the first few users had indicated the arrow recognition to be fragile at times. Particularly, we told the users that only single or two stroke arrows were permitted and both types had to be drawn from a source object toward a target object. Other than the recognition of arrows, no further explanation was given regarding the underlying parsing and recognition algorithms. At the end of this introduction, a brief warm up period of approximately 5 minutes was given to let the users become familiar with the hardware and Simu- Similarly, all users decided to use the pre-trained digit recognizer rather than training their own set of digits. However, in this case we did not provide sample figures of the trained digits. Although no time constraints were set, we encouraged users to complete their tasks in a total of 20 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observations, Evaluations and Discussions</head><p>One consistent pattern among the users was that their encounter with SimuSketch began with great excitement as observed from their reactions during the demo session. This was followed by a period of frustration at the beginning of the warm up period, and finally reaching a favorable equilibrium toward the end of the warm up period and during the actual testing. At the end, all users completed the first task successfully while all but four users completed the second task. In the case of the four users, either the program crashed unexpectedly and they did not have time to redo it, or it was taking too long for them to finish the task.</p><p>The users' main remark about SimuSketch was that it was intuitive and fast to use, and easy to learn. They particularly liked the idea of simply drawing the objects without having to navigate through an object library to find them. Most users  found the interaction mechanisms to be "natural" and "familiar." Many highlighted the ability to quickly train a custom set of symbols as an outstanding attribute, although they did not make use of it.</p><p>The user studies enabled us to evaluate the individual accuracies of our arrow recognizer, parsing algorithm, and symbol recognizer. In its current implementation, our program saves only the user's final sketch, and any objects that are deleted during a drawing session are lost. Our initial accuracy calculations thus do not reflect errors that users repaired by deleting and redrawing objects. This does not produce a significant error in our accuracy calculations, however, because users in the study rarely repaired interpretation errors in this way. In the results presented below, we include estimates of the accuracy that would have been obtained if all interpretation errors had been considered.</p><p>The study has shown the main strength of SimuSketch to be its parsing algorithm. In cases where the arrows were all correctly recognized, or the misrecognized ones were corrected by the user, the parsing algorithm had an accuracy above 95%. In the few cases it failed, two distinct symbols were drawn too close to each other and thus their strokes were grouped into a single cluster.</p><p>In cases where all stroke clusters were correctly identified, the symbol recognition accuracy was between 85 and 90%. Note that while this result is obtained in a user-independent setting (i.e., the training and test symbols belong to different individuals), it is similar to the result of the user-dependent study explained in the previous section. We believe that Simu-Sketch's ability to maintain the same level of accuracy in a more difficult setting can be attributed to its use of contextual knowledge for narrowing down the set of interpretations of a symbol prior to recognition. Nevertheless, when errors occurred, they were mostly due to: (1) the confusion between similarly shaped objects, or (2) the recognizer's sensitivity to non-uniform scaling. Figure <ref type="figure" target="#fig_11">10</ref> shows examples of these issues. However, contrary to our expectation, users did not seem to mind such occasional errors, mainly because they found the means for recovery -either by deleting and redrawing, or by selecting the right interpretation from the list of alternatives -to be intuitive and undemanding. In the latter case, the correct interpretation was always in the list of alternatives suggested by SimuSketch. The main complaint about SimuSketch centered around the arrow recognizer being too restrictive. Although several users quickly became adept at drawing arrows during the warm up period, most users continued having difficulty during the main test session. As we expected, the majority of the errors thus occurred due to the misrecognized arrows. For the most successful users, the arrow recognition accuracy was above 90%. However, when considering all users, the average accuracy for arrow recognition was between 65 and 70%. These results indicate that our arrow recognizer must be further improved to accommodate a wider variety of styles. One approach in this direction would be to replace the hard-coded thresholds of the geometric constraints with thresholds that are tunable to individual users. Besides the issue with arrows, some users had difficulty tapping the stylus to select an object or to bring up a dialog box. Usually, faulty taps were either too gentle, in which case the program did not receive a tap message, or persisted too long on the tablet, in which case the tap was interpreted as a drawing stroke. Another observed difficulty was with the digit recognition in the dialog boxes. While our pre-trained digit recognizer had acceptable performance for certain users, it could not accommodate the vastly dissimilar digit styles that it was not trained for. In cases where the numbers were misrecognized, we asked the users to re-enter them until they got it right. If each user had trained his or her set of digits, we expect that the accuracy would have been similar to the results presented in the previous section.</p><p>To obtain the users' evaluation of SimuSketch's performance, we asked each user to complete a questionnaire at the end of the session. The results shown in Table <ref type="table" target="#tab_0">1</ref> indicate that while there are a number of usability issues that must be addressed, most users viewed SimuSketch as a promising alternative to Simulink. Because SimuSketch is still at an early stage, we have delib-erately avoided a head-to-head comparison between Simu-Sketch and Simulink in our user studies. Nevertheless, as a subjective test of how an individual who is proficient in both environments would perform, one of the authors used the two programs to construct and simulate a variation of the second model shown in Figure <ref type="figure" target="#fig_9">9</ref>. The test involved creating the model, changing the default properties of several objects, and viewing the simulation results. While the task took 241 seconds to complete in Simulink, it took only 183 seconds in SimuSketch. Although simplistic, we believe this experiment helps reveal the latent value of SimuSketch as an enabling tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>We have presented a multi-level parsing and recognition approach designed to enable natural sketch-based computer interaction. This approach allows users to continuously sketch without indicating when one symbol ends and a new one begins. Additionally, it does not restrict the number of strokes, or the order in which they must be drawn.</p><p>Our approach differs from earlier techniques in that it acts selectively in the early stages to identify a small set of easily recognizable "marker symbols." These markers anchor a spatial analysis which parses the uninterpreted strokes into distinct clusters, each representing a single symbol. Finally, an image-based symbol recognizer, which is informed by clustering and domain specific knowledge, is used to find the best interpretations of the strokes. One advantage of this recognizer over traditional ones is that it can learn new definitions from single prototype examples. The recognizer is versatile in that we use it both for graphical symbol recognition and digit recognition.</p><p>We have demonstrated our approach with SimuSketch; a sketchbased interface for Simulink. User studies have indicated that we have sound algorithms for parsing and symbol recognition, and useful means for error recovery. However, our current arrow recognizer should be improved to enhance the user's experience with SimuSketch. Overall, most users had highly favorable opinions of our prototype system, and found it easy and straightforward to use.</p><p>While useful for the practicing engineer, SimuSketch is likely to have distinct advantages in engineering education. By its nature, SimuSketch is ideally suited for electronic whiteboard applications and thus can be readily integrated into the classroom environment. In the near future, we plan to explore this possibility with pilot studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Recognition Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (Top) SimuSketch, (Bottom) Automatically derived Simulink model.</figDesc><graphic coords="2,318.37,221.92,217.75,144.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The user can interact with the system through sketch-based dialog boxes. The simulation results are displayed through conventional Simulink graphs.</figDesc><graphic coords="4,54.88,66.48,242.59,130.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3, this brings up a window showing the simulation results. At any time, the user can add new objects to the model by simply sketching them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Arrow recognition. (a) A one-stroke arrow with the key points labeled. (b) Speed profile. Key points are speed minima.</figDesc><graphic coords="5,81.23,245.90,199.19,71.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of (a) arrows and (b) arrow heads, that are correctly recognized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustration of the cluster analysis. (a) Each stroke is assigned to the nearest arrowhead or tail. (b) Strokes assigned to the same arrow are grouped into clusters. (c) Clusters with overlapping bounding boxes are merged. (d) Arrows that did not receive any strokes are attached to the nearest cluster.</figDesc><graphic coords="5,357.96,260.74,170.97,53.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Examples of symbol templates.</figDesc><graphic coords="6,66.57,64.64,65.78,65.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Symbols used in the graphic symbol recognition experiment.</figDesc><graphic coords="7,341.05,66.23,195.57,135.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Test problems employed in the user studies.Sketch's interface.The main test involved the two Simulink models shown in Figure9. Users were asked to use SimuSketch to construct these models, run a simulation of each, and view the results with minimal help from us. The first model involved changing the parameters of several objects through their dialog boxes while in the second model the default values were accepted. Because the users were not involved in the training of the object shapes, none of them knew what the trained shapes looked like. Although users were given the option to train their own set of symbols before starting, none of them chose to do so. Hence, we provided a sketched version of each of the two models as a quick reference. Both the original models and the sample sketches were presented on paper. Similarly, all users decided to use the pre-trained digit recognizer rather than training their own set of digits. However, in this case we did not provide sample figures of the trained digits. Although no time constraints were set, we encouraged users to complete their tasks in a total of 20 minutes.</figDesc><graphic coords="8,63.32,138.03,222.28,145.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: (a) Pairs of most frequently confused objects. (b) A misrecognition due to non-uniform scaling. (Left) Definition symbol, (Right) One user's misrecognized symbol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average scores obtained from user questionnaire. Scale: 1-10, 10 being excellent.</figDesc><table><row><cell>Score</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Systems that interpret the sketch after each stroke, such as<ref type="bibr" target="#b1">[2]</ref>, often force the user to pause for a short duration between the strokes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our recognizer uses a polar coordinate representation to efficiently account for changes in orientation, but that is beyond the current scope.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Currently SimuSketch has the operational knowledge of 16 Simulink objects. However, the extension to other Simulink objects is straightforward and only requires the code for linking the objects recognized in SimuSketch to those in the actual Simulink program via the necessary constructs.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining multiple representations for pen-based handwritten digit recognition</title>
		<author>
			<persName><forename type="first">Fevzi</forename><surname>Alimoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethem</forename><surname>Alpaydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELEKTRIK: Turkish Journal of Electrical Engineering and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Natural Sketching Environment: Bringing the Computer into Early Stages of Mechanical Design</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Alvarado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Master thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dynamically constructed bayesian networks for sketch understanding</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Alvarado</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>MIT Project Oxygen Student Workshop Abstracts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Resolving ambiguities to create a natural sketch based interface</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Alvarado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-2001</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognizing multistroke geometric shapes: An experimental evaluation</title>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Apte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Takayuki</surname></persName>
		</author>
		<author>
			<persName><surname>Kimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST 93</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing multi-stroke symbols</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Stahovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Kurtoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Burak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kara</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Sketch Understanding</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Invariant matching and identification of curves using b-splines curve representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual language editors based on lr parsing techniques</title>
		<author>
			<persName><forename type="first">Gennaro</forename><surname>Costagliola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincenzo</forename><surname>Deufemia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Workshop on Parsing Technologies (IWPT&apos;03)</title>
		<meeting><address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A modified hausdorff distance for object matching</title>
		<author>
			<persName><forename type="first">Marie-Pierre</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Jerusalem, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="566" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Lee D Erman</surname></persName>
		</author>
		<author>
			<persName><surname>Hayes-Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D Raj</forename><surname>Victor R Lesser</surname></persName>
		</author>
		<author>
			<persName><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The hearsay-ii speech understanding system: Integrating knowldge to resolve uncertainty</title>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="213" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new association coefficient for molecular dissimilarity</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fligner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Verducci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bjoraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Blower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second Joint Sheffield Conference on Chemoinformatics</title>
		<meeting><address><addrLine>Sheffield, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cali-an online scribble recognizer for calligraphic interfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Manueal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesar</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaoquim</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><surname>Jorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Sketch Understanding</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using fuzzy logic to recognize geometric shapes interactively</title>
		<author>
			<persName><forename type="first">J</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquim</forename><forename type="middle">A</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><surname>Jorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Int. Conference on Fuzzy Systems</title>
		<meeting>the 9th Int. Conference on Fuzzy Systems<address><addrLine>San Antonio, USA</addrLine></address></meeting>
		<imprint>
			<publisher>FUZZ-IEEE</publisher>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The combinatorics of heuristic search termination for object recognition in cluttered environments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="920" to="935" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Satin: A toolkit for informal ink-based applications</title>
		<author>
			<persName><forename type="first">Jason I</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM UIST 2000 User Interfaces and Software Technology</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sketched symbol recognition using zernike moments</title>
		<author>
			<persName><forename type="first">Heloise</forename><surname>Hse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Richard</forename><surname>Newton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>EECS, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The use of grouping in visual object recognition</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<idno>1023</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>MIT AI Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A graphic diagram editor for pen computers</title>
		<author>
			<persName><surname>T D Kimura</surname></persName>
		</author>
		<author>
			<persName><surname>Apte</surname></persName>
		</author>
		<author>
			<persName><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Concepts and Tools</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="82" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpreting schematic sketches using physical reasoning</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Kurtoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Stahovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Sketch Understanding</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved method of handwritten digit recognition tested on mnist database</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Kussul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatyana</forename><surname>Baidyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Conference on Vision Interface</title>
		<meeting><address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sketching interfaces: Toward more human interface design</title>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Com</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="56" to="64" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparison of learning algorithms for handwritten digit recognition</title>
		<author>
			<persName><forename type="first">L D</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brunot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J S</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U A</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sackinger</surname></persName>
		</author>
		<author>
			<persName><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Denim: Finding a tighter fit between tools and practice for web site design</title>
		<author>
			<persName><forename type="first">James</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">I</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI Letters: Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Oops: a toolkit supporting mediation techniques for resolving ambiguity in recognition-based interfaces</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Mankoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="834" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognition of Handwritten Mathematical Expressions</title>
		<author>
			<persName><surname>Nicholas E Matsakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Master thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pen and Speech Recognition in the User Interface for Mobile Multimedia Terminals</title>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of California at Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.d. thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vision-based single-stroke character recognition for wearable computing</title>
		<author>
			<persName><forename type="first">Omer Faruk</forename><surname>Ozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oguz</forename><surname>Ozun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oncel Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Atalay</surname></persName>
		</author>
		<author>
			<persName><surname>Enis Cetin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems and Applications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="33" to="37" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Specifying gestures by example</title>
		<author>
			<persName><forename type="first">Dean</forename><surname>Rubine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="329" to="337" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient Visual Recognition Using the Hausdorff Distance</title>
		<author>
			<persName><forename type="first">W J</forename><surname>Rucklidge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Number 1173 Lecture Notes in computer Science</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual organisation as a foundation for intelligent sketch editing</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Saund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Larner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Sketch Understanding</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generic and hmm based approaches to freehand sketch recognition</title>
		<author>
			<persName><forename type="first">Tevfik</forename><surname>Metin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sezgin</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>MIT Project Oxygen Student Workshop Abstracts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistical visual language models for ink parsing</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Shilman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Pasula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Newton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Sketch Understanding</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="126" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A note on binary template matching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><surname>Tubbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="365" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A discrete hmm for online handwriting recognition</title>
		<author>
			<persName><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Articial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="675" to="688" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
