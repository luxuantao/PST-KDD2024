<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Meta-Study of Algorithm Visualization Effectiveness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hundhausen</surname></persName>
							<email>hundhaus@hawaii.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Sciences Department</orgName>
								<orgName type="laboratory">Laboratory for Interactive Learning Technologies Information</orgName>
								<orgName type="institution">University of Hawai`i Honolulu</orgName>
								<address>
									<postCode>96822</postCode>
									<region>HI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Information and Computer Sciences Department</orgName>
								<orgName type="institution">University of Hawai</orgName>
								<address>
									<addrLine>1680 East West Road</addrLine>
									<postCode>POST 303D, 96822</postCode>
									<settlement>Honolulu</settlement>
									<region>HI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sarah</forename><forename type="middle">A</forename><surname>Douglas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Human-Computer Interaction Lab Computer and Information Science Department University of Oregon Eugene</orgName>
								<address>
									<postCode>97403-1202</postCode>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Computing/GVU Center Georgia Institute of Technology Atlanta</orgName>
								<address>
									<postCode>30332-0280</postCode>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Meta-Study of Algorithm Visualization Effectiveness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B9AB50F85AB4FBDFC92EA9327D828ED8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Algorithm visualization (AV) technology graphically illustrates how algorithms work. Despite the intuitive appeal of the technology, it has failed to catch on in mainstream computer science education. Some have attributed this failure to the mixed results of experimental studies designed to substantiate AV technology's educational effectiveness. However, while several integrative reviews of AV technology have appeared, none has focused specifically on the software's effectiveness by analyzing this body of experimental studies as a whole. In order to better understand the effectiveness of AV technology, we present a systematic metastudy of 24 experimental studies. We pursue two separate analyses: an analysis of independent variables, in which we tie each study to a particular guiding learning theory in an attempt to determine which guiding theory has had the most predictive success; and an analysis of dependent variables, which enables us to determine which measurement techniques have been most sensitive to the learning benefits of AV technology.</p><p>Our most significant finding is that how students use AV technology has a greater impact on effectiveness than what AV technology shows them. Based on our findings, we formulate an agenda for future research into AV effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>BY GRAPHICALLY representing computer algorithms in action, algorithm visualization (AV) technology aims to help computer science students understand how algorithms work. Since its advent in the late 1970s, AV technology has evolved from batch-oriented software that enable instructors to construct animated films <ref type="bibr" target="#b0">[1]</ref>; to highly-interactive systems that enable students to explore dynamically-configurable animations of algorithms on their own (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>); to interactive programming environments that enable students to quickly construct their own visualizations (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>). Over the past two decades, this evolution of software has endeavored to enhance computer science education in a variety of capacities. For example, AV software has been used</p><p>• to help instructors illustrate algorithm operations in a lecture (e.g., <ref type="bibr" target="#b1">[2]</ref>);</p><p>• to help students as they study and learn about fundamental algorithms in a computer science course(e.g., <ref type="bibr" target="#b5">[6]</ref>);</p><p>• to help instructors track down bugs in students' linked-list programs during office hours (e.g., <ref type="bibr" target="#b6">[7]</ref>); and</p><p>• to help students learn about the basic operations of an abstract data type in a computer science laboratory (e.g., <ref type="bibr" target="#b7">[8]</ref>).</p><p>Despite its intuitive appeal as a pedagogical aid, algorithm visualization technology has failed to catch on in mainstream computer science education <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. While those few educators who are also AV technology developers tend to employ their own AV technology, the majority of computer science educators tend to stick to more traditional pedagogical technologies, such as blackboards, whiteboards, and overhead projectors.</p><p>Why do computer science educators tend not to use AV technology? Instructors commonly cite several reasons, including:</p><p>• They feel they do not have the time to learn about it.</p><p>• They feel that using it would take away time needed for other class activities.</p><p>• They feel that creating visualizations for classroom use requires too much time and effort. Note that, in the AV technology literature, this reason is frequently used to motivate new technology that is easier to use, and that supports the more rapid creation of visualizations (see, e.g., <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>).</p><p>• They feel that it is simply not educationally effective.</p><p>All of these reasons certainly contribute, in some way, to AV technology's failure to be adopted. However, the fourth reason-that AV technology does not help students learn algorithms better than conventional teaching methods-stands out as particularly important. Indeed, the underlying purpose of AV technology is to be educationally effective, so there certainly is no reason to adopt the technology if it is not effective.</p><p>Given that the underlying purpose of AV technology is to be educationally effective, it is noteworthy that eight extant taxonomic reviews of AV and software visualization technology (see Table <ref type="table" target="#tab_0">1</ref>) have focused largely on system expressiveness. In particular, these taxonomies have focused on three main questions:</p><p>(1) What kinds of programs can be visualized with a given visualization system?</p><p>(2) What kinds of visualizations can a given visualization system produce?</p><p>(3) What methods can one use to produce and interact with visualizations? Notably, in focusing on system expressiveness, these integrative reviews of AV technology have largely ignored a substantial body of over 20 experimental studies that have attempted to substantiate the educational value of AV technology. A casual review of these studies suggests that their results have been, as Gurka <ref type="bibr" target="#b6">[7]</ref> aptly puts it, "markedly mixed." Indeed, while some of the studies have demonstrated a pedagogical advantage for students using AV technology (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>), others have found either (a) no advantage at all (e.g., <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, ch. 2), or (b) an advantage that can be only partially attributed to AV technology (e.g., <ref type="bibr" target="#b16">[17]</ref>).</p><p>While computer science educators and technology critics tend to view the "markedly mixed" results of these studies as a reason not to adopt AV technology, we believe that, if we are willing to look below the surface of "markedly mixed," there are deeper lessons to be learned from trends in these studies. However, as we just pointed out, existing reviews of AV technology have neglected to systematically analyze this body of AV empirical studies for the purpose of gaining further insight into AV effectiveness.</p><p>In this article, we aim to fill this gap in the research by presenting a meta-study of past experimental studies of AV effectiveness. Specific research questions to be explored by our meta-study include • What factors have the studies posited to influence educational effectiveness? Have certain factors had more impact than others? Are there any notable trends?</p><p>• How have studies defined and measured "educational effectiveness?" Have those definitions and measurement techniques influenced the results?</p><p>In light of our answers to those questions, we are ultimately interested in addressing two broader questions:</p><p>• Is AV technology educationally effective?</p><p>• What are fruitful directions for future AV technology and effectiveness research?</p><p>We begin, in section 2, by defining the boundaries of the meta-study-that is, by specifying what research falls within and outside of its scope. Section 3 briefly overviews the 24 experimental studies that form the central data of our meta-study. In section 4, we outline the methods we employ to analyze our corpus of studies. Section 5 presents the main findings of our analysis. Finally, in section 6, we synthesize those findings into a set of conclusions and an agenda for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Scope</head><p>The focus of this meta-study is on algorithm visualization effectiveness. We adopt a standard definition of algorithm visualization: a subclass of software visualization <ref type="bibr" target="#b17">[18]</ref> concerned with illustrating computer algorithms in terms of their high-level operations, usually for the purpose of enhancing computer science students' understanding of the algorithms' procedural behavior. The notion of effectiveness, as it will be used here, concerns the union of humans and technology within the context of a scenario of use. Associated with such a scenario of use is (a) a particular objective to be fulfilled (e.g., "learn how the target algorithm works");</p><p>(b) a particular individual or group having that objective;</p><p>(c) a particular algorithm visualization artifact that will be enlisted; and (d) a particular target algorithm to be visualized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Within this context, effectiveness thus responds to the question:</head><p>To what extent does the algorithm visualization artifact assist the individual or group in fulfilling the objective?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AV scenarios of use</head><p>Given that the notion of effectiveness derives its meaning from the peculiarities of a scenario of use, as just described, what scenarios of use might be the focus of studies of AV effectiveness? One of the original scenarios of AV use envisioned by its pioneers was algorithms research. For example, Brown <ref type="bibr" target="#b1">[2]</ref> reports that his BALSA system was used to analyze a novel, stable version of Mergesort. However, since such scenarios have not been widely reported, and since we are unaware of effectiveness studies of such scenarios, we will not focus on them in this meta-study. Rather, our focus will be on scenarios of AV use within computer science education. Indeed, AV technology has seen by far its most use within computer science education, and educational scenarios of use have been by far the most widely studied.</p><p>Figure <ref type="figure">1</ref> presents a taxonomy of scenarios of use within computer science education. Each of these wellestablished scenarios distinguishes an educational use of AV technology:</p><p>• Lectures. In high school classrooms and college lectures, computer science instructors use graphic representations to help them explain aspects of the algorithms under study (see, e.g. <ref type="bibr" target="#b1">[2]</ref>, Appendix A; <ref type="bibr" target="#b18">[19]</ref>). As Gurka and Citrin <ref type="bibr" target="#b19">[20]</ref> put it, SV in lectures is essentially "an extension of the blackboard, but with more capabilities available" (p. 183).</p><p>• Assignments. Students work on course assignments on their own time, and hand them in before established deadlines. Several computer science educators have described their use of assigments in which students construct their own visualizations of the algorithms under study <ref type="bibr" target="#b1">[2]</ref>, App. A; <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Goldenson and Wang <ref type="bibr" target="#b20">[21]</ref> describe course assignments for which students use the Pascal Genie programming environment, which has built-in design-and run-time SV tools.</p><p>• Class Discussion. After completing assignments in which they construct their own visualizations, students might use AV technology to present their visualizations to their classmates and instructor for feedback and discussion <ref type="bibr" target="#b4">[5]</ref>. In such scenarios, AV technology serves both to enable greater student participation in the class, and to mediate student-instructor interaction.</p><p>• Laboratories. In AV laboratories, students interactively explore algorithms and data structures through structured laboratory exercises <ref type="bibr" target="#b7">[8]</ref>. Like assignments, laboratory sessions have a concrete goal, embodied in a deliverable assignment. However, unlike assignments, labs are constrained by both a location (a laboratory containing graphic workstations) and a contiguous block of time (a session or sitting).</p><p>• Study. Students enrolled in computer science courses have the opportunity to study for tests at any time.</p><p>Depending on their individual preferences, students may elect to enlist AV in their study by drawing their own visualizations, by examining hard copies of visualizations constructed by others (professors or book authors), or by using interactive SV software to which they have access.</p><p>• Office Hours. In college courses, professors and teaching assistants schedule weekly office hours, during which students in need of assistance may visit them. In this setting, instructors may use AV to help them di-agnose bugs in students' programs <ref type="bibr" target="#b19">[20]</ref>, or to help them answer student questions. In the latter case, AV plays an explanatory role, akin to its role in lectures.</p><p>• Tests. In closed test-taking conditions, AV can be used to help pose questions. For example, Brown ([2],</p><p>Appendix A) reports that exams in the algorithms courses at Brown often included stills of algorithm animations discussed in class; students would be asked to "name-that-algorithm," just as students in an art history class might be asked to identify paintings. Alternatively, one can imagine a test question that has students indicate the behavior of an algorithm by drawing a series of data structure snapshots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AV research techniques</head><p>The above scenarios of use provide a focal point for a body of research with a common interest in studying, evaluating, and ultimately improving the effectiveness of AV technology. Figure <ref type="figure">2</ref> presents a taxonomy of the general research techniques that this body of research has employed:</p><p>• Anecdotal techniques aim to appeal to reader intuition by presenting instances and examples of AV system use, as recounted by the system's authors of research methods. Nearly every paper and article reporting on a novel AV system includes such an analysis.</p><p>• Programmatic techniques use the actual programs required to produce visualizations within a given AV system as a basis for assessment. For example, Cox and Roman <ref type="bibr" target="#b21">[22]</ref> evaluate their Pavane system by summing the number of lines of code that are sufficient to specify a visualization within the system-the lower that number, the better.</p><p>• Analytic evaluation techniques (e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>) aim to provide a principled assessment of an interactive system's effectiveness, while avoiding the overhead of extensive empirical data collection. Effectiveness, in the case of analytic evaluation, boils down to usability-the fewer usability problems identified, the more effective the system.</p><p>• Empirical evaluation, in contrast to the other techniques, involves the collection of actual data on humans involved in tasks with AV systems. An analysis process attempts to transform the data into a set of statements that respond to the research questions posed by the evaluation.</p><p>In this meta-study, we focus on effectiveness evaluations that employ empirical techniques. Our position is that empirical techniques are ultimately the most informative of the techniques, because they are rooted in observable, and often measurable, phenomena. Although we believe that studies that employ any of the numerous empirical techniques (see <ref type="bibr" target="#b24">[25]</ref> for a review) have something potentially important to say about AV effectiveness, we restrict the analysis in this meta-study to those studies that use controlled experimentation. Controlled experiments aim to assert a causal relationship between factors (i.e., independent variables) and measures (i.e. dependent variables).</p><p>While there are many variants on controlled experiments, all of the published AV experiments have been betweensubjects experimental comparisons, in which two or more groups of participants use alternative means to learn about an algorithm. If statistically significant differences can be detected between the groups' performances, then experimenters may conclude that the factors significantly affect the measures.</p><p>We limit our scope to experimental evaluations for two reasons: first, because they constitute the largest, most mature subset of the empirical studies; and second, because the evolution of their design reveals an evolution in thinking about why and how AV technology is effective, implying that we can gain considerable insight from considering the legacy of these experiments as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head><p>Our meta-study's data consist of 24 experimental studies that have considered AV effectiveness. To the best of our knowledge, this corpus comprises close to the entire population, if not the entire population, of published AV effectiveness experiments. Table <ref type="table">2</ref> provides a synopsis of these studies. For each experiment, the factors (independent variables) appear in column 2; the measures (dependent variables) appear in column 3; and a summary of the experiment's key results appears in column 4.</p><p>Twenty-two of the 24 of the experiments attempt to determine whether various factors affect learning within the "study" scenario of use described in Section 2. In these experiments, learning is operationalized in terms of some sort of post-test, which participants take upon completing their study session. The two other experiments <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> consider "assignment" scenarios in which participants use AV to solve problems. In these experiments, problemsolving efficacy is operationalized in terms of whether the problems were solved, and how much time was needed.</p><p>Figure <ref type="figure">3</ref> summarizes the results of the 24 experiments. As the figure illustrates, the results have been widely mixed. Eleven of the 24 studies yielded a "significant" result-that is, a statistically-significant difference between the performance of (a) a group of students using some configuration of AV technology, and (b) another group of students using either an alternative configuration of AV technology, or no AV technology at all. For example, Law-rence ( <ref type="bibr" target="#b15">[16]</ref>, Ch. 6 and 9) found, in two separate experiments, that students who explored algorithm animations driven by self-constructed input data sets scored significantly higher than students who either watched the same animations driven by data supplied by the experimenter, or who had no access to such animations. Likewise, Crosby and Stelovsky <ref type="bibr" target="#b27">[28]</ref> found that students who interacted with an algorithm animation performed significantly better than students who listened to a lecture, and that "concrete" learners (as measured by the Myers-Briggs Type Indicator) benefited significantly more from the algorithm animation than did "abstract" learners. Finally, Hansen, Schrimpsher, and Narayanan <ref type="bibr" target="#b13">[14]</ref> found that students who learned an algorithm using their HalVis hypermedia environment, which provides its viewers with multiple views and engages them in input data set design, interactive prediction, and queston-answering, significantly outperformed students who learned the same algorithm using (a) textual materials (Study I), (b) a lecture (Study II), or (c) a conventional, one-view algorithm animation environment with no facilities for interactive prediction or question-answering (Study V). In follow-up studies, Hansen, Schrimpsher, and Narayanan <ref type="bibr" target="#b13">[14]</ref> additionally found that students who interacted with the HalVis conceptual and detailed views performed significantly better than students who did not have access to these views (Studies VII and VIII).</p><p>In contrast, a roughly equal number of studies <ref type="bibr" target="#b9">(10)</ref> did not have a significant result. In other words, in these studies, no statistically significant differences could be found between the performance of (a) groups of students using some configuration of AV technology, and (b) groups of students using either an alternative configuration of AV technology, or no AV technology at all. For instance, Price <ref type="bibr" target="#b25">[26]</ref> had two groups of students debug a 7,500 program using a debugger with and without an animated view. He found no significant differences in debugging efficacy between the two groups. Similarly, Stasko, Badre, and Lewis <ref type="bibr" target="#b14">[15]</ref> had two groups of students learn the pairing heap data structure (a) by interacting with an algorithm animation, and (b) by studying textual materials. methods. Although the animation group performed better on a post-test, the difference was not statistically significant. Finally, Lawrence <ref type="bibr">([16]</ref>, ch. 4 &amp; 5) compared the post-test performance of students who learned algorithms using animations with alternative representational characteristics: sticks versus dots; 9 vs. 24 vs. 41 data elements; and labels vs. no labels. She found no significant differences among the groups.</p><p>Two studies found a significant result in which the positive impact of visualization could not be disentangled from another factor. In these studies, Byrne, Catrambone, and Stasko <ref type="bibr" target="#b16">[17]</ref> found that students who made predictions regarding future animation frames while viewing algorithm animations performed significantly better than students who did not view animations or make predictions; however, the individual effects of prediction and animation could not be disentangled statistically.</p><p>Finally, one study yielded a "negative" result in which students who used AV technology actually performed significantly worse than students who used text-based tools. In particular, Mulholland <ref type="bibr" target="#b26">[27]</ref> found that, in Prolog tracing tasks, participants who used any of three textual tracers solved significantly more problems than participants who used TPM, a graphical tracer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In the field of psychology, meta-analytical techniques (see, e.g., <ref type="bibr" target="#b28">[29]</ref>) have been developed for statistically analyzing a body of related experimental studies with similar independent and dependent variables. The goal of these meta-analytical techniques is to infer invariant cause-effect relationships based on the findings of a corpus of related studies. To do so, a statistical meta-analysis systematically combines and compares the studies in order to determine the effect size and significance level of their independent variables. For example, in order to build a "task-feature taxonomy" that can guide the effective use of information visualization technology, Chen and Yu <ref type="bibr" target="#b29">[30]</ref> perform a statistical meta-analysis of a sample of 35 experimental studies of information visualization. However, the diversity of the studies in their sample makes it difficult to apply metaanalytic techniques; they are ultimately forced to reduce their sample to six sufficiently homogeneous studies focusing on information retrieval tasks. Interestingly, their analysis determines that individual differences have a larger and more consistent effect on human performance than does information visualization technology itself.</p><p>We considered using statistical meta-analytic techniques in this meta-study, but ultimately decided against them for two reasons. First, like the sample of Chen and Yu, the corpus of experimental studies on which we are focusing is quite diverse. Indeed, while their independent and dependent variables may appear similar on the surface, the ways in which those variables are manipulated and operationalized vary considerably. For example, while some studies compare the use of AV technology against the use of conventional learning materials (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref>), others compare competing versions of AV technology (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>). Likewise, while some studies operationalize learning in terms of a post-test consisting of exam-style questions (e.g., <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>), others opt to measure performance in terms of accuracy on programming <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>, tracing <ref type="bibr" target="#b30">[31]</ref>, and prediction <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref> tasks. Second, even if, like Chen and Yu, we were able to find a small number of sufficiently homogenous studies on which to perform a statistical meta-analysis, we do not believe that such a meta-analysis would be informative. This is not only because the sample that we would ultimately consider would be small, but also because large discrepancies in the results of the studies in our sample would give rise to statistically uninformative conclusions.</p><p>For these reasons, we believe that any meta-analysis of this corpus of studies must find a principled way to classify the studies into smaller groups, such that each group's results, when taken as a whole, is more uniform. What might be the basis of such a principled classification? In scrutinizing the studies in our corpus, we have observed (see also <ref type="bibr" target="#b4">[5]</ref>) notable differences in their choices of both independent variables (i.e., the factors they posit to cause effectiveness) and their choices of dependent variables (i.e., the ways in which they measure effectiveness). For example, some studies manipulate representational features of visualizations (e.g., color, shading, geometry), while others manipulate the learner's level of activity (e.g., learner views visualization; learner designs input data; learner makes predictions regarding future visualization states). Likewise, the studies measure effectiveness in terms of differing kinds of knowledge acquisition-both conceptual and procedural.</p><p>The analysis method we employ in this meta-study, then, involves first classifying the studies in various principled ways based on their independent and dependent variables, and then scrutinizing the results of the studies vis-àvis these classifications. Our analysis of study results includes both quantitative comparisons of the numbers of statistically significant and non-significant results in opposing classes of studies, and qualitative assessments of trends. It is important to note that our analysis will not make judgments about the validity of each experiment-that is, the extent to which each experiment's results can be trusted. Rather, we will make the simplifying assumption that the results of each experiment in our corpus can be trusted roughly equally. While the experimental results on which we base our analyses are clearly tempered by the soundness of their experimental designs, and while there clearly are differences in the soundness of the experiment designs in our corpus, we believe that the differences are minor.</p><p>Hence, critiquing experimental design would, we believe, constitute an unnecessary distraction from our focus on results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Independent variables</head><p>We begin our analysis by scrutinizing the studies' independent variables-that is, the factors they have posited to cause effectiveness. We argue that the studies' differences in independent variables actually reflect deeper differ-ences in underlying theories of effectiveness: the studies' assumptions about how and why AV might be effective. In the studies in our corpus, AV technology is enlisted as a pedagogical tool; its goal is to enhance learning. It follows that the theories of effectiveness underlying the studies in our corpus correspond with alternative theories of learning. This section begins by presenting the four alternative learning theories adopted by the studies in our corpus.</p><p>Next, we link each study to a particular theory based on its choice of independent variables. Following that, we quantitatively assess the robustness of each theory by considering the number of significant versus non-significant results of the studies associated with it. Finally, we perform a reality check on our findings by assessing the extent to which the empirical results of competing theories actually lend support to the theory that our analysis found to have the greatest support. This finer-grained analysis enables us to draw more definitive conclusions about the robustness of that theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Four theories of effectiveness</head><p>The varied designs of the AV experimental studies reflect a variety of underlying theories of effectiveness.</p><p>While finer distinctions may certainly be made among these studies' theoretical underpinnings, the broad-brush analysis presented here places them into four broad theoretical camps, which we label Epistemic Fidelity, Dual-Coding, Individual Differences, and Constructivism. Below, we briefly describe these theories; for a fuller treatment, see <ref type="bibr" target="#b4">[5]</ref>.</p><p>Epistemic Fidelity. Epistemic Fidelity theory <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> has its roots in a representationalist epistemological framework (see, e.g., <ref type="bibr" target="#b35">[36]</ref>), which assumes that humans carry around in their heads symbolic models of the physical world, and that such symbolic models are the basis for all of their reasoning and action. The key assumption of Epistemic Fidelity theory, then, is that graphics have an excellent ability to encode an expert's mental model of an algorithm, leading to the robust, efficient transfer of that mental model to the viewer (see Figure <ref type="figure">4</ref>). Thus, Epistemic Fidelity theory emphasizes the value of a good denotational match between the graphical representation and the expert's mental model. The higher the "fidelity" of the match, the more robust and efficient is the transfer of that mental model to the viewer of the visualization, who non-problematically decodes and internalizes the target knowledge.</p><p>Dual-coding. Based on Mayer and Anderson's <ref type="bibr" target="#b36">[37]</ref> integrated dual-code hypothesis, Dual-Coding theory proceeds from Paivio's <ref type="bibr" target="#b37">[38]</ref> assumption that "cognition consists largely of the activity of two partly interconnected but functionally independent and distinct symbolic systems" (p. 308). One encodes verbal events (words); the other en-codes nonverbal events (pictures). According to Mayer and Anderson's hypothesis, visualizations that encode knowledge in both verbal and non-verbal modes allow viewers to build dual representations in the brain, and referential connections between those representations. As a consequence, such visualizations facilitate the transfer of target knowledge more efficiently and robustly than do visualizations that do not employ dual-encoding.</p><p>Individual differences. A legacy of psychological experiments have attempted to operationalize, and to better understand, individual differences in human cognitive abilities and learning styles (see, e.g., <ref type="bibr" target="#b38">[39]</ref>). The key contribution of this research has been not only a battery of instruments for rating and classifying individuals along several dimensions, but also empirical results that make important statements about human performance relative to individual differences so measured. Thus, in the context of this analysis, Individual Differences theory asserts that measurable differences in human abilities and styles will lead to measurable performance differences in scenarios of AV use. For example, within the scope of Epistemic Fidelity theory's knowledge transfer model (see Figure <ref type="figure">5</ref>), Individual Differences with respect to learning style (see, e.g., <ref type="bibr" target="#b39">[40]</ref>) might enable some individuals to decode visualizations more efficiently and robustly than other individuals.</p><p>Cognitive Constructivism. Rather than regarding knowledge as representations of an objective reality that people carry around in their heads, Cognitive Constructivism (see, e.g., <ref type="bibr" target="#b40">[41]</ref>) asserts that there is no absolute knowledge. Note that Cognitive Constructivism's emphasis on active learning has important implications for the effective use of AV technology. In particular, it suggests that individuals do not stand to benefit from the technology by merely passively viewing visualizations, no matter how high the level of their epistemic fidelity. Instead, AV technology users must become more actively engaged with the technology in order to benefit most from it. The technology, on this view, is seen not as a conveyer of knowledge, but as a tool for knowledge construction.</p><p>In order to highlight the key distinctions among the four theories just discussed, Table <ref type="table">3</ref> synopsizes the theories and the role they ascribe to an AV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Linking studies to theories</head><p>The independent variables chosen by each of the 24 experimental studies summarized earlier, as well as the ways in which those independent variables are manipulated, provide insight into each study's theoretical underpinnings. By examining each study's experimental manipulation, we can link it to one of the theories of effectiveness just discussed.</p><p>Table <ref type="table">4</ref> links each of the 24 experimental studies to an underlying theory of effectiveness. In four cases ( <ref type="bibr" target="#b15">[16]</ref>,</p><p>Ch. 5 &amp; 7; <ref type="bibr" target="#b27">[28]</ref>; <ref type="bibr" target="#b13">[14]</ref>, Study VI), a study is linked to two underlying theories, because the study defined independent variables, or performed experimental manipulations, that were judged to support multiple underlying theories.</p><p>As this table indicates, we judged ten studies to be aligned with Epistemic Fidelity theory. These studies manipulated either (a) representational features of visualizations, or (b) the order in which visualizations are presented.</p><p>The hypothesis that these studies are positing is that certain representational features, or certain orderings of features, will promote the knowledge decoding process more robustly and efficiently than others.</p><p>In two separate studies, Lawrence ([16], Ch. 5 &amp; 7) compared the efficacy of representations in which information is singly-encoded (graphics only) and doubly-encoded (graphics and textual labels). These studies are clearly inspired by Dual-Coding theory, which holds that the dually-coded representations will promote more robust and efficient knowledge transfer.</p><p>Lawrence <ref type="bibr" target="#b15">[16]</ref> and Crosby &amp; Stelovsky <ref type="bibr" target="#b27">[28]</ref> considered the impact of spatial abilities, cognitive abilities, and learning styles on one's ability to learn from a visualization. These studies were clearly influenced by Individual Differences theory.</p><p>Finally, fourteen studies manipulated the way in which humans are engaged with their environment to accomplish some task for which AV is assumed to be beneficial. By varying primarily the kinds of activities and forms of engagement, and not the representation, these studies demonstrate a loyalty to Cognitive Constructivism, which views the benefits of AV technology not in its ability to transfer knowledge, but in its ability to promote the construction of knowledge through active engagement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Coarse-grained analysis of theory robustness</head><p>Given that each study shows a loyalty to one of the theories of effectiveness, an important question arises: What level of experimental support has been demonstrated for each of the four theories? To start to answer that question, Figure <ref type="figure">5</ref> presents a bar graph of the experimental results vis-à-vis each of the four theories. In this bar graph, each experimental study is assumed to have equal weight. The length of each bar corresponds to the number of studies that have been guided by the corresponding theory. The filled-in portion of each bar indicates the number of statistically significant results that were obtained in support of the theory. <ref type="foot" target="#foot_0">1</ref> The amount each bar is filled in thus graphically indicates the proportion of the results that were statistically-significant.</p><p>As Figure <ref type="figure">5</ref>   <ref type="bibr" target="#b9">(10)</ref>, and highest percentage (71%), of statistically significant differences. Based on this broad-brush analysis, one can conclude that Cognitive Constructivism has garnered the most consistent empirical support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Finer-grained analysis of Cognitive Constructivism</head><p>Clearly, the conclusion reached by the above analysis must be considered tentative, because the analysis fails to consider the global case for a given theory-that is, whether study results outside of a given theoretical camp lend support to, or weaken, a given theory. Focusing on the case for Cognitive Constructivism, the analysis that follows scrutinizes its support across the entire corpus of studies. As we shall see, this analysis lends further credence to the theory by illustrating its broader predictive success.</p><p>An important assumption of Cognitive Constructivist theory is that learner activity matters; active learning is assumed to be superior to passive learning. Rather than passively viewing algorithm visualizations, active learners augment their viewing with such activities as</p><p>• constructing their own input data sets (e.g., <ref type="bibr" target="#b15">[16]</ref>, ch. 9);</p><p>• making predictions regarding future visualization states (e.g., <ref type="bibr" target="#b16">[17]</ref>);</p><p>• programming the target algorithm (e.g., <ref type="bibr" target="#b32">[33]</ref>);</p><p>• answering strategic questions about the visualization (e.g., <ref type="bibr" target="#b13">[14]</ref>); and</p><p>• constructing their own visualizations (e.g., <ref type="bibr" target="#b30">[31]</ref>).</p><p>On the Cognitive Constructivist view, then, AV technology is seen as educationally effective to the extent that it actively engages learners in such activities.</p><p>Given this, an obvious question arises: Does the precise form of the activity matter? In other words, are some activities better at promoting "active learning" than others? The Cognitive Constructivist position is that, as long as the learner's activity is salient to the target knowledge or skill, the effort required to engage in the activity is more important than its actual form. Thus, assuming equal salience, Cognitive Constructivist theory would predict that the more effort required to engage in the activity, the more robust the learning.</p><p>Applying this reasoning to controlled experimental comparisons, we find that Cognitive Constructivism predicts an educational advantage only in cases in which the activity of one treatment group is more effortful than that of the other treatment group(s). Conversely, if all treatment groups' activity is roughly equivalent in terms of necessary effort, then the theory does not predict any learning differences.</p><p>To further scrutinize the robustness of Cognitive Constructivism, we can examine the results of the 24 experiments in our corpus vis-à-vis the equivalence of the effort required to perform the activities in their competing treatment groups. As a first step, we must judge the equivalence of the effort required to perform the activities in competing treatment groups (see Table <ref type="table">5</ref>). If, in a given experiment, the competing learning activities require roughly equivalent cognitive effort, we classify them as "Effort equivalent." For example, we judge that viewing two alternative visualizations ( <ref type="bibr" target="#b15">[16]</ref>, Ch. 4.4) requires roughly equivalent effort. Likewise, we estimate that paper-andpencil problem-solving exercises require effort roughly equivalent to that of interacting with the HalVis hypermedia system ( <ref type="bibr" target="#b13">[14]</ref>, Study III), which engages users in equivalent problem-solving activities. If, on the other hand, a notable difference exists in the effort levels required to engage in the competing treatment groups of a given experiment, we classify the experiment as "Effort not equivalent." For example, in our judgment, reading textual materials, listening to a lecture, and passively viewing a visualization all require markedly less effort than actively using a visualization in conjunction with some sort of learning exercise-for example, prediction <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref>, input data selection ( <ref type="bibr" target="#b15">[16]</ref>, ch. 9], or programming <ref type="bibr" target="#b32">[33]</ref>.</p><p>An important implication of Cognitive Constructivism is that experiments rated as "Effort not equivalent" will show a significant learning advantage for the more effortful group . Conversely, the theory suggests that experiments rated as "Effort equivalent" will show no significant differences between the treatment groups. <ref type="foot" target="#foot_1">2</ref> The next step in the analysis is to see whether these predictions are actually borne out. Under the assumption that each experimental study has equal weight, Figure <ref type="figure" target="#fig_5">6</ref> carries out this step by plotting the number of results and significant results vis-à-vis the "Effort not equivalent/Effort equivalent" classification presented in Table <ref type="table">5</ref>. In six of nine studies in which competing treatment group activities required roughly equivalent effort, no significant results were found. In contrast, in 10 of 14 studies in which one of the treatment groups required more effort, the group expending the higher level of effort significantly outperformed the other group. These results accord reasonably well with the theory's predictions.</p><p>In the case of "Effort equivalent" studies, Cognitive Constructivism predicts 67% of the results. In the case of "Effort not equivalent" studies, Cognitive Constructivism predicts 71% of the results. This quantitative finding is further reinforced by contrasting two studies classified as "Effort equivalent" with similarly-designed studies classified as "Effort not equivalent." Gurka <ref type="bibr" target="#b19">[20]</ref> attempted to replicate one of the experiments of Byrne, Catrambone, and Stasko <ref type="bibr" target="#b16">[17]</ref>, which obtained a significant result in favor of participants who made predictions and viewed an animation. However, Gurka eliminated the prediction conditions from her experiment, opting instead to compare a group of participants who used an animation against a group of participants who did not, with no substantial differences in the groups' levels of engagement. Both groups of participants, for example, engaged in group study as part of their learning session. As would be predicted by Cognitive Constructivist theory, Gurka's de facto equalization of participants' activities led to a non-significant result.</p><p>Likewise, in a series of studies, Hansen, Schrimpsher, and Narayanan <ref type="bibr" target="#b13">[14]</ref> found that participants who used their HalVis multimedia environment, which actively engages its users by having them construct their own input data sets, answer questions, and make predictions, significantly outperformed participants who learned from textual materials (Studies I and II), and viewed a lecture (Study IV). In one of their studies (Study III), however, the researchers attempted to equalize the activities of the HalVis and text-only groups by having students in the text-only group not only read articles on an algorithm, but also complete a series of paper-and-pencil exercises. As would be predicted by Cognitive Constructivist theory, the researchers' decision to engage the text-only participants as actively as participants in the HalVis group led to their failure to find a significant difference between the two groups' learning outcomes.</p><p>example, if the algorithm animation is poorly designed or lacks necessary information-then no amount of effort is likely to lead to a measurable learning difference.</p><p>In sum, our finer-grained analysis shows that Cognitive Constructivism predicts not only the greatest percentage of significant results (77%), but also a majority (60%) of the non-significant results. This analysis lends further credence to our coarse-grained analysis' conclusion that Cognitive Constructivist theory is the most robust. In contrast, Epistemic Fidelity theory has had the least predictive success, having predicted just 30% of the significant results, with another significant result running completely counter to the theory's predictions. Finally, due to low numbers of supporting studies, it is fair to say that the jury is still out on Dual-Coding and Individual Differences theories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dependent variables</head><p>We now move to an analysis of the studies' dependent variables-that is, the ways in which they have measured effectiveness. The two studies in our corpus that examined "assignment" scenarios measured effectiveness in terms of participants' success at solving debugging and tracing problems with the help of AV technology <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Neither of these experiments yielded a significant result. A survey of the remaining 22 studies' dependent variables suggests that they measured effectiveness in remarkably similar ways. Indeed, all of these experiments elected to measure effectiveness in terms of knowledge acquisition. The underlying hypothesis was that the experimental manipulation will lead some experimental treatments to acquire target knowledge more robustly than others.</p><p>On closer inspection of the 22 studies that measure effectiveness in terms of knowledge acquisition, we find notable differences both in terms of what they measure, and how they measure it. Below, we further investigate these two key differences, in an attempt to determine whether certain choices of dependent variables have led to greater success in detecting learning differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Differences in what knowledge is measured</head><p>The first notable difference among the studies' dependent variables lies in the precise types of knowledge they attempt to measure. Consistent with typical college exams in computer science, the studies have aimed to measure two distinct types of knowlege:</p><p>• conceptual or declarative-an understanding of the abstract properties of an algorithm, e.g., its Big-O efficiency, its range of output, or limits on the input data that it can process. A sample test question might be "What is the worst-case efficiency of the algorithm?"</p><p>• procedural-an understanding of the procedural, step-by-step behavior of an algorithm, that is, how it operates on a set of input data. A sample test question might involve tracing an algorithm's key variables and data structures for a given set of input data.</p><p>It is important to note that these two forms of knowledge are not necessarily distinct. Frequently, a high level of conceptual knowledge is needed to be able to understand an algorithm's procedural behavior. For example, in order to understand how the Quicksort algorithm works, one needs a conceptual understanding of recursion. Conversely, understanding procedural operations can help with conceptual questions. For example, grasping the pattern of Quicksort's divide-and-conquer strategy can give one insight into the algorithm's efficiency. Table <ref type="table">6</ref> classifies the 22 knowledge-measuring studies according to whether their evaluation instruments test (a) conceptual and procedural knowledge, (b) conceptual knowledge only, or (c) procedural knowledge only. (Note that some studies include multiple dependent measures that test more than one of these combinations. These studies are listed multiple times-once for each respective combination.) Since most visualizations focus on the procedural behavior of an algorithm, it is no surprise that a majority of the studies have attempted to measure procedural knowledge alone. For the same reason, only a small number (3) of the studies have measured conceptual knowledge alone.</p><p>Nine studies have forged a middle ground by measuring both forms of knowledge.</p><p>Under the assumption that each experimental study has equal weight, Figure <ref type="figure">7</ref> carries the analysis further by graphically presenting the number of significant and non-significant results vis-à-vis the three classes of experiments categorized in Table <ref type="table">6</ref>. As the figure illustrates, all three knowledge measures garnered comparable levels of experimental support; no one measure appears to be more sensitive to learning effects than any other. Given that algorithm visualizations illustrate the procedural behavior of algorithms, it is perhaps somewhat surprising that two out of the three studies that measured conceptual knowledge exclusively obtained significant results. However, the sample size (3) is clearly too small for general conclusions to be drawn.</p><p>Of the other two knowledge measurements, procedural knowledge only appears to have been more sensitive to learning differences than conceptual and procedural knowledge, although this result is difficult to interpret. One might speculate that the conceptual portion of the conceptual-and-procedural tests is what diminished their sensitivity. If that were the case, we would expect the conceptual-only tests to be even less sensitive than procedural-andconceptual tests. That is not the case, however. Indeed, we find that combining the conceptual-only results with the conceptual-and-procedural results actually raises the overall sensitivity of conceptual-and-procedural tests to 45% (5 of 11 significant results). In spite of these inconsistent results, procedural test questions still appear to be somewhat more sensitive to AV technology's educational benefits than conceptual questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Differences in how knowledge acquisition is measured</head><p>The methodology employed to measure knowledge acquisition constitutes the second notable difference in the studies' dependent variables. In thirteen of the 22 studies (see Table <ref type="table">7</ref>), a post-test designed to measure knowledge acquisition formed the sole basis of measurement. Post-test scores constituted the data that are statistically analyzed.</p><p>In contrast, the other nine studies (see Table <ref type="table">7</ref>) gave participants both a pre-test and a post-test. In these studies, pretest to post-test improvement formed the basis of the statistical analyses.</p><p>As Figure <ref type="figure">8</ref> illustrates, the results of experiments employing these two alternative measurement techniques differ noticeably. Seven of 13 (54%) of the studies that measured learning using a single post-test found a statistically significant difference between treatment groups. In contrast, seven of nine (78%) studies that measured learning based on pre-test to post-test improvement found statistically significant differences. In interpreting the 78% success rate of pre-to post-test improvement, one should keep in mind that it is biased in the sense that it is based almost entirely on the work of a single line of studies. Nonetheless, given the difference in success rates, one can cautiously conclude pre-test to post-test improvement may be more sensitive to learning differences than simple post-test performance.</p><p>Viewing these results in light of Cognitive Constructivism's predictions lends further credence to this finding. Just two experimental studies ( <ref type="bibr" target="#b13">[14]</ref>, Study III and VI) that measured pre-test to post-test improvement actually failed to obtain a significant difference. Of those two studies, Cognitive Constructivism would have predicted only one of the studies to find a significant difference between treatment groups. Thus, according to Cognitive Constructivism, the pre-test to post-test improvement measure failed just once. In contrast, the post-test only measure failed three times. Indeed, of the six "post-test only" studies that failed to find a significant difference, Cognitive Constructivism would have predicted three <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> to have found a significant difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this article, we have presented an integrative review of empirical studies of algorithm visualization effectiveness, in order to uncover trends in the research that might help us better understand how and why AV technology is effective. The focus of our review has been on one particular class of empirical studies: controlled experiments of AV technology in educational scenarios of use. We have pursued two separate analyses of 24 such experimental studies. Our first analysis examined their results vis-à-vis their independent variables in an attempt to better understand what factors have most consistently caused effectiveness. Conversely, our second analysis looked at the studies' results vis-à-vis their dependent variables, in order to understand both how effectiveness has been defined, and what measures are best able to detect it. What exactly have we learned from our meta-study's review and analysis?</p><p>We conclude by revisiting our four original research questions in light of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">What factors have had the most impact on effectiveness?</head><p>Four general theories of effectiveness, encompassing differing epistemological assumptions about knowledge and its relationship to an AV, have been embraced by our sample of experimental studies: Epistemic Fidelity, Dual- over, an analysis of participant groups' engagement effort equivalence with respect to experimental results across all experiments further substantiates the pedagogical impact of using AV technology to engage students actively in their own learning. Thus, according to our analysis, how students use AV technology, rather than what students see, appears to have the greatest impact on educational effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">What measures have been most sensitive to effectiveness?</head><p>Twenty-two of the 24 studies in our corpus operationalize effectiveness in terms of conceptual knowledge, procedural knowledge, or both. An analysis of experimental results based on type of knowledge measured suggests that procedural knowledge may serve as a more sensitive measure of AV technology's benefits than conceptual knowledge. We take comfort in the fact that this finding is borne out in other non-experimental studies of AV technology not included in our corpus (e.g., <ref type="bibr" target="#b41">[42]</ref>).</p><p>Closer inspection of the studies' dependent variables suggests that they have measured knowledge using two different techniques: post-test performance only, and pre-to post-test improvement Assuming that a learning differ-ence truly exists between treatment groups, our analysis suggests that pre-test to post-test improvement will be more likely to find such a difference than post-test performance alone. It is important to qualify this finding in two ways.</p><p>First, it is biased in the sense that it is based largely on the results of one particular line of studies <ref type="bibr" target="#b13">[14]</ref>. Second, we believe that measuring pre-test to post-test improvement introduces a methodological problem that has not been addressed by the studies that employed it: The pre-test may give participants knowledge of the target algorithm outside of the treatment condition. Thus, we believe that studies that use this method to measure effectiveness need to address this methodological issue.</p><p>Finally, we would be remiss not to point out that our conclusions are limited in that they are based on a corpus of studies that considered an extremely limited range of dependent variables. A few studies employed alternative measures-for example, time spent learning <ref type="bibr" target="#b31">[32]</ref>, the time spent taking the post-test <ref type="bibr" target="#b30">[31]</ref>, and success on graphical versus textual questions <ref type="bibr" target="#b27">[28]</ref>. By and large, however, the studies in our corpus operationalized learning in terms of conceptual and procedural knowledge acquisition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Is AV technology effective?</head><p>Clearly, answers to the previous two questions provide a backdrop for addressing the all-important question of whether AV technology is educationally effective. In light of our meta-study findings, our answer to this question would have to be a qualified "yes." In what ways is AV technology educationally effective, and in what ways is it not educationally effective? Let us begin with the ways in which it is not effective. With few exceptions, we found that studies in which students merely viewed visualizations did not demonstrate significant learning advantages over students who used conventional learning materials. Perhaps contrary to conventional wisdom, this observation suggests that the mere presence of AV technology-however well-designed and informative the visual representations it presents may appear to be-does not guarantee that students will learn an algorithm. To state this in terms of the learning theory (Epistemic Fidelity) underlying this form of AV technology use, our findings suggest that algorithm visualizations do not merely transfer an expert mental model of an algorithm to a student's brain. In successful applications of AV technology, something else appears to be going on.</p><p>In particular, our meta-study suggests that the most successful educational uses of AV technology are those in which the technology is used as a vehicle for actively engaging students in the process of learning algorithms. For example, as we have seen, AV technology has been successfully used to actively engage students in such activities as</p><p>• what-if analyses of algorithmic behavior (e.g., <ref type="bibr" target="#b15">[16]</ref>, ch. 9);</p><p>• prediction exercises (e.g., <ref type="bibr" target="#b16">[17]</ref>); and</p><p>• programming exercises (e.g., <ref type="bibr" target="#b32">[33]</ref>).</p><p>Notice that, in such cases, rather than being an instrument for the transfer of knowledge, AV technology serves as catalyst for learning. To state this in terms of the learning theory (Cognitive Constructivism) underlying this form of AV technology use, our results suggest that algorithm visualizations are educationally effective insofar as they enable students to construct their own understandings of algorithms through a process of active learning.</p><p>In sum, our meta-study suggests that AV technology is educationally effective, but not in the conventional way suggested by the old proverb "a picture is worth 1,000 words." Rather, according to our findings, the form of the learning exercise in which AV technology is used is actually more important than the quality of the visualizations produced by AV technology. This is not to say that visualization quality does not matter-in a successful learning activity, a well-designed visualization certainly contributes-but rather to say that the form of activity is more important than the form of the visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">What directions for future research appear most fruitful?</head><p>Finally, our meta-study findings suggest several directions for future research into AV effectiveness. We conclude by highlighting what we see as the most important of these, organizing our discussion around the topics of independent variables, dependent variables, and scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Independent variables</head><p>As we have seen, the majority of AV effectiveness research over the past decade has been guided by just two theories: Epistemic Fidelity and Cognitive Constructivism. In the interest of gaining deeper insight into AV effectiveness, we believe that future research would do well to explore other alternatives. One obvious place to start is with Dual-Coding and Individual Differences theories, both of which have already garnered support by studies in our corpus. Dual-Coding theory has, in fact, had predictive success in a legacy of experimental studies investigating the pedagogical benefits of instructional hypermedia (e.g., <ref type="bibr" target="#b36">[37]</ref>); we would be interested to see it pursued further within the scope of pedagogical AV.</p><p>We believe another promising source of theoretical inspiration is recent anthropological research into the social and situated nature of learning and communication. In the style of Table <ref type="table">3</ref> (p. 33), Table <ref type="table">8</ref> outlines two other general theories of effectiveness that come from that line of work. Building on Situated Action Theory (see, e.g., <ref type="bibr" target="#b42">[43]</ref>), Roschelle <ref type="bibr" target="#b34">[35]</ref> argues that visualizations are one of a multitude of mediational resources that help groups of people to negotiate a shared understanding of the topics under study. Alternatively, Sociocultural Constructivism (e.g., <ref type="bibr" target="#b43">[44]</ref>)</p><p>views AV effectiveness at the level of the community of practice, rather than the individual. In particular, AV technology is seen as effective insofar as it provides students with access to the kinds of expert activities normally carried out by algorithms teachers. Hundhausen <ref type="bibr" target="#b4">[5]</ref> uses sociocultural theory to guide his study of the use of AV technology in an undergraduate algorithms course.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Dependent variables</head><p>Measuring procedural and conceptual knowledge in terms of test performance has a solid foundation in practice; indeed, it is the same measure used in traditional computer science courses to evaluate student performance. However, in order to obtain a deeper understanding of AV technology's educational benefits, future empirical studies should, in our opinion, have the courage to explore alternative measures-preferably in concert with traditional measures so as not to disenfranchise themselves from the mainstream.</p><p>Since a study's dependent measures are a logical consequence of the learning theory that informs it, alternative measures can be explored within the context of alternative theories. In stark contrast to the learning theories that influenced the studies in our corpus, the two alternative learning theories introduced above (see Table <ref type="table">8</ref>) situate knowledge not in the head, but in the broader realms of social interaction (Situated Action) and community reproduction (Sociocultural Constructivism). According to these theories, individual knowledge acquisition does not serve as an adequate measure of learning, which must be measured within the broader context of social interaction within communities of practice.</p><p>For example, Situated Action theory would recommend evaluating an algorithm visualization by creating a social situation in which two learners use the visualization to establish a shared understanding of the underlying algorithm. In such an interaction, conversation analysis would be used to determine the extent to which the visualization serves as a mediational resource for the learners. For example, Douglas, Hundhausen, &amp; McKeown <ref type="bibr" target="#b44">[45]</ref> use this evaluation measure in their studies of the human visualization of sorting algorithms.</p><p>Likewise, Sociocultural Constructivism would recommend evaluating an algorithm visualization within the scope of its use as a cultural artifact in a community of practice such as the one being reproduced in an undergraduate algorithms course. In such a setting, the effectiveness of the visualization would be judged according to its ability to enable learners gradually to participate more fully in the community-that is, to increasingly take on the identity and roles of the course instructor. For example, within the scope of a junior-level algorithms course, Hundhausen <ref type="bibr" target="#b4">[5]</ref> uses ethnographic field techniques to qualitatively evaluate effectiveness in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Scope</head><p>We intentionally limited the scope of our meta-study to experimental studies of AV in educational scenarios of use. In so doing, we clearly neglected important lines of related empirical work that are grist for future analysis.</p><p>Below, we describe three of the most important of these.</p><p>Other research techniques. Our meta-study focuses exclusively on experimental studies of AV effectiveness.</p><p>However, as we pointed out in section 2.2, controlled experimentation is just one of several empirical evaluation techniques that one might use to study AV effectiveness. Four other relevant empirical techniques that have been employed by past AV effectiveness research include:</p><p>• Usability tests-these endeavor to identify, diagnose, and ultimately remedy problems with an interactive system's user interface by videotaping a small number of participants are videotaped as they complete representative tasks with the system (see, e.g., <ref type="bibr" target="#b45">[46]</ref>).</p><p>• Ethnographic field techniques-these include any of the qualitative techniques one might use to conduct a field study <ref type="bibr" target="#b46">[47]</ref> in a naturalistic setting-e.g., participant observation, interviews, and artifact collection.</p><p>• Questionnaires and surveys-these are often used as a complementary source of data in empirical studies.</p><p>They elicit written responses to a set of questions in which the researcher is interested (see, e.g., <ref type="bibr" target="#b47">[48]</ref>).</p><p>• Observational studies-these investigate some activity of interest in an exploratory, qualitative fashion, often through analysis of videotaped footage of humans interacting with AV technology (see, e.g., <ref type="bibr" target="#b24">[25]</ref>).</p><p>We believe that each of these alternative empirical methods can play a valuable role in helping us to gain insight into AV effectiveness. For example, usability tests can tell us whether an AV system's user interface is prevent-ing its users from reaping the benefits of a visualization (see, e.g., <ref type="bibr" target="#b48">[49]</ref>). Likewise, ethnographic field techniques and observational studies can help us understand how and why AV technology might be effective in a real classroom (e.g., <ref type="bibr" target="#b4">[5]</ref>), or in a realistic study session (e.g., <ref type="bibr" target="#b41">[42]</ref>). Questionnaires and surveys can help us understand AV technology users' preferences, opinions, and advice regarding AV technology design and use (see, e.g., <ref type="bibr" target="#b49">[50]</ref>). Thus, an important area for future research is to perform a meta-study that includes the published empirical studies that employ these alternative techniques to study AV effectiveness. According to our estimates, close to 30 such studies have been published. .</p><p>Other aspects of scenarios. The experimental studies in our corpus have maintained a narrow focus on specific parts of a given scenario of use. For example, in the "study" scenario considered by most of the experiments, the focus has been on students' use of the technology. The other tasks that must be done "behind the scenes," such as preparing the visualizations for student use and integrating the technology into a course curriculum, are left unexplored.</p><p>While the actual use of AV technology for learning is plainly the crux of any scenario of AV use, the studies' narrow focus on that use has prevented them from obtaining a broader perspective of effectiveness. Even if AV technology is found to be effective as a learning aid, a whole host of other considerations could figure equally prominently in an overall assessment of effectiveness. For example, how long did it take to prepare and set up the AV technology? Given the choice between using conventional materials and AV technology, which will instructors choose, and what considerations do they deem important in making that choice? In his ethnographic fieldwork, Hundhausen <ref type="bibr" target="#b4">[5]</ref> began to address these questions within the broader scope of an undergraduate algorithms course.</p><p>Other scenarios. Although AV has applications outside of educational scenarios-for example, in algorithms analysis and software engineering-our meta-study has neglected those scenarios. We are, in fact, unaware of a body of research that has empirically evaluated AV technology in non-educational scenarios. We suspect that this is because such research does not, in fact, exist. The empirical evaluation of AV effectiveness in non-educational scenarios is thus an important open area of research.</p><p>Notice also that AV is a subarea of software visualization, which encompasses not only the visualization of algorithms, but also the visualization of entire software systems. Visualizations of software systems are designed to help members of programming teams do such things as improve system performance <ref type="bibr" target="#b50">[51]</ref>, comprehend the structure and evolution of software systems <ref type="bibr" target="#b51">[52]</ref>, and track down bugs <ref type="bibr" target="#b52">[53]</ref>. A future meta-study would do well to consider the effectiveness of software visualization in such industrial scenarios. Pertinent guiding questions include:</p><p>• To what extent has software visualization been effectively applied in industry?</p><p>• Has software visualization increased productivity?</p><p>• Has software visualization benefited the large software teams typical in industry?</p><p>• Has software visualization benefited the kinds of distributed programming teams that are common today?</p><p>Our suspicion is that few empirical studies that address such questions have been published. If this is indeed true, then an important direction for future research is to subject industrial software visualization systems to the same kind of systematic effectiveness evaluation that educational AV technology has undergone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tables</head><p>SV TAXONOMY DESCRIPTIVE DIMENSIONS Myers, <ref type="bibr" target="#b53">[54]</ref> Aspect (code, data, algorithms) × Form (static, animated)</p><p>Shu <ref type="bibr" target="#b54">[55]</ref> What is visualized (data or information about data, program and/or execution, software design)</p><p>Brown <ref type="bibr" target="#b55">[56]</ref> Content (direct, synthetic) × Persistence (current, history) × Transformation (incremental, discrete)  Effort Equivalent </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effort Not Equivalent</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activity Equivalence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experim ental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significant Result</head><note type="other">Nonsignificant Result</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Instead, it holds that individuals construct their own individual knowledge out of their subjective experiences in the world. By becoming actively engaged with their environment, individuals actively construct new understandings by interpreting new experiences within the context of what they already know.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>illustrates, Cognitive Constructivism has been the most tested of the four theories (14 studies), with Epistemic Fidelity theory trailing close behind (10 studies), and Dual-Coding theory and Individual Differences theory lagging far behind. Moreover, notice that studies in support of Cognitive Constructivism have obtained the greatest number</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fidelity bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Coding, Individual Differences, and Cognitive Constructivism. By far the greatest number of studies have been guided by either Epistemic Fidelity or Cognitive Constructivism. The other two theories, Dual-Coding and Individual Differences, have not been explored thoroughly enough for definitive conclusions to be drawn. As our coursegrained analysis indicates, experimental manipulations of AV learner activities, indicative of adherence to Cognitive Constructivist theory, have had more significant and consistent impact on experiments' dependent variables than have experimental manipulations of AV representation, indicative of adherence to Epistemic Fidelity theory. More-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .Figure 4 .Figure 5 .</head><label>345</label><figDesc>Figure 3. Summary of the results of the 24 experiments in our corpus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Results of experimental studies vis-à-vis activity effort classification of Table 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Stasko &amp; Patterson<ref type="bibr" target="#b56">[57]</ref> Aspect × Abstractness × Animation × Automation Singh &amp; Chignell [58] What is visualized (program, algorithm, data) × Form (static, dynamic) Kraemer &amp; Stasko [59] Visualization task (data collection, data analysis, storage, display) × Visualization purpose (debugging, performance evaluation or optimization, program visualization) Roman &amp; Cox [60] Scope × Abstraction × Specification method × Interface × Presentation Price et al. [18] Scope × Content × Form × Method × Interaction × Effectiveness The descriptive dimensions of the eight extant taxonomies of AV and software visualization</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Figures</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Scenarios of AV use in Education</cell><cell></cell></row><row><cell>AV for</cell><cell>AV for</cell><cell>AV for</cell><cell>AV for</cell><cell>AV for</cell><cell>AV for</cell><cell>AV for</cell></row><row><cell>lectures</cell><cell>study</cell><cell>assign-</cell><cell>class</cell><cell>labs</cell><cell>office</cell><cell>tests</cell></row><row><cell></cell><cell></cell><cell>ments</cell><cell>discussion</cell><cell></cell><cell>hours</cell><cell></cell></row><row><cell cols="7">Figure 1. A taxonomy of scenarios of AV use in education</cell></row><row><cell cols="7">Research Techniques Used to Evaluate AV Effectiveness</cell></row><row><cell>Anecdotal</cell><cell cols="3">Programmatic</cell><cell>Analytic</cell><cell></cell><cell>Empirical</cell></row><row><cell cols="7">Figure 2. A taxonomy of methods for evaluating AV effectiveness</cell></row><row><cell>2 (8%)</cell><cell>1 (4%)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Non-significant result</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Signifcant result</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">10 (42%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Significant result in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">w hich contribution of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AV uncertain</cell></row><row><cell>11 (46%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Significant result in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">w rong direction</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that the one study that obtained negative significant result<ref type="bibr" target="#b26">[27]</ref> is counted as a non-significant result in the Epistemic</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Of course, these predictions assume that an experiment's learning materials are well designed and contain enough information to enable participants to perform well on the post-test. Indeed, if the experimental materials are deficient-for</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The first author wrote an early version of this paper as part of his Ph.D. comprehensive exam in the Computer and Information Science Department at the University of Oregon, and gratefully acknowledges the inspiration and astute guidance of his then-advisor, Sarah Douglas.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STUDY INDEP. VAR. DEPEND. VAR. KEY RESULTS</head><p>Price <ref type="bibr" target="#b25">[26]</ref> • Debugging medium (debugging with animated view vs.</p><p>debugging without animated view)</p><p>• Debugging time • Whether bug found • No significant differences were found.</p><p>Stasko et al. <ref type="bibr" target="#b14">[15]</ref> • Learning medium (text only vs. text-and-animation)</p><p>• Post-test accuracy • Non-significant trend was found favoring the text-and-animation group (t=1.111, df=18,p&lt;0.13) Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 4.4</p><p>• Data set size <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b24">25</ref>, or 41 elements) • Data representation style (horizontal/vertical sticks, dots)</p><p>• Post-test accuracy • No significant differences were found Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 5</p><p>• Order of algorithm presentation (Quick sort first vs.</p><p>Selection sort first) • Data representation style (labeled vs. unlabeled) • Covariates: Spatial and verbal abilities • Post-test accuracy • Time to take post-test</p><p>• No significant differences were found • Spatial and verbal abilities not correlated with performance Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 6</p><p>• Level of learner involvement (study text/passively view animation vs. study text /actively view by constructing own input data sets)</p><p>• Post-test accuracy • Time to take post-test • Participants who viewed animations for which they constructed their own data sets scored significantly higher on post-test Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 7</p><p>• Representation color (color vs. black-and-white)</p><p>• Representation labeling (algorithmic step labels vs. no labels)</p><p>• Post-test accuracy • Accuracy on a transfer task</p><p>• Participants who viewed black-and-white animations scored signficantly higher • Participants who viewed labeled animations scored significantly higher Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 8</p><p>• Order of medium (text-first vs. animation-first) • Order of algorithm presentation (selection sort first vs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kruskal MST first)</head><p>• Post-test accuracy</p><p>• No significant differences detected Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 9 • On post-test's "hard" questions, participants who viewed the animation and/or made predictions performed significantly better than participants who did neither. Byrne, et al. <ref type="bibr" target="#b16">[17]</ref> Study II</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Same as previous</head><p>• Same as previous • Same as previous, except that difference was detected on post-test's "procedural" questions, Gurka <ref type="bibr" target="#b19">[20]</ref> • Learning medium (animation vs. no-animation) (Note:</p><p>This experiment was an attempt to improve upon a porton of the Byrne, Catrambone, and Stasko <ref type="bibr" target="#b16">[17]</ref> experiment)</p><p>• Post-test accuracy • No significant differences were found between the two groups • Animation group appeared to be highly motivated by the animation Kann et al. <ref type="bibr" target="#b32">[33]</ref> • Level of learner involvement (program algorithm vs. • Level of Learner Involvement (Self-construct visualizations vs. actively view pre-defined visualizations)</p><p>• Accuracy and time on tracing and programming tasks</p><p>• No significant differences were detected Jarc et al. <ref type="bibr" target="#b31">[32]</ref> • Interactive prediction (use animation software that enbles prediction of next algorithm step vs. use animation software with no prediction)</p><p>• Post-test at end of 3 weekly lab sessions • Learning time</p><p>• No significant differences were detected on post-test • The prediction group spent significantly more time using the animation software than the no-prediction group   Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 6 Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. <ref type="bibr" target="#b4">5</ref> Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 9 Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. <ref type="bibr" target="#b6">7</ref> Crosby &amp; Stelovsky [28] Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. <ref type="bibr" target="#b7">8</ref> Byrne et al. <ref type="bibr" target="#b16">[17]</ref> Study I Gurka <ref type="bibr" target="#b19">[20]</ref> Byrne et al. <ref type="bibr" target="#b16">[17]</ref> Study II Mulholland <ref type="bibr" target="#b26">[27]</ref> Kann et al. <ref type="bibr">[</ref>   <ref type="bibr" target="#b15">[16]</ref> Ch. 9</p><p>Byrne et al. <ref type="bibr" target="#b16">[17]</ref> Study I Gurka <ref type="bibr" target="#b19">[20]</ref> Byrne et al. <ref type="bibr" target="#b16">[17]</ref> Study II Kann et al. <ref type="bibr" target="#b32">[33]</ref> Hundhausen &amp; Douglas [31] Jarc et al. <ref type="bibr" target="#b31">[32]</ref> Hansen et al. <ref type="bibr" target="#b13">[14]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POST-TEST ONLY PRE-TO POST-TEST IMPROVEMENT Stasko et al. [15]</head><p>Crosby &amp; Stelovsky [28] Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 4. <ref type="bibr" target="#b3">4</ref> Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 5 Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. <ref type="bibr" target="#b4">5</ref> Hansen et al. <ref type="bibr" target="#b13">[14]</ref> Study II Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. <ref type="bibr" target="#b5">6</ref> Hansen et al. <ref type="bibr" target="#b13">[14]</ref> Study III Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. <ref type="bibr" target="#b6">7</ref> Hansen et al. <ref type="bibr" target="#b13">[14]</ref> Study IV Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. <ref type="bibr" target="#b7">8</ref> Hansen et al. <ref type="bibr" target="#b13">[14]</ref> Study V Lawrence <ref type="bibr" target="#b15">[16]</ref> Ch. 9</p><p>Hansen et al. <ref type="bibr" target="#b13">[14]</ref> Study VI Gurka <ref type="bibr" target="#b19">[20]</ref> Hansen et al. <ref type="bibr" target="#b13">[14]</ref> Study VII Kann et al. <ref type="bibr" target="#b32">[33]</ref> Hansen et al. <ref type="bibr" target="#b13">[14]</ref> Study VIII Byrne et al. <ref type="bibr" target="#b16">[17]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method of Measuring Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experim ental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significant Result</head><p>Non-Significant Result </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two systems which produce animated representations of the execution of computer programs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGCSE Bulletin</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="158" to="167" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Algorithm animation The</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TANGO: A framework and system for algorithm animation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="27" to="39" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using student-built animations as learning aids</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Technical Symposium on Computer Science Education</title>
		<meeting>the ACM Technical Symposium on Computer Science Education<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="25" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Toward effective algorithm visualization artifacts: Designing for participation and communication in an undergraduate algorithms course</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Hundhausen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer and Information Science, University of Oregon</orgName>
		</respStmt>
	</monogr>
	<note>Unpublished Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Animated algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gloor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software visualization: Programming as a multimedia experience</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Domingue</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Testing effectiveness of algorithm animation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Gurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Citrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 IEEE Symposium on Visual Languages</title>
		<meeting>the 1996 IEEE Symposium on Visual Languages<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="182" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Algorithm visualization in computer science laboratories</title>
		<author>
			<persName><forename type="first">T</forename><surname>Naps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st SIGCSE Technical Symposium on Computer Science Education</title>
		<meeting>the 21st SIGCSE Technical Symposium on Computer Science Education<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sorting out sorting: A case study of software visualization for teaching computer science</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Visualization: Programming as a Multimedia Experience</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Domingue</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="369" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual programming of program visualizations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Duisberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 1987 Visual Language Workshop</title>
		<meeting>the IEEE 1987 Visual Language Workshop<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graphical specification of algorithm animations with ALLADDIN</title>
		<author>
			<persName><forename type="first">E</forename><surname>Helttula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyrskykari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp; K.-J</forename><surname>Raiha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference on Systems Sciences</title>
		<meeting>the 22nd Annual Conference on Systems Sciences</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="892" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using Direct Manipulation to Build Algorithm Animations by Demonstration</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CHI&apos;91 Conference on Human Factors in Computing Systems</title>
		<meeting>ACM CHI&apos;91 Conference on Human Factors in Computing Systems<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Empirically evaluating the use of animations to teach algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Badre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 IEEE Symposium on Visual Languages</title>
		<meeting>the 1994 IEEE Symposium on Visual Languages<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Helping learners visualize and comprehend algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schrimpsher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactive Multimedia Electronic Journal of Computer-Enhanced Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Do Algorithm Animations Assist Learning? An Empirical Study and Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM INTERCHI&apos;93 Conference on Human Factors in Computing Systems</title>
		<meeting>ACM INTERCHI&apos;93 Conference on Human Factors in Computing Systems<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="61" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Empirical studies of the value of algorithm animation in algorithm understanding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Georgia Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Unpublished Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating animations as student aids in learning computer algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Catrambone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Education</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="253" to="278" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A principled taxonomy of software visualization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Baecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="211" to="266" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Software visualization in teaching at Brown University</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bazik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tamassia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><forename type="middle">A</forename><surname>Van Dam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Visualization: Programming as a Multimedia Experience</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Domingue</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="383" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pedagogic Aspects of Algorithm Animation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Gurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science, University of Colorado</orgName>
		</respStmt>
	</monogr>
	<note>Unpublished Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Use of Structure Editing Tools by Novice Programmers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Goldenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Studies of Programmers: Fourth Workshop</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="99" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An evaluation of the Pavane visualization system</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Roman</surname></persName>
		</author>
		<idno>WUCS-94-09</idno>
		<imprint>
			<date>April, 1994 1994</date>
			<pubPlace>Washington University in St. Louis, St. Louis, MO</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding usability problems through heuristic evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CHI&apos;92 Conference on Human Factors in Computing Systems</title>
		<meeting>ACM CHI&apos;92 Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="373" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cognitive Walkthroughs: A Method for Theory-Based Evaluation of User Interfaces</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rieman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wharton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Man-Machine Studies</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="741" to="773" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Methodological issues in the study of programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Gilmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Programming</title>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Hoc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">R G</forename><surname>Green</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Samurcay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Gilmore</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="1990">1990</date>
			<publisher>Academic Press</publisher>
			<pubPlace>San Diego</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A framework for the automatic animation of concurrent programs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Unpublished M.S. Thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A principled approach to the evaluation of SV: a case study in Prolog</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mulholland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software visualization: Programming as a multimedia experience</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Domingue</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="439" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From multimedia instruction to multimedia evaluation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Crosby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stelovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Multimedia and Hypermedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="147" to="162" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Hedges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Olkin</surname></persName>
		</author>
		<title level="m">Statistical Methods for Meta-Analysis</title>
		<meeting><address><addrLine>Orlando</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Empirical studies of information visualization: a meta-analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">international Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="851" to="866" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using visualizations to learn algorithms: Should students construct their own, or view an expert&apos;s?</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Hundhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2000 IEEE International Symposium on Visual Languages</title>
		<meeting>2000 IEEE International Symposium on Visual Languages<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Assessing the benefits of interactive prediction using webbased algorithm animation courseware</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jarc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings SIGCSE</title>
		<meeting>SIGCSE<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="377" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Integrating algorithm animation into a learning environment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lindeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Education</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="223" to="228" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Artificial Intelligence and Tutoring Systems Morgan Kaufmann</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wenger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Los Altos, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Designing for conversations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Roschelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Symposium on Knowledge-Based Environments for Learning and Teaching</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Human Problem Solving Prentice-Hall</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<pubPlace>Englewood Cliffs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Animations need narrations: An experimental test of a dual-coding hypothesis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="484" to="490" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">In: Imagery, memory, and cognition: essays in honor of Allan Paivio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paivio</surname></persName>
		</author>
		<editor>J. C. Yuille</editor>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<pubPlace>Hillsdale, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>The empirical case for dual coding</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Cooper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Individual Differences Oxford Illustrated Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Riding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rayner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cognitvie Styles and Learning Strategies David Fulton Publishers</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Resnick</surname></persName>
		</author>
		<title level="m">Introduction. In: Knowing, Learning, and Instruction: Essays in Honor of Robert Glaser</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Resnick</surname></persName>
		</editor>
		<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the evaluation of algorithm animations as learning aids: an observational study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kehoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="265" to="284" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Plans and Situated Actions: The Problem of Human-Machine Communication Cam</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Suchman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>bridge University Press</publisher>
			<biblScope unit="page">203</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Situated Learning: Legitimate Peripheral Participation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wenger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">138</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring human visualization of computer algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Hundhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1996 Graphics Interface Conference Canadian Graphics Society</title>
		<meeting>1996 Graphics Interface Conference Canadian Graphics Society<address><addrLine>Toronto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Handbook of Usability Testing John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ethnography</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sanjek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedic dictionary of social and cultural anthropology</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Barnard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Spencer</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Raitledge</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Foddy</surname></persName>
		</author>
		<title level="m">Constructing Questions for Interviews and Questionnaires: Theory and Practice in Social Research</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A Pilot Study of Early Usability Evaluation Methods for Software Visualisations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lavery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cockton</surname></persName>
		</author>
		<idno>FIDE/95/141</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Glasgow, Scotland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">FIDE Technical Report</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Assessing program visualization systems as instructional aids</title>
		<author>
			<persName><forename type="first">A</forename><surname>Badre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baranek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Assisted Learning, ICCAL &apos;92</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Tomek</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="87" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Strata-Various: Multi-layer visualization of dynamics in software system behavior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kimelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Visualization &apos;94</title>
		<meeting>Visualization &apos;94<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="172" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Space-filling software visualization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Eick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Linked-list visualization for debugging</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shimomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Isoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="44" to="51" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Taxonomies of visual programming and program visualization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Visual programming Van Nostrand Reinhold</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Shu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Perspectives on algorithm animation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCHI &apos;88 Converence on Human Factors in Computing Systems</title>
		<meeting>the ACM SIGCHI &apos;88 Converence on Human Factors in Computing Systems<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding and characterizing software visualization systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1992 IEEE Symposium on Visual Languages</title>
		<meeting>the 1992 IEEE Symposium on Visual Languages<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Components of the visual computer: A review of relevant technologies</title>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Chignell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Computer</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="115" to="142" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The visualization of parallel systems: An overview</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kraemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A taxonomy of program visualization systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
