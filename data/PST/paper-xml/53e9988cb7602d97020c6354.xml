<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Applicatkons of Universal Context Modeling to Lossless Compression of Gray-Scale Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Marcelo</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jorma</forename><forename type="middle">J</forename><surname>Rissanen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ronald</forename><forename type="middle">B</forename><surname>Arps</surname></persName>
						</author>
						<title level="a" type="main">Applicatkons of Universal Context Modeling to Lossless Compression of Gray-Scale Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB673FBAE5C0FA5B37C00256424A8FAB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by theoretical results on universal modeling, a general framework for sequential modeling of gray-scale images is proposed and applied to lossless compression. The model is based on stochastic complexity considerations and is implemented with a tree structure. It is efficiently estimated by a modification of the universal Algorithm Context. Several variants of the algorithm are described. The sequential, lossless compression schemes obtained when the context modeler is used with an arithmetic coder are tested with a representative set of gray-scale images. The compression ratios are compared with those obtained with state-of-the-art algorithms available in the literature, with the' results of the comparison consistently favoring the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>OST of the literature in gray-scale image compression M deals with lossy schemes for which the original pixel intensities cannot be perfectly recovered from the encoded bit stream. However, many applications where the pictures are subjected to further processing, e.g., for the purpose of extraction of specific information, require lossless compression. The lossless (or "noiseless") requirement implies that the coding algorithms yield dec'ompressed images identical to the original digitized images. Thus, images from digital radiology in medicine or from satellites in space are usually compressed by reversible methods. Lossless compression is generally the choice also for images obtained at great cost, in applications where the desired quality of the rendered image is unknown at the time of acquisition or in applications where intensive editing or repeated compressionldecompression are required.</p><p>Gray-scale images are coiisidered as 2-D arrays of intensity values, digitized to some number of bits. In most applications eight bits are used, although 12 bits is customary in digital radiology. Color images, in tum, are usually represented in some color space (e.g., RGB, YUV, LAB), in which each component is a gray-scale image. Thus, the tools employed in the compression of color images are derived from those Manuscript received <ref type="bibr">April 24, 1994</ref>; revised July 10, 1995. The associate editor coordinating the review of this paper and approving it for publication was Prof. Dr.-Ing. Bemd Girod. developed for gray-scale images, and our discussions will focus on the latter. It should be noted, though, that the combination of these tools in the case of color should take into account the possible correlation between color planes (e.g., in an RGB representation).</p><p>Lossless image compression, when performed sequentially (i.e., pixel by pixel in some predefined order, e.g., rasterscan, as opposed to multiple-pass schemes), can be formulated as an inductive inference problem, in which a conditional probability distribution of future data is learned from the past <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr">[2]</ref>. At each time instant i and after having scanned past data x2 = 21x2 . . . x;, one wishes to make inferences on the next pixel value Z ~+ I by assigning a conditional probability distribution p ( . I x') to it.' In the long run, the goal is to maximize the probability assigned to the entire sequence n-1 P(x") = rI P(Zi+l I X i )</p><p>(1.1) where zo denotes the empty string, also written as A. This probability assignment finds its applications not only in coding for data compression <ref type="bibr" target="#b20">[19]</ref> but also in a variety of problems in signal processing and information theory (see, e.g., <ref type="bibr">[ 111 and</ref> the references therein). In noiseless coding, -log P ( 9 ) is the code length of a Shannon code, based on the above probability assignment, which can be implemented sequentially by arithmetic coding <ref type="bibr">[ 191.</ref> Clearly, a good inference procedure that induces a high probability P ( F ) also yields a short code to the given input. Since the conditional probability used to encode x;+1 depends only on xi, it is available to the decoder as it decodes the past string sequentially. Moreover, the correspondence between probability assignments in the above described sense and sequential, noiseless coding schemes is one-to-one: Any such scheme implicitly induces (or explicitly uses) a probabilistic model of the data, where the distribution employed in the encoding of xi+l depends only on xi. An example is the Lempel-Ziv algorithm <ref type="bibr" target="#b34">[33]</ref>, whose underlying model is analyzed in <ref type="bibr">[7]</ref>.</p><p>Inspired by theoretical results on universal modeling [ 191, <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b30">[29]</ref>, this work addresses the problem of finding "good" models for gray-scale images, the main application in which these models are tested being sequential, lossless compression. Starting from a coding scheme that is universal for a broad class of random processes (in the information theoretic sense i=O 'Notice that pixel values are indexed with only one subscript, despite corresponding to a 2-D array. This subscript denotes the "time" index in the predefined order. 1057-7149/96$05.00 0 1996 IEEE that the scheme is asymptotically optimal for any process in the class) [24], <ref type="bibr" target="#b30">[29]</ref>, we first aimed at modeling actual images as samples of such processes. We should notice, however, that by fitting a model we are not equating real images with random processes; ultimately, our goal is to find a compact representation of a specific image. The theoretical framework of universality guarantees that $ the image were indeed a sample of any process in this broad class, then it would be optimally compressed. Thus, the achieved compression is a measure of how well this model fits the image. Furthermore, facts of real images are that, in order to improve compression, it was necessary to reduce the model class in which the scheme can claim universality. Although some of the components of this decision are necessarily ad-hoc, the resulting, still quite broad, model class retains much of its theoretical framework.</p><p>Apart from "squeezing" as much information out of the data as possible, we require from "good" models reasonable storage resources and execution times. While these and other practical constraints are of concern, a good fit of the model to the image remains our main purpose, which is uniformly achieved as suggested by our comparison with other schemes. For many practical applications it may well happen that the advantage of a more universal model, optimized following the ideas presented in this work, over simpler models, does not justify the increase in complexity. In fact, while for some of our test images we obtain a 15% improvement on the compression ratios over the best available published results, for others the gains are less substantial. This suggests that in the later cases the intuitive nonoptimized schemes we tested take care of most of the fit that can be obtained, and that in many instances more complex models may not be worth the cost. However, before we raise this question we must estimate what the best attainable fit really is in order to evaluate such a simplified model. Hence, a reasonably complex "universal" model (in the sense stated above) remains of practical interest. Moreover, some of the concepts introduced in this modeling approach may have an intrinsic interest in other image processing applications (e.g., segmentation, denoising).</p><p>Basically, three steps can be identified in the search for a good model:</p><p>1) The choice of a family of model classes that can suitably represent the data. Examples are the broad class of finitestate (FS) models, widely studied in the information theoretic literature (see, e.g., <ref type="bibr" target="#b30">[29]</ref>), the more restricted autoregressive (AR) models on which predictive coding <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b26">[25]</ref> is based, or a combination of both, as will be considered here. For an FS model the definition of the states is also part of this process, which is mostly based on intuition and the type of data to be processed.</p><p>2) The selection of a model order. This step is based on the concept of stochastic complexity <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b23">[22]</ref> and it is essentially aimed at preventing "overfitting" the model. This phenomenon is best described by Rissanen's lower bound <ref type="bibr" target="#b24">[23,</ref><ref type="bibr">Theorem 11</ref>, which states the following: For any model family indexed by a parameter-vector theta in a K-dimensional compact set and for all vectors theta, except a set of Lebesgue volume zero, the expected code length -,!$heta log P(z") associated with any probability assignment P(.), where the expectation is taken with respect to a model that assigns a probability</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pthet,(.), satisfies</head><p>Roughly speaking, the left-hand side of (1.2) represents the fit of the model to the data and the inequality implies that choosing a model whose complexity K is unnecessarily large (i.e., a model for which the same fit can be achieved with fewer scalar parameters) will negatively affect the performance, since the "model cost" of each parameter is 0.5 log R bits (hereafter, the logarithms are taken to the base two). Intuitively, in an FS model this phenomenon can be interpreted as capturing the penalties of "context dilution" occurring when count statistics must be spread over too many states, which affects the accuracy of the corresponding estimates. In fact, this overfitting of data problem, in the spirit of Occam's Razor, was well known by scientists long before it was quantified by (1.2). Data compression engineers have also referred to it as "the sparse context problem" [9]. In nonsequential (two-pass) schemes, the model cost represents the code length required to encode the model parameters estimated in the first pass, which must be transmitted to the decoder. 3) Given the model and its order, choose the best parameters that fit the data. Note that this is a task to be performed sequentially (in one pass), so that the maximum-likelihood estimate based on the entire data is not available. Instead the parameters must be estimated from the data processed so far, which in fact is the basis of the model cost described above. As can be inferred from Step 2, the simplicity of the model and the fit to the data are not necessarily contradicting requirements. However, since the optimal number of parameters is unknown and must be estimated on-line from the data, it may be significantly smaller than the number of parameters corresponding to the order of the statistics that are being collected. In this sense, the determination of the maximal search space (namely, the maximum possible value of K ) is indeed a matter of computational resources.</p><p>While imaginative models have already been used for lossless compression of gray-scale images (see, e.g., <ref type="bibr" target="#b29">[28]</ref>, [9],</p><p>[12], and [IS]), the ideas of Step 2 have not been exploited and no systematic modeling tool has typically been used. On the other hand, an effective general tool, the so-called Context algorithm, was proposed in <ref type="bibr" target="#b25">[24]</ref> to incorporate the stochastic complexity considerations derived from (1.2). In <ref type="bibr" target="#b30">[29]</ref> it was proved that the algorithm eventually captures the optimal model cost for the class of tree models, which includes Markovian models as a special case. However, this class of models is inadequate whenever the size of the alphabet is very large and does not exploit prior knowledge of the smoothness that characterizes gray-scale images. Thus, modifications are proposed here in order to incorporate improvements in the aim for the first modeling step described above. As mentioned before, this implies a reduclion of the model class involving some ad-hoc decisions, which are guided by the ideas of an initial state. Then, recursively, universal modeling. As a result, a general framework for S(Zi+l) = f ( s ( z i ) , z;+1) (2.3) modeling gray-scale images inspired by Algorithm Context is proposed, and several implementations of the corresponding data compression algorithms are described. Note that the terminology "context modeling" has been widely used in the data compression literature to refer to models in which the encoding of each symbol is conditioned on the "context" or "state" in which it occiirs (see, e.g., <ref type="bibr" target="#b29">[28]</ref> and <ref type="bibr" target="#b19">[18]</ref>1 for image compression, and [5] and the references therein for text compression). However, these works do not address the above stochastic complexity considerations. Here, we reserve the term "universal context mod(e1ing" to techniques derived from schemes that are provably optimal, in the sense of attaining (1.2) for some broad class of models.</p><p>The remainder of this paper is organized as follows. In Section 11, we provide the information theoretic background on which our modeling approach is inspired. Algorithm Context is reviewed together with the closely related tree models, and its drawbacks when applied to gray-scale images are analyzed. In Section 111, an alternative model for gray-scale images is proposed, while in Section IV the corresponding modeler (and variations thereof) is described. Finally, in Section V, compression results are reported for standard image sets as well as for medical and sa.tellite images. These results are compared with the performance of existing algorithms, and the relation with transform coding techniques <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b28">[27]</ref> is discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">TREE MODELS AND1 THE CONTEXT ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tree Models</head><p>A sequential probabilistic model (or information source) over a finite alphabet A of Q! symbols is defined by a family of probability measures P,(x"), n = 0,1,. . . , each P, defined on the set A" of all strings zn = x1x2 . . .z, of length n, such that the marginality condition P,+l(zna) = Pn(zn)</p><p>(2.1) aEA is satisfied for all n 2 0, where P0(xo) a 1. As a rule, we write P,(z") as P(x"), omitting the subindex.</p><p>In applications, a reduced description of the probability function P( xn) becomes imperative for an efficient implementation, due to the size of the function domain, namely the set of all (long) sequences. This is often obtained by imposing an additional constraint on the conditional probability function p(z,+l = a I 2") = P(z"&lt;u)/P(x"), denoted p ( a I x") for short, which in view of (2.1) is well defined. Specifically, it is required that the Conditional distribution depend on zn only through a function s(zc") with finite range S , a fixed state space of cardinality k . Thus, we denote</p><formula xml:id="formula_0">P(" I x n ) = P(a I S ( x n ) ) (2.2)</formula><p>where f , mapping S x A into S , is the state transition map (or "next-state'' function) of the machine. The source is then defined by k x ( Q! -1) conditional probabilities and the function f ( s , x) together with the initial state. Such sources are called finite-state machine-generated (FSM). However, fitting FS models to image data is still a complex matter, especially when one wants to search through machines with different numbers of states, because of the large number of possible state transition maps to be explored. There is an important class of sources we call finite memory or tree sources <ref type="bibr" target="#b30">[29]</ref>, which can be estimated much more conveniently using a modification of Algorithm Context <ref type="bibr" target="#b25">[24]</ref> described below. These sources are characterized by the property that the conditional probability of the next emitted symbol, given all the past, depends only on a finite number of contiguous past observations, the smallest number of which is denoted by m. Thus, the function ~( 2 " ) not only has a finite range S, but also has the form of a finite (reversed) string Based on the stochastic complexity concerns, summarized in (1.2) in terms of the model cost, optimal convergence requires that the right-hand side string in (2.4) be the shortest one satisfying (2.2). However, fitting a plain Markov model of minimal order m to the data is not the most efficient way to estimate a finite-memory source, since in many instances of practical interest there exist equivalent states (i.e., mvectors) that yield identical conditional distributions. Thus, the number of states, which grows exponentially with m in a Markov model, can be dramatically reduced by removing redundant parameters after lumping together equivalent states. The reduced models can be represented with a simple tree structure, which is described in detail in <ref type="bibr" target="#b30">[29]</ref> and whose basics are reviewed next.</p><p>Before proceeding it is necessary to point out that for 2-D data, the relevant past string, defined by the shortest equivalent suffix of the right hand side of (2.4), may not be given by the symbols scanned in the immediate past. Rather, the relevant symbols are usually given by the physical closeness to x,+1, although other properties of the data might be considered as well. Thus, this past sequence should be regarded as the one obtained after reordering the data scanned prior to x,+1 in a suitable way, although our notation will not make this reordering apparent. For example, if an image is scanned row by row from the top to the bottom and from left to right, and m = 6, then the pixel x,+1 situated at the array coordinates ( i , j ) , i &gt; 1, j &gt; 1, occurs at state for all a E A. One way of specifying the function s(.) is by a finite-state machine (FSM) als follows. First, s(X) = so selects This is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. In general a state function defined as in (2.5) does not satisfy (2.3) (as s ( P ) is not uniquely current pixel value for some j , O 5 j 5 m, not necessarily the same for all strings (the sequence of indices is decreasing, except for the case j = 0 which is interpreted as defining the empty string A; thus, we write the past symbols in reverse order). In a minimal representation of the model, s(x") is the shortest suffix of xn (or context) satisfying (2.2). The "terminal contexts" s(zn) act as states, but they do not satisfy (2.3). Now, consider a complete2 wary tree whose branches are labeled by the symbols in the alphabet. Each context defines a node in the tree, reached by taking the path starting at the root with the branch z , , followed by the branch ~" -1 , and so on. Thus, the range S is given by the set of leaves of a complete subtree T . To specify a source we need, in addition, a set of probability distributions {p(a I s): s E S } , p ( a I s) &gt; 0 for all a E A and s E S. Much as a prefix code permits decoding of codewords without commas, the tree T permits finding the context of each symbol xi in the string xn and an implementation of the probability P(x") as for any string x n (a detailed description of how this is done for the very first symbols in the string, for which a leaf in the tree may not be defined, is given in <ref type="bibr" target="#b30">[29]</ref>). In particular, a Markov model of order m requires a perfectly balanced tree T of depth m. For the sake of simplicity, in the sequel, T will denote the whole model, i.e., both the tree and its associated conditional probabilities.</p><p>A tree T is called minimal <ref type="bibr" target="#b30">[29]</ref> if, for every node w in T such that all its successors wb are leaves, there exist a , b, c E A satisfying p(a I wb) # p ( a 1 we). Clearly, if for some such node w the distributions p ( . I wb) are equal for all b, we could lump the successors into w and have a smaller complete tree T representing the same process. Thus, a minimal tree guarantees that the children of such a node w are not all equivalent to it, and hence they cannot be replaced by the parent node. Notice that even in a minimal tree there may still be sets of equivalent leaves, not necessarily siblings, having the same associated conditional probability. These equivalent 'An a-art tree T is said to be complete if each node either is a leaf or has exactly cy children; T is perfectly balanced if it is complete and all of its leaves are a1 the same depth. nodes could, in principle, be lumped together, thus reducing the number of parameters in the model. However, such a reduced parametrization may no longer admit a simple tree implementation nor a practical construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithm Context</head><p>Algorithm Context, introduced in <ref type="bibr" target="#b25">[24]</ref>, improved in [4], and further analyzed in <ref type="bibr" target="#b30">[29]</ref>, provides a practical means to estimate tree sources, thus maximizing (2.6) (at least in a probabilistic sense) through a sequence of Markovian predictive probability assignments. We start by describing the algorithm as a general purpose scheme, not customized for image compression. Later on, we will focus on image data. The algorithm has two interleaved stages, the first for growing a large tree that contains the tree model to be estimated, and the second for selecting from the grown tree a distinguished context to define the state s(xi) for each i &gt; 0. The past string for xi+l is derived from the scanned sequence xi by a given, fixed rule (a "causal template" with respect to the scanned direction, as in Fig. <ref type="figure" target="#fig_0">1</ref> for image data). The algorithm grows the tree and updates a set of occurrence counts by the following rules. 0) Start with the root with its a symbol counts all zero. This tree TO represents the empty context A.</p><p>1) Recursively, having constructed the tree Ti (which may be incomplete) from x2, read the symbol z;+1. Traverse the tree along the path defined by the past string zizi-1 . . . and increment the count of symbol z i + l by one for every node visited until the deepest node (i.e., the farthest from the root), say xi . . . zi-j+l, is reached.</p><p>2) If the last updated count is not less than a preselected threshold no (in practice, no = 2 is used) and j is smaller than a preselected maximal context length M , create a new node xi . ' . xi-j, and initialize its symbol counts to zero, except for the symbol xi+l whose count is set to 1. This completes the construction of Ti+l. The goal of this stage is to accumulate all the relevant contexts and the associated symbol statistics as the length of the string grows. The tree will grow only in directions where repeated symbol occurrences take place (up to a depth M , given by the size of the causal template), and the counts of all the symbols in all the contexts that have occurred are gathered, except a few early occurrences prior to the creation of the corresponding node, depending on no. While letting the tree grow, the algorithm also selects a certain distinguished context for each symbol xi+l in the data sequence. Although there are several variants of the rule for the "optimal" context selection, all of them are based on a predictive argument: the context that would have sequentially assigned the largest probability for its symbol occurrences in the past string should be selected for encoding. The probability of occurrence of a symbol a E A in a selected context s may suitably be taken [6] as where ni(b I s) denotes the occurrence count of b E A at context s, in the tree Ti. This rule, which ensures a pointwise convergence to the entropy at an optimal rate, is seen to be a slight modification of Laplace's rule of succession. However, other variations of Laplace's rule may be preferable in practice [321, especially for large alphabets for which only a small subset of the a possible symbols actually occurs. Now, finding the context that would have performed best in the past by comparing all the nodes of T, along the path starting at the root, is generally a rather elaborate process. In practice, this is feasible only when the maximal possible size of the tree (which is a function of a and of the maximal searching space M ) is small enough to allow a backward pruning algorithm (from thle leaves toward the root), based on dynamic programming as described in [ 141. Instead, optimality in a probabilistic sense, to be specified later, can be achieved by comparing pairs of successive nodes only. This can conveniently be done by a method proposed in <ref type="bibr">[4]</ref>, which stores at each node an updated index representing the "efficiency" difference between the node and its parent node. Specifically, for a node s in the tree T, other than the root, let s' denote its parent. Let I ( s ) denote the set of time indexes t such that s is a suffix of z t , 0 5 t &lt; i Then, the efficiency index E ( s , i )</p><p>corresponding to s at time i is defined by</p><p>In words, the efficiency index represents the sum of the differences between the predictive code lengths accumulated at s and s' each time s is visited. Note that this excludes other possible visits to s'. If the difference (2.8) is negative, s has been more efficient than its parent to encode the past symbol occurrences at times t 6 Jr(s). This difference is recursively accumulated for all the past symbol occurrences in s. As for the root node, E(X, i ) is kept at a fixed negative value. Notice that the counts involved in the computation of p(xt+l I s ) in (2.8) include symbol Occurrences at s only, while the counts used for p(zt+l I s') contrtbute occurrences at the siblings of s as well. This does not necessarily imply a negative index because the code lengths are calculated in a predictive way. Now, in practice one may want to forget the remote past when updating E ( s , i ) , in order to account for the possible "nonstationarity" of the data. Although the significance of this phenomenon is diminished by the use of conditioning states in the model, a recursive updating rule that has proved useful to forget the remote past [4.] is obtained by including a simple "thresholding" for every suffix s of zz in TZ-l, namely where, for suitably selected thresholds bo &lt; 0 &lt; bl, the function w(.) is defined as</p><formula xml:id="formula_1">bl if z &gt; b l ~( z ) = bo if z &lt; bo (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10)</head><p>.I x otherwise.</p><p>Thus, instead of waiting until the cumulative difference (2.8) changes its sign when a new and better encoding node emerges due to the nonstationarity-which may happen too late as it would take repeated symbol occurrences at this node to discharge the cumulated index-we speed up the process by setting the above upper and lower thresholds. When a node is created, its efficiency index is initialized at the maximum value bl. The rule (2.8) corresponds to the case bo = -00, bl = 00.</p><p>Finally, we choose the encoding node s * ( z i ) for the i + 1-st symbol as the deepest node with negative efficiency index in the path xixi-1 . . . starting at the root of Ti. Consequently, the total probability assigned to the entire sequence xn by Algorithm Context is given by For an easier-to-analyze version of the algorithm, it was shown in <ref type="bibr">[29,</ref> Theorem 11 that this assignment is asymptotically optimal in the strong (probabilistic) sense that it reaches the stochastic complexity bound (1.2) in the class of tree models. By asymptotic optimality, it is meant that for every parameter-vector theta, the entropy -Etheta log Ptheta( xn) is achieved at the best possible rate, given by (1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Drawbacks</head><p>Although optimal in the above sense, the algorithm just described is not suited to encode data over very large alphabets, especially when the symbol values have a "physical" meaning (like pixel amplitude), which defines a "metric" and an associated notion of smoothness. This became apparent after experimental results showed its relatively poor performance in compressing gray-scale images. Due to its universality, the modeling algorithm will eventually capture these statistical features. However, the model cost involved may be significantly higher than the one obtainable with a definition of the tree model that makes use of this prior knowledge. In other words, of the three steps described in Section I regarding the search for a good model, Step 1 is failing. In particular, the following drawbacks can be identified. a) While the model cost is proportional to ( a -l)k, the algorithm concentrates on reducing k , treating a as a constant. However, when Q: is large, it may represent the most serious component of this cost. b) While the model optimally "prunes" the tree relative to its depth, it does not take care of siblings that could otherwise be partially lumped together (i.e., in this model all siblings are either lumped together or are not lumped at all). Clearly, such "partial lumping" cannot occur with binary data, but the phenomenon is very common with smooth gray-scale images, where it is very likely that if a1 and a2 are close pixel amplitude values, the distributions p ( . I u l ) and p ( . I u2) are identical, while for some amplitude values b such that Ib -a1 I is large p ( . I b) differs from them.</p><p>c) Smooth gray-scale images are also characterized by similar distributions for distant siblings in the grown tree but which are centered at different values. The algorithm, in its described form, is not capable of capturing these dependencies in order to reduce the model cost accordingly.</p><p>111. PROPOSED IMAGE MODELS In view of the concluding remarks of Section 11-C, we need to model the data in a different manner, which leads to significant savings in the model cost. Although the universality of Algorithm Context guarantees convergence to the compressibility of the data, a reduction of the model cost, which by (1.2) results in speeding the convergence rate, is still important for actual images. For example, it is widely accepted that a useful heuristic for modeling gray-scale images is prediction. While prediction is clearly beneficial when followed by a zeroorder entropy coder <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b26">[25)</ref>, at first sight its contribution might seem dubious when followed by a universal encoder (like Algorithm Context), since the same information that is used to predict is also available for building the compression model. However, experimentation shows that prediction indeed contributes to improving compression performance. Here, we generalize these types of ideas and formalize them in terms of modeling, as means to reduce the model cost. Later on in Section IV we present algorithms that are used to estimate the proposed models sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition of a General Model</head><p>First, we present the modeling tools that are used in building the global model. The paragraphs below are labeled in oneto-one correspondence with the model drawbacks considered in Section 11-C. a) Since the conditional pixel distributions of most images are expected to be smooth, a parametric model (where the number of parameters is reduced from 01 to, say, a couple) or a histogram estimator in the spirit of <ref type="bibr" target="#b21">[20]</ref>, are the obvious choices for reducing the degrees of freedom in the model class. The alphabet A is assumed to be the set of natural numbers (0, I,. . . , a -l}, where 01 is a power of two. On the other hand, parametrized distributions are usually real-valued. Thus, for a realvalued random variable U with cumulative distribution function F , we denote by D ( U ) the discretized variable taking values on A with probability law a F ( a + 0.5) -F ( a -0.5)</p><formula xml:id="formula_2">Prob{D(U) = a } = (3.1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ( o ~</head><p>-0.5) -F(-0.5) i.e., the probability of a is obtained by integrating in the region [ U -0.5, a+0.5], and by renormalizing so that the probability measure of the interval [-0.5, a-0.51 is one. b) "Partial lumping" can be obtained by assuming that the contexts are formed out of quantized3 pixel values. In a naive approach, this could be accomplished by uniformly dropping the least significant bits in all the contexts. However, the result is highly dependent on the quantization, and the optimal number of bits to be dropped varies quite drastically with the type of data. A universal algorithm should be able to estimate this optimal number, which, with the said approach, cannot be done on-line. Here, we build the various contexts out of individually quantized pixel values. To this end, let q(zi) denote a sequence of quantized values</p><p>Beyond the original quantization from digitization of the pixels in the past string xi. Even though each pixel is individually quantized (scalar quantization), the quantizing functions for x3 and z1, j # I , may differ.</p><p>The term "quantization" may suggest lossy compression, which is not the subject of our work, but the use of q ( . ) is limited to the definition of contexts only. Later on, after adding a constraint on the quantizing function, we will show how this function can be efficiently estimated. c) Lumping together several states via tree pruning (or via quantization given the approach suggested above) may cause similar peaked distributions centered at rather different values to collapse into a single flat one. By centering the distributions around a fixed reference point, this effect is avoided. This is done via prediction as follows. Let T be a positive integer and let s(.) be a state function as defined in (2.2). Given the past string z2, the next pixel x,+1 is modeled as a sample of a random process. This process is obtained by first predicting the pixel following the string z2-,+1 . . . z i at the state s(q(zi)) determined by the past quantized string, and then adding to the result a random process (the "noise") representing a parametrized distribution for prediction errors. Next, we summarize the proposed global model. For a state s E S , let g(")(.) denote a real-valued "predictor" function whose domain is the set A' of r-tuples over A. Let U(") denote a real i.i.d. stochastic process with zero-mean and density function f(")(.). Then, we model the image according to the process In words, the new process is obtained by adding a statedependent i.i.d. random process to the predicted value and discretizing the sum. Note that although each process U(") is i.i.d., the relevant process varies according to the state function, which makes the total process nonindependent even if g(")(.) was just a constant. In addition, we require that g ( s ) ( a , a , . . . , a ) = a (3.3) for every a E A. This condition, together with the zero mean of the processes U("), establishes the assumption of smoothness of the image, which otherwise would have to be learned from the statistics with an increased model cost as a result. The functions g(")( .) act as state-dependent, parametric predictors. Unlike the states, these predictors are functions of the unquantized data, which avoids the "flattening" effect of quantization. Conversely, if the state s(q(zi)) uniquely determines the past sequence zi . . . ~; -~+ l , which is the case in a tree model without quantization for a node whose depth is at least r, the predicted value is constant.</p><p>Note that prediction in (3.2) is used in a way that differs essentially from the one in <ref type="bibr" target="#b29">[28]</ref>, where prediction and compression are treated separately. In our model, prediction is state-dependent (in order to adapt itself to the local variations) and embedded within the compression process. At this point, the states are built out of the original data (PCM domain), instead of out of the prediction errors as in <ref type="bibr" target="#b29">[28]</ref>. However, it will be shown later that the state-construction process may be viewed in an alternative way, which leads to a model more reminiscent of <ref type="bibr" target="#b29">[28]</ref> While the above advantages of prediction are based on the assumed physical properties of the data, it is worth mentioning that its use can be justified even in a nonparametric, purely statistical sense <ref type="bibr" target="#b31">[30]</ref>. When used in conjunction with a universal scheme, prediction is aimed at achieving every possiblle reduction in the model cost in order to speed up the convergence toward the compressibility, rather than just the extreme case of "total decorrelation" (i.e., "whiten" the data), as in traditional DPCM techniques. Thus, our use of prediction does not contradict the results in [3], which establish that in general total decorrelation cannot be achieved by prediction alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Constraints</head><p>In order to turn the on-line estimation of the model (3.2) into a tractable problem, we further reduce the model class by the following assumptions. Again, (a), (b), and (c) below are in correspondence with the modeling tools that were presented in this section. a) A unique parametric model is assumed for the processes U("). Specifically, fclllowing [15] and <ref type="bibr" target="#b14">[13]</ref>, we assume that each process is distributed according to a symmetric doubly exponential ([.e., Laplacian) density function</p><formula xml:id="formula_3">(3.4)</formula><p>where p is the value predicted by g(")(.). Hence., there is only one free parameter y(") per state. Although this model has proved itself as a good representation of a great variety of images, other models with more parameters are considered as well. b) Each scalar pixel component of the vector quantizer q ( . ) can be implemented by a (not necessarily balanced) complete binary tree in which each node corresponds to a region of amplitude values. The regions are successively halved while traversing the tree. For example, the root node corresponds to the entire alphabet A, the nodes in the first level of the tree correspond to the two halves of the alphabet [O,. . . , (a/Z) -l] and</p><p>[a/Z,. . . , a -11, ancl so forth. Clearly, the leaves of the tree define a partition of A into "connected" regions of adjacent values. c) The predictor g ( s ) ( . ) is assumed to be linear for every s E S. Together with (3.3), this implies r-1 g(s)(Zz, G-1,. . . , L T t l ) = Z% + a J , s ( Z z -J -2 % ) <ref type="bibr">(</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.5)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J=1</head><p>where al,", . . . , ur-l," are real-valued constants. Notice are state-dependent and the states change throughout the data, the whole prediction process can be viewed as piecewise linear.</p><p>d) The state function s(.) admits a tree implementation like the one described in Section 11. While in the tree models, described in Section 11, the states were seen to represent the memory of the process, the states in the global model (3.2), which are used to estimate the parameters of the AR model and the variance of the "noise" process, represent the rate of local variations in the region close to the current pixel (or, in geometric terms, the local curvature). This interpretation suggests that just as the curvature of a curve is invariant under translation, we should further require from the state-map the condition that it be invariant with amplitude levels, namely where Z' denotes a quantized value. This shift invariance in the gray levels is consistent with the additive noise model. An easy way to implement (3.6) is to build the contexts out of differences z,-~ -x,-l, j 2 0 , 1 &gt; 0, where x , -~ and zz-l are the values of two adjacent pixels in the causal template associated with z,+l. The set of pairs ( j , 1) is fixed, and it is picked such that it determines the value of all the pixels in the template, up to an amplitude shift. Thus, the contexts are built in the DPCM domain and the total depth of the tree model is shortened by one. This model is more effective than the nondifferential one for most images, and its implementation is further described in Section IV. However, it suffers from a shortcoming and, therefore, the nondifferential model is also investigated. Specifically, one of the premises on which assumption (b) regarding the quantization functions relies, is that all the pixel values are equally likely a priori. For this reason, it is reasonable to constrain the quantization regions to successive (symmetric) halvings, as equiprobable partitioning provides maximum information on the state. A stochastic complexity argument, presented in Section IV, will then enable us to decide when the halving should be stopped. When the values to be quantized are pixel differences, this assumption is no longer valid; rather, the smoothness of the data suggests that the most common difference values will be close to zero. Thus, intuitively, the size of the quantized regions should grow as they get farther away from zero. Moreover, this behavior cannot be adaptively learned, since it affects the shape of the tree. Hence, some additional adhoc decisions, which are described in Section IV, are needed. Note that in <ref type="bibr" target="#b29">[28]</ref> the contexts are also built out of differences. However, in <ref type="bibr" target="#b29">[28]</ref> these differences are the encoded prediction errors. Here, the differentiation process is just a means to adjust the states to a fixed "reference point" to assure shift invariance. that this can be interpreted as an AR process of order requirement (3.3) a prediction model of order T would have been needed in order to learn the same statistics with its associated increased cost. Since the predictors T -1 on the amplitude increments Zt-? -ICi. <ref type="bibr">Without</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the IV. THE IMAGE-CUSTOMIZED CONTEXT ALGORITHM</head><p>In this section, we describe the algorithms used to estimate the models proposed in Section 111. First, we consider the state function without taking differences and a doubly exponential model for the prediction residuals. Then, we turn to the difference model based on (3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Context Building in the PCM Domain</head><p>Clearly, if the quantizing functions q(')(.) and the order T -1 of the AR difference models were known in advance, a few modifications to the general purpose Context Algorithm presented in Section I1 would suffice to estimate the proposed model (3.2). Specifically, by storing at each node s (which would represent a sequence of quantization regions) the relevant counts, the parameters of the AR processes g(s) (.) are estimated adaptively with a least-squares predictor, and the parameters y(') of the noise processes (3.4) are estimated from the error samples. Then, the pixel probabilities are computed using (3.1) instead of (2.7), and this distribution is employed for the stochastic-complexity-based context selection rule as in (2.8)-(2.11). Although T could be estimated at each state by applying the minimum description length principle <ref type="bibr" target="#b23">[22]</ref>, we will assume for the sake of simplicity that T = 3. The reason is that this is not a crucial decision, since our final goal is to maximize the assigned probabilities rather than performing optimal prediction. Probability maximization is done mainly by a tree of an optimized size, while prediction is introduced for the reasons mentioned in Section 111. Empirically, the compression improvement obtained with T = 4 was never beyond 0.5%, so this seems a good choice. The search of an optimal context quantizing function, in turn, is discussed next. It turns out from the discussion that it cannot be done with the tools provided in Section 11. Then, a major modification to the algorithm is proposed.</p><p>First, consider the trivial case where the maximum length of search in nearby pixels for the optimal context is M = 1.</p><p>In this case, the context selection process would consist in applying a rule to decide how to merge optimally the siblings stemming from the root, whose number is at most a. This can be viewed as a form of quantization, and an extreme case would be to merge all of them into a single node, the root. Now, by assumption (c) in Section 111, the (unknown) scalar quantizers can be implemented by binary trees. Thus, one could transform the context tree into a binary one, where we move according to the bit values forming each past pixel value, starting with the most significant bit. Each node in the binary tree represents a pixel value quantized to a number of bits equal to the level of the node. The leaves would be determined by a stochastic complexity argument as in the original algorithm: a pixel is quantized into more bits (i.e., a child is added to the path in the binary tree) only if the associated model cost is smaller than the savings in the empirical entropy.</p><p>The situation is more complicated when M &gt; 1. When the stopping rule indicates that a leaf of the quantizer tree of the first neighbor has been reached, we still need to investigate the second neighbor, starting with its most significant bit, and so forth. Such a generalization would be untractable: when a quantizer leaf is selected at one level of the tree, this decision should be reflected in the counts used by all its descendants in the subsequent levels. Consequently, the relevant counts would not be readily available and the information would have to be retrieved from descendants located in a multiplicity of paths. Alternatively, we can consider the context just as a sequence of bits within the nearby pixels, and select an encoding node from the entire binary tree in the usual way. This poses the problem of ordering the "context bit stream;" since the tree is traversed sequentially, an order in which all the bits within a i I neighbor precede the bits within all farther neighbors would result in a neighbor not being visited until all closer neighbors are observed with full resolution (log Q bits). Thus, a different order is needed, for otherwise only the last pixel value forming the selected context would be quantized. Much as the nearby neighbors have been ordered according to the importance that we (subjectively) assign to their influence in the current pixel reorder the corresponding sequence of bits within the pixels according to a similar criterion, and choose the encoding node out of the binary tree by the usual rule. Again, each node in the tree represents a sequence of past pixel values, each quantized into a different number of bits. Hence, we have transformed I value (for example, physical distance in images), we can 1 ~ the problem into one that can be solved off-line, namely, how to order these bits in a systematic manner. the order of the past symbols in any Markovian model is not the information that each individual context bit is expected to condition the current pixel value more heavily than, say, the</p><formula xml:id="formula_4">~ I ~</formula><p>Ideally, the order ought to be optimized. However, just like optimized for complexity reasons, here we pick it based on provide on the value of the current symbol. For example, the most significant bit of the second closest pixel is expected to least significant bit of the closest one. This is exactly what a fixed quantization would do in an extreme way: the least significant bits would be ignored when defining a context. Instead, we propose to assign to least significant bits a "low rank" in the order of the context bits. We can formalize these intuitive ideas in terms of the correlation between a specific context bit and the current pixel value. This is a function of two parameters: the distance d between the context pixel and the current pixel, and the position TJ of the bit within the pixel. For example, for the context pixels above and on the left, d = 1, for the diagonal (upper-left or upper-right) pixel d = 8, and so forth, while 'U = 1 for the most significant bit, and 'U = loga for the least significant bit. It has been observed [ 131, <ref type="bibr" target="#b17">[16]</ref> that, for isotropic images, the correlation between pixels tends to decay exponentially with d. On the other hand, assuming that the values of the current and the context pixels are close to each other, if a quantization region is represented by its centroid (as all values are, a priori, equally likely), then for a fixed context pixel the correlation between context bits and the current pixel decreases as 2-". It is then reasonable to rank the context bits according to decreasing values of (4.1) where p usually ranges between 0.90 and 0.95 <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b27">[26]</ref>. Therefore, for practical maximum values of d, we first rank the most significant bits of all the pixels in the conditioning template, we then proceed with the second most significant bits, and so forth. Empirically, this order indeed proved to be a good one.</p><formula xml:id="formula_5">~ I ~ J ( d , T J ) = pd ' 2-"</formula><p>Having solved the quiantization problem we can describe the proposed algorithm, which will be referred to as the CXT-BT/CARP Algorithm (Context Algorithm with binary-tree and conditional AR prediction). At each node s in Ti+l, instead of keeping occurrence coun1.s for each symbol, we keep only the sum of cross-products of the form (zt+l -zt)(zt--3.</p><formula xml:id="formula_6">-z t ) , (xt+l -zt)(xt--2 -z t ) , (zt--l -x t ) 2 , (zt--2 -xt)', and (xt--1 -xt)(xt--a -xt)</formula><p>, accumulated over all time instants t for which s is a possible context for xt+l, t 5 i. These sums are used for difference least-squares prediction with T = 3. In addition, the estimation of y(") for the Laplacian distributions U(") requires an accumuliated sum of the absolute values of the prediction errors at s and a count of the number of occurrences of the node.</p><p>CXT-BT/CARP Algorithm 0 ) Start with the root with its counts all zero. This tree To represents the empty context A. 1) Recursively, having constructed the tree Ti (which may be incomplete) from x2, read the symbol x;+l. Climb the tree along the path defined by the past reordered bit stream (derived from xi by use of the rank (4.1)) until a leaf is reached or there is no context available: (which may happen at the image edges). For each node s of T;</p><p>visited, follow these steps: a) Estimate the value of the AR coefficients using the least-squares method based on the sums accumulated in s, and predict the value &amp;+I of xi+l as in (3.5). b) Estimate y(") as the inverse of the average absolute value of the prediction error at s. c) Compute the probability (3.1) assigned to by the model (3.1,) at s and the corresponding code length which is; given by its negative logarithm. d) Increment the accumulated cross-products. e) Increment the accumulated absolute prediction error f) Update the efficiency index as in (2.8)-(2.10).</p><p>2) If a leaf is reached whose number of occurrences is not less than a preselected threshold (in practice, a suitable value for the threshold is 10) and whose depth is less than a preselected bound MI (typically M' = M.log a), create a new node labeled with the next context bit, and initialize the accumulated cross-products and prediction error suitably. This; completes the construction of T,+1. 3) Choose the encoding context s * ( z i ) for z;+l as the deepest node in the searched path having negative efficiency index before the updating Step l(f), and accumulate the total probability assigned to xn as in (2.11). Notice that the least squares estimation of the AR coefficients requires the inversion of a (T -1) x (T -1) matrix (which is another reason for keeping T small, namely three). In some cases, this matrix might be singular. This happens, for example, at nodes that are deep enough to determine the exact value of T nearby pixels, uniquely, so that each accumulated cross-product is formed by identical addends. Clearly, in this case c~j,~(z;-j -: E; ) is a constant. Hence, we estimate this constant using the average prediction error at the node. For this and other special cases (for example, special treatment of by 1&amp;+1 -G,-lI. the edges of the image), we also keep an accumulated sum of these errors. Other minor details in the algorithm (that can, however, improve the fit of the model to the image) are omitted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Context Building in a DPCM Domain</head><p>In order to implement the difference model (3.6) of Section I11 (hereafter DCXT-BT/CARP or Difference Context Algorithm with binary-tree and conditional AR prediction), we only need to redefine the bit stream that produces the contexts that are built now out of the differences x,-~ -x%-l, j 2 0, I &gt; 0. As discussed in Section 111, the quantization should not be performed in a similar manner. However, we still regard the context as a (reordered) bit stream, although different from the actual bits forming the pixels, which is the output of a (perfectly balanced) decision tree. Specifically, consider the case where a = 256. We treat the regions [0,255] and [-1, -2551 in the difference domain as symmetric, and we define eight quantization regions (or "buckets" <ref type="bibr" target="#b29">[28]</ref>) for each one of them, where values close to zero are more accurately described. The regions we have chosen are [0, 11, [a, <ref type="bibr">31, [4,7]</ref>,</p><p>[8,15], <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b32">31]</ref>, <ref type="bibr">[32,</ref><ref type="bibr">63]</ref>, <ref type="bibr">[64,</ref><ref type="bibr">127]</ref>, and [lag, 2551 (for negative differences, the last region contains one less element). As in the CXT-BTKARP Algorithm, these regions are obtained by successive binary (but nonsymmetrical) partitions, which are implemented by a decision tree. Thus, the first question asked is: Is the difference negative? This provides the first bit that this particular pixel contributes to the context. Then, if the answer is "NO," the next question is: Is it larger than 15? In case of a negative answer, the next question is: Is it larger than three? This process continues until we can identify the region by a four-bit sequence. Clearly, the outcome of the first questions plays the same role as the most significant bits in the CXT-BTKARP Algorithm. Thus, for example, in encoding x,+1, the information of whether the difference 5 , -3x,-2 is negative or not precedes in the context bitstream the information stating, say, in which exact bucket x,-1 -z, falls. In the absence of a sound theoretical way to estimate the information provided by each particular bit, unlike the CXT-BTKARP Algorithm, the bit "reordering" function, as well as the quantization regions, were empirically determined.</p><p>Note that the maximal context length is significantly shorter than with the first version. For example, for an eight-bit alphabet and a maximal search space M of six pixels, this maximal length is MI = ( M -1) x 4 = 20, instead of MI = M ' log a = 48. Moreover, with M = 4 pixels, which in many cases produces very similar results, the maximal depth of the grown binary tree is 12. With such a tree, the backward pruning context-selection rule of [ 141 becomes feasible.</p><p>Other versions of these algorithms encode the prediction residuals using a histogram-bucketing model instead of the Laplacian one. The possible values of the prediction errors are partitioned into 2loga + 1 buckets {0}, {l}, {2,3}, (4,. . . , 7 } , and so forth (similarly for negative values), and a probability is assigned to each bucket according to its frequency count for the past string, using, for example, the   rule of succession (2.7). Then, the probability mass within each bucket is considered as uniformly distributed, although more sophisticated models have been investigated as well. Note that these errors take on 201 -1 potential values but, given the predicted value, only a of them may effectively occur. This fact, which is the counterpart of (3.1), must be taken into account for an efficient probability estimation. A theoretical framework for density estimation based on histogram bucketing is given in <ref type="bibr" target="#b21">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND CONCLUSION</head><p>In this section we report compression results obtained with software implementations of the above described algorithms for a set of test images digitized at eight b/pixel. This set includes a) The 576 rows by 720 columns color test images from the JPEG standards committee, represented in the YUV system, with the chrominance components U and V subsampled to 576 x 360.</p><p>b) The 512 x 512 gray-scale standard images from the USC data set. c) Medical (MRI, X-ray, and ultrasound) images; the MRI images (human brains and a sequence of head slices) have dimensions 256 x 256, the X-ray images tested (human chest and lung) are 1024 x 1024, and the ultrasound images, in turn, are 480 x 512 human hearts.</p><p>d) A set of seven 480 x 640 images of the planet Neptune obtained by the Voyager spacecraft. In addition to the main image object the image files contain textual and graphic infomation near the edges. The two versions of the modified Context algorithm considered are CXT-BT/CARP and DCXT-BT/CARP. In both cases the noise process is modeled with the Laplacian distribution (3.4). The efficiency thresholds (2.10) employed to run the algorithms are bo = -5 and bl = 5. The search space template employed is the one depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, which contains M = 6 surrounding pixels and, consequently, the maximal length M' of the context bit stream is 48 for CXT-BT/CARP and 20 for DCXT-BT/CAF@. Prediction is based on the three closest neighbors in the template (North, West, and North-West).</p><p>In Table <ref type="table" target="#tab_1">I</ref> a representative sample of U, V, and Y components of four JPEG test images, three USC images ("Lena," "Couple," and "Pyramid"), typical MRI head and brain images, chest and lung X-rays, a typical heart ultrasound, and one image from the Voyager set are considered. These results were compared with the then state-of-the-art version <ref type="bibr">[lo]</ref> of the Sunset Algorithm and with the arithmetic coding version of the JPEG independent lossless compression system [ 121 (for conceptual and implementation details about Sunset, see <ref type="bibr" target="#b29">[28]</ref> and [8], respectively). Both reference algorithms use fixed DPCM and context modeling steps, followed by a Q-coder [17j, and are compared in <ref type="bibr">[9]</ref>. The results reported for JPEG were obtained by tuning its adjustable parameters to each specific image in order to achieve maximum compression. The optimal set of parameters is given for each image by the three digits in brackets in the corresponding column (predictor template T , upper bucketing parameter U , and lower bucketing parameter L). The various algorithms compare similarly for the nonreported images in each set. In Table <ref type="table" target="#tab_1">I1</ref> we consider the three components of each of the nine JPEG test images as a unity and total (per pixel) code lengths are reported. The JPEG results in Table <ref type="table" target="#tab_1">I1</ref> were optimized for the whole set of images [l], which yields T = 2, U = 1, and L = 0.</p><p>To the best of our knowledge, the algorithm of <ref type="bibr">[lo]</ref> provides the best published results available for comparison. It is seen that both versions of Context Algorithm outperform the other algorithms. Tables <ref type="table" target="#tab_1">I</ref> and<ref type="table" target="#tab_1">I1</ref> further show that building the contexts in the DPCM domain based on (3.6) is preferable in general to the nondifference model, as DCXT-BT/CARP outperforms CXT-BT/CARP except for the Voyager image set. The improvement with respect to the modified Sunset Algorithm is substantial for the Voyager image set and rather small for the JPEG images, composed of landscapes and human portraits. In many data compression applications, it may not justify the increase in complexity. However, as pointed out in Section I, our goal is to capture as much information as possible from the data and the consistency of the results suggests that universal modeling is indeed doing that. In some applications, not necessarily data compression, this additional information might be the most relevant one. In addition, the results with the Voyager image set suggest that these universal modeling techniques may well be necessary to compress other types of data that are not handled properly by the standard techniques. As for the JPEG images, our results confirm that the (nonuniversal) model employed by the improved Sunset Algorithm is very well suited for most of them.</p><p>The results reported for the Context Algorithm correspond to actual code lengths obtained by appending an arithmetic coder to the modeler. ThLe arithmetic code was adapted from <ref type="bibr" target="#b32">[31]</ref> to handle parametric distributions. On the other hand, the results reported for the improved Sunset Algorithm correspond to computed ideal code lengths, i.e., negative logarithms of the probabilities assigned by the algorithm to the data.</p><p>In all cases the Laplacian model that is used in both modifications of Algorithm Context behaves better than either a histogram bucketing or a nonparametric model (for example, the rules of succession of <ref type="bibr">[32]</ref>). The savings with respect to a bucketed histogram range from 5% (USC-pyramid) to 0.2% (Voyager images). However, a parametric model might not be the best option with oither types of images, especially when the Laplacian structure is broken by preprocessing. This shows the major importance that should be attached to Step 1 in the modeling process described in Section I.</p><p>In addition to the tested schemes, which perform a sequential probability assignment based on context models of different degrees of adaptivity, other (seemingly different) approaches to lossless image compression involve transform coding, a technique that was successfully applied in lossy compression. Traditional image transforms such as the DCT or the pyramidal, reversible 5'-transform yield poor results relative to a simple DPCM approach, as reported in <ref type="bibr" target="#b26">[25]</ref> for medical images. On the other hand, promising results are reported for the S + P-lransform <ref type="bibr" target="#b28">[27]</ref>, a pyramidal structure that interleaves the S-transform with prediction. As shown in [3], in general, these techniques cannot achieve total decorrelation and, consequently, are followed by some kind of Markov modeling [ 181, <ref type="bibr" target="#b28">[27]</ref>. However, transform coding can provide a context modeler with prior knowledge on the structure of images, thus helping to reduce the model cost associated with the learning process. This is similar to the use of prediction in our scheme, which is further formalized in <ref type="bibr" target="#b31">[30]</ref>. Thus, the interaction between universal context modeling and transform coding warrants further investigation.</p><p>To conclude, the following remarks are in order:</p><p>The results in Tables I and I1 for our modifications of Algorithm Context are given for a maximal search space of six surrounding pixels, while Sunset uses three. However, using four pixels and building the contexts with the difference model (3.6) (which amounts to three difference values) gives similar compression results as the six-pixel search space template for this set of images. This requires a maximal depth of 12 for the binary tree, and can admit a nonparametric model for prediction errors depending on the storage constraints.</p><p>As stated in Section 111-A, a fixed quantization function that builds the contexts out of the log ,B most significant bits of each pixel (i.e., growing a p-ary tree) poses the problem of how to chose the optimal value of on-line. An AR prediction model used in conjunction with this plain quantization method has been seen to improve the robustness of the model with respect to ,B. In fact, the results obtained using the best possible value of for each image differ from those obtained using a constant , B = fi by less than 1%. However, they are inferior to those obtained when the context is considered as a bit stream. Although the execution time needed for modeling is smaller when the context is not broken into bits and reordered, it is similar to that needed by a difference (bit)-context model with a small search space, whereas the latter approach yields better compression. Unlike the case of lossless JPEG, no effort has been done in these implementations of Algorithm Context in order to account for the possible "nonstationarity" of the images, except for the thresholding (2.9) that is performed to select the encoding node. In particular, the counts used to accumulate the total probability (2.11) are never reset. As stated previously, the use of conditioning classes diminishes the effects of nonstationarity. However, a clever quadtree segmentation of the images may result in additional savings. One such algorithm is proposed in <ref type="bibr">[14]</ref>.</p><p>A c m o WLEDGMENT</p><p>The authors are grateful to Glen Langdon for helpful discussions, to Tom Truong for his assistance with the software implementation of the algorithms, and to Aleks Jakunin for his assistance in the implementation of an arithmetic coder. Our thanks are also due to the anonymous referees whose comments helped improve the manuscript. Marcel0 Weinberger wishes to thank Gadiel Seroussi for many discussions. They wish to thank the Independent Broadcasting Authority in the United Kingdom for its efforts in preparing the JPEG test images and distributing them to the compression community. We also acknowledge NASA/JPL and Georgia Institute of Technology for making the Voyager copyrighted images publicly available for research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Six-pixel causal template</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TRANSACTIONS ON IMAGE PROCESSING, VOL. 5 , NO. 4, APRIL 1996</figDesc><table><row><cell>Image</cell><cell cols="2">E E E Context Diff. Context</cell></row><row><cell></cell><cell>BT/CARP</cell><cell>BTICARP</cell></row><row><cell>barb2</cell><cell>3.93</cell><cell>3.91</cell></row><row><cell>balloon</cell><cell>2.63</cell><cell>2.57</cell></row><row><cell>hotel</cell><cell>3.73</cell><cell>3.70</cell></row><row><cell>girl</cell><cell>3.34</cell><cell>3.30</cell></row><row><cell>gold</cell><cell>3.88</cell><cell>3.87</cell></row><row><cell>barb</cell><cell>3.76</cell><cell>3.71</cell></row><row><cell>board</cell><cell>3.13</cell><cell>3.10</cell></row><row><cell>Zelda</cell><cell>3.32</cell><cell>3.27</cell></row><row><cell>boats</cell><cell>3.30</cell><cell>3.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I NUMBER</head><label>I</label><figDesc>OF BITS PER PIXEL IN COMPRESSED IMAGE FOR VARIOUS ALGORITHMS</figDesc><table><row><cell></cell><cell>context</cell><cell>Diff. Context</cell><cell>M a d</cell><cell>LDSSlesS</cell></row><row><cell></cell><cell>BT/CARP</cell><cell>BT/CARP</cell><cell>sunset</cell><cell>IpEG V,UW</cell></row><row><cell>barb2 Y</cell><cell>4.60</cell><cell>4.56</cell><cell>4.81</cell><cell>5.12 (73.0)</cell></row><row><cell>balloon V</cell><cell>2.33</cell><cell>2.28</cell><cell>2.42</cell><cell>2.57 17.1.0)</cell></row><row><cell>girl U</cell><cell>2.80</cell><cell>2.78</cell><cell>2.87</cell><cell>3.03 (7.1.0)</cell></row><row><cell>hotel Y</cell><cell>4.29</cell><cell>4.26</cell><cell>4.48</cell><cell>4.76 (7.3.0)</cell></row><row><cell>hotel V</cell><cell>3.27</cell><cell>3 25</cell><cell>3.37</cell><cell>352 (73,O)</cell></row><row><cell>hotel U</cell><cell>3.06</cell><cell>3.05</cell><cell>3.16</cell><cell>3.33 (7,3.0)</cell></row><row><cell>lens</cell><cell>4.25</cell><cell>4.15</cell><cell>4.21</cell><cell>454 (7,3,1)</cell></row><row><cell>pyramid</cell><cell>3.06</cell><cell>2.97</cell><cell>3.30</cell><cell>3.28 (4.1,O)</cell></row><row><cell>C0"ple</cell><cell>2.35</cell><cell>2.25</cell><cell>2.56</cell><cell>2.42 (4,l.O)</cell></row><row><cell>MRI brain</cell><cell>4.38</cell><cell>4.35</cell><cell>4.51</cell><cell>4.77 (7,3,0)</cell></row><row><cell>MRI head dice</cell><cell>2.57</cell><cell>2.55</cell><cell>2.87</cell><cell>2.94 (7,2,0)</cell></row><row><cell>chest X-ray</cell><cell>1.05</cell><cell>1.03</cell><cell>1.40</cell><cell>1.11 (4,l.O)</cell></row><row><cell>lung X-ray</cell><cell>2.18</cell><cell>2.09</cell><cell>2.25</cell><cell>2.37 (4,1,0)</cell></row><row><cell>heart ultrasound</cell><cell>3.11</cell><cell>3.04</cell><cell>3.31</cell><cell>327 (1,2,0)</cell></row><row><cell>Neptune (Voyager)</cell><cell>2.98</cell><cell>3.04</cell><cell>3.54</cell><cell>3.54 (4.2.a)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COLOR</head><label>II</label><figDesc>JPEG TEST IMAGES FOR VARIOUS ALCORIT'HMS TOTAL NUMBER OF BITS PER PIXEL IN COMPRESSED "YW"</figDesc><table><row><cell>4.09</cell></row><row><cell>2.69</cell></row><row><cell>3.88</cell></row><row><cell>3.41</cell></row><row><cell>3.96</cell></row><row><cell>3.94</cell></row><row><cell>3.26</cell></row><row><cell>3.29</cell></row><row><cell>3.40</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Weinberger was with IBM-Almaden Research Center</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<pubPlace>San Jose, CA; Palo Alto, CA 94303 USA</pubPlace>
		</imprint>
	</monogr>
	<note>He is now with Hewlett-Packard Laboratories. e-mail: marcelo@hpl.hp.com</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rissanen is with IBM-Almaden Research Center</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<pubPlace>San Jose, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publisher Item Identifier S</title>
		<imprint>
			<biblScope unit="issue">96</biblScope>
			<biblScope unit="page" from="2762" to="2765" />
			<pubPlace>Stanford, CA; Almaden Research Center, San Jose, CA 95120 USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Arps was with Information Sciences Laboratory, Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparison of international standards for lossless still image compression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Arps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBA4 Res. Rep. RJ</title>
		<imprint>
			<biblScope unit="volume">9674</biblScope>
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Present position and potential developments: Some personal views, statistical theory, the prequential approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc., Ser. A</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="278" to="292" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relations between entropy and error probability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Merhav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="259" to="266" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Contribution a 1&apos;Etude et au DCveloppement d&apos; Algorithmes de traitement du signal en compression de donn6es et d&apos;images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Furlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Sophia Antipolis, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note>, 1&apos;Universite de Nice. in French</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Context modeling for text compression</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lelewer</surname></persName>
		</author>
		<editor>Image and Text Compression, J.A. Storer, Ed. Boston</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Kluwer</publisher>
			<biblScope unit="page" from="113" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A note on the Lempel-Ziv model for compressing individual sequences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Krichevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Trofimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Langdon</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="284" to="287" />
			<date type="published" when="1981-03">Mar. 1981. Mar. 1983</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Inform. Theory</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sunset: A hardware-oriented algorithm for lossless compression of gray-scale images</title>
		<author>
			<persName><surname>__</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Medical Imaging V: Image Capture, Formatting, Display</title>
		<meeting>SPIE Medical Imaging V: Image Capture, Formatting, Display</meeting>
		<imprint>
			<date type="published" when="1991-03">Mar. 1991</date>
			<biblScope unit="volume">1444</biblScope>
			<biblScope unit="page" from="272" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the JPEG model for lossless image compression</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Langdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1992 Data Compress. C o f</title>
		<meeting>1992 Data Compress. C o f<address><addrLine>Snowbird, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-03">Mar. 1992</date>
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Centering of context-dependent components of prediction error distributions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Langdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mareboyana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Applic. Digit. Image Processing XVI</title>
		<meeting>SPIE Applic. Digit. Image essing XVI</meeting>
		<imprint>
			<date type="published" when="1993-07">July 1993</date>
			<biblScope unit="volume">2028</biblScope>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Universal schemes for sequential decision from individual data sequences</title>
		<author>
			<persName><forename type="first">N</forename><surname>Merhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1280" to="1292" />
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Pennebaker</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>JPEG Still Image Data Compression Standard</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Picture coding: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">0</forename><surname>Limb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="366" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Topics in descriptive complexity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nohre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-10">Oct. 1993</date>
			<pubPlace>Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dep. of Computer Science, The Technical Univ. of Linkoping</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predictive quantizing differential pulse code modulation for the transmission of television signals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>O'neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="689" to="722" />
			<date type="published" when="1966-06">May-June 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coding isotropic images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>O'neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="697" to="707" />
			<date type="published" when="1977-11">Nov. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An overview of the basic principles of the Q-coder</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Develop</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="726" />
			<date type="published" when="1988-11">Nov. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The use of contextual information in the reversible compression of medical images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Ramabadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="185" to="195" />
			<date type="published" when="1992-06">June 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal modeling and coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Langdon</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="1981-01">Jan. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Density estimation by stochastic complexity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="1992-03">Mar. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic complexity and modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annuls Stat</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1080" to="1100" />
			<date type="published" when="1986-09">Sept. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic Complexity in Statistical Inquiry</title>
	</analytic>
	<monogr>
		<title level="j">NJ: World Scientific</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Universal coding, information, prediction, and estimation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="629" to="636" />
			<date type="published" when="1984-07">July 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A universal data compression system</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="656" to="664" />
			<date type="published" when="1983-09">Sept. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reversible intraframe compression of medical images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Roos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="328" to="336" />
			<date type="published" when="1988-12">Dec. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reversible 3-D decorrelation of medical images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">413420</biblScope>
			<date type="published" when="1993-09">Sept. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reversible image compression via multiresolution representation and predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Pearlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Visual Comm. Image Processing</title>
		<meeting>SPIE Visual Comm. Image essing</meeting>
		<imprint>
			<date type="published" when="1993-11">Nov. 1993</date>
			<biblScope unit="volume">2094</biblScope>
			<biblScope unit="page" from="664" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parameter reduction and context selection for compression of the gray-scale images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Develop</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="188" to="193" />
			<date type="published" when="1985-03">Mar. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A universal finite memory source</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="643" to="652" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sequential prediction and ranking in universal context modeling and data compression,&quot; submitted to IEEE Trans. Inform. Theory, also, HP Lab</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seroussi</surname></persName>
		</author>
		<idno>HPL-94.111</idno>
		<imprint>
			<date type="published" when="1994-11">Nov. 1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Arithmetic coding for data compression</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="520" to="540" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1085" to="1094" />
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Compression of individual sequences via variable rate coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="53C" to="536" />
			<date type="published" when="1978">Sept. 1978. 1993</date>
			<publisher>Van Nostrand Reinhold</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">He has done research in control, prediction, and system theories; numerical mathematics; coding, and information theory; probability theory; and statistics. He has published more than 100 papers and a monograph Stochastic Complexity in Statistical Inquiry. He also holds nine patents. He is a coordinating editor of the Joumal of Statistical Planning and Inference, an associate editor for</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jorma</surname></persName>
		</author>
		<author>
			<persName><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He is an advisory member for IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences. He was an associate editor of the IEEE TRANSACTIONS ON LNFORMATION THEORY. Dr. Rissanen received JBM Outstanding Innovation Awards in 1980 and 1988, an IBM Corporate Award in 1991</title>
		<meeting><address><addrLine>Linkoping, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1932-10-20">on October 20. 1932. 1993</date>
		</imprint>
	</monogr>
	<note>SM&apos;89) was born in Finland. Best Paper Awards from IFAC in 1981 and the IEEE Information Theory Group in 1986, and the IEEE</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hamming medal. He holds an Honorary Doctorate from the Technical University of Tampere</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>Finland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">During 1992-1993 he was on leave at Stanford University as IBM&apos;s Industrial Visitor to their Center for Integrated Systems. His assignments have included exploratory studies on processing and compressing binary images, advanced development of computer peripherals and systems, research into hardware-optimized adaptive compression, and implementation of algorithms in VLSI microsystems. During 1977-1978, he was on leave as a visiting associate professor at Linkoping Univerity, Sweden</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">D</forename><surname>Ph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Binary Image Compression&quot; in Image Transmission Techniques</title>
		<imprint>
			<date type="published" when="1960">1960. 1963. 1969. 1979. 1979</date>
			<publisher>Academic</publisher>
			<pubPlace>Los Gatos Laboratory, Los Gatos; Almaden Research Center; San Jose, CA; New York</pubPlace>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology ; Oregon State University ; and Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include adaptive data compression algorithms as well as image processing, office automation, and computer aided design of VLSI. Dr. A r p s has received from IBM recognition that has included a Resident Study. Award to Stanford University in 1967-1969 and teaching as an IBM Visiting Scientist at the Swiss Federal Institute of Technology-Zurich during 1970-1971</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">He received the electrical engineering degree from the Universidad de la Repliblica, Montevideo, Uruguay, in 1983, and the M.Sc. and DSc. degrees from the Technion-Israel Institute of Technology, Haifa, Israel, in 1987 and 1991, respectively, both in electrical engineering. From 1985 to 1992 he was with the Department of Electrical Engineering at the Technion, first as a teaching assistant and later as a faculty member. During the academic year 1992-1993 he was a Visiting Scientist at IBM-Almaden Research Center</title>
	</analytic>
	<monogr>
		<title level="m">M&apos;91) was born in Buenos Aires</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Marcel0</surname></persName>
		</editor>
		<editor>
			<persName><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Argentina; San Jose, CA; Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
	<note>His research interests are in information theory and statistical modeling. including data compression, source coding, and error-correcting codes</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
