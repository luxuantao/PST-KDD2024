<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrast-based Image Attention Analysis by Using Fuzzy Growing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu-Fei</forename><surname>Ma</surname></persName>
							<email>yfma@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Microsoft Research Asia 5F</orgName>
								<address>
									<addrLine>49 Zhichun Road</addrLine>
									<postCode>100080</postCode>
									<settlement>Beijing Sigma Center, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
							<email>hjzhang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Microsoft Research Asia 5F</orgName>
								<address>
									<addrLine>49 Zhichun Road</addrLine>
									<postCode>100080</postCode>
									<settlement>Beijing Sigma Center, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrast-based Image Attention Analysis by Using Fuzzy Growing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0CD8BCA246BF8CD04AE05B27EF935F5C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.4.10 [Image Processing and Computer Vision]: Image Representation -Statistical</term>
					<term>Hierarchical. I.4.6 [Image Processing and Computer Vision]: Image Segmentation -Region growing</term>
					<term>partitioning</term>
					<term>Pixel classification Algorithms</term>
					<term>Design</term>
					<term>Experimentation</term>
					<term>Theory Attention detection</term>
					<term>visual attention model</term>
					<term>contrast analysis</term>
					<term>fuzzy growing</term>
					<term>image analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual attention analysis provides an alternative methodology to semantic image understanding in many applications such as adaptive content delivery and region-based image retrieval. In this paper, we propose a feasible and fast approach to attention area detection in images based on contrast analysis. The main contributions are threefold: 1) a new saliency map generation method based on local contrast analysis is proposed; 2) by simulating human perception, a fuzzy growing method is used to extract attended areas or objects from the saliency map; and 3) a practicable framework for image attention analysis is presented, which provides three-level attention analysis, i.e., attended view, attended areas and attended points. This framework facilitates visual analysis tools or vision systems to automatically extract attentions from images in a manner like human perception. User study results indicate that the proposed approach is effective and practicable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The effective information retrieval in a large image library involves two key issues, one is retrieval accuracy, and the other is the suitable representations in various displays. It is widely recognized that region-based image retrieval is able to achieve higher accuracy than that based on global image features <ref type="bibr" target="#b1">[1]</ref>. Also, due to a variety of display screens used by users, especially those small display screens in PDA (Personal Digital Assistant), mobile phone and the size alterable webpage browsers, an adaptive image display scheme is needed <ref type="bibr" target="#b2">[2]</ref>. One of the core issues in the two popular applications is to determine the important and representative regions in whole image. Obviously, if the semantics of each region or object are known, this issue will be easily solved. However, this task is beyond the capability of contemporary computer vision systems. On the other hand, the perceptual attention mechanism plays an important role in biological vision and human cognition, which enable humans to filter and prioritize incoming information. If such an attention mechanism can be modeled, it can then be applied to determine the important regions in an image and serve the needs of region-based image retrieval and adaptive image delivery/browsing. According to these crucial requirements, this paper presents a three-level framework, composing of attended view, attended areas and attended points, from top to bottom, for fast image attention analysis.</p><p>Attention is at the nexus between cognition and perception. The control of the focus of attention may be goal-driven and stimulusdriven which corresponds to top-down and bottom-up processes in human perception, respectively. The study of attention involved in a few fields, including biology, psychology, neuro-psychology, cognitive science and computer vision. Although the attention mechanism is not completely understood yet, some proven conclusions can be used to guide its applications. The earlier attention research began with William James, who was the first person to outline a theory of human attention <ref type="bibr" target="#b3">[3]</ref>. Successively, Broadbent proposed his filter theory of attention in an attempt to explain many of the existing experimental results <ref type="bibr" target="#b4">[4]</ref>. The response selection theory of attention was proposed by Deutsch <ref type="bibr" target="#b5">[5]</ref>, who indicated that a part of attention involves high level processing. This process is called late selection in the later studies. From 1960s, Treisman proposed a series of models that combined early and late selection into a model known as Feature Integration Theory (FIT) <ref type="bibr" target="#b6">[6]</ref>. Treisman's recent study believes that early selection is most active when the perceptual load is high, whereas late selection (object-based and location-based) is used when perceptual load is low <ref type="bibr">[7]</ref>. Besides, advances in a neurophysiological model of attention were also made by Koch <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>.</p><p>In recent years, especially with emerging interest in active vision, computer vision researchers have been increasingly concerned with attention mechanisms as well. Consequently, a number of computational attention models were developed, such as the models proposed in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>. The basic principles behind these efforts are greatly influenced by psychophysical research. Niebur proposed a computational attention model in <ref type="bibr" target="#b12">[12]</ref>. He indicated that the so-called "focus of attention" scans the scene both in the form of a rapid, bottom-up, saliency-driven and task-independent manner and in a slower, top-down, volition-controlled and taskdependent manner. Based on the work in <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>, Itti proposed a saliency-based visual attention model for scene analysis in <ref type="bibr" target="#b15">[15]</ref>. In this work, visual input is first decomposed into a set of topographic feature maps which all feed into a master "saliency map" in a bottom-up manner. Then, a WTA (Winner-Take-All) competition is employed to select the most conspicuous image locations as attended points. In primates, such a map is believed to be located in the posterior parietal cortex as well as in the various visual maps in the pulvinar nuclei of the thalamus.</p><p>Another well known computational visual attention model is VISIT proposed by Ahmad <ref type="bibr" target="#b16">[16]</ref>, which is more biologicallyplausible than Itti's. VISIT consists of a gating network which corresponds to the pulvinar and whose output, the gated feature maps, corresponds to the areas V4, IT and MT of the optic nerve, a priority network corresponding to the superior colliculus, frontal eye field and posterior parietal areas, a control network corresponding to the posterior parietal areas, and a working memory corresponding to the prefrontal cortex. Both Itti's framework and Ahmad's model build up an elegant mapping from computational implementation to biological theories. However, their high computational complexity requires massively parallel method to obtain fast responses. This drawback usually exists in all biological structure based attention models. On the other hand, the study of human attention mechanism is not mature yet. Therefore, if we are only concerned with such high level applications as region-based image retrieval and browsing, it is not necessary to strictly reproduce biological structures by computer algorithms. For example, we have proposed a computational motion attention model for video skimming in <ref type="bibr" target="#b17">[17]</ref> and a user attention framework for video summarization in <ref type="bibr" target="#b18">[18]</ref>. We also proposed a pure computational algorithm for salient region extraction from video <ref type="bibr" target="#b19">[19]</ref>, which is mainly based on the relationship between texture and human perception. In <ref type="bibr" target="#b18">[18]</ref>, we present a comprehensive solution for user attention analysis in video, which integrates a series of algorithms of video, audio and image attention analysis. Itti's model was also employed in this user attention model as static image attention analysis module. Although the work in <ref type="bibr" target="#b19">[19]</ref> partly solves the issue of image attention, it focuses on the efficiency of video processing and is not a total solution for image attention analysis. Meanwhile, Itti's model needs massive computations, which makes it impracticable to be implemented on personal computers. Consequently, an alternative, more powerful and more practicable solution for image attention analysis is desirable.</p><p>Instead of only verifying human perception mechanism by computer algorithms, we proposed an image attention analysis framework according to the characters of digital images and the capabilities of personal computers in this paper. This framework extracts three-level attentions from image, namely, attended view, attended areas, and attended points. The attended view means the main part of image with balance composition and rich information. The attended areas are those regions which represent important semantics and most possibly attract human attentions. Whereas, the attended points, without semantics, are those locations which perceive local maximum stimulus. Such three-level attentions are able to support a number of applications related to image analysis. In our work, the primary principles about human attention mechanism explored in the aforementioned works are utilized as only high-level guidance. Firstly, we investigated the key factors in human visual perception. The conclusion is that contrast is the most important factor which dominantly influences human visual perception. Therefore, a contrast-based saliency map is proposed as an attention presentation of image, like Itti's work. Based on such saliency map, attended points and attended view are directly extracted. In addition, a fuzzy growing process is proposed to simulate the process of human perception, by which the attended areas are extracted from saliency map. The satisfactory user study results show that the proposed approach is an effective and fast solution for image attention analysis.</p><p>The rest of this paper is organized as follows: Section 2 introduces the concept and computation method of contrast-based saliency. In Section 3, fuzzy growing is discussed in detail. Based on the methods presented in Section 2 and Section 3, Section 4 addresses contrast-based image attention analysis framework as well as attention extraction methods. Additionally, the specific applications of the proposed framework are also discussed at the end of this section. Section 5 displays and discusses the evaluation results obtained from user study experiment. Finally, Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CONTRAST-BASED SALIENCY MAP</head><p>Contrast is an important parameter in assessing vision. Visual acuity measurement in the clinic uses high contrast, that is, black letters on a white background. In reality, objects and their surroundings are of varying contrast. Therefore, the relationship between visual acuity and contrast allows a more detailed understanding of human visual perception <ref type="bibr" target="#b20">[20]</ref>. Traditional image processing techniques usually consider an image by three basic properties, color, texture, and shape. These techniques have been successfully applied to a number of applications. However, they cannot provide high level understanding of image, because human usually do not simply understand an image from color, texture, and shape aspects separately. In fact, a basic principle behind the three basic characters is contrast. In other words, whether an object can be perceived or not depends on the distinctiveness between itself and its environment. Obviously, black box becomes attended area though red background occupies most of image. This phenomenon indicates that the color and size are not most pivotal factor for human perception, although human visual sensitivity is influenced by color and size. On the contrary, color contrast plays an important role in this perception process. Figure <ref type="figure" target="#fig_0">1</ref>(c) and (d) show two textured images with oriented dashes. The weak texture area is surrounded by the strong texture patches in (c), while the contrary case is shown in (d), the strong texture area being surrounded by the weak texture patches. Similarly to color, the strength of texture does not greatly influence human perception, but contrast still does. The same conclusion can also be drawn from Figure <ref type="figure" target="#fig_0">1</ref>(e) and (f). The complexity of shape is not the main factor in human perception either. From above comparison experiments, we may make a supposition that the regions with high contrast have rich information and are most likely to attract human attentions.</p><p>Based on this observation, we proposed a contrast-based saliency measure in this work. There have been a number of methods to compute contrast, such as color contrast and luminance contrast. However, a generic contrast is needed in this work. We define an effectual area perceiving stimulus as perceive field, which is equivalent to receptive field in human eye. An image with the size of M×N pixels can be regarded as a perceive field with M×N perception units, if each perception unit contains one pixel. The contrast value C i,j on a perception unit (i, j) is defined as follows:</p><p>( )</p><formula xml:id="formula_0">∑ ∈ = Θ q j i j i q p d C , , ,<label>(1)</label></formula><p>where p i,j (i∈[0, M], j∈[0, N]) and q denote the stimulus perceived by perception units, such as color. Θ Θ Θ Θ is the neighborhood of perception unit (i, j). The size of Θ Θ Θ Θ controls the sensitivity of perceive field. The smaller the size of Θ Θ Θ Θ is, the more sensitive the perceive field is. d is the difference between p i,j and q, which may employ any suitable distance measure according to applications. color contrast, but also reflects the strength of texture. Moreover, the areas close to the boundary of objects lean to have same or similar contrasts. Therefore, this contrast-based saliency map presents color, texture and approximate shape information at the same time, which provides sufficient information for image attention analysis.</p><p>The more samples of saliency map are given in Figure <ref type="figure">4</ref>. col.(c).</p><p>Experimental results show that the saliency map generated by our approach is good representation of human attention, and the mechanism embedded it effectively supports the following processes of attention detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FUZZY GROWING</head><p>In order to extract attended areas from saliency map, a method called fuzzy growing is proposed. Saliency map is a gray-level image in which the bright areas are considered as attended areas, as shown in Figure <ref type="figure" target="#fig_1">2</ref>(c). Obviously, a hard cut threshold is not effective for attended areas extraction, because the variation of gray-levels in saliency map are not consistent, even in one object. Consequently, conventional region growing approaches based on one strict measure cannot solve such problem well. In our implementation, fuzzy theory <ref type="bibr" target="#b21">[21]</ref> is employed in region growing process, named fuzzy growing, because fuzzy theory has been proven to be effective to imitate human mental behaviors.</p><p>According to the definition of a fuzzy event <ref type="bibr" target="#b22">[22]</ref>, saliency map is regarded as a fuzzy event modeled by a probability space. We assume that saliency map has L gray levels from g 0 to g L-1 and the histogram of saliency map is h k , k=0, …, L-1. Thus, the saliency map is modeled by a triplet (Ω, k, P), where Ω={g 0 , g 1 , …, g L-1 } and P is the probability measure of the occurrence of gray levels, i.e., Pr{g k } = h k /∑h k. The membership function, µ S (g k ), of a fuzzy set S∈Ω denotes the degree of some properties, such as attended areas, unattended areas, etc., possessed by gray level g k . In fuzzy set notation, it is written as</p><formula xml:id="formula_1">( ) k g k S g g S k / ∑ ∈ = Ω µ (2)</formula><p>The probability of this fuzzy event can be computed by</p><formula xml:id="formula_2">( ) ( ) ( ) k r L k k S g P g S P ∑ - = = 1 0 µ<label>(3)</label></formula><p>For saliency map, there exist two classes of pixels, attended areas and unattended areas. We define the two classes as two fuzzy sets, denoted by B A , and B U , respectively, which are mutually exclusive. Thus, these two fuzzy sets partition saliency map Ω. In such fuzzy partition, there is no sharp boundary between the two fuzzy sets, which is like human perception mechanism. We adopt fuzzy cpartition entropy as the criterion to measure the fitness of a fuzzy partition. Theoretically, a fuzzy c-partition is determined by 2(c-1) parameters, and the problem becomes to find the best combinations of these parameters, which can be considered a combinatorial optimization problem. Usually, simulated annealing or genetic algorithms is used to solve this problem, but they are all time-consuming. Fortunately, only 2 parameters are needed in our algorithm due to 2-partition. Therefore, we use an exhaust search to find optimal result without involving high computational complexity.</p><p>In saliency map Ω, considering the two fuzzy events, attended areas B A and unattended areas B U, the membership functions of fuzzy events are defined in ( <ref type="formula">4</ref>) and ( <ref type="formula" target="#formula_3">5</ref>), respectively</p><formula xml:id="formula_3">a u x x u u a u x a x A      ≤ &lt; &lt; - - ≥ = 0 1 µ (4) a u x x u a u a x a x U      ≤ &lt; &lt; - - ≥ = 1 0 µ (<label>5</label></formula><formula xml:id="formula_4">)</formula><p>where x is the independent variable denoting gray level and a and u are the parameters determining the shape of the above two membership functions. The optimal parameters a and u will be obtained, if an optimization objective function is satisfied. The gray-levels greater than a have the membership of 1.0 for fuzzy set B A , which means the pixels with these gray-levels definitely belong to the attended areas. On the contrary, when the gray levels is smaller than u, the membership for fuzzy set B A becomes 0, which means the pixels with these gray-levels do not belong to the attended areas. Similarly, B U has opposite variation form. While, the pixels with the gray-levels between a and u have the membership of (0, 1) for fuzzy sets B A and B U according to the definition ( <ref type="formula">4</ref>) and ( <ref type="formula" target="#formula_3">5</ref>), respectively.</p><p>Assuming that the prior probabilities of the attended areas and the unattended areas are approximately equal, the optimal partition requires the difference between the prior entropies of attended areas and that of unattended areas reaches the minimum. Sahoo et al. proposed a minimal difference of entropy as metric to obtain optimal threshold for image segmentation <ref type="bibr" target="#b23">[23]</ref>. We modify this measure according to fuzzy set definition as follows,</p><formula xml:id="formula_5">Г(a, u) = [ HA (a, u) -HU (a, u) ] 2<label>(6)</label></formula><p>where H A (a, u) and H U (a, u) are the prior entropies of fuzzy sets, attended areas and unattended areas, respectively. They are calculated by The global minima of Г(a, u) indicates the optimal fuzzy partition, i.e., optimal parameters a and u are found. This criterion can be expressed as:</p><formula xml:id="formula_6">( ) ( ) ( ) (<label>)</label></formula><p>(a, u) = arg min (Г(a, u)) <ref type="bibr" target="#b9">(9)</ref> With the optimal a and u, fuzzy growing process is performed on the saliency map. A number of initial attention seeds are needed first. The criteria for seed selection are: 1) the seeds must have maximum local contrast; and 2) the seeds should belong to the attended areas. Sequentially, starting from each seed, the pixels with the gray-levels satisfying the following criteria will be grouped. <ref type="bibr" target="#b10">(10)</ref> where s = (a + u)/2. The probabilities of the gray-level s belonging to attended areas and unattended areas are all 0.5, see (4) and <ref type="bibr" target="#b5">(5)</ref>. Then, the new group members are used as seeds to do iterative growing. Such fuzzy growing process simulates the bottom-up search process of human perception. Figure <ref type="figure" target="#fig_1">2(d)</ref> illustrates fuzzy 2-partition of saliency map by three layers which denote the gray level higher than a (highest), s (middle), and u (lowest), correspondingly. Figure <ref type="figure" target="#fig_1">2</ref>(e) shows the result of fuzzy growing, two main objects in scene being accurately detected and segmented.</p><formula xml:id="formula_7">seed j i C C ≤ , and s C j i &gt; ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IMAGE ATTENTION ANALYSIS</head><p>With the two techniques introduced in Section 2 and Section 3, we propose a framework for image attention analysis, which integrates bottom-up process and top-down process by simulating human perception. The top-down process is not the scope of this paper, which involves high level semantics understanding or late selection of human perception, such as object detection and recognition. However, as face detection technology has been approaching to mature, a face detection module <ref type="bibr" target="#b24">[24]</ref> is integrated into our framework as a component of top-down process. The bottom-up process corresponding to early selection of human perception is the focus of this paper.</p><p>As shown in Figure <ref type="figure">3</ref>, the image attention analysis framework is composed of two parts, bottom-up, and top-down. The former outputs three-level attentions based on saliency map from low level to high level, including attended points, attended areas, and attended view. The latter outputs the result of face detection. The two processes may have some interactions, which have been proven by neurobiology. However, how they interact is not very clear. Therefore, we simply implement a single direction process. That is, the face detection result may improve the outputs of attended areas and attended view. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. Architecture of image attention analysis</head><p>Within the workflow of bottom-up process, an original image passes through a pre-processing module first, by which the original image is transferred to a quantized block image, as shown in Figure <ref type="figure" target="#fig_1">2</ref>(b). The pre-processing steps include:</p><p>1) Image resizing. First of all, the original image is resized to a uniform image with its aspect ratio unchanged. The advantages of resizing are twofold, that is, all images are considered in the same scale and the computational complexity is effectively reduced.</p><p>2) Color space transformation. As LUV space is consistent with human color perception system well, the resized image is transformed from RGB space to LUV space.</p><p>3) Color quantization. Human vision perception usually is more sensitive to the changes in smooth areas than in texture areas. So color quantization is performed to make color smooth in texture areas. Here, peer group filtering method <ref type="bibr" target="#b25">[25]</ref> is employed.</p><p>4) Block dividing. In order to further smooth texture areas and reduce computational cost, the image is divided into the blocks with n×n pixels, namely, each perception unit of perceive field has n×n pixels. On each perception unit, the means of LUV elements are computed separately. In this manner, a quantized block image is obtained.</p><p>Based on such quantized block image, or perceive field, contrast is calculated on each perception unit. Then, the contrasts are smoothed and normalized to [0, 255] to form saliency map, as shown in Figure <ref type="figure" target="#fig_1">2(c</ref>). With such contrast-based saliency map, the three-level attentions are extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Attended Point Extraction</head><p>Attended point detection is analogous to the lowest level of human attention, which is directly caused by outside stimulus. Therefore, attended points do not have any semantics, which may be directly detected from saliency map by Winner-Take-All process like the work in <ref type="bibr" target="#b15">[15]</ref>. That is, the attended points are those points with local maximum contrast in saliency map. Figure <ref type="figure" target="#fig_1">2</ref>(f) shows a sample of attended points in image Figure <ref type="figure" target="#fig_1">2</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attended Area Extraction</head><p>Attended area extraction may be regarded as an extension of attend point detection, including two processes: seed selection and fuzzy growing. According to Section 3, the attended areas' seeds are the subset of attended points. We select those points with the contrast greater than a as seeds. Then, from each seed, fuzzy growing is performed until no candidate of perception units can be grouped. This process simulates early stage of human perception during which human search a semantic object looks like what they have seen. As shown in Figure <ref type="figure" target="#fig_1">2</ref>(d) and (e), the reasonable results are obtained. If more than one initial seeds belong to one region/object, the areas grown from these seeds are progressively merged into an area during fuzzy growing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attended View Extraction</head><p>Motivated by one of Gestalt laws, an attention center as well as an attended view is extracted from the saliency map. The Gestalt school of psychology <ref type="bibr" target="#b26">[26]</ref> has played a revolutionary role with its novel approach to visual form. Although the Gestalt theory is a non-computational theory of visual forms, which is a disadvantage for practical engineering applications, it provides some useful guidance.</p><p>According to one of Gestalt laws that visual forms may possess one or several centers of gravity about which the form is organized <ref type="bibr" target="#b27">[27]</ref>, we assume that there is a center of gravity in a saliency map, called attention center, which corresponds to the vision center of image. Based on this attention center, the whole image can be organized in the form of information maximization.</p><p>In our implementation, we define an attended view as a rectangle V(C 0 , W, H), where C 0 denotes attention center, W and H are the width and height of rectangle respectively. If contrast level in saliency map is regarded as density, the attention center is the centroid of saliency map. Similarly, there is a relationship between the size of attended view and the 1 st order central moment of saliency map. Specifically, let (x 0 , y 0 ) denote attention center, and (w', h') denote the 1 st order central moment of saliency map, the attention center and the attended view's width and height are computed by ( <ref type="formula" target="#formula_8">11</ref>) and ( <ref type="formula">12</ref>) respectively,</p><formula xml:id="formula_8">       × = × = ∑ ∑ - = - = 1 0 , 0 1 0 , 0 1 1 M i j i N j j i j C CM y i C CM x (<label>11</label></formula><formula xml:id="formula_9">)</formula><p>where</p><formula xml:id="formula_10">∑∑ - = - = = 1 0 1 0 , M i N j j i C CM</formula><p>is the 0 th order moment of saliency map.</p><formula xml:id="formula_11">   ⋅ = = ⋅ = = ' 2 2 ' 2 2 h h H w w W α α (<label>1 2 )</label></formula><p>where α &gt; 1 is a constant coefficient. w' and h' are computed by the 1 st order central moments of saliency map along x-axis and yaxis respectively, and the 0 th order moment CM, expressed by <ref type="bibr" target="#b13">(13)</ref>.</p><formula xml:id="formula_12">       - × = - × = ∑ ∑ - = - = 1 0 0 1 0 0 1 1 M i j i N j j i y j C CM h x i C CM w , , ' '<label>(13)</label></formula><p>The process of attended view extraction can be viewed as the last stage of human perception. That is, when human complete their attention searching, they usually adjust their views according to the attention center of image and attention distributions in whole image. In Figure <ref type="figure">4</ref>, col.(c) gives some examples of attended view.</p><p>Finally, if face detection results were available, attended areas and attended view could be improved according to the semantics provided by face rectangles, such as position of heads and shoulders. Actually, attended areas and attended view are also able to speed up the process of face searching, though it is not implemented in the current system. As our system does not strictly simulate the biological structure of human perception, the computational complexity is effectively reduced so that it can be easily implemented in high level applications of personal computers or integrated into real-time vision systems.</p><p>By using the three-level image attention analysis, the performance of information searching in large image library can be greatly improved in accuracy, speed and display aspects. The attended view effectively accelerates feature extraction during image retrieval by extracting a sub-image with the most important information. The attended areas provide more details about important areas for region-based image retrieval. Also, both attended view and attended areas may facilitate users to quickly browse important parts of image in a variety of display screens in different sizes. Although the attended points lack of semantics, they provide users possible search paths on images, which can be utilized to determine the browsing sequence of image regions. Therefore, the proposed image attention analysis framework not only provides effective supports to visual perception systems, but also can be used in such high level multimedia applications as <ref type="bibr" target="#b1">[1]</ref> and <ref type="bibr" target="#b2">[2]</ref>. In fact, the proposed framework can be employed by video attention analysis system at the stage of salient region detection in frames, such as <ref type="bibr" target="#b19">[19]</ref>. Meanwhile, this framework is also the supplement for our user attention model <ref type="bibr" target="#b18">[18]</ref>, which may significantly improve the performance of user attention model, both on effectiveness and efficiency. Figure <ref type="figure">4</ref> displays some results output from our system. It can be seen that our system is not sensitive to the scale of attended areas. Attended views successfully crop the main bodies of images, which are usually larger than attended areas. Most of detected attended areas give satisfactory results. For example, in Figure <ref type="figure" target="#fig_3">4A</ref>(1-7) and 4B(2), the dominant objects are accurately detected.</p><p>For some complex cases, our system may output multi-scale or multiple attended areas. For example, in Figure <ref type="figure" target="#fig_3">4A</ref>(8), a family photo, three attended areas are detected, including the father, the son and the region containing both the father and the son. If there are multiple clearly separate dominant objects/regions in an image, our system may distinguish the different attended areas, such as Figure 4A <ref type="bibr" target="#b9">(9)</ref>. However, if the objects/regions are not clearly isolated, the connected objects/regions are grouped, see Figure <ref type="figure" target="#fig_4">4B</ref>(1). Besides, we may find that the attended areas are the same as the attended views in Figure <ref type="figure" target="#fig_3">4A</ref>(10-12) and 4B(3). In Figure 4A <ref type="bibr" target="#b10">(10)</ref>, the dominant object occupies almost main body of image, so the attended area equals the attended view coincidently. As the scenes are over diverse in Figure <ref type="figure" target="#fig_3">4A</ref>(11-12), our system dose not segment attended areas particularly. Similarly, for portrait images (Figure <ref type="figure" target="#fig_4">4B</ref>(3)), the grown attended areas may larger or slightly smaller than attended view due to the complexity of contrast distribution in saliency map, our system uses attended view as attended area. The all attended points detected are reasonable except in portrait images, such as Figure <ref type="figure" target="#fig_4">4B</ref>(3). However, they still provide useful cues for attention search. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATIONS AND DISCUSSIONS</head><p>Due to the subjectivity of human attention perception, there is not a standardized objective correctness measure for image attention analysis evaluation. Therefore, we have carried out a user study experiment to evaluate the proposed approach. The experiment composes of three parts corresponding to the three levels of attention analysis. 20 human subjects were invited to take part in user study. These subjects are not computer experts and have no any special computer knowledge. They were required to assign one of assessments, GOOD, ACCEPT, or FAILED, to the results output by our system, i.e., attended view, attended areas, attended points, separately. The statistical results of evaluation are displayed in the following three tables by percentage averaging over all subjects and images in each of the three photo categories. Table <ref type="table" target="#tab_2">1</ref> shows the evaluation results of attended view. We can see that the results marked as "GOOD" account for about 84%, while only 2% of cases are marked as "FAILED". As amateurish family photos are often poor in composition, the effectiveness of attend view adjustment in the category of F. Photos is more distinctive than that in other two categories. Although the attended points do not reflect semantics, we still evaluate them by subjective method to investigate the relationship between low-level stimulus and high level semantics. From Table <ref type="table" target="#tab_4">3</ref>, we can see that 47% of assessments are "GOOD" which is much lower than that of other two evaluations. Meanwhile, the proportion of "FAILED" cases is also higher than that of others, that is, 16%. This result verifies our presupposition that there is a big gap between low level features and high level semantics or human perceptions. However, 84% of cases are acceptable if both the "GOOD and "ACCEPT" cases are considered. It indicates that the attended points have some correlativity with attended regions in images, and are able to guide users to search semantics from certain reasonable starting points. The experiment discussed in this section provides a basic verification of correctness for the automatically detected attentions in images. Although, the applications based on the proposed approach need for further evaluations, the encouraging results presented in this paper may boost the applications of the proposed framework to content-based image retrieval, adaptive image delivery and active vision systems.</p><p>The aim of this work is to provide a feasible solution for image attention analysis, which is totally different from the other systems focusing on computer vision issues. For example, the goal and result form are all different. Moreover, most of other attention analysis algorithms are time-consuming and not suitable for personal computer implementation. Therefore, we performed a subjective evaluation instead of comparing our system with other attention analysis systems. The satisfied user study results have shown the effectiveness and practicability of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>This paper presents an image attention analysis framework which simulates the two human cognition processes: bottom-up and topdown. This framework outputs three-level attentions including attended view, attended areas, and attended points. The attended points are analogous to the direct perceptions of stimulus. The attended area searching simulates the early selection of human perception. The attended view may be regarded as the result of late selection of human perception. In addition, two issues are discussed in this paper. By investigating the effects of contrast in human perception, a contrast-based saliency map is proposed, which is the base of our framework. The other issue is how to search attended areas in saliency map. A fuzzy growing algorithm is developed to simulate early selection of human perception, by which attended areas are accurately located. User study results show that the proposed approaches are effective in accuracy and efficient in computation. The proposed framework can be easily employed or integrated into a variety of vision systems and visual content analysis related multimedia applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Contrast behind color, texture and shape perceptionAs shown in Figure1, there are three pairs of synthesized images. In Figure1(a), there is a red box on black background. It goes without saying that the attended area is the red box. Red color is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Contrast-based image attention analysis (a) original image; (b) quantized block image; (c) saliency map; (d) fuzzy growing; (e) attended areas; (f) attended points. By normalizing to [0, 255], all contrasts C i,j on the perception units form a saliency map. A sample of contrast-based saliency map is shown in Figure 2(c). In our implementation, the colors in LUV space are used as stimulus on perceive field, and the difference d is computed by Gaussian distance. The image attention analysis is performed on such local contrast based saliency map, because this kind of saliency map not only reflects</figDesc><graphic coords="3,54.00,541.68,75.24,50.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4A .</head><label>4A</label><figDesc>Figure 4A. Some sample results of attention analysis col.(a) original image, col.(b) contrast-based saliency map, col.(c) attended view, col.(d) attended areas, col.(e) attended points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4B .</head><label>4B</label><figDesc>Figure 4B. Some sample results of attention analysis col.(a) original image, col.(b) contrast-based saliency map, col.(c) attended view, col.(d) attended areas, col.(e) attended points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 . Attended view evaluation</head><label>1</label><figDesc>Table2lists the results of attended area detection. 67% of images are considered as "GOOD", and the "FAILED" cases made up only 6%. The failed cases are mainly caused by poor exposure of photos, either underexposure or overexposure, and weak contrasts. As the contents in F. Photo are often cluttered, the satisfactory of the detected attended areas in this category is the lowest. On the contrary, the best results are obtained in Portrait category, because the themes of portraits are very explicit. The collection in Corel Draw category includes a variety of photos, so the results are near to the average.</figDesc><table><row><cell></cell><cell>GOOD</cell><cell>ACCEPT</cell><cell>FAILED</cell></row><row><cell>Corel Draw</cell><cell>0.85</cell><cell>0.14</cell><cell>0.01</cell></row><row><cell>F. Photo</cell><cell>0.86</cell><cell>0.13</cell><cell>0.01</cell></row><row><cell>Portrait</cell><cell>0.82</cell><cell>0.16</cell><cell>0.02</cell></row><row><cell>Avg.</cell><cell>0.84</cell><cell>0.14</cell><cell>0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . Attended areas evaluation</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>GOOD</cell><cell>ACCEPT</cell><cell>FAILED</cell></row><row><cell>Corel Draw</cell><cell>0.68</cell><cell>0.27</cell><cell>0.05</cell></row><row><cell>F. Photo</cell><cell>0.60</cell><cell>0.31</cell><cell>0.09</cell></row><row><cell>Portrait</cell><cell>0.74</cell><cell>0.22</cell><cell>0.05</cell></row><row><cell>Avg.</cell><cell>0.67</cell><cell>0.27</cell><cell>0.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 . Attended points evaluation</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>GOOD</cell><cell>ACCEPT</cell><cell>FAILED</cell></row><row><cell>Corel Draw</cell><cell>0.56</cell><cell>0.33</cell><cell>0.12</cell></row><row><cell>F. Photo</cell><cell>0.44</cell><cell>0.41</cell><cell>0.15</cell></row><row><cell>Portrait</cell><cell>0.40</cell><cell>0.39</cell><cell>0.21</cell></row><row><cell>Avg.</cell><cell>0.47</cell><cell>0.37</cell><cell>0.16</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Effective Region-Based Image Retrieval Framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia</title>
		<meeting>of ACM Multimedia<address><addrLine>Juan Les Pins, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-12-01">2002. December 1-6, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Visual Attention Model for Adapting Images on Small Displays</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Principles of Psychology</title>
		<author>
			<persName><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1890">1890</date>
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Perception and communication</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Broadbent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
			<publisher>Pergamon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attetnion: Some theoretical considerations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature analysis in early vision: evidence from search asymmetries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gormican</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology Review</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perception of features and objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Attention</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: towards the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Some reflections on visual awareness</title>
		<author>
			<persName><forename type="first">F</forename><surname>Crik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Cold Spring Harbor Symposia on Quantitative Biology</title>
		<meeting>the Cold Spring Harbor Symposia on Quantitative Biology</meeting>
		<imprint>
			<publisher>Cold Spring Harbor Laboratory Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">LV</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling visual attention via selective tuning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y K</forename><surname>Wai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="507" to="545" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deploying visual attention: The guided search model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Cave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI and the Eye</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</editor>
		<imprint>
			<publisher>John Wiley &amp; Sons Ltd</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="79" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Computational architectures for attention</title>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<editor>R. Parasuraman,</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="163" to="186" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>The attentive brain</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">attentive Mechanism for dynamic and static scene analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milanese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2428" to="2434" />
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Expectation-based selective attention dor visual monitoring and control of a robot vehicle</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous System</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="329" to="344" />
			<date type="published" when="1997-12">Dec.1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Model of Saliency-Based Visual Attention for Rapid Scene Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">VISIT: A neural model of covert attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="420" to="427" />
			<date type="published" when="1991">1991</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Model of Motion Attention for Video Skimming</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Image Processing</title>
		<meeting>of International Conference on Image essing<address><addrLine>Rochester, N.Y., U.S.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A User Attention Model for Video Summarization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia</title>
		<meeting>of ACM Multimedia<address><addrLine>Juan Les Pins, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-12-01">2002. Dec. 1-6, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Salient Region Detection and Tracking in Video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Multimedia &amp; Expo 2003</title>
		<meeting>of IEEE International Conference on Multimedia &amp; Expo 2003<address><addrLine>Baltimore, Maryland, U.S</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09">July 6-9, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Lamming</surname></persName>
		</author>
		<title level="m">Contrast Sensitivity. Chapter 5</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cronly-Dillon</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Macmillan Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>Vision and Visual Dysfunction</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fuzzy Sets and Fuzzy Logic: Theory and Applications</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Klir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probability measures of fuzzy events</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="421" to="427" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Threshold selection using a minimal histogram entropy difference</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Slaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1976" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical Learning of Multi-View Face Detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">2002. May, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Peer group filtering and perceptual color image quantization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kenney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Symposium on Circuits and Systems</title>
		<meeting>of IEEE International Symposium on Circuits and Systems</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contemporary theory of visual form perception: III</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zusne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The global Theories, chapter</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1970">1970</date>
			<biblScope unit="page" from="108" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Principles of Gestalt psychology</title>
		<author>
			<persName><forename type="first">K</forename><surname>Koffka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harcourt Brace Jovanovic</title>
		<imprint>
			<date type="published" when="1935">1935</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
