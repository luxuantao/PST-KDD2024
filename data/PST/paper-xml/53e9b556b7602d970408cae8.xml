<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selective spatio-temporal interest points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-10-31">31 October 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bhaskar</forename><surname>Chakraborty</surname></persName>
							<email>bhaskar@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center (CVC)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science (UAB)</orgName>
								<address>
									<addrLine>Edifici O, Campus UAB</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Holte</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Architecture, Design and Media Technology</orgName>
								<orgName type="institution">Aalborg Universtity</orgName>
								<address>
									<postCode>DK-9220</postCode>
									<settlement>Aalborg</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Architecture, Design and Media Technology</orgName>
								<orgName type="institution">Aalborg Universtity</orgName>
								<address>
									<postCode>DK-9220</postCode>
									<settlement>Aalborg</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jordi</forename><surname>Gonz√†lez</surname></persName>
							<email>jordi.gonzalez@cvc.uab.cat</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center (CVC)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science (UAB)</orgName>
								<address>
									<addrLine>Edifici O, Campus UAB</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Selective spatio-temporal interest points</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-10-31">31 October 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">FF08FEED073A9BFD6FAF2C330BD0C4E0</idno>
					<idno type="DOI">10.1016/j.cviu.2011.09.010</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Action recognition Complex scenes Multiple actors Spatio-temporal interest points Local descriptors</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in the field of human action recognition points towards the use of Spatio-Temporal Interest Points (STIPs) for local descriptor-based recognition strategies. In this paper, we present a novel approach for robust and selective STIP detection, by applying surround suppression combined with local and temporal constraints. This new method is significantly different from existing STIP detection techniques and improves the performance by detecting more repeatable, stable and distinctive STIPs for human actors, while suppressing unwanted background STIPs. For action representation we use a bagof-video words (BoV) model of local N-jet features to build a vocabulary of visual-words. To this end, we introduce a novel vocabulary building strategy by combining spatial pyramid and vocabulary compression techniques, resulting in improved performance and efficiency. Action class specific Support Vector Machine (SVM) classifiers are trained for categorization of human actions. A comprehensive set of experiments on popular benchmark datasets (KTH and Weizmann), more challenging datasets of complex scenes with background clutter and camera motion (CVC and CMU), movie and YouTube video clips (Hollywood 2 and YouTube), and complex scenes with multiple actors (MSR I and Multi-KTH), validates our approach and show state-of-the-art performance. Due to the unavailability of ground truth action annotation data for the Multi-KTH dataset, we introduce an actor specific spatio-temporal clustering of STIPs to address the problem of automatic action annotation of multiple simultaneous actors. Additionally, we perform cross-data action recognition by training on source datasets (KTH and Weizmann) and testing on completely different and more challenging target datasets (CVC, CMU, MSR I and Multi-KTH). This documents the robustness of our proposed approach in the realistic scenario, using separate training and test datasets, which in general has been a shortcoming in the performance evaluation of human action recognition techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Human action recognition</head><p>In this paper, we address the task of human action recognition in complex scenes in diverse and realistic settings (background clutter, camera motion, occlusions and illumination variations). During the last decade action recognition has been an important topic in the ''looking at people'' domain <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. A large number of methods for human action recognition have been proposed, stretching from human model and trajectory-based methods towards holistic and local descriptor-based methods.</p><p>Most of these previous approaches for human action recognition are constrained to well-controlled environments. Among the proposed action recognition techniques, one type of approach uses motion trajectories to represent actions and it requires target tracking <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. However, due to the difficulty in building robust object tracker only limited success has been achieved. Another type of approach uses sequences of silhouettes or body contours to model actions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> and it requires background subtraction. Boiman and Irani <ref type="bibr" target="#b6">[7]</ref> extract densely sampled local video patches for detecting irregular actions in videos with simple background. Rodriguez et al. <ref type="bibr" target="#b7">[8]</ref> designed a novel method to analyze the filtering responses of different actions. This approach has difficulties in aligning non-repetitive actions in complex scenes. Moreover, some researchers model the configuration of the human body and its evolution in the time domain <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, and others solely perform action recognition from still images by computing pose primitives <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>The research trend in the field of action recognition has, recently, led to more robust techniques <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, which to some extent are applicable for action recognition in complex scenes. Action recognition in complex scenes is an extremely difficult task, due to several challenges, like background clutter, camera motion, occlusions and illumination variations. To address these challenges, several methods, like tree-based template matching <ref type="bibr" target="#b13">[14]</ref>, tensor canonical correlation <ref type="bibr" target="#b14">[15]</ref>, prototype based action matching <ref type="bibr" target="#b15">[16]</ref>, a hierarchical approach <ref type="bibr" target="#b17">[18]</ref>, incremental discriminant analysis of canonical correlation <ref type="bibr" target="#b19">[20]</ref>, latent pose estimation <ref type="bibr" target="#b20">[21]</ref> and generalized Hough transform <ref type="bibr" target="#b21">[22]</ref> have been proposed. Most of these methods are very complex and require preprocessing, like segmentation, tree data structure building, target tracking, background subtraction or a human body model. Other methods  for action recognition in complex scenes, which demand less or no preprocessing, apply STIP detectors and local descriptors to characterize and encode the video data, and thereby perform action classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Spatio-temporal interest points</head><p>The extraction of appropriate features is critical to action recognition. Ideally, visual features are able to handle the following challenges for robust performance: (i) scale, rotation and viewpoint variations of the camera, (ii) performance speed variations for different people, (iii) different anthropometry of the actors and their movement style variations, and (iv) cluttered backgrounds and camera motion. The ultimate goal is to be able to perform reliable action recognition applicable for video indexing and search, intelligent human computer interaction, video surveillance, automatic activity analysis and behavior understanding. Recently, the use of STIPs has received increasing interest for local descriptor-based action recognition strategies. STIP-based methods avoid the temporal alignment problem, are exceptionally invariant to geometric transformations, and therefore distorted less by changes in scale, rotation and viewpoint than image data. Features are locally detected, thus inherently robust to occlusion and do not suffer from conventional figure-ground segmentation problems (imprecise segmentation, object splitting and merging etc.). Additionally, partial robustness to illumination variations and background clutter are incorporated.</p><p>Laptev and Lindeberg first proposed STIPs for action recognition <ref type="bibr" target="#b44">[45]</ref>, by introducing a space-time extension of the popular Harris detector <ref type="bibr" target="#b45">[46]</ref>. They detect regions having high intensity variation in both space and time as spatio-temporal corners. The STIP detector of <ref type="bibr" target="#b44">[45]</ref> usually suffers from sparse STIP detection. Later several other methods for detecting STIPs have been reported <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref>. Doll√†r et al. <ref type="bibr" target="#b46">[47]</ref> improved the sparse STIP detector by applying temporal Gabor filters and select regions of high responses. Dense and scale-invariant spatio-temporal interest points were proposed by Willems et al. <ref type="bibr" target="#b49">[50]</ref>, as a spatio-temporal extension of the Hessian saliency measure, previously applied for object detection <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>. Instead of applying local information for STIP detection Wong and Cipolla <ref type="bibr" target="#b50">[51]</ref> propose a global information-based approach. They use global structural information of moving points and select STIPs according to their probability of belonging to the relevant motion. Although promising results have been reported, these methods are quite vulnerable to camera motion and cluttered background, since they detect interest points directly in a spatio-temporal space.</p><p>Hence, STIP-based methods have some shortcomings. First of all, (i) STIPs focus on local spatio-temporal information instead of global motion, thus the detection of STIPs on human actors in complex scenes might fall on cluttered backgrounds, especially if the camera is not fixed. Secondly, (ii) the stability of STIPs varies due to the local properties of the detector, and therefore some STIPs can be unstable and imprecise, as a result they have low repeatability or the local descriptors can become ambiguous. Thirdly, (iii) redundancy can occur in the local descriptors extracted from the surrounding image region of two adjacent STIPs. According to Schmid et al. <ref type="bibr" target="#b53">[54]</ref> robust interest points should have high repeatability (geometric stability) and information content (distinctive-ness of features). Furthermore, Turcot and Lowe <ref type="bibr" target="#b54">[55]</ref> investigate and report that it is better to select a small subset of useful features for recognition problems, than a larger set of unreliable features which represent irrelevant clutter. We address these three shortcomings, by first (i) detecting Spatial Interest Points (SIPs), then (ii) suppressing unwanted background points, and finally (iii) imposeing local and (iv) temporal constraints, achieving a set of selective STIPs which are more robust to these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Local descriptors</head><p>Several local descriptors have been proposed in the past few years <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b58">59]</ref>. Local feature descriptors extract shape and motion in the neighborhoods of selected STIPs using image measurements, such as spatial or spatio-temporal image gradients or optical flow. Laptev et al. <ref type="bibr" target="#b29">[30]</ref> introduced a combined descriptor to characterize local motion and appearance by computing histograms of spatial gradient (HOG) and optic flow (HOF) accumulated in space-time neighborhoods of detected interest points. Willems et al. <ref type="bibr" target="#b49">[50]</ref> proposed the Extended SURF (ESURF) descriptor, which extends the image SURF descriptor <ref type="bibr" target="#b59">[60]</ref> to videos. The authors divide 3D patches into cells, where each cell is represented by a vector of weighted sums of uniformly sampled responses of the Haar-wavelets along the three axes. Doll√†r et al. <ref type="bibr" target="#b46">[47]</ref> proposed a descriptor along with their detector. The authors concatenate the gradients computed for each pixel in the neighborhood into a single vector and apply Principal Component Analysis (PCA) to project the feature vector onto a low dimensional space. Compared to the HOG-HOF descriptor proposed by Laptev et al. <ref type="bibr" target="#b29">[30]</ref>, it does not distinguish the appearance and motion features. The 3D-SIFT descriptor was developed by Scovanner et al. <ref type="bibr" target="#b58">[59]</ref>. This descriptor is similar to the Scale Invariant Feature Transformation (SIFT) descriptor <ref type="bibr" target="#b60">[61]</ref>, except that it is extended to video sequences by computing the gradient direction for each pixel spatio-temporally in three-dimensions. Another extension of the popular SIFT descriptor was proposed by Kl√§ser et al. <ref type="bibr" target="#b55">[56]</ref>. It is based on histograms of 3D gradient orientations, where gradients are computed using an integral video representation. Another popular descriptor is the N-jets <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b61">62]</ref>. An N-jet is the set of partial derivatives of a function up to order N, and is usually computed from a scale-space representation. The N-jets is an inherently strong local motion descriptor, where the two first levels implicitly represent velocity and acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Vocabulary building strategies</head><p>Bag-of-video words (BoV) models have become popular for generic action recognition <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b62">63]</ref>, whereas other techniques based on co-occurrence of STIP based motion features are also used <ref type="bibr" target="#b63">[64]</ref>. The basic BoV model computes and quantizes the feature vectors, extracted at the detected STIPs in the video, into video-words. Finally, the entire video sequence is represented by a statistical distribution of those video-words. For classification, discriminative learning models such as SVM <ref type="bibr" target="#b46">[47]</ref> and generative models, e.g. pLSA <ref type="bibr" target="#b50">[51]</ref>, have achieved excellent performance for action recognition. Since the BoV model does not provide a spatio-temporal distribution of features, the spatial correlogram and spatio-temporal pyramid matching are applied <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> to capture the spatio-temporal relationship between local features. Additionally, vocabulary compression techniques are used to reduce the final feature space <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. We introduce a novel vocabulary building strategy by first applying a spatial pyramid and then compress the vocabulary at each pyramid level, achieving a compact and efficient pyramid representation of actions. This is different from <ref type="bibr" target="#b32">[33]</ref>, where first a vocabulary is computed, then it is compressed, and finally a spatial correlogram and a spatio-temporal pyramid are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.">Complex scenes</head><p>While reliable human action recognition in simple scenes (KTH <ref type="bibr" target="#b64">[65]</ref> and Weizmann <ref type="bibr" target="#b65">[66]</ref>) has been achieved <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>, the task remains unsolved for complex scenes. These datasets have been recorded in well-controlled environments with clean or simple background, controlled lighting conditions, and no camera motion nor occlusions. In contrast, Real world human actions are often recorded in scenes of high complexity, with cluttered background, illumination variations, camera motion and occluded bodies. Hence, these datasets do not correspond very well to real world scenarios. The mentioned properties make action recognition in complex scenes much more challenging. New datasets for the purpose of evaluation of action recognition algorithms in complex and semi-complex scenes have therefore been produced (CMU <ref type="bibr" target="#b66">[67]</ref>, CVC <ref type="bibr" target="#b67">[68]</ref>, YouTube <ref type="bibr" target="#b31">[32]</ref>, Hollywood 2 <ref type="bibr" target="#b33">[34]</ref>, MSR I <ref type="bibr" target="#b62">[63]</ref> and Multi-KTH <ref type="bibr" target="#b40">[41]</ref>). We utilize all these datasets for evaluation of our approach (see Fig. <ref type="figure" target="#fig_1">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.">Cross-data evaluation</head><p>Conventional approaches usually build a classifier from labeled examples and assume the test samples are generated from the same distribution, which is rarely the case in realistic scenarios. In contrast, cross-data evaluation is highly necessary for commercial systems, where the classifier is trained on a specific dataset during a learning phase and then set up for operation in the field. Additionally, it also prevents the algorithm to benefit from the internal data correlation during the evaluation. Cross-data evaluation is more challenging, since the two dataset have usually been recorded in two different occasions. Only a few authors have recently reported cross-data evaluation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref>. The problem is related to transfer learning known from machine learning, which attempts to develop methods to transfer knowledge learned in one or more source tasks and use it to improve learning in a related target task <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b69">70]</ref>. We conduct a comprehensive set of cross-data experiments to carry out a more realistic evaluation of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7.">Our approach and contributions</head><p>In this work we follow the recent progress and employ a STIP and local descriptor-based recognition strategy. A schematic overview of our approach is outlined in Fig. <ref type="figure">2</ref>. (1) We introduce a novel approach for selective STIP detection, by applying surround suppression combined with local and temporal constraints, achieving robustness to camera motion and background clutter. For action representation we use a BoV model of local N-jet features, extracted at the detected STIPs, to build a vocabulary of visual-words.</p><p>(2) To this end, we introduce a novel vocabulary building strategy by combining (i) a pyramid structure to capture spatial information, and (ii) vocabulary compression to reduce the dimensionality of the feature space, resulting in improved performance and efficiency. Action class-specific SVM classifiers are trained and applied for categorization of natural human actions. (3) We evaluate our approach on both popular benchmark datasets (KTH and Weizmann), more challenging datasets (CVC, CMU), movie and YouTube video clips (Hollywood 2 and YouTube) and perform an exhaustive cross-data evaluation, trained on source dataset (KTH and Weizmann) and tested on more challenging target datasets (CVC, CMU, MSR I and Multi-KTH). Due to the unavailability of ground truth action annotation data for the Multi-KTH dataset, we introduce an actor specific spatio-temporal clustering of STIPs to address the problem of automatic action annotation of multiple simultaneous actors. To observe the performance our automatic STIP clustering-based annotation, we manually annotate the ground truth actions and compare the action recognition accuracies. Finally, we compare our approach to the most popular action recognition techniques and show beyond state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.8.">Paper structure</head><p>The remainder of the paper is organized as follows. We describe our STIP detector and local descriptor-based action representation in Section 2. Section 3 outlines our vocabulary building strategy and narrates the applied classifier for action categorization. Experimental results and comparisons, along with our technique for spatio-temporal clustering of STIPs for automatic action annotation of Multi-KTH, are reported in Section 4, followed up by concluding remarks in Section 5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Selective spatio-temporal interest points</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Detection of spatial interest points</head><p>Existing STIP detectors <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> are vulnerable to camera motion and moving background in videos, and therefore detect unwanted STIPs in the background (see Fig. <ref type="figure" target="#fig_4">4</ref>). Cao et al. <ref type="bibr" target="#b22">[23]</ref> have recently reported, that of all the STIPs detected by Laptev's STIP detector <ref type="bibr" target="#b44">[45]</ref>, only about 18% correspond to the three actions performed by the actors in the MSR I dataset <ref type="bibr" target="#b62">[63]</ref>, while the rest of the STIPs (82%) belong to the background. To overcome this problem, we first detect the spatial interest points (SIPs), then perform background suppression and impose local and temporal constraints (see Fig. <ref type="figure" target="#fig_2">3</ref>). We apply the basic Harris corner detector <ref type="bibr" target="#b45">[46]</ref> and compute the first set of interest points with corner strength C r , where r is the spatial scale. Apart from the detected SIPs on the human actors, the obtained spatial corners C r contain a significant amount of unwanted background SIPs (see Fig. <ref type="figure" target="#fig_2">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Suppressing background interest points</head><p>The main idea of our spatial interest point suppression originates in the fact that most corner points detected in the background texture or on non-human objects follow some particular geometric pattern, while those on humans do not have this property. For suppression we use a surround suppression mask (SSM) for each interest point, taking the current point under evaluation as the center of the mask. We then estimate the influence of all surrounding points of the mask on the central point, and accordingly, a suppression decision is taken. The idea is motivated by Grigorescu et al. <ref type="bibr" target="#b70">[71]</ref>, where surround suppression is used for texture edges to improve object contour and boundary detection in natural scenes. The similar concept of surround suppression based on center surround saliency measure is been adopted in tracking <ref type="bibr" target="#b71">[72]</ref>, spatio-temporal saliency algorithm <ref type="bibr" target="#b72">[73]</ref> and detection of suspicious coincidences in visual recognition <ref type="bibr" target="#b73">[74]</ref>. We implement surround suppression by computing an inhibition term for each point of C r . For this purpose we introduce a gradient weighting factor 4 H,r (x, y, x √Ä u, y √Ä v), which is defined as:</p><p>4 H;r √∞x; y; x √Ä u; y √Ä v√û ¬º j cos√∞H r √∞x; y√û √Ä H r √∞x √Ä u; y √Ä v√û√ûj √∞1√û where H r (x, y) and H r (x √Ä u, y √Ä v) are the gradients at point (x, y) and (x √Ä u, y √Ä v), respectively; u and v define the horizontal and vertical range of the SSM. If the gradient orientations at point (x, y) and (x √Ä u, y √Ä v) are identical, the weighting factor attains its maximum (4 H,r = 1), while the value of the factor decreases with the angle difference and reaches a minimum (4 H,r = 0), when the two gradient orientations are orthogonal. Hence, the surrounding interest points which have the same orientation, as that of (x, y), will have a maximal inhibitory effect.</p><p>For each interest point C r (x, y), we define a suppression term t r (x, y) as the weighted sum of gradient weights in the suppression surround of that point:</p><formula xml:id="formula_0">t r √∞x; y√û ¬º Z Z X C r √∞x √Ä u; y √Ä v√û √Ç 4 H;r √∞x; y; x √Ä u; y √Ä u√ûdu dv<label>√∞2√û</label></formula><p>where X is the image coordinate domain. We now introduce an operator C a,r (x, y), which takes its inputs: the corner magnitude C r (x, y) and the suppression term t r (x, y):</p><p>C a;r √∞x; y√û ¬º H√∞C r √∞x; y√û √Ä at r √∞x; y√û√û</p><p>where H(z) = z when z P 0 and zero for negative z values. The factor a controls the strength of the surround suppression. If no interest points have been detected in the surrounding texture of a given point, the response of the operator retains the original corner magnitude C r (x, y). However, if a large number of interest points are detected in the surrounding background texture, the suppression term t r (x, y) will be higher, resulting in a suppression of the current interest point under evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Imposing local constraints</head><p>We select a final set of interest points from the surround suppression responses C a,r (Eq. ( <ref type="formula" target="#formula_1">3</ref>)) by applying non-maxima Fig. <ref type="figure">2</ref>. A schematic overview of the system structure and data flow pipeline of our approach. suppression, similar to Grigorescu et al.'s method for suppressing gradients <ref type="bibr" target="#b70">[71]</ref>. Non-maxima suppression thins the areas in which C a,r is non-zero to one-pixel wide candidate contours as follows: for each position (x, y), the two responses C a,r (x 0 , y 0 ) and C a,r (x 00 , y 00 ) in adjacent positions (x 0 , y 0 ) and (x 00 , y 00 ), which are intersection points of a line passing through (x, y) with orientation H r (x, y) and a square defined by the diagonal points of an 8-neighborhood, are computed by linear interpolation (see Fig. <ref type="figure" target="#fig_5">5</ref>). A point is kept, if the response C a,r (x, y) is greater than that of the two adjacent points, i.e., it is a local maximum of the neighborhood. Otherwise its value is set to zero. Fig. <ref type="figure" target="#fig_6">6</ref> shows an example of the performance of our inhibitive SIP detector. As can be seen in Fig. <ref type="figure" target="#fig_6">6b</ref> some background SIPs might remain in C a,r . However, these static SIPs can be removed by imposing temporal constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Scale adaptive SIPs</head><p>Scale selection plays an important role in the detection of spatial interest points. Automatic scale selection can be achieved based on the maximization of normalized derivatives expressed over scale, or by the behavior of entropy or error measures evaluated over scale <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b52">53]</ref>. Instead of applying an automatic scale selection, as in <ref type="bibr" target="#b75">[76]</ref>, we apply a multi-scale approach <ref type="bibr" target="#b29">[30]</ref> and compute suppressed SIPs in five different scales S r ¬º r 4 ; r 2 ; r; 2r; 4r √à √â . We follow the idea of scale selection presented by Lindeberg <ref type="bibr" target="#b52">[53]</ref> to keep the best set of SIPs obtained for each scale. The best scales are selected by maximizing the normalized differential invariant,</p><formula xml:id="formula_2">jnorm ¬º r 2c 0 L y L xx :<label>√∞4√û</label></formula><p>where L = g(√Å; r 0 , s 0 ) I, i.e. the image I is convoluted with the Gaussian kernel g; L y is the first order y derivative and L xx is the second order x derivative of L. Lindeberg <ref type="bibr" target="#b52">[53]</ref> report that c ¬º 7   8 performs well in practice to achieve the maximum value of √∞j norm √û 2 for spatial interest point detected at multiple scales. After computing the suppressed SIPs in the scale-space in S r , we apply this scale selection procedure based on the normalized differential invariant (Eq. ( <ref type="formula" target="#formula_2">4</ref>)), and keep the n best SIPs as our final set of suppressed SIPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Imposing temporal constraints</head><p>After obtaining the final set of spatial interest points we impose temporal constraints to neglect static SIPs. We consider two consecutive frames at a time and remove the common interest points, since static interest points do not contribute any motion information:</p><formula xml:id="formula_3">P T a;r ¬º C T a;r n fC T a;r \ C T√Ä1 a;r g √∞<label>5√û</label></formula><p>where C T a;r is the set of interest points in the Tth frame. To avoid the camera motion we have used an interest point matching algorithm along with a temporal Gabor filter response to remove the static interest points (Eq. ( <ref type="formula" target="#formula_3">5</ref>)). The remaining points are the final set of detected STIPs, which are used to extract local features. The pseudo code for the full STIP detection is described in Algorithm 1. Parallelization can be adopted for speed optimization by parallel computation of the for loops in each algorithm (Algorithms 1,3,2, 4 and 5).   Responses at position (x 0 , y 0 ) and (x 00 , y 00 ) along the line passing through (x, y) <ref type="bibr" target="#b70">[71]</ref>. Non-maxima suppression retains the value in the central position (x, y), if it is greater than the values at (x 0 , y 0 ) and (x 00 , y 00 ). 4 H mask ¬º j cos√∞H mask √Ä H mask √∞x;y√û √ûj;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>t√∞x; y√û ¬º cp mask 4 H mask ; 9: cp(x, y) = H(cp (x,y) √Ä at (x,y) ); 10:</p><formula xml:id="formula_4">(x 0 , y 0 ) = round(line(x, x + 1,y, H(x, y))); 11:</formula><p>(x 00 , y 00 ) = round(line(x, x √Ä 1, y, H(x, y)));</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>if (cp(x, y) &gt; cp(x 0 , y 0 )) ^(cp(x, y) &gt; cp(x 00 , y 00 )) then 13: sip sip [ (x, y, r); gabor(i, j, :) = gaborFiler1D(iS(i, j, :)); 4:</p><p>end for 5: end for 6: for i = N ? 2 do 7:</p><p>f 1 = iS(:, :, i); f 2 = iS(:, :, i √Ä 1); 8:</p><p>g 1 = gabor(:, :, i); g 2 = gabor(:, :, i √Ä 1); 9:</p><p>im 1 = iS(:, :, i);im 2 = iS(:, :, i √Ä 1); 10:</p><p>cp f 1 cp f 1 n pointMatch√∞cp f 1 ; cp f 2 ; g 1 ; g 2 ; im 1 ; im 2 √û; 11: end for 12: Return(cp) Algorithm 5. pointMatch: Detect the set of matching corner points in two consecutive frames.</p><p>Require: Image frames: im 1 , im 2 ;</p><p>Corner strengths: cp 1 , cp 2 ; Gabor strength: g 1 , g 2 ;</p><p>Ensure: Detected matching STIPs: mS 1: mP = {}; 2: cornerPoints 1 = find(cp 1 &gt; 0); 3: cornerPoints 2 = find(cp 2 &gt; 0); 4: for Each point (x 1 , y 1 , r 1 ) 2 cornerPoints 1 do 5:</p><formula xml:id="formula_5">H = r 1 ; 6:</formula><p>for Each point (x 2 , y 2 , r 2 ) 2 cornerPoints 2 do 7: similarity ¬º min√∞cp 1 √∞x 1 ;y 1 √û;cp 2 √∞x 2 ;y 2 √û√û min√∞cp 1 √∞x 1 ;y 1 √û;cp 2 √∞x 2 ;y 2 √û√û ; 8:</p><formula xml:id="formula_6">W = r 2 ; 9:</formula><p>if similarity &gt; s sim then 10: </p><formula xml:id="formula_7">a 1 = cropRect(im 1 , x 1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Local feature descriptors</head><p>We use local N-jet features <ref type="bibr" target="#b56">[57]</ref> extracted at the detected STIPs. We extract N-jet features of order-2 in five different temporal scales. Consequently, we end up with a 10-dimensional feature vector, F norm √∞g√∞√Å; r 0 ; s 0 √û √Å I√û ¬º fL; rL x ; rL y ; . . . ; s 2 L tt g √∞ 6√û</p><p>at locally adopted scale level (r 0 , s 0 ) for the image sequence I; where g(√Å; r 0 , s 0 ) is the Gaussian kernel at spatio-temporal scale (r 0 , s 0 ) and r 0 is identical to the scale of the STIP detector; L = g(√Å; r 0 , s 0 ) I, i.e. the image I is convoluted with the Gaussian kernel g; L x is the first order x derivative and L xx is the second order x derivative of L etc. These features are computed with a fixed spatial scale r 0 but with five different temporal scales s 4 ; s 2 ; s; 2s; 4s √Ä √Å . We do not increase the order of N-jet, like Laptev et al. <ref type="bibr" target="#b61">[62]</ref>, since the two first levels represent velocity L xt and acceleration L tt information, while higher order spatial or temporal derivatives are sensitive to noise and do not bring significant additional motion information. The experimental results reported in section 4 document our feature selection by showing state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Vocabulary building and classification</head><p>We apply a BoV model to learn the visual vocabularies of the extracted local motion features. We extend the idea of <ref type="bibr" target="#b31">[32]</ref> by introducing pyramid levels in the feature space, but instead of applying a pyramid at feature level, as in <ref type="bibr" target="#b32">[33]</ref>, we apply it at STIP level. This makes the problem of grouping the local features much simpler yet robust, since our STIPs are detected in a selective and robust manner. Finally, we apply vocabulary compression, at each pyramid level, to reduce the dimensionality of the feature space (see Fig. <ref type="figure">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pyramid structure</head><p>Let I T be the Tth frame of the image sequence I and P T a;r (Eq. ( <ref type="formula" target="#formula_3">5</ref>)) the set of detected STIPs in this frame. We then quantize this set of STIPs into q levels, S ¬º fs 0 ; s 1 ; . . . ; s q√Ä1 g <ref type="bibr" target="#b33">[34]</ref>. For each of these levels, the STIPs are divided based on center of mass information. Accordingly, we group the motion features into different levels of the pyramid. The structure of our 2-level pyramid is illustrated in Fig. <ref type="figure">8</ref>. The horizontal division helps to capture the distinguishing characteristics of arm and leg-based actions, whereas the vertical division distinguishes the actions within each of these arm and leg-based action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Vocabulary compression</head><p>After dividing the motion features into the described pyramid levels, we create initial vocabularies of a relatively large size (about 400 words). To reduce the final feature dimensionality, we use vocabulary compression, as in <ref type="bibr" target="#b31">[32]</ref>, but at each level of the pyramid to achieve a compact yet discriminative visual-word representation of actions.</p><p>Let A be a discrete random variable which takes the value of a set of action classes A = {a 1 , a 2 , . . . , a n }, and W s be a random variable which range over the set of video-words W s = {w 1 , w 2 , . . . , w m } at pyramid level s. Then the information about A captured by W s can be expressed by the Mutual Information (MI), I(A, W s ). Now, let c W s ¬º f ≈µ1 ; ≈µ2 ; . . . ; ≈µk g for k &lt; m, be the compressed video-word cluster of W s . We can measure the loss of quality of the resulting compressed vocabulary c W s , as the loss of MI:</p><formula xml:id="formula_8">Q√∞ c W s √û ¬º I√∞A; W s √û √Ä I√∞A; c W s √û √∞<label>7√û</label></formula><p>To find the optimal compression c W s we use an Agglomerative Information Bottleneck (AIB) approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">AIB compression</head><p>AIB <ref type="bibr" target="#b76">[77]</ref> iteratively compresses the vocabulary W s by merging the visual-words w i and w j which cause the smallest decrease in MI, I(A, W s ). The algorithm can be summarized as follows: Initiate c W s W s , i.e., by taking each video-word of W s as a singleton cluster. Pair-wise distance computation: for every fw i ; w j g 2 c W s ; i &lt; j, the distance d ij (which is a measure of MI) is computed:</p><formula xml:id="formula_9">d ij ¬º √∞p√∞w i √û √æ p√∞w j √û√û √Å JS P ¬Ωp√∞ajw i √û; p√∞ajw j √û<label>√∞8√û</label></formula><p>where JS P [p(ajw i ),p(ajw j )] is the Jensen-Shannon divergence for a M class distribution, p i (x), each with a prior p i , and is defined as:</p><formula xml:id="formula_10">JS P ¬Ωp 1 ; p 2 ; . . . ; p M H X M i¬º1 p i p i √∞x√û " # √Ä X M i¬º1 p i H¬Ωp i √∞x√û<label>√∞9√û</label></formula><p>where H[p(x)] is Shannon's entropy:</p><p>H¬Ωp√∞x√û ¬º √Ä X</p><p>x p√∞x√û log p√∞x√û √∞ 10√û Fig. <ref type="figure">7</ref>. A schematic overview of the vocabulary building module and the associated data flow pipeline.</p><p>Fig. <ref type="figure">8</ref>. Spatial pyramid of level 2.</p><p>Merging: select the pair of video-words {w a , w b } for which the distance d ab is minimum and merge them. Hence, we merge the video-words which result in the minimum MI loss by optimizing the global criterion in Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><p>AIB is a greedy algorithm in nature and optimizes the merging of only two word clusters at every step (local optimization). Hence, it optimizes the global criteria defined in Eq. <ref type="bibr" target="#b6">(7)</ref>. We use the described vocabulary compression at each level of the pyramid per class, and obtain a final class-specific compact pyramid representation of video-words.</p><p>We use AIB for the vocabulary compression instead of Principal Component Analysis (PCA) based dimensionality reduction, since PCA is a linear model, whereas the relationship among the video words are highly non-linear in nature. Besides, PCA based dimensionality reduction will work on the first level cluster (k-means) of the bag-of-words model to reduce the final bag-of-words histogram dimensionality. Hence, it will not take inter and intra cluster similarities into account. Unlike PCA, the agglomerative information bottleneck (AIB) method presented in the article, is non-linear and it yields a set of compressed clusters from the first level clusters, such that the set of resulting compressed clusters maximally preserves the original information among them. Additionally, AIB based compression explores the mutual information present among video words and apply compression based on this information. Hence, in this case, AIB based compression is analytically more appropriate than PCA.</p><p>To empirically support our selection of AIB based compression, we have conducted experiments on the Weizmann dataset using PCA based dimensionality reduction. The obtained average accuracy is quite low (40% in the range of 30-f70% compression) compared to the recognition rate of AIB (99% in the same range of compression), which documents that AIB is a far better choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Action classification</head><p>After compression of the video-words at each pyramid level we compute a histograms of the video-words, using the extracted local motion features, and concatenate them to a final feature set for SVM learning. We design a class specific v-square kernel-based SVM, SVM a i k; h a i Wa i <ref type="bibr" target="#b77">[78]</ref>, where a i is the ith action class A, k is the SVM kernel and h a i Wa i is the histogram of action class a i , computed using the class-specific video-words W a i . For a test set a Test we detect its action class:</p><formula xml:id="formula_11">i √É a Test ¬º argmax j SVM a j k; h a Test Wa j ; 8a j 2 A<label>√∞11√û</label></formula><p>We conduct experiments using different SVM kernels, and observe that the v-square and intersection kernel are the best perfoming SVM kernels for all the datasets. Hence, we apply the v-square kernel for all our experiments on human action recognition in Section 4. Table <ref type="table" target="#tab_1">1</ref> shows the average recognition accuracy for the Weizmann dataset using a number of different SVM kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Human action datasets</head><p>To test our proposed approach for action recognition we conduct a comprehensive set of experiments using a number of publicly available human action datasets (see Fig. <ref type="figure" target="#fig_1">1</ref>), which are categorized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Single actor benchmark</head><p>To conduct benchmark testing we choose the two most popular human action datasets: KTH <ref type="bibr" target="#b64">[65]</ref> and Weizmann <ref type="bibr" target="#b65">[66]</ref>. Both of these datasets contain single actors and clean backgrounds. The KTH dataset consists of 6 different actions: walking, jogging, running, boxing, clapping and waving. These actions are performed in 4 different but well-controlled environments by 25 different actors, resulting in a total of 600 action instances. The Weizmann dataset contains 90 videos separated into 10 actions performed by 9 persons. The actions are: bend, jumping-jacks, jump, jump-inplace, run, gallop-sideways, skip, walk, one-hand-waving and twohands-waving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Single actor with complex background</head><p>In this category we choose the CVC action dataset <ref type="bibr" target="#b67">[68]</ref> and the CMU action dataset <ref type="bibr" target="#b66">[67]</ref>. The CVC dataset consists of 5 actors performing 7 actions: walking, jogging, running (with horizontal and vertical two-way paths), hand-waving, two-hands-waving, jumpin-place and bending. The dataset is rated ''semi-complex'' and is interesting, since it has a textured background. The CMU dataset is composed of 48 video sequences of five action classes: jumping-jacks, pick-up, push-button, one-hand-waving and two-handswaving. The test data contains 110 videos (events) which are down-scaled to 160 √Ç 120 in resolution. This dataset has been recorded by a hand-held camera with moving people and vehicles in the background, and is known to be very challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Movie and YouTube video clips</head><p>To evaluate our approach in different challenging stettings, we conduct experiments on movie and YouTube video clips. Concretely, we use the Hollywood 2 human actions and scenes dataset <ref type="bibr" target="#b33">[34]</ref> and the YouTube action dataset <ref type="bibr" target="#b31">[32]</ref>. The Hollywood 2 dataset is composed of video clips extracted from 69 Hollywood movies, and contains 12 classes of human actions: AnswerPhone, DriveCar, Eat, FightPerson, GetOutCar, HandShake, HugPerson, Kiss, Run, Sit-Down, SitUp and StandUp. In total, there are 1707 action samples divided into a training set (823 sequences) and a test set (884 sequences), where train and test sequences are obtained from different movies. The dataset intends to provide a comprehensive benchmark for human action recognition in realistic and challenging settings. The YouTube dataset is a collection of 1168 complex and challenging YouTube videos of 11 human actions categories: basketball shooting, volleyball spiking, trampoline jumping, soccer juggling, horseback riding, cycling, diving, swinging, golf swinging, tennis swinging and walking (with a dog). The dataset has the following properties: a mix of steady cameras and shaky cameras, cluttered background, low resolution, and variation in object scale, viewpoint and illumination. The first four actions are easily confused with jumping, the next two may have similar camera motion, and all the swing actions share some common motions. Some actions are also performed with objects such as a horse, bike or dog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Multiple actors with complex background</head><p>We use two multiple actor datasets: the Microsoft research action dataset I (MSR I) <ref type="bibr" target="#b62">[63]</ref> and the Multi-KTH dataset <ref type="bibr" target="#b40">[41]</ref>. MSR I consists of 16 video sequences and a total of 63 actions: 14 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Automatic action annotation for Multi-KTH</head><p>When multiple actors appear simultaneously in a scene, it is necessary to group the detected STIPs into actor-specific clusters. An excellent example is the Multi-KTH dataset, where five actors are present in the scene. Based on this dataset we introduce a spatio-temporal clustering technique for actor-specific STIP grouping and evaluate its performance in Section 4.8. This spatio-temporal clustering is only a part of Multi-KTH dataset for automatic annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Actor-specific STIP clustering</head><p>The actions present in the Multi-KTH dataset can be divided into two main groups: the actions with moving actors, like walking and jogging, and the actions with static actors, like boxing, waving and clapping. These two different nature of actions can be analyzed in the 2D spatio-temporal XT-space (see Fig. <ref type="figure" target="#fig_8">10b</ref>). The actor-specific STIP clustering exploits the 2D spatio-temporal XT-space and consist of two main steps:</p><p>(i) detection of lines in the XT-space and cluster STIPs accordingly, (ii) after the first set of STIP clusters have been estimated, the associated STIPs are excluded and the resulting subset is clustered using morphological operations and a spatio-temporal distance measurement.</p><p>The surround suppression effect of our STIP detector, resulting in a low detection rate of unwanted background STIPs, facilitates STIP clustering in the XT-space. This will simply not be possible with a high number of background STIPs. Fig. <ref type="figure">9</ref> illustrates the concept of the spatio-temporal clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">The spatio-temporal XT-space</head><p>A plot of the detected STIPs in 3D spatio-temporal XYT-space for the Multi-KTH sequence is shown in Fig. <ref type="figure" target="#fig_8">10a</ref>. As can be seen, actorspecific clustering of the STIPs is non-trivial due to camera motion and occlusions. Hence, successful clustering cannot be accom-plished by commonly used methods, e.g., k-means or Mean Shift clustering. Instead, we project the 3D spatio-temporal STIPs onto a 2D spatio-temporal XT-space, as shown in Fig. <ref type="figure" target="#fig_8">10b</ref>, which reveals some interesting and useful patterns. The XT-space can be seen as the top-down view of the 3D spatio-temporal XYT-space (Fig. <ref type="figure" target="#fig_8">10a</ref>), with the horizontal and vertical axes representing the X-position and the time T, respectively. Hence, the T-axis demonstrates the evolution of STIPs in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Detection of lines in XT-space</head><p>Actions like walking, jogging or running create lines in the XTspace. Hence, we detect line segments in XT-space to cluster STIPs detected for the actors. This is valid, since actors with a certain target destination move in a linear pattern for those actions. Hough transform <ref type="bibr" target="#b78">[79]</ref> is applied for the detection of these linear patterns (i.e., line segments) and the candidates with high response in the Hough Space are kept. Furthermore, a post candidate approval is applied based on the slope of the lines. Fig. <ref type="figure" target="#fig_8">10</ref> shows this process and the intermediate results. As can been seen, the erroneously detected (magenta colored) line can be discarded according to its steep slope. Furthermore, Line segments for the crossing actors are detected but due to a high amount of camera motion, it is not possible to detect good candidates for the other actors performing upper body acations, like boxing, clapping and waving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">STIP clustering in XT-space</head><p>We use the detected lines to cluster the STIPs by applying a point-line distance measure d(x, t), and threshold according to a maximum distance d max for each line segment:</p><formula xml:id="formula_12">d√∞x; t√û ¬º j√∞p √Ä q 1 √û √Ç √∞p √Ä q 2 √ûj jq 2 √Ä q 1 j &lt; d max<label>√∞12√û</label></formula><p>where p is the current STIP under evaluation, and q 1 and q 2 are two points lying on a detected line. The maximum distance d max is set according to the size of the actors appearing in the dataset. After clustering the first set of STIPs, we exclude them and use the remaining STIPs for further clustering. We merge the new subset of STIPs by morphological operations (see Fig. <ref type="figure" target="#fig_8">10f</ref>) and use the resulting blobs to cluster the STIPs, by considering the spatio-temporal distance between a STIP and the contours. Fig. <ref type="figure" target="#fig_9">11</ref> shows the resulting actor-specific STIP clustering in the XT-space, and in Fig. <ref type="figure" target="#fig_11">12</ref> the grouped STIPs are superimposed on a number of frames from the Multi-KTH dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of STIP detector</head><p>We evaluate our STIP detector by estimating a score for the number of detected STIPs for the actors in comparison to those detected in the background. Cao et al. <ref type="bibr" target="#b22">[23]</ref> have recently reported that of all the STIPs detected by Laptev's STIP detector <ref type="bibr" target="#b44">[45]</ref>, only 9. A schematic overview of the spatio-temporal clustering module and the associated data flow pipeline. <ref type="bibr" target="#b17">18</ref>.73% correspond to the three actions performed by the actors in the MSR I, while the rest of the STIPs (81.27%) belong to the background. Ground truth bounding boxes are used to determine if a STIP belongs to an action instance. We evaluate our STIP detector on MSR I in a similar way, and detect 76.21% STIPs for the actors. We observe that our detector tends to detect more points in the background, when applied to the sequences of MSR I with several moving people in the background. Our STIP detector is designed to detect interest point for people, hence it will also consider moving people in the background as candidates. We also conduct this experiment for the Multi-KTH dataset by manually annotating ground truth bounding boxes, and find that 89.35% STIPs belong to the actors (see Fig. <ref type="figure" target="#fig_4">4</ref>). This is consistent with the concept of    our STIP detector, and documents the effectiveness of our incorporated surround suppression followed up by imposing local and temporal constraints. Table <ref type="table" target="#tab_2">2</ref> shows STIP detection ratios of the state-of-the-art methods, and clearly documents the superior performance of our STIP detector.</p><p>The time complexity of our STIP computation highly depends on the size of the input video. For a video of size (160 √Ç 120 √Ç 550), the STIP computation, executed on a standard dual core Desktop PC (Intel(R) Core(TM)2 CPU 6400@2.13 GHz 6 GB RAM) using MAT-LAB R2010, takes approximately 10 min.</p><p>Fig. <ref type="figure" target="#fig_12">13</ref> shows the perfomance of the STIP detector in complex scenarios. Despite of the camera movement, the STIP detector performs well (Fig. <ref type="figure" target="#fig_12">13a</ref> and<ref type="figure">b</ref>). However, in some cases, due to the combination of complex backgorund, low resolution and large background motion, the STIP detector loses focus and detects a larger number of background STIPs (Fig. <ref type="figure" target="#fig_12">13c</ref>) or an insufficient number of actor STIPs (Fig. <ref type="figure" target="#fig_12">13d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Vocabulary building</head><p>The purpose of this experiment is to reveal the optimal initial vocabulary size and compression rate for our vocabulary building strategy. We divide each dataset into 50% training, 20% validation and 30% testing partitions. The final training of the SVMs uses both the training and validation sets. The recognition rates are computed by averaging over 50 random instances of these sets. We conduct experiments using a similar vocabulary size range as Liu et al. <ref type="bibr" target="#b31">[32]</ref>, with an initial vocabulary size of 50 video-words and incrementing it up to 400. We weight the initial vocabulary size according to the pyramid level using a weight factor 2 √Äs , where s is the pyramid level. The vocabulary size is weighted to avoid the empty/singleton cluster creation in finer levels of the pyramid. We reduce the dimensionality of the final feature vectors for the SVM classifiers by applying vocabulary compression at each pyra-mid level. To choose the optimal vocabulary size and compression rate, we vary the initial vocabulary size range  with an increment of 20, and for each of these vocabularies we vary the compression rate from 0% to 95% with an increment of 5%. Fig. <ref type="figure" target="#fig_13">14a</ref> shows the resulting 3D plot of the recognition rate as a function of the initial vocabulary size and the compression rate, for the KTH dataset. The maximum recognition rate indicates the optimal vocabulary size and compression rate. We observe that the best result is obtained at a compression rate upto 65%, and the performance starts to degrade rapidly above 80%. In Fig. <ref type="figure" target="#fig_13">14b</ref> the recognition rate, as a function of the initial vocabulary size for the three other single actor datasets: CMU, CVC and Weizmann, is shown. We obtain approximately 100% recognition rate in the initial vocabulary size range [230-300] for the Weizmann, CMU and CVC datasets, which is similar to the the middle peak in Fig. <ref type="figure" target="#fig_13">14a</ref> for KTH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Benchmark testing</head><p>We use the KTH and Weizmann datasets for benchmark testing, and achieve an accuracy of 96.35% for KTH and 99.50% for Weizmann. Table <ref type="table" target="#tab_3">3</ref> shows a comparison of the recognition rates of our approach and several other state-of-the-art methods for these two datasets. It should be noted that we achieve state-of-the-art recognition rate for KTH. We obtained this recognition with an initial vocabulary size of 350 and a 60% compression rate. The main reasons for this improvement are the selective STIP detection and the spatial pyramids, which capture the local characteristics of actions, and thereby reduce interclass confusion. The accuracy for Weizmann is approximately 100%, which is comparable to the state-of-the-art. Lin et al. <ref type="bibr" target="#b15">[16]</ref> report a clear 100% recognition rate for Weizmann. However, this work applies a template matching technique, using holistic features extracted from global boundary box-based interest regions. Furthermore, it requires background subtraction and target tracking. In contrast, our approach uses local features and does not require any preprocessing. Since, Weizmann is a simple datasets without any further challenges, it favors global and holistic methods. In contrast, our approach is applicable for all types of scenes, including very challenging scenes of high complexity, which we will validate in the following.</p><p>We analyze the error-frames of the 0.50% videos of the Weizmann dataset, which are miss-classified. Similarly, we analyze the miss-classified frames from the confusion matrix for KTH. Fig. <ref type="figure" target="#fig_14">15</ref> shows some example error-frames. Due to low resolution only a limited number of STIPs are detected for the important body  parts (arms and legs), which are taking major part in actions like boxing and running. In these few cases this results in missclassification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Evaluation on complex scene</head><p>The main objective of this evaluation is to test the capability of our method to handle background clutter. For this purpose we choose the CMU action dataset and the CVC Action dataset with textured background. Despite the presence of strong background texture and clutter, we achieve a 100.0% accuracy rate for CVC and 99.42% for CMU (see Table <ref type="table">5</ref>). The high performance for both of these dataset is consistence with the theoretical foundation of our proposed STIP detector. The detector's selective behavior, achieved by incorporating surround suppression and imposing local and temporal constraints, results in robustness to background texture and clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Action recognition in movie and YouTube video clips</head><p>Next, we conduct experiments on movie and YouTube video clips, using the YouTube and Hollywood 2 action datasets. We achieve 99.13% recognition rate for the YouTube actions. Table <ref type="table" target="#tab_3">3</ref> shows the comparison with other state-of-the-art method for this dataset. Our approach is far superior compared to the other reported methods, due to our STIP detector's capability to handle complex and challenging scenes with camera motion, cluttered background, and variation in scale, viewpoint and illumination.</p><p>For the Hollywood 2 dataset, the performance is evaluated as suggested in <ref type="bibr" target="#b33">[34]</ref>, i.e., by computing the average precision (AP) for each of the action classes and reporting the mean AP over all classes (MAP). Table <ref type="table">4</ref> shows the AP for the actions in comparison to other state-of-the-art methods. The Hollywood 2 dataset contains very complex scenes from movies with no ground truth information available, and moreover the different instances of an action are sometimes viewed from different camera angles.</p><p>Notes: ''Answerphone and Handshake are quite small, and therefore need a very complex set of compound features in order to classify the action over the background noise. In contrast, FightPerson and DriveCar use more global contextual features and therefore they work with lower level features.''</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Cross-data experiments</head><p>We perform exhaustive cross-data evaluation to test our proposed method in more realistic scenarios and use the KTH and Weizmann datasets for training data. We observe that the Weizmann dataset is not appropriate for training, and results in a poor 40% and 45% recognition rate for CVC and CMU, respectively. This is due to inadequate training data since Weizmann contains a very limited number of action instances per category compared to KTH. Table <ref type="table">5</ref> shows the accuracy rates obtained using KTH as training. These cross-data results validate that our approach is applicable for more practical scenarios, where training and test data are coming from different sources.   <ref type="bibr" target="#b16">[17]</ref> 96.00 --Yu et al. <ref type="bibr" target="#b43">[44]</ref> 95.67 --Kim et al. <ref type="bibr" target="#b14">[15]</ref> 95.33 --Wu et al. <ref type="bibr" target="#b19">[20]</ref> 95.10 98.90 -Cao et al. <ref type="bibr" target="#b22">[23]</ref> 95.02 --Ka√¢niche and Br√©mond <ref type="bibr" target="#b27">[28]</ref> 94.67 --Kovashka and Grauman <ref type="bibr" target="#b28">[29]</ref> 94.53 --Gilbert et al. <ref type="bibr" target="#b25">[26]</ref> 94.50 --Sadek et al. <ref type="bibr" target="#b36">[37]</ref> 94.30 --Liu and Shah <ref type="bibr" target="#b32">[33]</ref> 94.16 --Sun et al. <ref type="bibr" target="#b39">[40]</ref> 94.00 97.80 -Saghafi et al. <ref type="bibr" target="#b37">[38]</ref> 93.94 --Shao and Gao <ref type="bibr" target="#b38">[39]</ref> 93.89 --Liu et al. <ref type="bibr" target="#b31">[32]</ref> 93.80 -71.20 Uemura et al. <ref type="bibr" target="#b40">[41]</ref> 93.70 --Lin et al. <ref type="bibr" target="#b15">[16]</ref> 93.43 100.00 -Yuan et al. <ref type="bibr" target="#b62">[63]</ref> 93.30 --Liu et al. <ref type="bibr" target="#b30">[31]</ref> 92.30 -76.10 a Yao et al. <ref type="bibr" target="#b21">[22]</ref> 93.00 92.20 -Schindler and van Gool <ref type="bibr" target="#b18">[19]</ref> 92.70 100.00 -Laptev et al. <ref type="bibr" target="#b61">[62]</ref> 91.80 --Jhuang et al. <ref type="bibr" target="#b47">[48]</ref> 91.70 98.80 kl√§ser et al. <ref type="bibr" target="#b55">[56]</ref> 91.40 84.30 -Yang et al. <ref type="bibr" target="#b11">[12]</ref> 87.30 99.40 -Wong and Cipolla <ref type="bibr" target="#b50">[51]</ref> 86.62 --Willems et al. <ref type="bibr" target="#b49">[50]</ref> 84.26 --Niebles et al. <ref type="bibr" target="#b34">[35]</ref> 81.50 --Doll√†r et al. <ref type="bibr" target="#b46">[47]</ref> 81.17 --Sch√ºldt et al. <ref type="bibr" target="#b64">[65]</ref> 71. The KTH dataset has only one common action, two-hands-waving, with the CMU action dataset. We use the KTH running sequence as negative data and obtain a 91.94% recognition rate. It is noticeable, that the accuracy is actually higher for Weizmann (100%) and CMU (99.42%), than when training and testing on the same dataset, due to the sufficient action instances for training. Additionally, for CMU we only recognize one action, two-hands-waving, compared to five actions when both training and testing on CMU. On the contrary, the accuracy decreases by 3% for CVC, due to its lower inter-dataset correlation with KTH. For the Multi-KTH dataset we manually annotate the action labels as ground truth, using bounding boxes, and obtain 98.40% accuracy. We perform another test using our automatic action annotation described in Section 4.2, and obtain a 94.20% recognition rate, which is comparable to the results of the manual annotation. For the MSR I dataset we achieve 84.77% accuracy. The difficult part of MSR I is that some sequences contain moving people in the background depicted by the bounding box of the agent performing the action, which result in unwanted STIP in the background, and thereby a lower recognition rate compared to the other datasets. In conclusion, these results outperform the state-of-the-art significantly (see Table <ref type="table">5</ref>) and hereby validate the robustness of our method in more realistic action recognition scenarios. Although these datasets are very complex and contain several practical challenges: cluttered and moving backgrounds (including people and vehicles), camera motion and multiple actors, our approach performs robustly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have presented a novel approach for human action recognition in complex scenes. Our approach is based on selective STIPs which are detected by suppressing background SIPs and imposing local and temporal constraints, resulting in more robust STIPs for actors and less unwanted background STIPs. We apply a BoV model of local N-jet descriptors extracted at the detected STIPs and introduce a novel vocabulary building strategy by combining a spatial pyramid and vocabulary compression. Action class-specific SVM classifiers are trained to finally identify human actions.</p><p>The strong aspect of our proposed STIP detection method is, it can detect dense STIPs at the motion region without affected by the complex background. This is an important property to detect actions in complex scenarios. Regarding the weak aspect, our method suffers in the presence of other motion (presence of multiple actors) together with the region of action. In this scenario we detect several STIPs from different motion region results in poor classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>The average precision (%) and mean average precision (MAP) for the actions of Hollywood 2, using our apporach in comparison to the state-of-the-art.</p><p>In the current system, we use greedy approach for vocabulary compression. Sometimes, the time complexity is higher with this approach. A non-greedy method for vocabulary compression might be an interesting inclusion for the future work. Our automatic action annotation using STIP clustering works well for the multi-KTH dataset, yet it is not generalized for other multi-actor action datasets. The automatic action annotation for multi-actor datasets is a very difficult and challenging task. We could include more complex shape matching algorithm along with a human model in the XTspace to minimize the overlap in the STIP clusters of the moving and non-moving actors.</p><p>We have reported superior action recognition results in comparison to the state-of-the-art, when testing on benchmark datasets of simple scenes (96.35% accuracy for KTH and 99.50% for Weizmann), and similar performance for complex scenes (CVC and CMU). Additionally, we have shown state-of-the-art performance and proven the applicability of our approach for action recognition in movie and YouTube video clips by significantly outperforming other methods evaluated on the YouTube action dataset, and showing the highest mean average precision for the Hollywood 2 dataset. A comprehensive cross-data evaluation has been performed by separating the training (KTH) and test datasets (CVC, CMU, MSR I and Multi-KTH). To our best knowledge we are the first to report exhaustive cross-data evaluation. Compared to state-of-the-art we have reported superior results by raising the recognition rates from approximately 60-75% to 85-100%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Multi-KTH (c) MSRI (b) Hollywood 2 (a) YouTube</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example images with superimposed STIPs from the eight action datasets applied for evaluation of our approach: KTH, Weizmann, CVC, CMU, YouTube, Hollywood 2, MSR I and Multi-KTH. The examples give an indication of the described challenges and differences in the datasets: simple scenes (KTH and Weizmann), semi-complex (CVC), and scenes of high complexity (CMU, YouTube, Hollywood 2, MSR I and Multi-KTH).</figDesc><graphic coords="3,63.86,530.57,459.40,176.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A schematic overview of the spatio-temporal interest point detection module and the associated data flow pipeline.</figDesc><graphic coords="4,110.54,618.47,385.62,120.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 .</head><label>1</label><figDesc>STIP detection from an image stack. Require: An image stack (H √Ç W √Ç N): iS; (contains all the video frames) Array containing spatial scales: sA; Alpha: a; Mask: m; Ensure: Detected STIPs: stip 1: sip = {}; stip = {}; 2: N = size(iS,3); (Total no. of frames) 3: for i = 1 ? N do 4: for j = 1 ? size(sA) do 5: sip sip [ {SCD(iS(:, :, i), sA(j), a, m), sA(j)}; 6: end for 7: stip stip [ blobDetector(iS(:, :, i), sip); 8: end for 9: stip = temporalConstraint(iS, stip); 10: Return(stip);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. STIP detection results for the Multi-KTH dataset. (a) Laptev and Lindeberg [45], (b) Doll√†r et al. [47], (c) Willems et al. [50] and (d) our approach. Due to background clutter and camera motion (a), (b) and (c) detect quite a large number of STIPs in the background compared to our approach.</figDesc><graphic coords="5,72.40,67.92,437.05,91.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Responses at position (x 0 , y 0 ) and (x 00 , y 00 ) along the line passing through (x, y)<ref type="bibr" target="#b70">[71]</ref>. Non-maxima suppression retains the value in the central position (x, y), if it is greater than the values at (x 0 , y 0 ) and (x 00 , y 00 ).</figDesc><graphic coords="5,310.05,206.42,234.45,97.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Performance of our SIP detector with a = 1.5. Detected SIPs (a) before suppression and (b) after suppression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>14:end if 15: end for 16: Return(sip); Algorithm 3. blobDetector: Corner strength detection using Gaussian blob. Require: An image (H √Ç W): im; Corner points: corners; Ensure: Detected selective spatial interest points based on Gaussian blob strength: cornerPoints 1: cornerPoints = {}; 2: for Each point (X, Y, r) 2 corners do 3: bS = r 1.75 √É L y,im (X, Y) √É L xx,im (X, Y); 4: if (bS &gt; s) then 5: cornerPoints cornerPoints [ (X, Y, r); 6: end if 7: end for 8: Return(cornerPoints); Algorithm 4. temporalConstraint: Imposed temporal constraint on the selected spatial corner points Require: An image stack (H √Ç W √Ç N): iS; Spatial corner points: cp; Ensure: Detected STIPs: stip 1: for i = 1 ? H do 2: for j = 1 ? W do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Plots of the detected STIPs for the Multi-KTH dataset, and detection of linear patterns in the XT-space. (a) k-Means clustered STIPs in the 3D spatio-temporal XYTspace and (b) ungrouped STIPs in the 2D spatio-temporal XT-space; (c) line segments in XT-space caused by actions like walking, jogging or running; (d) candidates with high responses in the Hough space; (e) detected line segment using the Hough transfrom and (f) blobs obtained by morphological operations.</figDesc><graphic coords="10,56.69,67.92,491.44,256.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Actor-specific STIP clustering in the XT-space.</figDesc><graphic coords="10,79.54,388.06,177.14,107.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Automatic annotation of STIPs detected for multiple simultaneously actors for a number of frames from the Multi-KTH dataset.</figDesc><graphic coords="10,79.34,540.89,445.38,183.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Performance of the STIP detector in sequences with complex scenarios. Successful STIP detection is shown for frames of the (a) YouTube and (b) Hollywood 2 dataset, respectively. Additionally, the failure frames of (c) YouTube and (d) Hollywood 2 are also shown. In (a) and (b) our STIP detector successfully handles camera motion and the STIPs are detected only in the motion of interest. On the contrary, in the frames of (c) and (d), due to high background motion and difference in scene resolution the STIP detector loses the focus on the motion of the human actors.</figDesc><graphic coords="11,72.40,555.82,441.34,157.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Revealing the influence of the vocabulary size and compression on the average action recognition rates. (a) A 3D Plot of the recognition rate, as a function of the initial vocabulary size and the compression rate, for the KTH dataset. (b) Recognition rates, as a function of the initial vocabulary size, for the three single actor datasets: CMU, CVC and Weizmann. The compression rate is fixed to 65%, i.e., 35% of the initial vocabulary size is used.</figDesc><graphic coords="12,73.70,67.92,457.91,160.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Error-frames of the videos that are miss-classified for the KTH and Weizmann datasets. The first three frames depict miss-classified boxing, running and waving actions from the KTH dataset, respectively. The last two error-frames are skip and walking actions from the Weizmann dataset. These frames show cases which result in missclassification. Due to low resolution only a limited number of STIPs are detected for the important body parts (arms and legs), which are taking major part in these actions.</figDesc><graphic coords="13,61.04,67.91,453.63,63.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,78.07,67.92,425.54,132.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,95.07,559.33,396.74,179.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,86.54,610.01,412.28,128.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>y 1 , H, W); 11: a 2 = cropRect(im 2 , x 2 , y 2 , H, W); 12: sC = crossCorrelation(a 1 , a 2 ); 13:if (sC &gt; s corr ) ^(g 1 (x 1 , y 1 ) &gt; s gabor ) then</figDesc><table><row><cell>14:</cell><cell>mP</cell><cell>mP [ (x 1 , y 1 , r 1 );</cell></row><row><cell>15:</cell><cell>end if</cell><cell></cell></row><row><cell>16:</cell><cell>end if</cell><cell></cell></row><row><cell>17:</cell><cell>end for</cell><cell></cell></row><row><cell cols="2">18: end for</cell><cell></cell></row><row><cell cols="2">19: Return(mS);</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Average recognition accuracy for the Weizmann dataset using different SVM kernels. We have used a Polynomial kernel of degree 3. 24 hand-waving and 25 boxing, performed by 10 subjects. The sequences contain multiple types of action recorded in indoor and outdoor scenes with cluttered and moving backgrounds. Some sequences contain multiple actions performed by different people. Each video is of low resolution 320 √Ç 240 with a frame rate of 15 frames per second, and their lengths are between 32 to 76 seconds. The Multi-KTH dataset is a more challenging version of the KTH dataset. It contains 5 (except running) of the 6 KTHactions, which have been recorded by a hand-held camera, with multiple simultaneous actors, a significant amount of camera motion, scale changes and a more realistic cluttered background.</figDesc><table><row><cell>SVM kernel</cell><cell>Recognition rate (%)</cell></row><row><cell>v-square</cell><cell>99.50</cell></row><row><cell>Intersection</cell><cell>97.78</cell></row><row><cell>Radial basis function</cell><cell>87.77</cell></row><row><cell>Polynomial</cell><cell>78.67</cell></row><row><cell>Linear</cell><cell>58.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>STIP detection ratios (%): the number of STIPs detected on the actors with respect to the total number of detected STIPs, estimated for the MSR I and Multi-KTH datasets using our approach and state-of-the-art methods.</figDesc><table><row><cell>Method</cell><cell>MSR I</cell><cell>Multi-KTH</cell></row><row><cell>Our approach</cell><cell>76.21</cell><cell>90.34</cell></row><row><cell>Laptev and Lindeberg [45]</cell><cell>18.73</cell><cell>48.16</cell></row><row><cell>Doll√†r et al. [47]</cell><cell>21.36</cell><cell>16.03</cell></row><row><cell>Willems et al. [50]</cell><cell>24.02</cell><cell>20.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>State-of-the-art recognition accuracies (%) for the KTH, Weizmann and YouTube datasets.</figDesc><table><row><cell>Method</cell><cell>KTH</cell><cell>Weiz.</cell><cell>YouTube</cell></row><row><cell>Our approach</cell><cell>96.35</cell><cell>99.50</cell><cell>86.98</cell></row><row><cell>Lui et al.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>B. Chakraborty et al. / Computer Vision and Image Understanding 116 (2012) 396-410</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the Spanish Research Programs Consolider-Ingenio 2010:MIPRCV (CSD200700018); Avanza I+D ViCoMo (TSI-020400-2009-133); the Spanish Project TIN2009-14501-C02-02; and the Danish National Research Councils -FTP under the research project ''Big Brother is watching you!''.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action</head><p>Marszalek et al. <ref type="bibr" target="#b33">[34]</ref> Han et al. <ref type="bibr" target="#b26">[27]</ref> Wang et al. <ref type="bibr" target="#b42">[43]</ref> Gilbert et al. <ref type="bibr">[</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of advances in vision-based human motion capture and analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kr√ºger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="90" to="126" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey of vision-based methods for action representation, segmentation and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<idno>RR-7212</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="54" to="111" />
		</imprint>
	</monogr>
	<note type="report_type">INRIA Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Chaotic invariants for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning and detecting activities from movement trajectories using the hierarchical hidden markov model</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning variable-length markov models of behavior</title>
		<author>
			<persName><forename type="first">A</forename><surname>Galata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="398" to="413" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Detecting irregularities in images and in video</title>
		<author>
			<persName><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Action mach: a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Searching video for complex activities with finite state models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ikizler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical analysis of dynamic actions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1530" to="1535" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pose primitive based human action recognition in videos or still images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Discriminative topics modelling for action feature selection and recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bregonzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tree-based approach to integrated action localization, recognition and segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Tensor canonical correlation analysis for action classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recognizing actions by shape-motion prototype trees</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Action classification on product manifolds</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploiting simple hierarchies for unsupervised human behavior analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Action snippets: How many frames does human action recognition require</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Incremental discriminative-analysis of canonical correlations for action recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Human action detection by boosting efficient motion features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A hough transform-based voting framework for action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cross-dataset action detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action recognition using mined hierarchical compound features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Illingworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fast realistic multi-action recognition using mined dense spatio-temporal features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Illingworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selection and context for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gesture recognition by learning local motion signatures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Ka√¢niche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Br√©mond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning a hierarchy of discriminative space-time neighborhood features for human action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning semantic visual vocabularies using diffusion distance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos &apos;&apos;in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="&lt;http://www.cs.ucf.edu/$liujg/YouTube_Action_dataset.html&gt;" />
	</analytic>
	<monogr>
		<title level="m">CVPR, 2009, the YouTube dataset is</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning human actions via information maximization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="//www.irisa.fr/vista/actions/hollywood2&gt;" />
		<imprint/>
	</monogr>
	<note>Actions in context, in: CVPR, 2009, the Hollywood 2 dataset is available at &lt;http</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName><forename type="first">J</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="318" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Temporal causality for the analysis of visual events</title>
		<author>
			<persName><forename type="first">K</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Toward robust action retrieval in video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sadek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Hamadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sayed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Embedding visual words into concept space for action and scene recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saghafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Farahzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sluzek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A wavelet based local descriptor for human action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Action recognition via local descriptors and holistic features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Feature tracking and motion compensation for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Uemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<ptr target="&lt;http://www.openvisor.org/video_details.asp?idvideo=303&gt;" />
	</analytic>
	<monogr>
		<title level="m">BMVC, 2008, the Multi-KTH dataset is</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improving bag-of-features action recognition with non-local cues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Evaluation of local spatiotemporal features for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kl√§ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Real-time action recognition by spatiotemporal semantic and structural forests</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<title level="m">Space-time interest points</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
		<title level="m">Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doll√†r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>VS-PETS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A biologically inspired system for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spatiotemporal salient points for visual recognition of human actions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oikonomopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SMC-B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="710" to="719" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Extracting spatiotemporal interest points using global information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Rotationally invariant image operators</title>
		<author>
			<persName><forename type="first">P</forename><surname>Beaudet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>ICPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feature detection with automatic scale selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Evaluation of interest point detectors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Better matching with fewer features: The selection of useful features in large database recognition problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Turcot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV Workshops</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A spatio-temporal descriptor based on 3dgradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kl√§ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Representation of local geometry in the visual system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="367" to="375" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Local descriptors for spatio-temporal recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Workshop on Spatial Coherence for Visual Motion Analysis</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Surf: speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Local velocity-adapted motion events for spatio-temporal recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sch√ºldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="229" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Discriminative subvolume search for efficient action detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="www.research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc&gt;" />
	</analytic>
	<monogr>
		<title level="m">CVPR, 2009, the MSR dataset is</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Representing pairwise spatial and temporal relations for action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Matikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="508" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Recognizing human actions: A local svm approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sch√ºldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<ptr target="&lt;http://www.nada.kth.se/cvap/actions&gt;" />
		<imprint/>
	</monogr>
	<note>in: ICPR, 2004, the KTH dataset is available at</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<ptr target="&lt;http://www.wisdom.weizmann.ac.il/$vision/SpaceTimeActions.html&gt;" />
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2247" to="2253" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>the Weizmann dataset is available at</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Event detection in crowded videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<ptr target="&lt;http://www.cs.cmu.edu/$yke/video/#Dataset&gt;" />
	</analytic>
	<monogr>
		<title level="m">ICCV, 2007, the CMU dataset instructions are</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<ptr target="&lt;http://www.iselab.cvc.uab.es/files/Tools/Cvc-ActionDataSet/index.htm&gt;" />
		<title level="m">The CVC dataset is available at</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Soria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Magdalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serrano</surname></persName>
		</author>
		<title level="m">Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Transfer learning for text classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="299" to="306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Contour and boundary detection improved by surround suppression of texture edges</title>
		<author>
			<persName><forename type="first">C</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Westenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="609" to="622" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Discriminative spatial attention for robust tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="480" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Spatiotemporal saliency in dynamic scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="171" to="177" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="989" to="1005" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Feature tracking with automatic selection of spatial scales</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bretzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="392" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<title level="m">Agglomerative information bottleneck, in: NIPS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="&lt;http://www.csie.ntu.edu.tw/$cjlin/libsvm&gt;" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Use of the hough transformation to detect lines and curves in pictures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
