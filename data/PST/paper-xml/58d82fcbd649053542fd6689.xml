<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Self-Paced Learning for Cost-Effective and Progressive Face Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</orgName>
								<address>
									<postCode>2015</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keze</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</orgName>
								<address>
									<postCode>2015</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</orgName>
								<address>
									<postCode>2015</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</orgName>
								<address>
									<postCode>2015</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</orgName>
								<address>
									<postCode>2015</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Active Self-Paced Learning for Cost-Effective and Progressive Face Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4278595E3BCDD025929C2CCCA334BEB4</idno>
					<idno type="DOI">10.1109/TPAMI.2017.2652459</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2017.2652459, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cost-effective model</term>
					<term>Active learning</term>
					<term>Self-paced learning</term>
					<term>Incremental processing</term>
					<term>Face identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper aims to develop a novel cost-effective framework for face identification, which progressively maintains a batch of classifiers with the increasing face images of different individuals. By naturally combining two recently rising techniques: active learning (AL) and self-paced learning (SPL), our framework is capable of automatically annotating new instances and incorporating them into training under weak expert recertification. We first initialize the classifier using a few annotated samples for each individual, and extract image features using the convolutional neural nets. Then, a number of candidates are selected from the unannotated samples for classifier updating, in which we apply the current classifiers ranking the samples by the prediction confidence. In particular, our approach utilizes the high-confidence and low-confidence samples in the self-paced and the active user-query way, respectively. The neural nets are later fine-tuned based on the updated classifiers. Such heuristic implementation is formulated as solving a concise active SPL optimization problem, which also advances the SPL development by supplementing a rational dynamic curriculum constraint. The new model finely accords with the "instructor-student-collaborative" learning mode in human education. The advantages of this proposed framework are two-folds: i) The required number of annotated samples is significantly decreased while the comparable performance is guaranteed. A dramatic reduction of user effort is also achieved over other state-of-the-art active learning techniques. ii) The mixture of SPL and AL effectively improves not only the classifier accuracy compared to existing AL/SPL methods but also the robustness against noisy data. We evaluate our framework on two challenging datasets, which include hundreds of persons under diverse conditions, and demonstrate very promising results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the growth of mobile phones, cameras and social networks, a large amount of photographs is rapidly created, especially those containing person faces. To interact with these photos, there have been increasing demands of developing intelligent systems (e.g., content-based personal photo search and sharing from either his/her mobile albums or social network) with face recognition techniques <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Thanks to several recently proposed pose/expression normalization and alignment-free approaches <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, identifying face in the wild has achieved remarkable progress. As for the commercial product, the website "Face.com" once provided an API (application interface) to automatically detect and recognize faces in photos. The main problem in such scenarios is to identify individuals from images under a relatively unconstrained environment. Traditional methods usually handle this problem by supervised learning <ref type="bibr" target="#b13">[14]</ref>, while it is typically expensive and time-consuming to prepare a good set of labeled samples. Since only a few data are labeled, Semi-supervised learning <ref type="bibr" target="#b14">[15]</ref> may be a good candidate to solve this problem. But it has been pointed out by <ref type="bibr" target="#b15">[16]</ref>: Due to large amounts of noisy samples and outliers, directly using the unlabeled data may significantly reduce learning performance.</p><p>This paper targets on the challenge of incrementally learning a batch of face recognizers with the increasing face images of different individuals. Here we assume Contact: linliang@ieee.org; deyum@andrew.cmu.edu.</p><p>that the person faces can be basically detected and localized by existing face detectors. However, to build such a system is quite challenging in the following aspects.</p><p>• Person faces have large appearance variations (see the examples in Fig. <ref type="figure" target="#fig_0">1</ref> (a)) caused by diverse views and expressions as well as facial accessories (e.g., glasses and hats) and aging. The different lighting condition is also required to be considered in practice.</p><p>• It is possible that only a few labeled samples are accessible at the beginning, and the changes of personal faces are rather unpredictable over time, especially under the current scenarios that there are large amount of images swarmed into Internet every day. • Satisfaction of user experience is one of the critical concerns. Even though a few user interventions (e.g., labeling new samples) could be allowed, the user effort is desired to be kept minimizing over time.</p><p>Conventional incremental face recognition methods such as incremental subspace approaches <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> often fail on complex and large-scale environments. Their performances could be dropped drastically when the initial training set of face images is either insufficient or inappropriate. In addition, most of existing incremental approaches suffer from noisy samples or outliers in the model updating. In this work, we propose a novel active self-paced learning framework (ASPL) to handle the above difficulties, which absorbs powers of two recently rising techniques: active learning (AL) <ref type="bibr" target="#b18">[19]</ref> and selfpaced learning (SPL) <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In particular, our framework tends to conduct a "Cost-less-Earn-more" working manner: as much as possible pursuing a high performance while reducing costs.</p><p>The basic approach of the AL methods is to progressively select and annotate most informative unlabeled samples to boost the model, in which user interaction is allowed. The sample selection criteria is the key in AL, and it is typically defined according to the classification uncertainty of samples. Specifically, the samples of low classification confidence, together with other informative criteria like diversity, are generally treated as good candidates for model retraining. On the other hand, SPL is a recently proposed learning regime to mimic the learning process of humans/animals that gradually incorporates easy to more complex samples into training <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, where an easy sample is actual the one of high classification confidence by the currently trained model. Interestingly, the two categories of learning methods select samples with the opposite criteria. This finding inspires us to investigate the connection between the two learning regimes and the possibility of making them complementary to each other. Moreover, as pointed out in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>, learning based features are considered to be able to exploit information with better discriminative ability for face recognition, compared to the hand-crafted features. We thus utilize the deep convolutional neural network (CNN) <ref type="bibr" target="#b25">[26]</ref> for feature extraction instead of using handcraft image features.. In sum, we aim at designing a cost-effective and progressive learning framework, which is capable of automatically annotating new instances and incorporating them into training under weak expert recertification. In the following, we discuss the advantage of our ASPL framework in two aspects: "Cost-less" and "Earn-more".</p><p>(I) Cost less: Our framework is capable of building effective classifiers with less labeled training instances and less user efforts, compared with other state-of-theart algorithms. This property is achieved by combining the active learning and self-paced learning in the incremental learning process. In certain feature space of model training as Fig. <ref type="figure" target="#fig_0">1</ref> (b) illustrates, samples of low classification confidence are scattered and close to the classifier decision boundary while high confidence samples distribute compactly in the intra-class regions. Our approach takes both categories of samples into consideration for classifier updating. The benefit of this strategy includes: i) High-confidence samples can be automatically labeled and consistently added into model training throughout the learning process in a self-paced fashion, particularly when the classifier becomes more and more reliable at later learning iterations. This significantly reduce the burden of user annotations and make the method scalable in large-scale scenarios. ii) The low-confidence samples are selected by allowing active user annotations, making our approach more efficiently pick up informative samples, more adapt to practical variations and converge faster, especially in the early learning stage of training.</p><p>(II) Earn more: The mixture of self-paced learning and active learning effectively improves not only the classifier accuracy but also the classifier robustness against noisy samples. From the perspective of AL, extra highconfidence samples are automatically incorporated into the retraining without cost of human labor in each iteration, and faster convergence can be thus gained. These introduced high-confidence samples also contribute to suppress noisy samples in learning, due to their compactness and consistency in the feature space. From the SPL perspective, allowing active user intervention generates the reliable and diverse samples that can avoid the learning been misled by outliers. In addition, utilizing the CNN facilitates to pursue a higher classification performance by learning the convolutional filters instead of hand-craft feature engineering.</p><p>In brief, our ASPL framework includes two main phases. At the initial stage, we first learn a general face representation using an architecture of convolutional neural nets, and train a batch of classifiers with a very small set of annotated samples of different individuals. In the iteration learning stage, we rank the unlabeled samples according to how they relate to the current classifiers, and retrain the classifiers by selecting and annotating samples in either active user-query or selfpaced manners. We can also make the CNN fine-tuned based on the updated classifiers.</p><p>The key point in designing such an effective interactive learning system is to make an efficient labor division between computers and human participants, i.e., we should possibly feed computable and faithful tasks into computers, and to possibly arrange labor-saving and intelligent tasks to humans <ref type="bibr" target="#b26">[27]</ref>. The proposed ASPL framework provides a rational realization to this task by automatically distinguishing high-confidence samples, which can be easily and faithfully recognized by computers in a self-paced way, and low-confidence ones, which can be discovered by requesting user annotation.</p><p>The main contributions of this work are several folds. i) To the best of our knowledge, our work is the first one to make a face recognition framework capable of automatically annotating high-confidence samples and involve them into training without need of extra human labor in a purely self-paced manner under weak recertification of active learning. Especially in that along the learning process, we can achieve more and more pseudolabeled samples to facilitate learning totally for free. Our framework is thus suitable in practical large-scale scenarios. The proposed framework can be easily extended to other similar visual recognition tasks. ii) We provide a concise optimization problem and theoretically interpret that the proposed ASPL is an rational implementation for solving this problem. iii) This work also advances the SPL development, by setting a dynamic curriculum variation. The new SPL setting better complies with the "instructor-student-collaborative" learning mode in human education than previous models. iv) Extensive experiments on challenging CACD and CASIA-WebFace datasets show that our approach is capable of achieving competitive or even better performance under only small fraction of sample annotations than that under overall labeled data. A dramatic reduction (&gt; 30%) of user interaction is achieved over other state-of-the-art active learning methods.</p><p>The rest of the paper is organized as follows. Section II presents a brief review of related work. Section III overview the pipeline of our framework, followed by a discussion of model formulation and optimization in Section IV. The experimental results, comparisons and component analysis are presented in Section V. Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we first present a review for the incremental face recognition, and then briefly introduce related developments on active learning and self-paced learning.</p><p>Incremental Face Recognition. There are two categories of methods addressing the problem of identifying faces with incremental data, namely incremental subspace and incremental classifier methods. The first category mainly includes the incremental versions of traditional subspace learning approaches such as principal component analysis (PCA) <ref type="bibr" target="#b27">[28]</ref> and linear discriminant analysis (LDA) <ref type="bibr" target="#b17">[18]</ref>. These approaches map facial features into a subspace, and keep the eigen representations (i.e., eigen-faces) updated by incrementally incorporating new samples. And face recognition is commonly accomplished by the nearest neighbor-based feature matching, which is computational expensive when a large number of samples are accumulated over time. On the other hand, the incremental classifier methods target on updating the prediction boundary with the learned model parameters and new samples. Exemplars include the incremental support vector machines (ISVM) <ref type="bibr" target="#b28">[29]</ref> and the online sequential forward neural network <ref type="bibr" target="#b29">[30]</ref>. In addition, several attempts have been made to absorb advantages from both of the two categories of methods. For example, Ozawa et al., <ref type="bibr" target="#b30">[31]</ref> proposed to integrate the Incremental PCA with the resource allocation network in an iterative way. Although these mentioned approaches make remarkable progresses, they suffer from low accuracy compared with those of batch-based state-of-theart face recognizers, and none of these approaches have been successfully validated on large-scale datasets (e.g., more than 500 individuals). And these approaches are basically studied in the context of fully supervised learning, i.e., both initial and incremental data are required to be labeled.</p><p>Active Learning. This branch of works mainly focus on actively selecting and annotating the most informative unlabeled samples, in order to avoid unnecessary and redundant annotation. The key part of active learning is thus the selection strategy, i.e., which samples should be presented to the user for annotation. One of the most common strategies is the certainty-based selection <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, in which the certainties are measured according to the predictions on new unlabeled samples obtained from the initial classifiers. For example, Lewis et al., <ref type="bibr" target="#b31">[32]</ref> proposed to take the most uncertain instance as the one that has the largest entropy on the conditional distribution over its predicted labels. Several SVM-based methods <ref type="bibr" target="#b32">[33]</ref> determine the uncertain samples as they are relatively close to the decision boundary. The sample certainty was also measured by applying a committee of classifiers in <ref type="bibr" target="#b33">[34]</ref>. These certainty-based approaches usually ignore the large set of unlabeled instances, and are thus sensitive to outliers. A number of later methods present the information density measure by exploiting the information of unlabeled data when selecting samples. For example, the informative samples are sequentially selected to minimize the generalization error of the trained classifier on the unlabeled data, based on a statistical approach <ref type="bibr" target="#b34">[35]</ref> or prior information <ref type="bibr" target="#b35">[36]</ref>. In <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, instances are taken to maximize the increase of mutual information between the candidate instances and the remaining ones based on Gaussian Process models. The diversity of the selected instance over the unlabeled data has been also taken into consideration <ref type="bibr" target="#b38">[39]</ref>. Recently, Elhamifar et al., <ref type="bibr" target="#b18">[19]</ref> presented a general framework via convex programming, which considered both the uncertainty and diversity measure for sample selection. However, these mentioned active learning approaches usually emphasize those low-confidence samples (e.g., uncertain or diverse samples) while ignoring the other majority of high-confidence samples. To enhance the discriminative capability, wang <ref type="bibr" target="#b14">[15]</ref> et al. proposed a unified semisupervised learning framework, which incorporates the high confidence coding vectors of unlabeled data into training under the proposed effective iterative algorithm, and demonstrate its effectiveness in dictionary-based classification. Our work inspires by this work, and also employs the high-confidence samples to improve both accuracy and robustness of classifiers.</p><p>Self-paced Learning. Inspired by the cognitive principle of humans/animals, Bengio et al. <ref type="bibr" target="#b22">[23]</ref> initialized the concept of curriculum learning (CL), in which a model is learned by gradually including samples into training from easy to complex. To make it more implementable, Kumar et al. <ref type="bibr" target="#b23">[24]</ref> substantially prompted this learning philosophy by formulating the CL principle as a concise optimization model named self-paced learning (SPL). The SPL model includes a weighted loss term on all samples and a general SPL regularizer imposed on sample weights. By sequentially optimizing the model with gradually increasing pace parameter on the SPL regularizer, more samples can be automatically discovered in a pure self-paced way. Jiang et al. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b21">[22]</ref> provided more comprehensive understanding for the learning insight underlying SPL/CL, and formulated the learning model as a general optimization problem as:</p><formula xml:id="formula_0">min w,v∈[0,1] n n i=1 v i L(w; x i , y i ) + f (v; λ) s.t. v ∈ Ψ (1)</formula><p>where D = {(x i , y i )} n i=1 corresponds to the training dataset, L(w; x i , y i ) denotes the loss function which calculates the cost between the objective label y i and the estimated one, w represents the model parameter inside the decision function,</p><formula xml:id="formula_1">v = [v 1 , v 2 , • • • , v n ]</formula><p>T denote the weight variables reflecting the samples' importance. λ is a parameter for controlling the learning pace, which is also referred as "pace age".</p><p>In the model, f (v; λ) corresponds to a self-paced regularizer. Jiang et al. abstracted three necessary conditions it should be satisfy <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b19">[20]</ref>: <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_2">f (v; λ) is convex with respect to v ∈ [0, 1]; (2)</formula><p>The optimal weight of each sample should be monotonically decreasing with respect to its corresponding loss; and (3) The optimal weight of each sample should be monotonically decreasing with respect to the pace parameter λ.</p><p>In this axiomic definition, Condition 2 indicates that the model inclines to select easy samples (with smaller errors) in favor of complex samples (with larger errors). Condition 3 states that when the model "age" λ gets larger, it embarks on incorporating more, probably complex, samples to train a "mature" model. The convexity in Condition 1 further ensures that the model can find good solutions.</p><p>Ψ is the so called curriculum region that encodes the information of predetermined curriculums. Its axiomic definition contains two conditions <ref type="bibr" target="#b19">[20]</ref>: (1) It should be nonempty and convex; and (2) If x i is ranking before x j in curriculum (more important for the problem), the expectation Ψ v i dv should be larger than Ψ v j dv. Condition 1 ensures the soundness for the calculation of this specific constraint, and Condition 2 indicates that samples to be learned earlier is supposed to have larger expected values. This constraint weakly implies a prior learning sequence of samples, where the expected value for the favored samples should be larger.</p><p>The SPL model (1) finely simulates the learning process of human education. Specifically, it builds an "instructor-student collaborative" paradigm, which on one hand utilizes prior knowledge provided by instructors as a guidance for curriculum designing (encoded by the curriculum constraint), and on the other hand leaves certain freedom to students to ameliorate the actual curriculum according to their learning pace (encoded by the self-paced regularizer). Such a model not only includes all previous SPL/CL methods as its special cases, but also provides a general guild line to extend a rational SPL implementation scheme against certain learning task. Based on this framework, multiple SPL variations have been recently proposed, like SPaR <ref type="bibr" target="#b21">[22]</ref>, SPLD <ref type="bibr" target="#b20">[21]</ref>, SPMF <ref type="bibr" target="#b39">[40]</ref> and SPCL <ref type="bibr" target="#b19">[20]</ref>.</p><p>The SPL related strategies have also been recently attempted in a series of applications, such as specificclass segmentation learning <ref type="bibr" target="#b40">[41]</ref>, visual category discovery <ref type="bibr" target="#b41">[42]</ref>, long-term tracking <ref type="bibr" target="#b42">[43]</ref>, action recognition <ref type="bibr" target="#b20">[21]</ref> and background subtraction <ref type="bibr" target="#b39">[40]</ref>. Especially, the SPaR method, constructed based on the general formulation (1), was applied to the challenging SQ/000Ex task of the TRECVID MED/MER competition, and achieved the leading performance among all competing teams <ref type="bibr" target="#b43">[44]</ref>.</p><p>Complementarity between AL and SPL: It is interesting that the function of SPL is very complementary to that of AL. The SPL methods emphasize easy samples in learning, which correspond to the high-confidence intra-class samples, while AL inclines to pick up the most uncertain and informative samples for the learning task, which are always located in low-confidence area near classification boundaries. SPL is capable of easily attaining large amount of faithful pseudo-labeled samples with less requirement of human labors (by reranking technique <ref type="bibr" target="#b21">[22]</ref>. We will introduce details in Section 4), while tends to underestimate the roles of those most informative ones intrinsically configuring the classification boundaries; on the contrary, AL inclines to get informative samples, while need more human labors to manually annotate these samples with more carefully annotation. We thus expect to effectively mix these two learning schemes to help incremental learning both improve the efficiency with less human labors (i.e., Cost Less) and achieve better accuracy and robustness of the learned classifier against noisy samples (i.e., Earn More). This constructs the basic motivation of our ASPL framework for face identification under large-scale scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FRAMEWORK OVERVIEW</head><p>In this section, we illustrate how our ASPL model works. As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, the main stages in our framework pipeline include: CNN pretraining for face representation, classifier updating, high-confidence sample pseudo-labeling in a self-paced fashion, low-confidence sample annotating by active users, and CNN fine-tuning. CNN pretraining: Before running the ASPL framework, we need to pretrain a CNN for feature extraction based on a pre-given face dataset. These images are extra selected without overlapping to all our experimental data. Since several public available CNN architectures <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> have achieved remarkable success on visual recognition, our framework supports to directly employ these architectures and their pretrained model as initialized parameters. In our all experiments, AlexNet <ref type="bibr" target="#b44">[45]</ref> is utilized. Given the extra selected of annotated samples, we further fine-tune the CNN for learning discriminative feature representation.</p><p>Initialization: At the beginning, we randomly select few images for each individual, extract feature representation for them by pretrained CNN, and manually annotate labels to them as the starting point.</p><p>Classifier updating: In our ASPL framework, we use one-vs-all linear SVM as our classifier updating strategies. In the beginning, only a small part of samples are labeled, and we train an initial a classifier for every individual using these samples. As the framework gets mature, samples manually annotated by the AL and pseudo-labeled by the SPL are growing, we adopt them to retrain the classifiers.</p><p>High-confidence sample pseudo-labeling: We rank the unlabeled samples by their important weights via the current classifiers, e.g., using the classification prediction hinge loss, and then assign pseudo-labels to the topranked samples of high confidences. This step can be automatically implemented by our system.</p><p>Low-confidence sample annotating: Based on certain AL criterion obtained under the current classifiers, rank all unlabeled samples, select those top-ranked ones (most informative and generally with low-confidence) from the unlabeled samples, and then manually annotate these samples by active users.</p><p>CNN fine-tuning: After several steps of the interaction, we make the neural nets fine-tuned by the backward propagation algorithm. All self-labeled samples by the SPL and manually annotated ones by the AL are added into the network, we utilize the softmax loss to optimize the CNN parameters via stochastic gradient decent approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FORMULATION AND OPTIMIZATION</head><p>In this section we will discuss the formulation of our proposed framework, and also provide a theoretical interpretation of its entire pipeline from the perspective of optimization. In specific, we can theoretically justify that the entire pipeline of this framework finely accords with a solving process for an active self-paced learning (ASPL) optimization model. Such a theoretical understanding will help deliver more insightful understanding on the intrinsic mechanism underlying the ASPL system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Active Self-paced Learning</head><p>In the context of face identification, suppose that we have n facial photos which are taken from m subjects. Denote the training samples as</p><formula xml:id="formula_3">D = {x i } n i=1 ⊂ R d</formula><p>, where x i is the d-dimensional feature representation for the ith sample. We have m classifiers for recognizing each sample by the one-vs-all strategy.</p><p>After a period of pace increasing, our model will be ameliorated based on the currently learned knowledge from data. Correspondingly, we denote the label set of x i as y i = {y</p><formula xml:id="formula_4">(j) i ∈ {-1, 1}} m j=1</formula><p>, where y (j) i corresponds to the label of x i for the jth subject. That is, if y (j) i = 1, this means that x i is categorized as a face from the jth subject.</p><p>We should give two necessary remarks on our problem setting. One is that in our investigated face identification problems, almost all data have not been labeled before our system running. Only very small amount of samples are annotated as the initialization. That is, most of {y i } n i=1 are unknown and needed to be completed in the learning process. In our system, a minority of them is manually annotated by the active users and a majority is pseudo-labeled in a self-paced manner. The other remark is that the data {x i } n i=1 might possibly been inputted into the system in an incremental way. This means that the data scale might be consistently growing.</p><p>Thanks to the proposed mechanism of combining SPL and AL, our proposed ASPL model can adaptively handle both manually annotated and pseudo-labeled samples, and still progressively fit the consistently growing unlabeled data in such a incremental manner. The ASPL is formulated as follows: </p><formula xml:id="formula_5">min {w,b,v,yi∈{-1,1} m ,i / ∈Ω λ } m j=1 1 2 w (j) 2 2 + (2) C • L w (j) , b (j) , D, y (j) , v (j) + f v (j) ; λ j s.t. v ∈ Ψ λ , where w ={w (j) } m j=1 ⊂ R d and b = {b (j) } m j=1 ⊂ R represent the</formula><formula xml:id="formula_6">v = {[v (j) 1 , v (j) 2 , • • • , v (j) n ] T } m</formula><p>j=1 denotes the weight variables reflecting the training samples' importance, and λ j is a parameter (i.e. the pace age) for controlling the learning pace of the jth classifier. f v (j) ; λ j is the self-paced regularizer controlling the learning scheme. We denote the index collection of all currently active annotated samples as Ω λ = ∪ m j=1 {Ω λj }, where Ω λj corresponds to the set of the jth subject with the pace age λ j . Here Ω λ is introduced as a constraint on y i . Ψ λ = ∩ n i=1 {Ψ λ i } composes of the curriculum constraint of the model at the m classifiers' pace age λ = {λ j } m j=1 . In particular, we specify two alternative types of the curriculum constraint for each sample x i , as:</p><formula xml:id="formula_7">• Ψ λ i = [0, 1]</formula><p>is for the pseudo-labeled sample, i.e., i /</p><p>∈ Ω λ . Then, its importance weights with respect to all the classifiers {v (j) i } m j=1 need to be learned in the SPL optimization.</p><p>• Ψ λ i = {1} is for the sample annotated by the AL process, i.e., ∃j s.t. i ∈ Ω λj . Thus, its importance weights are deterministically set during the model training, i.e., v</p><formula xml:id="formula_8">(j) i = 1.</formula><p>Please refer to Section 2 for the technical background. Note that different from the previous SPL settings, this curriculum Ψ λ i can be dynamically changed with respect to all the pace ages λ of m classifiers. This conducts the superiority of our model, as we discuss in the end of this section.</p><p>The loss function L w (j) , b (j) , D, y (j) , v (j) on x is defined as:</p><formula xml:id="formula_9">L w (j) , b (j) , D, y (j) , v (j) = n i=1 v (j) i l w (j) , b (j) ; x i , y (j) i = n i=1 v (j) i 1 -y (j) i (w (j)T x i + b (j) ) + s.t. m j=1 |y (j) i + 1| ≤ 2, y (j) i ∈ {-1, 1}, i / ∈ Ω λ ,<label>(3)</label></formula><p>where 1 -y</p><formula xml:id="formula_10">(j) i (w (j)T x i + b (j) ) +</formula><p>is the hinge loss of x i in the jth classifier. The cost term corresponds to the summarized loss of all classifiers, and the constraint term only allows two kinds of feasible solutions: i) for any i, there exists y (j) i</p><p>= 1 while for all other y</p><formula xml:id="formula_11">(k) i = -1 for all k = j; ii) y (j) i = -1 for all j = 1, 2, • • • , m (i.e.,</formula><p>background or an unknown person class). These samples x i will be added into the unknown sample set U . It is easy to see that such constraint complies with real cases where a sample should be categorized into one prespecified subject or not classified into any of the current subjects.</p><p>The alternative search strategy is readily employed to solve this optimization. Specifically, the algorithm is designed by alternatively updating the classifier parameters w, b via one-vs-all SVM, the sample importance weights v via the SPL, the pseudo-label y via reranking. Along with gradually increasing pace parameter λ, the optimization updates: i) the curriculum constraint Ψ λ via AL and ii) the feature representation via CNN finetuning. In the following we introduce the details of these optimization steps, and give their physical interpretations. The correspondence of this algorithm to the practical implementation of the ASPL system will also be discussed in the end.</p><p>Initialization: As introduced in the framework, we initialize our system running by using pre-trained CNN to extract feature representations of all samples {x i } n i=1 . Set an initial m classifiers' pace parameter set λ = {λ j } m j=1 . Initialize the curriculum constraint Ψ λ with currently user annotated samples Ω λ and corresponding {y (j) } m j=1 and v.</p><p>Classifier Updating: This step aims to update the classifier parameters {w (j) , b (j) } m j=1 by one-vs-all SVM. Fixing {{x i } n i=1 , v, {y i } n i=1 , Ψ λ }, the original ASPL model Eqn. (2) can be simplified into the following form:</p><formula xml:id="formula_12">min w,b m j=1 1 2 w (j) 2 2 + C n i=1 v (j) i l w (j) , b (j) ; x i , y<label>(j) i</label></formula><p>, which can be equivalently reformulated as solving the following independent sub-optimization problems for each classifier j = 1, 2, • • • , m:</p><formula xml:id="formula_13">min w (j) ,b (j) 1 2 w (j) 2 2 + C n i=1 v (j) i l w (j) , b (j) ; x i , y (j) i .<label>(4)</label></formula><p>This is a standard one-vs-all SVM model with weights by taking one-class sample as positive while all others as negative. Specifically, when the weights v (j) i are only of values {0, 1}, it corresponds to a simplified SVM model under sampled instances with v (j) i = 1; otherwise when v j i sets values from [0, 1], it corresponds to the weighted SVM model. And both of them can be readily solved by many off-the-shelf efficient solvers. Thus, this step can be interpreted as implementing one-vs-all SVM over instances manually annotated from the AL and selfannotated from the SPL. High-confidence Sample Labeling: This step aims to assign pseudo-labels y and corresponding important weights v to the top-ranked samples of high confidences.</p><p>We start by employing the SPL to rank the unlabeled samples according to their importance weights v. Under fixed {w, b, {x i } n i=1 , {y i } n i=1 , Ψ λ }, our ASPL model in Eqn. ( <ref type="formula">2</ref>) can be simplified to optimize v as:</p><formula xml:id="formula_14">min v∈[0,1] m j=1 C n i=1 v (j) i l w (j) , b (j) ; x i , y (j) i + f v (j) ; λ j , s.t. v ∈ Ψ λ .</formula><p>(5) This problem then degenerates to a standard SPL problem as in Eqn. <ref type="bibr" target="#b0">(1)</ref>. Since both the self-paced regularizer f (v (j) ; λ j ) and the curriculum constraint Ψ λ is convex (with respect to v), various existing convex optimization techniques, like the gradient-based or interiorpoint methods, can be used for solving it. Note that we have multiple choices for the self-paced regularizer, as those built in <ref type="bibr" target="#b21">[22]</ref> <ref type="bibr" target="#b20">[21]</ref>. All of them comply with three axiomic conditions required for a self-paced regularizer, as defined in Section 2.</p><p>Based on the second axiomatic condition for selfpaced regularizer, any of the above f (v (j) ; λ j ) inclines to conduct larger weights on high-confidence (i.e., easy) samples with less loss values while vice versa, which evidently facilitates the model with the "learning from easy to hard" insight. In all our experiments, we utilize the linear soft weighting regularizer due to its relatively easy implementation and well adaptability to complex scenarios. This regularizer penalizes the sample weights linearly in terms of the loss. Specifically, we have</p><formula xml:id="formula_15">f (v (j) , λ j ) = λ j ( 1 2 v (j) 2 2 - n i=1 v (j) i ),<label>(6)</label></formula><p>where λ j &gt; 0. Eqn. ( <ref type="formula" target="#formula_15">6</ref>) is convex with respect to v (j) , and we can thus search for its global optimum by computing the partial gradient equals. Considering v (j) i ∈ [0, 1], we deduce the analytical solution for the linear soft weighting, as,</p><formula xml:id="formula_16">v (j) i = - C ij λj + 1, 0, C ij &lt; λ j otherwise,<label>(7)</label></formula><p>where ij = l w (j) , b (j) ; x i , y</p><p>(j) i is the loss of x i in the jth classifier. Note that the deducing way to Eqn. ( <ref type="formula" target="#formula_16">7</ref>) is similar with in <ref type="bibr" target="#b21">[22]</ref>, but our resulting solution is different since our ASPL model in Eqn. ( <ref type="formula">2</ref>) is new.</p><p>After obtaining the weight v for all unlabeled samples (i / ∈ Ω λ ) according to the optimized v (j) in a descending order. Then we consider the samples with larger important weight than others are high confidences. We form these samples into high-confidence sample set S and assign them pseudo-labels: Fixing {w, b, {x i } n i=1 , Ψ λ , v}, we optimize y i of Eqn. (2) which corresponds to solve:</p><formula xml:id="formula_17">min yi∈{-1,1} m ,i∈S n i=1 m j=1 v (j) i ij s.t., m j=1 |y (j) i + 1| ≤ 2. (<label>8</label></formula><formula xml:id="formula_18">)</formula><p>where v i is fixed and can be treated as constant. When i belongs to a certain person class, Eqn. ( <ref type="formula" target="#formula_17">8</ref>) has an optimum, which can be exactly extracted by the Theorem 1.</p><p>The proof is specified in the supplementary material. Denote those js that satisfy w (j)T x i + b (j) = 0 and v (j) i ∈ (0, 1] as a set M and set all y (b) When ∀j ∈ M except j = j * , w (j)T x i + b (j) &lt; 0, i.e., v (j * ) i ij * &gt; 0, then Eqn. (8) has a solution:</p><formula xml:id="formula_19">y (j) i = -1, j = j * 1, j = j * ;</formula><p>(c) Otherwise, Eqn. (8) has a solution:</p><formula xml:id="formula_20">y (j) i = -1, j = j * 1, j = j * ,</formula><p>where</p><formula xml:id="formula_21">j * = arg min 1≤j≤m v (j) i ij -1 + (w (j) T x i + b (j) ) + .<label>(9)</label></formula><p>Actually, only those high-confidence samples with positive weights, as calculated in the last updating step for v, are meaningful for the solution. This implies the physical interpretation for this optimization step: we 1. v (j) i = 0 actually implies that the i-th sample is with lowconfidence to be annotated as the j-th class, and thus it is natural to pseudo-label it as a negative sample for the j-th class. w T x + b = 0 implies that a sample is located in the classification boundary of the class, and thus it is also a low-confidence j-class sample and thus we directly annotate it as negative. Actually, for these samples, pseudo-label them as positive or negative will not affect the value of the objective function of Eq. ( <ref type="formula" target="#formula_17">8</ref>). We tend to annotate these lowconfidence samples as negative since due to the constraint of Eq. ( <ref type="formula" target="#formula_17">8</ref>) (at most one positive class one sample is allowed to be annotated), this will not influence selecting a more rational positive class for each sample.</p><p>iteratively find the high-confidence samples based on the current classifier, and then enforce pseudo-labels y i on those top-ranked high-confidence ones (i ∈ S). This is exactly the mechanism underlying a reranking technique <ref type="bibr" target="#b21">[22]</ref>.</p><p>The above optimization process can be understood as the self-learning manner of a student. The student tends to pick up most high-confident samples, which imply easier aspects and faithful knowledge underlying data, to learn, under the regularization of the pre-designed curriculum Ψ λ . Such regularization inclines to rectify his/her learning process so as to avoid him/her stuck into a unexpected overfitting point.</p><p>Low-confidence Sample Annotating: After pseudolabeling high-confidence samples in such a self-paced uncertainty modeling, we employ AL fashion to update the curriculum constraint Ψ λ in the model by supplementing more informative curriculums based on human knowledge. The AL process aims to select most lowconfidence unlabeled samples and to annotate them as either positive or negative by requesting user annotation. Our selection criteria are based on the classical uncertainty-based strategy <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Specifically, given the current classifiers, we randomly collected a number of randomly unlabeled samples, which are usually located in low-confidence area near the classification boundaries.</p><p>1) Annotated Sample Verifying: Considering the user annotation may contain outliers (incorrectly annotated samples), we introduce a verification step in the AL process. Specifically, in this step we first employ the current classifiers to obtain the prediction scores of all the annotated samples. Then we re-rank them and select Top-L ones with lowest prediction scores and ask the user to verify these selected samples, i.e., doublechecking them. We can set L as a small number (L = 5 in our experiments), since we do believe the chance of human making mistakes is low. In sum, we improve the robustness of the AL process by further validating Top-L most uncertain samples with the user. In this way, we can reduce the effects of accumulated human annotation errors and enable the classifier to be trained in a robust manner.</p><p>2) Low-confidence Definition: When we utilize the current classifiers (m classifiers for discriminating m object categories) to predict the label of unlabeled samples, those predicted as more than two positive labels (i.e., predicted as the corresponding object category) actually represent these samples making the current classifiers ambiguous. We thus adopt them as so called "lowconfident" samples and require active user to manually annotate them. Actually, in this step, other "lowconfidence" criterion can be utilized. We employed this simple strategy just due to its intuitive rationality and efficiency.</p><p>After users perform manual annotation, we update the Ψ λ by additionally incorporating those newly annotated sample set φ into the current curriculum Ψ λ . For each annotated sample, our AL process includes the following two operations: i) Set its curriculum constraint, i.e., {Ψ λ i } i∈φ = {1}; ii) Update its labels {y i } i∈φ and add its index into the set of currently annotated samples Ω λ . Such specified curriculum still complies with the axiomic conditions for the curriculum constraint as defined in <ref type="bibr" target="#b19">[20]</ref>. For those annotated samples, the corresponding Ψ λ i = {1} with expectation value 1 over the whole set, while for others Ψ λ i = [0, 1] with expectation value 1/2. Thus the more informative samples still have a larger expectation than the others. Also, it is easy to see Ψ λ is non-empty and convex. It thus complies traditional curriculum understanding.</p><p>New Class Handling: After the AL process, if active user annotates the selected unlabeled samples with u unseen person classes, new classifiers for these unseen classes are needed to be initialized without affecting the existed classifiers. Moreover, there is another difficulty that the samples of the new class are not enough for classifier training. Thanks to the proposed ASPL framework, we can employ the following four steps to address above mentioned issues.</p><p>1) For each of these new class samples, search all the unlabeled samples and pick out its K-nearest neighbors from the unseen class set U in the feature space; 2) Require active user to annotate these selected neighbors to enrich the positive samples for these new person classes; 3) Initialize and update {w (j) , b (j) , v (j) , y (j) , λ j } m+u j=m+1 for these new person classes according to above mentioned iteration process of {initialization, classifier updating, high-confidence sample labeling, lowconfidence sample annotating}.</p><p>This step corresponds to the instructor's role in human education, which aims to guide a student to involve more informative curriculums in learning. Different from the previous fixed curriculum setting in SPL throughout the learning process, here the curriculum is dynamically updated based on the self-paced learned knowledge of the model. Such an improvement better simulates the general learning process of a good student. With the learned knowledge of a student increasing, his/her instructor should vary the curriculum settings imposed on him from more in the early stage to less in later. This learning manner evidently should conduct a better learning effect which can well adapt the personal information of the student.</p><p>Feature Representation Updating: After several of the SPL and AL updating iterations of {w, b, {y i } n i=1 , v, Ψ λ }, we now aim to update the feature representation {x i } n i=1 through finetuning the pretrained CNN by inputting all manually labeled samples from the AL and self-annotated ones from the SPL. These samples tend to deliver data knowledge into the network and improve the representation of the training samples. A better feature representation is thus expected to be extracted from this ameliorated CNN.</p><p>This learning process simulates the updating of the knowledge structure of a human brain after a period of domain learning. Such updating tends to facilitate a person grasp more effective features to represent newly coming samples from certain domain and make him/her with a better learning performance. In our experiments, we generally conduct the CNN feature fine-tuning after around 50 rounds of the SPL and AL updating, and the learning rate is set as 0.001 for all layers.</p><p>Pace Parameter Updating: We utilize a heuristic strategy to update pace parameters {λ j } m j=1 for m classifiers in our implementation. </p><p>Pseudo-label high-confidence samples {y i } i∈S by the reranking via Eqn. (</p><p>Update the unclear class set U</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Verify the annotated samples by AL. In every T iterations:</p><p>• Update {x i } n i=1 through fine-tuning CNN • Update λ according to Eqn. <ref type="bibr" target="#b9">(10)</ref> 9: end while 10: return w, b;</p><p>After multiple iterations of the ASPL, we specifically set the pace parameter λ j for each individual classifier, and utilize a heuristic strategy in our implementation for parameter updating. For the tth iteration, we compute the pace parameter for optimizing Eqn. (2) by :</p><formula xml:id="formula_24">λ t j =      λ 0 , t = 0 λ (t-1) j + α * η t j , 1 ≤ t ≤ τ λ (t-1) j , t &gt; τ,<label>(10)</label></formula><p>where η t j is the average accuracy of the j-th classifier in the current iteration, and α is a parameter which controls the pace increasing rate. In our experiments, we empirically set {λ 0 , α} = {0.2, 0.08}. Note that the pace parameters λ should be stopped when all training samples are with v = {1}. Thus, we introduce an empirical threshold τ constraining that λ is only updated in early iterations, i.e., t ≤ τ . τ is set as 12 in our experiments.</p><p>The entire algorithm can then be summarized into Algorithm 1. It is easy to see that this solving strategy for the ASPL model finely accords with the pipeline of our framework.</p><p>Convergence Discussion: As illustrated in Algorithm 1, the ASPL algorithm alternatively updates variables including: the classifier parameters w, b (by weighted SVM), the pseudo-labels y (closed-form solution by Theorem 1), the importance weight v (by SPL), and lowconfidence sample annotations φ (by AL). For the first three parameters, these updates are calculated by a global optimum obtained from a sub-problem of the original model, and thus the objective function can be guaranteed to be decreased. However, just as other existing AL techniques, human efforts are involved in the loop of the AL stage, and thus the objective function cannot be guaranteed to be monotonically decreased in this step. However, just as shows in Sect. 5, as the learning processing, the model tends to be more and more mature, and the labor of AL tends to be less and less in the later learning stage. Thus with gradually less involvement of the AL calculation in our algorithm, the monotonic decrease of the objective function in iteration tends to be promised, and thus our algorithm tends to be convergent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relationship with Other SPL/AL Models</head><p>It is easy to see that the proposed ASPL model extends the previous AL/SPL models and includes all of them as special cases. When we fix the curriculum and feature representations and only update other parameters, it degenerates to the traditional SPL models by rationally setting the self-paced regularizer. When we fix the SPL parameters, feature representations and do not involve pseudo-labels in learning, the model degenerates the a general AL learning regime. The amelioration to both SPL and AL is expected to bring benefits to both regimes. On one hand, introducing more high-confidence samples in the self-paced fashion is helpful to reduce the burden of user annotations, particularly when the classifier becomes reliable at later learning iterations. On the other hand, the low confidence samples selected by active user annotations tends to make our approach workable with less initial labeled samples than existing self-paced learning algorithms. All these benefits are comprehensively substantiated by our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first introduce the datasets and implementation setting, and then discuss the experimental results and comparisons with other existing approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Setting</head><p>We adopt two public datasets in our experiments, the Cross-Age Celebrity Dataset (CACD) <ref type="bibr" target="#b46">[47]</ref> and CASIA-WebFace-Sub dataset <ref type="bibr" target="#b47">[48]</ref>.</p><p>CACD is a large-scale and challenging dataset for evaluating face recognition and retrieval, and it contains a batch of images of 2, 000 celebrities collected from Internet, which are varying in age, pose, illumination, and occlusion. And only a subset of 200 celebrities are manually annotated by Chen et al. <ref type="bibr" target="#b46">[47]</ref>. For better convincing evaluation, we augment this subset by extra labeling 300 individuals and obtain a set of 56, 138 images in total.</p><p>CASIA-WebFace dataset <ref type="bibr" target="#b47">[48]</ref> is a large scale face recogdataset with 10,575 subjects/persons and 494,414 images. CASIA-WebFace is extremely challenging for its images are all collected from Internet with different view points and light illumination under different scenes. Though the total person/subject number of CASIA-WebFace dataset is very large, the sample number for each person, varying from 3 to 804, is heavily unbalanced. For those persons who has very few samples (say below 100), the experiment analysis is not able to be performed. Hence, we select a subset of the CASIA-WebFace dataset by discarding its persons with less than 100 samples to form the CASIA-WebFace-Sub dataset. The CASIA-WebFace-Sub dataset has 181,901 images with 925 persons inside.</p><p>The detailed information of above mentioned datasets is summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Experiment setting. We detect the facial points using the method proposed in <ref type="bibr" target="#b48">[49]</ref> and align the faces based on the eye locations. The experiments on both of the datasets are conducted as the following steps. We first randomly select 80% images of each individual to form the unlabeled training set, and the rest samples are used for testing, according to the setting in the existing active learning method <ref type="bibr" target="#b18">[19]</ref>. Then, we randomly annotate n samples of each person in the training set to initialize the classifier. To get rid of the influence of randomness, we average the results over times of execution with different sample selections. All of the experiments are conducted on a common desktop PC with i7 3.4GHz CPU and a NVIDIA Titan X GPU.</p><p>On the two above mentioned datasets, we evaluate the performance of incremental face identification in two aspects: the recognition accuracy and user annotation amount in the incremental learning process. The recognition accuracy is defined as the rank-one rate for face identification. We compare our ASPL framework with several existing active learning algorithms and baseline methods under the same setting: i) CPAL (Convex Programming based Active Learning) <ref type="bibr" target="#b18">[19]</ref>: Annotate a few samples in each step based on prediction uncertainty and sample diversity; ii) CCAL (Confidence-based Active Learning via SVMs) <ref type="bibr" target="#b32">[33]</ref>: Select only one sample having lowest prediction confidence; iii) AL RAND: Randomly select unlabeled samples to be annotated during the training phase. This method discards all active learning techniques and can be considered as the lower bound, and iv) AL ALL: All unlabeled samples are annotated for training the classifier. This method can be regarded as the upper bound (best performance the classifier can achieve). For fair comparison, all of these methods utilize the same feature representation as ours in the beginning. As the training iteration increase, active user annotation is employed to those selected most informative and representative samples. Then, CNN fine-tuning is also exploited to improve the feature extractor for ASPL, CPAL, CCAL, AL RAND, AL ALL.</p><p>Details of CNN implementation. The architecture of AlexNet <ref type="bibr" target="#b44">[45]</ref> is utilized in our all experiments. Thanks to the well pre-training, the CNN updating is only implemented few times during ASPL iteration in all our experiments, each only containing no more than 5 CNN updating steps. We generally conducted CNN steps after around 5 rounds of the SPL and AL updating, and the learning rate is set as 0.001 for all layers. Equal importance is imposed between the previous training examples and the newly labeled examples, and CNN is updated using the stochastic gradient decent methods with the momentum 0.9 and weight decay 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Comparisons</head><p>The results on the two datasets are reported in Fig. <ref type="figure" target="#fig_6">3(a)</ref> and Fig. <ref type="figure" target="#fig_6">3(b)</ref>, respectively, where we can observe how the recognition accuracy changes with increasingly incorporating more unlabeled samples. In CACD dataset, to achieve the same recognition accuracy, ASPL model requires few annotation of the unlabeled data. On the other hand, ASPL outperforms the competing methods in accuracy when the same amount annotations. ASPL can still have a superior performance as the iteration goes on. The similar results and phenomena can be discovered in CASIA-WebFace-Sub dataset. As one can see that, ASPL only requires about 40% and 45% annotations to achieve the-state-of-art performance on CACD and CASIA-WebFace-Sub dataset, respectively. While the compared methods AL RAND, CCAL and CPAL all requires about 81% and 65%, respectively. Hence, our ASPL can performs as well as the AL ALL with minimal annotations.</p><p>Note that the performances of RAND and CCAL are relatively close, and the similar results were reported in <ref type="bibr" target="#b18">[19]</ref>. According to the explanation in <ref type="bibr" target="#b18">[19]</ref>, this comes from the fact that many samples have low prediction confidences and distribute not densely in the feature space. Thus, the randomizing sample selection achieves similar results compared to CCAL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Component Analysis</head><p>To further analyze how different components contribute to performance, we implement several variants of our framework: i) ASPL (w/o FT): allowing both active and self-paced sample selection during learning while disabling the CNN fine-tuning, i.e., the feature extractor is kept the same as the iteration goes on for training; ii) ASPL (w/o SPL): discarding high-confidence sample pseudo-labeling via self-paced learning; iii) ASPL (w/o AL): ignoring low confidence samples for active user annotation; iv) AL ALL: fine-tuning the CNN and train classifiers with all the labels of the training samples and v) AL ALL (w/o FT): training classifiers with all the labels of the training samples without fine-tuning. Moreover, the full version of our proposed model is denoted as ASPL, which allows the convolutional nets to be fine-tuned during the training process. We further evaluate the ASPL variants in the following aspects. Fig. <ref type="figure">4</ref>. Accuracies with the increase of annotated samples of different variants of our framework, using CASIA-Webface-Sub dataset.</p><p>gradually add the AL, SPL and fine-tuning components to ASPL. These experiments are executed on the CASIA-Webface dataset. Fig. <ref type="figure">4</ref> illustrates the accuracy obtained using ASPL, ASPL (w/o FT), ASPL (w/o AL) and ASPL (w/o SPL). One can observe that any of the three components is useful in improving the recognition accuracy. Especially, the additional SPL component can significantly improve the recognition accuracy and reduce the number of annotation samples by automatically exploiting the majority of high-confidence samples for feature learning.</p><p>We also observe that the CNN feature fine-tuning can dramatically improve the recognition accuracy in the early steps. This is mainly because the information gain (i.e., individual appearance diversity) deceases with progressively introducing new samples to the neural nets.</p><p>Analysis on initial samples. In SPL <ref type="bibr" target="#b23">[24]</ref>, classifier is first trained using the initial samples. With the current classifier, easy samples are preferred to be selected in the early training steps, and thus it is expected that the performance of SPL heavily relies on the initial samples. Fortunately, by incorporating with active learning, ASPL can evidently alleviate this problem. To verify this, we compare the performance of ASPL and SPL on 20 randomly selected individuals of CASIA-Webface-Sub dataset. The result is shown in Fig. <ref type="figure">5</ref>. Given the same initialized feature representations, we also conduct the experiments to analyze the performance vs different initial portions to be handled by AL on this dataset. The results are illustrated in Fig. <ref type="figure">6</ref>.</p><p>As one can see from Fig. <ref type="figure">5</ref>, with different initial samples, ASPL reaches similar/stable results as the training continues, while SPL still varies a lot. This result indicates that the AL component is effective in handling the poor initialization. Fig. <ref type="figure">6</ref> illustrates that though poor performance is obtained at the beginning, the performance of our model increases during the training process. In summary, our model is insensitive to the diversity and quantity of initial samples.  Performance with new classes. To justify the effectiveness of our ASPL for handling unseen new classes, we conduct the following experiment on the CASIA-WebFace-Sub dataset: We compare the performance of incrementally giving some classes (our ASPL) and directly giving all person classes. Specifically, given all person classes, we initialize all the classifiers at the beginning of the training and optimize them without handling unseen new classes. We denote this variant  as ASPL (ALL). The experimental result is illustrated in Table <ref type="table" target="#tab_2">2</ref> and shows that our proposed ASPL can handle unseen new classes effectively without substantially performance drop or even with slightly better performance, compared with the all classes given version ASPL (ALL).</p><p>Annotation required for large scale dataset. To demonstrate that our ASPL can be adopted under large scale scenario, we analyze the training phase of ASPL on the large scale CASIA-WebFace-Sub dataset. As illustrated in Fig. <ref type="figure" target="#fig_5">7</ref>, the x-axis denotes the number of training iterations and the y-axis denotes the amount of required user annotation. The curve in Fig. <ref type="figure" target="#fig_5">7</ref> demonstrates that our proposed ASPL model requires relatively larger annotations when the training iteration number is small. As the training continues, the amount required annotations began to be reduced due to the gradually mature model incrementally ameliorated in the learning process. This observation indicates that the burden of user annotations would be indeed relieved when the classifier becomes reliable at the later learning stage of the proposed ASPL method. Moreover, as illustrated in Table <ref type="table" target="#tab_3">3</ref>, with the increase of user annotations over time, ASPL can automatically assign more reliable pseudo-labels to the unlabeled samples selected in the self-paced way.</p><p>Robustness analysis.</p><p>We further analyze the robustness of ASPL when noisy images are deliberately included in two experiments. (i) Ex-1: a (a = 0%, 10%, 30%, 50%) noisy images are added to the initial samples for each individual. (ii) Ex-2: noise-free initials are used, but b (b = 0%, 10%, 30%, 50%) importers are deliberately annotated during the training process. These experiments are conducted on the CASIA-Webface-Sub dataset. To validate the effectiveness of the proposed annotated sample verifying step, we disable the verifying step and denote these modification as "Noise w/o VF".</p><p>Fig. <ref type="figure" target="#fig_9">8</ref>(a) shows the result of Ex-1, where ASPL is initialized with different number of noisy images. In early steps of the iteration, noisy data have huge adverse effect on test accuracy. Along with the increase iteration number, the genuine data gradually dominate the results. Fig. <ref type="figure" target="#fig_9">8</ref>(b) illustrates the result of Ex-2, where noisy images are added to the labeled training set the 2-th step of iteration. We can see that a sharp decline in the recognition accuracy. However, with the evolving of ASPL training, similar accuracy as compared with that got on the original clean data can be obtained when the number of iterations increases. As one can comparing "Noise (10/30/50%)" with "Noise (10/30/50%) w/o VF" from Fig. <ref type="figure" target="#fig_9">8(a)</ref>, with the verifying step, ASPL can recover from noisy images in a slightly fast way. This justifies the effectiveness of the proposed annotated sample verifying step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we have introduced, first, an effective framework to solve incremental face identification, which build classifiers by progressively annotating and selecting unlabeled samples in an active self-paced way, and second, a theoretical interpretation of the proposed framework pipeline from the perspective of optimiza-tion. Third, we evaluate our approach on challenging scenarios and show very promising results.</p><p>In future work, we can generalize our framework into other generic large-scale object recognition tasks (e.g., 20 categories in PASCAL VOC and 1000 categories in ImageNet). Moreover, we plan to study the parallelized version of our framework that can be deployed in distributed computing environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of high-and low-confidence samples in the feature space. (a) shows a few face instances of different individuals, and these instances have large appearance variations. (b) illustrates how the samples distribute in the feature space, where samples of high classification confidence distribute compactly to form several clusters and low confidence samples are scattered and close to the classifier decision boundary.</figDesc><graphic coords="2,74.16,60.21,87.39,93.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our proposed cost-effective framework. The pipeline includes stages of CNN and model initialization; classifier updating; high-confidence sample labeling by the SPL, low-confidence sample annotating by AL and CNN fine-tuning, where the arrows represent the workflow. The images highlighted by blue in the left panel represent the initially selected samples.</figDesc><graphic coords="5,48.96,53.14,514.07,159.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>weight and bias parameters of the decision functions for all m classifiers. C(C &gt; 0) is the standard regularization parameter trading off the loss function and the margin, and we set C = 1 in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>= - 1</head><label>1</label><figDesc>for others in default1 . The solution of Eqn. (8) for y (j) i , j ∈ M can be obtained by the following theorem.Theorem 1:(a) If ∀j ∈ M , w (j)T x i + b (j) &lt; 0, Eqn. (8) has a solution: y (j) i = -1, j = 1, ..., m;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>The sketch of ASPL framework Input: Input dataset {x i } n i=1 Output: Model parameters w, b 1: Use pre-trained CNN to extract feature representations of {x i } n i=1 . Initialize multiple annotated samples into the curriculum Ψ λ and corresponding {y i } n i=1 and v. Set an initial pace parameter λ = {λ 0 } m . while not converged do 2: Update w, b by one-vs-all SVM 3: Update v by the SPL via Eqn. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 :</head><label>7</label><figDesc>Update low-confidence samples {y i , Ψ λ i } i∈φ by the AL if u unseen classes have labeled, Handle u new classes via the steps in Sect. 4.1 Go to the step 2 end if 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results on (a) CACD and (b) CASIA-WebFace-Sub datasets. The vertical axes represent the recognition accuracy and the horizontal axes represent the percentage of annotated samples of the whole set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. The accuracy and standard deviation of ASPL and SPL on the CASIA-Webface-Sub dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The comparison of different number of initial samples and the further required annotation ported of the AL process on the CASIA-WebFace-Sub dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Robust analysis of ASPL under two types of noisy samples. (a) Using different number of noisy samples as the initial annotation. (b) Adding different number of noisy samples at the 10-th step (denoted by the black spots).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>The summarization of datasets we used.</figDesc><table><row><cell>Dataset</cell><cell># images</cell><cell># persons</cell><cell># images/person</cell></row><row><cell>CACD</cell><cell>56,138</cell><cell>500</cell><cell>79∼306</cell></row><row><cell>CASIA-WebFace-Sub</cell><cell>181,901</cell><cell>925</cell><cell>100∼804</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>The performance comparison of whether handling unseen new classes or not on the CASIA-WebFace-Sub dataset. ASPL (ALL) denotes the ASPL version of no unseen classes.</figDesc><table><row><cell># Class Number</cell><cell>300</cell><cell>600</cell><cell>925</cell></row><row><cell>ASPL (ALL)</cell><cell>88.3%</cell><cell></cell><cell></cell></row><row><cell>ASPL</cell><cell>88.3%</cell><cell>81.6%</cell><cell>76.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>The error rates of the pseudo-labels assigned by SPL on high-confidence samples.</figDesc><table><row><cell cols="3"># iteration</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell></row><row><cell cols="3">ASPL (w/o FT)</cell><cell>8.2%</cell><cell>6.9%</cell><cell>5.1%</cell><cell>5.0%</cell><cell>4.9%</cell></row><row><cell cols="3">ASPL</cell><cell>4.5%</cell><cell>4.1%</cell><cell>3.4%</cell><cell>3.3%</cell><cell>3.3%</cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>percentage of labeled samples</cell><cell>10% 20% 40% 60%</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ASPL ASPL (w/o FT) ASPL (w/o SPL) ASPL (w/o AL)</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">number of iterations</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 2015.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic personality and interaction style recognition from facebook profile pictures</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Celli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Lepri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">With one look: robust face recognition using single sample per person</title>
		<author>
			<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition from multiple images per subject</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental face recognition for large-scale social network services</title>
		<author>
			<persName><forename type="first">Kwontaeg</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kar-Ann</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward large-scale face recognition using social network context</title>
		<author>
			<persName><forename type="first">Zak</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face identification using large feature sets</title>
		<author>
			<persName><forename type="first">Huimin</forename><surname>William Robson Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Imag. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2245" to="2255" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extending face identification to open-set face recognition</title>
		<author>
			<persName><forename type="first">Cassio</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santos</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Robson</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIBGRAPI on Graphics, Patterns and Images</title>
		<meeting>of SIBGRAPI on Graphics, Patterns and Images</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="188" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised metric learning for face identification in TV video</title>
		<author>
			<persName><forename type="first">Ramazan</forename><surname>Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1559" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward large-population face identification in unconstrained videos</title>
		<author>
			<persName><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hairong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Techn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1874" to="1884" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning stacked image descriptor for face recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Partial face recognition: Alignment-free approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1193" to="1205" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards pose robust face recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3539" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Highfidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiangyu Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hybrid deep learning for face verification</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoo</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptively unified semisupervised dictionary learning with active points</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1787" to="1795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards making unlabeled data never hurt</title>
		<author>
			<persName><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="175" to="188" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A novel incremental principal component analysis and its application for face recognition</title>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SMC, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental linear discriminant analysis using sufficient spanning set approximations</title>
		<author>
			<persName><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwan-Yee Kenneth</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Bj Örn Stenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A convex optimization framework for active learning</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sapiro</forename><surname>Guillermo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasrty S</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Conference on Artificial Intelligence</title>
		<meeting>of AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Easy samples first: self-paced reranking for zero-example multimedia search</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Jér Ôme Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Machine Learning</title>
		<meeting>of IEEE International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">When face recognition meets with deep learning: An evaluation of convolutional neural networks for face recognition</title>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional networks and applications in vision</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCAS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interactive surveillance event detection through mid-level discriminative representation</title>
		<author>
			<persName><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoquan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A tutorial on principal components analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">52</biblScope>
		</imprint>
		<respStmt>
			<orgName>Cornell University, USA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiple incremental decremental learning of support vector machines</title>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ichiro</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A fast and accurate online sequential learning algorithm for feedforward networks</title>
		<author>
			<persName><forename type="first">Nan-Ying</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Incremental learning of feature space and classifier for face recognition</title>
		<author>
			<persName><forename type="first">Seiichi</forename><surname>Ozawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Employing em and pool-based active learning for text classification</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kachites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mccallumzy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Machine Learning</title>
		<meeting>of IEEE International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-class active learning for image classification</title>
		<author>
			<persName><forename type="first">Ajay</forename><forename type="middle">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Which faces to tag: Adding prior constraints into active learning</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active learning with gaussian processes for object categorization</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive active learning for image classification</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Incorporating diversity in active learning with support vector machines</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Machine Learning</title>
		<meeting>of IEEE International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-paced learning for matrix factorization</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Conference on Artificial Intelligence</title>
		<meeting>of AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning specific-class segmentation from diverse data</title>
		<author>
			<persName><forename type="first">Haithem</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Turki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Preston</surname></persName>
		</author>
		<author>
			<persName><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning the easy things first: Self-paced visual category discovery</title>
		<author>
			<persName><forename type="first">Jae</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-paced learning for longterm tracking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cmu-informedia@ trecvid 2014 multimedia event detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Video Retrieval Evaluation Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-age reference coding for age-invariant face recognition and retrieval</title>
		<author>
			<persName><forename type="first">Chu-Song</forename><surname>Bor-Chun Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning face representation from scratch</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">7923</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
