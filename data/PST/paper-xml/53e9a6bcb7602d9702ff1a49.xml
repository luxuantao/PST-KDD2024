<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sample complexity of testing the manifold hypothesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hariharan</forename><surname>Narayanan</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Sanjoy</forename><surname>Mitter</surname></persName>
							<email>mitter@mit.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Information and Decision Systems EECS</orgName>
								<orgName type="institution">MIT Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory for Information and Decision Systems EECS</orgName>
								<orgName type="institution">MIT Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sample complexity of testing the manifold hypothesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B4BA0D3C0B82262F2D09A7DFEF527C2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of fitting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is independent of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume and curvature is unavoidable. Whether the known lower bound of O( k 2 + log 1 δ 2 ) for the sample complexity of Empirical Risk minimization on k-means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 <ref type="bibr" target="#b2">[3]</ref>. Here is the desired bound on the error and δ is a bound on the probability of failure. We improve the best currently known upper bound <ref type="bibr" target="#b13">[14]</ref> of</p><p>. Based on these results, we devise a simple algorithm for k-means and another that uses a family of convex programs to fit a piecewise linear curve of a specified length to high dimensional data, where the sample complexity is independent of the ambient dimension.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We are increasingly confronted with very high dimensional data in speech signals, images, geneexpression data, and other sources. Manifold Learning can be loosely defined to be a collection of methodologies that are motivated by the belief that this hypothesis (henceforth called the manifold hypothesis) is true. It includes a number of extensively used algorithms such as Locally Linear Embedding <ref type="bibr" target="#b16">[17]</ref>, ISOMAP <ref type="bibr" target="#b18">[19]</ref>, Laplacian Eigenmaps <ref type="bibr" target="#b3">[4]</ref> and Hessian Eigenmaps <ref type="bibr" target="#b7">[8]</ref>. The sample complexity of classification is known to be independent of the ambient dimension <ref type="bibr" target="#b14">[15]</ref> under the manifold hypothesis, (assuming the decision boundary is a manifold as well,) thus obviating the curse of dimensionality. A recent empirical study <ref type="bibr" target="#b5">[6]</ref> of a large number of 3 × 3 images, represented as points in R 9 revealed that they approximately lie on a two dimensional manifold known as the Klein bottle. On the other hand, knowledge that the manifold hypothesis is false with regard to certain data would give us reason to exercise caution in applying algorithms from manifold learning and would provide an incentive for further study.</p><p>It is thus of considerable interest to know whether given data lie in the vicinity of a low dimensional manifold. Our primary technical results are the following.</p><p>1. We obtain uniform bounds relating the empirical squared loss and the true squared loss over a class F consisting of manifolds whose dimensions, volumes and curvatures are bounded in Theorems 1 and 2. These bounds imply upper bounds on the sample complexity of Empirical Risk Minimization (ERM) that are independent of the ambient dimension, exponential in the intrinsic dimension, polynomial in the curvature and almost linear in the volume.</p><p>2. We obtain a minimax lower bound on the sample complexity of any rule for learning a manifold from F in Theorem 6 showing that for a fixed error, the the dependence of the sample complexity on intrinsic dimension, curvature and volume must be at least exponential, polynomial, and linear, respectively.</p><p>3. We improve the best currently known upper bound <ref type="bibr" target="#b13">[14]</ref> on the sample complexity of Empirical Risk minimization on k-means applied to data in a unit ball of arbitrary dimension from O(</p><formula xml:id="formula_0">k 2 2 + log 1 δ 2 ) to O k 2 min k, log 4 k 2 + log 1 δ 2 . Whether the known lower bound of O( k 2 + log 1<label>δ</label></formula><p>2 ) is tight, has been an open question since 1997 <ref type="bibr" target="#b2">[3]</ref>. Here is the desired bound on the error and δ is a bound on the probability of failure.</p><p>One technical contribution of this paper is the use of dimensionality reduction via random projections in the proof of Theorem 5 to bound the Fat-Shattering dimension of a function class, elements of which roughly correspond to the squared distance to a low dimensional manifold. The application of the probabilistic method involves a projection onto a low dimensional random subspace. This is then followed by arguments of a combinatorial nature involving the VC dimension of halfspaces, and the Sauer-Shelah Lemma applied with respect to the low dimensional subspace. While random projections have frequently been used in machine learning algorithms, for example in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>, to our knowledge, they have not been used as a tool to bound the complexity of a function class. We illustrate the algorithmic utility of our uniform bound by devising an algorithm for k-means and a convex programming algorithm for fitting a piecewise linear curve of bounded length. For a fixed error threshold and length, the dependence on the ambient dimension is linear, which is optimal since this is the complexity of reading the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Connections and Related work</head><p>In the context of curves, <ref type="bibr" target="#b9">[10]</ref> proposed "Principal Curves", where it was suggested that a natural curve that may be fit to a probability distribution is one where every point on the curve is the center of mass of all those points to which it is the nearest point. A different definition of a principal curve was proposed by <ref type="bibr" target="#b11">[12]</ref>, where they attempted to find piecewise linear curves of bounded length which minimize the expected squared distance to a random point from a distribution. This paper studies the decay of the error rate as the number of samples tends to infinity, but does not analyze the dependence of the error rate on the ambient dimension and the bound on the length. We address this in a more general setup in Theorem 4, and obtain sample complexity bounds that are independent of the ambient dimension, and depend linearly on the bound on the length. There is a significant amount of recent research aimed at understanding topological aspects of data, such its homology <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16]</ref>. It has been an open question since 1997 <ref type="bibr" target="#b2">[3]</ref> , using an argument that bounds the Fat-Shattering dimension of the appropriate function class using random projections and the Sauer-Shelah Lemma. Generalizations of principal curves to parameterized principal manifolds in certain regularized settings have been studied in <ref type="bibr" target="#b17">[18]</ref>. There, the sample complexity was related to the decay of eigenvalues of a Mercer kernel associated with the regularizer. When the manifold to be fit is a set of k points (k-means), we obtain a bound on the sample complexity s that is independent of m and depends at most linearly on k, which also leads to an approximation algorithm with additive error, based on sub-sampling. If one allows a multiplicative error of 4 in addition to an additive error of , a statement of this nature has been proven by Ben-David (Theorem 7, <ref type="bibr" target="#b4">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Upper bounds on the sample complexity of Empirical Risk Minimization</head><p>In the remainder of the paper, C will always denote a universal constant which may differ across the paper. For any submanifold M contained in, and probability distribution P supported on the unit ball B in R m , let L(M, P) := d(M, x) 2 dP(x). Given a set of i.i.d points x = {x 1 , . . . , x s } from P, a tolerance and a class of manifolds F, Empirical Risk Minimization (ERM) outputs a manifold in</p><formula xml:id="formula_1">M erm (x) ∈ F such that s i=1 d(x i , M erm ) 2 ≤ /2+inf N ∈F d(x i , N ) 2 .</formula><p>Given error parameters , δ, and a rule A that outputs a manifold in F when provided with a set of samples, we define the sample complexity s = s( , δ, A) to be the least number such that for any probability distribution P in the unit ball, if the result of A applied to a set of at least s i.i.d random samples from P is N , then P [L(N , P) &lt; inf M∈F L(M, P) + ] &gt; 1 -δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bounded intrinsic curvature</head><p>Let M be a Riemannian manifold and let p ∈ M. Let ζ be a geodesic starting at p. Definition 1. The first point on ζ where ζ ceases to minimize distance is called the cut point of p along M. The cut locus of p is the set of cut points of M. The injectivity radius is the minimum taken over all points of the distance between the point and its cut locus. M is complete if it is complete as a metric space.</p><formula xml:id="formula_2">Let G i = G i (d, V, λ, ι</formula><p>) be the family of all isometrically embedded, complete Riemannian submanifolds of B having dimension less or equal to d, induced d-dimensional volume less or equal to V , sectional curvature less or equal to λ and injectivity radius greater or equal to ι. Let</p><formula xml:id="formula_3">U int ( 1 , d, V, λ, ι) := V C d min( ,ι,λ -1/2 ) d</formula><p>, which for brevity, we denote U int .</p><p>Theorem 1. Let and δ be error parameters. If</p><formula xml:id="formula_4">s ≥ C min 1 2 log 4 U int , U int U int 2 + 1 2 log 1 δ ,</formula><p>and x = {x 1 , . . . , x s } is a set of i.i.d points from P then,</p><formula xml:id="formula_5">P L(M erm (x), P) -inf M∈Gi L(M, P) &lt; &gt; 1 -δ.</formula><p>The proof of this theorem is deferred to Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bounded extrinsic curvature</head><p>We will consider submanifolds of B that have the property that around each of them, for any radius r &lt; τ , the boundary of the set of all points within a distance r is smooth. This class of submanifolds has appeared in the context of manifold learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>. The condition number is defined as follows.</p><p>Definition 2 (Condition Number). Let M be a smooth d-dimensional submanifold of R m . We define the condition number c(M) to be 1 τ , where τ is the largest number to have the property that for any r &lt; τ no two normals of length r that are incident on M have a common point unless it is on M.</p><p>Let G e = G e (d, V, τ ) be the family of Riemannian submanifolds of B having dimension ≤ d, volume ≤ V and condition number ≤ 1 τ . Let and δ be error parameters.</p><formula xml:id="formula_6">Let U ext ( 1 , d, τ ) := V C d min( ,τ ) d</formula><p>, which for brevity, we denote by U ext .</p><formula xml:id="formula_7">Theorem 2. If s ≥ C min 1 2 log 4 U ext , U ext U ext 2 + 1 2 log 1 δ ,</formula><p>and x = {x 1 , . . . , x s } is a set of i.i.d points from P then,</p><formula xml:id="formula_8">P L(M erm (x), P) -inf M L(M, P) &lt; &gt; 1 -δ.<label>(1)</label></formula><p>4 Relating bounded curvature to covering number</p><p>In this subsection, we note that that bounds on the dimension, volume, sectional curvature and injectivity radius suffice to ensure that they can be covered by relatively few -balls. Let V M p be the volume of a ball of radius M centered around a point p. See ( <ref type="bibr" target="#b8">[9]</ref>, page 51) for a proof of the following theorem. Theorem 3 (Bishop-Günther Inequality). Let M be a complete Riemannian manifold and assume that r is not greater than the injectivity radius ι. Let K M denote the sectional curvature of M and let λ &gt; 0 be a constant. Then,</p><formula xml:id="formula_9">K M ≤ λ implies V M p (r) ≥ 2π n 2 Γ( n 2 ) r 0 sin(t √ λ) √ λ n-1 dt. Thus, if &lt; min(ι, πλ -1 2 2 ), then, V M p ( ) &gt; Cd d .</formula><p>Proof of Theorem 1. As a consequence of Theorem 3, we obtain an upper bound of V </p><formula xml:id="formula_10">U 1 ≤ V Cd min( ,λ -1 2 ,ι) d .</formula><p>The proof of Theorem 2 is along the lines of that of Theorem 1, so it has been deferred to the journal version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Class of manifolds with a bounded covering number</head><p>In this section, we show that uniform bounds relating the empirical squares loss and the expected squared loss can be obtained for a class of manifolds whose covering number at a different scale has a specified upper bound. Let U : R + → Z + be any integer valued function. Let G be any family of subsets of B such that for all r &gt; 0 every element M ∈ G can be covered using open Euclidean balls of radius r centered around U ( 1 r ) points; let this set be Λ M (r). Note that if the subsets consist of k-tuples of points, U (1/r) can be taken to be the constant function equal to k and we recover the k-means question. A priori, it is unclear if</p><formula xml:id="formula_11">sup M∈G s i=1 d(x i , M) 2 s -E P d(x, M) 2 ,<label>(2)</label></formula><p>is a random variable, since the supremum of a set of random variables is not always a random variable (although if the set is countable this is true). However ( <ref type="formula" target="#formula_11">2</ref>) is equal to</p><formula xml:id="formula_12">lim n→∞ sup M∈G s i=1 d(x i , Λ M (1/n)) 2 s -E P d(x, Λ M (1/n)) 2 ,<label>(3)</label></formula><p>and for each n, the supremum in the limits is over a set parameterized by U (n) points, which without loss of generality we may take to be countable (due to the density and countability of rational points). Thus, for a fixed n, the quantity in the limits is a random variable. Since the limit as n → ∞ of a sequence of bounded random variables is a random variable as well, (2) is a random variable too. Theorem 4. Let and δ be error parameters. If</p><formula xml:id="formula_13">s ≥ C U (16/ ) 2 min U (16/ ), 1 2 log 4 U (16/ ) + 1 2 log 1 δ ,</formula><p>Then,</p><formula xml:id="formula_14">P sup M∈G s i=1 d(x i , M) 2 s -E P d(x, M) 2 &lt; 2 &gt; 1 -δ.<label>(4)</label></formula><p>Proof. For every g ∈ G, let c(g, ) = {c 1 , . . . , c k } be a set of k := U (16/ ) points in g ⊆ B, such that g is covered by the union of balls of radius /16 centered at these points. Thus, for any point x ∈ B,</p><formula xml:id="formula_15">d 2 (x, g) ≤ 16 + d(x, c(g, )) 2<label>(5)</label></formula><formula xml:id="formula_16">≤ 2 256 + min i x -c i 8 + d(x, c(g, )) 2 .<label>(6)</label></formula><p>Since min i x -c i is less or equal to 2, the last expression is less than 2 + d(x, c(g, )) 2 . Our proof uses the "kernel trick" in conjunction with Theorem 5. Let Φ : (x 1 , . . . , x m ) T → 2 -1/2 (x 1 , . . . , x m , 1) T map a point x ∈ R m to one in R m+1 . For each i, let c i := (c i1 , . . . , c im ) T , and ci := 2 -1/2 (-c i1 , . . . , -c im , ci 2 2 ) T . The factor of 2 -1/2 is necessitated by the fact that we wish the image of a point in the unit ball to also belong to the unit ball. Given a collection of points c := {c 1 , . . . , c k } and a point x ∈ B, let f c (x) := d(x, c(g, )) 2 . Then,</p><formula xml:id="formula_17">f c (x) = x 2 + 4 min(Φ(x) • c1 , . . . , Φ(x) • ck ).</formula><p>For any set of s samples x 1 , . . . , x s ,</p><formula xml:id="formula_18">sup fc∈G s i=1 f c (x i ) s -E P f c (x) ≤ s i=1 x i 2 s -E P x 2<label>(7)</label></formula><formula xml:id="formula_19">+ 4 sup fc∈G s i=1 min i Φ(x i ) • ci s -E P min i Φ(x) • ci . (<label>8</label></formula><formula xml:id="formula_20">)</formula><p>By Hoeffding's inequality,</p><formula xml:id="formula_21">P s i=1 x i 2 s -E P x 2 &gt; 4 &lt; 2e -( 1 8 )s 2 , (<label>9</label></formula><formula xml:id="formula_22">)</formula><p>which is less than δ 2 .</p><p>By Theorem 5, P sup</p><formula xml:id="formula_23">fc∈G s i=1 min i Φ(xi)•ci s -E P min i Φ(x) • ci &gt; 16 &lt; δ 2 .</formula><p>Therefore, P sup </p><formula xml:id="formula_24">fc∈G s i=1 fc(xi) s -E P f c (x) ≤ 2 ≥ 1 -δ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Bounding the Fat-Shattering dimension using random projections</head><p>The core of the uniform bounds in Theorems 1 and 2 is the following uniform bound on the minimum of k linear functions on a ball in R m . Theorem 5. Let F be the set of all functions</p><formula xml:id="formula_25">f from B := {x ∈ R m : x ≤ 1} to R, such that for some k vectors v 1 , . . . , v k ∈ B, f (x) := min i (v i • x). Independent of m, if s ≥ C k 2 min 1 2 log 4 k , k + 1 2 log 1 δ , then P sup F ∈F s i=1 F (x i ) s -E P F (x) &lt; &gt; 1 -δ.<label>(10)</label></formula><p>It has been open since 1997 <ref type="bibr" target="#b2">[3]</ref>, whether the known lower bound of C k 2 + 1 2 log 1 δ on the sample complexity s is tight. Theorem 5 in <ref type="bibr" target="#b13">[14]</ref>, uses Rademacher complexities to obtain an upper bound of</p><formula xml:id="formula_26">C k 2 2 + 1 2 log 1 δ .<label>(11)</label></formula><p>(The scenarios in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref> are that of k-means, but the argument in Theorem 4 reduces k-means to our setting.) Theorem 5 improves this to</p><formula xml:id="formula_27">C k 2 min 1 2 log 4 k , k + 1 2 log 1 δ<label>(12)</label></formula><p>by putting together <ref type="bibr" target="#b10">(11)</ref> with a bound of</p><formula xml:id="formula_28">C k 4 log 4 k + 1 2 log 1 δ (<label>13</label></formula><formula xml:id="formula_29">)</formula><p>obtained using the Fat-Shattering dimension. Due to constraints on space, the details of the proof of Theorem 5 will appear in the journal version, but the essential ideas are summarized here.</p><p>Let u := fat F ( 24 ) and x 1 , . . . , x u be a set of vectors that is γ-shattered by F . We would like to use VC theory to bound u, but doing so directly leads to a linear dependence on the ambient dimension m. In order to circumvent this difficulty, for g := C log(u+k)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>, we consider a g-dimensional random linear subspace and the image under an appropriately scaled orthogonal projection R of the points x 1 , . . . , x u onto it. We show that the expected value of the γ 2 -shatter coefficient of {Rx 1 , . . . , Rx u } is at least 2 u-1 using the Johnson-Lindenstrauss Lemma <ref type="bibr" target="#b10">[11]</ref> and the fact that {x 1 , . . . , x u } is γ-shattered. Using Vapnik-Chervonenkis theory and the Sauer-Shelah Lemma, we then show that γ 2 -shatter coefficient cannot be more than u k(g+2) . This implies that 2 u-1 ≤ u k(g+2) , allowing us to conclude that fat F ( 24 ) ≤ Ck 2 log 2 k . By a well-known theorem of <ref type="bibr" target="#b0">[1]</ref>, a bound of Ck 2 log 2 k on fat F ( 24 ) implies the bound in (13) on the sample complexity, which implies Theorem 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Minimax lower bounds on the sample complexity</head><p>Let K be a universal constant whose value will be fixed throughout this section. In this section, we will state lower bounds on the number of samples needed for the minimax decision rule for learning from high dimensional data, with high probability, a manifold with a squared loss that is within of the optimal. We will construct a carefully chosen prior on the space of probability distributions and use an argument that can either be viewed as an application of the probabilistic method or of the fact that the Minimax risk is at least the risk of a Bayes optimal manifold computed with respect to this prior. Let U be a K 2d k-dimensional vector space containing the origin, spanned by the basis {e 1 , . . . , e K 2d k } and S be the surface of the ball of radius 1 in R m . We assume that m be greater or equal to K 2d k +d. Let W be the d-dimensional vector space spanned by {e K 2d k+1 , . . . , e K 2d k+d }. Let S 1 , . . . , S K 2d k denote spheres, such that for each i, S i := S ∩ ( √ 1 -τ 2 e i + W ), where x + W is the translation of W by x. Note that each S i has radius τ . Let = K 2d k K d k and {M 1 , . . . , M } consist of all K d k-element subsets of {S 1 , . . . , S K 2d k }. Let ω d be the volume of the unit ball in R d . The following theorem shows that no algorithm can produce a nearly optimal manifold with high probability unless it uses a number of samples that depends linearly on volume, exponentially on intrinsic dimension and polynomially on the curvature.</p><formula xml:id="formula_30">Theorem 6. Let F be equal to either G e (d, V, τ ) or G i (d, V, 1 τ 2 , πτ ). Let k = V dω d (K 5 4 τ ) d .</formula><p>Let A be an arbitrary algorithm that takes as input a set of data points x = {x 1 , . . . , x k } and outputs a manifold</p><formula xml:id="formula_31">M A (x) in F. If + 2δ &lt; 1 3 1 2 √ 2 -τ 2 then, inf P P L(M A (x), P) -inf M∈F L(M, P) &lt; &lt; 1 -δ,</formula><p>where P ranges over all distributions supported on B and x 1 , . . . , x k are i.i.d draws from P.</p><p>Proof. Observe from Lemma ?? and Theorem 3 that F is a class of a manifolds such that each manifold in F is contained in the union of K 3d 2 k m-dimensional balls of radius τ , and {M 1 , . . . , M } ⊆ F. (The reason why we have K 3d 2 rather than K 5d 4 as in the statement of the theorem is that the parameters of G i (d, V, τ ) are intrinsic, and to transfer to the extrinsic setting of the last sentence, one needs some leeway.) Let P 1 , . . . , P be probability distributions that are uniform on {M 1 , . . . , M } with respect to the induced Riemannian measure. Suppose A is an algorithm that takes as input a set of data points x = {x 1 , . . . , x t } and outputs a manifold M A (x). Let r be chosen uniformly at random from {1, . . . , }. Then,</p><formula xml:id="formula_32">inf P P L(M A (x), P) -inf M∈F L(M, P) &lt; ≤ E Pr P x L(M A (x), P r ) -inf M∈F L(M, P r ) &lt; = E x P Pr L(M A (x), P r ) -inf M∈F L(M, P r ) &lt; x = E x P Pr L(M A (x), P r ) &lt; x .</formula><p>We first prove a lower bound on inf</p><formula xml:id="formula_33">x E r [L(M A (x), P r )|x].</formula><p>We see that</p><formula xml:id="formula_34">E r L(M A (x), P r ) x = E r,x k+1 d(M A (x), x k+1 ) 2 x .<label>(14)</label></formula><p>Conditioned on x, the probability of the event (say E dif ) that x k+1 does not belong to the same sphere as one of the x 1 , . . . , x k is at least 1 2 . Conditioned on E dif and x 1 , . . . , x k , the probability that x k+1 lies on a given sphere S j is equal to 0 if one of x 1 , . . . , x k lies on S j and . However, it is easy to check that for any dimension m, the cardinality of the set S y of all S i that have a nonempty intersection with the balls of radius 1 2 √ 2 centered around y 1 , . . . , y</p><formula xml:id="formula_35">K 3d 2 k , is at most K 3d 2 k. Therefore, P d(M A (x), x k+1 ) ≥ 1 2 √ 2 -τ x is at least P d({y 1 , . . . , y K 3d 2 k }, x k+1 ) ≥ 1 2 √ 2 x ≥ P [E dif ] P [x k+1 ∈ S y |E dif ] ≥ 1 2 K 2d k -k -K 3d 2 k K 2d k -k ≥ 1 3 . Therefore, E r,x k+1 d(M A (x), x k+1 ) 2 x ≥ 1 3 1 2 √ 2 -τ 2</formula><p>. Finally, we observe that it is not possible for E x P Pr L(M A (x), P r ) &lt; x to be more than 1 -δ if inf x P Pr L(M A (x), P r ) x &gt; + 2δ, because L(M A (x), P r ) is bounded above by 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Algorithmic implications 8.1 k-means</head><p>Applying Theorem 4 to the case when P is a distribution supported equally on n specific points (that are part of an input) in a unit ball of R m , we see that in order to obtain an additive approximation for the k-means problem with probability 1 -δ, it suffices to sample</p><formula xml:id="formula_36">s ≥ C k 2 log 4 k 2 , k + 1 2 log 1 δ</formula><p>points uniformly at random (which would have a cost of O(s log n) if the cost of one random bit is O(1)) and exhaustively solve k-means on the resulting subset. Supposing that a dot product between two vectors x i , x j can be computed using m operations, the total cost of sampling and then exhaustively solving k-means on the sample is O( msk s log n). In contrast, if one asks for a multiplicative (1 + ) approximation, the best running time known depends linearly on n <ref type="bibr" target="#b12">[13]</ref>. If P is an unknown probability distribution, the above algorithm improves upon the best results in a natural statistical framework for clustering <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Fitting piecewise linear curves</head><p>In this subsection, we illustrate the algorithmic utility of the uniform bound in Theorem 4 by obtaining an algorithm for fitting a curve of length no more than L, to data drawn from an unknown probability distribution P supported in B, whose sample complexity is independent of the ambient dimension. This curve, with probability 1 -δ, achieves a mean squared error of less than more than the optimum. The proof of its correctness and analysis of its run-time have been deferred to the journal version. The algorithm is as follows:</p><p>1. Let k := L and s ≥ C k</p><formula xml:id="formula_37">2 log 4 ( k ) 2 , k + 1 2 log 1 δ</formula><p>. Sample points x 1 , . . . , x s i.i.d from P for s =, and set J := span({x i } s i=1 ). 2. For every permutation σ of [s], minimize the convex objective function n i=1 d(x σ(i) , y i ) 2 over the convex set of all s-tuples of points (y 1 , . . . , y s ) in J, such that s-1 i=1 y i+1y i ≤ L.</p><p>3. If the minimum over all (y 1 , . . . , y s ) (and σ) is achieved for (z 1 , . . . , z s ), output the curve obtained by joining z i to z i+1 for each i by a straight line segment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Fitting a torus to data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Random projections are likely to preserve linear separations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 K 2</head><label>12</label><figDesc>k-k otherwise, where k ≤ k is the number of spheres in {S i } which contain at least one point among x 1 , . . . , x k . By construction, A(x 1 , . . . , x k ) can be covered by K 3d 2 k balls of radius τ ; let their centers be y 1 , . . . , y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, whether the known lower bound of O( k 2 +</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>log 1 δ 2 ) for</cell></row><row><cell cols="5">the sample complexity of Empirical Risk minimization on k-means applied to data in a unit ball</cell></row><row><cell cols="5">of arbitrary dimension is tight. Here is the desired bound on the error and δ is a bound on the</cell></row><row><cell cols="3">probability of failure. The best currently known upper bound is O( k 2 2 +</cell><cell cols="2">log 1 δ 2 ) and is based on</cell></row><row><cell>Rademacher complexities. We improve this bound to O k 2</cell><cell>min k,</cell><cell>log 4 k 2</cell><cell>+</cell><cell>log 1 δ 2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>We are grateful to Stephen Boyd for several helpful conversations.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> *   <p>Research supported by grant CCF-0836720</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scale-sensitive dimensions, uniform convergence, and learnability</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Noga Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="615" to="631" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An algorithmic theory of learning: Robust concepts and random projection</title>
		<author>
			<persName><forename type="first">Rosa</forename><forename type="middle">I</forename><surname>Arriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="616" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The minimax distortion redundancy in empirical quantizer design</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A framework for statistical clustering with constant time approximation algorithms for k-median and k-means clustering</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="243" to="257" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Topology and data</title>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="255" to="308" />
			<date type="published" when="2009-01">January 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning mixtures of gaussians</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="634" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Grimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5591" to="5596" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Tubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Principal curves</title>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">J</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="502" to="516" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName><forename type="first">William</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joram</forename><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="419" to="441" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning and design of principal curves</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Krzyzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamás</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="281" to="297" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple linear time (1+ )-approximation algorithm for k-means clustering in any dimensions</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yogish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="454" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalization bounds for k-dimensional coding schemes in hilbert spaces</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALT</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="79" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the sample complexity of learning smooth cuts on a manifold</title>
		<author>
			<persName><forename type="first">H</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Annual Conference on Learning Theory (COLT)</title>
		<meeting>of the 22nd Annual Conference on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding the homology of submanifolds with high confidence from random samples</title>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Smale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="419" to="441" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SCIENCE</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularized principal manifolds</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="209" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Global Geometric Framework for Nonlinear Dimensionality Reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Computing persistent homology</title>
		<author>
			<persName><forename type="first">Afra</forename><surname>Zomorodian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="274" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
