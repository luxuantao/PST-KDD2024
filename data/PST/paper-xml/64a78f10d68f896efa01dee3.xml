<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chiplet Cloud: Building AI Supercomputers for Serving Large Generative Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-07-05">5 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huwan</forename><surname>Peng</surname></persName>
							<email>hwpeng@uw.edu</email>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><surname>Davidson</surname></persName>
							<email>stdavids@uw.edu</email>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Taylor</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Shuaiwen Leon Song</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Chiplet Cloud: Building AI Supercomputers for Serving Large Generative Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-05">5 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2307.02666v1[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) such as OpenAI's ChatGPT and GPT-4 have demonstrated the unprecedented capabilities of autoregressive generation in multiple AI tasks, triggering disruptive technology innovations around the world. However, with the increase of model size and context length, and the slowdown of Moore's Law, it becomes increasingly challenging to serve these large models efficiently on existing cloud hardware platforms that are powered by TPUs and GPUs. Hardware inefficiency has become a major factor limiting the democratization of LLMs.</p><p>In this paper, we propose Chiplet Cloud, a chiplet-based ASIC AIsupercomputer architecture that optimizes total cost of ownership (TCO) per generated token for serving large generative language models to reduce the overall cost to deploy and run these applications in the real world. The Chiplet Cloud architecture utilizes a unique approach to scale-up cloud computing by leveraging thousands of replicated chiplet accelerator modules to collaboratively perform token generation at an unprecedented TCO per token. A key architectural feature to achieve this is the ability to fit all model parameters inside the on-chip SRAMs of the chiplets to eliminate bandwidth limitations. Doing so is non-trivial as the amount of memory required is very large and growing for modern LLMs. This has led to larger chips with worse die yield and server level thermal dissipation thus increasing the total cost of the system. By using chiplets, we can moderate the die size to improve the system cost while leveraging software mappings to exploit parallelism found within the computation to overcome the potential data communication overhead.</p><p>To explore the software-hardware co-design space and perform software mapping -aware Chiplet Cloud optimizations across the architectural design space, we propose a comprehensive design methodology that not only accurately explores a spectrum of major design trade-offs in the joint space of hardware and software, but also generates a detailed performance-cost analysis on all valid design points and then outputs the Pareto frontier. We design and evaluate Chiplet Cloud on four popular LLMs on the market representing a range of model sizes. Compared to A100 GPU clouds and TPUv4 clouds, our results show that Chiplet Cloud can achieve up to 94? and 15? improvement in TCO/Token respectively. This significantly reduces the cost for realistically serving modern LLMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few months, a new generative Large Language Model (LLM) called ChatGPT <ref type="bibr" target="#b24">[24]</ref> has gained significant attention around </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PaLM 540B</head><p>TPUv4 Ours TCO per 1K Tokens ($)</p><p>Token Generation Latency (ms) the world due to its unprecedented ability to perform a variety of natural language tasks. Compared with the previous language models, ChatGPT is much better at understanding user intent, generating human-like responses, and keeping multi-round conversations coherent with context. While the technology behind ChatGPT is impressive, it is the way it exposed LLMs in a way that was useful for the general population and introduced a new business model for these AI systems that has sent a shock wave of excitement throughout the technology industry. Since ChatGPT, we have already seen several announcements of LLM being integrated with other services such as web search, word processing, and programming IDEs <ref type="bibr" target="#b8">[9]</ref>. LLMs are currently driving a technology revolution at planet-scale, changing the way we interact with AI models on a daily basis. Much of the transformational increase in ML capabilities comes from the unprecedented scale of the LLMs being deployed. For example, ChatGPT is based on OpenAI's GPT-3.5, an updated version of GPT-3 <ref type="bibr" target="#b3">[4]</ref> and one of the most powerful large language models available at the time of deployment. The model is trained on 570GB of text data and has more than 175B parameters, requiring a huge amount of hardware resources for both training and inference. For instance, GPT-3 requires 23 days to train on 1536 A100 GPUs <ref type="bibr" target="#b22">[23]</ref>. Inference alone also requires a significant amount of hardware resources, e.g., serving GPT-3 typically requires 8 A100 GPUs simply to satisfy the minimum memory requirements.</p><p>Serving generative transformer-based large language models on commodity hardware, like GPUs, is already hitting a scalability wall. State-of-the-art GPT-3 throughput on GPU is 18 tokens/sec per A100 <ref type="bibr" target="#b0">[1]</ref>. ChatGPT and the promise of integrating large-language models into various existing technologies (e.g. web-search) puts into question the scalability and profitability of large-language models. For example, Google Searching processes over 99,000 queries <ref type="bibr" target="#b20">[21]</ref> per second. If GPT-3 is embedded in every query, and assuming each query generates 500 tokens, Google needs 340,750 NVIDIA DGX servers (2,726,000 A100 GPUs) to keep up. The cost of these GPUs exceeds $40 billion in capital expenditure alone. Energy consumption will also be huge. Assuming a 50% utilization, the average power would be over 1 Gigawatt, which is enough energy to power 750,000 homes. The CO2 emissions of producing 1 Gigawatt are equivalent to the annual emissions of more than 200,000 cars. Hardware inefficiencies will significantly limit the impact of the large generative language models in the future. To address both the high capital expenditure and energy cost of running LLMs, we must design and build hardware systems that attain significantly better total-cost-of-ownership (TCO) per token served.</p><p>We propose Chiplet Cloud, a chiplet-based ASIC AI-supercomputer architecture for LLMs which aims to reduce the TCO per generated token. Figure <ref type="figure" target="#fig_1">1</ref> shows the TCO/Token and latency Pareto frontier of Chiplet Cloud for GPT-3 <ref type="bibr" target="#b3">[4]</ref> and PaLM 540B <ref type="bibr" target="#b4">[5]</ref>. Compared to A100 GPU <ref type="bibr" target="#b0">[1]</ref> and TPUv4 <ref type="bibr" target="#b25">[25]</ref> clouds, our design achieves up to 94.4? and 15.2? TCO/Token improvement respectively. The cost analysis are based on Lambda GPU Cloud <ref type="bibr" target="#b15">[16]</ref> and Google Cloud <ref type="bibr" target="#b5">[6]</ref>.</p><p>Chiplet Cloud employs a unique scale-up architecture to design a full cloud-scale system for running large generative language models at unrivaled TCO per performance that will drive and enable future LLM applications to run at planet-scale. To achieve this, we aggressively customize the architecture of Chiplet Cloud for the targeted LLMs. Driven by the design trade-offs between cost and performance, the architecture of Chiplet Cloud stores all model parameters and KV values in on-chip SRAM memory (Sec.3.2.1). On-chip memories such as SRAM have better read latency and read/write energy than external memories such as DDR or HBM but require more silicon per bit. We show this design choice wins in the competition of TCO per performance for serving large generative language models but requires careful consideration with respect to the chiplet die size, chiplet memory capacity and total number of chiplets to balance the fabrication cost and model performance (Sec.3.2.2) We observe that the inter-chiplet communication issues can be effectively mitigated through proper software-hardware codesign leveraging mapping strategies such as tensor and pipeline model parallelism <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b31">31]</ref>.</p><p>To explore the massive hardware-software co-design space of Chiplet Cloud and find TCO per performance optimal system mapping strategies, we propose a two-phase design-search methodology for hardware exploration and software evaluation. The hardware exploration phase (Sec.4.1) conducts a bottom-up design space exploration of Chiplet Cloud hardware architecture from a flexible accelerator architecture up to a 1U rack mounted server architecture taking power budget, floorplan, and thermal limits into account. The software evaluation phase (Sec.4.2) then performs a detailed performance and TCO analysis of the server designs given a specific workload while simultaneously searching for software mapping strategies that complements the server architecture. While software mapping strategies for LLMs are now considered standard techniques for improving performance on existing hardware platforms, our design methodology flips the order and allows us to explore mapping strategies across all possible Chiplet Cloud hardware platforms to further optimize the TCO per performance capabilities of Chiplet Cloud. In summary, this paper makes the following contributions:</p><p>? We conduct a thorough study on the current hardware limitations for serving generative LLMs and motivate the need of building ASIC supercomputers (Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Democratizing LLMs Through Specialization</head><p>As large generative language models are being widely used in more applications, we are heading towards a scalability wall due to inefficiencies of generalized commodity hardware platforms such as GPUs. This will lead to the cost of running LLMs to increase over time as these workloads grow in scale and complexity leading us to a world where very powerful AI systems are prohibitively expensive to use for the layman. In order to democratize this disruptive technology, specialized hardware will be the best way to go. The rest of this section is dedicated to a brief background on large generative language models in the context of what makes them difficult to run on modern hardware platforms and how moving to ASIC supercomputers is more financially feasible despite the daunting upfront cost of developing and designing such a machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Large Generative Language Models</head><p>The architecture of a generative language model is shown in the top of Figure <ref type="figure" target="#fig_2">2</ref>. The architecture of a generative language model is built around the transformer decode block, with each block defining a layer of the model <ref type="bibr" target="#b37">[37]</ref>. Within the decoder layer is a multi-head self attention mechanism followed by a 2-layer feed-forward network. In Figure <ref type="figure" target="#fig_2">2</ref> we show the number of operations for each step of the decode block. For modern large language models, fully connected (FC) layers dominate the runtime of the decode block since the model dimension ? is much larger than the input length ? and the context length ? ??? , thus ? (?? 2 ) &gt;&gt; ? (? 2 ??? ?). For example, more than 99% of MAC operations in GPT-3 are spent on just the FC layers.</p><p>Generative language models use autoregressive inference to generate tokens, as shown at the bottom of Figure <ref type="figure" target="#fig_2">2</ref>. The first pass through the model takes takes the input sequence as a whole and generates a single output token. That output token is then used as the input for the next pass, continuing until a specific "end of text" token is generated. This dependency between generated tokens poses a challenge for massively parallel LLM inference, limiting system utilization. A common technique used to reduce the amount of computation required for every token generated after the initial input sequence is known as KV caching where intermediate results of the multi-headed self-attention mechanism from previous input tokens are cached and don't need to be recomputed. The maximum size of the KV cache is dependant on the context length of the model and batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A High-Performance and More Profitable</head><p>Hardware Solution: ASIC</p><p>In order to run LLMs at scale and achieve high performance and high energy efficiency, we propose building ASIC supercomputers specifically for LLMs. ASICs are known to have the potential to deliver better performance and energy efficiency than CPUs and GPUs, since they are optimized for specific tasks. One major factor limiting the deployment of ASICs is non-recurring engineering (NRE) costs <ref type="bibr" target="#b13">[14]</ref>. The barrier for overcoming NRE is primarily about the opportunity cost of running the workload on the current hardware offerings. The difference in TCO between running a workload on an ASIC supercomputer vs the current available hardware platform determines the break even point for NRE, where the NRE cost directly equals the savings from using an ASIC supercomputer. The current cost of running workloads like web search with integrated LLMs is so massive that it not only justifies the cost of creating ASIC super computers but going even further as to co-optimize those super computers for specifics LLMs for additional improvement in the TCO per token. The NRE of ASIC supercomputers includes silicon mask cost, CAD tools, IP licensing, flip-chip BGA packing, server designs, and labor. We extend the NRE model from Moonwalk <ref type="bibr" target="#b13">[14]</ref> to use a 7nm technology node and estimate the NRE of a 7nm ASIC accelerator for large language models to be approximately $35M. In Figure <ref type="figure">3</ref>, we compare the cost to generate 1K tokens on GPT-3 on modern GPUs clouds vs ASIC supercomputers. The GPU cost is based on the state-of-the-art throughput from DeepSpeed-Inference <ref type="bibr" target="#b0">[1]</ref>) and the best available rental price of $1.1/GPU/hour from Lambda <ref type="bibr" target="#b15">[16]</ref>. GPT-3 Tokens Generated per Second Cost per 1K Tokens ($)</p><p>Break-even Bing Google Search ChatGPT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPU (TCO) ASIC (NRE+TCO) Improvement</head><p>Figure <ref type="figure">3</ref>: For many real-world applications, designing ASIC supercomputers for LLMs will be more cost-effective than using GPUs, saving up to 2 orders of magnitude in cost per token.</p><p>The ASIC cost combines the NRE and TCO on a lifetime of 1.5 year, while the performance is based on our TCO optimized Chiplet Cloud. The shaded region shows the saving per 1K tokens switching from GPU to ASIC, with the break-even point for GPT-3 being approximately 46,000 tokens/sec. This shows that the NRE cost of ASIC supercomputers is justifiable for modern workloads and thus customized hardware still remains the best solution to democratize LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Chiplet Cloud: A TCO-Optimized ASIC Architecture for LLMs</head><p>This section introduces the architecture and main insights of Chiplet Cloud. Instead of just optimizing raw hardware performance, more companies start to design accelerators for better TCO per performance <ref type="bibr" target="#b10">[11]</ref>. TCO of an accelerator consists of the capital expenditures (CapEx, mainly from chip fabrication) and the operation expenditures (OpEx, mainly from power consumption); thus, ??? = ????? + ?? ? ? ? ????. We list some of the main challenges of designing ASICs for large generative language models, and give our solutions on how to optimize the TCO per performance. We believe that an architecture that enables these solutions, i.e. Chiplet Cloud, is what is demanded for a large ASIC LLM serving system. Operational Intensity (FLOP/Byte) context length of 2K, every input sequence needs 2 GB of KV cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Challenges</head><p>With the batch size of 256, the KV cache further grows to 512 GB while the total model size is 350 GB. Figure <ref type="figure" target="#fig_5">4</ref> shows the roofline of GPT-2 on a V100 GPU, where most kernels are bounded by the memory bandwidth. To fully utilize the 112 TFLOPS computing power on V100, we need a total bandwidth of 85000 GB/s, which is about 100? of HBM bandwidth. Reducing parameter and KV access latency and power is critical for better TCO/Token in LLM inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.2</head><p>Chip Costs Dominate TCO Due to the autoregressive feature of the LLMs, the next token depends on previous generated tokens, which greatly limits the hardware utilization. The best hardware utilization on the state-of-the-art implementation is around 50% <ref type="bibr" target="#b0">[1]</ref> on GPUs and 40% on TPUs (during the decoding phase) <ref type="bibr" target="#b25">[25]</ref>. Note that these are achieved with a very large batch size (e.g., 1024), the utilization can be as low as 1% when batch sizes are small (e.g., 4) <ref type="bibr" target="#b25">[25]</ref>, which are the most common cases for real-world LLM inference. Another issue with the current systems is that chips used (e.g., A100 and TPUv4) are massive and close to the wafer reticle limit of around 800 mm 2 , which poses a huge challenge to control the fabrication cost. Under such low utilization and high chip fabrication cost, the capital expenditures will account for a significant portion of the TCO. According to our evaluation, at 50% utilization, an A100 GPU purchased at manufacturer's suggested retail price has a TCO of 97.7% on CapEx. Even if people tapeout their own GPUs, the CapEx percentage can still be as high as 58.7%.</p><p>Reducing chip costs will be the key to lower the TCO per generated token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Key Architectural Solutions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Design Tradeoffs Driven by TCO per Performance</head><p>SRAM has a much higher bandwidth and much lower access energy than external memory such as HBM, but has lower density.</p><p>To address the memory bandwidth bottleneck, we want to argue that from a better TCO per performance perspective, buffering all model parameters and intermediate data (such as K, V) in on-chip memory such as SRAM is a preferred design option for our case.  Consider a language model accelerator with die area (????) and average power (????? ), that provides a total memory bandwidth of ?? . The accelerator's TCO can be approximately modeled as ? ? ???? + ? ? ????? , where ? and ? are constant coefficients. The chip cost actually grows superlinearly with the area. We use a linear model here for simplicity, which will not affect our subsequent analysis. As discussed earlier, since LLM inference is usually memory bandwidth bounded, throughput (token/sec) will be proportional to the bandwidth ?? . As a result, we have ??? ? ???? ? ? ???? ?? + ? ????? ?? . To optimize the TCO/Token, we want a smaller ????/?? and ????? /?? . Assuming that other modules in the chip remain unchanged, area, read energy, and bandwidth of the the parameter memory will be they key factors that affect this figure of merit. We list typical memory blocks of DDR4, HBM2e and SRAM in the table in Fig. <ref type="figure" target="#fig_6">5</ref>, and plot the area per total bandwidth and the read energy per total bandwidth when we use these blocks to store model parameters. Compared with DDR4 and HBM2e, on-chip SRAM has much smaller ????/?? ( ?? 2 ??/? ) and ????? /?? ( ? ? /??? ??/? ). Although the same memory technology can have blocks of different sizes, which affects the bandwidth and read energy, the trend shown in Fig. <ref type="figure" target="#fig_6">5</ref> remains the same. For example, SRAM has an order-ofmagnitude better area per bandwidth and read energy per bandwidth than DRAM. Thus, although buffering all parameters on-chip requires more silicon, it will likely to reduce the TCO per generated token.</p><p>Recently, there has been an industry trend to deploy more onchip memory on deep learning accelerators to reduce the excessive off-chip memory access. Microsoft's Brainwave <ref type="bibr" target="#b7">[8]</ref> pins DNN model weights in distributed on-chip SRAM. Google's TPUv4i <ref type="bibr" target="#b10">[11]</ref> contains 144 MB SRAM, and GraphCore's IPU 2 <ref type="bibr" target="#b14">[15]</ref> has 896 MB SRAM, Adding more on-chip memory to provide higher bandwidth is easier, cheaper, and lower power than doubling the HBM bandwidth, which benefits the TCO per performance design orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Design Choice: Chiplet for Reducing Fabrication Cost.</head><p>An extreme case of adding on-chip memory is to go wafer-scale. Cerebaras WSE-2 <ref type="bibr" target="#b33">[33]</ref>   <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b31">31]</ref>) can effectively reduce the inter-node communication. Additionally, by storing all parameters and keeping the KV-cache in SRAM on each chiplet, the NUMA issues will be significantly avoided since the access time to local memory structures becomes generally much more uniform. To find the optimal mapping strategy for Chiplet Cloud, a software-hardware co-design methodology is essential (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Our Proposal: Chiplet Cloud Architecture</head><p>Combing the two solutions above, we propose a chiplet-based cloudscale system design for LLM inference, called Chiplet Cloud. Our general Chiplet Cloud architecture is shown in Figure <ref type="figure">6</ref>, which includes the high-level abstract architecture of different levels, from process unit (PU) to chiplet module,server,and cloud.</p><p>Starting from the bottom left, we start with the heart of Chiplet Cloud, a language model accelerator chiplet module that scales to data center inference. The module adopts a frequently-used PUbased architecture, with additional attention, nonlinear function, and normalization units. The Chiplet Cloud PU includes a local memory for holding parameters, multiple parallel MACs (multiplyaccumulate) units, and an accumulation buffer. Model parameters and KVs are stored in a large on-chip global memory, as shown in Figure <ref type="figure">6</ref>. No external memory like DRAM is required. The input buffer reads activation from another chiplet or the host through a chip-to-chip (C2C) router and broadcasts it to all PUs. Meanwhile, PUs load parameters into local memories. The PUs employ an output stationary dataflow pattern <ref type="bibr" target="#b6">[7]</ref> with spatial activation reuse and temporal partial sum accumulation reuse. This dataflow was selected as FC layers are the most compute intensive operation in the transformer decode layer which cannot exploit any weight reuse unless the batch size is greater than 1. The design maximizes parallelization and eliminates the need for inter-PU communication, reducing on-chip network overhead. The output of a PU either flows into the attention unit and then goes back to the PU array, or inputs into some nonlinear units like GeLu and normalization and is then sent outside the chip.</p><p>Figure <ref type="figure">6</ref> also shows one important feature of our design-a single chiplet module is a package, and multiple chiplets are connected through the board. Conventionally, chiplets are integrated into a single package via a package-level solution such as a silicon interposer (e.g., <ref type="bibr" target="#b12">[13]</ref>) or organic substrate (such as Simba <ref type="bibr" target="#b30">[30]</ref> and AMD EPYC processor <ref type="bibr" target="#b21">[22]</ref>) for better inter-chiplet communication. However, the package-level integration brings design challenges and overhead. Silicon interposers have a limited signal length. The organic substrate solution brings design challenges in package-level routing and power delivery <ref type="bibr" target="#b21">[22]</ref>. Instead, we use board-level chiplets in our Chiplet Cloud design according to the communication requirements. With an optimized workload partitioning and mapping strategy, we can reduce data traffic across chiplets. Chiplets have individual packages and are connected together via a 2D torus on-PCB network, which is able to accommodate the potentially different mapping strategies for different models and still reuse the same server. Compared to conventional package-level chiplet, the board-level chiplet architecture eliminates cost of advanced packaging.</p><p>Each Chiplet Cloud server contains a printed circuit board (PCB) with multiple chiplets, a controller and an off-PCB interface. The controller, which can be an FPGA or a microcontroller, dispatches remote procedure calls (RPCs) from off-PCB interface to all chiplets via the on-PCB 2D torus network. In the analysis of this paper, we model the use of ground-referenced signaling (GRS) links <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b36">36]</ref> for inter-chiplet communication. Each chiplet module connects with adjacent chiplets over a full-duplex data link capable of 25 GB/s with up to 80 mm reach over the PCB. Other candidate chip-to-chip interfaces can be high-speed PCI-e, which has been widely used as interconnects for many deep learning chips <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32]</ref>, or customdesigned links such as Google TPU's Inter-Core Interconnect <ref type="bibr" target="#b11">[12]</ref> and Graphcore's IPU-links <ref type="bibr" target="#b14">[15]</ref>. Off-PCB interfaces could be 10/100 Gigbit Ethernet or InfiniBand, enabling communication between adjacent servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Design Space Discussion</head><p>The Design space of our Chiplet Cloud includes multiple aspects from cross-stacks that affect the end-to-end performance. Some aspects include (1) Chiplet Module Size: small chips benefit from higher yields while incurring more per-chip overhead; (2) Per Chiplet Memory Size: more memory on chips means few chips required but few FLOPS per chip; (3) Silicon Per Server: more silicon per server reduces the communication between servers, but it is limited by the chiplet size, power and the cooling system; and (4) Software Mapping: the tradeoff between tensor model and pipeline model parallelism affects utilization and interconnect data communication. Since all of these aspects are highly coupled, a comprehensive design methodology is critical to optimize the end-to-end performance and TCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design Methodology: Chiplet Cloud</head><p>A key challenge for optimizing the TCO of large scale-out systems is balancing the cost per operation and the watts per operation. To address this challenge, we propose a design methodology that can accurately explore the large design space of the Chiplet Cloud architectures and perform a cost-performance analysis of serving large generative language model. This design methodology, shown in Figure <ref type="figure" target="#fig_7">7</ref>, is a two phase software-hardware co-design methodology: first a hardware exploration phase followed by a software evaluation phase. The methodology uses a brute-force exploration, with feasibility checks and pruning throughout to maintain a reasonable runtime performance.</p><p>This methodology is unlike traditional software-hardware codesign methodologies which typically start with extracting software characteristics and then derive hardware parameterizations based on preconceived assumptions about best practices. The design space for Chiplet Cloud is very large with many hardware parameterizations that need a careful balance to optimize for TCO/token. This makes traditional software-hardware co-design ineffective at finding truly optimal hardware design points. To avoid the need to bake-in these assumptions, our approach starts with a feasibility constrained design space exploration to generate every possible hardware design and then purge designs with power budgets, thermal outputs, or physical layouts that cannot be realized. Only then do we start evaluating hardware design points against specific generative LLMs to find a co-optimized Chiplet Cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Phase 1: Hardware Exploration</head><p>The hardware exploration phase of the Chiplet Cloud design methodology (as shown in Figure <ref type="figure" target="#fig_7">7(a)</ref>) is a bottom-up, LLM agnostic, design space exploration resulting in thousands of varying Chiplet Cloud server designs. The exploration starts with a flexible architectural description model of the Chiplet Cloud accelerator module described in more detail in section 3.3. This flexible architecture allows users to scale the on-chip memory capacity and the number of processing units within the constraints determined by microarchitectural limitations resulting in candidate architectures with varying peak power (TDP), peak performance (TFLOPs), and on-chip memory capacity (MBs).</p><p>Candidate architectures are then evaluated against a set of server level constraints including the max power budget and physical layout constraints. These constraints are based on the limitations of building a traditional 1U 19 inch rack mount server as found in ASIC Cloud <ref type="bibr" target="#b18">[19]</ref>. The floorplan of the Chiplet Cloud server is derived from the optimal server floorplan found leveraging computation fluid dynamic simulations of airflow cooling of a 1U server. The floorplan is 8 lanes of silicon chips with physical ducting separating each lane to reduce turbulence.</p><p>Each candidate Chiplet Cloud accelerator is evaluated with these constraints to determine the best die size before undergoing thermal power analysis. Thermal analysis includes determining the total power dissipation per lane that can be achieved. A key factor to consider is the size of the chiplet die as this impacts the heatsink selection and thermal distribution throughout the server. Smaller chiplets allow for greater cooling efficiency within a lane by physically distributing the heat more evenly thus reducing hotspots.</p><p>The constraint analysis for Chiplet Cloud server designs as well as the overall TCO estimation that is performed during the second phase of the design methodology are heavily reliant on accurate power, area and performance estimation modeling of the accelerator module.</p><p>Area Estimation. The area estimation model for the Chiplet Cloud accelerator is derived from actual 7nm ASIC tapeouts found in the literature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, giving us a model for the ?????/?? 2 and ? ????/?? 2 for the on-chip memory and PU array respectively. The silicon area is likely dominated by compute and memory device area. However, for a flexible accelerator architecture, auxiliary components of the chip can start to have a significant area overhead when the amount of memory and compute is small. Therefore, we also include the area overhead for the attention unit, activation unit, IO, on-chip networks, and control logic in the model.</p><p>Power Estimation. The power estimation model is also derived from the actual 7nm ASIC tapeout numbers found in the literature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. This gives us the TDP for running these 7nm ASICs at 100% utilization. We normalize this value to an W/FLOP at 100% utilization and use that value in combination with the parameterized compute power of the flexible architecture to determine a 100% utilization TDP estimation for our Chiplet Cloud accelerator. Combined with the area model, we limit the power density to be no more than 1? /?? 2 , however during server design optimization we will further refine the peak power density limitations based on the full-server thermal analysis.</p><p>Performance Modeling. The performance model for the Chiplet Cloud accelerator used during hardware exploration is the peakperformance of the chiplet and is based on the number of PUs which is determined by the compute parameterization of the architecture. Every PU in the chiplet has a peak-performance of 2 operations per cycle (multiply-accumulate) and is estimated to run at 1GHz giving us 2 GFLOPs/s/PU. During the software evaluation phase of the design methodology, we will further refine the performance as a function of the software kernel, microarchitectural utilization and IO communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Phase 2: Software Evaluation</head><p>The second phase of the design methodology models the execution time of specific workloads across the hardware design points and searches for optimal Chiplet Cloud architectural configurations. The software evaluation flow is shown in Figure <ref type="figure" target="#fig_7">7(b)</ref>.</p><p>The first step is the software optimizer which takes the Chiplet Cloud server designs from phase 1 and a generative LLM workload and performs several optimizations including tensor parallelism and pipeline parallelism with microbatch tuning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">31]</ref>. The optimizer will first look at the hyperparameters of the LLM, such as the model dimension ? ????? , number of layers, context length, attention mechanism type (multi-head or multi-query), as well as expect batch size. Then it will decompose the full model into a collection of small kernels that can be mapped to the individual chiplets throughout the system. In cases where the model cannot fit into a single server, the server will be replicated to scale up the entire system until there  are enough resources to execute the application. This results in a system mapping which has the portion of the model that each chiplet in the whole system will be responsible for executing, based on the chosen tensor and pipeline parallelism optimizations. There also exists a chiplet memory profile and chiplet compute profile for the portion of the model that will be running on the individual chiplet, which will allow us to accurately model the end-to-end performance of the full system. The memory profile contains information about the required memory for weights, activations, and the KV cache, while the compute profile contains the size and operations that the chiplet will need to perform including reduction operations caused by tensor parallelism optimizations. The end-to-end performance model for the chiplet cloud system starts with understanding the compute latency inside each chiplet. This is based on the analytical analysis of the dataflow and compute latency of the mapped portion of the model as described by the compute profile. The compute profile defines which operations and the size of the operations that each chiplet will perform. We analyze the size and shape of these operations as they would be executed at the microarchitectural level to get a kernel compute latency and thus a kernel level utilization. This kernel level utilization is then used to scale the TDP of the chiplet to estimate the average kernel computational power.</p><p>Since the model has been split up across all of the chiplets, we must also model the data communication latency between the chiplets including all-reduce operations between collaborating chiplets that are working on operations that have been optimized with tensor parallelism. As mentioned in Section 3.3, each chiplet is equipped with a 25 GBps link to all the adjacent chiplets. However, the system mapping might have two chiplets that must communicate that exist on different servers. When this occurs, the performance of this link goes over a 10 Gbps Ethernet connection. This bandwidth reduction discourages many system mappings, particularly to those which all-reduce operations that must cross over server boundaries. These design points are generally removed during the Pareto optimization search given their large performance penalty.</p><p>Improved Software-Hardware Co-Design Model. The TCO model is a refined version of the model by Barroso et al <ref type="bibr" target="#b2">[3]</ref>, which includes both capital expenditures (CapEx) and operating expenses (OpEx) from the system as well as the datacenter hosting the system. The CapEx for our server includes the silicon die cost, package cost, PCB cost, power supply unit cost, DC/DC converter costs, heatsink cost, fan costs, Ethernet controller cost, and control processor cost. The OpEx is calculated based on the power consumption of the system. Based on the system TDP and utilization, the full system average power consumption is used to determine the power draw from the silicon dies of the Chiplet Cloud accelerators. Additionally, the power cost due to inefficiencies of on-server DC/DC converts and the server power supply unit is also taken into consideration as is the power draw of the control processor, Ethernet controller and server fans.</p><p>To estimate the die cost, we first calculate the number of fully patterned dies per wafer (DPW). This is the number of rectangular dies with the given die size dimensions that we can slice out of a traditional 300?? circular wafer. Cost per die is then calculated as: Where ???? ??? ?? is wafer price, ???? ???? is testing cost, and ? ??? is die yield. We use the classical negative binomial model for yield which is as follow:</p><formula xml:id="formula_0">???? ??? = ( ???? ??? ?? ??? + ???? ???? )/? ??? (1)</formula><formula xml:id="formula_1">? ??? = (1 + ?? 0 ? ) -?<label>(2)</label></formula><p>Where ? is die area, ? 0 is defect density and ? is cluster parameter. Since manufacturing yields drop with chip area, it makes economic sense to design smaller chips.</p><p>For each feasible server design that we found in the hardware exploration phase of the design methodology, we will profile the memory, compute and power for a given software optimization found from the original workload specification. With the memory profile, we can determine if we have enough storage for the software's per-chip kernel with respect to the parameters, activation and KV cache size. If we do not have enough, then we consider this server configuration a non-feasible point to run this specific workloadsoftware optimization combination. The number of servers is then selected to ensure that we have enough resources to fit the entire model given the per-chip memory profile. Additionally, compiling a network topography and assigning bandwidth numbers for all chiplet-to-chiplet communication links in the system depend on if the communication is on-server or inter-server. The network topography, multi-server architecture and chip computer profile are then all used to find the full end-to-end inference runtime of the workload. This performance along with the multi-server architecture and chip power profile are then fed into our TCO estimation engine where we can compute all TCO related metrics such as TCO/Token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalizing the Design Methodology</head><p>While this work is focused on trying to find cloud-scale architectures with best-in-class TCO/Token performance, the methodology of designing scale-up cloud systems is still applicable to existing ASIC architectures or architectures designed for programmable devices such as CGRAs or FPGAs. Given an appropriate power, performance and area estimation model for the accelerator module, this methodology is applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Case Studies</head><p>To evaluate our architecture and design methodology, we perform case studies of Chiplet Cloud on four language models at different scales, including GPT-2 <ref type="bibr" target="#b28">[28]</ref> and GPT-3 <ref type="bibr" target="#b3">[4]</ref> proposed by OpenAI, Turing NLG (T-NLG) <ref type="bibr" target="#b19">[20]</ref> proposed by Microsoft and NVIDIA, and PaLM <ref type="bibr" target="#b4">[5]</ref> proposed by Google. Dimensions of these models are shown in Table <ref type="table" target="#tab_5">1</ref>. Note that PaLM uses multiquery attention, where </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design Space Exploration</head><p>We did a thorough design exploration on each model. For each one, we explored 3 different context length scenarios: 256, 2048 and 8192. Normally, the longer the context length, the better the model performance, but it requires more memory and more computation.</p><p>For the analysis below, the context length is 2048 if not explicitly mentioned. We also explore the input batch sizes from 1 to 1024. In total we generate over 10M valid design points. Each design point combines the result from both hardware exploration and software evaluation, which includes hardware design (chip and server), software mapping (tensor parallelism size, pipeline parallelism size, batch size and micro-batch size), cost (power, CapEx and TCO) and performance (latency and throughput), etc. Figure <ref type="figure" target="#fig_8">8</ref> shows all valid design points for 4 models. The color indicates the batch size. The rich information from design space exploration helps designers find the most suitable design given any type of constraint and any optimization goal. In Table <ref type="table" target="#tab_8">2</ref>, we show the latency and TCO/Token optimal designs for four models. Each design is optimized just for the model. Latency-optimal designs are always preferred when the batch size is 1, since it requires fewer operations. These designs also tend to use large chips with higher TOPS to minimize inter-node communication and computation latency. At the same time, the TCO per token of these designs is high, which mainly depends on utilization. It is well known that LLMs have low hardware utilization when the batch size is small. For the TCOoptimal designs, the batch size are much larger (64 to 128). Large batch size is good for utilization while requiring larger KV cache and thus more silicon. This means we either need bigger chips which greatly increase CapEx, or more chips which generate more inter-node traffic and hurt throughput. An appropriate batch size to balance each factor is essential to achieve good TCO/Token, but can be difficult to find. The 8 optimal design points all have different chip, server designs, and different mapping strategies, demonstrating the importance of our design methodology-every aspect of PaLM-540B Multi-Query Batch Size TCO/1K Tokens ($)</p><p>Figure <ref type="figure" target="#fig_10">10</ref>: The optimal TCO/Token under different batch sizes. Small batch requires less silicon, and large batch benefit utilization. The optimal batch size for multi-head models is around 64, while the multi-query model PaLM can maintain a near-optimal TCO/Token at batch size 1024. The cost of longer contexts is not noticeable for PaLM.</p><p>the system affects performance and cost, and different workloads require different optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Design Insights</head><p>We first study how chip size affects TCO and performance. Figure <ref type="figure">9</ref> shows the results of GPT-3 in two different scenarios. On the left is how we should choose die size to lower TCO for a given minimum throughput requirement. Compared to chips over 700mm 2 , which is the size of many traditional large monolithic chips, a chip around 200 mm 2 reduces TCO by about 1.5? and still meets the throughput constraint. We can also find the CapEX exceeds 80% of TCO for most designs. Figure <ref type="figure">on</ref> the right shows what are the best latency for chips of different sizes given the TCO budget. Except for the smallest size chip, others all achieve similar latencies. This shows that proper chip size can effectively reduce TCO without compromising performance.</p><p>We further investigate how the batch size affects TCO/Token. Figure <ref type="figure" target="#fig_10">10</ref> shows the results on 4 different models and 3 different context length. When the batch size is increased from 1, the TCO/Tokens become better because the compute unit utilization increases. Larger batch sizes also provide more opportunities to  exploit pipeline parallelism to improve overall system utilization. As the batch size continues to increase, the utilization will reach a peak. For the traditional multi-head model, more silicon is required for KV cache in large batch size and long contexts, which significantly increases TCO/Token. Chiplet Cloud supports batch sizes up to 64 with near-optimal TCO/Token for these models. For multi-query model PaLM, Chiplet Cloud supports batch sizes up to 1024 with near-optimal TCO/Token. The cost of longer contexts is negligible, especially when the batch size is not too large.</p><p>Lastly, we study how the mapping strategy affects TCO/Token for a given batch size. As shown in Figure <ref type="figure" target="#fig_9">11</ref>, when the number of pipeline stages (i.e. the pipeline parallelism size) is close to the batch size, the system utilization is the largest and the TCO/Token is optimal. When these two numbers are similar, the system can take full advantage of pipeline parallelism with a micro-batch size of 1, so the number of micro-batches is also close (if not equal) to the pipeline stage <ref type="bibr" target="#b0">[1]</ref>. This helps balance the latency of micro-batches passing through all pipeline stages and pipeline stages completing all micro-batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>In this section, we evaluate the performance and cost of Chiplet Cloud for serving large language models. The key metric we are targeting is TCO/Token. TCO/Token is measured as cost per token generated and is the key factor in the ability to democratize of LLMs. One of the most popular business models for generative LLMs is also to charge users per generated token. Lower TCO/Token not only adds more profit margins, but also makes LLMs more approachable. We compare Chiplet Cloud to state-of-the-art GPU and TPU cloud implementations. We also demonstrate the benefits of our architectural design choices and evaluate the flexibility of Chiplet Cloud architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison with GPUs and TPUs</head><p>In Table <ref type="table" target="#tab_9">3</ref>, we compare Chiplet Cloud versus state-of-the-art GPU <ref type="bibr" target="#b0">[1]</ref> and TPU <ref type="bibr" target="#b25">[25]</ref> implementations. Neither work is specifically optimized for TCO/Token. For our comparison, we choose the throughput optimal result for GPU, and the utilization optimal result for  TPU, which are key indicators that you are close to TCO/Token optimal. TCO for GPUs and TPUs are based on the best cloud rental price we could find <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. Compared to A100 GPU and TPUv4 clouds, Chiplet Cloud achieves 94? and 15? improvement on TCO/Token, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Design Choice Sanity Check</head><p>To demonstrate the benefits of our design choices, including fitting all parameters in on-chip SRAM and using many smaller chiplet accelerators, we compare Chiplet Cloud with two baseline systems.</p><p>The first system (HBM) is a conventional HBM-based accelerator system. We assume each chip has a 8GB HBM die that supports up to 900GB/s bandwidth. All model parameters and KV cache are on HBM. The second system (Large Chip) is Chiplet Cloud without chip size moderation. Like a traditional large monolithic chip, it tries to put as much memory and as many processing units as possible on the chip. We pass the hardware specs of both systems to our design methodology and search for the optimal mapping strategies. Figure <ref type="figure" target="#fig_10">12</ref> shows the optimal TCO/Token of both systems on 4 models. All numbers are normalized to Chiplet Cloud. On average, Chiplet Cloud outperforms the Large Chip system by 2.49? TCO/TOken and the HBM system by 1.24? TCO/Token.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Generic Chiplet Cloud Servers</head><p>Up until now, we have only shown Chiplet Cloud optimized for a single workload (one language model with fixed context length). However, the separation of hardware exploration and software evaluation in our design methodology makes it possible to find suitable server designs for multiple workloads simultaneously, thus increasing the flexibility of the resulting Chiplet Cloud. This is done by applying a performance-cost analysis of each Chiplet Cloud server design across multiple workloads simultaneously to find design points that are in aggregate TCO/Token optimal.</p><p>During hardware exploration, we generate 1073 valid unique server designs. In Figure <ref type="figure" target="#fig_11">13</ref>, we list the top 10 servers that achieve the best aggregated performance (TCO/Token) on different models (left), and on the same model but with different context lengths (right). The performance shown of the server across all workloads is normalized to the aggregated performance achieved by the optimal server for each individual workload. If a server achieves the optimal performance on all 4 workloads, it would have an aggregated performance of 4.0. Compared to the aggregated performance of the individually optimized servers, the best single server TCO/Token across the 4 LLMs only dropped 12% while the best single server TCO/Token across the 3 context lengths only dropped 9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Training and serving large language models on GPU and TPU has attracted a lot of attention in recent years. Megatron-LM <ref type="bibr" target="#b31">[31]</ref> proposes a tensor partitioning strategy that reduces the inter-node communication. <ref type="bibr" target="#b22">[23]</ref> improves pipeline parallelism and combines it with Megatron-LM and achieves high aggregate throughput. Deep-Speed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">29]</ref> proposes multi-GPU training and inference solution to minimize latency while maximizing throughput. PaLM <ref type="bibr" target="#b4">[5]</ref> train a 540B parameter model on 6144 TPUv4 chips using Pathways <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr" target="#b25">[25]</ref> develops an analytical model to select the mapping strategy optimized for inference on TPUv4. Many ASIC accelerators for transformer NNs have been proposed. SpAtten <ref type="bibr" target="#b39">[39]</ref> exploits the token and head sparsity and quantization opportunities in the attention block. ELSA <ref type="bibr" target="#b9">[10]</ref> presents an approximation scheme for the attention mechanism. EdgeBERT <ref type="bibr" target="#b34">[34]</ref> leverages dynamic voltage-frequency scaling based on early exit prediction of ALBERT <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b40">[40]</ref> designs a transformer accelerator in 28nm using approximate-computing and sparsity speculation. These designs focus on optimizing the attention block, which is usually not the bottleneck for LLMs.</p><p>To exploit chiplet technology on deep learning accelerators, Simba <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b38">38]</ref> implements the first 36-chiplet prototype multichip-modules (MCM) system for deep learning inference. COMB-MCM <ref type="bibr" target="#b41">[41]</ref> uses chiplets to improve the scalability of their computingon-memory-boundary NN processors. NN-Baton <ref type="bibr" target="#b35">[35]</ref> proposes a framework to explore the chiplet design space for convolutional NN. These works focus on small-scale optimizations-chiplets in a single package, and do not demonstrate scalability to support LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper presents Chiplet Cloud, a chiplet-based ASIC AI supercomputer architecture that achieves unprecedented TCO/Token for serving large generative language model. Chiplet Cloud fits all model parameters inside the on-chip SRAMs to eliminate bandwidth limitations while moderating the die size to improve system costs while leveraging software mappings to overcome data communication overhead. We propose a comprehensive design methodology that accurately explores a spectrum of major design trade-offs in the joint space of hardware-software and generates a detailed performance-cost analysis on all valid design points. With this methodology, we design Chiplet Cloud systems for four language models of different sizes and achieved 94? and 15? better TCO/Token compared to A100 GPU and TPUv4, respectively. We believe Chiplet Cloud to be the best solution to democratise modern and future large generative language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Compared to A100 GPU and TPUv4, Chiplet Cloud can achieve up to 94? and 15? improvement in TCO/Token on GPT-3 and PaLM 540B, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A generative language model is constructed of multiple decoder layers with the same architecture. It generates outputs in an autoreggressive fashion. FC layers often dominate the computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Roofline of GPT-2 model on a V100 GPU. Most kernels have low operational intensity and are HBM bandwidth bounded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Compared with DDR4 and HBM2e, SRAM has an order-of-magnitude better bandwidth and read energy, resulting in better TCO/Token designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Two phase design methodology flow diagram. The first phase is the hardware exploration flow (a) which performs a bottom-up, LLM agnostic design space exploration generating thousands of realizable Chiplet Cloud server designs. The second phase is the software evaluation flow (b) which takes the realizable server design points along with a generative LLM specification to perform software optimized inference simulations and TCO estimations to find Pareto optimal Chiplet Cloud design points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Design space exploration of four major LLMs on the market.</figDesc><graphic url="image-8.png" coords="8,342.26,155.49,89.36,50.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Pipeline stages sweeping for different models and batch sizes. The number of pipeline stages close to the batch usually achieves the highest utilization, resulting in the optimal TCO/Token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>GPT2- 1 .</head><label>1</label><figDesc>4B Turing NLG-17B GPT3-175B PaLM-540B GeoMean 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Generic Chiplet Cloud servers support different models and context lengths with only 9% and 12% performance loss, respectively. The aggregated performance (TCO/Token) of 10 servers on different models (left) and different context length (right) are plotted. Performance are normalized to the optimal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>is a 46,255 mm 2 chip with 40 GB on-chip memory. The niche wafer-scale designs are expensive to manufacture, resulting in limited potential for TCO reduction. Instead, Figure 6: Our High-Level Chiplet Cloud architecture.we believe that chip with large SRAM should remain at a relative small size, to reduce the fabrication cost. We argue that chiplet is a major method for managing TCO of LLM supercomputers. Chiplet technology has recently become a new trend in the industry. It breaks down a traditional monolithic silicon chip into multiple small chiplets and integrates them into a single package. This ap-</figDesc><table><row><cell>Process Unit</cell><cell cols="2">Local Buffer MACs</cell><cell>Buffer Accu.</cell><cell>Controller Off-PCB Interface</cell><cell>1U 1U</cell><cell>1U 1U</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1U</cell><cell>1U</cell></row><row><cell>Ctrl</cell><cell>PU Array</cell><cell>GeLu Norm ?</cell><cell>C2C</cell><cell></cell><cell>1U 1U 1U</cell><cell>1U 1U 1U</cell></row><row><cell>C2C Input Buffer</cell><cell>BCast Gather</cell><cell cols="2">Softmax Global Memory Attention (Parameters, KV)</cell><cell></cell><cell>1U 1U 1U 1U 1U 1U</cell><cell>1U 1U 1U 1U 1U 1U</cell></row><row><cell cols="4">Chiplet Module (Single Package)</cell><cell>Server</cell><cell cols="2">Cloud</cell></row></table><note><p>proach improves fabrication yield, reduces manufacturing costs and enables die-level reuse for different system scales. For TSMC 7nm technology with a defect density of 0.1 per cm 2 , the unit price of a 750 mm 2 chip is twice that of a 150 mm 2 chip. It is a currently available commodity technology that all architects can use, aligning with our TCO/token optimization focus. One potential drawback of chiplet design is the high inter-chiplet communication. Studies on GPUs and TPUs have shown that proper mapping strategies (e.g., tensor and pipeline parallelism</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Target workloads in case studies.</figDesc><table><row><cell>Model</cell><cell>GPT-2</cell><cell>T-NLG</cell><cell>GPT-3</cell><cell>PaLM</cell></row><row><cell>? ?????</cell><cell>1600</cell><cell>4256</cell><cell>12288</cell><cell>18432</cell></row><row><cell>Layers</cell><cell>48</cell><cell>78</cell><cell>96</cell><cell>118</cell></row><row><cell>Attention Heads</cell><cell>25</cell><cell>28</cell><cell>96</cell><cell>48</cell></row><row><cell>Multi-query</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell></row><row><cell># of Parameters</cell><cell>1.4B</cell><cell>17B</cell><cell>175B</cell><cell>540B</cell></row><row><cell>Context Length</cell><cell></cell><cell cols="2">256, 2048, 8192</cell><cell></cell></row><row><cell>Batch Size</cell><cell></cell><cell>1?1024</cell><cell></cell><cell></cell></row><row><cell cols="5"># of Design Points 5,058,831 2,634,071 2,718,410 971,906</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Optimal Chiplet Cloud system for different language models. Context length is 2K.</figDesc><table><row><cell cols="2">Optimization Target (per Token) Latency</cell><cell>TCO</cell><cell cols="2">Latency TCO</cell><cell cols="2">Latency TCO</cell><cell cols="2">Latency TCO</cell></row><row><cell>Die Size (mm2)</cell><cell>800</cell><cell>620</cell><cell>800</cell><cell>160</cell><cell>600</cell><cell>160</cell><cell>800</cell><cell>260</cell></row><row><cell>MB per Chip</cell><cell>1110</cell><cell>831</cell><cell>1170</cell><cell>210</cell><cell>884</cell><cell>216</cell><cell>1170</cell><cell>364</cell></row><row><cell>TOPS per Chip</cell><cell>140</cell><cell>143</cell><cell>46</cell><cell>17</cell><cell>13.8</cell><cell>8.6</cell><cell>46</cell><cell>14.5</cell></row><row><cell>Chips per Server</cell><cell>8</cell><cell>8</cell><cell>32</cell><cell>80</cell><cell>80</cell><cell>144</cell><cell>32</cell><cell>96</cell></row><row><cell>Number of Servers</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>8</cell><cell>6</cell><cell>32</cell><cell>30</cell><cell>30</cell></row><row><cell>Tensor Parallelism Size</cell><cell>2</cell><cell>1</cell><cell>32</cell><cell>16</cell><cell>80</cell><cell>48</cell><cell>16</cell><cell>24</cell></row><row><cell>Pipeline Parallelism Size</cell><cell>2</cell><cell>16</cell><cell>1</cell><cell>39</cell><cell>6</cell><cell>96</cell><cell>59</cell><cell>118</cell></row><row><cell>Batch Size</cell><cell>1</cell><cell>16</cell><cell>1</cell><cell>32</cell><cell>1</cell><cell>64</cell><cell>1</cell><cell>128</cell></row><row><cell>Micro-Batch Size</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Latency (ms)</cell><cell>0.018</cell><cell>0.025</cell><cell>0.133</cell><cell>0.28</cell><cell>0.81</cell><cell>1.89</cell><cell>2.86</cell><cell>4.8</cell></row><row><cell>Cents/1K Tokens</cell><cell cols="2">0.0002 0.00005</cell><cell>0.004</cell><cell>0.001</cell><cell>0.185</cell><cell>0.018</cell><cell>2.28</cell><cell>0.031</cell></row><row><cell>Model</cell><cell cols="2">GPT-2</cell><cell cols="2">Turing NLG</cell><cell cols="2">GPT-3</cell><cell cols="2">PaLM 540B</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Comparison with GPUs and TPUs.</figDesc><table><row><cell>Models</cell><cell cols="2">GPT-3</cell><cell cols="2">PaLM 540B</cell></row><row><cell>Systems</cell><cell>32 CC</cell><cell>32 DGX</cell><cell>30 CC</cell><cell>64 TPUv4</cell></row><row><cell>*CC=Chiplet Cloud</cell><cell>Servers</cell><cell>A100 Srvs</cell><cell>Servers</cell><cell>Chips</cell></row><row><cell>TCO/sec (cents)</cell><cell>0.61</cell><cell>7.82</cell><cell>0.83</cell><cell>2.61</cell></row><row><cell>Latency (ms)</cell><cell>1.9</cell><cell>620.0</cell><cell>4.8</cell><cell>93.8</cell></row><row><cell>Tokens/sec</cell><cell>33,791</cell><cell>4,608</cell><cell>26,667</cell><cell>5,461</cell></row><row><cell>TCO per 1K tokens (cents)</cell><cell>0.018</cell><cell>1.698</cell><cell>0.031</cell><cell>0.478</cell></row><row><cell>Improvement</cell><cell></cell><cell>94?</cell><cell></cell><cell>15?</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elton</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC41404.2022.00051</idno>
		<ptr target="https://doi.org/10.1109/SC41404.2022.00051" />
	</analytic>
	<monogr>
		<title level="m">SC22: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting><address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudip</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Shafey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandramohan</forename><forename type="middle">A</forename><surname>Thekkath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12533</idno>
		<idno>arXiv: 2203.12533</idno>
		<ptr target="http://arxiv.org/abs/2203.12533" />
		<title level="m">Pathways: Asynchronous Distributed Dataflow for ML</title>
		<imprint>
			<date type="published" when="2022-03">2022. March 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Datacenter as a Computer: Designing Warehouse-Scale Machines</title>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urs</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>H?lzle</surname></persName>
		</author>
		<author>
			<persName><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on computer architecture</title>
		<imprint>
			<biblScope unit="page">209</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><surname>Mccandlish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<idno>arXiv: 2005.14165</idno>
		<ptr target="http://arxiv.org/abs/2005.14165" />
	</analytic>
	<monogr>
		<title level="m">Language Models are Few-Shot Learners</title>
		<imprint>
			<date type="published" when="2020-07">2020. July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<idno>arXiv: 2204.02311</idno>
		<ptr target="http://arxiv.org/abs/2204.02311" />
		<title level="m">PaLM: Scaling Language Modeling with Pathways</title>
		<meeting><address><addrLine>Jeff Dean, Slav Petrov, and Noah Fiedel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-04">2022. April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Google</forename><surname>Cloud</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/tpu/pricing#v4-pricing" />
		<title level="m">Cloud TPU pricing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872887.2750389</idno>
		<ptr target="https://doi.org/10.1145/2872887.2750389" />
	</analytic>
	<monogr>
		<title level="m">ShiDianNao: Shifting Vision Processing Closer to the Sensor</title>
		<imprint>
			<date type="published" when="2015-06">2015. jun 2015</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalin</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomi</forename><surname>Alkalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Ghandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prerak</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Sapek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitaram</forename><surname>Lanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2018.00012</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2018.00012" />
		<title level="m">A Configurable Cloud-Scale DNN Processor for Real-Time AI</title>
		<meeting><address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><surname>Github</surname></persName>
		</author>
		<ptr target="https://github.com/features/copilot" />
		<title level="m">GitHub Copilot Your AI pair programmer</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soosung</forename><surname>Hoon Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Jun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00060</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00060" />
	</analytic>
	<monogr>
		<title level="m">ISCA. IEEE, Valencia</title>
		<meeting><address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="692" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ten Lessons From Three Generations Shaped Google&apos;s TPUv4i : Industrial Product</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ashcraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gottscho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Jablin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushma</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00010</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00010" />
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A domain-specific supercomputer for training deep neural networks</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3360307</idno>
		<ptr target="https://doi.org/10.1145/3360307" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2020-06">2020. June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enabling interposer-based disintegration of multi-core processors</title>
		<author>
			<persName><forename type="first">Ajaykumar</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><forename type="middle">Enright</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<idno type="DOI">10.1145/2830772.2830808</idno>
		<ptr target="https://doi.org/10.1145/2830772.2830808ISSN" />
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2379" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moonwalk: NRE Optimization in ASIC Clouds</title>
		<author>
			<persName><forename type="first">Moein</forename><surname>Khazraee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bedford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename></persName>
		</author>
		<idno type="DOI">10.1145/3037697.3037749</idno>
		<ptr target="https://doi.org/10.1145/3037697.3037749" />
	</analytic>
	<monogr>
		<title level="m">ASPLOS. ACM, Xi&apos;an China</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="511" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graphcore Colossus Mk2 IPU</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Knowles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Hot Chips</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><surname>Lambda</surname></persName>
		</author>
		<ptr target="https://lambdalabs.com/service/gpu-cloud" />
		<title level="m">The best prices for cloud GPUs</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<idno>arXiv: 1909.11942</idno>
		<ptr target="http://arxiv.org/abs/1909.11942" />
		<imprint>
			<date type="published" when="2020-02">2020. Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AI Compute Chip from Enflame</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1109/HCS52781.2021.9567224</idno>
		<ptr target="https://doi.org/10.1109/HCS52781.2021.9567224" />
	</analytic>
	<monogr>
		<title level="m">Hot Chips</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ASIC Clouds: Specializing the Datacenter</title>
		<author>
			<persName><forename type="first">Ikuo</forename><surname>Magaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moein</forename><surname>Khazraee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Vega Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bedford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2016.25</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2016.25" />
	</analytic>
	<monogr>
		<title level="m">ISCA. IEEE</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="178" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Turing-NLG: A 17-billion-parameter language model by Microsoft</title>
		<ptr target="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">10 Google Search Statistics You Need to Know in 2023</title>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Mohsin</surname></persName>
		</author>
		<ptr target="https://www.oberlo.com/blog/google-search-statistics" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>text=We% 20know%20that%20there%20are,Internet%20Live%20Stats%2C%202022</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pioneering Chiplet Technology and Design for the AMD EPYC? and Ryzen? Processor Families : Industrial Product</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Naffziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Burd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahesh</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Subramony</surname></persName>
		</author>
		<author>
			<persName><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00014</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00014" />
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="57" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<title level="m">Efficient large-scale language model training on GPU clusters using megatron-LM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">In</forename><forename type="middle">Sc</forename><surname>Acm</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">Louis</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><surname>Missouri</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458817.3476209</idno>
		<ptr target="https://doi.org/10.1145/3458817.3476209" />
		<imprint>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt" />
		<title level="m">Introducing ChatGPT</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sholto</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kefan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05102</idno>
		<ptr target="http://arxiv.org/abs/2211.05102" />
		<title level="m">Efficiently Scaling Transformer Inference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A 1.17-pJ/b, 25-Gb/s/pin Ground-Referenced Single-Ended Serial Link for Off-and On-Package Communication Using a Process-and Temperature-Adaptive Voltage Regulator</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walker</forename><forename type="middle">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudhir</forename><forename type="middle">S</forename><surname>Kudva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Nedovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Thomas</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Dally</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSSC.2018.2875092</idno>
		<ptr target="https://doi.org/10.1109/JSSC.2018.2875092Conference" />
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="43" to="54" />
			<date type="published" when="2019-01">2019. Jan. 2019</date>
		</imprint>
		<respStmt>
			<orgName>IEEE Journal of Solid-State Circuits</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SambaNova SN10 RDU:Accelerating Software 2.0 with Dataflow</title>
		<author>
			<persName><forename type="first">Raghu</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumti</forename><surname>Jairath</surname></persName>
		</author>
		<idno type="DOI">10.1109/HCS52781.2021.9567250</idno>
		<ptr target="https://doi.org/10.1109/HCS52781.2021.9567250" />
	</analytic>
	<monogr>
		<title level="m">Hot Chips</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05596</idno>
		<ptr target="http://arxiv.org/abs/2201.05596" />
		<title level="m">DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture</title>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Thomas</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><surname>Keckler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358302</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358302" />
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<meeting><address><addrLine>Columbus OH USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<idno>arXiv: 1909.08053</idno>
		<ptr target="http://arxiv.org/abs/1909.08053" />
		<title level="m">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
		<imprint>
			<date type="published" when="2020-03">2020. March 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zion: Facebook Next-Generation Large Memory Training Platform</title>
		<author>
			<persName><forename type="first">Misha</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<idno type="DOI">10.1109/HOTCHIPS.2019.8875650</idno>
		<ptr target="https://doi.org/10.1109/HOTCHIPS.2019" />
	</analytic>
	<monogr>
		<title level="m">Hot Chips</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8875650</biblScope>
			<biblScope unit="page" from="2573" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wafer-Scale Deep Learning</title>
		<ptr target="https://www.hotchips.org/hc31/HC31_1.13_Cerebras.SeanLie.v02.pdf" />
	</analytic>
	<monogr>
		<title level="m">Hot Chips</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Cerebras Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference</title>
		<author>
			<persName><forename type="first">Coleman</forename><surname>Thierry Tambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Hooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pentecost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">En-Yu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gu-Yeon</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1145/3466752.3480095</idno>
		<ptr target="https://doi.org/10.1145/3466752.3480095" />
	</analytic>
	<monogr>
		<title level="m">MICRO. ACM, Virtual Event Greece</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="830" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NN-Baton: DNN Workload Orchestration and Chiplet Granularity Exploration for Multichip Accelerators</title>
		<author>
			<persName><forename type="first">Zhanhong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runpei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00083</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00083" />
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2575" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ground-referenced signaling for intra-chip and short-reach chip-to-chip interconnects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">H</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanquan</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudhir</forename><forename type="middle">S</forename><surname>Nedovic</surname></persName>
		</author>
		<author>
			<persName><surname>Kudva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rizwan</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxu</forename><surname>Bashirullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Thomas</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1109/CICC.2018.8357077</idno>
		<ptr target="https://doi.org/10.1109/CICC.2018.8357077" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Custom Integrated Circuits Conference (CICC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<idno>arXiv: 1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<title level="m">Attention Is All You Need</title>
		<imprint>
			<date type="published" when="2017-12">2017. Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable Multi-Chip-Module-Based Deep Neural Network Accelerator Designed with A High-Productivity VLSI Methodology</title>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Thomas</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Khailany</surname></persName>
		</author>
		<idno type="DOI">10.1109/HOTCHIPS.2019.8875657</idno>
		<idno>A 0.11 pJ/Op, 0.32-128 Tops</idno>
		<ptr target="https://doi.org/10.1109/HOTCHIPS.2019.8875657" />
	</analytic>
	<monogr>
		<title level="m">Hot Chips</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2573" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09852</idno>
		<idno>arXiv: 2012.09852</idno>
		<ptr target="http://arxiv.org/abs/2012.09852" />
		<title level="m">SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning</title>
		<imprint>
			<date type="published" when="2021-01">2021. Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A 28nm 27.5TOPS/W Approximate-Computing-Based Transformer Processor with Asymptotic Sparsity Speculating and Out-of-Order Computing</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dazheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leibo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouyi</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISSCC42614.2022.9731686</idno>
		<ptr target="https://doi.org/10.1109/ISSCC42614.2022.9731686ISSN" />
	</analytic>
	<monogr>
		<title level="j">In ISSCC</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="2376" to="8606" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinru</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhengmao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengcheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chixiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISSCC42614.2022.9731657</idno>
		<ptr target="https://doi.org/10.1109/ISSCC42614.2022.9731657ISSN" />
	</analytic>
	<monogr>
		<title level="m">COMB-MCM: Computing-on-Memory-Boundary NN Processor with Bipolar Bitwise Sparsity Optimization for Scalable Multi-Chiplet-Module Edge Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="2376" to="8606" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
