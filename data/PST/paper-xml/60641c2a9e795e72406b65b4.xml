<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heterogeneous Graph Neural Network via Attribute Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
							<email>jindi@tju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Cuiying</forename><surname>Huo</surname></persName>
							<email>huocuiying@tju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chundong</forename><surname>Liang</surname></persName>
							<email>liangchundong@tju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Yang</surname></persName>
							<email>yangliang@vip.qq.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Hebei University of Technology</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">SKLOIS</orgName>
								<orgName type="institution">IIE, CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Heterogeneous Graph Neural Network via Attribute Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449914</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Heterogeneous information networks</term>
					<term>Graph neural networks</term>
					<term>Missing data</term>
					<term>Attribute completion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heterogeneous information networks (HINs), also called heterogeneous graphs, are composed of multiple types of nodes and edges, and contain comprehensive information and rich semantics. Graph neural networks (GNNs), as powerful tools for graph data, have shown superior performance on network analysis. Recently, many excellent models have been proposed to process hetero-graph data using GNNs and have achieved great success. These GNN-based heterogeneous models can be interpreted as smooth node attributes guided by graph structure, which requires all nodes to have attributes. However, this is not easy to satisfy, as some types of nodes often have no attributes in heterogeneous graphs. Previous studies take some handcrafted methods to solve this problem, which separate the attribute completion from the graph learning process and, in turn, result in poor performance. In this paper, we hold that missing attributes can be acquired by a learnable manner, and propose a general framework for Heterogeneous Graph Neural Network via Attribute Completion (HGNN-AC), including pre-learning of topological embedding and attribute completion with attention mechanism. HGNN-AC first uses existing HIN-Embedding methods to obtain node topological embedding. Then it uses the topological relationship between nodes as guidance to complete attributes for no-attribute nodes by weighted aggregation of the attributes from these attributed nodes. Our complement mechanism can be easily combined with an arbitrary GNN-based heterogeneous model making the whole system end-to-end. We conduct extensive experiments on three real-world heterogeneous graphs. The results demonstrate the superiority of the proposed framework over stateof-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Neural networks; • Information systems → Social networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the real world, the complex systems are always represented in graph data structures, such as social networks, citation networks and so on. There is a surge of interest in learning on graph data, especially the heterogeneous graphs <ref type="bibr" target="#b30">[31]</ref>. Heterogeneous graphs consist of multi-typed nodes and edges, corresponding to depicting various entities and their interactions in the real-world system. Taking DBLP as an example, we can model it as a heterogeneous graph which contains four node types (author, paper, term, venue) and three edge types (paper-author, paper-term, paper-venue), as shown in Figure <ref type="figure" target="#fig_4">1 (b)</ref>. Heterogeneous graphs can better model the real-world systems than traditional homogeneous graphs since they contain more comprehensive information and rich semantics.</p><p>Graph neural networks (GNNs) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b47">48]</ref>, first proposed on homogeneous graphs, have shown state-of-the-art performance and caught a great attention of researchers. GNNs have been widely adopted in various tasks over graphs, such as graph classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43]</ref>, link prediction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref> and node classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref>. Recently, many well-performed models have been proposed to process hetero-graph <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref> data using GNNs, such as HAN <ref type="bibr" target="#b38">[39]</ref> and MAGNN <ref type="bibr" target="#b10">[11]</ref>. These GNN-based heterogeneous models can be interpreted as smooth node attributes in neighbors guided by graph structure. In order to learn node representation, all nodes' attributes are required. However, this is not always satisfied. Some nodes have no attributes because the cost is prohibitively expensive or even impossible (like sensitive personal information). Especially in heterogeneous graphs, we usually cannot get the attributes of all types of nodes, which will affect the performance of GNN-based models. We divide the attribute missing in the heterogeneous graph into two categories. One is that the nodes that need to be analyzed have no attributes, as shown the author nodes in DBLP in Figure <ref type="figure" target="#fig_4">1 (b)</ref>. The other is that the nodes that do not need to be analyzed have no attributes, as shown the actor nodes in IMDB in Figure <ref type="figure" target="#fig_4">1 (a)</ref>. IMDB contains three types of nodes: movie, actor and director. However, only movie nodes have attributes which are bag-of-words representation of their plots. Similarly, in DBLP, only paper nodes have attributes directly derived from their keywords. Specifically, in DBLP, we usually perform analysis tasks on author nodes, but the attributes of author nodes are hard to get. In IMDB, we usually perform analysis tasks on movie nodes. Although movie nodes have attributes, research shows that the attribute information of actors and directors will also have a great impact on node analysis tasks <ref type="bibr" target="#b10">[11]</ref>. The missing attributes will significantly affect the performance.</p><p>Although some types of nodes have no attributes, in most cases these nodes without attributes will be directly connected to nodes with attributes, so previous studies have adopted some handcrafted ways to deal with this problem of missing attributes in heterogeneous graphs. Taking MAGNN and HAN as an example, in the DBLP dataset, they use bag-of-words representation of paper keywords as the attributes of papers, which seems to be reasonable. But for authors, because no more relevant information is provided in the dataset, they use bag-of-words representation of keywords extracted from their published papers. This is the same as that an author's attribute vector comes from the mean of its directly connected papers' attribute vectors (which is actually done in IMDB). What's more, they use no computer-science-specialized pre-trained word vectors <ref type="bibr" target="#b24">[25]</ref> and one-hot representation as the attributes of terms and venues, which may provide less effective information.</p><p>In this paper, we propose a general framework for Heterogeneous Graph Neural Network via Attribute Completion (HGNN-AC). The goal of the proposed framework is to solve the problem of missing some types of node attributes in heterogeneous graphs via learning. We use topological relationship between nodes as guidance to complete attributes for no-attribute nodes by weighted aggregation of the attributes of these attributed nodes. Specifically, HGNN-AC first uses HIN-Embedding methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref> to obtain node embeddings, and then distinguishes different contributions of different nodes by computing the attention value of node embeddings when conducting weighted aggregation. This complement mechanism can be easily combined with an arbitrary HINs model making the whole system end-to-end. Note that there is a weak supervision loss in the process of node attribute completion. The weak supervision loss, combining with the model's prediction loss, are used to optimize the learning process of attribute completion.</p><p>Our attribute completion framework can give a performance improvement to GNN-based heterogeneous models. Specifically, when the target types of nodes being analyzed have no attributes (e.g., author nodes in DBLP), using the proposed framework to complete attributes for this type of nodes can greatly improve model performance. Even the target types of nodes already have attributes (e.g., movie nodes in IMDB), using the proposed framework to complete attributes for other types of nodes also can improve model performance. This is because the nature of the GNN-based model, i.e., completed attributes of other types nodes will be propagated and aggregated to the target types of nodes to help the model predict better.</p><p>The contributions of our work are summarized as follows:</p><p>(1) We propose the problem of missing whole attributes of some types of nodes which is the unique property of attributes missing in heterogeneous graphs. Previous heterogeneous methods usually do not consider the problem of missing attributes, or use handcrafted ways (e.g., summing or averaging) to replace missing attributes. To the best of our knowledge, this is the first attempt to complete node attributes for heterogeneous graphs. <ref type="bibr" target="#b1">(2)</ref> We propose a general framework for Heterogeneous Graph Neural Network via Attribute Completion (HGNN-AC), which solves the problem of missing attributes of some types of nodes in HINs. This framework addresses the deficiencies of the previous handcrafted approach by a learnable manner, and is easy to be combined with an arbitrary HINs model. (3) We conduct extensive experiments on the DBLP, ACM and IMDB datasets to evaluate the performance of the proposed framework. The results show that the performance of existing models can be significantly improved after combining the proposed framework. We conduct a case study on the ACM dataset to further demonstrate the superiority of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Graph Embedding</head><p>Graph embedding <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref> aims to project nodes in a graph into a low-dimensional vector space, in which the representation of nodes can reflect the relationship between nodes, so as to retain the semantic information of nodes. This challenging topic has first been addressed in homogeneous graphs, such as DeepWalk <ref type="bibr" target="#b25">[26]</ref>, LINE <ref type="bibr" target="#b34">[35]</ref>, node2vec <ref type="bibr" target="#b12">[13]</ref> and struc2vec <ref type="bibr" target="#b26">[27]</ref>. Recently there are some embedding methods for heterogeneous graphs. In heterogeneous graphs, the most important thing is how to distinguish the heterogeneity of nodes and edges, and how to capture the rich semantic information brought by network heterogeneity. For example, metapath2vec <ref type="bibr" target="#b7">[8]</ref> generates random node sequences guided by meta-paths <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>, and then feeds the sequences to skip-gram <ref type="bibr" target="#b20">[21]</ref> model to generate node embeddings. HHNE <ref type="bibr" target="#b39">[40]</ref> has further improved metapath2vec by embedding nodes into the hyperbolic space. Unlike Metapath2vec and HHNE, ESim <ref type="bibr" target="#b29">[30]</ref> does not use random walk to obtain node sequences, instead it generates node The set of nodes with attributes in V V −</p><p>The set of nodes without attributes in</p><formula xml:id="formula_0">V v A node v ∈ V N + v The set of neighbors of node v ∈ V + A Topological structure X Node attributes X C</formula><p>Node attributes after attribute completion H Node embedding based on topology A Z</p><p>The final node embedding embeddings by learning from sampled positive and negative metapath instances. All these above embedding methods can learn good representations of nodes, which can be used directly for downstream tasks. However, all of the graph embedding methods introduced above have the limitations of ignoring node attribute information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>Graph neural networks (GNNs) are introduced in <ref type="bibr" target="#b27">[28]</ref> with the purpose of extending deep neural networks to deal with arbitrary graph-structured data. They are divided into two types: spectral domain <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> and spatial domain <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref>. The methods based on spectral domain adopt the spectral representation form of graphs. The methods based on spatial domain define convolutions directly on the graph, and aggregate feature information from spatial neighbors for each node, such as GraphSAGE <ref type="bibr" target="#b13">[14]</ref> and GAT <ref type="bibr" target="#b37">[38]</ref>. However, the above graph neural networks are used to deal with homogeneous graphs. Very recently, some studies have attempted to extend GNNs to heterogeneous graphs. For example, HAN <ref type="bibr" target="#b38">[39]</ref> model based on the hierarchical attention, including node-level and semantic-level attentions, learns the importance between meta-path <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref> based nodes and the importance of different meta-paths respectively. Then HAN aggregates attributes from meta-path <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref> based neighbors in a hierarchical manner. MAGNN <ref type="bibr" target="#b10">[11]</ref> model which contains the node content transformation to encapsulate input node attributes, the intra-meta-path aggregation to incorporate intermediate semantic nodes, and the inter-meta-path aggregation to combine messages from multiple meta-paths. GTN <ref type="bibr" target="#b43">[44]</ref>, which generates new graph structures by identifying useful connections between unconnected nodes on the original graph, can learn effective node embeddings on the new graphs in an end-to-end fashion.</p><p>Without exception, these methods mentioned above do not give an appropriate solution for the problem of missing whole attributes of some types of nodes in heterogeneous graphs. They only complete attributes by averaging or summing, or directly use one-hot vectors, which are not satisfactory. In this paper, we propose the attribute completion for heterogeneous graphs to fill this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>We first give formal definitions of some important terminologies related to heterogeneous graphs. We then give some notations and explanations used throughout this paper. Definition 1. Heterogeneous Graph. A heterogeneous Graph, denoted as G(V, E, F , R, φ, ϕ), where V represents the set of nodes, E the set of edges, F the set of node types and R the set of edge types, where |F | + |R| &gt; 2. Each node i ∈ V is associated with a node type mapping function φ : V → F , and each edge e ∈ E is associated with an edge type mapping function ϕ : E → R.</p><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref> (a), we construct a heterogeneous graph to model IMDB. It consists of three types of nodes (movie, actor and director) and two types of edges (movie-actor and movie-director).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2. Incomplete Attributes in Heterogeneous Graph.</head><p>Given a heterogeneous graph G(V, E, F , R, φ, ϕ), X is defined as node attributes. Incomplete Attributes means that ∃F ′ ⊂ F and F ′ , in which each node i ∈ V associated with a node type mapping function φ : V → F ′ has no attributes.</p><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref> (a), in IMDB, only movie nodes have attributes, while directors and actors have no attributes. Definition 3. Heterogeneous Graph Embedding. Given a heterogeneous graph G, the task is to learn a d-dimensional node representation h v ∈ R d for all v ∈ V with d ≪ |V | that are able to capture rich structural and semantic information involved in G.</p><p>The notations are summarized in Table <ref type="table" target="#tab_0">1</ref>. Next we will introduce the framework of attribute completion for heterogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HGNN-AC FRAMEWORK</head><p>In this section, we propose a node Attribute Completion framework for Heterogeneous Graph Neural Network (HGNN-AC). The framework follows the principle that the generated attributes of a no-attribute node (i.e., v ∈ V − ) should come from the attributed nodes (i.e., v ∈ V + ). The main idea is that we use topological information as guidance to calculate the contribution to the node v ∈ V − among the directly connected neighbors v ′ ∈ N + v , which can be a reference when we execute attribute completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>The goal of the proposed framework is to solve the problem of some types of nodes having no attributes in heterogeneous graphs. It uses topological relationship between nodes as guidance to complete attributes for the no-attribute nodes (i.e., v ∈ V − ) through attributed nodes (i.e., v ∈ V + ). Figure <ref type="figure" target="#fig_1">2</ref> shows the proposed framework for node attribute completion. Given a heterogeneous graph G with only some types of nodes having attributes, HGNN-AC first computes nodes' embeddings H via network topological structure A, and then uses H to evaluate node topological relationship by exploiting the attention mechanism <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref> to learn a sortable score to determine which directly connected attributed nodes are best suited to contribute attributes to no-attribute nodes. After knowing the best nodes, HGNN-AC completes attributes for nodes in set V − by weighted aggregating the attributes of nodes in set V + according to the score. In order to prevent overfitting, The overview of the proposed framework. Given a orginal heterogeneous graph with some types of nodes having no attributes as input, we first calculate node embeddings via network topology A. Then we randomly drop some attributes and perform attribute completion. The attribute completion process is essentially a weighted aggregation of directly connected neighbors' attributes, where weights are decided by the attention derived from node' embeddings. After attribute completion, we get a new heterogeneous graph with all node having attributes and send it to HINs model. Note that we have a completion loss between dropped attributes and reconstructed attributes. The combination of completion loss and model's prediction loss makes the whole model end-to-end.</p><p>and also to ensure that the attribute completion process can be guided, HGNN-AC first randomly drops some attributes of nodes in V + , and then reconstructs these attributes at the same time of the attribute completion process. In this way a completion loss can be calculated between dropped attributes and reconstructed attributes, making the attribute completion process guided. Finally, nodes with completed attributes, together with the network topology A, as a new graph, are fed to HINs models. The overall model can be optimized in an end-to-end manner by combining the loss of prediction and the loss of attribute completion as the final loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-learning of Topological Embedding</head><p>In heterogeneous graphs, nodes have topology information (all nodes have) and attribute information (some types of nodes don't have, e.g., author and subject nodes have no attributes in the ACM dataset). Homophily is the principle that a contact between similar entities occurs at a higher rate than among dissimilar entities <ref type="bibr" target="#b21">[22]</ref>. Due to the existence of network homophily <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49]</ref>, the topology and attribute information always express the similar or same semantics. For example, a scholar has a closer connection with the papers and venues in the field of his/her study, so scholar, paper and venue nodes share similar topologies. These nodes also have similar attributes because they are in the same field. With that in mind, we make an assumption that the relations of nodes' topology information can reflect well the relations of nodes' attribute information. In this paper, HGNN-AC uses existing heterogeneous graph embedding methods such as metapath2vec <ref type="bibr" target="#b7">[8]</ref> or HHNE <ref type="bibr" target="#b39">[40]</ref> to get node embeddings based on network topology. However, these skip-gram based methods always utilize single meta-path and may ignore some useful information. In order to get better embeddings, HGNN-AC first obtains more comprehensive node sequences by random walk according to the frequently used multiple meta-paths, and then feeds these sequences to the skip-gram model to learn node embeddings H .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attribute Completion with Attention Mechanism</head><p>For the case that some types of nodes have no attributes, some previous studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref> solve this problem by averaging aggregate attributes of the directly connected neighbors. But we noticed that the directly connected neighbors of each node play different roles and have different importance in attribute aggregation. This may be because these nodes are of different types, or because their local topologies are different, that is, the more neighbors a node has, the less important it is to each neighbor. After getting the embeddings of the nodes, HGNN-AC uses attention mechanism to automatically learn the importance of different direct neighbors, then aggregates attribute information for nodes in set V − from their first-order neighbors in set V + . Given a node pair (v, u) which are directly connected, the attention layer can learn the importance e vu which means the contribution of node u to node v. The contribution of node u can be formulated as:</p><formula xml:id="formula_1">e vu = att(h v , h u ),</formula><p>where h v and h u are the topological embeddings of nodes v and u, and u ∈ V + . att(•) denotes the function which can perform attention mechanism. Note that att(•) is shared for all node pairs because our assumption is universal for all node pairs. Furthermore, HGNN-AC adopts a masked attention mechanism which means we only calculate e vu for nodes u ∈ N + v , where N + v denotes the first-order neighbors of node v in set V + . It is obvious that first-order neighbors are likely to have more contributions, so this strategy can filter a large number of non-contributing nodes and reduce computation by adopting masked attention:</p><formula xml:id="formula_2">e vu = σ (h T v W h u ),</formula><p>where W is the parametric matrix, and σ an activation function.</p><p>After obtaining all direct neighbors' scores, a softmax function is applied to get normalized weighted coefficient a vu :</p><formula xml:id="formula_3">a vu = so f tmax(e vu ) =</formula><p>exp(e vu )</p><formula xml:id="formula_4">s ∈N + v exp(e vs )</formula><p>.</p><p>Then, HGNN-AC can perform weighted aggregation of attributes for node v according to weighted coefficient a vu :</p><formula xml:id="formula_5">x C v = u ∈N + v a vu x u .</formula><p>For node v, if none of its neighbor nodes has attributes (i.e. N + v = ), we set the attribute vector of node v to a zero vector. But in fact, this situation almost never occurs, so it has little impact on model performance.</p><p>Specially, the attention process is extended to a multi-head attention to stabilize the learning process and reduce the high variance (brought by the heterogeneity of networks), as done in many existing methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. In this way, the weighted aggregation of attributes for node v can be rewritten as:</p><formula xml:id="formula_6">x C v = mean( K k u ∈N + v a vu x u ),</formula><p>where K means that we perform K independent attention process and mean(•) means we average the K results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dropping some Attributes</head><p>The proposed framework is for node attribute completion, and we expect the completed attributes can give a performance improvement to the HINs models. However, there is a question that how do we evaluate these new attributes generated by the proposed framework? In other words, how can we ensure the attribute completion process is learnable and the generated attribute is indeed correct?</p><p>In order to solve this problem, the proposed framework adopts a strategy of dropping some attributes. HGNN-AC first randomly drops some attributes of nodes in V + , and then reconstructs these attributes at the same time of the attribute completion process. In this way, a completion loss can be calculated between dropped attributes and reconstructed attributes, making the attribute completion process guided and learnable.</p><p>To be specific, for nodes in V + , HGNN-AC randomly divides them into two parts V + dr op and V + keep according to a small ratio α, i.e. |V + dr op | = α |V + |. HGNN-AC first drops attributes of nodes in V + dr op and then reconstructs these attributes via attributes of nodes in V + keep by conducting attribute completion. The reconstructed attribute of node v can be formulated as:</p><formula xml:id="formula_7">x C v = mean( K k u ∈V + k e ep ∩V + i a vu x u ),</formula><p>where v ∈ V + dr op , u ∈ V + keep and V + keep ∩ V + i means that masked attention is also adopted here. The K and function mean(•) have the same meanings as above.</p><p>Our goal is that the reconstructed attributes are as close to the raw as possible. Here we introduce a weakly supervised loss to optimize the parameters of attribute completion. We use euclidean distance as the metric to design the loss function as:</p><formula xml:id="formula_8">L compl et ion = 1 |V + dr op | i ∈V + d r op (X C i − X i ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Combination with HIN Model</head><p>Now, we have completed attributes nodes in V − , and the raw attributes nodes in V + , then the new attributes of all nodes are defined as:</p><formula xml:id="formula_9">X new = {X C i , X j |∀i ∈ V − , ∀j ∈ V + }.</formula><p>The proposed framework keeps topological structure unchanged, so the new attributes X new , together with network topology A, as a new graph, are sent to the HIN model:</p><formula xml:id="formula_10">Y = Φ(A, X new ), L pr edict ion = f ( Y , Y ),</formula><p>where Φ denotes an arbitrary HINs model, Y and Y are model's prediction and label respectively, and f is the loss function, depending on the specific task of the model.</p><p>Finally, we apply the proposed framework to a HIN model. The loss of label prediction and the loss of attribute completion are combined as the final loss of this new model. After that, the overall model can be optimized via back propagation in an end-to-end manner:</p><formula xml:id="formula_11">L = λL compl et ion + L pr edict ion ,</formula><p>where λ is a weighted coefficient to balance these two parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Datasets</head><p>To evaluate the effectiveness of attribute completion by the proposed framework, we use three common HINs datasets. The statistics of the three datasets are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p><p>(1) DBLP <ref type="foot" target="#foot_0">1</ref> . We extract a subset of DBLP with 14328 papers (P), 4057 authors (A), 20 venue (V) and 8789 terms (T). Authors are divided into four research areas according to the conferences they submitted. In this dataset, papers' attributes are bag-of-words representation of their keywords, authors' attributes are bag-of-words representations of keywords extracted from their published papers, terms' attributes are no computer-science-specialized pre-trained word vectors <ref type="bibr" target="#b24">[25]</ref> and venues' attributes are one-hot vectors. Only papers' attributes are directly derived from the dataset.</p><p>(2) ACM<ref type="foot" target="#foot_1">2</ref> . We extract a subset of ACM using the same principle as HAN <ref type="bibr" target="#b38">[39]</ref>. The papers are divided into three classes according to the conference they published. Then we construct a heterogeneous graph that comprises 4019 papers (P), 7167 authors (A) and 60 subjects (S). In this dataset, papers' attributes are bag-of-words representations of their keywords, authors' and subjects' attribute vectors come from the mean of their directly connected papers' attribute vectors. Only papers' attributes are directly derived from the dataset. (3) IMDB <ref type="foot" target="#foot_2">3</ref> . We extract a subset of IMDB with 4780 movies (M), 5841 actors (A) and 2269 directors (D). Movies are divided into three classes according to their genres. In this dataset, movies' attributes are bag-of-words representations of their plots; actors' and directors' attribute vectors come from the mean of their directly connected movies' attribute vectors.</p><p>Only movies' attributes are directly derived from the dataset.</p><p>Note that, while all types of nodes in the above datasets seems to have attributes, it is in fact the result of some handcrafted designs from previous works. That is, only papers in DBLP and ACM, and movies in IMDB have their own attributes that can express their real semantic information. In the following experiments, if the comparison experiment requires node attributes, we use these handcrafted attributes as input. But when conducting experiment of our framework, we get rid of these handcrafted attributes as our framework can complete missing attributes automatically.</p><p>For semi-supervised learning on the above three datasets, the labeled nodes are divided into training, validation, and testing sets by 10%, 10%, 80%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We combined the proposed framework with two state-of-the-art models, i.e., MAGNN and GTN, denoted as MAGNN-AC and GTN-AC respectively. We compare the performance of MAGNN-AC and GTN-AC with some existing methods, including MAGNN and GTN.</p><p>(1) Metapath2vec <ref type="bibr" target="#b7">[8]</ref>: a skip-gram based heterogeneous embedding method which performs meta-path based random walk and utilizes skip-gram to generate embedding. We test on all meta-paths separately and report the best results. (2) GCN <ref type="bibr" target="#b18">[19]</ref>: a homogeneous GNN. This model uses an efficient layer-wise propagation rule that is based on a first-order approximation of spectral convolutions on graphs. We test GCN on serveral meta-path based homogeneous graphs and report the best results. (3) GAT <ref type="bibr" target="#b37">[38]</ref>: a homogeneous GNN. This model operates graphstructured data by leveraging masked self-attentional layers. We test GAT on serveral meta-path based homogeneous graphs and report the best results. (4) HetGNN <ref type="bibr" target="#b44">[45]</ref>: a heterogeneous GNN. This model jointly considers node heterogeneous contents encoding, type-based neighbors aggregation, and heterogeneous types combination.  <ref type="formula">6</ref>) MAGNN <ref type="bibr" target="#b10">[11]</ref>: a heterogeneous GNN. This model realizes the prediction task through three steps (Node Content Transformation, Intra-meta-path Aggregation, Inter-meta-path Aggregation), and uses encoder functions to further improve performance. ( <ref type="formula">7</ref>) GTN <ref type="bibr" target="#b43">[44]</ref>: a heterogeneous GNN. This model is able to learn a new graph structure which involves identifying useful metapaths and multi-hop connections automatically. Due to the large memory and computing power this model required, we sampled multiple sub-networks and trained in batches according to the sampling method provided by <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>For the original settings/parameters of the model (MAGNN or GTN), we keep them unchanged. For the proposed framework, we use the following settings: We set the dropout ratio to 0.5 when conducting the masked attention and we extend masked attention to a multihead attention with the number of attention head K = 8. We set divided ratio α of N + to 0.3 , and loss weighted coefficient λ to 0.5. The learning rate of the proposed framework's parameters is 0.005. For a fair comparison, we set the embedding dimension to 64 for all methods compared. The code and data of this work are available at https://github.com/liangchundong/HGNN-AC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Node Classification</head><p>Node classification is a traditional task to evaluate the quality of the learned node embeddings. We perform node classification tasks under two types of datasets to compare the performance of different models. One type of dataset is that the nodes that need to be classified have no raw attributes. Here we choose the DBLP dataset for experimentation. Another type of dataset is that the nodes that need to be classified have raw attributes, while other types of nodes have no attributes. Here we choose the ACM and IMDB datasets for experimentation. We verify the effectiveness by combining the proposed framework with MAGNN and GTN respectively. First, we generate embeddings of the labeled nodes (i.e., authors in DBLP, authors in ACM and movies in IMDB). Then we feed the embeddings  After using the proposed framework to complete the attributes of authors, conferences and term nodes, MAGNN-AC is 0.34%-1.39% more accurate than MAGNN which exceeds that of all comparison algorithms. In another way, compared with MAGNN, MAGNN-AC reduces the error rate by 6.16%-22.86%.</p><p>In Table <ref type="table" target="#tab_3">4</ref>, we perform node classification tasks on the paper nodes in ACM and the movie nodes in IMDB. These two types of nodes have attributes, while other types of nodes have no raw attributes. After completing attributes by the proposed framework, MAGNN-AC also performs the best across different training ratios. On the ACM dataset, model performance has improved significantly (3.00%-4.24%) after attribute completion. That is, the error rate is reduced by 30.50%-50.08%. Most notably, MAGNN-AC outperforms HAN while MAGNN underperforms HAN. This is due to the fact that the proposed framework provides a better representation of node attributes. On the IMDB dataset, MAGNN-AC has a 0.63%-3.80% improvement, that is, it reduces the error rate of MAGNN by 1.60%-9.10%. Note that MAGNN-AC has a greater superiority when the training ratio decreases, which illustrates that the model will be more stable when combined with the proposed framework.</p><p>Here we present the comparative results between GTN and GTN-AC visually in the form of bar charts. From Figure <ref type="figure" target="#fig_2">3</ref>, we can see that the performance of GTN-AC has been significantly improved on the ACM and DBLP datasets. It further proves the superiority of the proposed framework.</p><p>All above results show that better node attributes can be obtained through the proposed framework that uses the attention mechanism for attribute completion under the guidance of topological relations. For a node, its neighbors are often not equally important. Vanilla methods use handcrafted manner and ignore unequal importance of neighbors, making the generated attributes of no-attribute nodes inefficient and compromising model performance. Our framework can automatically learn the importance of different direct neighbors. Moreover, the proposed framework can solve the problem of more than one type of missing attributes of heterogeneous datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Visualization</head><p>We conduct the task of visualization to show a more intuitively comparation. We learn the node embeddings of methods mentioned above (i.e., MAGNN, GTN, MAGNN-AC and GTN-AC) on the ACM dataset and project the embeddings into a 2-dimensional space.</p><p>Then we utilize t-SNE <ref type="bibr" target="#b35">[36]</ref> to visualize the paper embeddings in ACM and colored the nodes based on their classes. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, MAGNN and GTN do not perform so well. Some papers belong to different classes are mixed with each other, and the boundary is blurry. After combining the proposed framework, MAGNN-AC shows a clearer boundary and denser cluster structures to distinguish different classes in visualization, and so is GTN-AC. It demonstrates that the accurate attribute information can make a significant contribution to heterogeneous graph analysis. However, MAGNN and GTN only use handcrafted methods to obtain the missing node attributes, resulting in poor model performance. Under the guidance of topological relations, the framework improves the performance of the model greatly by using the attention mechanism for attribute completion, and can effectively distinguish papers belonging to different research fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Case Study</head><p>In order to prove the superiority of the proposed framework, we use MAGNN as an example to compare and analyze the ACM dataset by combining different node attributes processing methods. The ACM dataset contains three types of nodes: paper, author, and subject. Only papers' attributes are directly derived from the dataset. We carry out the following five designs according to different processing methods of attributes, and compare the performance of the models through the node classification task. We also repeat the process 5 times and report the averaged Macro-F1 and Micro-F1.  The results are shown in Tables 5. As we can see, MAGNN-AC2 achieves the best performance. MAGNN-AC2 has an obvious improvement, i.e. 3.00%-4.24% and 1.23%-3.19% more accurate than MAGNN and MAGNN-onehot2. MAGNN-AC1 is the second best, right after MAGNN-AC2. So we believe that using the mean of the attribute vectors of the directly connected paper nodes as the attributes of the author nodes and the subject nodes will result in similar node attributes and reduce the effective information provided to the model. For any author node, not all paper nodes connected to it are equally important. At the same time, it is not advisable to simply assume that there is no relationship between node attributes. Therefore, the proposed framework learns different importance of neighbors and obtains better node attributes to improve performance by using the attention mechanism for attribute completion under the guidance of topological relations, as shown in results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Parameters Experiments</head><p>We investigate the sensitivity of parameters and report the scores of node classification (i.e., Macro-F1 and Micro-F1) on ACM dataset with the divided ratio of node attributes and weighted coefficient of completion loss in HGNN-AC. Each presented score is an average of the scores in different training proportions (explained in Section 5.4). We show the results in the form of line charts in Figure <ref type="figure" target="#fig_5">5</ref> .</p><p>(1) Divided ratio of node attributes α: We first test the effect of the divided ratio of node attributes. The result is shown in Figure <ref type="figure" target="#fig_5">5</ref> (a). We can see that with the growth of the divided ratio, the performance shows a trend of first rising and then slowly decreasing. This is because HGNN-AC needs a proper attribute divided ratio. Dropping too many attributes will result in insufficient completed attributes. Dropping too few attributes will result in biased loss calculations. (2) Weighted coefficient of completion loss λ: In order to achieve the best performance of the model, we test the effect of the divided ratio of node attributes. The result is shown in <ref type="bibr">Figure 5 (b)</ref>. We also find that with the growth of the weighted coefficient, the performance shows a trend of first rising and then slowly decreasing. This is reasonable. Too small coefficient of completion loss will weaken the guiding role of completion loss on the process of attribute completion, while too large coefficient will weaken the role of prediction loss. Therefore, we choose an appropriate proportion to achieve the best overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose HGNN-AC to solve the problem of missing attributes in heterogeneous graphs by a learnable manner. HGNN-AC includes two parts, pre-learning of topological embedding and attribute completion. We obtain node topological embedding by using HIN-Embedding methods. Then we use topological relationship between nodes as guidance to complete attributes for no-attribute nodes by weighted aggregation of the attributes of these attributed nodes with attention mechanism. Finally, we get an end-to-end model after combining the proposed framework with arbitrary HIN method. In experiments, we combine HGNN-AC with MAGNN and GTN model. The results on node classification demonstrate the superiority of the proposed framework over the state-of-the-arts.</p><p>The task on visualization shows the effectiveness of HGNN-AC.</p><p>The case study also demonstrates its good interpretability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The network schemas of IMDB and DBLP with incomplete attributes. The colored items represent node types to be analysed (classification, clustering). Only the movie nodes in IMDB and paper nodes in DBLP have raw attributes, while other types of nodes have no attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The overview of the proposed framework. Given a orginal heterogeneous graph with some types of nodes having no attributes as input, we first calculate node embeddings via network topology A. Then we randomly drop some attributes and perform attribute completion. The attribute completion process is essentially a weighted aggregation of directly connected neighbors' attributes, where weights are decided by the attention derived from node' embeddings. After attribute completion, we get a new heterogeneous graph with all node having attributes and send it to HINs model. Note that we have a completion loss between dropped attributes and reconstructed attributes. The combination of completion loss and model's prediction loss makes the whole model end-to-end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison between GTN and GTN-AC.</figDesc><graphic url="image-3.png" coords="8,57.95,186.53,113.69,69.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the paper nodes of embeddings in the ACM dataset. Different colors correspond to different research areas in ground truth.</figDesc><graphic url="image-7.png" coords="8,340.37,202.31,85.75,85.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )</head><label>1</label><figDesc>MAGNN: The attributes of paper nodes are bag-of-words representations of their keywords. The attribute vectors of author and subject nodes come from the mean of the attribute vectors of paper nodes they are directly related to. (2) MAGNN-onehot1: The attributes of paper nodes are bag-ofwords representations of their keywords. The attributes of author nodes are one-hot vectors. The attribute vectors of subject nodes come from the mean of the attribute vectors of the paper nodes directly related to them. (3) MAGNN-onehot2: The attributes of paper nodes are bag-ofwords representations of their keywords. The attributes of author and subject nodes are one-hot vectors. (4) MAGNN-AC1: The attributes of paper nodes are bag-ofwords representations of their keywords. The attributes of the author nodes are obtained by using the proposed framework to complete. The attributes of subject nodes are one-hot vectors. (5) MAGNN-AC2: The attributes of paper nodes are bag-ofwords representations of their keywords. The attributes of author and subject nodes are both obtained by using the proposed framework to complete.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Parameters Analysis.</figDesc><graphic url="image-9.png" coords="9,322.11,297.09,113.69,66.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notations and Explanations.</figDesc><table><row><cell>Notations</cell><cell>Explanations</cell></row><row><cell>V</cell><cell>The set of nodes</cell></row><row><cell>E</cell><cell>The set of edges</cell></row><row><cell>G</cell><cell>A heterogeneous graph</cell></row><row><cell>V +</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Datasets</cell><cell>Nodes</cell><cell>Edges</cell><cell>Attributes</cell></row><row><cell>DBLP</cell><cell># author(A):4057 # paper(P):14328 # term(T):7723 # venue(V):20</cell><cell># A-P:19645 # P-T:85810 # P-V:14328</cell><cell># A:handcrafted # P:raw # T:handcrafted # V:handcrafted</cell></row><row><cell></cell><cell># paper(P):4019</cell><cell># P-P:9615</cell><cell># P:raw</cell></row><row><cell>ACM</cell><cell># author(A):7167</cell><cell># P-A:13407</cell><cell># A:handcrafted</cell></row><row><cell></cell><cell># subject(S):60</cell><cell># P-S:4019</cell><cell># S:handcrafted</cell></row><row><cell>IMDB</cell><cell># movie(M):4278 # director(D):2081 # actor(A):5257</cell><cell># M-D:4278 # M-A:12828</cell><cell># M:raw # D:handcrafted # A:handcrafted</cell></row><row><cell cols="4">(5) HAN [39]: a heterogeneous GNN. This model is based on</cell></row><row><cell cols="4">the hierarchical attention mechanism. It generates node em-</cell></row><row><cell cols="4">bedding by aggregating attributes from meta-path based</cell></row><row><cell cols="3">neighbors in a hierarchical manner.</cell><cell></cell></row><row><cell>(</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results (%) on DBLP dataset on the node classification task.</figDesc><table><row><cell>Datasets</cell><cell>Metrics</cell><cell cols="7">Training Metapath2vec GCN GAT HetGNN HAN MAGNN MAGNN-AC</cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell>88.76</cell><cell>86.99 32.68</cell><cell>86.61</cell><cell>89.37</cell><cell>92.45</cell><cell>92.99</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell>90.49</cell><cell>89.03 57.20</cell><cell>90.60</cell><cell>90.83</cell><cell>92.44</cell><cell>93.64</cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell>91.09</cell><cell>89.53 64.57</cell><cell>91.09</cell><cell>91.24</cell><cell>92.44</cell><cell>93.80</cell></row><row><cell></cell><cell>Macro-F1</cell><cell>20%</cell><cell>91.50</cell><cell>90.06 66.92</cell><cell>91.72</cell><cell>91.69</cell><cell>92.53</cell><cell>93.92</cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell>92.55</cell><cell>90.37 73.23</cell><cell>92.03</cell><cell>91.84</cell><cell>92.97</cell><cell>94.06</cell></row><row><cell></cell><cell></cell><cell>60%</cell><cell>93.25</cell><cell>90.57 77.17</cell><cell>92.26</cell><cell>92.01</cell><cell>93.30</cell><cell>94.04</cell></row><row><cell>DBLP</cell><cell></cell><cell>80% 1%</cell><cell>93.48 89.91</cell><cell>90.74 78.20 87.55 48.74</cell><cell>92.39 87.73</cell><cell>92.15 90.12</cell><cell>93.77 93.11</cell><cell>94.22 93.51</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell>91.19</cell><cell>89.58 70.79</cell><cell>91.17</cell><cell>91.49</cell><cell>93.02</cell><cell>94.09</cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell>91.74</cell><cell>90.02 75.90</cell><cell>91.64</cell><cell>91.88</cell><cell>93.02</cell><cell>94.22</cell></row><row><cell></cell><cell>Micro-F1</cell><cell>20%</cell><cell>92.14</cell><cell>90.53 76.98</cell><cell>92.23</cell><cell>92.30</cell><cell>93.08</cell><cell>94.34</cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell>93.09</cell><cell>90.83 79.61</cell><cell>92.55</cell><cell>92.46</cell><cell>93.50</cell><cell>94.46</cell></row><row><cell></cell><cell></cell><cell>60%</cell><cell>93.76</cell><cell>91.01 81.62</cell><cell>92.79</cell><cell>92.65</cell><cell>93.83</cell><cell>94.46</cell></row><row><cell></cell><cell></cell><cell>80%</cell><cell>93.94</cell><cell>91.15 82.22</cell><cell>92.92</cell><cell>92.78</cell><cell>94.27</cell><cell>94.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results (%) on ACM and IMDB datasets on the node classification task.</figDesc><table><row><cell>Datasets</cell><cell>Metrics</cell><cell cols="7">Training Metapath2vec GCN GAT HetGNN HAN MAGNN MAGNN-AC</cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell>35.23</cell><cell cols="2">39.72 49.52</cell><cell>37.32</cell><cell>52.49</cell><cell>50.78</cell><cell>53.50</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell>42.37</cell><cell cols="2">42.95 53.08</cell><cell>42.70</cell><cell>56.16</cell><cell>54.28</cell><cell>57.94</cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell>44.29</cell><cell cols="2">43.70 53.61</cell><cell>45.68</cell><cell>57.02</cell><cell>56.39</cell><cell>58.69</cell></row><row><cell></cell><cell>Macro-F1</cell><cell>20%</cell><cell>46.42</cell><cell cols="2">44.75 54.81</cell><cell>48.92</cell><cell>57.61</cell><cell>58.11</cell><cell>59.67</cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell>47.70</cell><cell cols="2">45.26 55.09</cell><cell>51.61</cell><cell>57.75</cell><cell>59.39</cell><cell>60.18</cell></row><row><cell></cell><cell></cell><cell>60%</cell><cell>48.25</cell><cell cols="2">46.72 55.71</cell><cell>53.00</cell><cell>57.66</cell><cell>59.97</cell><cell>60.60</cell></row><row><cell>IMDB</cell><cell></cell><cell>80% 1%</cell><cell>48.73 39.55</cell><cell cols="2">47.13 55.40 44.01 51.32</cell><cell>53.24 38.62</cell><cell>57.23 54.38</cell><cell>60.02 51.62</cell><cell>60.75 54.74</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell>44.33</cell><cell cols="2">46.41 53.73</cell><cell>43.52</cell><cell>56.74</cell><cell>54.46</cell><cell>58.26</cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell>46.15</cell><cell cols="2">47.02 54.14</cell><cell>46.56</cell><cell>57.35</cell><cell>56.53</cell><cell>58.97</cell></row><row><cell></cell><cell>Micro-F1</cell><cell>20%</cell><cell>48.08</cell><cell cols="2">47.44 55.02</cell><cell>49.70</cell><cell>57.82</cell><cell>58.16</cell><cell>59.84</cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell>49.55</cell><cell cols="2">47.62 55.29</cell><cell>52.47</cell><cell>57.98</cell><cell>59.46</cell><cell>60.38</cell></row><row><cell></cell><cell></cell><cell>60%</cell><cell>50.06</cell><cell cols="2">48.49 55.91</cell><cell>53.91</cell><cell>57.87</cell><cell>60.05</cell><cell>60.79</cell></row><row><cell></cell><cell></cell><cell>80%</cell><cell>50.68</cell><cell cols="2">48.73 55.67</cell><cell>54.25</cell><cell>57.46</cell><cell>60.15</cell><cell>60.98</cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell>58.38</cell><cell cols="2">66.83 88.58</cell><cell>81.67</cell><cell>86.95</cell><cell>85.58</cell><cell>88.95</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell>66.20</cell><cell cols="2">72.45 89.20</cell><cell>86.49</cell><cell>88.65</cell><cell>86.12</cell><cell>90.31</cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell>67.81</cell><cell cols="2">70.79 89.29</cell><cell>88.20</cell><cell>89.39</cell><cell>86.60</cell><cell>90.29</cell></row><row><cell></cell><cell>Macro-F1</cell><cell>20%</cell><cell>69.95</cell><cell cols="2">70.41 89.59</cell><cell>89.16</cell><cell>90.01</cell><cell>88.01</cell><cell>91.51</cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell>71.15</cell><cell cols="2">70.82 89.77</cell><cell>90.14</cell><cell>90.82</cell><cell>89.42</cell><cell>92.75</cell></row><row><cell></cell><cell></cell><cell>60%</cell><cell>71.74</cell><cell cols="2">69.67 89.72</cell><cell>90.71</cell><cell>91.51</cell><cell>90.39</cell><cell>93.46</cell></row><row><cell>ACM</cell><cell></cell><cell>80% 1%</cell><cell>72.18 63.18</cell><cell cols="2">67.23 89.42 71.74 88.60</cell><cell>91.01 82.20</cell><cell>91.71 87.37</cell><cell>90.79 85.95</cell><cell>93.79 89.26</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell>68.88</cell><cell cols="2">74.95 89.10</cell><cell>86.55</cell><cell>88.62</cell><cell>86.24</cell><cell>90.48</cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell>70.29</cell><cell cols="2">74.10 89.19</cell><cell>88.18</cell><cell>89.32</cell><cell>86.67</cell><cell>90.43</cell></row><row><cell></cell><cell>Micro-F1</cell><cell>20%</cell><cell>72.12</cell><cell cols="2">74.02 89.47</cell><cell>89.12</cell><cell>89.89</cell><cell>88.08</cell><cell>91.64</cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell>73.17</cell><cell cols="2">74.57 89.65</cell><cell>90.11</cell><cell>90.73</cell><cell>89.48</cell><cell>92.90</cell></row><row><cell></cell><cell></cell><cell>60%</cell><cell>73.65</cell><cell cols="2">74.10 89.60</cell><cell>90.64</cell><cell>91.37</cell><cell>90.42</cell><cell>93.57</cell></row><row><cell></cell><cell></cell><cell>80%</cell><cell>74.14</cell><cell cols="2">72.69 89.29</cell><cell>90.93</cell><cell>91.56</cell><cell>90.80</cell><cell>93.87</cell></row><row><cell cols="5">to a linear support vector machine (SVM) [34] classifier with differ-</cell><cell cols="4">variance of graph-structured data can be very high, we repeat the</cell></row><row><cell cols="5">ent training ratios from 1% to 80%. Note that for a fair comparison,</cell><cell cols="4">process 5 times and report the averaged Macro-F1 and Micro-F1.</cell></row><row><cell cols="5">only the nodes in the testing set are fed to the linear SVM, because</cell><cell cols="2">As shown in</cell><cell></cell></row><row><cell cols="5">in semi-supervised experiments, labels in training set and testing</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">set have participated in the process of model training. Since the</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 ,</head><label>3</label><figDesc>MAGNN-AC performs best across different training ratios on the DBLP dataset after combining the proposed framework. On the DBLP dataset, researchers usually perform task analysis on author nodes with labels which have no raw attributes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results (%) of node classification on ACM dataset using different attributes.</figDesc><table><row><cell>Datasets</cell><cell>Metrics</cell><cell cols="6">Training MAGNN MAGNN-onehot1 MAGNN-onehot2 MAGNN-AC1 MAGNN-AC2</cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell>85.58</cell><cell>83.16</cell><cell>86.71</cell><cell>88.24</cell><cell>88.95</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell>86.12</cell><cell>85.00</cell><cell>87.24</cell><cell>89.60</cell><cell>90.31</cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell>86.60</cell><cell>85.86</cell><cell>87.76</cell><cell>89.61</cell><cell>90.29</cell></row><row><cell></cell><cell>Macro-F1</cell><cell>20%</cell><cell>88.01</cell><cell>88.14</cell><cell>89.66</cell><cell>90.86</cell><cell>91.51</cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell>89.42</cell><cell>89.87</cell><cell>91.13</cell><cell>92.18</cell><cell>92.75</cell></row><row><cell></cell><cell></cell><cell>60%</cell><cell>90.39</cell><cell>90.84</cell><cell>92.01</cell><cell>93.03</cell><cell>93.46</cell></row><row><cell>ACM</cell><cell></cell><cell>80% 1%</cell><cell>90.79 85.95</cell><cell>91.15 84.09</cell><cell>92.56 87.02</cell><cell>93.55 88.67</cell><cell>93.79 89.26</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell>86.24</cell><cell>85.31</cell><cell>87.29</cell><cell>89.87</cell><cell>90.48</cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell>86.67</cell><cell>86.08</cell><cell>87.77</cell><cell>89.89</cell><cell>90.43</cell></row><row><cell></cell><cell>Micro-F1</cell><cell>20%</cell><cell>88.08</cell><cell>88.30</cell><cell>89.66</cell><cell>91.12</cell><cell>91.64</cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell>89.48</cell><cell>89.98</cell><cell>91.17</cell><cell>92.42</cell><cell>92.90</cell></row><row><cell></cell><cell></cell><cell>60%</cell><cell>90.42</cell><cell>90.87</cell><cell>92.02</cell><cell>93.20</cell><cell>93.57</cell></row><row><cell></cell><cell></cell><cell>80%</cell><cell>90.80</cell><cell>91.17</cell><cell>92.53</cell><cell>93.70</cell><cell>93.87</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://dblp.uni-trier.de/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://dl.acm.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://www.imdb.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by National Natural Science Foundation of China (61772361, 61876128, 61972442), Key Research and Development Project of Hebei Province of China (20350802D), Natural Science Foundation of Hebei Province of China (F2020202040), and Natural Science Foundation of Tianjin of China (20JCYBJC00650).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications</title>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1616" to="1637" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Mithal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Gungor Polatkan</surname></persName>
		</author>
		<author>
			<persName><surname>Ramanath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02874</idno>
		<title level="m">An Attentive Survey of Attention Models</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Survey on Network Embedding</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="833" to="852" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno>NIPS. 2224-2232</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning</title>
		<author>
			<persName><forename type="first">Tao-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Chien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2331" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph Embedding Techniques, Applications, and Performance: A Survey</title>
		<author>
			<persName><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="78" to="94" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mining Heterogeneous Information Networks by Exploring the Power of Links</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>ALT. 3</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep Convolutional Networks on Graph-Structured Data</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno>WWW. 2704-2710</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-Attention Graph Pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Birds of a Feather: Homophily in Social Networks</title>
		<author>
			<persName><forename type="first">Mcpherson</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent Models of Visual Attention</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">struc2vec: Learning Node Representations from Structural Identity</title>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Filipe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigues</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">H P</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09769</idno>
		<title level="m">Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Survey of Heterogeneous Information Network Analysis</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="17" to="37" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Mining Heterogeneous Information Networks: A Structural Analysis Approach. SIGKDD Explor</title>
				<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Path-Sim: Meta Path-Based Top-K Similarity Search in Heterogeneous Information Networks. Proc. VLDB Endow</title>
				<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Support Vector Machines: A Nonlinear Modelling and Control Perspective</title>
		<author>
			<persName><forename type="first">Johan</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Control</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="311" to="327" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Attention Network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>WWW. 2022-2032</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hyperbolic Heterogeneous Information Network Embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5337" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Comprehensive Survey on Graph Neural Networks. IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph Transformer Networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<idno>NIPS. 11960-11970</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Neural Network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs</title>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="339" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Link Prediction Based on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5171" to="5181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph Neural Networks: A Review of Methods and Applications</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalizing Graph Neural Networks Beyond Homophily: Current Limitations and Effective Designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7793" to="7804" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
