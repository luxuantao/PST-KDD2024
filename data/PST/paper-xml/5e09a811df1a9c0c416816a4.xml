<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised 3D End-to-End Medical Image Registration with Volume Tweening Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tingfung</forename><surname>Lau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Luo</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Eric I-Chao</forename><surname>Chang</surname></persName>
							<email>echang@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">are with Institute for Interdis-ciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Software Development Environ-ment and Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="institution">Research Institute of Beihang University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised 3D End-to-End Medical Image Registration with Volume Tweening Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F738922DB58E433A08D9ADC09A2961F</idno>
					<idno type="DOI">10.1109/JBHI.2019.2951024</idno>
					<note type="submission">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Registration</term>
					<term>unsupervised</term>
					<term>convolutional neural networks</term>
					<term>end-to-end</term>
					<term>medical image</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D medical image registration is of great clinical importance. However, supervised learning methods require a large amount of accurately annotated corresponding control points (or morphing), which are very difficult to obtain. Unsupervised learning methods ease the burden of manual annotation by exploiting unlabeled data without supervision. In this paper, we propose a new unsupervised learning method using convolutional neural networks under an end-to-end framework, Volume Tweening Network (VTN), for 3D medical image registration. We propose three innovative technical components: (1) An end-to-end cascading scheme that resolves large displacement;</p><p>(2) An efficient integration of affine registration network; and</p><p>(3) An additional invertibility loss that encourages backward consistency. Experiments demonstrate that our algorithm is 880x faster (or 3.3x faster without GPU acceleration) than traditional optimization-based methods and achieves state-of-theart performance in medical image registration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image registration is the process of finding the non-linear spatial correspondence between the input images (see Figure <ref type="figure" target="#fig_1">1</ref>). It has a wide range of applications in medical image processing, such as aligning images of one subject taken at different times. Another example is to match an image of one subject to some predefined coordinate system, such as an anatomical atlas <ref type="bibr" target="#b0">[1]</ref>.</p><p>Over recent decades, many traditional algorithms have been developed and studied to register medical images <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>,  An image (d) similar to the fixed one can be produced by warping the moving image with the flow. The images are rendered by mapping grayscale to white with transparency (the more intense, the more opaque) and dimetrically projecting the volume.</p><p>[4], <ref type="bibr" target="#b4">[5]</ref>. Most of them define a hypothesis space of possible transforms, often described by a set of parameters, and a metric of the quality of such a transform, and then find the optimum by iteratively updating the parameters. Traditional methods have achieved good performance on several datasets, and are state-of-the-art, but their registration speed is barely practical for clinical applications. These methods do not exploit the patterns that exist in the registration task. In contrast, learningbased methods are generally faster. Computationally, they do not need to iterate over and over before producing the result. Conceptually, they learn those patterns represented by the parameters of the underlying learning model. The trained model is an efficient replacement of the optimization procedure. However, there has not been an effective approach to generate ground-truth fields for medical images. Widely used supervised methods require accurately labeled ground truth with a vast number of instances. The quality of these labels directly affects the result of supervision, which entails much effort in traditional tasks such as classification and segmentation. But flow fields are dense and ambiguous quantities that are almost impossible to be labeled manually, and moreover, automatically generated dataset (e.g., the Flying Chairs dataset <ref type="bibr" target="#b5">[6]</ref>) which deviates from the realistic demands is not appropriate. Consequently, supervised methods are hardly applicable. In contrast, unlabeled medical images are universally available, and sufficient to advance the state of the art through our unsupervised framework shown in this paper.</p><p>Optical flow estimation is a closely related problem that aims to identify the correspondence of pixels in two images of the same scene taken from different perspectives. FlowNet <ref type="bibr" target="#b5">[6]</ref> and its successor FlowNet 2.0 <ref type="bibr" target="#b6">[7]</ref> are CNNs that predict optical flow from input images using fully convolutional networks (FCN <ref type="bibr" target="#b7">[8]</ref>), which are capable of regressing pixellevel correspondence. FlowNet is trained on Flying Chairs, a synthetic dataset that consists of images generated using computer graphics algorithms and the ground-truth flows.</p><p>Figure <ref type="figure">2</ref>: Illustration of the overall structure of Volume Tweening Network (VTN) and how gradients back-propagate. Every registration subnetwork is responsible for finding the deformation field between the fixed image and the current moving image. The moving image is repeatedly warped according to the deformation field and fed into the next level of cascaded subnetworks. The current moving images are compared against the fixed image for a similarity loss function to guide training. There is also regularization loss, but not drawn for the sake of cleanness. The number of cascaded subnetworks may vary; only two are illustrated here. In the figure, yellow (lighter) indicates the similarity loss between the first warped image and the fixed image, and blue (darker) that between the second warped image and the fixed image. Solid bold arrows indicate how the loss is computed, and dashed bold arrows indicate how gradients back-propagate. Note that the second loss will propagate gradients to the first subnetwork as a consequence of the first warped image being a differentiable function of the first subnetwork.</p><p>Spatial Transformer Networks (STN) <ref type="bibr" target="#b8">[9]</ref> is a component in neural networks that spatially transforms feature maps to ease back-end tasks. It learns a localization net to produce an appropriate transformation to "straighten" the input image. The localization net is learnt without supervision, though back-end task might be supervised. Given a sampling grid and a transformation, STN applies the warping operation and outputs the warped image for further consumption by deeper networks. The warping operation deals with off-grid points by multi-linear interpolation, hence is differentiable and can back-propagate gradients.</p><p>Inspired by FlowNet and STN, we propose Volume Tweening Network (VTN), which enables the unsupervised training of end-to-end CNNs that perform voxel-level 3D medical image registration (see Figure <ref type="figure">2</ref>). The moving image is registered and warped, and the warped image is compared against the fixed image to form a similarity loss. There is a rich body of research into similarity losses <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The model is trained to minimize a combination of regularization loss and similarity loss. As the method is unsupervised, the performance potentially increases as it is trained with more unlabeled data. The network consists of several cascaded subnetworks, the number of which might vary, and each subnetwork is responsible for producing a transform that aligns the fixed image and the moving one. Deeper layers register moving images warped according to the output of previous layers with the initial fixed image. The final prediction is the composition of all intermediate flows. It turns out that network cascading significantly improves the performance in the presence of large displacement between the input images. While the idea of cascading subnetworks is found in FlowNet 2.0, our approach does not include as much artificial intervention of network structures as FlowNet 2.0 does. Instead, we employ a natural dichotomy in a subnetwork structure consisting of affine and deformable registration processes, which is also found in traditional methods, including ANTs (affine and SyN for deformable) <ref type="bibr" target="#b3">[4]</ref> and Elastix (affine and B-spline for deformable) <ref type="bibr" target="#b4">[5]</ref>. Besides these structural innovations, we also introduce the invertibility loss to 3D medical image registration, which encourages backward consistency while achieving better accuracy. Compared with traditional optimization-based algorithms, ours is 880x faster (or 3.3x faster without GPU acceleration) and achieves state-of-the-art performance.</p><p>To summarize, we present a new unsupervised end-toend learning system using convolutional neural networks for deformation field prediction between 3D medical images. In this framework, we develop 3 technical components: (1) We cascade the registration subnetworks, which improves performance for registering largely displaced images without much slow-down; (2) We integrate affine registration into our network, which proves to be effective and faster than using a separate tool; (3) We incorporate an additional invertibility loss into the training process, which improves registration performance. The contributions of this work are closely related. An unsupervised approach is very suitable for this problem, as the images are abundant and the ground truth is costly to acquire. The use of the warping operation is crucial to our work, providing the backbone of unsupervision, network cascading and invertibility loss. Network cascading further allows us to plug in different subnetworks, and in this case, the affine registration subnetwork and the deformable ones, enabling us to adapt the natural structure of multiple stages in image registration. The efficient implementation of our algorithm gives a satisfactory speed. The proposed VTN is also used as a building block in <ref type="bibr" target="#b12">[13]</ref>, which thoroughly exploits the idea of deep cascades and the code is publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Traditional Algorithms</head><p>Previously, there has been much effort on automating image registration. Tools like FAIR <ref type="bibr" target="#b13">[14]</ref>, ANTs <ref type="bibr" target="#b3">[4]</ref> and Elastix <ref type="bibr" target="#b4">[5]</ref> 1 https://github.com/microsoft/Recursive-Cascaded-Networks have been developed for automated image registration <ref type="bibr" target="#b0">[1]</ref>. Generally, these algorithms define a space of transformations and a metric of alignment quality, and then find the optimal transformation by iteratively updating the parameters. The optimization process is highly time-consuming, rendering such methods impractical for clinical applications.</p><p>The transformation space can be either parametric or nonparametric. Affine transforms can be described by only a few real numbers, whereas a free-form dense deformable field specifies the displacement for each grid point. Though the latter contains all possible transforms, it is common to apply a multi-stage approach to the problem. For example, ANTs <ref type="bibr" target="#b3">[4]</ref> registers the input images with a rigid transform, then a general affine transform, and finally a deformable transform modeled by SyN. In this paper, we also have components for affine/deformable registration, which are modeled by neural networks.</p><p>Selecting an informative metric is crucial to registration quality. The metrics, also called loss functions, often consist of two parts, one measuring the level of correspondence between the input images implied by the transform, the other regularizing the transform itself. Examples of the former part include photo-metric difference, correlation coefficient and mutual information <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> among others <ref type="bibr" target="#b9">[10]</ref>. Some of these measures, notably mutual information, require binning or quantizing, which makes gradients vanish thus continuous optimization techniques such as gradient descent inapplicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Supervised Learning Methods</head><p>Lee et al. <ref type="bibr" target="#b14">[15]</ref> employ Support Vector Machines (SVM) to learn the similarity measure for multi-modal image registration for brain scans. The training process of their algorithm requires pre-aligned image pairs thus the method is supervised. There have been some works tackling medical image registration with CNNs <ref type="bibr" target="#b15">[16]</ref>. Sokooti et al. <ref type="bibr" target="#b16">[17]</ref> develop a patch-based CNN to register chest CT scans and trains it with synthetic data. Miao et al. <ref type="bibr" target="#b17">[18]</ref> use CNN to perform 2D/3D registration, in which CNN regressors are used to estimate transformation parameters. Their CNNs are trained over synthetic data and the method is not end-to-end. FlowNet <ref type="bibr" target="#b5">[6]</ref>, developed by Dosovitskiy et al., is an FCN <ref type="bibr" target="#b7">[8]</ref> for optical flow prediction. The network is trained with synthetic data and estimates pixellevel correspondence.</p><p>While supervised learning methods achieve good performance, either abundant groud-truth alignment must be available, or synthetic data are used. Generation of synthetic data has to be carefully designed so that the generated data resemble the real ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Unsupervised Learning Methods</head><p>In an earlier work towards fast and accurate medical image registration by Shan et al. <ref type="bibr" target="#b18">[19]</ref>, an unsupervised end-to-end learning-based method for deformable medical image registration is proposed. The method in <ref type="bibr" target="#b18">[19]</ref> registers 2D images. It is evaluated by registering corresponding sections of MR brain images and CT liver scans. Some later work directly performs 3D registration <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, fully exploiting the information available from the 3D images. They aim to learn sparse parameters introduced by the traditional algorithms, which probably limits their performance. de Vos et al. <ref type="bibr" target="#b20">[21]</ref> also try to stack multiple networks and investigate an affine network, however, each of them is trained independently and separately. Their method is not end-to-end and only achieves a comparable performance to their baseline methods. Another work <ref type="bibr" target="#b21">[22]</ref> proposes to train a multi-step affine network followed by a momentum generator network. Although their framework is claimed to be "end-to-end", each stage is still trained separately after fixing the previous ones. In contrast, with the help of jointly trained cascaded subnetworks, the affine stage is naturally integrated into our framework, rather than done or trained out-of-band in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>VoxelMorph, proposed by Balakrishnan et al. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, is an unsupervised learning-based method for 3D medical image registration that predicts a dense deformation field. Another recently proposed unsupervised method by Krebs et al. <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, based on a low-dimensional probablistic model, performs comparably to VoxelMorph. VoxelMorph contains an encoderdecoder structure, uses warping operation to produce warped moving images, and is trained to minimize the dissimilarity between the warped image and the fixed image. Their method does not consider affine registration and assumes the input images are already affinely aligned, whereas ours embeds the process of affine registration as an integrated part of the network. Furthermore, their algorithm does not work well when large displacement between the images is present, which is common for liver CT scans. Finally, VoxelMorph is designed to take any two images as input, but <ref type="bibr" target="#b22">[23]</ref> only evaluates it in the atlas-based registration scenario. Consider a clinical scenario where the brain of a patient is captured before and after an operation. It would be better, in terms of accuracy and convenience, to register these two images, instead of registering both to an atlas.</p><p>It is noticeable that all the unsupervised methods use the warp operation to train networks without supervision. This paper exploits the operation in a trio (namely enabling unsupervised training, enabling cascading, and implementing invertibility loss, which will be detailed later). We present a more sophisticated and robust design that works well in the presence or absence of large displacement, and we evaluate the methods for general registration among images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD A. Problem Formulation</head><p>The input of an image registration problem consists of two images I 1,2 , both of which are functions Ω → R c , where Ω is a region of R n and c denotes the number of channels. Since this work focuses on 3D medical image registration, we confine ourselves to the case where Ω is a cuboid and c = 1 (grayscale image). Specifically, this means Ω ⊆ R 3 and each image is a function Ω → R. The objective of image registration is to find a displacement field (or flow field) f 12 : Ω → R 3 so that</p><formula xml:id="formula_0">I 1 (x) ≈ I 2 (x + f (x)) ,<label>(1)</label></formula><p>where the precise meaning of "≈" depends on specific application. The field f 12 is called the flow from I 1 to I 2 since it tells where each voxel in I 1 is in I 2 . We define warp (I 2 , f ) as the image I 2 warped according to f , i.e., warp (I 2 , f ) (x) = I 2 (x + f (x)). The above objective can be rephrased as finding f maximizing the similarity between I 1 and warp (I 2 , f ).</p><p>The image I 1 is also called the fixed image, and I 2 the moving one. The term "moving" suggests that the image is transformed during the registration process.</p><p>Consider warping an image twice, first with g 1 then with g 2 . What this procedure produces is</p><formula xml:id="formula_1">warp (warp (I, g 1 ) , g 2 ) (x) = warp (I, g 1 ) (x + g 2 (x)) = I (x + g 2 (x) + g 1 (x + g 2 (x))) = warp (I, g 2 + warp (g 1 , g 2 )) (x) .</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>This motivates the definition of the composition of two flows.</p><p>If we define the composition of the flow fields g 1 , g 2 to be</p><formula xml:id="formula_3">g 1 g 2 = g 2 + warp (g 1 , g 2 ) ,<label>(3)</label></formula><p>Equation ( <ref type="formula" target="#formula_2">2</ref>) can be restated as</p><formula xml:id="formula_4">warp (warp (I, g 1 ) , g 2 ) = warp (I, g 1 g 2 ) .<label>(4)</label></formula><p>It is noticeable that the warp operation in the above formulation should be further specified in practice. Real images as well as flow fields are only defined on lattice points. We continuate them onto the enclosing cuboid by trilinear interpolation as done in <ref type="bibr" target="#b26">[27]</ref>. Furthermore, we deal with out-of-bound indices by nearest-point interpolation. That is, to evaluate a function defined on lattice points at any point x, we first move x to the nearest in the enclosing cuboid of those lattice points, then interpolate the value from the 8 nearest lattice points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unsupervised End-to-End Registration Network</head><p>Our network, called Volume Tweening Network (VTN), consists of several cascaded registration subnetworks, after each of which the moving image is warped. The unsupervised training of network parameters is guided by the dissimilarity between the fixed image and each of the warped images, with the regularization losses on the flows predicted by the subnetworks.</p><p>The warping operation, also known as the sampler in STN <ref type="bibr" target="#b26">[27]</ref>, is differentiable to both the input image and the input flow field due to the trilinear interpolation. Those warping operations can back-propagate gradients to all the preceding subnetworks, which is critical for the end-to-end learning of our cascaded networks.</p><p>In deformable image registration, it is common to apply an initial rigid transformation as a global alignment before predicting the dense flow field. Instead of prepending a timeconsuming preprocessing stage with a tool like ANTs <ref type="bibr" target="#b3">[4]</ref> as done in VoxelMorph <ref type="bibr" target="#b22">[23]</ref>, we integrate this procedure as a top-level subnetwork. Our affine registration subnetwork predicts a set of affine parameters, after which a flow field can be generated for warping. The integrated affine registration subnetwork not only works in negligible running time, but also outperforms the traditional affine stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Functions</head><p>To train our model in an unsupervised manner, we measure the (dis)similarity between the moving images warped by the spatial transformer and the fixed image. Regularization losses are introduced to prevent the flow fields from being unrealistic or overfitting. We use correlation coefficient as the similarity measurement, and the total variation loss as the regularization term for dense flow predictions. Furthermore, we introduce orthogonality loss and determinant loss as the regularization terms for the affine registration subnetwork, which are essential in preventing the gradients from exploding and ensuring training stability. Those loss functions are discussed as follows.</p><p>a) Correlation Coefficient: The covariance between I 1 and I 2 is defined as</p><formula xml:id="formula_5">Cov [I 1 , I 2 ] = 1 |Ω| x∈Ω I 1 (x) I 2 (x)- 1 |Ω| 2 x∈Ω I 1 (x) y∈Ω I 2 (y),<label>(5)</label></formula><p>where Ω denotes the cuboid (or grid) on which the input images are defined. Their correlation coefficient is defined as</p><formula xml:id="formula_6">CorrCoef [I 1 , I 2 ] = Cov [I 1 , I 2 ] Cov [I 1 , I 1 ] Cov [I 2 , I 2 ] .<label>(6)</label></formula><p>The images are regarded as random variables whose sample space is the points on which voxel values are available. The range of correlation coefficient is [-1, 1], it measures how much the two images are linear related, and attains ±1 if and only if the two are linear function of each other. Applying a non-degenerate linear function to any of the images does not change their correlation coefficient, therefore, this measure is more robust than L 2 loss. For real-world images, the correlation coefficient should be non-negative (unless one of the images is a negative film). The correlation coefficient loss is defined as</p><formula xml:id="formula_7">L Corr (I 1 , I 2 ) = 1 -CorrCoef [I 1 , I 2 ] .<label>(7)</label></formula><p>b) Total Variation Loss (Smooth Term): For a dense flow field, we regularize it with the following loss that discourages discontinuity:</p><formula xml:id="formula_8">L TV = 1 3|Ω| x 3 i=1 (f (x + e i ) -f (x)) 2 ,<label>(8)</label></formula><p>where e 1,2,3 form the natural basis of R 3 . This varies from the initial definition of the total variation loss <ref type="bibr" target="#b27">[28]</ref> (which includes a square root), but our formula is more natural as a loss term (referring to the L2 regularization). c) Orthogonality Loss: For the specific task discussed in this paper (medical image registration), it is usually the case that the input images need only a small scaling and a rotation before they are affinely aligned. We would like to penalize the network for producing overly non-rigid transform. To this end, we introduce a loss on the non-orthogonality of I +A, where I denotes the identity matrix and A denotes the transform matrix produced by the affine registration network (see Section IV-B for more details). Let λ 1,2,3 be the singular values of I + A, the orthogonality loss is</p><formula xml:id="formula_9">L ortho = -6 + 3 i=1 λ 2 i + λ -2 i . (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>The motivation of this formula is mainly that a matrix is orthogonal if and only if all its singular values are 1. Hence, the more deviant I + A is from being an orthogonal matrix (i.e., the more its singular values deviating from ones), the larger its orthogonality loss. If I + A is orthogonal, the value will be zero.</p><p>Computing orthogonality loss involves singular values of I + A. The square of those singular values are exactly the eigenvalues of (I + A)</p><p>T (I + A). Since the loss is a symmetric function of those eigenvalues, it can be rewritten as a fraction w.r.t. the coefficients of the characteristic polynomial of (I + A) T (I + A) by Viète's theorem. Then the derivatives could be directly calculated. d) Determinant Loss: We assume images are taken with the same chirality, therefore, an affine transform involving reflection is not allowed. This imposes the requirement that det (I + A) &gt; 0. Together with the orthogonality requirement, we set the determinant loss to be</p><formula xml:id="formula_11">L det = (-1 + det (A + I)) 2 , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where I denotes the identity matrix and A denotes the transform matrix produced by the affine registration network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NETWORK ARCHITECTURE A. Cascading</head><p>Each subnetwork is responsible for aligning the fixed image and the current moving image. Following each subnetwork, the moving image is warped with the predicted flow, and the warped image is fed into the next cascaded subnetwork. The flow fields are composed to produce the final estimation. Figure <ref type="figure">2</ref> illustrates how the networks are cascaded, how the images are transformed and how each part contributes to the loss. With all layers being differentiable, gradients can backpropagate to the whole system and enable the unsupervised end-to-end learning.</p><p>It might be tempting to compare our scheme with that of FlowNet 2.0 <ref type="bibr" target="#b6">[7]</ref>. FlowNet 2.0 stacks subnetworks in a different way than our method. It performs two separate lines of flow estimations (large/small displacement flows) and fuses them into the final estimation. Each of its intermediate subnetworks has inputs of not only the warped moving image and the fixed image, but also the initial moving image, the current flow, and the brightness error. Its subnetworks are carefully crafted, having similar yet different structures and expected to solve specific problems (e.g., large/small displacement) in flow estimation. In contrast, our method does not involve two separate lines of registration, i.e., each subnetwork works on the fixed image and the warped moving image produced by the previous one, and intermediate subnetworks do not get more input than the initial one. We do not interfere much with the structures of subnetworks. Despite the initial affine subnetwork, we do not assign specific tasks to the remaining subnetworks since they share the same structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Affine and Dense Deformable Subnetworks</head><p>The affine registration subnetwork aims to align the input image with an affine transform. It is only used as our first subnetwork. As illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, the input is downsampled  by strided 3D convolutions, and finally a fully-connected layer is applied to produce 12 numeric parameters as output, which represents a 3 × 3 transform matrix A and a 3-dimensional displacement vector b. As a common practice, the number of channels doubles as the length of resolution halves. The flow field produced by this subnetwork is defined as</p><formula xml:id="formula_13">f (x) = Ax + b. (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>The moving image is then transformed according to the output affine parameters and fed into the subsequent deformable registration subnetworks. The dense deformable registration subnetwork is used as all subsequent subnetworks, each of which refines the registration based on the output of the subnetwork preceeding it. It follows an encoder-decoder architecture, as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, which is commonly used for dense prediction. We use strided 3D convolution to progressively downsample the image, and then use deconvolution (transposed convolution) <ref type="bibr" target="#b7">[8]</ref> to recover spatial resolution. As suggested in U-Net <ref type="bibr" target="#b28">[29]</ref>, skip connections between the convolutional layers and the deconvolutional layers are added to help refining dense prediction. The subnetwork will output the dense flow field, a volume feature map with 3 channels (x, y, z displacements) of the same size as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Invertibility</head><p>Given two images I 1,2 , going from a voxel in I 1 to the corresponding voxel in I 2 then back to I 1 should give zero displacement. Otherwise stated, the registration should be round-trip. In Figure <ref type="figure" target="#fig_4">5</ref>, we demonstrate the possible situations. The pair of solid arrows exemplifies round-trip registration, whereas the pair of dashed arrows exemplifies non-round-trip registration. If we have computed two flow fields (back and forth), f 12 and f 21 the composed fields exhibit the roundtrip behavior of the registration, as illustrated by the magenta We capture the round-tripness for a pair of images with the invertibility loss, namely</p><formula xml:id="formula_15">L inv = f 12 f 21 2 2 + f 21 f 12 2 2 . (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>The larger the invertibility loss, the less round-trip the registration. For perfectly round-trip registration, the invertibility loss is zero. We come up with, formulate, and implement the invertibility loss independently of <ref type="bibr" target="#b29">[30]</ref>. We use L2 invertibility loss whereas <ref type="bibr" target="#b29">[30]</ref> uses L1 left-right disparity consistency loss, which is just a matter of choice. We are the first to incorporate the invertibility loss into 3D images to boost performance on medical image tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT</head><p>We evaluate our algorithm with extensive experiments on both liver CT datasets and brain MRI datasets. The models are trained separately for the two types of images. We train and test the models for pairwise, subject-to-subject registration. This setting is more general than the atlas-based registration as studied in VoxelMorph <ref type="bibr" target="#b22">[23]</ref>, where all moving images are assumed to be registered to the fixed atlas.</p><p>The input images to the models are assumed to be of size 128 3 . We apply spatial augmentation during training, where the input images are transformed by random B-Spline <ref type="bibr" target="#b30">[31]</ref> fields of 5 × 5 × 5 control points within a maximum displacement of 12.</p><p>We compare our algorithm against state-of-the-art traditional registration algorithms including ANTs 2 <ref type="bibr" target="#b3">[4]</ref> and Elastix 3 <ref type="bibr" target="#b4">[5]</ref>, as well as VoxelMorph <ref type="bibr" target="#b22">[23]</ref>. Our algorithm achieves stateof-the-art performance while being much faster. Our experiments prove that the performance of our unsupervised method is improved as more unlabeled data are used in training. We show that cascading subnetworks significantly improves the 3 The command we use for Elastix: -f &lt;Fixed&gt; -m &lt;Moving&gt; -out &lt;OutFileSpec&gt; -p Affine -p BSpline_1000 performance, and that integrating affine registration into the method is effective.</p><p>We evaluate the performance of algorithms with the following metrics:</p><p>• Seg. IoU is the Jaccard coefficient between the warped liver segmentation and the ground truth. We warp the segmentation of the moving image by the predicted deformable field, and compute the Jaccard coefficient of the warped segmentation with the ground-truth segmentation of the fixed image. "IoU" means "intersection over union", i.e., |A∩B| |A∪B| , where A, B are the set of voxels the organ consists of.</p><p>• Lm. Dist. is the average distance between warped landmarks (points of anatomical interest) and the ground truth. • Time is the average time taken for each pair of images to be registered. Some methods are implemented with GPU acceleration, therefore there are two versions of this metric (with or without GPU acceleration).</p><p>Our model is defined and trained using TensorFlow <ref type="bibr" target="#b31">[32]</ref>. We accelerate training with nVIDIA TITAN Xp and CUDA 8.0. We use the Adam optimizer <ref type="bibr" target="#b32">[33]</ref> with the default parameters in TensorFlow 1.4. The batch size is 8 pairs per batch. The initial learning rate is 10 -4 and halves every epoch after the 4 th epoch. Each epoch consists of 20000 batches and the number of epochs is 5. Performance evaluation uses the same GPU model. Traditional methods work on CPU. We also test neuralnetwork-based methods with GPU acceleration disabled for a fairer comparison of speed. The CPU model used is Intel R Xeon R CPU E5-2690 v4 @ 2.60GHz (14 Cores).</p><p>A. Experiments on Liver Datasets 1) Settings: The input to the algorithms are liver CT scans of size 128 3 . Affine subnetworks downsample the images to 4 3 before applying the fully-connected layer. Dense deformable subnetworks downsample the images to 2 3 before doing transposed deconvolution.</p><p>We cascade up to 4 registration subnetworks. The reason we need to cascade multiple subnetworks is that large displacement is very common among CT liver scans and that with more subnetworks, images with large displacement can be progressively aligned. Among these networks, the one with one affine registration subnetwork and three dense deformable registration networks (referred to as "ADDD") is used to be trained with different amount of data and compared with other algorithms. We use the correlation coefficient as our similarity loss, the orthogonality loss and the determinant loss as regularization losses for the affine subnetwork, and the total variation loss as that for dense deformable subnetworks. The ratio of losses for "ADDD" is listed in Table <ref type="table" target="#tab_0">I</ref>. The performance is not very sensitive to the choice of hyperparameters, since each of the dense deformable subnetworks can automatically learn to progressively align the images, and only the final subnetwork and the affine subnetwork need to be trained with similarity loss.</p><p>2) Datasets: We have three datasets available:  <ref type="bibr" target="#b34">[35]</ref> consists of 20 volumes with liver segmentation ground truth. We choose 4 points of anatomical interest as the landmarks <ref type="foot" target="#foot_0">4</ref> and ask 3 expert doctors to annotate them, taking their average as the ground truth. This dataset is used as the test data. All pairs of images in the mixture of LITS and BFH are sampled into mini-batches for training. All image pairs in MICCAI are tested during evaluation. We crop raw liver CT scans to a volume of size 128 3 around the liver, and normalize them by adjusting exposure so that the histograms of the images match each other. The preprocessing stage is necessary as the images come from different sources.</p><p>3) Comparison among Methods: In Table <ref type="table" target="#tab_1">II</ref>, "ADDD" is our model detailed in Section V-A1, and "ADDD + inv" is that model trained with additional term of invertibility loss in the central area (the beginning and the ending quaters of each side are removed) with relative weight 10 -3 . Learning-based methods (VTN and VoxelMorph) are trained on LITS and BFH datasets. All methods are evaluated on the MICCAI dataset. To prove the effectiveness of our unsupervised method, we also train "ADDD" supervised (the row "supervised"), where the output of ANTs is used as the ground truth (using end-point error <ref type="bibr" target="#b5">[6]</ref>  VoxelMorph-2 is trained (using code released by <ref type="bibr" target="#b22">[23]</ref>) with a batch size of 4 and an iteration count of 5000. Its performance with a batch size of 8 or 16 is worse so a batch size of 4 is used. After 5000 iterations, the model begins to overfit. The reason that it does not perform well on liver datasets might be that it is designed for brain registration thus cannot handle large displacement.</p><p>The results in Table <ref type="table" target="#tab_1">II</ref> show the vast speed-up of learningbased methods against optimization-based methods. The Wilcoxon signed-rank test indicates that our methods significantly surpass state-of-the-art registration algorithms in terms of both Segmentation IoU and Landmark Distance.</p><p>Table <ref type="table" target="#tab_3">III</ref> presents the smoothness metrics (as done in <ref type="bibr" target="#b20">[21]</ref>) of our method compared to the traditional methods. It is not surprising that our significantly better performance comes at the price of worse deformation smoothness, but the fraction of folding is still well acceptable (less than 1%). The invertibility loss has a considerable impact on reducing the fraction of folding.  Figure <ref type="figure" target="#fig_6">6</ref> compares different methods listed in Table <ref type="table" target="#tab_1">II</ref>, where three landmarks are selected and the sections of the volumes at the height of each landmark in the fixed image are rendered. This means the red crosses (landmarks in the moving and warped images) indicate the projections of the landmarks onto those planes. It should be noted that though the sections of the warped segmentations seem to be less overlapping with those of the fixed one, the Segmentation IoU is computed for the volume and not the sections. It might well be the case that the overlap is not so satisfactory when viewed from those planes yet is better when viewed as a volume. Similarly, overlapping red and yellow crosses do not necessarily imply overlapping fixed and warped landmarks as they might deviate along z-axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In Figure <ref type="figure" target="#fig_7">7</ref>, we compare ADDD + inv, ADDD, ADD, AD and D. It shows that network cascading better aligns the images with the presence of large displacement, while the invertibility loss has a remarkable effect on the liver boundary.</p><p>4) Performance with Different Amount of Data: In Table IV, the "ADDD" network (see Section V-A1) trained with full data (LITS + BFS) is compared with that trained with only a part of data. The result demonstrates that training with more unlabeled data improves the performance. Since we do not need any annotations during the training phase, there are abundant clinical data ready for use.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Network Cascading:</head><p>In Table <ref type="table">V</ref>, all networks are trained on LITS + BFH. The models whose name does not include "A" have the affine registration subnetwork removed, and the number of "D"s is the number of dense deformable registration subnetworks. (See Section V-A1 for "ADDD".) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Brain Datasets</head><p>a) Settings: The input to the algorithms are brain MR images of size 128 3 . For some experiments, the selection of which will be detailed later, the brain scans are preprocessed to be aligned to a specific atlas from LONI <ref type="bibr" target="#b35">[36]</ref>. We use ANTs for this purpose. There are two reasons we align the brain scans with ANTs. One is that VoxelMorph <ref type="bibr" target="#b22">[23]</ref> requires the input to be affinely registered. The other is that we will compare the performance between "ANTs + deformable registration subnetworks" and "affine registration subnetwork + deformable registration subnetworks", i.e., to compare the effectiveness of integrating our affine registration subnetwork in place of ANTs.</p><p>The following methods will have ANTs-affine-aligned images as input: VoxelMorph methods and VTN without "A" (i.e., "D", "DD" and "DDD"). A more precise naming is "ANTs + VoxelMorph" and "ANTs + VTN". The following methods will not have ANTs-affine-aligned images as input: ANTs, Elastix, VTN with "A" (i.e., "AD", "ADD" and "ADDD"). Those methods have affine registration integrated. The comparison inside the former group focuses on dense deformable registration performance, that inside the latter group on overall performance, and that among the two groups benchmarks affine registration subnetwork versus ANTs affine registration in the context of a complete registration pipeline.</p><p>We will show 3 sets of comparisons similar to those for liver datasets. In the tables listed in later paragraphs, the time (with or without GPU) does not include the preprocessing with ANTs even if it is used. Preprocessing an image with ANTs costs 73.94 seconds on average. We will mention this fact again when such emphasis is needed.</p><p>Care should be taken when evaluating methods with ANTs affine alignment. For the data to be comparable with those with affine registration integrated, the fixed image should be equivalent. Methods with ANTs affine alignment have both moving and fixed images aligned to an atlas. Those with integrated affine registration never move the fixed image. The affine transform produced by ANTs might not be orthogonal, which is the source of unfair comparison. If the affine transform is shrinking, methods with ANTs affine alignment gain advantage. Otherwise, with integrated affine alignment do.</p><p>One measure, Segmentation IoU, is not affected, because the volumes of all objects get multiplied by the determinant of the affine transform and the evaluation measure is homogeneous. For Landmark Distance, we perform the inverse of the linear part of the affine transform (which aligns the fixed image to the atlas) to the difference vector between warped landmark and landmark in the (aligned) fixed image, so that the length goes back to the coordinate defined by the original fixed image. This way, we minimize loss of precision to prevent unfairly underevaluating methods with ANTs affine alignment. Speaking of the actual data, the affine transformations produced by ANTs are slightly shrinking. Our correction restores a fair comparison among all methods.</p><p>b) Datasets: We use volumes from the following datasets for training:</p><p>• ADNI <ref type="bibr" target="#b36">[37]</ref> (67 volumes); • ABIDE-1 <ref type="bibr" target="#b37">[38]</ref> (318 volumes): part of data from ABIDE;</p><p>• ABIDE-2 <ref type="bibr" target="#b37">[38]</ref> (1101 volumes): the rest from ABIDE;</p><p>• ADHD <ref type="bibr" target="#b38">[39]</ref> (973 volumes). We acquire the second part of ABIDE after a while when the first part was downloaded and processed, thus the split. This only helps us to understand how performance improves as more data are used for training. For comparison among different methods, it is always the case that all the data mentioned above are used for training. All pairs of images in the mixture of those datasets are sampled into mini-batches for training.</p><p>Raw MR scans are cropped to 128 3 around the brain. The scans are normalized based on the histograms of their foreground color distribution, which might vary because they are captured on different sites.</p><p>For evaluation, we use 20 volumes from the LONI Probabilistic Brain Atlas (LPBA40) <ref type="bibr" target="#b35">[36]</ref>. LONI consists of 40 volumes, 20 of which have tilted head positions and are discarded. For the remaining 20 volumes, 18 landmarks 5 are annotated by 3 experts and the average are taken as the ground truth.</p><p>c) Comparison Among Methods: In Table <ref type="table" target="#tab_6">VI</ref>, we compare different methods on brain datasets. All neural networks are trained on all available training data, i.e., ADNI, ABIDE and ADHD. In the table, "supervised" is our "ADD" model supervised with ANTs as the ground truth. Its loss is the endpoint error <ref type="bibr" target="#b5">[6]</ref>.  Among these methods, our "ADD" achieves the lowest Landmark Distance with a competitive speed. If we compare "ADD" with "DD", the Wilcoxon signed-rank test indicates that the integration of affine registration subnetwork significantly improves the Landmark Distance, compared to using ANTs for out-of-band affine alignment. Although there is still a performance gap compared to ANTs in terms of Segmentation IoU, it is remarkable that our method generates significantly less folding area.</p><p>Figure <ref type="figure" target="#fig_9">8</ref> exemplifies the methods on three pairs of scans. Comparison between our methods and traditional methods proves the applicability of our methods to 3D brain registration. Comparison between ADD and DD shows that integrating affine registration subnetwork is effective. d) Performance with Different Amount of Data: In Table VIII, we summarize the performance of "DD" (with ANTs affine alignment) trained on different amount of unlabeled data. As more data are used to train the network, its performance in terms of Landmark Distance consistently increases.  e) Network Cascading and Integration of Affine Registration: Table <ref type="table" target="#tab_9">IX</ref> compares the performances of differently cascaded networks. The networks without "A" have ANTsaffine-aligned images as input, whereas the networks with "A" do not. As one would expect, the performance in each group improves as the model gets more levels of cascaded subnetworks. While the methods with ANTs affine alignment have higher Segmentation IoU, integrating affine registration subnetwork yields better Landmark Distance. Worth mentioning is that the better Segmentation IoU comes at the price of a rather slow preprocessing phase (74 seconds). VI. DISCUSSION Our method achieves significant performance gain over traditional methods on the liver CT dataset, however, some method (ANTs) still performs slightly better on the brain MRI dataset. This is probably because brain MRIs are mainly of small displacements which might be better suited for traditional iterative methods. Besides the correlation coefficient loss used in this paper, other similarity measurements like cross correlation can also be explored for better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we present Volume Tweening Network (VTN), a new unsupervised end-to-end learning framework using convolutional neural networks for 3D medical image registration.</p><p>The network is trained in an unsupervised manner without any ground-truth deformation. Experiments demonstrate that our method achieves state-of-the-art performance, and that it witnesses an 880x (or 3.3x without GPU acceleration) speed-up compared to traditional medical image registration methods. Our thorough experiments prove our contributions, each on its own being useful and forming a strong union when put together. Our networks can be cascaded. Cascading deformable subnetworks tackles the difficulty of registering images in the presence of large displacement. Network cascading also enables the integration of affine registration into the algorithm, resulting in a truly end-to-end method. The integration proves to be more effective than using out-of-band affine alignment. We also incorporate the invertibility loss into the training process, which further enhances the performance. Our methods can potentially be applied to various other medical image registration tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of 3D medical image registration. Given a fixed image (a) and a moving image (b), a deformation field (c) indicates the displacement of each voxel in the fixed image to the moving image (represented by a grid skewed according to the field).An image (d) similar to the fixed one can be produced by warping the moving image with the flow. The images are rendered by mapping grayscale to white with transparency (the more intense, the more opaque) and dimetrically projecting the volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Affine registration subnetwork. The number of channels is annotated above the layer. A smaller canvas means lower spatial resolution.</figDesc><graphic coords="5,317.96,56.07,239.10,101.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Dense deformable registration subnetwork. Number of channels is annotated above the layer. Curved arrows represent skip paths (layers connected by an arrow are concatenated before transposed convolution). Smaller canvas means lower spatial resolution.</figDesc><graphic coords="5,317.96,192.74,239.10,76.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of how invertibility loss enforces round-trip registration. Green (darker) and yellow (lighter) curves represent two images. Curved arrows: flow fields. Solid curved arrows: flow inversion. Dashed arrows: failure of flow inverseion. Straight arrow (magenta, very dark): example vector in the composed flow green → yellow → green, which is non-zero because the voxel fails to trip back. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2</head><label></label><figDesc>The command we use for ANTs: -d 3 -o &lt;OutFileSpec&gt; -u 1 -w [0.025,0.975] -r [&lt;Fixed&gt;,&lt;Moving&gt;,1] -t Rigid[0.1] -m MI[&lt;Fixed&gt;,&lt;Moving&gt;,1,32,Regular,0.2] -c [2000x2000x2000,1e-9,15] -s 2x1x0 -f 4x2x1 -t Affine[0.1] -m MI[&lt;Fixed&gt;,&lt;Moving&gt;,1,32,Regular,0.1] -c [2000x2000x2000,1e-9,15] -s 2x1x0 -f 4x2x1 -t SyN[0.15,3.0,0.0] -m CC[&lt;Fixed&gt;,&lt;Moving&gt;,1,4] -c [100x100x100x50,1e-9,15] -s 3x2x1x0 -f 6x4x2x1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example comparison among VTN ADDD + inv (c/d), Elastix (e/f), ANTs (g/h) and VoxelMorph-2 (i/j). (a) sections of the fixed image (a CT liver scan); (b) sections of the moving image (another CT liver scan); (c/e/g/i) sections of the warped images and landmark distances; (d/f/h/j) sections of the warped segmentations (white for the fixed and semi-transparent red for the warped) and segmentation IoUs. Crosses indicate the projection of landmarks (L2, L3 and L4 from top to bottom), yellow (lighter) for one in the fixed image, red (darker) for the corresponding one in the moving/warped images. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example comparison among ADDD + inv (c/d), ADDD (e/f), ADD (g/h), AD (i/j) and D (k/l) networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>5</head><label></label><figDesc>The landmarks: (L1) right lateral ventricle posterior; (L2) left lateral ventricle posterior; (L3) anterior commissure corresponding to the midpoint of decussation of the anterior commissure on the coronal AC plane; (L4) right lateral ventricle superior; (L5) right lateral ventricle inferior; (L6) left lateral ventricle superior; (L7) left lateral ventricle inferior; (L8) middle of lateral ventricle; (L9) posterior commissure corresponding to the midpoint of decussation; (L10) right lateral ventricle superior; (L11) left lateral ventricle superior; (L12) middle of lateral ventricle; (L13) corpus callosum inferior; (L14) corpus callosum superior; (L15) corpus callosum anterior; (L16) corpus callosum posterior tip of genu corresponding to the location of the most posterior point of corpus callosum posterior tip of genu on the midsagittal planes; (L17) corpus callosum fornix junction; and (L18) pineal body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example comparison among VTN ADD (c/d), VTN DD (e/f), Elastix (g/h), ANTs (i/j) and VoxelMorph-2 (k/l). The input images to methods with " " are affinely aligned to a fixed atlas by ANTs and their warped images are transformed backwards according to the affine transformation aligning the fixed image and the atlas for sensible comparison. Columning and coloring are the same as those in Figure 6, except that the fixed image and the moving image are a pair of MR brain scans and that the landmarks are L7, L12 and L15. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,51.95,56.07,508.10,168.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Ratio of loss functions.• LITS<ref type="bibr" target="#b33">[34]</ref> consists of 130 volumes. LITS comes with segmentation of liver, but we do not use such information. This dataset is used for training. • BFH is provided by Beijing Friendship Hospital and consists of 92 volumes. This dataset is used for training.</figDesc><table><row><cell>Subnetwork</cell><cell>Loss</cell><cell>Relative Ratio</cell></row><row><cell>Affine</cell><cell>Similarity</cell><cell>1</cell></row><row><cell></cell><cell>Determinant</cell><cell>0.1</cell></row><row><cell></cell><cell>Orthogonality</cell><cell>0.1</cell></row><row><cell>Dense 1</cell><cell>Similarity</cell><cell>0</cell></row><row><cell></cell><cell>Total variation</cell><cell>1</cell></row><row><cell>Dense 2</cell><cell>Similarity</cell><cell>0.05</cell></row><row><cell></cell><cell>Total variation</cell><cell>1</cell></row><row><cell>Dense 3</cell><cell>Similarity</cell><cell>1</cell></row><row><cell></cell><cell>Total variation</cell><cell>1</cell></row></table><note><p>• MICCAI (MICCAI'07)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>plus regularization term as the loss function). Comparison among traditional methods, VoxelMorph and our VTN (liver).</figDesc><table><row><cell>Method</cell><cell>Seg. IoU</cell><cell>Lm. Dist.</cell><cell>Time</cell><cell>Time (w/o GPU)</cell></row><row><cell>ANTs [4]</cell><cell>0.8124</cell><cell>11.93</cell><cell>N/A</cell><cell>748 s</cell></row><row><cell>Elastix [5]</cell><cell>0.8365</cell><cell>12.36</cell><cell>N/A</cell><cell>115 s</cell></row><row><cell>VoxelMorph-2 [23]</cell><cell>0.6796</cell><cell>18.10</cell><cell>0.20 s</cell><cell>17 s</cell></row><row><cell>VTN ADDD</cell><cell>0.8868</cell><cell>12.04</cell><cell>0.13 s</cell><cell>26 s</cell></row><row><cell>VTN ADDD + inv</cell><cell>0.8882</cell><cell>11.42</cell><cell>0.13 s</cell><cell>26 s</cell></row><row><cell>supervised</cell><cell>0.7680</cell><cell>13.38</cell><cell>0.13 s</cell><cell>26 s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table III :</head><label>III</label><figDesc>Standard deviation of Jacobian determinant and fraction of folding on the liver dataset. Areas with negative Jacobian determinant are considered folding. Standard deviations across instances are in parentheses.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table VI :</head><label>VI</label><figDesc>Table VII presents the smoothness metrics of our method compared to the traditional methods. Comparison among traditional methods, VoxelMorph [23] and our algorithm on brain datasets. Bold: best among all methods. Star: best among methods with ANTs affine pre-alignment. Methods with " " use ANTs for affine pre-alignment. The preprocessing time (about 74 seconds) is not included in the table.</figDesc><table><row><cell>Method</cell><cell>Seg. IoU</cell><cell>Lm. Dist.</cell><cell>Time</cell><cell>Time (w/o GPU)</cell></row><row><cell>ANTs [4]</cell><cell>0.9387</cell><cell>2.85</cell><cell>N/A</cell><cell>764 s</cell></row><row><cell>Elastix [5]</cell><cell>0.9180</cell><cell>3.23</cell><cell>N/A</cell><cell>121 s</cell></row><row><cell>VoxelMorph-2 [23]</cell><cell>0.9268</cell><cell>2.84</cell><cell>0.19 s</cell><cell>14 s</cell></row><row><cell>VTN DD</cell><cell>0.9305</cell><cell>2.83</cell><cell>0.09 s</cell><cell>19 s</cell></row><row><cell>VTN ADD</cell><cell>0.9270</cell><cell>2.62</cell><cell>0.10 s</cell><cell>17 s</cell></row><row><cell>VTN ADD + inv</cell><cell>0.9278</cell><cell>2.64</cell><cell>0.10 s</cell><cell>17 s</cell></row><row><cell>supervised</cell><cell>0.9060</cell><cell>2.94</cell><cell>0.10 s</cell><cell>19 s</cell></row><row><cell cols="2">Method</cell><cell>Std. Jacobian</cell><cell cols="2">Folding (%)</cell></row><row><cell cols="2">ANTs [4]</cell><cell>0.230 (0.037)</cell><cell cols="2">0.294 (0.228)</cell></row><row><cell cols="2">Elastix [5]</cell><cell>0.171 (0.034)</cell><cell cols="2">0.000 (0.000)</cell></row><row><cell cols="2">VTN ADD</cell><cell>0.339 (0.126)</cell><cell cols="2">0.018 (0.043)</cell></row><row><cell cols="2">VTN ADD + inv</cell><cell cols="3">0.332 (0.122) 0.017 (0.055)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VII :</head><label>VII</label><figDesc>Standard deviation of Jacobian determinant and fraction of folding on the brain dataset. Areas with negative Jacobian determinant are considered folding. Standard deviations across instances are in parentheses.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table VIII :</head><label>VIII</label><figDesc>Comparison of performance of VTN DD (with ANTs affine alignment) with different amount of unlabeled training data (brain).</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c) 2.06</cell><cell>(d) 0.9414</cell><cell>(e) 2.10</cell><cell>(f) 0.9408</cell><cell>(g) 2.46</cell><cell>(h) 0.9211</cell><cell>(i) 2.18</cell><cell>(j) 0.9439</cell><cell>(k) 2.27</cell><cell>(l) 0.9345</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Training Dataset for VTN DD</cell><cell></cell><cell cols="2">Seg. IoU Lm. Dist.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ADNI + ABIDE-1</cell><cell></cell><cell></cell><cell>0.9299</cell><cell>2.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ADNI + ABIDE-1 + ABIDE-2</cell><cell></cell><cell>0.9312</cell><cell>2.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">ADNI + ABIDE-1 + ABIDE-2 + ADHD</cell><cell>0.9305</cell><cell>2.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table IX :</head><label>IX</label><figDesc>Comparison of performance with different number of cascaded subnetworks (brain), and comparison between using ANTs as affine alignment and end-to-end network (integrated affine registration subnetwork). The first group (with " ") uses ANTs to affinely align input images, whereas the second group does not. Bold: best among all methods. Star: best among methods with ANTs affine pre-alignment. Methods with " " use ANTs for affine pre-alignment. The preprocessing time (about 74 seconds) is not included in the table.</figDesc><table><row><cell>Method</cell><cell cols="2">Seg. IoU Lm. Dist.</cell><cell>Time</cell><cell>Time (w/o GPU)</cell></row><row><cell>D</cell><cell>0.9241</cell><cell>2.91</cell><cell>0.07 s</cell><cell>10 s</cell></row><row><cell>DD</cell><cell>0.9305</cell><cell>2.83</cell><cell>0.09 s</cell><cell>19 s</cell></row><row><cell>DDD</cell><cell>0.9320</cell><cell>2.85</cell><cell>0.10 s</cell><cell>28 s</cell></row><row><cell>AD</cell><cell>0.9214</cell><cell>2.73</cell><cell>0.08 s</cell><cell>9 s</cell></row><row><cell>ADD</cell><cell>0.9270</cell><cell>2.62</cell><cell>0.10 s</cell><cell>17 s</cell></row><row><cell>ADDD</cell><cell>0.9286</cell><cell>2.63</cell><cell>0.11 s</cell><cell>26 s</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>The landmarks: (L1) the top point of hepatic portal; (L2) the intersection of the superior and anteroir branches of the right lobe; (L3) the intersection of the superior and inferior branches of the right lobe; and (L4) the intersection of the medial and inferior branches of the left lobe.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Beijing Friendship Hospital for providing the "BFH" dataset as well as other data providers for making their data publicly available.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported by the National Science and Technology Major Project of the Ministry of Science and Technology in China under Grant 2017YFC0110903, National Science and Technology Major Project of the Ministry of Science and Technology in China under Grant 2017YFC0110503, Microsoft Research under the eHealth program, the National Natural Science Foundation in China under Grant 81771910, the Beijing Natural Science Foundation in China under Grant 4152033, the Technology and Innovation Commission of Shenzhen in China under Grant shenfagai2016-627, Beijing Young Talent Project in China, the Fundamental Research Funds for the Central Universities of China under Grant SKLSDE-2017ZX-08 from the State Key Laboratory of Software Development Environment in Beihang University in China, the 111 Project in China under Grant B13003. Dagger indicates equal contribution. Asterisk indicates the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Medical image registration: a review</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P M</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M R S</forename><surname>Tavares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Biomechanics and Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="93" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational methods for multimodal image matching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hermosillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chefd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hotel</surname></persName>
		</author>
		<author>
			<persName><surname>Faugeras</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1020830525823</idno>
		<ptr target="https://doi.org/10.1023/A:1020830525823" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="343" />
			<date type="published" when="2002-12">Dec 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape registration in implicit spaces using information theory and free form deformations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2006.171</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2006.171" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1303" to="1318" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Advanced normalization tools (ants)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Insight j</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">elastix: A toolbox for intensity-based medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="196" to="205" />
			<date type="published" when="2010-01">Jan 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Der Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<ptr target="http://lmb.informatik.uni-freiburg.de/Publications/2017/IMKDB17" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Image similarity metrics in image registration</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J H A</forename><surname>Melbourne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ridgway</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.840389</idno>
		<ptr target="https://doi.org/10.1117/12.840389" />
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="7623" to="7623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mutualinformation-based registration of medical images: a survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B A</forename><surname>Maintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSCATIONS ON MEDICAL IMAGING</title>
		<imprint>
			<biblScope unit="page" from="986" to="1004" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">D.j.hawkes. an overlap invariant entropy measure of 3d medical image alignment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Studholme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L G</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">32</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recursive cascaded networks for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">FAIR: Flexible Algorithms for Image Registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Modersitzki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>SIAM</publisher>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning similarity measure for multi-modal 3d image registration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J S</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A W M</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
		<idno>abs/1702.05747</idno>
		<ptr target="http://arxiv.org/abs/1702.05747" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonrigid image registration using multi-scale 3d convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sokooti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2017</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="232" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A cnn regression approach for real-time 2d/3d registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1352" to="1363" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised end-to-end learning for deformable medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1711.08608</idno>
		<ptr target="http://arxiv.org/abs/1711.08608" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">End-to-end unsupervised deformable image registration with a convolutional neural network</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06065</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A deep learning framework for unsupervised affine and deformable image registration</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sokooti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="128" to="143" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Networks for joint affine and non-parametric image registration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4224" to="4233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An unsupervised learning model for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9252" to="9260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voxelmorph: a learning framework for deformable medical image registration</title>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a probabilistic model for diffeomorphic registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mailhé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mailhé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
	<note>Unsupervised probabilistic deformation modeling for robust diffeomorphic registration</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: nonlinear phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nonrigid registration using free-form deformations: application to breast mr images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Sonoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="712" to="721" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>software available from tensorflow.org. [Online</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Liver tumor segmentation challenge</title>
		<ptr target="http://lits-challenge.com/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Comparison and evaluation of methods for liver segmentation from ct datasets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Arzhaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aurich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bekes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Binnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M M</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cordova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fidrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Furst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grenacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kainmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Kitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kobatake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lamecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Meinzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nemeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Raicu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Van Rikxoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rousson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rusko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Saddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seghers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Slagmolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sorantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Susomboon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1251" to="1265" />
			<date type="published" when="2009-08">Aug 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Construction of a 3d probabilistic atlas of human cortical structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Shattuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Adisetiyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hojatkashani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Narr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bilder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Toga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1064" to="1080" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ways toward an early diagnosis in alzheimer&apos;s disease: the alzheimer&apos;s disease neuroimaging initiative (adni)</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Thal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jagust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Trojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Toga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beckett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s &amp; Dementia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Di</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-G</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Denio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Bookheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dapretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular psychiatry</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">659</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The neuro bureau adhd-200 preprocessed repository</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chouinard-Decorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Benhajali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Margulies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Craddock</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S105381191630283X" />
	</analytic>
	<monogr>
		<title level="m">Sharing Part II</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="275" to="286" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
