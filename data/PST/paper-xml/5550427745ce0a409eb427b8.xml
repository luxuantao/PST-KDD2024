<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating Spatiotemporal Interest Point Features for Depth-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-01-01">January 1, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering</orgName>
								<orgName type="institution">West Virginia University</orgName>
								<address>
									<postCode>26506</postCode>
									<settlement>Morgantown</settlement>
									<region>WV</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering</orgName>
								<orgName type="institution">West Virginia University</orgName>
								<address>
									<postCode>26506</postCode>
									<settlement>Morgantown</settlement>
									<region>WV</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenbin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering</orgName>
								<orgName type="institution">West Virginia University</orgName>
								<address>
									<postCode>26506</postCode>
									<settlement>Morgantown</settlement>
									<region>WV</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering</orgName>
								<orgName type="institution">West Virginia University</orgName>
								<address>
									<postCode>26506</postCode>
									<settlement>Morgantown</settlement>
									<region>WV</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guodong</forename><surname>Guo</surname></persName>
							<email>guodong.guo@mail.wvu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering</orgName>
								<orgName type="institution">West Virginia University</orgName>
								<address>
									<postCode>26506</postCode>
									<settlement>Morgantown</settlement>
									<region>WV</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Lane Department of Computer Science and Electrical Engineering</orgName>
								<orgName type="institution">West Virginia University</orgName>
								<address>
									<postCode>26506</postCode>
									<settlement>Morgantown</settlement>
									<region>WV</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating Spatiotemporal Interest Point Features for Depth-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-01-01">January 1, 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">EB5CEE2BE8CD335484651752E1D06BA1</idno>
					<idno type="DOI">10.1016/j.imavis.2014.04.005</idno>
					<note type="submission">Received date: 11 August 2013 Revised date: 6 January 2014 Accepted date: 2 April 2014 Preprint submitted to Image and Vision Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image and Vision Computing Action recognition</term>
					<term>spatiotemporal interest point (STIP)</term>
					<term>detectors</term>
					<term>descriptors</term>
					<term>STIP features</term>
					<term>RGB-D sensor</term>
					<term>evaluation</term>
					<term>STIP feature refinement</term>
					<term>feature fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human action recognition has lots of real-world applications, such as natural user interface, virtual reality, intelligent surveillance, and gaming. However, it is still a very challenging problem. In action recognition using the visible light videos, the spatiotemporal interest point (STIP) based features are widely used with good performance. Recently, with the advance of depth imaging technology, a new modality has appeared for human action recognition. It is important to assess the performance and usefulness of the STIP features for action analysis on the new modality of 3D depth map. In this paper, we evaluate the spatiotemporal interest point (STIP) based features for depth-based action recognition. Different interest point detectors and descriptors are combined to form various STIP features. The bag-of-words representation and the SVM classifiers are used for action learning. Our comprehensive evaluation is conducted on four challenging 3D depth databases. Further, we use two schemes to refine the STIP features, one is to detect the interest points in RGB videos and apply to the aligned depth sequences, and the other is to use the human skeleton to remove irrelevant interest points. These refinements can help us have a deeper understanding of the STIP features on 3D depth data. Finally, we investigate a fusion of the best STIP features with the prevalent skeleton features, to present a complementary use of the STIP features for action recognition on 3D data. The fusion approach gives significantly higher accuracies than many state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human actions convey a significant amount of information for human interaction with the environment, human-to-human communication and human-to-machine interaction. Human action recognition is a very active research topic in computer vision, aiming to automatically recognize and interpret ongoing human actions. The ability to recognize complex human actions from videos enables the construction of several important applications such as natural user interfaces, virtual reality, intelligent surveillance and gaming <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Although human action recognition is very important for many real-world applications, it is still a challenging problem. A number of methods have been proposed to solve the action recognition problem <ref type="bibr" target="#b1">[2]</ref>. Among various methods, the spatiotemporal interest point (STIP) based features have shown good performance for action recognition in RGB videos <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Very recently, depth imaging technology has made a significant progress, which brings a broader scope for human action recognition. Using a consumer depth sensor, e.g., the Kinect <ref type="bibr" target="#b3">[4]</ref>, depth information can be captured simultaneously with the RGB videos. Moreover, from the depth maps the geometric positions of skeleton points can also be detected effectively <ref type="bibr" target="#b3">[4]</ref>. As a result, the depth data provides a promising modality for action recognition.</p><p>In traditional RGB video-based action recognition, several spatiotemporal features have been proposed to characterize human actions using local motions in a space-time volume. Local features possess many advantages, e.g., it can avoid possible problems caused by inaccurate segmentation or partial occlusions. In the literature, many spatiotemporal feature detectors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and descriptors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed and shown promising performance for action recognition in RGB videos. However, it has not been well studied yet on whether these spatiotemporal interest point (STIP) features can be useful or not for depth-based action recognition.</p><p>In this paper, we perform a comprehensive evaluation of different spatiotemporal interest point features for depth-based human action recognition. In particular, three interest point detectors and six local descriptors are adopted, in total there are 14 different detector/descriptor combinations adopted for the evaluation. Experiments are conducted on four challenging depth action databases with the same experimental setup for each feature. Besides, we also extend the capability of using spatiotemporal features by utilizing the corresponding RGB videos, and the skeleton joints positions, in order to have a deep understanding of the STIP features on depth data. Two different interest points refinement approaches are examined. Moreover, a feature-level fusion method is presented to combine the best spatiotemporal features on each database with the skeleton joints features. From the experimental results and comparisons with the state-of-the-art approaches for depth-based action recognition, we show the usefulness of spatiotemporal features for action recognition in depth videos.</p><p>The rest of the paper is organized as follows: the related work on depth-based action recognition is reviewed in Section 2. Different spatiotemporal interest point features are introduced in Section 3. Four different depth action/activity databases are presented in Section 4. Experiments are conducted and presented in Section 5. Two STIP refinement approaches are introduced and evaluated experimentally. A fusion of the best STIP features with skeleton features is shown in Section 6. Finally, we draw conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work on Depth-based Action Recognition</head><p>The depth sensors offer several advantages over traditional video cameras, e.g., working in low light conditions, giving a real 3D measure invariant to surface color and texture, resolving silhouette ambiguities in pose <ref type="bibr" target="#b3">[4]</ref>, etc. Depth sensors can significantly simplify the task of background subtraction and human detection. Because of the advantages, the depth sensors, e.g., the Kinect, have attracted researchers' attentions from many areas including 3D modeling, object recognition, gesture analysis, etc. Recently, action analysis and recognition in depth videos have become a very active topic. In this quite novel area, different approaches have been proposed. Here we give a brief overview of the methods for depth-based action recognition.</p><p>Li et al. <ref type="bibr" target="#b12">[13]</ref> proposed a sampling of 80 representative 3D points to describe a salient posture. In order to select the representative points, each depth map was projected onto three orthogonal Cartesian planes: xy, xz and zy, and then a specified number of 2D points were sampled at equal distance along the contours of the projected depth data. An action graph was used to model the dynamics of actions. Their method has smaller error rates than using 2D silhouettes.</p><p>Xia et al. <ref type="bibr" target="#b13">[14]</ref> proposed to use histograms of 3D joint locations (HOJ3D) for action recognition. In order to be view invariant, they aligned the spherical coordinates with the person's specific direction. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>hip center joint served as the center of the coordinate system. By projecting the vector from left-hip center to the right-hip center to the horizontal plane, the horizontal reference vector was obtained. The zenith reference vector passes through the coordinate center and is perpendicular to the ground plane. According to different joint's contribution to the body motion, they chose 9 joints to compute the 3D spatial histogram by partitioning the 3D space into 84 bins. After that, the LDA was performed to extract the dominant features, so that each frame will have a n -1 dimensional feature vector, where n is the number of classes. The K-means clustering was performed to represent each posture as a visual word. A discrete HMM was trained for action recognition. Vieira et al. <ref type="bibr" target="#b14">[15]</ref> proposed the Space-Time Occupancy Patterns (STOP) to represent sequences of depth maps. In their representation, the space and time axes were divided into multiple segments so that each depth map sequence was embedded in multiple 4D grids. They computed occupancy feature in each cell. After that, they employed a Nearest Neighbor classifier based on the cosine distance for action recognition.</p><p>Yang and Tian <ref type="bibr" target="#b15">[16]</ref> combined static posture, motion property, and overall dynamics to form an action feature descriptor called EigenJoints. In order to remove noisy frames and reduce computational cost, they performed informative frame selection based on Accumulated Motion Energy (AME). A non-parametric Naive-Bayes-Nearest-Neighbor (NBNN) classifier was used for action classification.</p><p>In order to make skeleton representation invariant to sensor orientation and global translation of the body, Miranda et al. <ref type="bibr" target="#b16">[17]</ref> proposed a pose descriptor vector in a torso-based coordinate system. A predefined key pose set was used to build SVM classifiers. Because each gesture can be viewed as a sequence of key poses, a decision forest was used to search for key pose sequences. In recognition stage, the key pose classifiers can recognize key poses performed by the user and then determine the corresponding gesture class.</p><p>Yang et al. <ref type="bibr" target="#b17">[18]</ref> proposed to generate three 2D Depth Motion Maps (DMM) from each 3D depth frame according to front, side, and top views. The HOG feature is computed from DMM to represent an action video. They used a linear SVM classifier to recognize actions.</p><p>In <ref type="bibr" target="#b18">[19]</ref>, Wang et al. extracted two features, pairwise relative positions and Local Occupancy Patterns at each joint. Each skeleton joint i has 3 coordinates F i (t) = (x i (t), y i (t), z i (t)) at frame t, the pairwise relative position features are extracted for joint i as: p i = p i j |i j = p ip j |i j . In order to model the interaction between human subject and objects, they computed the LOP feature based on the 3D point cloud around a particular joint. After that, Fourier temporal pyramid was used to represent the temporal dynamics of the frame-level features. In order to deal with the errors of the skeleton tracking and better characterize the intra-class variations, they defined an actionlet as a conjunction structure on base features. One base feature is the Fourier pyramid feature of each joint. A data mining algorithm was used to find discriminative actionlets for action recognition.</p><p>In <ref type="bibr" target="#b19">[20]</ref>, Sung et al. used all three channels, i.e., RGB, depth and skeleton positions, for human activity recognition. They extracted hand position information, body pose features and motion from skeleton joints. For both RGB and depth images, they used the Histogram of Oriented Gradients (HOG) feature in two settings. One is to compute HOG in both the RGB and depth within the bounding box of the person. The other is to get the bounding boxes for the head, torso, left arm, and right arm, based on the skeleton locations, and compute the HOG in RGB and depth with each of the four bounding boxes. A two-layered maximum-entropy Markov model was trained to capture the hierarchies of human activities and transitions between sub-activities over time.</p><p>Wang et al. <ref type="bibr" target="#b20">[21]</ref> proposed a semi-local feature called Random Occupancy Patterns (ROP). A depth sequence is treated as a 4D volume. Given a subvolume, the ROP feature was computed as: o xyz = δ(Σ q∈bin xyzt I q ), where I q = 1 if the point cloud has a point in the location q and I q = 0 otherwise. δ(•) is</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCEPTED MANUSCRIPT</head><p>a sigmoid normalization function: δ(x) = 1 1+e -βx . Because the sizes of the 4D subvolume are extremely large and the features are highly redundant, a weighted sampling method was applied to reduce the complexity and obtain the discriminative features. They also utilized a sparse coding method to robustly encode those features. The SVM classifier was used for classification.</p><p>More recently, Oreifej et al. <ref type="bibr" target="#b21">[22]</ref> represented the depth sequence using a histogram capturing the distribution of the surface normal orientation in 4D space of time, depth, and spatial coordinates (HON4D feature). A 600-cell polychoron with 120 vertices was used to quantize the 4D space and represent possible directions of the 4D normals. The SVM classifier was used for action classification.</p><p>Koppula et al. <ref type="bibr" target="#b22">[23]</ref> proposed to jointly model the human activities and object affordances as a Markov Random Field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolutions over time. In order to find atomic movements in an activity, they also performed temporal segmentation of the frames. They used a multi-class SVM classifier for action recognition.</p><p>Ni et al. <ref type="bibr" target="#b23">[24]</ref> proposed the Depth-Layered Multi-Channel STIP (DLMC-STIP) and Three-Dimensional Motion History Images (3D-MHIs). For DLMC-STIP, after getting local feature descriptors in a video, they introduced a set of (M) depth layers</p><formula xml:id="formula_1">L z 1 = [z l 1 , z u 1 ] , L z 2 = [z l 2 , z u 2 ], . . . , L z M = [z l M , z u M ]</formula><p>, with lower and upper boundaries denoted as z l M and z u M for the mth depth layer, so a detected spatio-temporal interest point by Harris3D detector would be located in one specific layer. In this way they formed multi-channel histograms for feature description using the HOGHOF descriptor. The 3D-MHIs are motion history images (MHIs), including both forward-DMHIs (fDMHIs) and backward-DMHIs (bDMHIs). The SVM classifiers were used for action recognition.</p><p>Zhao et al. <ref type="bibr" target="#b24">[25]</ref> explored the combination of RGB channel and depth for action recognition. They extracted interest points from RGB videos. They proposed local depth pattern (LDP) to represent each local video volume at each interest point position extracted from visible light videos, and adopted to the corresponding depth videos. Given an interest point p, its local region is partitioned into N x × N y spatial cells. Each cell is of size (S x , S y ) pixels. For each cell, they computed an average depth value and then the difference of average depth values between every cell pair to form the LDP feature. For each interest point p, which can be detected by the Harris3D detector on either RGB video or depth sequence, the output feature vector can be denoted as S p = (x, y, t, F), where (x, y, t) denote the coordinates and time of interest point, and action feature F could be obtained either by HOGHOF descriptor or LDP. They explored different combinations of RGB and depth map features and used the SVM classifiers.</p><p>Inspired by Dollar's work on local features <ref type="bibr" target="#b5">[6]</ref>, Zhang et al. <ref type="bibr" target="#b25">[26]</ref> developed a 4D local spatio-temporal feature which combines both intensity and depth information. They first applied separate filters along the 3D spatial dimensions and the temporal dimension to detect interest point. Then they computed and concatenated the intensity and depth gradients with a 4D hyper cuboid to obtain features for an action sequence. The Latent Dirichlet Allocation with Gibbs sampling was used as the classifier.</p><p>Also inspired by Dollar's local interest point detector <ref type="bibr" target="#b5">[6]</ref>, Xia and Aggarwal <ref type="bibr" target="#b26">[27]</ref> proposed a spatiotemporal interest point detector on depth map, which effectively eliminate the noise ('value jumps' and 'holes') appear on depth maps. They extended the Cuboid detector <ref type="bibr" target="#b5">[6]</ref> to the fourth dimension. A depth cuboid similarity descriptor is proposed to describe the local feature, based on the similarity between all pair of blocks in the 3D cuboid. Finally a feature selection process based on F-score is applied to generate the feature vector, and then used for action classification.</p><p>From the overview of related works on depth-based action recognition, we show that many of the approaches were motivated by the methods originally developed for RGB action recognition, e.g., motion history and the spatiotemporal interest point features. Although the STIP features prevail for analyzing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>color/intensity actions with good performance, only very limited types of STIP features were applied to depth-based action recognition. It has not been well studied yet on the performance of the typical STIP features on 3D depth actions. Thus it is important to evaluate the representative STIP features, so that a better understanding of the STIP features can be obtained for 3D depth-based action analysis. Our goal is to measure the usefulness of the STIP features for 3D action recognition, and build benchmark results of these features on several depth-based action databases.</p><p>In the following, we briefly introduce the STIP features that we used for the evaluation, and then present the databases and the evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatiotemporal Interest Point Features</head><p>Different Spatiotemporal Interest Point (STIP) features have been proposed for action characterization in RGB videos with good performance <ref type="bibr" target="#b2">[3]</ref>. For example, Laptev and Lindeberg <ref type="bibr" target="#b27">[28]</ref> used some effective methods to make STIP velocity-adaptive as well as spatially and temporally invariant. Willems et al. <ref type="bibr" target="#b8">[9]</ref> presented a method to detect features under scale changes, in-plane rotations, video compression and camera motion, the extended SURF descriptor was also proposed in this work. Dollar et al. <ref type="bibr" target="#b5">[6]</ref> proposed the cuboids detectors and descriptors for action analysis. Jhuang et al. <ref type="bibr" target="#b6">[7]</ref> used local descriptors with spacetime gradients as well as optical flow. Klaser et al. <ref type="bibr" target="#b28">[29]</ref> compared space-time HOG3D descriptor with HOG and HOF descriptors <ref type="bibr" target="#b29">[30]</ref>. Recently, Wang et al. <ref type="bibr" target="#b2">[3]</ref> conducted an evaluation of different detectors and descriptors on four RGB/intensity action databases. Shabani et al. <ref type="bibr" target="#b30">[31]</ref> evaluated the motion-based and structured-based detectors for action recognition in color/intensity videos. However, there is no systematic evaluation of the STIP features on 3D depth videos.</p><p>In Wang et al.'s work <ref type="bibr" target="#b2">[3]</ref>, it was observed that although the spatiotemporal interest point features perform differently on different databases, their performances are quite similar on the same database. Our evaluation will show that the STIP features perform quite differently on the same depth database (See Section 5). In the following, we introduce the specific STIP features that are used in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Interest points detectors</head><p>The Harris3D detector was proposed in <ref type="bibr" target="#b4">[5]</ref>. It locates the spatiotemporal volumes with large variations along space and temporal directions in a video sequence. A spatiotemporal second-moment matrix is used to model a video sequence f ,</p><formula xml:id="formula_2">µ = g(•) ×           L 2 x L x L y L x L t L x L y L 2 y L y L t L x L t L y L t L 2 t          </formula><p>, where g(•) is a Gaussian function for weighting and L is the convolution of f with a spatiotemporal Gaussian derivative kernel. The interest point locations are determined by computing the local maxima of the response function</p><formula xml:id="formula_3">H = det(µ) -k • trace 3 (µ).</formula><p>The Cuboids <ref type="bibr" target="#b5">[6]</ref> detector computes the interest point location by the local maxima of the response function R, which is defined as: R = (I * g * h ev ) 2 + (I * g * h od ) 2 , where g is the 2D Gaussian smoothing kernel, h ev and h od are a quadrature pair of 1D Gabor filter, which are computed by h ev = -cos(2πtω)e -t 2 /τ 2 and h ev = -sin(2πtω)e -t 2 /τ 2 .</p><p>Willems et al. <ref type="bibr" target="#b8">[9]</ref> proposed the Hessian detector, which measures the strength of each interest point using the Hessian matrix. The response function is defined as S = |det(H)|, where H is the Hessian matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local feature descriptors</head><p>Given a set of interest point locations, various feature descriptors can be applied to characterize the local space-time content. Given the spatial scale σ and temporal scale τ at each interest point location, a local volume is used to extract features. HOG/HOF descriptor was proposed by Laptev et al. <ref type="bibr" target="#b29">[30]</ref>, using the combination of histogram of gradient (HoG) and histogram of optical flow (HoF) accumulated from the local volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>The Cuboids descriptor was proposed along with the Cuboids detector in <ref type="bibr" target="#b5">[6]</ref>. For each detected point (x, y, t, σ, τ), a feature descriptor is computed in a 3D patch centered at (x, y, t). The gradient at each spatiotemporal location is computed within the cuboid and the histogram is computed as the feature vector. The PCA can be applied to reduce the dimensionality.</p><p>The extended SURF (ESURF) descriptor <ref type="bibr" target="#b8">[9]</ref> was proposed with the Hessian detector, which is an extension of the SURF <ref type="bibr" target="#b31">[32]</ref>. For each local volume, the feature vector is computed using the sum of uniformly sampled responses of Haar-waveletes along three directions.</p><p>We will evaluate the above three interest point detectors and six local descriptors for 3D action recognition. Although there exist some works using the STIP features for depth-based action recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, only very limited types of STIP features were investigated. Through the evaluation of several representative STIP features on multiple depth databases, we will not only provide the benchmark results of STIP features on depth data, but also find the best, appropriate STIP features that may help to improve the accuracies significantly <ref type="bibr" target="#b32">[33]</ref> for depth-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Databases</head><p>In order to perform a comprehensive evaluation, we conduct experiments on four different depth databases, which were captured under different scenarios and/or environments. The evaluation on these databases can provide a thorough test of various STIP features on depth data. Table <ref type="table" target="#tab_0">1</ref> shows a brief description of the four depth-based action/activity databases. More details of these databases are given as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MSR-Action3D Dataset</head><p>MSR-Action3D Dataset <ref type="bibr" target="#b12">[13]</ref> was captured by a depth camera similar to the Kinect sensor. This dataset contains 20 actions, and each action was performed by 10 subjects three times. Two channels of data are provided: depth sequences at 15 frames per second (fps) with resolution of 640 × 480, and skeleton joint positions in each frame. The 20 actions are: high arm wave, horizontal arm wave, hammer, hand catch, forward punch, high throw, draw x, draw tick, draw circle, hand clap, two hand wave, sideboxing, bend, forward kick, side kick, jogging, tennis swing, tennis serve, golf swing, and pick up &amp; throw (see Figure <ref type="figure" target="#fig_0">1</ref> for some example images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MSRDailyActivity3D Dataset</head><p>This dataset was collected for human daily activities by a Kinect device <ref type="bibr" target="#b18">[19]</ref>. In total there are 16 activities in this dataset: drink, eat, read book, call cellphone, write on a paper, use laptop, use vacuum    cleaner, cheer up, sit still, toss paper, play game, lay down on sofa, walk, play guitar, stand up, and sit down. Each subject performed an activity twice, one "sitting on sofa" and the other "standing". The total number of videos is 320. Three channels of data, i.e., RGB, depth and skeleton joint positions are provided in this dataset. See Figure <ref type="figure" target="#fig_1">2</ref> for some examples of depth images in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">UTKinect-Action Dataset</head><p>The action videos of the UTKinect-Action Dataset <ref type="bibr" target="#b13">[14]</ref>, were collected by a single stationary Kinect with the distance ranges from 4 to 11 feet. There are totally 10 action classes performed by 10 subjects. Each subject performed each action twice. The RGB, depth and skeleton joint locations are synchronized and all three channels are provided. Some examples of depth images are shown in Figure <ref type="figure" target="#fig_2">3</ref>. The resolution of RGB images is 640 × 480, the depth image resolution is 320 × 240. The 10 action classes are: walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, and clap hands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CAD-60 Dataset</head><p>Cornell Activity Dataset-60 (CAD-60) <ref type="bibr" target="#b19">[20]</ref>, contains 60 RGB-D videos collected by a Kinect sensor with the distance ranges from 1.2m to 3.5m, the resolution of the depth sequences is 640×480, and captured at 15 fps. There are 4 different subjects and 12 different actions. The action videos were captured in five different locations, with 3 to 4 common activities performed at each location. The five locations are: office, kitchen, bedroom, bathroom and living room. Figure <ref type="figure" target="#fig_3">4</ref> shows some example depth images from this dataset. All the RGB, depth and skeleton data are provided in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluations</head><p>We present the experimental settings in Section 5.1, the evaluation results for various combinations of detectors and descriptors in Section 5.2, and two STIP refinement approaches along with the corresponding results in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental settings</head><p>The bag-of-words representation is used for the spatiotemporal interest points. First, different STIP detectors are applied to the depth sequences. Given the detected locations, different local descriptors are used to characterize the space-time volume around each interest point. These local features are then quantized into visual words, so that a depth action sequence can be represented as a histogram of the visual words. In our evaluation, vocabularies are constructed using the K-means clustering technique. We empirically set the vocabulary size to be 200, 300, 850 and 1550, respectively, for the MSRDailyActivity3D, MSRAction3D, CAD-60 dataset and UTKinect-Action datasets, depending on the database size and empirical performance. After quantization, the histograms of visual words are used as the features for action classification. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>multi-class support vector machines (SVMs) are used for action learning, with a linear kernel for the CAD-60 dataset and χ 2 -kernel for the other three datasets, based on our empirical comparisons between different kernels. The χ 2 -kernel is defined by:</p><formula xml:id="formula_4">K(H i , H j ) = exp       - 1 2A Σ V n=1 (h in -h jn ) 2 h in + h jn      </formula><p>, where H i = {h in } and H j = h jn are the frequency histograms of the visual word occurrences, and V is the vocabulary size. A is the mean value of distances between all training samples.</p><p>For different feature representations, we utilize the implementations or source code provided by the authors, mostly with the default parameter settings, since some executable code cannot be modified. All the experiments were conducted on a 64-bit operating system DELL Optiplex 790 PC, with i7 3.4GHz CPU and 12G RAM.</p><p>Specifically, for the Harris3D detector, we used the original implementation with the default parameter settings: k = 0.0005, σ 2 = [4, 8, 16, 32, 64, 128] and τ 2 = [2, 4]. For the Cuboids detector <ref type="bibr" target="#b5">[6]</ref>, we ran the authors' implementation and the default scale values σ = 2, τ = 4 were used in our evaluation. The UTKinect-Action dataset has typically shorter video clips, we used σ = 2, τ = 2 for the Cuboids detector. For the Hessian detector <ref type="bibr" target="#b8">[9]</ref>, the executable code was used with the default parameter setting.</p><p>For the HOG/HOF descriptor, we followed <ref type="bibr" target="#b29">[30]</ref> and adopted the grid parameters n x = n y = 3, n t = 2, σ 2 = 4 and τ 2 = 2. For the HOG3D descriptor <ref type="bibr" target="#b28">[29]</ref>, we used the parameters n x = n y = 5, n t = 4, σ = 2 and τ = 2 for the UTKinect-Action dataset and n x = n y = 2, n t = 5, σ = 2 and τ = 4 for the other three datasets in our evaluation. For the Cuboid descriptor <ref type="bibr" target="#b5">[6]</ref>, we applied the descriptor size ∆ x (σ) = ∆ y (σ) = 2σ + 1, ∆ t (τ) = 2τ + 1, where σ = 2, τ = 4. The PCA was applied to reduce the feature dimensions to 100. For the ESURF descriptor, we used the executable code with default parameter settings <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_5">∆ x (σ) = ∆ y (σ) = 3σ, ∆ t (τ) = 3τ.</formula><p>For all depth databases, the depth sequences are firstly transformed and stored into gray level videos (depth videos). The skeleton joint positions are also stored for each frame. Then the spatiotemporal features are extracted from the depth videos for each database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Results</head><p>The evaluation results are presented in the following, using all four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">On MSRAction3D Dataset</head><p>MSRAction3D is a commonly used dataset for 3D action recognition. We followed the same settings as <ref type="bibr" target="#b12">[13]</ref>, where the dataset is divided into 3 subsets, each consisting of 8 actions (see Table2). Then a cross-subject scheme is used in our evaluation, with half of the subjects for training and the remaining half for testing. The overall accuracy is computed by taking the average over the three subsets. The results of different detectors/descriptors on this dataset are showed in Table <ref type="table">3</ref>. One can see that the STIP features have very different accuracies on the same database, ranging from 47.1% to 80.8%, when different detectors and descriptors are used. This observation is very different from the results on color/gray level action videos <ref type="bibr" target="#b2">[3]</ref>, where the different STIP features have similar accuracies on the same database. This evaluation indicates the significant difference between 3D depth and color/gray level videos in action recognition.</p><p>The highest accuracy is achieved by Harris3D+HOG/HOF feature with a recognition accuracy of 80.8%. This accuracy is comparable to some state-of-the-art approaches, but lower than the highest in the literature by more than 10% (see Table <ref type="table" target="#tab_7">10</ref> for the state-of-the-art results on MSRAction3D). Note that in <ref type="bibr" target="#b18">[19]</ref> the skeleton joints information was used while in our evaluation of STIP features, only the depth videos are used. One reason that might impact the accuracy is that the interest points cannot be detected for several depth sequences where the lengths of the sequences are quite short.  descriptors are presented in Table <ref type="table" target="#tab_2">5</ref>. Again, the STIP features achieved very different accuracies. The highest accuracy is obtained by Cuboids+HOG/HOF and Hessian+HOG3D, with an accuracy of 70.6%. The result is lower than the reported results, e.g., Oreifej et al. got 80% accuracy with HON4D feature in <ref type="bibr" target="#b21">[22]</ref>. The highest accuracy from previous approaches is 85.8% obtained in <ref type="bibr" target="#b18">[19]</ref>. In our evaluation, all the combinations of detector/descriptors are above 58%. In the subset with more motion, the performance of STIP is much better (∼ 80%) than the subset with less motion (∼ 50%). This demonstrates that the STIP features can characterize actions with significant motions, but not static actions like sitting. Further, the STIP features cannot represent the human-object interaction. There are several activities in this dataset with similar motion but different objects, e.g., reading and writing, eating and drinking, etc. We also observe that many of the interest points are detected on depth sequences irrelevant to the actions (see Figure <ref type="figure" target="#fig_4">6</ref>). This inspires us to evaluate some refinement schemes for the STIP features (to be shown later).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">On UTKinect-Action Dataset</head><p>The evaluation results on the UTKinect-Action dataset are showed in Table <ref type="table" target="#tab_3">6</ref>. Note that because many depth sequences in this dataset are of length about 10 frames, which is too short for space-time interest point detection. Thus a preprocessing is conducted for the depth videos where 10 frames are copied to expand the length of video from both the starting and ending frames.</p><p>From the results, the best accuracy is 81%, obtained by Harris3D+HOG3D. This result is lower than the result 90.9% in <ref type="bibr" target="#b13">[14]</ref>, and the highest accuracy 91.5% in <ref type="bibr" target="#b33">[34]</ref>. Note that in <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b33">[34]</ref> the leave-oneout cross-validation scheme was applied but we use half of the subjects for training and the other half for testing. Figure <ref type="figure" target="#fig_5">7</ref> shows the confusion matrix of the best STIP feature. Most of the actions are correctly recognized, while the action "carry" has a much lower recognition rate, i.e., 60% of the testing samples are incorrectly classified as "walk". These two actions are quite similar in the dataset, since "carrying" is performed by a "walking" subject who holds an object. The STIP features might mainly focus on the body motions rather than a relatively small object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">On Cornell Activity Dataset (CAD-60)</head><p>For the CAD-60 dataset, all the depth videos are sampled to 500 frames in our evaluation. All the activity categories (12 desired activities and a random activity) in this dataset are used in our evaluation as in <ref type="bibr" target="#b19">[20]</ref>. The same experimental settings are adopted, i.e., three subjects for training, while the remaining for testing.</p><p>The evaluation results are shown in Table <ref type="table" target="#tab_4">7</ref>. Among the various features, the Hesian+ESURF gives the highest accuracy 62.5%. From the confusion matrix (Figure <ref type="figure" target="#fig_6">8</ref>), one can see that some of the similar activities on depth sequence are incorrectly recognized, e.g., talkOnCouch and relaxOnCouch, and the random activity in this dataset also influences the recognition rate, where the talkOnPhone activity is recognized incorrectly as the random activity.</p><p>In <ref type="bibr" target="#b19">[20]</ref>, the precision/recall is reported as the performance measurement (67.9%/55.5%). Yang et al. <ref type="bibr" target="#b15">[16]</ref> reported 71.9%/66.6% on this dataset. Koppula et al. <ref type="bibr" target="#b22">[23]</ref> reported the 80.8%/71.4%. We also compute precision/recall for the feature Hessian+ESURF. The result achieves 66.7%/59.0%. Note that in our experiment we do not divide the different environment into different subset as <ref type="bibr" target="#b22">[23]</ref>. The noisy background in depth sequences (see Figure <ref type="figure">5</ref>) impact the detection of interest points with many interest points detected from the background. This drawback can be overcome when human segmentation is applied. We will investigate some refinement to reduce the effect of background noise on depth-based action recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><note type="other">ACCEPTED MANUSCRIPT</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Refinements of the STIP features</head><p>In the above experiments, various STIP features are evaluated on depth videos with recognition accuracies reported. The best accuracies on each database are comparable to, but lower than some state-of-the-art methods that are developed especially for 3D action analysis. Note that the synchronized RGB videos and the human skeleton joints positions <ref type="bibr" target="#b3">[4]</ref> are usually provided with the depth sequences. Intuitively these different sources of data can be used as the complementary information for human action recognition. Thus in our evaluation, we attempt to further utilize the RGB videos and the skeleton joints positions, to enhance the performance for action recognition on depth data. In this way, we can understand the STIP features deeper in depth videos. Two approaches are investigated in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">STIP feature refinement using Skeleton Joints</head><p>Shotton et al. <ref type="bibr" target="#b3">[4]</ref> developed an efficient technique for human skeleton detection with 20 joint positions. Since the STIP features have a drawback, i.e., the spatial relations or distributions of the interest points cannot be utilized. From the above experiments, we observe that the detected interest points on depth images can be in the background or not accurate because of the noise in depth data. Therefore, we demonstrate that on depth images, the refinement of interest point detection could be done by using the skeleton. It is based on constraining the locations of STIP according to the skeleton joints. The idea is different from the work <ref type="bibr" target="#b26">[27]</ref>, but aims at the same goal-interest points refinement. Specifically, we define a bounding box around the subject at each frame t. The bounding box at frame t is obtained by the temporal images from time t -5 to t + 5, and the maximum boundaries are selected and shifted by 30 pixels to each side to construct the new bounding box. Then the STIP which are detected on the whole depth sequences are constrained within the new box. STIP detections which lie outside the bounding box are considered as from the background, and thus are eliminated (see Figure <ref type="figure" target="#fig_7">9</ref>). Finally, we do the evaluation again using the same experimental settings as previous, only a smaller K in K-means clustering because of the reduced number of interest points. The evaluation results using this STIP refinement scheme on four datasets are shown in Figure <ref type="figure" target="#fig_8">10</ref>. From the results we observe that (1) most of the features can get better results when applied the STIP refinement, e.g., on MSRAction3D dataset, the accuracy of Cuboids + Cuboids feature increases by 4.2% after the refinement; on MSRDailyActivity3D dataset, an 11.9% increase is achieved for the Hessian + ESURF feature; and on UTKinect-Action dataset, the accuracy is increased by 13% for Cuboids + HOG/HOF feature. We also notice that on the CAD-60 dataset, the STIP refinement method does not improve the accuracies. One reason might be that the dataset was collected in five different locations and certain actions are "correlated" to some specific scene/location, e.g., the action 'cooking' is performed in kitchen, while the action 'brushing teeth' is performed in bathroom, etc. The eliminated STIPs, which are mainly from the background, could contain some helpful information for action encoding. Eliminating the interest points from background will "lose" the scene or context information, thus the refinement may have some negative impact on action analysis; (2) The overall accuracies on MSRAction3D and MSRActivity3D datasets increase after applying the STIP refinement. On MSRAction3D dataset, the refined accuracy is 80.5%, comparing to the original accuracy 78.7%, on MSRActivity3D dataset, the best accuracy is 77.5%, which is higher than the original 70.6%, after the refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><note type="other">ACCEPTED MANUSCRIPT</note><formula xml:id="formula_6">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">STIP feature refinement using RGB images and Skeleton Joints</head><p>We have shown above that in most cases the STIP refinement with the 20 skeleton joint positions can increase the action recognition rates. However, the performance is still highly relied on the interest point detection accuracy. When the interest point detection performs poorly on the depth maps because of the noisy depth data, the skeleton constraints may not help too much. Based on this consideration, we pursue another refinement scheme. The idea is to adopt the interest point detection on RGB videos, i.e., using the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T STIP locations detected in RGB videos for depth sequences. In other words, the interest point detection is conducted on RGB sequences, and just duplicated to the depth maps. The feature descriptors are still executed on the depth videos. Experiments are conducted on three datasets except the MSRAction3D because it does not have the RGB data. We use the same settings as previous. The evaluation results are shown in Table <ref type="table" target="#tab_5">8</ref>. The best STIP feature on each dataset are selected (because separate implementation of ESURF descriptor is not available, we chose the 2nd best STIP feature instead). From the results, one can see that the accuracies are improved significantly after using RGB refinement approach, either the skeleton refinement is applied or not. On MSRDailyActivity3D dataset, the accuracy is increased from 70.6% to 75.6%, on CAD-60 dataset, the accuracy is improved from 56.3% to 68.8%, and on the UTKinect-Action dataset, the accuracy is improved from 81.0% to 85.0%, when using the RGB refinement approach.</p><p>For the refinement with skeleton joints, the accuracies can be improved or keep the same on the MSR-DailyActivity3D and UTKinect-Action datasets, but reduced on the CAD-60 dataset. The reason could be that the interest points located in the background or scene may help to improve the action recognition accuracies (the CAD-60 dataset contains different actions in different scenes), while the removal of those interest points (constrained by the skeleton joints) can reduce the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T The refinement results show that it may not be accurate enough to use the detected locations of interest points on depth sequences directly, because of the noisy depth values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCEPTED MANUSCRIPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Fusing spatiotemporal features and skeleton joints for action recognition</head><p>In the above, two approaches have been presented to refine the STIP features. These approaches can be viewed as posing constraints to the interest point locations on depth videos, by using either RGB videos or the skeleton joints. On the other hand, the skeleton joints positions extracted from the depth videos can be used as another feature, representing human posture information. In this section we want to evaluate the performance of combining the STIP features with the skeleton joints feature. This evaluation can tell if the STIP features can complement the skeleton joints features, and if the combination can improve the accuracies significantly. If the accuracies can be improved greatly, it can indicate the usefulness of the STIP features from another aspect.</p><p>Specifically, the combination approach has four major steps, which has been presented in a workshop <ref type="bibr" target="#b32">[33]</ref>. Firstly, the STIP features are extracted on depth sequences. Then skeleton joints features are computed from the skeleton joint positions. A quantization is performed for the two features respectively to encode the action sequences with histograms. Finally, a feature-level fusion is executed for action recognition using the random forests method <ref type="bibr" target="#b34">[35]</ref>. We chose the detector/descriptor combinations which performs the best based on our evaluation presented above. The evaluation of the STIP features in Section 5 is the basis for our fusion approach <ref type="bibr" target="#b32">[33]</ref>.</p><p>We use the histogram of the skeleton joints features proposed in <ref type="bibr" target="#b15">[16]</ref> to combine with the best STIP features on each database. Different from <ref type="bibr" target="#b15">[16]</ref> where the Naive Bayes classifier was used, we compute the histogram of the joints to combine with the STIP features by the random forests method.</p><p>The features from joint locations consist of three parts: (1) current posture: pair-wise joint distances in current posture; (2) motion: joints difference between current posture and the original (in the first frame); and (3) offset: joints differences between current posture and the previous one. A concatenation of the three feature vectors is taken to represent the feature. The PCA method is applied for dimensionality reduction.</p><p>To represent each action sequence, we quantize the STIP features and the skeleton joints features, respectively, based on the K-means clustering. The cluster centers are used as the keywords to construct the histogram bins. These features are used in the next step for feature-level fusion and action classification.</p><p>In order to perform the fusion and feature selection of spatiotemporal features and the skeleton joints features, the random forests (RFs) method <ref type="bibr" target="#b34">[35]</ref> is used. RFs are usually considered as a classifier using tree predictors in which each tree splits the data depending on the randomly selected features. And there are many nice properties to use the random forests: (1) robustness to noise, (2) efficiency for classification, and (3) the improvement of accuracy by growing multiple trees and vote for the most popular class. Here we use the RFs for fusion of distinct features and action classification together. The experiments are conducted on the four datasets (MSRAction3D, UTKinect-Action, CAD-60, and MSRDailyActivity3D) while three of them were used in our study in <ref type="bibr" target="#b32">[33]</ref>. Our fusion approach can improve the recognition rates to 94.3%, 91.9%, 87.5%, and 80.0%, respectively, on the four databases, which are significantly higher than the STIP feature or skeleton. This result shows that the STIP features can be useful to complement the often-used skeleton features for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>We also compare the fusion results to other approaches reported in the literature on the four datasets. Table <ref type="table" target="#tab_7">10</ref> shows all reported results that we can find on the MSRAction3D dataset. Under the same experimental settings, it can be seen that the fusion result of 94.3% accuracy is the second best result among all of the previous methods. Our result is only 0.5% lower than the best result in <ref type="bibr" target="#b35">[36]</ref>. On the UTKinect-Action dataset from Table <ref type="table" target="#tab_8">11</ref>, the fusion approach has an accuracy of 91.9% which is higher than the DSTIP+DCSF feature <ref type="bibr" target="#b26">[27]</ref>, and slightly higher than the HOJ3D feature in <ref type="bibr" target="#b13">[14]</ref> (90.9%) and the space-time pose representation in <ref type="bibr" target="#b33">[34]</ref> (91.5%). Note that we used the same settings as <ref type="bibr" target="#b26">[27]</ref>, which is more challenging than the settings in <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b33">[34]</ref>. On the CAD-60 dataset, the experimental settings are kept the same as <ref type="bibr" target="#b19">[20]</ref> and the presicion/recall of our fusion method is computed for a direct comparison with other methods, shown in Table <ref type="table" target="#tab_9">12</ref>. Our fusion approach obtained a much higher accuracy than the state-of-the-art results on this dataset. Finally, Table <ref type="table" target="#tab_10">13</ref> shows the results on the MSRDailyActivity3D dataset, an accuracy 80.0% is obtained using our fusion approach. Slightly different settings are used in our experiment, since the actions are divided into two groups to measure the performance difference between them. Our fusion result is comparable but about 8% lower than the highest accuracy. Note that all the 16 activities are used in our experiment, while in <ref type="bibr" target="#b26">[27]</ref>, four activities (with less motion) were removed in their experiment.</p><p>From the comparison with various approaches, we demonstrate the usefulness of the STIP features for depth-based action recognition, when combined with the skeleton feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCEPTED MANUSCRIPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy High Dimensional Convolutional Network <ref type="bibr" target="#b20">[21]</ref> 72.5% Action Graph on Bag of 3D Points <ref type="bibr" target="#b12">[13]</ref> 74.7% HOJ3D feature <ref type="bibr" target="#b13">[14]</ref> 79.0% Key Pose Learning <ref type="bibr" target="#b16">[17]</ref> 80.3% Eigenjoints <ref type="bibr" target="#b15">[16]</ref> 82.3% STOP feature <ref type="bibr" target="#b14">[15]</ref> 84.8% Random Occupancy Patterns <ref type="bibr" target="#b20">[21]</ref> 86.2% Actionlet <ref type="bibr" target="#b18">[19]</ref> 88.2% HON4D <ref type="bibr" target="#b21">[22]</ref> 88.9% DSTIP+DCSF <ref type="bibr" target="#b26">[27]</ref> 89.3% Depth Motion Maps <ref type="bibr" target="#b17">[18]</ref> 91.6% Space-time Pose Representation <ref type="bibr" target="#b33">[34]</ref> 92.8% JAS (Cosine)+MaxMin+HOG2 <ref type="bibr" target="#b35">[36]</ref> 94.8% STIP + Skeleton 94.3% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy DSTIP+DCSF <ref type="bibr" target="#b26">[27]</ref> 85.8% HOJ3D <ref type="bibr" target="#b13">[14]</ref> 90.9% space-time pose representation <ref type="bibr" target="#b33">[34]</ref> 91.5% STIP+Skeleton 91.9% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Precision/Recall J. Sung et al. <ref type="bibr" target="#b19">[20]</ref> 67.9%/55.5% X. Yang et al. <ref type="bibr" target="#b15">[16]</ref> 71.9%/66.6% Koppula et al. <ref type="bibr" target="#b22">[23]</ref> 80.8%/71.4% STIP + Skeleton 93.2%/84.6% Method Accuracy NBNN + parts + time <ref type="bibr" target="#b36">[37]</ref> 70.0% Local HON4D <ref type="bibr" target="#b21">[22]</ref> 80.0% DCSF <ref type="bibr" target="#b26">[27]</ref> 83.6% RGGP + Fusion <ref type="bibr" target="#b37">[38]</ref> 85.6% Actionlet <ref type="bibr" target="#b18">[19]</ref> 85.8% DCSF+Joint <ref type="bibr" target="#b26">[27]</ref> 88.2% STIP+Skeleton 80.0%</p><p>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We have presented a comprehensive evaluation of the spatiotemporal interest point features for action recognition in 3D. The evaluated STIP features include three spatiotemporal interest point detectors and six descriptors. The combinations of these detectors and descriptors form 14 different features. These STIP features have been evaluated on four different depth action/activity databases. The comparisons to the stateof-the-art methods have shown that the STIP features are still useful for depth-based action recognition.</p><p>From the evaluation, we have shown that most of the results are comparable to the current state-ofthe-art approaches. However, under the bag-of-words framework, the extracted features do not contain the spatial distribution of the interest points in depth maps, this is one reason that limits the performance. We have also shown that the noisy depth data and background have a great impact on interest point detection. Moreover, the interest point detection may not perform well on actions without much motion, resulting in lower accuracies.</p><p>The evaluation has shown that different STIP features perform quite differently on depth actions. It discovers that the feature with Harris3D and HOG/HOF performs the best on the MSRAction3D dataset, the Cuboids detector with HOG/HOF descriptor performs the best on the MSRDailyActivity3D dataset, while the Harris3D detector combined with HOG3D descriptor is the best on UTKinect-Action dataset. On the CAD-60 dataset, the Hessian detector with ESURF descriptor gives the highest accuracy.</p><p>Two interest point refinement schemes have been presented for the STIP features, based on constraining the STIP features using skeleton joint positions and/or the detection in RGB videos. We have shown that the STIP features can be refined to achieve better performance in most cases. We have also proposed a fusion scheme to combine the best STIP features with the skeleton joint features in each database. Significant improvements of the recognition accuracies have been achieved on all four databases. Overall, we have explored the STIP features for 3D action recognition from different aspects. 2. Two schemes to refine STIP features for a deeper understanding of their behaviors;</p><p>3. A fusion approach is developed which outperforms many state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Some samples from MSRAction3D Dataset. 7 depth images are showed. The actions shown are (from left to right): side kick, bend, jog, high arm wave, golf swing, pickup&amp;throw and high throw.</figDesc><graphic coords="8,86.75,168.71,438.38,113.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample depth images from MSRDailyActivity3D Dataset. Actions in the top row (left to right): use laptop, use vacuum cleaner, cheer up, and lay down on sofa. Action classes in the bottom row: toss paper, stand up, walk, and play guitar.</figDesc><graphic coords="8,152.51,460.91,306.98,119.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sample images from UTKinect-Action Dataset. Action classes in the top row: walk, wave hands, sit down, and throw. Action classes in the bottom row: pick up, clap hands, carry and push.</figDesc><graphic coords="9,152.51,247.19,307.22,121.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples depth images from the CAD-60 Dataset to illustrate the actions.</figDesc><graphic coords="10,86.75,104.39,438.98,345.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Examples of interest points that are detected from the background (MSRActivity3D dataset).</figDesc><graphic coords="13,130.67,186.83,351.02,264.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Confusion matrix for the feature Harris3D+HOG3D on UTKinect-Action dataset.</figDesc><graphic coords="15,86.75,104.99,438.98,329.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Confusion matrix for the feature Hessian+ESURF on CAD-60 dataset.</figDesc><graphic coords="16,86.75,104.39,438.26,323.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Examples of STIP refinement on different datasets. Left column shows the original interest points detected, right column shows the interest points after refinement by the human bounding box derived from the skeleton joints.</figDesc><graphic coords="17,130.67,104.63,351.14,344.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Bar graph of the recognition accuracies before and after the refinements on different datasets. The vertical axis denotes the recognition accuracy (%).</figDesc><graphic coords="18,86.75,104.87,438.86,316.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>comprehensive evaluation of STIP based features on depth-based action recognition;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,130.67,104.63,350.78,166.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Depth-based action/activities databases. In the 4th column, RGB denotes color images, DEP denotes depth maps, and SK denotes skeleton joints positions. The 5th column shows the average length of each video in the dataset.Kläser et al. extended the histograms of oriented gradient (HOG) to HOG3D, which is the histogram of 3D gradient orientations. Integral videos are computed for efficiency.</figDesc><table><row><cell>Database</cell><cell cols="5"># of Actions # of Subjects # of sequences # of channels Video Length</cell></row><row><cell>MSR-Action3D</cell><cell>20</cell><cell>10</cell><cell>557</cell><cell>DEP, SK</cell><cell>˜1s</cell></row><row><cell>MSRDailyActivity3D</cell><cell>16</cell><cell>10</cell><cell>320</cell><cell>RGB, DEP,SK</cell><cell>˜6s</cell></row><row><cell>UTKinect-Action</cell><cell>10</cell><cell>10</cell><cell>200</cell><cell>RGB, DEP,SK</cell><cell>˜3s</cell></row><row><cell>CAD-60</cell><cell>12</cell><cell>4</cell><cell>60</cell><cell>RGB, DEP,SK</cell><cell>˜45s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Three subsets of actions used for the experiments on MSRAction3D dataset.</figDesc><table><row><cell>AS1</cell><cell>AS2</cell><cell>AS3</cell></row><row><cell cols="2">Horizontal arm wave High arm wave</cell><cell>Hight throw</cell></row><row><cell>Hammer</cell><cell>Hand catch</cell><cell>Forward kick</cell></row><row><cell>Forward punch</cell><cell>Draw x</cell><cell>Side kick</cell></row><row><cell>High throw</cell><cell>Draw tick</cell><cell>Jogging</cell></row><row><cell>Hand clap</cell><cell>Draw circle</cell><cell>Tennis swing</cell></row><row><cell>Bend</cell><cell>Two hand wave</cell><cell>Tennis serve</cell></row><row><cell>Tennis serve</cell><cell>Forward kick</cell><cell>Golf swing</cell></row><row><cell>Pickup &amp; throw</cell><cell>Side boxing</cell><cell>Pickup &amp; throw</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Accuracies of various STIP features on MSRDailyActivity3D dataset.</figDesc><table><row><cell></cell><cell cols="5">HOG3D HOG/HOF HOG HOF Cuboids ESURF</cell></row><row><cell>Harris3D</cell><cell>60.6%</cell><cell>67.5%</cell><cell>63.8% 59.4%</cell><cell>-</cell><cell>-</cell></row><row><cell>Cuboids</cell><cell>68.8%</cell><cell>70.6%</cell><cell>68.1% 58.1%</cell><cell>64.4%</cell><cell>-</cell></row><row><cell>Hessian</cell><cell>70.6%</cell><cell>63.8%</cell><cell>61.9% 63.1%</cell><cell>-</cell><cell>65.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>Accuracies of various STIP features on UTKinect-Action dataset. Note that we use half subjects for training and the remaining half for testing. There are 100 samples in total in the test set.</figDesc><table><row><cell></cell><cell cols="5">HOG3D HOG/HOF HOG HOF Cuboids ESURF</cell></row><row><cell cols="2">Harris3D 81.0%</cell><cell>80.0%</cell><cell>66.0% 69.0%</cell><cell>-</cell><cell>-</cell></row><row><cell>Cuboids</cell><cell>65.0%</cell><cell>65.0%</cell><cell>56.0% 57.0%</cell><cell>67.0%</cell><cell>-</cell></row><row><cell>Hessian</cell><cell>69.0%</cell><cell>56.0%</cell><cell>57.0% 53.0%</cell><cell>-</cell><cell>65.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>Accuracies of various STIP features on CAD-60 dataset.</figDesc><table><row><cell></cell><cell cols="5">HOG3D HOG/HOF HOG HOF Cuboids ESURF</cell></row><row><cell>Harris3D</cell><cell>43.8%</cell><cell>50.0%</cell><cell>43.8% 37.5%</cell><cell>-</cell><cell>-</cell></row><row><cell>Cuboids</cell><cell>50.0%</cell><cell>31.3%</cell><cell>37.5% 37.5%</cell><cell>43.8%</cell><cell>-</cell></row><row><cell>Hessian</cell><cell>43.8%</cell><cell>50.0%</cell><cell>56.3% 43.8%</cell><cell>-</cell><cell>62.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Accuracies using skeleton and RGB refinement approaches. Two cells have no results since the MSRAction 3D dataset does not contain RGB data.</figDesc><table><row><cell></cell><cell cols="4">MSRDailyActivity3D UTKinect-Action CAD-60 MSRAction3D</cell></row><row><cell>Original</cell><cell>70.6%</cell><cell>81.0%</cell><cell>56.3%</cell><cell>80.8%</cell></row><row><cell>RGB Refined</cell><cell>75.6%</cell><cell>85.0%</cell><cell>68.8%</cell><cell>-</cell></row><row><cell>Skeleton Refined</cell><cell>72.5%</cell><cell>84.0%</cell><cell>50.0%</cell><cell>81.7%</cell></row><row><cell>Skeleton &amp; RGB Refined</cell><cell>77.5%</cell><cell>85.0%</cell><cell>62.5%</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>Accuracies of the fusion method compared to each single feature on four datasets. RFs denotes the random forests method.</figDesc><table><row><cell>MSRAction3D</cell><cell>Acc.</cell></row><row><cell cols="2">STIP (Harris3D+HOG/HOF) 77.5%</cell></row><row><cell>Skeleton Joint Features</cell><cell>90.9%</cell></row><row><cell cols="2">Combined features with RFs 94.3%</cell></row><row><cell>UTKinect-Action</cell><cell>Acc.</cell></row><row><cell>STIP (Harris3D+HOG3D)</cell><cell>80.8%</cell></row><row><cell>Skeleton Joint Features</cell><cell>87.9%</cell></row><row><cell cols="2">Combined features with RFs 91.9%</cell></row><row><cell>CAD-60</cell><cell>Acc.</cell></row><row><cell>STIP (Hessian+ESURF)</cell><cell>75.0%</cell></row><row><cell>Skeleton Joint Features</cell><cell>81.3%</cell></row><row><cell>STIP + Skeleton</cell><cell>87.5%</cell></row><row><cell>MSRDailyActivity3D</cell><cell>Acc.</cell></row><row><cell>STIP (Hessian+HOGHOF)</cell><cell>70.6%</cell></row><row><cell>Skeleton Joint Features</cell><cell>73.8%</cell></row><row><cell cols="2">Combined features with RFs 80.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Comparisons of different methods on MSRAction3D dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 :</head><label>11</label><figDesc>Comparisons of different methods on UTKinect-Action dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 :</head><label>12</label><figDesc>Comparisons of different methods on CAD-60 dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13 :</head><label>13</label><figDesc>Comparisons of different methods on MSRDailyActivity3D dataset.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors thank the anonymous reviewers for their detailed comments to improve the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T  </p><note type="other">ACCEPTED MANUSCRIPT</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">On MSRDailyActivity3D Dataset</head><p>The MSRDailyActivity3D dataset contains 16 activities performed by 10 subjects in two scenarios: sitting and standing. Similar to the partition in <ref type="bibr" target="#b12">[13]</ref>, we divided this dataset into 2 subsets, and evaluate the performance considering two different scenarios, sitting and standing, respectively. We consider the activities in each subset according to the motions: subset 1 (AS1) contains activities without much motion and subset 2 (AS2) with obvious motion. Table <ref type="table">4</ref> shows how we divide the subsets. In our evaluation, we adopt the cross-subject test scheme, using half of the subjects for training and the remaining half for testing. The final results are obtained by averaging accuracies over the subsets.</p><p>The evaluation results on MSRDailyActivity3D dataset using different combinations of detectors and </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The visual analysis of human movement: A survey</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="82" to="98" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC British Machine Vision Conf</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Behavior recognition via sparse spatio-temporal features, in: 2nd Joint IEEE Int&apos;l Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A biologically inspired system for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE 11th Int&apos;l Conf. on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal salient points for visual recognition of human actions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oikonomopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="710" to="719" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting spatiotemporal interest points using global information</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th Int&apos;l Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Local descriptors for spatio-temporal recognition, Spatial Coherence for Visual Motion Analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="91" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th Int&apos;l Conf. on Multimedia</title>
		<meeting>of the 15th Int&apos;l Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
	<note>Action recognition based on a bag of 3d points</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stop: Space-time occupancy patterns for 3d action recognition from depth map sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective 3d action recognition using eigenjoints</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2" to="11" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time gesture recognition from depth data through key poses learning and decision forests</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F M C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th SIBGRAPI Conf. on Graphics, Patterns and Images 0</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognizing actions using depth motion maps-based histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th ACM Int&apos;l Conf. on Multimedia</title>
		<meeting>of the 20th ACM Int&apos;l Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1057" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unstructured human activity detection from rgbd images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="842" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Robust 3d action recognition with random occupancy patterns, in: Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="872" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Redmond</surname></persName>
		</author>
		<title level="m">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>IEEE Conf. on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Rob. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rgbd-hudaact: A color-depth video database for human daily activity recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1147" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combing rgb and depth map features for human activity recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal Information Processing Association Annual Summit and Conf</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">4-dimensional local spatio-temporal features for human activity recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int&apos;l Conf. on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2044" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatio-temporal depth cuboid similarity feature for activity recognition using depth camera</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2834" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Velocity adaptation of space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int&apos;l Conf. on Pattern Recognition</title>
		<meeting>of the IEEE Int&apos;l Conf. on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="52" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conf</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal salient feature detectors for human action recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Shabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Ninth Conf. on Computer and Robot Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">Speeded up robust features</title>
		<meeting><address><addrLine>Surf</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fusing spatiotemporal features and joints for 3d action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="486" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Space-time pose representation for 3d human action recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="456" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint angles similiarities and hog2 for action recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recognizing actions from depth cameras as weakly aligned multi-part bag-of-poses</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Varano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR Int. Workshop on Human Activity Understanding from 3D Data</title>
		<meeting>of CVPR Int. Workshop on Human Activity Understanding from 3D Data</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="479" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning discriminative representations from rgb-d video data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. on Artificial Intelligence</title>
		<meeting>Int. Joint Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1493" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
