<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Vector Quantizer Based on a Classification in the DCT Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>Sik</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, ZEEE</roleName><forename type="first">Sang</forename><forename type="middle">Uk</forename><surname>Lee</surname></persName>
						</author>
						<title level="a" type="main">Image Vector Quantizer Based on a Classification in the DCT Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ACA432135C896C97AD048E4813750187</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In vector quantization (VQ) of image signal at low bit rate, edge degradations are sometimes observed because of the use of the conventional mean square error (MSE) distortion measure. A classified VQ (CVQ) technique, based on the edge-oriented classifier, can reduce the edge degradation as well as the encoding complexity. In this paper, we propose a classification algorithm in the discrete cosine transform (DCT) domain for the CVQ. The classifier employs four DCT coefficients of 4 x 4 subblock as edge-oriented features. Then the classifier is designed using a cluster-seeking algorithm to ensure that the centroid of a set of vector in a class always belong to that class. Since the classification is performed in the DCT-domain, this approach can be easily extended to the well-established DCT transform coding technique. Simulation results show that a good visual quality of the coded image at fixed rates in the 0.625-0.813 b/pixel (bpp) is obtained with comparable complexity. In addition, the weighted MSE (WMSE) analysis in coqjunction with the proposed classifier is also discussed in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>ECENTLY the vector quantizer (VQ) has received con-R siderable interests because of many attractive features in applications where high compression ratios are desired. The performance of the VQ is known to be asymptotically optimum in the sense of minimum rate distortion as the vector dimension increases [3], [4], <ref type="bibr">[15]</ref>. It has been known that the VQ is very effective in image data compression for low bit rate. However, the most serious problem in the ordinary VQ is an edge degradation caused by employing the conventional distortion measures such as the mean square error (MSE). The MSE is the most commonly used performance criterion for image coder. However, the coded image quality is not always matched with the MSE value. This argument holds because the conventional MSE does not take account of the human visual system <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Since an edge is a very significant feature perceptually in the image, a faithful coding that preserves the edge integrity is of importance. Unfortunately, the MSE does not process any edge preserving property.</p><p>In order to alleviate the edge degradation in VQ, Ramamurthi and Gersho introduced a classified VQ (CVQ) technique based on a composite source model [l]. In the composite source model, the image is represented by the shade blocks and the blocks with an edges at a particular orientation and location. By the shade we mean that the block contains no significant edge component. In [l], the image is divided into 4 x 4 or 5 x 5 subblocks, and the subblocks are classified into various edge classes. The classifier separates these two sources. Then the subblocks belong to a class are coded only with codevectors belong to the same class in order to preserve the perceptual feature associated with each class. The reported number of the Paper approved by the Editor for Image Processing of the IEEE Communications Society. Manuscript received March 4, 1988; revised November 17,  1989.</p><p>The authors are with the Department of Control and Instrumentation Engineering, Seoul National University, Shinlim-Dong, Kwanak-Ku, Seoul 151-742, Korea.</p><p>IEEE Log Number 9143105.</p><p>classes in [ l ] was 31 with 4 x 4 subblock size and 43 with 5 x 5 subblock size, respectively. The computer simulation results in <ref type="bibr">[ l ]</ref> showed that a very good encoded image quality, with no detectable annoying degradations, was obtained at fixed rates of 0.6-1.0 b/pixel (bpp). The encoding complexity, a critical issue in VQ, is also significantly reduced by this approach because the size of subcodebook for each class is much smaller than that of the entire codebook. The CVQ technique which preserves the perceptual feature associated with each class improves the quality of reconstructed image significantly because it has a firm basis in the psychophysics of vision. Needless speaking, the classifier which classifies the image subblocks according to the perceptual feature is important to the CVQ. The classifier algorithm used in [ l ] is implemented in two steps: an edge enhancement step, followed by a complicated decision tree which extracts the edge description from the enhanced version. Various edge classes, depending on the orientation, location, and the polarity (positive or negative), have been defined. However, the classes have been defined heuristically without taking care to ensure properties desirable for codebook design. In the subcodebook design for each class, by defining a set of weights {tuz}, the subcodebook size was determined by trial and error so as to the average weighted MSE (WMSE) distortion in each class be equal</p><formula xml:id="formula_0">[l].</formula><p>But, in practical situation it is not easy to determine a set of weights {w,} experimentally. Moreover, the design of edge oriented classifier based on edge-detection in the spatial domain is not a simple task in general, since it is usually necessary to establish several thresholds in the gradient comparison process. Thus, it is strongly felt that rather systematic and simple design algorithm for the classifier is necessary in order to utilize the CVQ coding technique more effectively. In this paper, we present a classifier design algorithm for the CVQ <ref type="bibr">[lo]</ref>. In our approach, instead of attempting to classify the input vectors in the spatial domain, we propose a simple classification algorithm employing four discrete cosine transform (DCT) coefficients of the input subblock as edge oriented features. Then a classifier is designed using a cluster-seeking algorithm to ensure that the centroid of a set of vector in a class always belong to that class. The WMSE analysis shows that the classifier can reproduce the regions near to edge very faithfully because more weight is given to these regions. The weight set {tut} is implicitly determined by the classifier in our approach. In addition, we let the subcodebook size for each class be equal, rather than determining the subcodebook size experimentally. This approach makes the subcodebook design problem relatively simple and easy task, while providing adequate performance for our purpose. The differences between the approach in [l] and the work reported in this paper are the classifier design methodology and the classifier structure. Since the classification is based on the edge feature vector, a good visual quality can be obtained with smaller number of classes (typically, 16)   of the classes. Subcodebooks are designed separately for each class, and after classifying the input vectors, a search through the corresponding subcodebook completes the encoding equivalent to the gray-level distribution in the x and y directions, respectively. Then the classifier maps the input vector into one of three categories: no-edge, one-edge with different orientation process. Thus, it is evident that the classifier plays a central role in CVQ. In designing an edge oriented classifier, various edge masks are defined in order to represent the image pattern such and location, and rather assumption made in Our approach is that the edge pattern is simp1e and projections in each direction is not In region' The as edge shape, orientation and location [ll. However, in this other words, the edge segment in the subblock is restricted to a paper, rather than attempting to detect edges for classification Experimental results presented later indicate that the DCT and the Hadamard transform provide almost same performance in feature extraction. However, in our approach, we have chosen to use the DCT, since the principle advantage of employing the in associated with the x and y directions are obtained, and are given by v = {V, : i = 0 , 1 , . . . , 1 -l},</p><formula xml:id="formula_1">H = { H , : i =0,1,..-,1-1)</formula><p>to the edge detection is to expand both a given image f ( z , y )</p><formula xml:id="formula_2">(1)</formula><p>where V, and H, are the DCT coefficients of the projection in the x and y directions, respectively. It is noted that the features V and H also can be obtained directly by application of a 2-D DCT to the subblock as shown in Fig. <ref type="figure">2</ref>. Now we shall explain how to use the DCT coefficients to classify the input vectors. In Fig. <ref type="figure">3</ref> and Fig. <ref type="figure" target="#fig_2">4</ref>, we show the DCT coefficients of the synthetic image of 4 x 4 in order to demonstrate the relation between the spatial domain pattern of the subblock and the corresponding DCT coefficients. From Fig. <ref type="figure">3</ref>(a), it is seen that the direction of vertical edge is in west bound and only the DCT coefficients of x direction components are zero. Another example is illustrated in Fig. <ref type="figure">3@</ref>). Note that the direction of 135 degrees diagonal edge is in the south west bound, and also that the coefficients of both x and y direction components are not zero, indicating that the DCT coefficients have also the components in the x and y directions. From these observations, it is evident that the edge direction can be distinguished from the feature sets V, = { V, : i = 1,2,3,4}</p><p>and H e = { H , : i = 1,2,3,4}. Also note that an edge becomes more distinct as the values of V, or H e increases. As is expected, a subblock with no edge component exhibits nearly zero edge features except Vo and Ho. Next, we shall show how to identify the edge location. The edge location can be easily identified by the polarities of four DCT coefficients, namely, VI, V,, H I , and H,. For example, Fig. <ref type="figure" target="#fig_2">4</ref> illustrates the polarities of VI, V, for various vertical edges. With the related coefficient sign, we can distinguish not only the edge location, but also the polarity of edge (i.e., whether the intensity transition across the edge is from high to low, or vice versa). Thus it is concluded that the four DCT coefficients are an appropriate set of feature for edge-oriented classifier. After selecting the edge-oriented features, the remaining step is to devise a classification rule. In this paper, in order to avoid the use of thresholds as in [l], we shall consider a classifier whose decision functions are generated from training patterns by means of iterative "learning algorithm." One approach to the problem of identifying groups of similar pattern in a given set of training data is to search for clusters in these data. In our approach, the LBG algorithm, which is one of cluster-seeking algorithms, is used to find m different classes (clusters). Thus, once the number of classes is specified, the classification procedure is quite automatic in nature, compared to the approach used in [l]. in classifying the class. The classes with relatively smaller Vl and H I , which locates near to the origin, represent no edge or mild region. On the other hand, the classes with larger VI and/or H1 represent the subblocks with distinct edge component. We also show the corresponding 16 spatial domain patterns of 4 x 4 subblock in Fig. <ref type="figure" target="#fig_4">6</ref>. As is expected from Fig. <ref type="figure" target="#fig_3">5</ref>, we can see that only one class shows two edge components inside, while the rest of classes show one-edge or no-edge. But it is noted that the proposed classifier is simple to implement, and sufficient to our purpose as demonstrated later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">WEIGHTED MEAN SQUARE ERROR</head><p>In designing the CVQ, after the classification step, the most critical issue is the determination of the optimal subcodebook size { N z * } that yields the minimum total average distortion D'. It is shown in [l] that by defining a weight set {w,} to each class, the asymptotically optimal set { Nt*} satisfies the relation w,P,D,*/N,* = constant for all i where P, is the probability of a vector being in the ith class. The quantity P,D: is called the average partial distortion for the ith class. However, the problem of determining the optimal set {Nt*} is difficult to solve analytically, since the probability density of the input is usually unknown. Thus, on the assumption that 0: = constant for all the edge classes, the set { Nt*} was determined experimentally until above requirement was approximately met [l]. The basic notion of the minimum average distortion 0: is understood, because the edge classifier in [l] separates the input vectors only according to the edge orientation and location of the edge. But it does not take account of the variance of the subblock. However, in our approach, rather than attempting to determine the set {N:} by trial and error to satisfy the above relation in the edge class, we decide that the subcodebook size for all classes is equal. It is worth to note that the encoding complexity of the CVQ in which the subcodebook size is equal for all classes is lower than that of the CVQ in which the subcodebook sizes are implicitly defined by a set of weights {wz} and P, (i.e., each weighting factor w, and P, affects only the size of the subcodebook proportionally).</p><p>Since N, = constant for all i, a set of weight {wl} needs a quite different interpretation in our approach. In fact, it turns out that the set {wz} is implicitly determined by the classifier. Now, we shall explore the property of the set {w.} further in conjunction with the proposed classifier. As mentioned previously, we designed the classifier by the use of a cluster-seeking algorithm. It is based on the minimization of a performance measure, which is defined to be the average squared distance where M is the total number of samples, S, is the set of samples belonging to the ith class, U is the edge feature vector, and v, is the ith cluster center, respectively. Note that the edge feature vector U consists of 21 DCT coefficients, where 21 is less than k. Examining the details of WMSE with the proposed classifier, let the average distortion with respect to the ith cluster center in the edge feature vector domain be</p><formula xml:id="formula_3">(3) 1 D , , = - C I/u--JI2 "CSZ n,</formula><p>where n, is the size of set S, (i.e., M = performance measure can be expressed as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">~~) .</head><p>Then, the</p><formula xml:id="formula_4">J = P,D,,.<label>(4)</label></formula><p>, = 1</p><p>The quantity P,D,, is called the partial average edge feature distortion in the ith class. In the classifier design, the cluster center vi is computed in the cluster-seeking process [2], [16] so that the average edge feature distortion in (4) is suboptimally minimized. In this case, it can be shown that [3] P,D:, = constant for all i .</p><p>(5)</p><p>This is a unique property of the proposed classifier since the cluster-seeking algorithm is employed for classification. In other words, the proposed classifier guarantees that the partial average edge feature distortion is equal for each class with equal subcodebook size. Another feature of the proposed classifier is that since the classes have been defined in the cluster-seeking process, the centroid of a set of vectors in a class always belong to that class. This is a desired property if one wants to use the finite state VQ (FSVQ) [14], <ref type="bibr">[23]</ref>. Now, it is worth to note that the classifier employing four DCT coefficients as features may be alternatively viewed as four-dimensional VQ in the DCT domain employing the WMSE with the weighting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>matrix W = ( S T ) T S T ,</head><p>where S is a selection matrix to extract 4 DCT coefficients from the DCT transformed subblock and T is the DCT transform matrix, respectively. Thus the P,D:, can be interpreted as the average WMSE distortion. Let us experimentally exploit the properties of the set of weight {w,} in more detail. It is well known that the quantization error in scalar quantization is proportional to the input signal variance. Similarly, the average distortion in VQ is proportional to the variance of the input vector [15], <ref type="bibr" target="#b10">[26]</ref>. As shown in the Appendix, an asymptotic lower bound of the average distortion DT, in VQ</p><p>[4] is given by</p><formula xml:id="formula_5">Dr. = K(k)NP2/'{(det P}"' (6)</formula><p>where K ( k ) is a constant determined by the probability density of the input and by the input vector dimension k, N is the codebooksize and P is the covariance matrix of the input vector, respectively. But, under the assumption of i.i.d. vector source, {det P}l'hcan be replaced by the geometric mean of variance of the input vector element o2 <ref type="bibr">[12]</ref>. Then DI, = K ( k ) N P 2 / " a 2 , implying that the asymptotic lower bound is proportional to the vector variance. Similarly, in CVQ, an asymptotic lower bound in the ith class is DI,, = K,(k)NL-"/"a:.</p><p>For experiments described hereafter, 16 classes were actually designed with 40960 input vectors of size of 4 x 4 using the proposed classifier at a rate of 0.625 bpp. The relation between the vector variances and the average distortion in each class is shown in Fig. <ref type="figure" target="#fig_5">7</ref>. In this experiment, it is assumed that the DCT coefficients of image is an i.i.d. Laplacian source <ref type="bibr">[24]</ref>. It is seen that the average distortion is proportional to the vector variance and there is a close agreement between the experimental values and the theoretical ones. In Fig. <ref type="figure">8</ref>, we show the relation of the vector variance o2 and the edge variance 0:. The edge variance is defined to be {n:=l a f -r o ~I z } l "</p><p>where n: and a$* represent the variance of vertical and horizontal component, respectively. It is seen that the classes with lower vector variance have lower edge variance, while the difference between the vector variance and the edge variance increases as the vector variance increases. This observation indicates that, if the input vector is a distinct edge class (i.e., the edge variance is relatively large), then more energy is compacted to the edge feature V , (or He). Because of this "compacting effect" as shown in Fig. <ref type="figure">8</ref>, we can see that the weight set {wz} is proportional to the edge variance in our approach. In other words, the larger the edge variance, the larger the weight for a class. Thus, it is expected that the partial average distortion in the distinct edge classes will be smaller than that of rather uniform or shade classes, because the classifier gives more weight to the distinct edge classes. In Fig. <ref type="figure" target="#fig_8">9</ref>, we show the partial average distortion of the sixteen classes versus the vector variance. The center line in Fig. <ref type="figure" target="#fig_8">9</ref> is the average distortion per class. It is seen that, indeed, the partial average distortion P,D: decrease as the vector variance increases. Also note that the classes below the center line in Fig. <ref type="figure" target="#fig_8">9</ref> are rather distinct edge classes. Since the partial average distortions in these classes are smaller than the average distortion per class, one can expect that the classifier can faithfully reproduce the regions near to the edge. On the other hand, the classes above the center line, which are uniform or rather shade region, suffer somewhat larger degradations relatively. However, it is noted that the contouring effect can be easily reduced by increasing the bit rate slightly or by applying the postprocessing filter such as in <ref type="bibr">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>The performance of the proposed CVQ coding technique based on a classification in the DCT domain was evaluated through computer simulations by encoding a 8 bit intensity, monochrome images of size of 512 x 512. As shown in Fig. <ref type="figure">10</ref>, three 512 x 512 images, (GIRL, LENA, and BRIDGE) outside the training set were used for testing. The training set was obtained from five different 512 x 512 images. We used the LBG algorithm to design the subcodebook for each class separately. Codebooks were designed in the range of 0.625-0.813 bpp. All our results are based upon 4 x 4 pixel subblock (i.e., k = 16). As a measure of reconstructed image quality, the peak signal-tonoise ratio (PSNR) in dB is used, defined as follows:  Table <ref type="table" target="#tab_1">I</ref> summarizes the PSNR performance of the proposed CVQ for the three images shown in Fig. <ref type="figure">10</ref>. Also shown in Table <ref type="table" target="#tab_1">I</ref> is the PSNR performance of the proposed CVQ with 32 classes. We found that the quality of the image with 16 classes is slightly better than that of 32 classes subjectively. In simulation, it was also observed that several irrelevant and insignificant  classes were designed with 32 classes. Consequently, there is a slight performance degradation in image quality with 32 classes, compared to that of 16 classes. So we believe that 16 classes are appropriate for our purpose. But it is noted that the encoding complexity for 32 classes is reduced by a factor of about 2 as compared to that of 16 classes. It is not possible to subjectively compare the image coding results of <ref type="bibr">Ramamurthi and Gersho [l]</ref> to the present results. However, the CVQ with the proposed classifier enjoys a better PSNR performance over <ref type="bibr">[l]</ref>. In case of the LENA image, with 16 classes, the reported PSNR in <ref type="bibr">[l]</ref> is 29.79 dB at 0.7 bpp and 31.55 dB at 1.0 bpp, respectively, while the CVQ with the Next, in order to compare the PSNR performance between the DCT and the Hadamard transform in feature selection for classifier, we repeated the simulation with the classifier in which the Hadamard transform coefficients are used as feature vectors. It is found that the PSNR performance of the DCT based classifier is almost same as that of the Hadamard based classifier. In fact, it is observed the PSNR difference is less than 0.2 dB. However, as explained previously, the DCT based approach provides the advantage of combining the relatively recent VQ technique with the well-established DCT transform coding technique [21]. Thus, it is worthwhile to give some remarks on the CVQ (or VQ) in the transform domain approach, namely, the DCT-CVQ [21] (or DCT-VQ). Since the DCT is a distance preserving transformation, the average distortion in the spatial domain is the same as the average distortion in the transform domain as long as the vector dimension k in the spatial domain is equal to that in the transform domain. But the energy compaction property of the DCT allows us to reduce the vector dimension for VQ in the transform domain by retaining k' (k' &lt; k) significant DCT coefficients and setting the rest of the coefficients to zero. As a result, the encoding complexity can be further reduced. We found that about 30% reduction in encoding complexity can be obtained by adopting this approach, while providing the same image quality. However, it is felt that small subblock size of 4 x 4 is not effective in transform coding. When we use larger subblock size, such as 8 x 8 or 16 x 16, the input vector dimension is too large to be vector quantized. The complexity of VQ usually limits the vector dimension to about 16-20. Thus, it is necessary to partition the transformed subblock into several smaller vectors. Thus, further improvement in DCT-CVQ performance would be expected if an adaptive zonal coding, in addition to the optimum partition scheme, and the optimum bit allocation for each class were used [21]. But these problems need further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we have described a simple classification technique for the CVQ encoder for image coding. We considered the classifier as a typical pattern recognition system. For a classifier design, the underlying assumption made is that the spatial pattern of the subblock can be represented by two projection components in the x and y direction. The proposed classifier employed four DCT coefficients,of 4 x 4 subblock as edge oriented features.</p><p>The WMSE analysis revealed that the classifier gives more weight to the distinct edge classes. Thus, in addition to the edge location and orientation preserving properties, the classifier can faithfully reconstruct the region near to the edge. Simulation results showed that a good visual quality of the coded image at fixed rates in the 0.625-0.813 bpp is obtained with comparable complexity. The classifier algorithm developed here offers two advantages over the previously used in thresholds in detecting edges. Second, since the classification is performed in the DCT domain, this approach can be easily extended to the well-established transform coding technique, namely, DCT-CVQ <ref type="bibr">[21]</ref>. Finally, since the classes have been defined in the cluster-seeking process, the classifier ensures that the representative vectors of the subcodebook in a class always belong to that class. This is a desired property when we employ the FSVQ. Thus, we feel that the FSVQ or the predictive VQ approach, combining with the proposed classification technique, will provide a better performance because the edge block can be predicted by the adjacent blocks. These are future research topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPENDIX</head><p>An asymptotic lower bound of vector quantizer for large N was derived by <ref type="bibr">Gersho [3]</ref> and <ref type="bibr">Yamada,</ref><ref type="bibr">Tazaki,</ref><ref type="bibr">and Gray [4]</ref>. <ref type="bibr">In [4]</ref>, the asymptotic lower bound in terms of a conventional quadratic norm based distance function is given by where X(x) is the probability density function of the reproduction vectors, Vk is the "unit cube" in R k , respectively. By using a weighted quadratic norm llzl12 = z T B x , vi becomes [4], <ref type="bibr">[13]</ref> where r(.) is the gamma function. And since the covariance matrix P is positive definite, there exists a nonsingular matrix Q such that (A61</p><formula xml:id="formula_6">P = E { Z Z ~} = Q Q ~. (<label>A31</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>an image is partitioned into contiguous subblocks and the input vectors are formed with the pixels of the subblocks. These vecton are input to the encoder and classified into one edge component L 8 1 9 then the edge projection components in the as shown in Fig* be represented wiih the and Y directions [17i? In Fig* '7 the projection "POnents are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[6], we consider the classifier as a typical pattern recognition system. The first step in any pattern recognition process is how finite set Of edge patterns such as assumption edge classes. It may Or "ring" Or "L-shaped*" This more as the subblock size to select and how to extract the features. Any object or pattern in size* which can be recognized and classified possesses a number of discriminatory properties or features. In CVQ, it is desired that the selected feature should possess a very important portion of Now? we can use the Dm in Order to extract the edge features By performing a l-D Dm Dm transformed vectors from the On the each projection component, projection the perceptual information content in an image. For example, edge is an important feature perceptually in the image. Thus, the problem leads to the selection of feature in conjunction with edge in the image. But it is noted that one of well known approaches and an ideal step edge s(z,y) in terms of a set of orthogonal basis function, and use the sum of the squared difference between corresponding coefficients as an error measure [16]. Hueckel [19] was the first to take this approach using a set of eight basis functions defined on a disk. Later, Hummel [20] used a set of optimal basis functions derived from the Karhunen-Loeve (KL) expansion of the local image values. The object of an edge feature detector using a basis function is to find the parameter values of the edges or lines closest to the local input data. Motivated by these works, we shall show that in the transform domain an edge oriented classifier also can be designed. At this point we should mention that the idea of using orthogonal basis functions to assist in feature selection is a standard technique in the field of pattern recognition [16]. Ideally, we can make use of the KL transform in carrying out feature extraction. However, implementation of the KL transform involves a determination of the eigenvalues and corresponding eigenvectors of the covariance matrix, and there is no general algorithm for their fast computation. In view of the problem associated with the implementation of KL transform, other discrete orthogonal transforms, although not optimal, can be utilized [6], [12], [17], [NI, [25]. For example, the Hadamard transform, which can be considered as an edge pattern matching, exhibits many fundamental edge patterns, and transform coefficients of that pattern has meaningful values of the edge. However, in the transform domain, another alternative is to use the DCT, which is asymptotically equivalent to the KL transform [5].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 1. Edge components and their projections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5. Scatter diagram (16 classes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>IFig 6 .</head><label>6</label><figDesc>Fig 6. The 16 representative spatial domain patterns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The vector variance and the average distortion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig</head><label></label><figDesc>Fig. ll(c) shows a magnified portion of the reconstructed image of Fig. 11@). It is observed that the edge integrity is faithfully maintained and the contouring effect in the monotone shoulder area is noticeably diminished. The performance improvement in increasing the bit rate is further demonstrated as examining the reconstructed image at 0.813 bpp as shown in Fig. ll(d). There is no detectable degradation in the reconstructed image. The subjective improvement over Fig. 1 l(a) is significant. Another encoding results on the BRIDGE image are shown in Fig. 12. TableIsummarizes the PSNR performance of the proposed CVQ for the three images shown in Fig.10. Also shown in TableIis the PSNR performance of the proposed CVQ with 32 classes. We found that the quality of the image with 16 classes is slightly better than that of 32 classes subjectively. In simulation, it was also observed that several irrelevant and insignificant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig. 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>:Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The partial average distortion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>[l]. First, the classification is quite automatic in nature, since it does not require to use the ( 4 Fig. 10. Original images (512 x 512, dbpp). (a) GIRL. (b) LENA. (c) BRIDGE. 16. m = 16). (a) 0.625 buu. bpp. (c) 0.75 bpp (magnified image). (d) 0.813 bpp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. CVQ simulation results (IC = 16, m = 16). (a) 0.75 bpp. (b) 0.813 bpp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>But (A3) can be rewritten asI = E { Q -'z z ~Q -~} = ~{ y y ~} (A41 where y = Q -' x is a source vector which has unit variance and is mutually uncorrelated. If we use a nonweighting quadratic norm 11 z 11 = zTx, then it can be expressed as where P = B in (A2). Therefore we can obtain an asymptotic distortion function in (A6), which includes the determinant function of input vector covariance matrix P. DL = K(k)N-2/k{det P } ' l k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>l]. 0090-6778/91/04~0549$01.00 0 1991 IEEE The</head><label></label><figDesc>rest of the paper is organized as follows. In Section 11, we describe the general methodology for the classifier design and devote to the development of the classifier based on classification in the DCT domain. In Section 111, several experimental results to verify the WMSE analysis are described and discussed. Computer simulation results on the several natural images are presented in Section IV. Finally, we give our conclusions.</figDesc><table><row><cell>DCT coefficients in feature selection (i.e., classification is done</cell></row><row><cell>in the DCT domain) is that it can be easily extended to the</cell></row><row><cell>DCT-CVQ coding technique [21]. The DCT-CVQ has received</cell></row><row><cell>a great interest recently because this techniques takes advantage</cell></row><row><cell>of both VQ and transform coding techniques. However, in</cell></row><row><cell>this paper, we shall not consider the DCT-CVQ technique</cell></row><row><cell>further. Subsequently, the methodology for classifier design in</cell></row><row><cell>the DCT-domain is described. Suppose a subblock is square,</cell></row><row><cell>which consists of k pixels of image, and also it is assumed that the</cell></row><row><cell>at rates in the range of 0.625-0.813 bpp, while the encoding complexity comparable to in [11. A CLASSIFTER DESIGN edge pattern of the subblock can be characterized with only one</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I CVQ SIMULATION RESULTS ( k = 16) m bPP</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>GIRL</cell><cell>LENA</cell><cell>BRIDGE</cell></row><row><cell></cell><cell>0.625</cell><cell>31.76</cell><cell>31.26</cell><cell>25.64</cell></row><row><cell>16</cell><cell>0.688</cell><cell>32.34</cell><cell>31.79</cell><cell>26.01</cell></row><row><cell></cell><cell>0.750</cell><cell>32.88</cell><cell>32.23</cell><cell>26.36</cell></row><row><cell></cell><cell>0.813</cell><cell>33.27</cell><cell>32.57</cell><cell>26.64</cell></row><row><cell></cell><cell>0.625</cell><cell>31.51</cell><cell>31.06</cell><cell>25.62</cell></row><row><cell>32</cell><cell>0.688</cell><cell>32.18</cell><cell>31.68</cell><cell>25.99</cell></row><row><cell></cell><cell>0.750</cell><cell>32.78</cell><cell>32.15</cell><cell>26.31</cell></row><row><cell></cell><cell>0.813</cell><cell>33.19</cell><cell>32.52</cell><cell>26.62</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(PSNR)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classified vector quantization of image</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ramamurthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1105" to="1115" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Asymptotically optimal block quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="373" to="380" />
			<date type="published" when="1979-07">July 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Asymptotic performance of block quantizers with difference distortion measure</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="6" to="14" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discrete cosine transform</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="90" to="93" />
			<date type="published" when="1974-01">Jan. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Pratt</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>New York Wiley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The role of human visual models in image processing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Granrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1981-05">May 1981</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Edge-preserving image coding based on local modeling of images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1320" to="1323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Picture coding: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Limb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1980-03">Mar. 1980</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="366" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image vector quantization based on a classification in the DCT domain</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transform Coding of Images</title>
		<meeting><address><addrLine>New York Academic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Proc. IEEE TENCON</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Asymptotic quantization error of continuous signals and the quantization dimension</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Zador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="139" to="149" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Digital Coding of Wuveforms</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Prudnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Brychkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">0</forename><forename type="middle">I</forename><surname>Marichev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integrals and Series</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Queen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1986">1986</date>
			<publisher>Gordon and Breach Science Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-rate image coding with finitestate vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1986-04">Apr. 1986</date>
			<biblScope unit="page" from="137" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<title level="m">Rate Distortion Theory</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Tou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<title level="m">Pattern Recognition Principle</title>
		<meeting><address><addrLine>New York Addison-Wesley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<title level="m">Digital Picture Processing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Fundamentals of Digital Image Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An operator which locates edges in digitized pictures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Hueckel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="113" to="125" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Edge-oriented spatial filtering of images with application to post-processing of vector quantized images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramamurthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1979-05">1979. May 1989,. Mar. 1984</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="48" to="58" />
		</imprint>
	</monogr>
	<note>Proc. IEEE ICASSP</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">New finite state vector quantizers for images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
			<biblScope unit="page" from="1180" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">He received the B.S. and M.S. degrees from Seoul National University, Seoul, Korea, all in the electrical and electronic engineering field, in 1986 and 1988, respectively. He served in the Republic of Korea Air Forces, and is now working toward the Ph.D. degree in the Department of</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Reininger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1831" to="1834" />
			<date type="published" when="1963">June 1983. 1963. 1973. 1976. 1980</date>
			<publisher>MA-COM Research Center</publisher>
			<pubPlace>Seoul, Korea; Ames, IA; Los Angeles, CA; Lynchburg, Virginia; Rockville, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Control and Instrumentation Engineering, Seoul National University ; S. degree from Seoul National University ; Iowa State University ; University of Southern Califomia</orgName>
		</respStmt>
	</monogr>
	<note>all in electrical engineering. In 1983, he joined the Department of Control and Instrumentation Engineering at Seoul National University as an Assistant Professor, where he is now an Associate Professor. In 1980-1981, he was with the General Electric Company. and in 1981-1983, he was a Member of Technical Staff. His current research interests are in the areas of image, speech signal processing and communication, including HDTV, ATV and VLSI signal processing. He is a member of Phi Kappa Phi</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
