<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Graph Classification: A Hierarchical Graph Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
							<email>lijia@se.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@hotmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
							<email>hcheng@se.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Helen</forename><surname>Meng</surname></persName>
							<email>hmmeng@se.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>joehhuang@tencent.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab Shenzhen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Tencent AI Lab Shenzhen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Tencent AI Lab Shenzhen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<postCode>2019</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Graph Classification: A Hierarchical Graph Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3308558.3313461</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>‚Ä¢ Mathematics of computing ‚Üí Graph algorithms</term>
					<term>‚Ä¢ Information systems ‚Üí Social networks</term>
					<term>‚Ä¢ Computing methodologies ‚Üí Supervised learning by classification hierarchical graph</term>
					<term>graph embedding</term>
					<term>semi-supervised learning</term>
					<term>active learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Node classification and graph classification are two graph learning problems that predict the class label of a node and the class label of a graph respectively. A node of a graph usually represents a realworld entity, e.g., a user in a social network, or a protein in a proteinprotein interaction network. In this work, we consider a more challenging but practically useful setting, in which a node itself is a graph instance. This leads to a hierarchical graph perspective which arises in many domains such as social network, biological network and document collection. For example, in a social network, a group of people with shared interests forms a user group, whereas a number of user groups are interconnected via interactions or common members. We study the node classification problem in the hierarchical graph where a "node" is a graph instance, e.g., a user group in the above example. As labels are usually limited in realworld data, we design two novel semi-supervised solutions named SEmi-supervised grAph cLassification via Cautious/Active Iteration (or SEAL-C/AI in short). SEAL-C/AI adopt an iterative framework that takes turns to build or update two classifiers, one working at the graph instance level and the other at the hierarchical graph level. To simplify the representation of the hierarchical graph, we propose a novel supervised, self-attentive graph embedding method called SAGE, which embeds graph instances of arbitrary size into fixed-length vectors. Through experiments on synthetic data and Tencent QQ group data, we demonstrate that SEAL-C/AI not only outperform competing methods by a significant margin in terms of accuracy/Macro-F1, but also generate meaningful interpretations of the learned representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph has been widely used to model real-world entities and the relationship among them. Two graph learning problems have received a lot of attention recently, i.e., node classification and graph classification. Node classification is to predict the class label of nodes in a graph, for which many studies in the literature make use of the connections between nodes to boost the classification performance. For example, <ref type="bibr" target="#b24">[25]</ref> enhances the recommendation precision in LinkedIn by taking advantage of the interaction network, and <ref type="bibr" target="#b26">[27]</ref> improves the performance of document classification by exploiting the citation network. Graph classification, on the other hand, is to predict the class label of graphs, for which various graph kernels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> and deep learning approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> have been designed. In this work, we consider a more challenging but practically useful setting, in which a node itself is a graph instance. This leads to a hierarchical graph in which a set of graph instances are interconnected via edges. This is a very expressive data representation, as it considers the relationship between graph instances, rather than treating them independently. The hierarchical graph model applies to many real-world data, for example, a social network can be modeled as a hierarchical graph, in which a user group is represented by a graph instance and treated as a node in the hierarchical graph, and then a number of user groups are interconnected via interactions or common members. As another example, a document collection can be modeled as a hierarchical graph, in which a document is regarded as a graph-of-words <ref type="bibr" target="#b25">[26]</ref>, and then a set of documents are interconnected via the citation relationship. In this paper, we study graph classification in a hierarchical graph, which predicts the class label of graph instances in a hierarchical graph.</p><p>One challenge in this problem is that a hierarchical graph is a much too complicated input for building a classifier. To tackle this challenge, we design a new graph embedding method which embeds a graph instance of arbitrary size into a fixed-length vector. All graph instances in the hierarchical graph are transformed to embedding vectors which are the common input format for classification. Specifically, the embedding method builds an instance-level classifier called IC from graph instances, and produces embedding vectors and predicted class probabilities of the graph instances. Another classifier HC at the hierarchical graph level takes the embedding vectors and their connections as input, and outputs the predicted class probabilities of all graph instances. To enforce a consistency between the two classifiers, we define a disagreement loss to measure the degree of divergence between the predictions by them and aim to minimize the disagreement loss.</p><p>Another challenge is that the amount of available class labels is usually very small in real-world data, which limits the classification performance. To address this challenge, we take a semi-supervised learning approach to solving the graph classification problem. We design an iterative algorithm framework which takes turns to build or update classifiers IC and HC. We start with the limited labeled training set and build IC, which produces the embedding vectors of graph instances. HC takes the embedding vectors as input and produces predictions. We cautiously select a subset of predicted labels by HC with high confidence to enlarge the training set. The enlarged training set is then fed into IC in the next iteration to update its parameters in the hope of generating more accurate embedding vectors and predictions. HC further takes the new embedding vectors for model update and class prediction. This is our proposed solution, called SEmi-supervised grAph cLassification via Cautious Iteration (SEAL-CI), to the graph classification problem.</p><p>We also extend this iterative algorithm to the active learning framework, in which we iteratively select the most informative instances for manual annotation, and then update the classifiers with the newly labeled instances in a similar process as described above. This method is called SEAL-AI in short.</p><p>Our contributions are summarized as follows.</p><p>‚Ä¢ We study semi-supervised graph classification from a hierarchical graph perspective, which, to the best of our knowledge, has not been studied before. Our proposed solutions SEAL-C/AI achieve superior classification performance to the state-of-the-other graph kernel and deep learning methods, even when given very few labeled training instances. ‚Ä¢ We design a novel supervised, self-attentive graph embedding method called SAGE to embed graphs of arbitrary size into fixed-length vectors, which are used as a common form of input for classification. The embedding approach not only simplifies the representation of a hierarchical graph greatly, The remainder of this paper is organized as follows. Section 2 gives the problem definition and Section 3 describes the design of SEAL-C/AI. We report the experimental results in Section 4 and discuss related work in Section 5. Finally, Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><p>We denote a set of objects as O = {o 1 , o 2 , . . . , o N } which represent real-world entities. We use œï attributes to describe properties of objects, e.g., age, gender, and other information of people.</p><p>We use a graph instance to model the relationship between objects in O, which is denoted as –¥ = (V , A, X ), V ‚äÜ O is the node set and |V | = n, A is an n √ó n adjacency matrix representing the connectivity in –¥, and X ‚àà R n√óœï is a matrix recording the attribute values of all nodes in –¥.</p><p>A set of graph instances G can be interconnected, and the connectivity between the graph instances is represented by an adjacency matrix Œò. The graph instances and their connections are modeled as a hierarchical graph.</p><p>A graph instance –¥ ‚àà G is a labeled graph if it has a class label, represented by a vector y ‚àà {0, 1} c , where c is the number of classes. A graph instance is unlabeled if its class label is unknown. Then G can be divided into two subsets: labeled graphs G l and unlabeled graphs G u , where</p><formula xml:id="formula_0">G = G l ‚à™ G u , |G l | = L and |G u | = U .</formula><p>In this paper, we study the problem of graph classification, which determines the class label of the unlabeled graph instances in G u from the available class labels in G l and the hierarchical graph topological structure. As the amount of available class labels is usually very limited in real-world data, we take a semi-supervised learning approach to solving this problem.</p><p>Figure <ref type="figure">1</ref> depicts a hierarchical graph in the context of a social network. A, B, C, D denote four user groups. Group A has the class label of "game", B has the label of "non-game", while the class labels of C and D are unknown. These four groups are connected via some kind of relationships, e.g., interactions or common members. The internal structure of each user group shows the connections between individual users. From this hierarchical graph, we want to determine the class labels of groups C and D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Problem Formulation</head><p>In our problem setting, we have two kinds of information: graph instances and connections between the graph instances, which provide us with two perspectives to tackle the graph classification problem. Accordingly, we build two classifiers: a classifier IC constructed for graph instances and a classifier HC constructed for the hierarchical graph, both of which make predictions for unlabeled graph instances in G u .</p><p>For both classifiers, one goal is to minimize the supervised loss, which measures the distance between the predicted class probabilities and the true labels. Another goal is to minimize a disagreement loss, which measures the distance between the predicted class probabilities by IC and HC. The purpose of this disagreement loss is to enforce a consistency between the two classifiers.</p><p>Formally, we formulate the graph classification problem as an optimization problem:</p><formula xml:id="formula_1">min Œ∂ (G l ) + Œæ (G u ),<label>(1)</label></formula><p>where Œ∂ (G l ) is the supervised loss for the labeled graph instances, and Œæ (G u ) is the disagreement loss for the unlabeled graph instances. Specifically, Œ∂ (G l ) includes two parts:</p><formula xml:id="formula_2">Œ∂ (G l ) = –¥ i ‚ààG l (L(y i ,œà i ) + L(y i , Œ≥ i )),<label>(2)</label></formula><p>where œà i is a vector of predicted class probabilities by IC, and Œ≥ i is a vector of predicted class probabilities by HC. L(‚Ä¢, ‚Ä¢) is the cross-entropy loss function.</p><p>The disagreement loss Œæ (‚Ä¢) is defined as:</p><formula xml:id="formula_3">Œæ (G u ) = –¥ i ‚ààG u D K L (Œ≥ i ||œà i ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">D K L (‚Ä¢||‚Ä¢) is the Kullback-Leibler divergence, D K L (P ||Q ) = j P j log P j Q j .</formula><p>In the following subsections, we describe our design of classifiers IC and HC, and our approach to minimizing the supervised loss and the disagreement loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Design of Classifiers</head><p>Classifier IC takes a graph instance as input. As different graph instances have different numbers of nodes, IC is expected to handle graph instances of arbitrary size. Classifier HC takes the hierarchical graph as input, in which individual graph instances are the "nodes". This is a much too complicated input for a classifier. To deal with the above challenges, we propose to embed a graph instance –¥ i ‚àà G into a fixed-length vector e i via IC first. Then HC can take as input the embedding vectors of graph instances and the adjacency matrix Œò. In particular, IC takes as input the adjacency matrix A i and attribute matrix X i of an arbitrary-sized graph instance –¥ i , and outputs an embedding vector e i and a vector of predicted class probabilities œà i , i.e., (e i ,œà i ) = IC(A i , X i ). HC takes the embedding vectors E = {e i } L+U i=1 and Œò, and outputs the predicted class probabilities Œì = {Œ≥ i } L+U i=1 , i.e., Œì = HC(E, Œò). In the following, we illustrate the design of IC which performs discriminative graph embedding, and then the design of HC which performs graph-based classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Discriminative graph embedding.</head><p>Our graph embedding task is to produce a fixed-length discriminative embedding vector of a graph instance. In the literature, graph representation techniques have recently shifted from hand-crafted kernel methods <ref type="bibr" target="#b32">[33]</ref> to neural network based end-to-end methods, which achieve better performance in graph-structured learning tasks. In this vein, we adopt neural network methods for the graph embedding task, for which, however, we identify three challenges:</p><p>‚Ä¢ Size invariance: How to design the neural network structure to flexibly take an arbitrary-sized graph instance and produce a fixed-length embedding vector? ‚Ä¢ Permutation invariance: How to derive the representation regardless of the permutation of nodes? ‚Ä¢ Node importance: How to encode the importance of different nodes into a unified embedding vector?</p><p>In particular, the third challenge is node importance, i.e., different nodes in a graph instance have different degrees of importance. For example, in a "game" group the "core" members should be more important than the "border" members in contributing to the derived embedding vector. We need to design a mechanism to learn the node importance and then encode it in the embedding vector properly.</p><p>To this end, we propose a self-attentive graph embedding method, called SAGE, which can take a variable-sized graph instance, and combine each node to produce a fixed-length vector according to their importance within the graph. In SAGE, we first utilize a multilayer GCN <ref type="bibr" target="#b15">[16]</ref> to smooth each node's features over the graph's topology. Then we use a self-attentive mechanism to learn the node importance and then transform a variable number of smoothed nodes into a fixed-length embedding vector, as proposed in <ref type="bibr" target="#b17">[18]</ref>. Finally, the embedding vector is cascaded with a fully connected layer and a softmax function, in which the label information can be leveraged to discriminatively transform the embedding vector e into œà . Figure <ref type="figure" target="#fig_0">2</ref> depicts the overall framework of SAGE.</p><p>Formally, we are given the adjacency matrix A ‚àà R n√ón and the attribute matrix X ‚àà R n√óœï of a graph instance –¥ as input. In the preprocessing step, the adjacency matrix A is normalized: where I n is the identity matrix and Dii = m (A + I n ) im . Then we apply a two-layer GCN network:</p><formula xml:id="formula_5">√Ç = D‚àí 1 GCN GCN ùëí ‚àà ‚Ñù ùëü√óùë£ Embedding Matrix + Softmax ùêª ‚àà ‚Ñù ùëõ√óùë£ ùëÜ ‚àà ‚Ñù ùëü√óùëõ Self Attention ùêª ‚àà ‚Ñù ùëõ√óùë£ Two Step Smoothing</formula><formula xml:id="formula_6">H = √Ç ReLU( √ÇXW 0 )W 1 .</formula><p>(</p><formula xml:id="formula_7">)<label>5</label></formula><p>Here W 0 ‚àà R œï√óh and W 1 ‚àà R h√óv are two weight matrices. GCN can be considered as a Laplacian smoothing operator for node features over graph structures, as pointed out in <ref type="bibr" target="#b16">[17]</ref>. Then we get a set of representation H ‚àà R n√óv for nodes in –¥. Note that the representation H does not provide node importance, and it is size variant, i.e., its size is still determined by the number of nodes n. So next we utilize the self-attentive mechanism to learn node importance and encode it into a unified graph representation, which is size invariant:</p><formula xml:id="formula_8">S = softmax W s2 tanh(W s1 H T ) ,<label>(6)</label></formula><p>where W s1 ‚àà R d √óv and W s2 ‚àà R r √ód are two weight matrices. The function of W s1 is to linearly transform the node representation from a v-dimensional space to a d-dimensional space, then nonlinearity is introduced by tying with the function tanh. W s2 is used as r views of inferring the importance of each node within the graph. It acts like inviting r experts to give their opinions about the importance of each node independently. Then softmax is applied to derive a standardized importance of each node within the graph, which means in each view the summation of all the node importance is 1.</p><p>After that, we compute the final graph representation e ‚àà R r √óv by multiplying S ‚àà R r √ón with H ‚àà R n√óv :</p><formula xml:id="formula_9">e = SH .<label>(7)</label></formula><p>e is size invariant since it does not depend on the number of nodes n any more. It is also permutation invariant since the importance of each node is learned regardless of the node sequence, and only determined by the task labels. One potential risk in SAGE is that r views of node importance may be similar. To diversify their views of node importance, a penalization term is imposed:</p><formula xml:id="formula_10">P = SS T ‚àí I r 2 F . (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>Here ‚Ä¢ F represents the Frobenius norm of a matrix. We train the classifier in a supervised way with the task at hand, in the hope of minimizing both the penalization and the cross-entropy loss.</p><p>To summarize, we use SAGE to construct the instance-level classifier IC. It produces not only the estimated class probability vector œà , but also a graph embedding e, which is the input for classifier HC described in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Graph-based classification. Given the graph embedding</head><formula xml:id="formula_12">E = {e i } L+U</formula><p>i=1 and the adjacency matrix Œò ‚àà R (L+U )√ó(L+U ) , our next task is to infer the parameters of classifier HC and derive the predicted probabilities Œì = {Œ≥ i } L+U i=1 . This problem falls into the setting of traditional graph-based learning where E can be treated as the set of node features. Recently neural network based approaches such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref> have demonstrated their superiority to traditional methods such as ICA <ref type="bibr" target="#b26">[27]</ref>. In this context we make use of GCN <ref type="bibr" target="#b15">[16]</ref> again for the consideration of efficiency and effectiveness. In the following, we consider a two-layer GCN and apply preprocessing by Œò = DŒò</p><formula xml:id="formula_13">‚àí 1 2 (Œò + I L+U ) DŒò ‚àí 1 2 .</formula><p>Then the model becomes:</p><formula xml:id="formula_14">Œì = HC (E, Œò) = softmax Œò ReLU( ŒòEW 0 Œò )W 1 Œò ,<label>(9)</label></formula><p>where W 0 Œò ‚àà R (rv )√óM is an input-to-hidden weight matrix with M feature maps and W 1 Œò ‚àà R M √óc is a hidden-to-output weight matrix. The softmax function is applied row-wise and we get Œì. With Œì and Œ® we can compute the supervised loss in problem (2) and the disagreement loss in problem (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Proposed SEAL-CI Model</head><p>In this subsection, we present our method to minimize the objective function <ref type="bibr" target="#b0">(1)</ref>. In real-world scenarios, the number of labeled graph instances L can be quite small compared to the number of unlabeled instances U . In this context, neural network based classifiers such as IC may suffer from the problem of overfitting. To mitigate this, we have both the disagreement loss (3) and the supervised loss (2) included in the objective function <ref type="bibr" target="#b0">(1)</ref>. The disagreement loss can be regarded as a regularization to prevent overfitting.</p><p>Problem ( <ref type="formula" target="#formula_1">1</ref>) is a mixed combinatorial and continuous optimization problem. The supervised loss (2) includes two parts, L(y i ,œà i ) and L(y i , Œ≥ i ), i.e., the supervised loss of IC and HC. L(y i , Œ≥ i ) depends on classifier IC to provide accurate graph embedding. All these issues make the problem highly non-convex. As such, we use the idea of iterative algorithm to alternate minimizing the supervised loss of IC and HC, and minimizing the disagreement loss by trusting a subset of predictions by HC in the next iteration of graph embedding by IC.</p><p>To be more specific, we combine the graph embedding algorithm in Section 3.2.1 and graph-based classification algorithm in Section 3.2.2 into one iterative algorithm. We build IC to produce graph embedding E t for all graph instances in iteration t, and then feed E t into HC to get the predicted probabilities Œì t . We then make use of Œì t to update the parameters of IC and generate E t +1 , which is</p><formula xml:id="formula_15">Algorithm 1: SEAL-CI Input: A, X , Œò. Output: Œ® t , Œì t . 1 Initial: G tmp = ‚àÖ, G 0 l = G l , t = 0; 2 while tŒª ‚â§ U do 3 W t +1 ‚Üê arg min Œ∂ (G t l |W t ); 4 Œ® t +1 , E t +1 ‚Üê IC(A, X |W t +1 ); 5 Œì t +1 ‚Üê HC(E t +1 , Œò|W t +1 ); 6 G tmp ‚Üê h(tŒª, Œì t +1 G u ); 7 G t +1 l ‚Üê G l ‚à™ G tmp ; 8 G tmp = ‚àÖ; 9 Return Œ® t , Œì t ;</formula><p>then used as the input of HC in iteration t + 1. Figure <ref type="figure" target="#fig_1">3</ref> depicts the overall framework of this iterative process. Although this method may not reach the global optimum, similar setting <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref> has been proven to be effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">How to utilize Œì t ?</head><p>To update the graph embedding vectors, a naive approach is feeding the whole set of Œì t for the parameter update in IC, which is the idea of the original ICA <ref type="bibr" target="#b26">[27]</ref>. However, not all Œì t are correct in their predictions. The false predictions may lead the update of embedding neural network to the wrong direction.</p><p>To this end, we make use of the idea of <ref type="bibr" target="#b19">[20]</ref>, a variant of the original ICA, and cautiously exploit a subset of Œì t to update the parameters of IC in each iteration. Specifically, in iteration t, we choose the tŒª most confident predicted labels while ignoring the less confident predicted labels. This operation continues until all the unlabeled samples have been utilized. To further improve the efficiency, the parameters of IC are not re-trained but fine-tuned based on the parameters obtained in the previous iteration. This algorithm is called SEmi-supervised grAph cLassification via Cautious Iteration (SEAL-CI) and is presented in Algorithm 1. Note here W is the set of all the parameters of IC and HC. In line 6, the training set for IC has been enlarged by tŒª instances and it is done by "committing" these instances' labels from their maximum probability. In other words, the newly enrolled training instances are found by:</p><formula xml:id="formula_16">h(Œª, Œì) = top(max Œ≥ ‚ààŒì Œ≥ , Œª).<label>(10)</label></formula><p>Here function top(‚Ä¢, Œª) is used to select the top Œª instances and function max Œ≥ is used to select the maximum value in the probability vector Œ≥ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Proposed SEAL-AI Model</head><p>Our proposed model is easy to extend to the active learning scenario. In case further annotation is available, we can perform active learning and ask for annotations with a budget of B. Denote the set of graph instances being annotated as G B , then the objective function in the active learning setting is re-written as:</p><formula xml:id="formula_17">min f (G |B, W ) s.t. |G B | ‚â§ B,<label>(11)</label></formula><p>where</p><formula xml:id="formula_18">f |B, W ) = Œ∂ (G l ‚à™ G B |W ) + Œæ (G u \ G B |W )</formula><p>. This is still a mixed combinatorial and continuous optimization problem. It is At the beginning of this iterative process, we optimize the supervised loss Œ∂ (G l |W ) based on current labeled graphs in G l (line 3 in Algorithm 2). In active learning, the choice of candidate generator is a key component. We exploit the idea of ALFNET <ref type="bibr" target="#b0">[1]</ref> and choose the candidate graph instances G tmp by maximizing the decrease of the current disagreement loss based on the new parameter obtained in the first step (line 6 in Algorithm 2). At last we label G tmp and update G B , G l and G u respectively (line 7-9 in Algorithm 2).</p><formula xml:id="formula_19">Algorithm 2: SEAL-AI Input: A, X , Œò. Output: Œ® t , Œì t . 1 Initial: G tmp = ‚àÖ,G 0 B = ‚àÖ, G 0 l = G l , G 0 u = G u , t = 0; 2 while |G t B | ‚â§ B do 3 W t +1 ‚Üê arg min Œ∂ (G t l |W t ); 4 Œ® t +1 , E t +1 ‚Üê IC(A, X |W t +1 ); 5 Œì t +1 ‚Üê HC(E t +1 , Œò|W t +1 ); 6 G tmp ‚Üê arg min |G t mp |=k Œæ (G t u \ G tmp |W t +1 ); 7 G t +1 B ‚Üê G t B ‚à™ G tmp ; 8 G t +1 l ‚Üê G t l ‚à™ G tmp ; 9 G t +1 u ‚Üê G t u \ G tmp ; 10 G tmp = ‚àÖ;</formula><p>It is worth noting that from the hard example mining perspective, the disagreement score is an excellent criterion for the active learning setting. Specifically, we choose the candidates by first calculating the distribution divergence of</p><formula xml:id="formula_20">(Œ≥ i ,œà i ) from Œì u = {Œ≥ i } U i=1 and Œ® u = {œà i } U i=1 : z(œà i , Œ≥ i ) = D K L (Œ≥ i ||œà i ).<label>(12)</label></formula><p>Then we choose k instances with the largest KL divergence. Intuitively, the KL divergence between œà i and Œ≥ i can be viewed as the conflict of two supervised models. A large KL divergence indicates that one of the models gives wrong predictions. To this end, the instances with a large KL divergence are more informative to help the algorithm converge more quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Complexity Analysis</head><p>We analyze the computational complexity of our proposed methods.</p><p>Here we only focus on Algorithm 1, since Algorithm 2 is almost the same except the step of selecting candidate graph instances to the training set. In Algorithm 1, the intensive parts in each iteration contain the updates of IC and HC as well as the selection of candidate instances. We discuss each part in details below.</p><p>Regarding IC, the core is to compute the activation matrix H in Eq. ( <ref type="formula" target="#formula_7">5</ref>) where the matrix-vector multiplications are up to O (E 1 œï)  flops for one input graph instance; here E 1 denotes the number of edges in the graph instance and œï is the input feature dimension. Thus, it leads to the complexity of O (E 1 (L +U )œï) by going through all L + U graph instances. Next, the computation by HC in Eq. ( <ref type="formula" target="#formula_14">9</ref>) requires O (E 2 rv) flops in total, where E 2 denotes the number of links between graph instances. Then in candidate selection, performing comparisons between all unlabeled graph instances has a complexity of O (L +U ) given the outputs of two classifiers IC and HC.</p><formula xml:id="formula_21">ùëí 1 ùëí 2 ùëí 3 ùëí 4 ‚Ñé(Œì)</formula><p>Overall, the complexity of our method is O (E 1 (L + U )œï + E 2 rv) which scales linearly in terms of the number of edges in each graph instance (i.e., E 1 ), the number of links between graph instances (i.e., E 2 ) and the number of graph instances (i.e., (L + U )). Thus, our method is computationally comparable to the GCN-based method <ref type="bibr" target="#b15">[16]</ref>, and more efficient than PSCN <ref type="bibr" target="#b22">[23]</ref> that is quasi-linear with respect to the numbers of nodes and edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We first validate the effectiveness of our graph embedding algorithm SAGE on two data sets: PROTEINS and D&amp;D. Then we evaluate our SEAL-C/AI methods on both synthetic and Tencent QQ group data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance of SAGE</head><p>We use two benchmark data sets, PROTEINS and D&amp;D, to evaluate the classification accuracy of SAGE, and compare it with the stateof-the-art graph kernels and deep learning approaches. PROTEINS <ref type="bibr" target="#b3">[4]</ref> is a graph data set where nodes are secondary structure elements and edges represent that two nodes are neighbors in the amino-acid sequence or in 3D space. D&amp;D <ref type="bibr" target="#b6">[7]</ref> is a set of structures of enzymes and non-enzymes proteins, where nodes are amino acids, and edges represent spatial closeness between nodes. Table <ref type="table" target="#tab_3">1</ref> lists the statistics of these two data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Baselines and Metrics. The baselines include four graph kernels and two deep learning approaches:</head><p>‚Ä¢ the shortest-path kernel (SP) <ref type="bibr" target="#b2">[3]</ref>,</p><p>‚Ä¢ the random walk kernel (RW) <ref type="bibr" target="#b8">[9]</ref>,</p><p>‚Ä¢ the graphlet count kernel (GK) <ref type="bibr" target="#b29">[30]</ref>,</p><p>‚Ä¢ the Weisfeiler-Lehman subtree kernel (WL) <ref type="bibr" target="#b28">[29]</ref>,</p><p>‚Ä¢ PATCHY-SAN (PSCN) <ref type="bibr" target="#b22">[23]</ref>, and ‚Ä¢ graph2vec <ref type="bibr" target="#b21">[22]</ref>.</p><p>We follow the experimental setting as described in <ref type="bibr" target="#b22">[23]</ref>, and perform 10-fold cross validation. In each partition, the experiments are repeated for 10 times. The average accuracy and the standard deviation are reported. We list results of the graph kernels and the best reported results of PSCN according to <ref type="bibr" target="#b22">[23]</ref>.</p><p>For SAGE, we use the same network architecture on both data sets. The first GCN layer has 128 output channels, and the second GCN has 8 output channels. We set d = 64, r = 16, and the penalization term coefficient to be 0.15. The dense layer has 256 rectified linear units with a dropout rate of 0.5. We use minibatch based Adam <ref type="bibr" target="#b14">[15]</ref> to minimize the cross-entropy loss and use Henormal <ref type="bibr" target="#b10">[11]</ref> as the initializer for GCN. For both data sets, the only hyperparameter we optimized is the number of epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results. Table 2 lists the experimental results.</head><p>As we can see, SAGE outperforms all the graph kernel methods and the two deep learning methods by 1.27% -5.59% in accuracy. This shows that our graph embedding method SAGE is superior. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SEAL-C/AI on Synthetic Data</head><p>We evaluate the performance of SEAL-C/AI on synthetic data. We first give a description of the synthetic generator, then visualize the learned embeddings and analyze the self-attentive mechanism on the generated data. Finally we compare our methods with baselines in terms of classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Synthetic Data Generation.</head><p>The benchmark data set Cora <ref type="bibr" target="#b18">[19]</ref> contains 2708 papers which are connected by the "citation" relationship. We borrow the topological structure of Cora to provide the skeleton (i.e., edges) of our synthetic hierarchical graph. Then we generate a set of graph instances, which serve as the nodes of this hierarchical graph. Since there are 7 classes in Cora, we adopt 7 different graph generation algorithms, that is, Watts-Strogatz <ref type="bibr" target="#b31">[32]</ref>, Tree graph, Erd≈ës-R√©nyi <ref type="bibr" target="#b7">[8]</ref>, Barbell <ref type="bibr" target="#b12">[13]</ref>, Bipartite graph, Barab √°si-Albert graph <ref type="bibr" target="#b1">[2]</ref> and Path graph, to generate 7 different types of graph instances, and connect them in the hierarchical graph. Specifically, to generate a graph instance –¥, we randomly sample a number from <ref type="bibr">[100,</ref><ref type="bibr">200]</ref> as its size n. Then we generate its structure and assign the class label according to the graph generation algorithm. In this step, the parameter p in Watts-Strogatz, Erd≈ës-R√©nyi, Bipartite graph and Barab √°si-Albert graph is randomly sampled from [0.1, 0.5], the branching factor for Tree graph is randomly sampled from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. At last, to make this problem more challenging, we randomly remove 1% to 20% edges in the generated graph –¥. The statistics of the generated graph instances are listed in Table <ref type="table" target="#tab_5">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Visualization.</head><p>To have a better understanding of the synthesized graph instances, we split all 2708 graph instances into two parts. 1708 instances are used for training and the remaining 1000 instances are used for testing. We apply SAGE on the training set and derive the embeddings of the 1000 testing instances. We then project these learned embeddings into a two-dimensional space by t-SNE <ref type="bibr" target="#b30">[31]</ref>, as depicted in Figure <ref type="figure" target="#fig_2">4</ref>. Each color in Figure <ref type="figure" target="#fig_2">4</ref> represents a graph type. As we can see from this two-dimensional space, the geometric distance between the graph instances can reflect their graph similarity properly.</p><p>We then examine the self-attentive mechanism of SAGE. We calculate the average attention weight across r views and normalize the resulting attention weights to sum up to 1. From the testing instances, we select three examples: a Tree graph, an Erd≈ës-R√©nyi graph and a Barbell graph, for which SAGE has a high confidence (&gt; 0.9) in predicting their class label. The three examples are depicted in Figure <ref type="figure" target="#fig_3">5</ref>, where a bigger node implies a larger average attention weight, and a darker color implies a larger node degree. On the left is a Tree graph, in which most of the important nodes learned by SAGE are leaf nodes. This is reasonable since leaves are discriminative features to distinguish Tree graph from the other 6 types of graphs. In the center is an Erd≈ës-R√©nyi graph. We cluster these nodes into 5 groups by hierarchical clustering <ref type="bibr" target="#b13">[14]</ref>, and see that SAGE tends to highlight those nodes with large degrees within each cluster. On the right is a Barbell graph, in which SAGE pays attention to two kinds of nodes. The first kind is those nodes that connect a dense graph and a path, and the second kind is the nodes that are on the path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Watts-Strogatz Tree Erdos-Renyi Barbell Bipartite Barabasi-Albert Path</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Baselines and Metrics. We use 6 approaches as our baselines:</head><p>‚Ä¢ GK-SVM/GCN <ref type="bibr" target="#b29">[30]</ref>, which calculates the graphlet count kernel (GK) matrix, then GK-SVM feeds the kernel matrix into SVM <ref type="bibr" target="#b11">[12]</ref> whereas GK-GCN feeds the kernel vector of each graph instance to GCN. ‚Ä¢ WL-SVM/GCN <ref type="bibr" target="#b28">[29]</ref>, which is similar as above but using the Weisfeiler-Lehman subtree kernel (WL). ‚Ä¢ graph2vec-GCN <ref type="bibr" target="#b21">[22]</ref>, which embeds the graph instances by graph2vec and then feeds the embeddings to GCN. ‚Ä¢ cautious-SAGE-Cheby, which is similar to SEAL-CI except that we replace GCN with Cheby-GCN <ref type="bibr" target="#b5">[6]</ref>. ‚Ä¢ active-SAGE-Cheby, which is similar to SEAL-AI except that we replace GCN with Cheby-GCN <ref type="bibr" target="#b5">[6]</ref>. ‚Ä¢ SAGE, which ignores the connections between graph instances and treats them independently.</p><p>We use 300 graph instances as the training set for all methods except SEAL-AI and active-SAGE-Cheby, for which only 140 graphs are used as labeled graph instances at hand and then B = 160 is set for active learning. We use 1000 graph instances as the testing set. We run each method 5 times and report its average accuracy. The number of epochs for graph2vec is 1000 and the learning rate is 0.3. To avoid overfitting of SAGE on this small data set, we use a relatively small number of neurons. The first GCN layer has 32 output channels and the second GCN layer has 4 output channels. We set d = 32 and r = 10. The dense layer has 48 units with a dropout rate of 0.3. We set M = 16 in HC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Results</head><p>. Table <ref type="table" target="#tab_6">4</ref> shows the experimental results for semisupervised graph classification. Among all approaches, SEAL-C/AI achieve the best performance. In the following, we analyze the performance of all methods categorized into 4 groups. Group *1: Both GK-SVM and WL-SVM outperform their GCN-based counterparts, indicating that SVM is more effective than GCN with the computed kernel matrix. All the embedding-based methods perform better than these two kernel methods, which proves that embedding vectors are effective representations for graph instances and are suitable input for graph neural networks. Group *2: graph2vec-GCN achieves 85.2% accuracy, which is comparable to that of SAGE, but lower than that of SEAL-C/AI. One possible explanation is that graph2vec is an unsupervised embedding method, which fails to generate discriminative embeddings for classification. Another possibility is that there is no iteration in this method, and the 300 training instances do not include very informative ones. These limitations of graph2vec are also motivations for us to design the supervised embedding method SAGE and the iterative framework in SEAL-CI. Group *3: cautious-SAGE-Cheby outperforms SAGE by only 0.8%, which is not remarkable considering that it exploits many more training instances. The accuracy of active-SAGE-Cheby is 3.3% lower than that of SEAL-AI, which means that Cheby-GCN is inferior to GCN. Group *4: Both SEAL-CI and SEAL-AI outperform SAGE significantly, which proves the effectiveness of our hierarchical graph based perspective and the iterative algorithm for graph classification. SEAL-AI outperforms SEAL-CI only slightly, by 1.2%. This shows, although SEAL-CI can make use of more training samples, it is still influenced by the misclassified cases of GCN.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SEAL-C/AI on Tencent QQ Group</head><p>In this section, we evaluate SEAL-C/AI on Tencent QQ group data. We describe the characteristics of this data set and then present the experimental results. Finally, we have some open discussions on how to construct a hierarchical graph from real-world data.  are around 100 million active online QQ groups. In this experiment, we select 37,836 QQ groups with 18,422,331 unique anonymized users. For each user, we extract seven personal features:</p><p>‚Ä¢ number of days ever since the registration day;</p><p>‚Ä¢ most frequently active area code in the past 90 days;</p><p>‚Ä¢ number of friends;</p><p>‚Ä¢ number of active days in the past 30 days;</p><p>‚Ä¢ number of logging in the past 30 days;</p><p>‚Ä¢ number of messages sent in the past 30 days;</p><p>‚Ä¢ number of messages sent within QQ groups in the past 30 days.</p><p>We have 298,837,578 friend relationships among these users. 1,773 groups are labeled as "game" and the remaining groups are labeled as "non-game".</p><p>We construct the hierarchical graph from this Tencent QQ group data as follows. A user is treated as an object, and a QQ group as a graph instance. The users in one group are connected by their friendship. The attribute matrix X is filled with the attribute values of the users. The statistics of the graph instances are listed in Table <ref type="table" target="#tab_7">5</ref>. We build the hierarchical graph from the graph instances via common members across groups. That is, if groups A and B have more than one common member, we connect them.   <ref type="figure">7</ref> shows the false prediction rate (i.e., the percentage of misclassified instances) within the Œª most confident predictions of GCN. As we can see, the false prediction rate increases as Œª increases and it reaches 2.4% when Œª = 2000. In the framework of SEAL-CI, as the iteration goes on, we shall bring in more noise to the parameter update of SAGE, while all the training samples in SEAL-AI are informative and correct. This explains why SEAL-AI outperforms SEAL-CI on this Tencent QQ group data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Visualization.</head><p>We provide visualization of a "game" group and its neighborhood in Figure <ref type="figure">8</ref>. The left part is the ego network of the center "game" group. In the one-hop neighborhood of this "game" group, there are 10 "game" groups and 19 "non-game" groups. "Game" groups are densely interconnected with a density of 34.5%, whereas "non-game" groups are sparsely connected with a density of 8.8%. The much higher density among "game" groups validates that common membership is an effective way to relate them in a hierarchical graph for classification. The right part depicts the internal structure of the ego "game" group with 22 users. A bigger node indicates a larger importance, and a darker green color implies a larger node degree. These 22 members are loosely connected and there are no triangles. This makes sense because in reality online "game" groups are not acquaintance networks. Regarding : A Non-game Group : A Game Group : A User Figure <ref type="figure">8</ref>: The ego network of a "game" group. The left side is the ego network, in which "game" groups are in red and "non-game" groups are in blue. The right side is the internal structure of the ego "game" group, in which a bigger node indicates a larger importance, and a darker color implies a larger node degree.</p><p>the learned node importance, node 1 has the highest importance as it is the second active member and has a large degree in this group. Node 16 is also important since it has the highest degree in this group. The "border" member 5 has a big attention weight since it has the largest number of days ever since the registration day and is quite active in this group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Discussion</head><p>. How to construct a hierarchical graph from raw data is an open question. In the above experiment, we connect two QQ groups if they have more than one common member (i.e., &gt; 1). When we change the threshold, it directly affects the edge density in the hierarchical graph, and may influence the classification performance. For example, if we connect two QQ groups when they have one common member or more (i.e., ‚â• 1), the edge density is 2.8% compared with 0.27% in the first setting. A proper setting of this threshold is data dependent, and can be determined through a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>This work is related to semi-supervised classification of networked data, variable-sized graph embedding and active learning. Most work on semi-supervised learning for networked data aims to utilize the network structure to boost the learning performance.</p><p>The assumption is that network context can provide additional information that is not covered by node attributes. Ever since the pioneer work of Sen et al. <ref type="bibr" target="#b26">[27]</ref>, Iterative Classification Algorithm (ICA) has become a paradigm for networked data with limited annotations. In ICA, for each node a local classifier takes the estimated labels of its neighborhood and its own features as input, and outputs a new estimated label. The iteration continues until adjacent estimations stabilize. In ALFNET <ref type="bibr" target="#b0">[1]</ref>, the authors first cluster the network nodes into several groups, and design a content-only classifier CO and a collective classifier CC. Based on the disagreement score of CO and CC in each iteration, a candidate instance set is generated from different clusters and labeled. Then both CO and CC are re-trained using the labeled set until convergence. One main difference between ICA and ALFNET is that ICA does not require human intervention while ALFNET needs human annotation in case labels of the candidate set are not available.</p><p>Recent work has focused on using deep learning neural networks to further improve the performance. <ref type="bibr" target="#b33">[34]</ref> leverages both network context and node features by jointly training node embedding to predict the class label and the context of the network. Later Kipf and Welling <ref type="bibr" target="#b15">[16]</ref> simplify the loss design by only considering the supervised loss while network context is exploited by the GCN operator. Our problem setting is different from all of the above, as the node is no longer a fixed-size feature vector but a variable-size graph. It can be regarded as a generalization of the previous setting, and cannot be handled by existing solutions effectively.</p><p>Representation learning on graphs has been proposed to transform instances in topological space into fixed-size vectors in Euclidean space in which geometric distance reflects their structural similarity. There are two trends on this topic, one of which is a shift from node embedding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref> to whole graph embedding. <ref type="bibr" target="#b32">[33]</ref> uses CBOW and skip-gram model <ref type="bibr" target="#b20">[21]</ref>, previously proven to be successful in natural language processing, to learn a new graph kernel. Meanwhile, some other methods focus on generating graph embeddings by integrating node embeddings. <ref type="bibr" target="#b22">[23]</ref> proposes a spatial-based graph CNN operator and then concatenates these obtained node representations by imposing a problem-specific node ordering. <ref type="bibr" target="#b5">[6]</ref> defines a "graph coarsening" operation by first clustering the node representations and then applying a max-pooling operation. However, all these methods need some preprocessing steps such as node ordering or clustering, which is not a necessity from a data-driven perspective. Another trend is a shift from unsupervised embedding <ref type="bibr" target="#b20">[21]</ref> to supervised embedding <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>, which provides better performance for downstream classification tasks. In this sense, our embedding method SAGE performs whole graph embedding in a supervised way.</p><p>Active learning has been integrated in many collective classification methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref> to find the most informative samples to be labeled. However, research that generalizes active learning with deep semi-supervised learning is still lacking. The closest work is <ref type="bibr" target="#b34">[35]</ref> in which the authors utilize active learning to incrementally fine-tune a CNN network for image classification. Our solution SEAL-AI is different in the sense that the informative samples selected by active learning are used to update the parameters of the graph embedding network, whose output is then fed into HC in an iterative framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we study semi-supervised graph classification from a hierarchical graph perspective. The hierarchical graph is a much too complicated input for classification, thus we first design a supervised, self-attentive graph embedding method SAGE to embed graph instances into fixed-length vectors, which are a common input form for classification. We build two classifiers IC and HC at the graph instance level and the hierarchical graph level respectively to fully exploit the available information. Our semi-supervised solutions SEAL-C/AI adopt an iterative framework to update IC and HC alternately with an enlarged training set. Experimental results on synthetic graphs and Tencent QQ group data show that SEAL-C/AI outperform other competitors by a significant margin in accuracy/Macro-F1, and they also generate meaningful interpretations of the learned representations for graph instances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The supervised self-attentive graph embedding method SAGE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Schematic diagram of the learning framework SEAL-CI. There are two subroutines: discriminative graph embedding (in the orange box) and graph-based classification (in the green box).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two-dimensional visualization of graph embeddings generated from the synthesized graph instances using SAGE. The nodes are colored according to their graph types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Attention of graph embeddings on 3 different types of graphs (left: Tree graph; middle: Erd≈ës-R√©nyi graph; right: Barbell graph). A bigger node indicates a larger importance, and a darker color implies a larger node degree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 2 . 5</head><label>25</label><figDesc>Influence of the number of labeled training instances. We examine how the number of labeled training instances affects the performance of our methods. We train SAGE and SEAL-CI with a label size of {140, 180, 220, 260, 300}. We train SEAL-AI with 140 labeled instances and then set the budget B for active learning at {0, 40, 80, 120, 160}. Thus the three methods have the same number</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 3 . 1 Figure 6 :</head><label>316</label><figDesc>Figure 6: Accuracy with different number of labeled training instances on synthetic data for semi-supervised graph classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 3 . 2 Figure 7 :</head><label>327</label><figDesc>Figure 7: The false prediction rate of GCN with Œª in SEAL-CI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Statistics of PROTEINS and D&amp;D</figDesc><table><row><cell></cell><cell cols="2">PROTEINS D&amp;D</cell></row><row><cell>Max number of nodes</cell><cell>620</cell><cell>5748</cell></row><row><cell>Avg number of nodes</cell><cell>39.06</cell><cell>284.32</cell></row><row><cell>Number of graphs</cell><cell>1113</cell><cell>1178</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of different classifiers</figDesc><table><row><cell cols="2">Approach PROTEINS</cell><cell>D&amp;D</cell></row><row><cell>SP</cell><cell>75.07¬±0.54%</cell><cell>-</cell></row><row><cell>RW</cell><cell>74.22¬±0.42%</cell><cell>-</cell></row><row><cell>GK</cell><cell cols="2">71.67¬±0.55% 78.45¬±0.26%</cell></row><row><cell>WL</cell><cell cols="2">72.92¬±0.56% 77.95¬±0.70%</cell></row><row><cell>PSCN</cell><cell cols="2">75.89¬±2.76% 77.12¬±2.41%</cell></row><row><cell cols="2">graph2vec 73.30¬±2.05%</cell><cell>-</cell></row><row><cell>SAGE</cell><cell cols="2">77.26¬±2.28% 80.88¬±2.33%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Statistics of generated graph instances</figDesc><table><row><cell>Type</cell><cell cols="4">Number Nodes Edges Density</cell><cell></cell></row><row><cell>Watts-Strogatz</cell><cell>351</cell><cell>173</cell><cell>347</cell><cell>2.3%</cell><cell></cell></row><row><cell>Tree</cell><cell>217</cell><cell>127</cell><cell>120</cell><cell>1.5%</cell><cell></cell></row><row><cell>Erd≈ës-R√©nyi</cell><cell>418</cell><cell>174</cell><cell>3045</cell><cell>20%</cell><cell>The</cell></row><row><cell>Barbell</cell><cell>818</cell><cell>169</cell><cell>2379</cell><cell>16.3%</cell><cell></cell></row><row><cell>Bipartite</cell><cell>426</cell><cell>144</cell><cell>1102</cell><cell>10.6%</cell><cell></cell></row><row><cell>Barab √°si-Albert</cell><cell>298</cell><cell>173</cell><cell>509</cell><cell>3.4%</cell><cell></cell></row><row><cell>Path</cell><cell>180</cell><cell>175</cell><cell>170</cell><cell>1.1%</cell><cell></cell></row><row><cell cols="6">node and edge numbers and density are the average for each type</cell></row><row><cell></cell><cell></cell><cell>of graph.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different methods on the synthetic data set for semi-supervised graph classification</figDesc><table><row><cell></cell><cell>Algorithm</cell><cell>Accuracy</cell></row><row><cell>*1</cell><cell>GK-SVM/GCN WL-SVM/GCN</cell><cell>77.8%/73.4% 83.4%/75.5%</cell></row><row><cell>*2</cell><cell>graph2vec-GCN</cell><cell>85.2%</cell></row><row><cell>*3</cell><cell>cautious-SAGE-Cheby active-SAGE-Cheby</cell><cell>86.5% 89.1%</cell></row><row><cell></cell><cell>SAGE</cell><cell>85.7%</cell></row><row><cell>*4</cell><cell>SEAL-CI</cell><cell>91.2%</cell></row><row><cell></cell><cell>SEAL-AI</cell><cell>92.4%</cell></row><row><cell cols="3">of labeled training instances. We set Œª = 40 in SEAL-CI and k = 10</cell></row><row><cell cols="3">in SEAL-AI. We run all methods 5 times, and plot their average</cell></row><row><cell cols="3">accuracy in Figure 6. As we can see from Figure 6, when the number</cell></row><row><cell cols="3">of labeled training instances is 140, SEAL-CI performs best since</cell></row><row><cell cols="3">it can utilize more training samples. As the number of labeled</cell></row><row><cell cols="3">training instances increases, the performance of SEAL-AI improves</cell></row><row><cell cols="3">dramatically. SEAL-AI catches up with SEAL-CI at 260 labeled</cell></row><row><cell cols="3">training instances and outperforms SEAL-CI at 300 labeled training</cell></row><row><cell cols="3">instances. It validates that SEAL-AI can make use of the iterations to</cell></row><row><cell cols="3">find informative and accurate training samples. Meanwhile SEAL-</cell></row><row><cell cols="3">CI trusts the prediction of GCN conditionally on its confidence,</cell></row><row><cell cols="3">which may bring some noise to the learning process. SEAL-C/AI</cell></row><row><cell cols="3">outperform SAGE in all cases, which makes sense because SEAL-</cell></row><row><cell cols="3">C/AI make good use of the hierarchical graph setting and consider</cell></row><row><cell cols="3">the connections between the graph instances for classification.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Statistics of collected Tencent QQ groups</figDesc><table><row><cell cols="5">Class label Number Nodes Edges Density</cell><cell></cell></row><row><cell>game</cell><cell>1,773</cell><cell>147</cell><cell>395</cell><cell>5.48%</cell><cell>The</cell></row><row><cell>non-game</cell><cell>36,063</cell><cell>365</cell><cell>1586</cell><cell>3.28%</cell><cell></cell></row><row><cell cols="6">node and edge numbers and density are the average for each type</cell></row><row><cell></cell><cell></cell><cell>of QQ group.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different methods on Tencent QQ group data for semi-supervised graph classification</figDesc><table><row><cell></cell><cell>Algorithm</cell><cell>Macro-F1</cell></row><row><cell>*1</cell><cell>GK-SVM WL-SVM</cell><cell>48.8% 47.8%</cell></row><row><cell>*2</cell><cell>graph2vec-GCN</cell><cell>48.1%</cell></row><row><cell>*3</cell><cell>cautious-SAGE-Cheby active-SAGE-Cheby</cell><cell>64.3% 66.7%</cell></row><row><cell></cell><cell>SAGE</cell><cell>54.7%</cell></row><row><cell>*4</cell><cell>SEAL-CI</cell><cell>70.8%</cell></row><row><cell></cell><cell>SEAL-AI</cell><cell>73.2%</cell></row><row><cell cols="3">its average accuracy. The hyperparameters of SAGE are the same</cell></row><row><cell cols="3">as the settings in Section 4.1.1. Since the class distribution is quite</cell></row><row><cell cols="3">imbalanced in this data set, we report the Macro-F1 instead of</cell></row><row><cell>accuracy.</cell><cell></cell><cell></cell></row><row><cell cols="3">4.3.3 Results. Table 6 shows the experimental results. SEAL-C/AI</cell></row><row><cell cols="3">outperform GK, WL and grah2vec by at least 12% in Macro-F1.</cell></row><row><cell cols="3">Within our framework, GCN is better than Cheby-GCN for about</cell></row><row><cell cols="3">6%. SEAL-AI outperforms SEAL-CI by 2.4%. Next we provide the</cell></row><row><cell cols="3">reason why SEAL-AI outperforms SEAL-CI on this data set. Figure</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">(A + I n ) D‚àí 1 2 ,(4)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">https://www.tencent.com/en-us/articles/17000391523362601.pdf</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Tencent Security Platform Department for discussions and suggestions. The work described in this paper was supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China [Project No.: CUHK 14205618], Tencent AI Lab Rhino-Bird Focused Research Program GF201801 and the CUHK Stanley Ho Big Data Decision Analytics Research Centre.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active learning for networked data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mihalkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mathematical results on scale-free random graphs. Handbook of graphs and networks: from the genome to the internet</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollob√°s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Riordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sch√∂nauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from nonenzymes without alignments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the evolution of random graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erd≈ës</surname></persName>
		</author>
		<author>
			<persName><surname>R√©nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Inst. Hung. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="17" to="60" />
			<date type="published" when="1960">1960. 1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">T</forename><surname>G√§rtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<title level="m">Support vector machines. IEEE Intelligent Systems and their Applications</title>
				<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="18" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prediction on a graph with a perceptron</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herbster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical clustering schemes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="241" to="254" />
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Structured Self-attentive Sentence Embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cautious inference in collective classification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="596" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05005</idno>
		<title level="m">graph2vec: Learning Distributed Representations of Graphs. CoRR abs/1707</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">5005</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2016. 2014-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards Deep and Representation Learning for Talent Search at LinkedIn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Polatkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ozcaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Geyik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2253" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Text categorization as a graph classification problem</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kiagias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1702" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Active learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="114" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">2011. Sep (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno>AISTATS. 488-495</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collective dynamics of √¢ƒÇ≈∏small-world√¢ƒÇ≈π networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Graph Kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fine-Tuning Convolutional Neural Networks for Biomedical Image Analysis: Actively and Incrementally</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4761" to="4772" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
