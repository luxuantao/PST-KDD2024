<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-21">21 Aug 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
							<email>changhua.pch@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
							<email>jiangpeng.jp@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-21">21 Aug 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3357895</idno>
					<idno type="arXiv">arXiv:1904.06690v2[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sequential Recommendation</term>
					<term>Bidirectional Sequential Model</term>
					<term>Cloze</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modeling users' dynamic preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks to encode users' historical interactions from left to right into hidden representations for making recommendations. Despite their effectiveness, we argue that such left-to-right unidirectional models are sub-optimal due to the limitations including: a) unidirectional architectures restrict the power of hidden representation in users' behavior sequences; b) they often assume a rigidly ordered sequence which is not always practical. To address these limitations, we proposed a sequential recommendation model called BERT4Rec, which employs the deep bidirectional self-attention to model user behavior sequences. To avoid the information leakage and efficiently train the bidirectional model, we adopt the Cloze objective to sequential recommendation, predicting the random masked items in the sequence by jointly conditioning on their left and right context. In this way, we learn a bidirectional representation model to make recommendations by allowing each item in user historical behaviors to fuse information from both left and right sides. Extensive experiments on four benchmark datasets show that our model outperforms various state-of-the-art sequential models consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Accurately characterizing users' interests lives at the heart of an effective recommendation system. In many real-world applications, users' current interests are intrinsically dynamic and evolving, influenced by their historical behaviors. For example, one may purchase accessories (e.g., Joy-Con controllers) soon after buying a Nintendo Switch, though she/he will not buy console accessories under normal circumstances.</p><p>To model such sequential dynamics in user behaviors, various methods have been proposed to make sequential recommendations based on users' historical interactions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">40]</ref>. They aim to predict the successive item(s) that a user is likely to interact with given her/his past interactions. Recently, a surge of works employ sequential neural networks, e.g., Recurrent Neural Network (RNN), for sequential recommendation and obtain promising results <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>. The basic paradigm of previous work is to encode a user's historical interactions into a vector (i.e., representation of user's preference) using a left-to-right sequential model and make recommendations based on this hidden representation.</p><p>Despite their prevalence and effectiveness, we argue that such left-to-right unidirectional models are not sufficient to learn optimal representations for user behavior sequences. The major limitation, as illustrated in Figure <ref type="figure">1c and 1d</ref>, is that such unidirectional models restrict the power of hidden representation for items in the historical sequences, where each item can only encode the information from previous items. Another limitation is that previous unidirectional models are originally introduced for sequential data with natural order, e.g., text and time series data. They often assume a rigidly ordered sequence over data which is not always true for user behaviors in real-world applications. In fact, the choices of items in a user's historical interactions may not follow a rigid order assumption <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b54">54]</ref> due to various unobservable external factors <ref type="bibr" target="#b4">[5]</ref>. In such a situation, it is crucial to incorporate context from both directions in user behavior sequence modeling.</p><p>To address the limitations mentioned above, we seek to use a bidirectional model to learn the representations for users' historical behavior sequences. Specifically, inspired by the success of BERT <ref type="bibr" target="#b5">[6]</ref> in text understanding, we propose to apply the deep bidirectional self-attention model to sequential recommendation, as illustrated in Figure <ref type="figure">1b</ref>. For representation power, the superior results for deep bidirectional models on text sequence modeling tasks show that it is beneficial to incorporate context from both sides for sequence representations learning <ref type="bibr" target="#b5">[6]</ref>. For rigid order assumption, our model is more suitable than unidirectional models in modeling user behavior sequences since all items in the bidirectional model can leverage the contexts from both left and right side.</p><p>However, it is not straightforward and intuitive to train the bidirectional model for sequential recommendation. Conventional sequential recommendation models are usually trained left-to-right by predicting the next item for each position in the input sequence. As shown in Figure <ref type="figure">1</ref>, jointly conditioning on both left and right context in a deep bidirectional model would cause information leakage, i.e., allowing each item to indirectly "see the target item". This could make predicting the future become trivial and the network would not learn anything useful.</p><p>To tackle this problem, we introduce the Cloze task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">50]</ref> to take the place of the objective in unidirectional models (i.e., sequentially predicting the next item). Specifically, we randomly mask some items (i.e., replace them with a special token <ref type="bibr">[mask]</ref>) in the input sequences, and then predict the ids of those masked items based on their surrounding context. In this way, we avoid the information leakage and learn a bidirectional representation model by allowing the representation of each item in the input sequence to fuse both the left and right context. In addition to training a bidirectional model, another advantage of the Cloze objective is that it can produce more samples to train a more powerful model in multiple epochs. However, a downside of the Cloze task is that it is not consistent with the final task (i.e., sequential recommendation). To fix this, during the test, we append the special token "[mask]" at the end of the input sequence to indicate the item that we need to predict, and then make recommendations base on its final hidden vector. Extensive experiments on four datasets show that our model outperforms various state-of-the-art baselines consistently.</p><p>The contributions of our paper are as follows:</p><p>• We propose to model user behavior sequences with a bidirectional self-attention network through Cloze task. To the best of our knowledge, this is the first study to introduce deep bidirectional sequential model and Cloze objective into the field of recommendation systems. • We compare our model with state-of-the-art methods and demonstrate the effectiveness of both bidirectional architecture and the Cloze objective through quantitative analysis on four benchmark datasets. • We conduct a comprehensive ablation study to analyze the contributions of key components in the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we will briefly review several lines of works closely related to ours, including general recommendation, sequential recommendation, and attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Recommendation</head><p>Early works on recommendation systems typically use Collaborative Filtering (CF) to model users' preferences based on their interaction histories <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b43">43]</ref>. Among various CF methods, Matrix Factorization (MF) is the most popular one, which projects users and items into a shared vector space and estimate a user's preference on an item by the inner product between their vectors <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b41">41]</ref>. Another line of work is item-based neighborhood methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b43">43]</ref>. They estimate a user's preference on an item via measuring its similarities with the items in her/his interaction history using a precomputed item-to-item similarity matrix. Recently, deep learning has been revolutionizing the recommendation systems dramatically. The early pioneer work is a two-layer Restricted Boltzmann Machines (RBM) for collaborative filtering, proposed by Salakhutdinov et al. <ref type="bibr" target="#b42">[42]</ref> in Netflix Prize<ref type="foot" target="#foot_0">1</ref> .</p><p>One line of deep learning based methods seeks to improve the recommendation performance by integrating the distributed item representations learned from auxiliary information, e.g., text <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b53">53]</ref>, images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">55]</ref>, and acoustic features <ref type="bibr" target="#b51">[51]</ref> into CF models. Another line of work seeks to take the place of conventional matrix factorization. For example, Neural Collaborative Filtering (NCF) <ref type="bibr" target="#b11">[12]</ref> estimates user preferences via Multi-Layer Perceptions (MLP) instead of inner product, while AutoRec <ref type="bibr" target="#b44">[44]</ref> and CDAE <ref type="bibr" target="#b57">[57]</ref> predict users' ratings using Auto-encoder framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequential Recommendation</head><p>Unfortunately, none of the above methods is for sequential recommendation since they all ignore the order in users' behaviors.</p><p>Early works on sequential recommendation usually capture sequential patterns from user historical interactions using Markov chains (MCs). For example, Shani et al. <ref type="bibr" target="#b45">[45]</ref> formalized recommendation generation as a sequential optimization problem and employ Markov Decision Processes (MDPs) to address it. Later, Rendle et al. <ref type="bibr" target="#b40">[40]</ref> combine the power of MCs and MF to model both sequential behaviors and general interests by Factorizing Personalized Markov Chains (FPMC). Besides the first-order MCs, high-order MCs are also adopted to consider more previous items <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Recently, RNN and its variants, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">[4]</ref> and Long Short-Term Memory (LSTM) <ref type="bibr" target="#b16">[17]</ref>, are becoming more and more popular for modeling user behavior sequences <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>. The basic idea of these methods is to encode user's previous records into a vector (i.e., representation of user's preference which is used to make predictions) with various recurrent architectures and loss functions, including session-based GRU with ranking loss (GRU4Rec) <ref type="bibr" target="#b14">[15]</ref>, Dynamic REcurrent bAsket Model (DREAM) <ref type="bibr" target="#b58">[58]</ref>, user-based GRU <ref type="bibr" target="#b6">[7]</ref>, attention-based GRU (NARM) <ref type="bibr" target="#b28">[28]</ref>, and improved GRU4Rec with new loss function (i.e., BPR-max and TOP1-max) and an improved sampling strategy <ref type="bibr" target="#b13">[14]</ref>.</p><p>Other than recurrent neural networks, various deep learning models are also introduced for sequential recommendation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b49">49]</ref>. For example, Tang and Wang <ref type="bibr" target="#b49">[49]</ref> propose a Convolutional Sequence Model (Caser) to learn sequential patterns using both horizontal and vertical convolutional filters. Chen et al. <ref type="bibr" target="#b2">[3]</ref> and Huang et al. <ref type="bibr" target="#b18">[19]</ref> employ Memory Network to improve sequential recommendation. STAMP captures both users' general interests and current interests using an MLP network with attention <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention Mechanism</head><p>Attention mechanism has shown promising potential in modeling sequential data, e.g., machine translation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b52">52]</ref> and text classification <ref type="bibr">[? ]</ref>. Recently, some works try to employ the attention mechanism to improve recommendation performances and interpretability <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b33">33]</ref>. For example, Li et al. <ref type="bibr" target="#b28">[28]</ref> incorporate an attention mechanism into GRU to capture both the user's sequential behavior and main purpose in session-based recommendation.</p><p>The works mentioned above basically treat attention mechanism as an additional component to the original models. In contrast, Transformer <ref type="bibr" target="#b52">[52]</ref> and BERT <ref type="bibr" target="#b5">[6]</ref> are built solely on multi-head self-attention and achieve state-of-the-art results on text sequence modeling. Recently, there is a rising enthusiasm for applying purely attention-based neural networks to model sequential data for their  </p><formula xml:id="formula_0">v 1 • • • v t −1 v [mask] v 1 • • • [mask] v t −1 Trm . . . Trm + Trm . . . Trm + Trm . . . Trm + L× p 1 • • • p t −1 p t Projection h L 1 h L t −1 v t h L t Embedding Layer • • • . . . • • • Multi-</formula><formula xml:id="formula_1">• • • v 1 . . . v t v t −1 GRU GRU GRU . . . v 2 . . . v t v t +1<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BERT4REC</head><p>Before going into the details, we first introduce the research problem, the basic concepts, and the notations in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>In sequential recommendation, let</p><formula xml:id="formula_2">U={u 1 , u 2 , . . . , u | U | } denote a set of users, V={v 1 , v 2 , . . . , v | V | }</formula><p>be a set of items, and list</p><formula xml:id="formula_3">S u =[v (u) 1 , . . . , v (u) t , . . . , v<label>(u)</label></formula><p>n u ] denote the interaction sequence in chronological order for user u ∈ U, where v (u) t ∈ V is the item that u has interacted with at time step<ref type="foot" target="#foot_1">2</ref> t and n u is the the length of interaction sequence for user u. Given the interaction history S u , sequential recommendation aims to predict the item that user u will interact with at time step n u + 1. It can be formalized as modeling the probability over all possible items for user u at time step n u +1:</p><formula xml:id="formula_4">p v (u) n u +1 = v | S u</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>Here, we introduce a new sequential recommendation model called BERT4Rec, which adopts Bidirectional Encoder Representations from Transformers to a new task, sequential Recommendation. It is built upon the popular self-attention layer, "Transformer layer".</p><p>As illustrated in Figure <ref type="figure">1b</ref>, BERT4Rec is stacked by L bidirectional Transformer layers. At each layer, it iteratively revises the representation of every position by exchanging information across all positions at the previous layer in parallel with the Transformer layer. Instead of learning to pass relevant information forward step by step as RNN based methods did in Figure <ref type="figure">1d</ref>, self-attention mechanism endows BERT4Rec with the capability to directly capture the dependencies in any distances. This mechanism results in a global receptive field, while CNN based methods like Caser usually have a limited receptive field. In addition, in contrast to RNN based methods, self-attention is straightforward to parallelize.</p><p>Comparing Figure <ref type="figure">1b</ref>, 1c, and 1d, the most noticeable difference is that SASRec and RNN based methods are all left-to-right unidirectional architecture, while our BERT4Rec uses bidirectional self-attention to model users' behavior sequences. In this way, our proposed model can obtain more powerful representations of users' behavior sequences to improve recommendation performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transformer Layer</head><p>As illustrated in Figure <ref type="figure">1b</ref>, given an input sequence of length t, we iteratively compute hidden representations h l i at each layer l for each position i simultaneously by applying the Transformer layer from <ref type="bibr" target="#b52">[52]</ref>. Here, we stack h l i ∈ R d together into matrix H l ∈R t ×d since we compute attention function on all positions simultaneously in practice. As shown in Figure <ref type="figure">1a</ref>, the Transformer layer Trm contains two sub-layers, a Multi-Head Self-Attention sub-layer and a Position-wise Feed-Forward Network.</p><p>Multi-Head Self-Attention. Attention mechanisms have become an integral part of sequence modeling in a variety of tasks, allowing capturing the dependencies between representation pairs without regard to their distance in the sequences. Previous work has shown that it is beneficial to jointly attend to information from different representation subspaces at different positions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b52">52]</ref>. Thus, we here adopt the multi-head self-attention instead of performing a single attention function. Specifically, multi-head attention first linearly projects H l into h subspaces, with different, learnable linear projections, and then apply h attention functions in parallel to produce the output representations which are concatenated and once again projected:</p><formula xml:id="formula_5">MH(H l ) = [head 1 ; head 2 ; . . . ; head h ]W O head i = Attention H l W Q i , H l W K i , H l W V i (<label>1</label></formula><formula xml:id="formula_6">)</formula><p>where the projections matrices for each headW</p><formula xml:id="formula_7">Q i ∈ R d ×d /h ,W K i ∈ R d ×d /h , W V i ∈ R d ×d /h</formula><p>, and W O i ∈ R d ×d are learnable parameters. Here, we omit the layer subscript l for the sake of simplicity. In fact, these projection parameters are not shared across the layers. Here, the Attention function is Scaled Dot-Product Attention:</p><formula xml:id="formula_8">Attention(Q, K, V ) = softmax QK ⊤ d/h V (<label>2</label></formula><formula xml:id="formula_9">)</formula><p>where query Q, key K, and value V are projected from the same matrix H l with different learned projection matrices as in Equation <ref type="formula" target="#formula_5">1</ref>.</p><p>The temperature d/h is introduced to produce a softer attention distribution for avoiding extremely small gradients <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b52">52]</ref>.</p><p>Position-wise Feed-Forward Network. As described above, the self-attention sub-layer is mainly based on linear projections. To endow the model with nonlinearity and interactions between different dimensions, we apply a Position-wise Feed-Forward Network to the outputs of the self-attention sub-layer, separately and identically at each position. It consists of two affine transformations with a Gaussian Error Linear Unit (GELU) activation in between:</p><formula xml:id="formula_10">PFFN(H l ) = FFN(h l 1 ) ⊤ ; . . . ; FFN(h l t ) ⊤ ⊤ FFN(x) = GELU xW (1) + b (1) W (2) + b (2) GELU(x) = xΦ(x)<label>(3)</label></formula><p>where Φ(x) is the cumulative distribution function of the standard gaussian distribution,</p><formula xml:id="formula_11">W (1) ∈ R d ×4d , W (2) ∈ R 4d ×d , b<label>(1)</label></formula><p>∈ R 4d and b (2) ∈ R d are learnable parameters and shared across all positions. We omit the layer subscript l for convenience. In fact, these parameters are different from layer to layer. In this work, following OpenAI GPT <ref type="bibr" target="#b38">[38]</ref> and BERT <ref type="bibr" target="#b5">[6]</ref>, we use a smoother GELU <ref type="bibr" target="#b12">[13]</ref> activation rather than the standard ReLu activation. Stacking Transformer Layer. As elaborated above, we can easily capture item-item interactions across the entire user behavior sequence using self-attention mechanism. Nevertheless, it is usually beneficial to learn more complex item transition patterns by stacking the self-attention layers. However, the network becomes more difficult to train as it goes deeper. Therefore, we employ a residual connection <ref type="bibr" target="#b8">[9]</ref> around each of the two sublayers as in Figure <ref type="figure">1a</ref>, followed by layer normalization <ref type="bibr" target="#b0">[1]</ref>. Moreover, we also apply dropout <ref type="bibr" target="#b47">[47]</ref> to the output of each sub-layer, before it is normalized. That is, the output of each sub-layer is LN(x + Dropout(sublayer(x))), where sublayer(•) is the function implemented by the sub-layer itself, LN is the layer normalization function defined in <ref type="bibr" target="#b0">[1]</ref>. We use LN to normalize the inputs over all the hidden units in the same layer for stabilizing and accelerating the network training.</p><p>In summary, BERT4Rec refines the hidden representations of each layer as follows:</p><formula xml:id="formula_12">H l = Trm H l −1 , ∀i ∈ [1, . . . , L]<label>(4)</label></formula><formula xml:id="formula_13">Trm(H l −1 ) = LN A l −1 + Dropout PFFN(A l −1 )<label>(5)</label></formula><formula xml:id="formula_14">A l −1 = LN H l −1 + Dropout MH(H l −1 )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Embedding Layer</head><p>As elaborated above, without any recurrence or convolution module, the Transformer layer Trm is not aware of the order of the input sequence. In order to make use of the sequential information of the input, we inject Positional Embeddings into the input item embeddings at the bottoms of the Transformer layer stacks. For a given item v i , its input representation h 0 i is constructed by summing the corresponding item and positional embedding:</p><formula xml:id="formula_15">h 0 i = v i + p i where v i ∈E is the d−dimensional embedding for item v i , p i ∈P is the d−dimensional positional embedding for position index i.</formula><p>In this work, we use the learnable positional embeddings instead of the fixed sinusoid embeddings in <ref type="bibr" target="#b52">[52]</ref> for better performances. The positional embedding matrix P ∈ R N ×d allows our model to identify which portion of the input it is dealing with. However, it also imposes a restriction on the maximum sentence length N that our model can handle. Thus, we need to truncate the the input sequence</p><formula xml:id="formula_16">[v 1 , . . . , v t ] to the last N items [v u t −N +1 , . . . , v t ] if t &gt; N .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Output Layer</head><p>After L layers that hierarchically exchange information across all positions in the previous layer, we get the final output H L for all items of the input sequence. Assuming that we mask the item v t at time step t, we then predict the masked items v t base on h L t as shown in Figure <ref type="figure">1b</ref>. Specifically, we apply a two-layer feed-forward network with GELU activation in between to produce an output distribution over target items:</p><formula xml:id="formula_17">P(v) = softmax GELU(h L t W P + b P )E ⊤ + b O<label>(7)</label></formula><p>where W P is the learnable projection matrix, b P , and b O are bias terms, E ∈ R |V |×d is the embedding matrix for the item set V. We use the shared item embedding matrix in the input and output layer for alleviating overfitting and reducing model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model Learning</head><p>Training. Conventional unidirectional sequential recommendation models usually train the model by predicting the next item for each position in the input sequence as illustrated in Figure <ref type="figure">1c and 1d</ref>. Specifically, the target of the input sequence</p><formula xml:id="formula_18">[v 1 , . . . , v t ] is a shifted version [v 2 , . . . , v t +1 ].</formula><p>However, as shown in Figure <ref type="figure">1b</ref>, jointly conditioning on both left and right context in a bidirectional model would cause the final output representation of each item to contain the information of the target item. This makes predicting the future become trivial and the network would not learn anything useful.</p><p>A simple solution for this issue is to create t − 1 samples (subsequences with next items like ([v 1 ], v 2 ) and ([v 1 , v 2 ], v 3 )) from the original length t behavior sequence and then encode each historical subsequence with the bidirectional model to predict the target item. However, this approach is very time and resources consuming since we need to create a new sample for each position in the sequence and predict them separately.</p><p>In order to efficiently train our proposed model, we apply a new objective: Cloze task <ref type="bibr" target="#b50">[50]</ref> (also known as "Masked Language Model" in <ref type="bibr" target="#b5">[6]</ref>) to sequential recommendation. It is a test consisting of a portion of language with some words removed, where the participant is asked to fill the missing words. In our case, for each training step, we randomly mask ρ proportion of all items in the input sequence (i.e., replace with special token "[mask]"), and then predict the original ids of the masked items based solely on its left and right context. For example:</p><formula xml:id="formula_19">Input: [v 1 , v 2 , v 3 , v 4 , v 5 ] [v 1 , [mask] 1 , v 3 , [mask] 2 , v 5 ]</formula><p>Labels:</p><formula xml:id="formula_20">[mask] 1 = v 2 , [mask] 2 = v 4 randomly mask</formula><p>The final hidden vectors corresponding to "[mask]" are fed into an output softmax over the item set, as in conventional sequential recommendation. Eventually, we define the loss for each masked input S ′ u as the negative log-likelihood of the masked targets:</p><formula xml:id="formula_21">L = 1 |S m u | v m ∈S m u − log P(v m = v * m |S ′ u )<label>(8)</label></formula><p>where S ′ u is the masked version for user behavior history S u , S m u is the random masked items in it, v * m is the true item for the masked item v m , and the probability P(•) is defined in Equation <ref type="formula" target="#formula_17">7</ref>.</p><p>An additional advantage for Cloze task is that it can generate more samples to train the model. Assuming a sequence of length n, conventional sequential predictions in Figure <ref type="figure">1c and 1d</ref> produce n unique samples for training, while BERT4Rec can obtain n k samples (if we randomly mask k items) in multiple epochs. It allows us to train a more powerful bidirectional representation model.</p><p>Test. As described above, we create a mismatch between the training and the final sequential recommendation task since the Cloze objective is to predict the current masked items while sequential recommendation aims to predict the future. To address this, we append the special token "[mask]" to the end of user's behavior sequence, and then predict the next item based on the final hidden representation of this token. To better match the sequential recommendation task (i.e., predict the last item), we also produce samples that only mask the last item in the input sequences during training. It works like fine-tuning for sequential recommendation and can further improve the recommendation performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Discussion</head><p>Here, we discuss the relation of our model with previous related work.</p><p>SASRec. Obviously, SASRec is a left-to-right unidirectional version of our BERT4Rec with single head attention and causal attention mask. Different architectures lead to different training methods. SASRec predicts the next item for each position in a sequence, while BERT4Rec predicts the masked items in the sequence using Cloze objective.</p><p>CBOW &amp; SG. Another very similar work is Continuous Bag-of-Words (CBOW) and Skip-Gram (SG) <ref type="bibr" target="#b35">[35]</ref>. CBOW predicts a target word using the average of all the word vectors in its context (both BERT. Although our BERT4Rec is inspired by the BERT in NLP, it still has several differences from BERT: a) The most critical difference is that BERT4Rec is an end-to-end model for sequential recommendation, while BERT is a pre-training model for sentence representation. BERT leverages large-scale task-independent corpora to pre-train the sentence representation model for various text sequence tasks since these tasks share the same background knowledge about the language. However, this assumption does not hold in the recommendation tasks. Thus we train BERT4Rec end-to-end for different sequential recommendation datasets. b) Different from BERT, we remove the next sentence loss and segment embeddings since BERT4Rec models a user's historical behaviors as only one sequence in sequential recommendation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets</head><p>We evaluate the proposed model on four real-world representative datasets which vary significantly in domains and sparsity.</p><p>• Amazon Beauty<ref type="foot" target="#foot_2">3</ref> : This is a series of product review datasets crawled from Amazon.com by McAuley et al. <ref type="bibr" target="#b34">[34]</ref>. They split the data into separate datasets according to the toplevel product categories on Amazon. In this work, we adopt the "Beauty" category. • Steam<ref type="foot" target="#foot_3">4</ref> : This is a dataset collected from Steam, a large online video game distribution platform, by Kang and McAuley <ref type="bibr" target="#b21">[22]</ref>. • MovieLens <ref type="bibr" target="#b7">[8]</ref>: This is a popular benchmark dataset for evaluating recommendation algorithms. In this work, we adopt two well-established versions, MovieLens 1m (ML-1m) <ref type="foot" target="#foot_4">5</ref> and MovieLens 20m (ML-20m) <ref type="foot" target="#foot_5">6</ref> . For dataset preprocessing, we follow the common practice in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b49">49]</ref>. For all datasets, we convert all numeric ratings or the presence of a review to implicit feedback of 1 (i.e., the user interacted with the item). After that, we group the interaction records by users and build the interaction sequence for each user by sorting these interaction records according to the timestamps. To ensure the quality of the dataset, following the common practice <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b49">49]</ref>, we keep users with at least five feedbacks. The statistics of the processed datasets are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task Settings &amp; Evaluation Metrics</head><p>To evaluate the sequential recommendation models, we adopted the leave-one-out evaluation (i.e., next item recommendation) task, which has been widely used in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">49]</ref>. For each user, we hold out the last item of the behavior sequence as the test data, treat the item just before the last as the validation set, and utilize the remaining items for training. For easy and fair evaluation, we follow the common strategy in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">49]</ref>, pairing each ground truth item in the test set with 100 randomly sampled negative items that the user has not interacted with. To make the sampling reliable and representative <ref type="bibr" target="#b18">[19]</ref>, these 100 negative items are sampled according to their popularity. Hence, the task becomes to rank these negative items with the ground truth item for each user.</p><p>Evaluation Metrics. To evaluate the ranking list of all the models, we employ a variety of evaluation metrics, including Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG), and Mean Reciprocal Rank (MRR). Considering we only have one ground truth item for each user, HR@k is equivalent to Recall@k and proportional to Precision@k; MRR is equivalent to Mean Average Precision (MAP). In this work, we report HR and NDCG with k = 1, 5, 10. For all these metrics, the higher the value, the better the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines &amp; Implementation Details</head><p>To verify the effectiveness of our method, we compare it with the following representative baselines:</p><p>• POP: It is the simplest baseline that ranks items according to their popularity judged by the number of interactions. • BPR-MF <ref type="bibr" target="#b39">[39]</ref>: It optimizes the matrix factorization with implicit feedback using a pairwise ranking loss. • NCF <ref type="bibr" target="#b11">[12]</ref>: It models userâĂŞitem interactions with a MLP instead of the inner product in matrix factorization. For NCF<ref type="foot" target="#foot_6">7</ref> , GRU4Rec<ref type="foot" target="#foot_7">8</ref> , GRU4Rec +8 , Caser<ref type="foot" target="#foot_8">9</ref> , and SASRec<ref type="foot" target="#foot_9">10</ref> , we use code provided by the corresponding authors. For BPR-MF and FPMC, we implement them using TensorFlow. For common hyperparameters in all models, we consider the hidden dimension size d from {16, 32, 64, 128, 256}, the ℓ 2 regularizer from {1, 0.1, 0.01, 0.001, 0.0001}, and dropout rate from {0, 0.1, 0.2, • • • , 0.9}. All other hyper-parameters (e.g., Markov order in Caser) and initialization strategies are either followed the suggestion from the methods' authors or tuned on the validation sets. We report the results of each baseline under its optimal hyper-parameter settings.</p><p>We implement BERT4Rec<ref type="foot" target="#foot_10">11</ref> with TensorFlow. All parameters are initialized using truncated normal distribution in the range [−0.02, 0.02]. We train the model using Adam <ref type="bibr" target="#b24">[24]</ref> with learning rate of 1e-4, β 1 = 0.9, β 2 = 0.999, ℓ 2 weight decay of 0.01, and linear decay of the learning rate. The gradient is clipped when its ℓ 2 norm exceeds a threshold of 5. For fair comparison, we set the layer number L = 2 and head number h = 2 and use the same maximum sequence length as in <ref type="bibr" target="#b21">[22]</ref>, N = 200 for ML-1m and ML-20m, N = 50 for Beauty and Steam datasets. For head setting, we empirically set the dimensionality of each head as 32 (single head if d &lt; 32). We tune the mask proportion ρ using the validation set, resulting in ρ = 0.6 for Beauty, ρ = 0.4 for Steam, and ρ = 0.2 for ML-1m and ML-20m. All the models are trained from scratch without any pre-training on a single NVIDIA GeForce GTX 1080 Ti GPU with a batch size of 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Overall Performance Comparison</head><p>Table <ref type="table" target="#tab_2">2</ref> summarized the best results of all models on four benchmark datasets. The last column is the improvements of BERT4Rec relative to the best baseline. We omit the NDCG@1 results since it is equal to HR@1 in our experiments. It can be observed that:</p><p>The non-personalized POP method gives the worst performance<ref type="foot" target="#foot_11">12</ref> on all datasets since it does not model user's personalized preference using the historical records. Among all the baseline methods, sequential methods (e.g., FPMC and GRU4Rec + ) outperforms non-sequential methods (e.g., BPR-MF and NCF) on all datasets consistently. Compared with BPR-MF, the main improvement of FPMC is that it models users' historical records in a sequential way. This observation verifies that considering sequential information is beneficial for improving performances in recommendation systems.</p><p>Among sequential recommendation baselines, Caser outperforms FPMC on all datasets especially for the dense dataset ML-1m, suggesting that high-order MCs is beneficial for sequential recommendation. However, high-order MCs usually use very small order L since they do not scale well with the order L. This causes Caser to perform worse than GRU4Rec + and SASRec, especially on sparse datasets. Furthermore, SASRec performs distinctly better than GRU4Rec and GRU4Rec + , suggesting that self-attention mechanism is a more powerful tool for sequential recommendation.</p><p>According to the results, it is obvious that BERT4Rec performs best among all methods on four datasets in terms of all evaluation metrics. It gains 7.24% HR@10, 11.03% NDCG@10, and 11.46% MRR improvements (on average) against the strongest baselines. Question 1: Do the gains come from the bidirectional self-attention model or from the Cloze objective?  To answer this question, we try to isolate the effects of these two factors by constraining the Cloze task to mask only one item at a time. In this way, the main difference between our BERT4Rec (with 1 mask) and SASRec is that BERT4Rec predicts the target item jointly conditioning on both left and right context. We report the results on Beauty and ML-1m with d = 256 in Table <ref type="table" target="#tab_3">3</ref> due to the space limitation. The results show that BERT4Rec with 1 mask significantly outperforms SASRec on all metrics. It demonstrates the importance of bidirectional representations for sequential recommendation. Besides, the last two rows indicate that the Cloze objective also improves the performances. Detailed analysis of the mask proportion ρ in Cloze task can be found in § 4.6 Question 2: Why and how does bidirectional model outperform unidirectional models?</p><p>To answer this question, we try to reveal meaningful patterns by visualizing the average attention weights of the last 10 items during the test on Beauty in Figure <ref type="figure" target="#fig_3">2</ref>. Due to the space limitation, we only report four representative attention heat-maps in different layers and heads.  We make several observations from the results. a) Attention varies across different heads. For example, in layer 1, head 1 tends </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRU4Rec</head><p>GRU4Rec + Caser SASRec BERT4Rec</p><p>Figure <ref type="figure">3</ref>: Effect of the hidden dimensionality d on HR@10 and NDCG@10 for neural sequential models.</p><p>to attend on items at the left side while head 2 prefers to attend on items on the right side. b) Attention varies across different layers. Apparently, attentions in layer 2 tend to focus on more recent items. This is because layer 2 is directly connected to the output layer and the recent items play a more important role in predicting the future. Another interesting pattern is that heads in Figure <ref type="figure" target="#fig_3">2a</ref> and 2b also tend to attend on [mask] 13 . It may be a way for self-attention to propagate sequence-level state to the item level. c) Finally and most importantly, unlike unidirectional model can only attend on items at the left side, items in BERT4Rec tend to attend on the items at both sides. This indicates that bidirectional is essential and beneficial for user behavior sequence modeling.</p><p>In the following studies, we examine the impact of the hyperparameters, including the hidden dimensionality d, the mask proportion ρ, and the maximum sequence length N . We analyze one hyper-parameter at a time by fixing the remaining hyper-parameters at their optimal settings. Due to space limitation, we only report NDCG@10 and HR@10 for the follow-up experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of Hidden Dimensionality d</head><p>We now study how the hidden dimensionality d affects the recommendation performance. Figure <ref type="figure">3</ref> shows NDCG@10 and HR@10 for neural sequential methods with the hidden dimensionality d varying from 16 to 256 while keeping other optimal hyper-parameters unchanged. We make some observations from this figure.</p><p>The most obvious observation from these sub-figures is that the performance of each model tends to converge as the dimensionality increases. A larger hidden dimensionality does not necessarily lead to better model performance, especially on sparse datasets like Beauty and Steam. This is probably caused by overfitting. In terms of details, Caser performs unstably on four datasets, which might limit its usefulness. Self-attention based methods (i.e., SASRec and BERT4Rec) achieve superior performances on all datasets. Finally, our model consistently outperforms all other baselines on all 13 This phenomenon also exists in text sequence modeling using BERT.</p><p>datasets even with a relatively small hidden dimensionality. Considering that our model achieves satisfactory performance with d ≥64, we only report the results with d=64 in the following analysis. As described in § 3.6, mask proportion ρ is a key factor in model training, which directly affects the loss function (Equation <ref type="formula" target="#formula_21">8</ref>). Obviously, mask proportion ρ should not be too small or it is not enough to learn a strong model. Meanwhile, it should not be too large, otherwise, it would be hard to train since there are too many items to guess based on a few contexts in such case. To examine this, we study how mask proportion ρ affects the recommendation performances on different datasets.</p><p>Figure <ref type="figure">4</ref> shows the results with varying mask proportion ρ from 0.1 to 0.9. Considering the results with ρ &gt; 0.6 on all datasets, a general pattern emerges, the performances decreasing as ρ increases. From the results of the first two columns, it is easy to see that ρ = 0.2 performs better than ρ = 0.1 on all datasets. These results verify what we claimed above.</p><p>In addition, we observe that the optimal ρ is highly dependent on the sequence length of the dataset. For the datasets with short sequence length (e.g., Beauty and Steam), the best performances are 0.6788 0.6854 0.6947 0.6955 0.6898 NDCG@10 0.4631 0.4743 0.4758 0.4759 0.4715 achieved at ρ=0.6 (Beauty) and ρ=0.4 (Steam), while the datasets with long sequence length (e.g., ML-1m and ML-20m) prefer a small ρ=0.2. This is reasonable since, compared with short sequence datasets, a large ρ in long sequence datasets means much more items that need to be predicted. Take ML-1m and Beauty as example, ρ=0.6 means we need to predict 98=⌊163.5×0.6⌋ items on average per sequence for ML-1m, while it is only 5=⌊8.8×0.6⌋ items for Beauty. The former is too hard for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Impact of Maximum Sequence Length N</head><p>We also investigate the effect of the maximum sequence length N on model's recommendation performances and efficiency.</p><p>Table <ref type="table" target="#tab_5">4</ref> shows recommendation performances and training speed with different maximum length N on Beauty and ML-1m. We observe that the proper maximum length N is also highly dependent on the average sequence length of the dataset. Beauty prefers a smaller N = 20, while ML-1m achieves the best performances on N = 200. This indicates that a user's behavior is affected by more recent items on short sequence datasets and less recent items for long sequence datasets. The model does not consistently benefit from a larger N since a larger N tends to introduce both extra information and more noise. However, our model performs very stably as the length N becomes larger. This indicates that our model can attend to the informative items from the noisy historical records.</p><p>A scalability concern about BERT4Rec is that its computational complexity per layer is O(n 2 d), quadratic with the length n. Fortunately, the results in Table <ref type="table" target="#tab_5">4</ref> shows that the self-attention layer can be effectively parallelized using GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Ablation Study</head><p>Finally, we perform ablation experiments over a number of key components of BERT4Rec in order to better understand their impacts, including positional embedding (PE), position-wise feed-forward network (PFFN), layer normalization (LN), residual connection (RC), dropout, the layer number L of self-attention, and the number of heads h in multi-head attention. Table <ref type="table">5</ref> shows the results of our default version (L = 2, h = 2) and its eleven variants on all four datasets with dimensionality d = 64 while keeping other hyperparameters (e.g., ρ) at their optimal settings.</p><p>We introduce the variants and analyze their effects respectively:</p><p>(1) PE. The results show that removing positional embeddings causes BERT4Rec's performances decreasing dramatically on long sequence datasets (i.e., ML-1m and ML-20m). Without the positional embeddings, the hidden representation H L i Table <ref type="table">5</ref>: Ablation analysis (NDCG@10) on four datasets. Bold score indicates performance better than the default version, while ↓ indicates performance drop more than 10%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>BERT4Rec model architecture. (a) Transformer Layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>• FPMC<ref type="bibr" target="#b40">[40]</ref>: It captures users' general taste as well as their sequential behaviors by combing MF with first-order MCs.• GRU4Rec<ref type="bibr" target="#b14">[15]</ref>: It uses GRU with ranking based loss to model user sequences for session based recommendation. • GRU4Rec + [14]: It is an improved version of GRU4Rec with a new class of loss functions and sampling strategy. • Caser [49]: It employs CNN in both horizontal and vertical way to model high-order MCs for sequential recommendation. • SASRec [22]: It uses a left-to-right Transformer language model to capture users' sequential behaviors, and achieves state-of-the-art performance on sequential recommendation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Heat-maps of average attention weights on Beauty, the last position "9" denotes "[mask]" (best viewed in color).</figDesc><graphic url="image-5.png" coords="7,328.78,532.32,86.62,86.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 6 Figure 4 :</head><label>64</label><figDesc>Figure 4: Performance with different mask proportion ρ on d = 64. Bold symbols denote the best scores in each line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>d) RNN based sequential recommendation methods.</figDesc><table><row><cell>v 2</cell><cell></cell><cell>v t</cell><cell>v t +1</cell></row><row><cell>Trm</cell><cell>. . .</cell><cell>Trm</cell><cell>Trm</cell></row><row><cell>Trm</cell><cell>. . .</cell><cell>Trm</cell><cell>Trm</cell></row><row><cell></cell><cell>. . .</cell><cell></cell><cell></cell></row><row><cell>v 1</cell><cell>. . .</cell><cell>v t −1</cell><cell>v t</cell></row><row><cell cols="4">(c) SASRec model architecture.</cell></row><row><cell cols="4">Figure 1: Differences in sequential recommendation model architectures. BERT4Rec learns a bidirectional model via Cloze</cell></row><row><cell cols="4">task, while SASRec and RNN based methods are all left-to-right unidirectional model which predict next item sequentially.</cell></row><row><cell>effectiveness and efficiency [30, 32, 38, 46? ]. For sequential recom-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mendation, Kang and McAuley [22] introduce a two-layer Trans-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>former decoder (i.e., Transformer language model) called SASRec</cell><cell></cell><cell></cell><cell></cell></row><row><cell>to capture user's sequential behaviors and achieve state-of-the-art</cell><cell></cell><cell></cell><cell></cell></row><row><cell>results on several public datasets. SASRec is closely related to our</cell><cell></cell><cell></cell><cell></cell></row><row><cell>work. However, it is still a unidirectional model using a casual at-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tention mask. While we use a bidirectional model to encode users'</cell><cell></cell><cell></cell><cell></cell></row><row><cell>behavior sequences with the help of Cloze task.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="5">#users #items #actions Avg. length Density</cell></row><row><cell>Beauty</cell><cell cols="2">40,226 54,542</cell><cell>0.35m</cell><cell>8.8</cell><cell>0.02%</cell></row><row><cell>Steam</cell><cell cols="2">281,428 13,044</cell><cell>3.5m</cell><cell>12.4</cell><cell>0.10%</cell></row><row><cell>ML-1m</cell><cell>6040</cell><cell>3416</cell><cell>1.0m</cell><cell>163.5</cell><cell>4.79%</cell></row><row><cell cols="3">ML-20m 138,493 26,744</cell><cell>20m</cell><cell>144.4</cell><cell>0.54%</cell></row><row><cell cols="6">left and right). It can be seen as a simplified case of BERT4Rec, if we</cell></row><row><cell cols="6">use one self-attention layer in BERT4Rec with uniform attention</cell></row><row><cell cols="6">weights on items, unshare item embeddings, remove the positional</cell></row><row><cell cols="6">embedding, and only mask the central item. Similar to CBOW, SG</cell></row><row><cell cols="6">can also be seen as a simplified case of BERT4Rec following similar</cell></row><row><cell cols="6">reduction operations (mask all items except only one). From this</cell></row><row><cell cols="6">point of view, Cloze can be seen as a general form for the objective of</cell></row><row><cell cols="6">CBOW and SG. Besides, CBOW uses a simple aggregator to model</cell></row><row><cell cols="6">word sequences since its goal is to learn good word representations,</cell></row><row><cell cols="6">not sentence representations. On the contrary, we seek to learn</cell></row><row><cell cols="6">a powerful behavior sequence representation model (deep self-</cell></row><row><cell cols="6">attention network in this work) for making recommendations.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of different methods on next-item prediction. Bold scores are the best in each row, while underlined scores are the second best. Improvements over baselines are statistically significant with p &lt; 0.01.</figDesc><table><row><cell cols="2">Datasets Metric</cell><cell>POP</cell><cell>BPR-MF</cell><cell>NCF</cell><cell cols="7">FPMC GRU4Rec GRU4Rec + Caser SASRec BERT4Rec Improv.</cell></row><row><cell></cell><cell>HR@1</cell><cell>0.0077</cell><cell>0.0415</cell><cell cols="2">0.0407 0.0435</cell><cell>0.0402</cell><cell>0.0551</cell><cell>0.0475</cell><cell>0.0906</cell><cell>0.0953</cell><cell>5.19%</cell></row><row><cell></cell><cell>HR@5</cell><cell>0.0392</cell><cell>0.1209</cell><cell cols="2">0.1305 0.1387</cell><cell>0.1315</cell><cell>0.1781</cell><cell>0.1625</cell><cell>0.1934</cell><cell>0.2207</cell><cell>14.12%</cell></row><row><cell>Beauty</cell><cell>HR@10 NDCG@5</cell><cell>0.0762 0.0230</cell><cell>0.1992 0.0814</cell><cell cols="2">0.2142 0.2401 0.0855 0.0902</cell><cell>0.2343 0.0812</cell><cell>0.2654 0.1172</cell><cell>0.2590 0.1050</cell><cell>0.2653 0.1436</cell><cell>0.3025 0.1599</cell><cell>14.02% 11.35%</cell></row><row><cell></cell><cell cols="2">NDCG@10 0.0349</cell><cell>0.1064</cell><cell cols="2">0.1124 0.1211</cell><cell>0.1074</cell><cell>0.1453</cell><cell>0.1360</cell><cell>0.1633</cell><cell>0.1862</cell><cell>14.02%</cell></row><row><cell></cell><cell>MRR</cell><cell>0.0437</cell><cell>0.1006</cell><cell cols="2">0.1043 0.1056</cell><cell>0.1023</cell><cell>0.1299</cell><cell>0.1205</cell><cell>0.1536</cell><cell>0.1701</cell><cell>10.74%</cell></row><row><cell></cell><cell>HR@1</cell><cell>0.0159</cell><cell>0.0314</cell><cell cols="2">0.0246 0.0358</cell><cell>0.0574</cell><cell>0.0812</cell><cell>0.0495</cell><cell>0.0885</cell><cell>0.0957</cell><cell>8.14%</cell></row><row><cell></cell><cell>HR@5</cell><cell>0.0805</cell><cell>0.1177</cell><cell cols="2">0.1203 0.1517</cell><cell>0.2171</cell><cell>0.2391</cell><cell>0.1766</cell><cell>0.2559</cell><cell>0.2710</cell><cell>5.90%</cell></row><row><cell>Steam</cell><cell>HR@10 NDCG@5</cell><cell>0.1389 0.0477</cell><cell>0.1993 0.0744</cell><cell cols="2">0.2169 0.2551 0.0717 0.0945</cell><cell>0.3313 0.1370</cell><cell>0.3594 0.1613</cell><cell>0.2870 0.1131</cell><cell>0.3783 0.1727</cell><cell>0.4013 0.1842</cell><cell>6.08% 6.66%</cell></row><row><cell></cell><cell cols="2">NDCG@10 0.0665</cell><cell>0.1005</cell><cell cols="2">0.1026 0.1283</cell><cell>0.1802</cell><cell>0.2053</cell><cell>0.1484</cell><cell>0.2147</cell><cell>0.2261</cell><cell>5.31%</cell></row><row><cell></cell><cell>MRR</cell><cell>0.0669</cell><cell>0.0942</cell><cell cols="2">0.0932 0.1139</cell><cell>0.1420</cell><cell>0.1757</cell><cell>0.1305</cell><cell>0.1874</cell><cell>0.1949</cell><cell>4.00%</cell></row><row><cell></cell><cell>HR@1</cell><cell>0.0141</cell><cell>0.0914</cell><cell cols="2">0.0397 0.1386</cell><cell>0.1583</cell><cell>0.2092</cell><cell>0.2194</cell><cell>0.2351</cell><cell>0.2863</cell><cell>21.78%</cell></row><row><cell></cell><cell>HR@5</cell><cell>0.0715</cell><cell>0.2866</cell><cell cols="2">0.1932 0.4297</cell><cell>0.4673</cell><cell>0.5103</cell><cell>0.5353</cell><cell>0.5434</cell><cell>0.5876</cell><cell>8.13%</cell></row><row><cell>ML-1m</cell><cell>HR@10 NDCG@5</cell><cell>0.1358 0.0416</cell><cell>0.4301 0.1903</cell><cell cols="2">0.3477 0.5946 0.1146 0.2885</cell><cell>0.6207 0.3196</cell><cell>0.6351 0.3705</cell><cell>0.6692 0.3832</cell><cell>0.6629 0.3980</cell><cell>0.6970 0.4454</cell><cell>4.15% 11.91%</cell></row><row><cell></cell><cell cols="2">NDCG@10 0.0621</cell><cell>0.2365</cell><cell cols="2">0.1640 0.3439</cell><cell>0.3627</cell><cell>0.4064</cell><cell>0.4268</cell><cell>0.4368</cell><cell>0.4818</cell><cell>10.32%</cell></row><row><cell></cell><cell>MRR</cell><cell>0.0627</cell><cell>0.2009</cell><cell cols="2">0.1358 0.2891</cell><cell>0.3041</cell><cell>0.3462</cell><cell>0.3648</cell><cell>0.3790</cell><cell>0.4254</cell><cell>12.24%</cell></row><row><cell></cell><cell>HR@1</cell><cell>0.0221</cell><cell>0.0553</cell><cell cols="2">0.0231 0.1079</cell><cell>0.1459</cell><cell>0.2021</cell><cell>0.1232</cell><cell>0.2544</cell><cell>0.3440</cell><cell>35.22%</cell></row><row><cell></cell><cell>HR@5</cell><cell>0.0805</cell><cell>0.2128</cell><cell cols="2">0.1358 0.3601</cell><cell>0.4657</cell><cell>0.5118</cell><cell>0.3804</cell><cell>0.5727</cell><cell>0.6323</cell><cell>10.41%</cell></row><row><cell>ML-20m</cell><cell>HR@10 NDCG@5</cell><cell>0.1378 0.0511</cell><cell>0.3538 0.1332</cell><cell cols="2">0.2922 0.5201 0.0771 0.2239</cell><cell>0.5844 0.3090</cell><cell>0.6524 0.3630</cell><cell>0.5427 0.2538</cell><cell>0.7136 0.4208</cell><cell>0.7473 0.4967</cell><cell>4.72% 18.04%</cell></row><row><cell></cell><cell cols="2">NDCG@10 0.0695</cell><cell>0.1786</cell><cell cols="2">0.1271 0.2895</cell><cell>0.3637</cell><cell>0.4087</cell><cell>0.3062</cell><cell>0.4665</cell><cell>0.5340</cell><cell>14.47%</cell></row><row><cell></cell><cell>MRR</cell><cell>0.0709</cell><cell>0.1503</cell><cell cols="2">0.1072 0.2273</cell><cell>0.2967</cell><cell>0.3476</cell><cell>0.2529</cell><cell>0.4026</cell><cell>0.4785</cell><cell>18.85%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Analysis on bidirection and Cloze with d = 256.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Beauty</cell><cell></cell><cell>ML-1m</cell><cell></cell></row><row><cell></cell><cell cols="5">HR@10 NDCG@10 MRR HR@10 NDCG@10 MRR</cell></row><row><cell>SASRec</cell><cell>0.2653</cell><cell>0.1633</cell><cell>0.1536 0.6629</cell><cell>0.4368</cell><cell>0.3790</cell></row><row><cell cols="2">BERT4Rec (1 mask) 0.2940</cell><cell>0.1769</cell><cell>0.1618 0.6869</cell><cell>0.4696</cell><cell>0.4127</cell></row><row><cell>BERT4Rec</cell><cell>0.3025</cell><cell>0.1862</cell><cell>0.1701 0.6970</cell><cell>0.4818</cell><cell>0.4254</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance with different maximum length N .</figDesc><table><row><cell></cell><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell></cell><cell>#samples/s</cell><cell>5504</cell><cell>3256</cell><cell>2284</cell><cell>1776</cell><cell>1441</cell></row><row><cell>Beauty</cell><cell>HR@10</cell><cell cols="5">0.3006 0.3061 0.3057 0.3054 0.3047</cell></row><row><cell></cell><cell cols="6">NDCG@10 0.1826 0.1875 0.1837 0.1833 0.1832</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>400</cell></row><row><cell></cell><cell cols="2">#samples/s 14255</cell><cell>8890</cell><cell>5711</cell><cell>2918</cell><cell>1213</cell></row><row><cell>ML-1m</cell><cell>HR@10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.netflixprize.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Here, following<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">40]</ref>, we use the relative time index instead of absolute time index for numbering interaction records.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">http://jmcauley.ucsd.edu/data/amazon/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://grouplens.org/datasets/movielens/1m/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://grouplens.org/datasets/movielens/20m/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/hexiangnan/neural_collaborative_filtering</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://github.com/hidasib/GRU4Rec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">https://github.com/graytowne/caser_pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9">https://github.com/kang205/SASRec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10">https://github.com/FeiSun/BERT4Rec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11">What needs to be pointed out is that such low scores for POP are because the negative samples are sampled according to the items' popularity.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for each item v i depends only on item embeddings. In this situation, we predict different target items using the same hidden representation of "[mask]". This makes the model illposed. This issue is more serious on long sequence datasets since they have more masked items to predict. (2) PFFN. The results show that long sequence datasets (e.g., ML-20m) benefit more from PFFN. This is reasonable since a purpose of PFFN is to integrate information from many heads which are preferred by long sequence datasets as discussed in the analysis about head number h in ablation study <ref type="bibr" target="#b4">(5)</ref>. (3) LN, RC, and Dropout. These components are introduced mainly to alleviate overfitting. Obviously, they are more effective on small datasets like Beauty. To verify their effectiveness on large datasets, we conduct an experiment on ML-20m with layer L=4. The results show that NDCG@10 decreases about 10% w/o RC. (4) Number of layers L. The results show that stacking Transformer layer can boost performances especially on large datasets (e.g, ML-20m). This verifies that it is helpful to learn more complex item transition patterns via deep self-attention architecture. The decline in Beauty with L = 4 is largely due to overfitting. ( <ref type="formula">5</ref>) Head number h. We observe that long sequence datasets (e.g., ML-20m) benefit from a larger h while short sequence datasets (e.g., Beauty) prefer a smaller h. This phenomenon is consistent with the empirical result in <ref type="bibr" target="#b48">[48]</ref> that large h is essential for capturing long distance dependencies with multi-head self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>Deep bidirectional self-attention architecture has achieved tremendous success in language understanding. In this paper, we introduce a deep bidirectional sequential model called BERT4Rec for sequential recommendation. For model training, we introduce the Cloze task which predicts the masked items using both left and right context. Extensive experimental results on four real-world datasets show that our model outperforms state-of-the-art baselines.</p><p>Several directions remain to be explored. A valuable direction is to incorporate rich item features (e.g., category and price for products, cast for movies) into BERT4Rec instead of just modeling item ids. Another interesting direction for the future work would be introducing user component into the model for explicit user modeling when the users have multiple sessions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer Normalization. CoRR abs/1607</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page">6450</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequential Recommendation with User Memory Networks</title>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
				<meeting>WSDM<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="108" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RecSys</title>
				<meeting>RecSys<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequential User-based Recurrent Neural Network Recommendations</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Donkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Loepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Ziegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RecSys</title>
				<meeting>RecSys<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="152" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The MovieLens Datasets</title>
				<imprint>
			<date type="published" when="2015-12">2015. Dec. 2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">pages</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Translation-based Recommendation</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RecSys</title>
				<meeting>RecSys<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
				<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>CoRR abs/1606.08415</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent Neural Networks with Top-k Gains for Session-based Recommendations</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Session-based Recommendations with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Representation Learning Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">1997. Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diversifying Personalized Recommendation with User-session Context</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1858" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
				<meeting>SIGIR<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FISM: Factored Item Similarity Models for top-N Recommender Systems</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="659" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visually-Aware Fashion Recommendation and Design with Generative Image Models</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM. IEEE Computer Society</title>
				<meeting>ICDM. IEEE Computer Society</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-Attentive Sequential Recommendation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
				<meeting>ICDM</meeting>
		<imprint>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional Matrix Factorization for Document Context-Aware Recommendation</title>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinoh</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RecSys</title>
				<meeting>RecSys<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Advances in Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook</title>
				<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="145" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009-08">2009. Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural Attentive Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-Head Attention with Disagreement Regularization</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of tEMNLP</title>
				<meeting>tEMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2897" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Structured Self-attentive Sentence Embedding</title>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cícero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Amazon.Com Recommendations: Item-to-Item Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003-01">2003. Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating Wikipedia by Summarizing Long Sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">STAMP: Short-Term Attention/Memory Priority Model for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Refuoe</forename><surname>Mokhosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1831" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image-Based Recommendations on Styles and Substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
				<meeting>SIGIR<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and Their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Quadrana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RecSys</title>
				<meeting>RecSys<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OpenAI Technical report</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
				<meeting>UAI<address><addrLine>Arlington, Virginia, United States</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Factorizing Personalized Markov Chains for Next-basket Recommendation</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Probabilistic Matrix Factorization</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann Machines for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Item-based Collaborative Filtering Recommendation Algorithms</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AutoRec: Autoencoders Meet Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An MDP-Based Recommender System</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1265" to="1295" />
			<date type="published" when="2005-12">2005. Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-Attention with Relative Position Representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01">2014. Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures</title>
		<author>
			<persName><forename type="first">Gongbo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annette</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4263" to="4272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
				<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="565" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<author>
			<persName><forename type="first">Wilson</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">âĂĲCloze ProcedureâĂİ: A New Tool for Measuring Readability</title>
				<imprint>
			<date type="published" when="1953">1953. 1953</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="415" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Collaborative Deep Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Attention-Based Transactional Context Embedding for Next-Item Recommendation</title>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2532" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">What Your Images Reveal: Exploiting Visual Contents for Point-of-Interest Recommendation</title>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Recurrent Recommender Networks</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">How</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
				<meeting>WSDM<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Collaborative Denoising Auto-Encoders for Top-N Recommender Systems</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
				<meeting>WSDM<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A Dynamic Recurrent Model for Next Basket Recommendation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
				<meeting>SIGIR<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="729" to="732" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
