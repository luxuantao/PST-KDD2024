<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Microaneurysm Detection using Fully Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Piotr</forename><surname>Chudzik</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Lincoln</orgName>
								<address>
									<postCode>LN6 7TS</postCode>
									<settlement>Lincoln</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Somshubra</forename><surname>Majumdar</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesco</forename><surname>Caliv</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bashir</forename><surname>Al-Diri</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Lincoln</orgName>
								<address>
									<postCode>LN6 7TS</postCode>
									<settlement>Lincoln</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Hunter</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Lincoln</orgName>
								<address>
									<postCode>LN6 7TS</postCode>
									<settlement>Lincoln</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Comm</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Francesco</forename><surname>Calivá</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Lincoln</orgName>
								<address>
									<postCode>LN6 7TS</postCode>
									<settlement>Lincoln</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Methods and Programs in Biomedicine</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Microaneurysm Detection using Fully Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">07B7EF9051621383DE427A31529F47C8</idno>
					<idno type="DOI">10.1016/j.cmpb.2018.02.016</idno>
					<note type="submission">Received date: 3 July 2017 Revised date: 18 January 2018 Accepted date: 22 February 2018 Preprint submitted to Journal of Computer Methods and Programs in BiomedicineFebruary 22, 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Medical Image Analysis, Microaneurysm Detection,</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Backround and Objectives: Diabetic retinopathy is a microvascular complication of diabetes that can lead to sight loss if treated not early enough. Microaneurysms are the earliest clinical signs of diabetic retinopathy. This paper presents an automatic method for detecting microaneurysms in fundus photographies.</p><p>Methods: A novel patch-based fully convolutional neural network with batch normalization layers and Dice loss function is proposed. Compared to other methods that require up to five processing stages, it requires only three. Furthermore, to the best of the authors' knowledge, this is the first paper that shows how to successfully transfer knowledge between datasets in the microaneurysm detection domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The proposed method was evaluated using three publicly available and widely used datasets: E-Ophtha, DIARETDB1, and ROC. It achieved better results than state-of-the-art methods using the FROC metric. The proposed algorithm accomplished highest sensitivities for low false positive rates, which is particularly important for screening purposes.</p><p>Conclusions: Performance, simplicity, and robustness of the proposed method demonstrates its suitability for diabetic retinopathy screening applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• An automatic method for detecting microaneurysms in fundus images is proposed.</p><p>• It uses a fully convolutional neural network with batch normalization and Dice loss.</p><p>• It requires only two processing stages.</p><p>• Shows how to transfer knowledge between datasets in microaneurysm domain.</p><p>• Produces better results than state-of-the-art methods. Diabetic retinopathy (DR) is a microvascular complication of diabetes which is the leading cause of vision loss in the working-age population <ref type="bibr" target="#b0">[2]</ref>. One out of three diabetics has DR <ref type="bibr" target="#b1">[3]</ref> and one in ten diabetic patients develops most 5 vision-threatening form of DR <ref type="bibr" target="#b2">[4]</ref>. Early detection of DR can prevent blindness in 90% of cases <ref type="bibr" target="#b3">[5]</ref>.</p><p>DR screening is manually performed by ophthalmologists and trained graders through a visual inspection of fundus photographs (FP). Unfortunately, the grading process is time-consuming, tedious, and error-prone with high inter-10 observer variability. Due to the rising number of DR patients worldwide (expected to exceed 640m by 2040 <ref type="bibr">[1]</ref>) and their location (75% live in underdeveloped areas <ref type="bibr" target="#b4">[6]</ref>) the development of computer-assisted diagnosis and automatic DR screening approaches are of the utmost importance.</p><p>Microaneurysms (MAs) are spherical swellings of the capillaries caused by 15 weakening of the vascular walls; they appear as small round red dots. They are the earliest clinical sign of DR and continue to be present as the disease progresses. Consequently, automated detection of MAs can drastically reduce the screening workload. MA detection is a challenging task even for the human eye due to many factors including uneven image illumination, reflections, limited 20 resolution and media opacity. The boundaries of MAs are not always welldefined and local contrast to the background is low, even in high-resolution images. Moreover, MAs may be confounded with visually similar anatomical structures such as haemorrhages, junctions in thin vessels, disconnected vessel segments, dark patches on vessels, background pigmentation patches and dust Training CNNs from scratch is not a trivial task, as they require large amounts of labelled data for training. In the MA detection domain, public datasets are small, scarce, and local lesion annotations on a per-pixel level are 80 almost non-existent (to the best of authors knowledge, only one such dataset exists <ref type="bibr" target="#b7">[18]</ref>). Moreover, the CNNs have vast capacity as learning models with millions of learnable parameters. As a result, they are very prone to overfitting and various convergence difficulties. Consequently, the initial values of a network's weights have paramount importance in the learning process, especially 85 for avoidance of local minima and saddle points.</p><p>To address these challenges, prior knowledge in the form of a network's weights can be transferred between models that are later fine-tuned with new data. Azizpour et al. <ref type="bibr" target="#b8">[19]</ref> showed that the success of knowledge transfer depends on the similarity between the training dataset of a CNN, and the dataset to which the knowledge is transferred. Given the limited availability of large medical datasets, research on transfer learning in medical imaging is largely focussed on transferring knowledge from general natural images datasets. However, these datasets have very different properties to medical datasets, including the fact that in medical datasets objects of interest may be very small and boundaries 95 are of paramount importance. Consequently, knowledge transfer between these two domains is not optimal and produces various success rates <ref type="bibr" target="#b8">[19]</ref>. In this paper we show that knowledge transfer even between small medical datasets can produce state-of-the-art results with an appropriate network architecture. To the best of our knowledge, this is the first time that deep transfer learning has 100 been applied in the MA detection domain.</p><p>The main contributions of this paper are as follows. First, we propose a MA detection method that requires only three stages of analysis. Second, we present a novel CNN with a dedicated architecture for MA detection that does not require hand-crafted features. Third, we show how to successfully transfer 105 knowledge between small datasets in MA domain -an important innovation in this domain as retinal image set characteristics vary between cameras, so that any practically useful method must be capable of simple and reliable retraining. This paper is organized as follows. The proposed method is described in Section II. Section III describes the datasets and performance metrics used for 110 experiments. In Section IV the evaluation results are presented and compared with existing approaches. Finally, in Section V discussion and conclusions are given.  First, we extract the green plane of the fundus image as it provides the highest contrast between foreground structures, such as lesions and vessels, and the background. Since we are only interested in pixels inside a Field-of-View (FOV), we automatically generate a mask for pixels outside the FOV. A mask 125 is generated by applying Otsu thresholding <ref type="bibr" target="#b9">[20]</ref> to the green plane of the image. Noisy regions are removed by morphological opening and closing with a structuring element of size five. Next, the image is cropped to the size defined by its FOV to accelerate further processing. Subsequently, the image is resized to the smallest image width of the E-ophtha dataset <ref type="bibr" target="#b7">[18]</ref>, while maintaining the 130 aspect ratio, using bicubic interpolation. Simultaneously, the same operations are applied to the corresponding annotation image. Finally, each image (I) was preprocessed (I p ) by computing a weighted sum as in Eq. 1:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Method</head><formula xml:id="formula_0">I p = I • α + I Gauss • β + γ<label>(1)</label></formula><p>where alpha = 4 and β = -10 are weight factors; I Gauss is Gaussian blurred image that was created using filter computed as described in Eq. 2 with σ = 10; 135 γ = 128 is a scalar added to each sum.</p><formula xml:id="formula_1">G(x, y) = 1 2πσ 2 e -x 2 +y 2 2σ 2 (2)</formula><p>All values were determined experimentally. Fig. <ref type="figure" target="#fig_17">3</ref> shows an example preprocessed image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pixel-Wise Classification</head><p>The main goal of this stage is to classify each pixel as either MA or non-140 MA. We cast pixel-wise classification as a probabilistic classification task, where each pixel can be assigned a continuous value between 0 (non-MA) and 1 (MA).</p><p>Compared to other works which perform a binary classification, this learning task is more challenging because the expected output is more complex, hence the underlying data distribution function is harder to model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The CNN is trained to map an image patch P to the corresponding annotation A(P ) for all possible locations within an image. A training sample consists of S × S sized P and A(P ) : {P, A(P )}.</p><p>The goal of training is to learn a mapping P → A(P ) in the form of a CNN by minimizing</p><formula xml:id="formula_2">150 L = N i=1 l(A(P ) i , f (P i ; Θ)) + Φ(Θ)),<label>(3)</label></formula><p>where A(P ) i and P i are the i-th annotation patch and i-th image patch, N is the number of training samples, l(•) is the loss function, Θ are learning parameters, and Φ(Θ) is the regularization term. MA is assigned a probability of v/S 2 . As a result, a confidence map for pixel MA membership is created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Patch Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">CNN Architecture and Training</head><p>Inspired by <ref type="bibr" target="#b10">[21]</ref>, we adopted a fully convolutional approach when designing the CNN. The architecture of the CNN is similar to a convolutional autoencoder:</p><p>180 it consists of "contracting" and "expanding" paths. The "contracting" path is used to extract most discriminative features from input (encode the input), whereas the "expanding" path is tasked with recreating and classifiying the input by using upscaling and 1 × 1 convolution operations. Skip connections between the two paths allow for a direct flow of feature maps from earlier to 185 latter layers, which is beneficial for the learning process <ref type="bibr" target="#b11">[22]</ref>. Ronnenberg et al. <ref type="bibr" target="#b10">[21]</ref> designed their fully convolutional neural networks for segmentation of whole images in one pass. As MAs are local features, it is more appropriate here to use a network with a small receptive field and a sliding window approach to processing. Compared with <ref type="bibr" target="#b10">[21]</ref>, the proposed architecture works on small The use of BN layers in CNNs results in faster convergence (higher learning rates) and better regularization (by constraining layer's inputs, it's weights are also indirectly constrained).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>220</head><p>The CNN architecture was determined experimentally and is depicted in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Fine-tuning is a process of training a neural network from a set of predefined weights <ref type="bibr" target="#b18">[28]</ref>. A traditional approach to fine-tine deep neural networks (DNN) is to train only the final layers of a network using a small learning rate. Similarly to <ref type="bibr" target="#b18">[28]</ref>, it was observed that such approach can provide sub-235 optimal performance. To find the best ratio between trained and frozen layers, an iterative approach with varying train/freeze ratio was employed on a small dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Materials and Evaluation</head><p>The proposed algorithm was evaluated using most widely used performance 240 metrics and publicly available datasets which are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>E-Ophtha dataset <ref type="bibr" target="#b7">[18]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>To assess the performance of the proposed method we performed two sets 290 of experiments. In the first set of experiments we evaluate and compare finetuning schemes. In the second, we compare the performance of proposed MA detection technique with other state-of-the-art methods.</p><p>The implementation was based on Keras deep learning framework <ref type="bibr" target="#b21">[31]</ref> and Tensorflow numerical computation library <ref type="bibr" target="#b22">[32]</ref>. The experiments were con-295 ducted using a PC with Intel Core i7-6700K CPU, two NVIDIA TitanX graphics cards, and 64GB of RAM.  impact on speed of error convergence. We conclude that the system is not sensitive to small parameters change, however such changes can affect the amount of time needed for training. To find the optimal fine-tuning scheme we performed 10 experiments using  Table <ref type="table" target="#tab_2">2</ref> shows a comparison of all fine-tuning schemes. The Dice metric was calculated on per-pixel basis for the test dataset. In our experiments we applied both "shallow" and "deep" fine-tuning by iteratively freezing more initial</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-Tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T layers as proposed by <ref type="bibr" target="#b18">[28]</ref>. As expected, networks trained from scratch (no finetuning) and fully retrained (full fine-tuning) provided the worst results. The 325 network without any fine-tuning did not produce a FROC score because the lowest achieved FPI was just below 0.5, and to calculate the FROC score all seven FROC values are required. For comparison purposes we assign a 0 value to all methods that fail to produce the FROC score. These approaches do not take full advantage of already provided knowledge in the form of a base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>330</head><p>Freezing BN layers results in worse performance compared with the same models when BN layers are trainable. The network with 14 initial layers frozen achieved a comparably high test DICE, which means that it still produced competitive results for all possible pixels. However, the per-lesion evaluation showed that the lowest FPI it managed to reach was around 0.25 which is not enough to cal-    of the base model were frozen with remaining 7 trained with new data. Table <ref type="table">6</ref> presents the performance comparison using E-Ophtha dataset. This dataset is much bigger than the previous datasets which results in bigger training datasets.</p><p>The DNNs benefit from bigger datasets <ref type="bibr" target="#b24">[34]</ref> hence the results are better than compared with other datasets. Fig. <ref type="figure" target="#fig_18">4</ref> presents FROC curves produced by the 360 proposed algorithm for all three datasets.</p><p>Table <ref type="table">7</ref> shows results of Wilcoxon signed rank tests between the proposed method and Freeze All method for ROC and DIARETDB1 datasets. The null hypothesis is that the proposed method provides similar results to Freeze All method, whereas the alternative hypothesis is that the proposed method pro- were calculated at 1.08 FPI rate which is regarded as clinically acceptable <ref type="bibr" target="#b19">[29]</ref>.</p><p>We observe that many false positive detections are difficult to discern even for a 375 human eye. Similarly to <ref type="bibr" target="#b20">[30]</ref> we observe high inter-observer variability between human graders, which negatively affects the quality of provided ground truths and trained models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>The proposed algorithm achieves better results than state-of-the-art methods in terms of the FROC metric. Most importantly, it provides highest performance at low FPIs which are particularly significant for screening application. An MA 390 detection system for screening purposes does not have to find all MAs, but enough MAs to help a clinician decide if a patient needs referral. As such, we think that the proposed algorithm would prove useful as a component of a DR screening process.</p><p>The total time required to process a single image is around 220 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>395</head><p>The majority of this time is spent on forward propagating the large amount of patches through the network. However, during this study we did not concentrate on algorithm's efficiency, hence the implementation is experimental and can be improved. The processing time per image could be drastically reduced if the forward propagation step would be parallelized across multiple devices. This </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper presents a novel MA detection method evaluated using three publicly available datasets. The proposed algorithm uses a novel FCNN archi-   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>S C R I P T Convolutional Neural Networks, Retinal Fundus Images 1. Introduction Diabetes affects one in eleven adults (over 400 million people worldwide) [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>25 particles</head><label>25</label><figDesc>on the camera lense. In general, the majority of MA detection methods consists of up to five stages: 1) Preprocessing, 2) MA candidate extraction, 3) Vessel removal, 4) Can-A C C E P T E D M A N U S C R I P T didate feature extraction, and 5) Classification. The main goal of preprocessing is to remove noise, correct non-uniform illumination, and to improve contrast 30 between the MAs and background. The MA candidate extraction stage uses a simple algorithm to identify a reasonably small set of locations with somewhat "lesion-like" appearance, attempting to identify all actual lesions together with many false positive regions. The vessel removal stage addresses the large number of false positives that may otherwise be produced by vessels. Next, hand-crafted 35 features are extracted from candidate regions; this is the most labour-intensive and time-consuming part of the design stage. Finally, a classifier is trained to distinguish MAs from non-MAs based on the extracted features. Baudoin et al. [7] introduced the first MA detection algorithm applied to fluorescein angiogram images. They employed a mathematical morphology based 40 approach to remove vessels and applied a top-hat transformation with linear structuring elements to detect MAs. Several methods were built on this approach [8], however, since intravenous use of fluorescein can cause death in 1 in 222 000 cases [9], such methods are not suited for screening purposes. Walter et al. [10] also used a top-hat based method and automated thresholding to extract 45 MA candidates. They extracted 15 features and applied kernel density estimation with variable bandwith for MA classification. In general, morphology-based approaches are sensitive to changes in size and shape of structuring elements which result in significant variations in MAs detection results. Zhang et al. [11] proposed a method based on dynamic thresholding and correlation coefficients 50 of a multi-scale Gaussian template. They used 31 manually designed features based on intensity, shape and response of a Gaussian filter. Veiga et al. [12] presented an algorithm using Law texture features. Support Vector Machines (SVM) were used in a cascading manner: first SVM was used to extract MA candidates whereas the second SVM performed final MA classification. Haloi [13] 55 used a vanilla convolutional network with 3 convolutional layers and 2 fully connected layers to detect MAs. Javidi et al. [8] proposed a technique which used 2D Morlet wavelet to find MA candidates. At the next stage, a discriminative dictionary learning approach was employed to distinguish MAs from other A C C E P T E D M A N U S C R I P T structures. Srivastava et al. [14] used Frangi-based filters that were manually 60 designed to distinguish vessels from red lesions. Filters were applied to multiple sized image patches to extract features. Finally, these features were classified using a SVM. Compared to the methods mentioned above, the proposed algorithm requires only three stages instead of five (preprocessing, patch extraction and classifica-65 tion). There is no need for MA candidate detection, vessel removal or feature extraction. Furthermore, the proposed method does not require manually handcrafted features, it automatically learns the most discriminative features for MA detection. The vast majority of MA detection algorithms employ features based on MA shape, colour and texture. Unfortunately, many image modalities makes 70 it virtually impossible to model them manually. To address this challenge, a Convolutional Neural Network (CNN) was used. CNNs have emerged as a powerful family of algorithms for solving computer vision tasks such as object detection [15], semantic segmentation [16] and image classification [17]. Compared with [13] method, the presented algorithm proposes a novel fully convo-75 lutional neural network (FCNN) architecture and transfers knowledge between MA datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1</head><label>1</label><figDesc>Fig.1shows a general overview of the proposed method. It consists of three</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>At training time, all possible image patches are extracted from each training 155 image using a sliding window approach with 2 × 2 stride. The patches are divided into two groups: MA patches containing at least 1 MA pixel and non-MA patches consisting of all remaining patches. Both MA and non-MA patches are randomly sampled from the set of all possible patches. Patches that are completely outside the FOV are discarded. Each training sample is subject 160 to random artificial transformations (AT) including rotation, horizontal and vertical reflections with 0.5 probability. The ATs are performed to increase variety in the training set and combat overfitting; they are performed during CNN training so their computational footprint is limited. The proposed method works on a pixel level hence even MA patches consist of more non-MA pixels 165 than MA pixels. As such, MA patches provide both positive and negative training samples. Nevertheless, we added a small set of non-MA patches to the training set to provide network with examples of as many as possible retinal structures(e.g. fovea, optic nerve head) and backgrounds. As a result, the training set consists in 80% of MA patches and in 20% of non-MA patches. 170 At testing time, all possible image patches from inside of a FOV are extracted. To reconstruct the final image segmentation a voting mechanism is A C C E P T E D M A N U S C R I P T used. Each A(P ) produced by the model provides a single vote for all pixels it contains. Given that patches are centred at all possible locations and the A(P ) size is S × S, each pixel receives S 2 votes, and a pixel receiving v votes as an 175</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>190</head><label></label><figDesc>image patches, incorporates batch normalization (BN) layers and uses different loss function. As MAs occupy a very small proportion of fundus images that feature them, there is a significant class imbalance in the problem domain. To address this we incorporated a Dice coefficient function<ref type="bibr" target="#b12">[23]</ref> as a loss function as it effectively handles the overwhelming number of true negatives. The Dice 195 coefficient loss function was used before with CNNs<ref type="bibr" target="#b11">[22]</ref> but not in context of MA detection. The training algorithm maximises the Dice loss function which measures the overlap between ground truths y and predicted segmentation ŷ. Its values range between 0 (no overlap) and 1 (perfect agreement) and is calculated as200 DICE = 2 * |y ŷ| + δ |y| + |ŷ| + δ (4) A C C E P T E D M A N U S C R I P Twhere δ is a small smoothing factor that counteracts against zero value and zero denominator.The MA detection domain suffers from a common problem in medical imaging that stems from data scarcity, known as Covariate Shift: the distribution of features is different for subsets of training and test datasets which violates 205 the i.i.d.(independent and identically distributed) assumption of many machine learning (ML) algorithms<ref type="bibr" target="#b13">[24]</ref>. This may result from the use of different retinal camera systems and/or camera settings. The Covariate Shift in small datasets renders the modelling of true data distribution using ML models virtually impossible. To mitigate this difficulty and make data comparable across features, 210 a normalization technique (shifting data to zero mean and unit variance) is used as a preprocessing step [24]. The same phenomenon occurs during training deep CNNs which are hierarchical in nature and is called Internal Covariate Shift [25]. A small change in lower layers can cause a landslide effect in upper layers due to changes in the distribution of upper layer inputs. Ioffe and 215 Szegedy [25] proposed a batch normalization layer that partially alleviates the Internal Covariate Shift by normalizing/whitening data flowing between layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. It consists of 18 convolutional layers, each followed by a BN layer apart from the final classification layer; three 2×2 max-pooling layers in the "contractive" path and corresponding three 2×2 simple upsampling layers that replicate rows and columns of data in the "expanding" path; 4 skip connections between 225</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>300</head><label></label><figDesc>early stopping criteria is used: training stops when validation error does not improve for 20 epochs. If the validation error does not improve for 10 epochs, the learning rate is reduced by a factor of 0.3. During testing all possible patches are extracted from the FOV and forward propagated through the network. All experiments apart from the E-Ophtha evaluation use a network trained on 354 305 randomly selected E-Ophtha images, and evaluated on remaining 27 images, as the base model. All parameters were determined empirically based on authors experience or successful deep learning works ( [15], [16], [21]). We observe that the proposed approach is robust to changes in parameters' values. The modification of parameters barely affects the final results, however it has a moderate 310 A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>315</head><label></label><figDesc>ROC training dataset; we randomly divided this into a 25 image training set and 25 image test set, using the same split for all experiments. The base model for fine-tuning was trained on the E-Ophtha dataset as described above. Unless stated otherwise, during fine-tuning the same early stopping and training hyperparameters were used as in the case of base model training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>320</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>335</head><label></label><figDesc>culate a FROC score. As expected, freezing the final most task-specific layers results in decreased performance. We observe that by increasing the number of frozen initial layers, our model accomplishes the best performance by freezing 11 initial layers and training 7 final layers. As a result, all following experiments will use this fine-tuning scheme when transferring knowledge between datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>340 4 . 3 .</head><label>43</label><figDesc>Microaneurysm detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 presents examples of lesion detection results. The detection results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 shows examples of various challenging detections. Many detection algorithms have to extract and remove vessels first to correctly detect MAs close 380</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>400</head><label></label><figDesc>will reduce the per-image processing time by a factor close to the number of A C C E P T E D M A N U S C R I P T used devices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>405Figure 2 :</head><label>2</label><figDesc>Figure 2: CNN Architecture. Each block provides the shape of its output. Solid line blocks consists of a convolutional and batch normalization layers. Dashed line blocks correspond to pooling layers. Dotted line blocks represent upsampling layers. The final grey block is the final convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example image from E-Ophtha dataset. From left to right: original image; preprocessed image.</figDesc><graphic coords="28,118.02,196.53,408.92,163.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: FROC curves produced by the proposed method. (a) E-Ophtha; (b) DIARETDB1; (c) ROC Training.</figDesc><graphic coords="28,118.02,497.13,403.08,102.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="26,189.86,205.33,190.65,399.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="27,175.27,253.43,219.83,262.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="30,193.96,151.23,182.44,480.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Training data.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Nr of training images Nr training patches</cell></row><row><cell>ROC</cell><cell>50</cell><cell>72 481</cell></row><row><cell>DIARETDB1</cell><cell>28</cell><cell>40 549</cell></row><row><cell>E-Ophtha</cell><cell>381</cell><cell>552 451</cell></row><row><cell cols="3">Table 1 shows the amount of training images and patches used for experi-</cell></row><row><cell cols="3">ments. 10% of the training samples are held back as a validation set and an</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison od fine-tuning schemes.</figDesc><table><row><cell cols="4">Fine-tuning scheme % trainable parameters Test Dice FROC score</cell></row><row><cell>No fine-tuning</cell><cell>100</cell><cell>0.0376</cell><cell>0</cell></row><row><cell>Full fine-tuning</cell><cell>100</cell><cell>0.0271</cell><cell>0.139</cell></row><row><cell>Freeze 3</cell><cell>98.44</cell><cell>0.0616</cell><cell>0.195</cell></row><row><cell>Freeze 5</cell><cell>94.12</cell><cell>0.0715</cell><cell>0.215</cell></row><row><cell>Freeze 5+BN</cell><cell>94.10</cell><cell>0.0257</cell><cell>0.152</cell></row><row><cell>Freeze 8</cell><cell>73.96</cell><cell>0.0970</cell><cell>0.218</cell></row><row><cell>Freeze 8+BN</cell><cell>73.88</cell><cell>0.0255</cell><cell>0.154</cell></row><row><cell>Freeze 11</cell><cell>39.40</cell><cell>0.1030</cell><cell>0.233</cell></row><row><cell>Freeze 14</cell><cell>4.85</cell><cell>0.1060</cell><cell>0</cell></row><row><cell>Freeze 16</cell><cell>1.24</cell><cell>0.0981</cell><cell>0.109</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The sensitivies at various FPIs using ROC training dataset.</figDesc><table><row><cell>Method</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell><cell>20</cell><cell>Score</cell></row><row><cell>Zhou et al. [33]</cell><cell cols="8">0.135 0.155 0.232 0.288 0.325 0.370 0.420 0.275±0.099</cell></row><row><cell>Javidi et al. [8]</cell><cell cols="8">0.130 0.147 0.209 0.287 0.319 0.353 0.383 0.261±0.093</cell></row><row><cell>Zhang et al. [11]</cell><cell cols="8">0.127 0.150 0.197 0.289 0.31 0.316 0.330 0.246±0.079</cell></row><row><cell cols="9">Niemeijer et al. [29] 0.072 0.087 0.101 0.121 0.130 0.185 0.210 0.129±0.047</cell></row><row><cell>Freeze All</cell><cell cols="8">0.090 0.108 0.128 0.139 0.156 0.163 0.177 0.137±0.029</cell></row><row><cell cols="9">Proposed Method 0.174 0.243 0.306 0.385 0.431 0.461 0.485 0.355±0.109</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The sensitivies at low FPIs using ROC training dataset. Table 3 presents a performance comparison between the proposed method and state-of-the-art methods using the ROC training dataset. The Freeze All method corresponds to a FCNN without any fine-tuning. Compared to other techniques, the proposed algorithm achieves the highest average FROC score of</figDesc><table><row><cell>Method</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>Score</cell></row><row><cell>Freeze All</cell><cell cols="8">0.028 0.040 0.063 0.090 0.108 0.128 0.139 0.085±0.040</cell></row><row><cell cols="9">Proposed Method 0.039 0.067 0.141 0.174 0.243 0.306 0.385 0.193±0.116</cell></row></table><note><p><p><p><p><p>345 0.355. Most importantly, it provides much better performance for low FPIs. For comparison purposes, we present the sensitivites at seven high FPIs. Nonetheless, similarly to</p><ref type="bibr" target="#b19">[29]</ref> </p>we think that sensitivity values at FPI higher than 1.08 are of little clinical importance. Consequently, we provide the performance metrics for much lower FPI in Table</p>4</p>. 350</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>shows a comparison of MA detection methods using the DIARETDB1</cell></row><row><cell></cell><cell>dataset. Consistently with ROC results, the proposed algorithm produces the</cell></row><row><cell></cell><cell>highest average score of 0.392. Furthermore, the sensitivities for all FPIs are</cell></row><row><cell></cell><cell>higher than provided by other methods. To transfer knowledge from the base</cell></row><row><cell>355</cell><cell>model to models used with ROC and DIARETDB1 datasets, 11 initial layers</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was made possible by a Marie Curie grant from the Euro-420 pean Commission in the framework of the REVAMMAD ITN (Initial Training Research network), Project number 316990.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T vides better results than Freeze All method. In our case, the null and alternative hypotheses can be defined as H 0 : M P = M F and H 1 : M P &gt; M F , where M P and M F are medians of sensitivity values produced by the proposed method and Freeze All method respectively. Following common practice, we set the significance level at 0.05. Wilcoxon signed rank tests show statistically signifi-370 cant improvement in the sensitivity values when using the proposed approach (p 0.05).  [13] M. Haloi, Improved microaneurysm detection using deep neural networks, arXiv preprint arXiv:1505.04424.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Diabetic retinopathy</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page" from="124" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Current epidemiology of diabetic retinopathy and diabetic macular edema</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current diabetes reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="346" to="354" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global prevalence and major risk factors of diabetic retinopathy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kawasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lamoureux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grauslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes care</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="556" to="564" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The prevalence of and factors associated with diabetic retinopathy in the australian population</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>De Courten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Balkau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mccarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Welborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Z</forename><surname>Zimmet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes care</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1731" to="1737" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global estimates of diabetes prevalence for 2013 and projections for 2035</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guariguata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hambleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beagley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Linnenkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes research and clinical practice</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="149" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic detection of microaneurysms in diabetic fluorescein angiography</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baudoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Revue d&apos;épidémiologie et de santé publique</title>
		<imprint>
			<biblScope unit="volume">445</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="254" to="261" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vessel segmentation and microaneurysm detection using discriminative dictionary learning and sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-R</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teleophta: Machine learning and image processing methods for teleophthalmology</title>
		<author>
			<persName><forename type="first">E</forename><surname>Decencière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cazuguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thibault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Danno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRBM</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="203" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
		<title level="m">A threshold selection method from gray-level histograms, IEEE 485 transactions on systems, man, and cybernetics</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Workshop on Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">495</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="8609" to="8613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv 505 preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for medical image analysis: full training or fine tuning?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1312" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retinopathy online challenge: automatic detection of microaneurysms in digital color fundus photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mizutani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hornero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Muramatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="195" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The diaretdb1 diabetic retinopathy database and evaluation protocol</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kauppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalesnykiene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lensu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sorri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raninen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Voutilainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uusitalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kälviäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pietilä</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>BMVC</publisher>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
	</analytic>
	<monogr>
		<title level="j">Keras</title>
		<imprint>
			<biblScope unit="page">520</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic microaneurysm de-525 tection using the sparse principal component analysis-based unsupervised classification method</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2563" to="2572" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Red lesion detec-530 tion using dynamic shape features for diabetic retinopathy screening</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hurtut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chelbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Langlois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1116" to="1126" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An ensemble-based system for microaneurysm detection and diabetic retinopathy grading</title>
		<author>
			<persName><forename type="first">B</forename><surname>Antal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hajdu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1720" to="1726" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated detection of microaneurysms using scale-adapted blob analysis and semi-supervised learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Adal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sidibé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Karnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mériaudeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
