<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal Multimode Background Subtraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hasan</forename><surname>Sajid</surname></persName>
							<email>hasan.sajid@smme.nust.edu.pk.sen-chings.cheung</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Sen-Ching</forename><surname>Samson Cheung</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Kentucky. He is currently with National University of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Kentucky in Lexington</orgName>
								<address>
									<region>KY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Universal Multimode Background Subtraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4889369EF7D4416D372D2FBBE81F3735</idno>
					<idno type="DOI">10.1109/TIP.2017.2695882</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2695882, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer vision</term>
					<term>change detection</term>
					<term>background model bank</term>
					<term>background subtraction</term>
					<term>color spaces</term>
					<term>binary classifiers</term>
					<term>foreground segmentation</term>
					<term>pixel classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a complete change detection system named Multimode Background Subtraction (MBS). The universal nature of system allows it to robustly handle multitude of challenges associated with video change detection such as illumination changes, dynamic background, camera jitter, moving camera etc. The system comprises of multiple innovative mechanisms in background modeling, model update, pixel classification and the use of multiple color spaces. The system first creates multiple background models of the scene followed by an initial foreground/background probability estimation for each pixel. Next, the image pixels are merged together to form mega-pixels, which are used to spatially denoise the initial probability estimates to generate binary masks for both RGB and YCbCr color spaces. The masks generated after processing these input images are then combined to separate foreground pixels from the background. Comprehensive evaluation of the proposed approach on publicly available test sequences from the CDnet and the ESI datasets show superiority in performance of our system over other state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Video change detection or Background Subtraction (BS) is one of the most widely studied topics in computer vision. It is a basic pre-processing step in video processing and therefore has numerous applications including video surveillance, traffic monitoring, human detection, gesture recognition, etc. Typically, a BS process produces a foreground (FG) binary mask given an input image and a background (BG) model.</p><p>BS is a difficult problem because of the diversity in background scenes and the changes originated from the camera itself. Scene variations can be in many forms such as, to name just a few, dynamic background, illumination changes, intermittent object motion, shadows, highlights, camouflage as well as a multitude of environmental conditions like rain, snow, and change in sunlight <ref type="bibr" target="#b0">[1]</ref>. Likewise, the changes linked to camera can be due to auto-iris, camera jitter, sensor noise and pan-tilt-zoom. Existing state-of-the-art techniques can address only a subset of these challenges and most of them are sensitive to illumination changes, camera/background motion and environmental conditions <ref type="bibr" target="#b1">[2]</ref> [3]. No single technique exists that is able to simultaneously handle all key challenges and produce satisfactory results.</p><p>In this paper, we propose a BS system that is robust against various challenges associated with real world videos. The proposed approach uses a Background Model Bank (BMB) that comprises of multiple Background (BG) models of the scene. To separate foreground pixels from changing background pixels caused by scene variations or camera itself, we apply Mega-Pixel (MP) based spatial denoising to pixel level probability estimates on different color spaces to obtain multiple Foreground (FG) masks. They are then combined to produce a final output FG mask. The major contribution of this paper is a universal background subtraction system called Multimode Background Subtraction (MBS) with following major innovations: Background Model Bank (BMB), model update mechanism, MP-based spatial denoising of pixel-based probability estimates, fusion of multiple binary masks, and use of multiple color spaces for BS process. Preliminary results of using our system to handle illumination changes and camera movements were presented in <ref type="bibr" target="#b2">[4]</ref> and <ref type="bibr" target="#b3">[5]</ref> respectively. Improvements upon these prior works include:</p><p> a detailed analysis of the fusion of appropriate color spaces for BS,  a novel model update mechanism, and  a novel MP-based spatial denoising and a dynamic model selection scheme that significantly reduces the number of parameters and improve computational speed. BS is well-researched topics in computer vision, therefore, we demonstrate the performance of MBS by providing a comprehensive comparison with 15 other state-of-the-art BS algorithms on a set of publicly-available challenging sequences across 12 different categories, totalling to 56 video sets. To avoid bias in our evaluations, we have adopted the same sets of metrics as recommended by the CDnet 2014 <ref type="bibr" target="#b1">[2]</ref>. The extensive evaluation of our system demonstrates better foreground segmentation and superiority of our system in comparison with existing state-of-the-art approaches.</p><p>The rest of paper is organized as follows. Relevant work is discussed in Section II. We present and discuss our contributions in Section III and overall system in section IV, followed by experiments and result comparison in Section V and conclude the paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There are a plethora of BS techniques, many of which reviewed in surveys like <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr" target="#b5">[7]</ref> and <ref type="bibr" target="#b6">[8]</ref>. We can broadly divide these into four categories: pixel-based, region-based, frame-based and learning based <ref type="bibr" target="#b7">[9]</ref>.</p><p>Pixel-based algorithms form a pixel-wise statistical model of the scene. The algorithms in this category are based on simple statistics from mean, mode, running average to complex multimodal distributions <ref type="bibr" target="#b4">[6]</ref>  <ref type="bibr" target="#b5">[7]</ref>. Although methods relying on simple statistics like unimodal Gaussian methods are very fast and computationally inexpensive, they produce relatively poor segmentation results due to the limited capacity in modelling real world changes such as camera noise, moving background, camera jitter, sudden illumination changes etc. The most popular multimodal techniques in pixel based category are pixel-wise Gaussian Mixture Model (GMM) <ref type="bibr" target="#b8">[10]</ref> and Kernel Density Estimates (KDE) <ref type="bibr" target="#b9">[11]</ref>.</p><p>The GMM based techniques model the per-pixel distribution of values observed overtime with a mixture of Gaussians. The multimodal nature of these techniques allow them to cope with dynamic background. GMM has been widely used for different BS systems and various improved versions have been proposed. For example in <ref type="bibr" target="#b10">[12]</ref>, the authors take advantage of color and texture invariance and combine them with GMM algorithm resulting in a more robust algorithm. However, the improvement has proved to be computationally expensive and unsuitable for real time operation. In <ref type="bibr" target="#b11">[13]</ref>, instead of fixing the number of components for each pixel authors estimate the appropriate number of components for each pixel dynamically and thus it overcomes the problem of choosing right number of components for each pixel. In <ref type="bibr" target="#b12">[14]</ref>, the authors further combined motion with pixellevel GMM appearance models. Other improvements in GMM-based techniques are summarized in <ref type="bibr" target="#b6">[8]</ref>.</p><p>Another popular algorithm in this category are based on KDE such as <ref type="bibr" target="#b9">[11]</ref> and <ref type="bibr" target="#b13">[15]</ref>. For each pixel, these methods accumulate values from pixel's recent history and then estimate the probability distribution of the background values. The distribution is then used to classify whether a pixel belongs to foreground or background. The kernel density estimator helps to overcome two problems inherent in GMM based models; (a) choice of suitable shape for pixel probability distribution function and, (b) constant need for parameter estimation.</p><p>Sample consensus is another non-parametric method that relies on recently observed pixels to determine if the new incoming pixel is a FG or BG. SuBSENSE is an example of sample consensus methods that uses pixel-level feedback loop mechanism to continuously update and maintain the pixel's model <ref type="bibr" target="#b14">[16]</ref>. A spatiotemporal feature descriptor is also used for increased sensitivity, which however entails high computational costs.</p><p>Codebook is another class of techniques that has been reported in <ref type="bibr" target="#b15">[17]</ref>  <ref type="bibr" target="#b16">[18]</ref>. It comprises of a codebook for each pixel which is a compressed form of background. Each codebook has multiple codewords that are based on a sequence of training images using a color distortion metric. Incoming pixels are matched against all background codewords for classification.</p><p>Regardless of the choice of statistical models, pixel-based algorithms in general suffer from a lack of inter-pixel spatial dependencies and the constant need of updating the distribution parameters or model. However, it is difficult to determine an appropriate update rate to differentiate true foreground from drastic background changes such as those caused by sudden variation in illumination or fast moving object.</p><p>The second class of techniques are region-based techniques. Unlike their pixel-based counterparts, region-based techniques exploit local spatial relationships among pixels. In <ref type="bibr" target="#b13">[15]</ref>, the authors enforce spatial context among pixels by incorporating pixel locations into their background and foreground KDEs using a Markov Random Field framework. Another region based method is presented in <ref type="bibr" target="#b17">[19]</ref> which uses statistical circular shift moments (SCSM) in image regions for change detection. Although these methods incorporate spatial information, their ability in handling change events at various speeds is questionable -there does not seem to be a rational approach in determining proper time interval for model update.</p><p>A different region-based approach, introduced in <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b19">[21]</ref> and <ref type="bibr" target="#b20">[22]</ref>, models spatial dependencies by considering blocks of different sizes instead of pixels individually. The basic underlying assumption is that the neighbouring pixels undergo similar variation as the pixel itself. The blocks are formed over a sequence of training images, followed by training a Principal Component Analysis (PCA) Model for each spatial block. In <ref type="bibr" target="#b19">[21]</ref>, classification is done by comparing a block in current frame to its reconstruction from PCA coefficients and declaring it as background if the reconstruction is close. In contrast to <ref type="bibr" target="#b19">[21]</ref>, <ref type="bibr" target="#b20">[22]</ref> performs classification using threshold based on difference between current image and the back projection of PCA coefficients. PCA-based techniques are more robust against noise and illumination changes in comparison to their pixel based counterparts but lack any update mechanism.</p><p>Another region based method named Multiscale Spatio-Temporal uses a three-level spatio-temporal color/luminance Gaussian pyramid BG model for each pixel <ref type="bibr" target="#b21">[23]</ref>. While it is robust against dynamic background and shadows, selecting an appropriate update rate is challenging for this method.</p><p>Frame-based methods create statistical BG models for the entire frame. Many of the frame-based techniques are based on a shading model, which calculates the ratio of intensities between an input image and the reference frame or BG model <ref type="bibr" target="#b7">[9]</ref>  <ref type="bibr" target="#b22">[24]</ref>. Frame-based techniques have not gained as much as popularity as pixel based approaches but are known to offer more robust solution against gradual as well as sudden illumination changes <ref type="bibr" target="#b6">[8]</ref>.</p><p>Based on the shading model, Pilet et al. <ref type="bibr" target="#b23">[25]</ref> propose a Statistical Illumination (SI) model that uses GMM to model the distribution of the ratio of intensities. In this method, spatial dependence is incorporated in the framework by learning a spatial-likelihood model. Although this technique is robust against global illumination changes, it is not able to handle local illumination changes <ref type="bibr" target="#b7">[9]</ref>.</p><p>Eigen Background (EB) is a frame-based method that builds an Eigen space over expected illumination changes and reconstructs the BG image by projecting an input image on the learned Eigen space <ref type="bibr" target="#b24">[26]</ref>. The performance of EB strongly depends on an ad-hoc threshold and whether the global and local illumination changes can be well represented by a linear combination of background scenes in training set.</p><p>Vosters et al. present an improved frame-based technique by combining both EB and SI models in <ref type="bibr" target="#b7">[9]</ref> at the expense of higher computational cost. EB reconstructs the BG image and then SI model segments the image into FG and BG regions. The authors also improve SI by introducing an online instead of an offline spatial-likelihood model.</p><p>Another frame-based technique is Tonal Alignment (TA) <ref type="bibr" target="#b25">[27]</ref>. For an input image, it first uses the change detection algorithm in <ref type="bibr" target="#b26">[28]</ref> to extract out BG pixels, subset of which are then used for histogram specification transform computation. This transformation tonally aligns the input and background image. FG segmentation is done by pixel-wise comparison between the input and the tonally-aligned background image. TA is able to handle global illumination changes but also fails to deal with local lighting changes. Apart from these, there exist methods such as those in <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33]</ref> that take advantage of illumination invariant features such as texture with edge or color. However, they suffer from the possible absence of texture in certain areas of image or poor color discrimination in low lighting conditions.</p><p>The fourth class of methods apply traditional machine learning on different features to build the BG model. For example, in <ref type="bibr" target="#b27">[29]</ref>, the authors combine Haar, color, and gradient features for each pixel in a kernel density framework, and apply SVM for segmentation. Neural network based approaches have also gained popularity in recent years. SC_SOBS <ref type="bibr" target="#b28">[30]</ref> models the BG with weights of a neural network, whereas a weightless neural network named CwisarDH is proposed in <ref type="bibr" target="#b29">[31]</ref>. It buffers previous FG values to robustly handle intermittent objects. The dependence on training data with positive and negative labels makes these methods impractical for real world deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSTEM INNOVATIONS</head><p>Background Subtraction can be summarized as a five-step process: pre-processing, background modelling, foreground detection, data validation and model update. Pre-processing involves simple image processing on input video such as format conversion and image resizing for subsequent steps. Background modelling is responsible for constructing a statistical model of the scene, followed by pixel classification in the foreground detection step. In the data validation step, falsely-detected foreground pixels are removed to form the final foreground mask <ref type="bibr" target="#b4">[6]</ref>. The final step is to update the model if necessary.</p><p>Our innovations primarily fall in the use of multiple color spaces, background model bank for background modelling process, MP formation and label correction for foreground detection, and a novel model update procedure. In the following sub-sections, we detail each of these innovations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multiple Color Spaces for BS</head><p>The choice of color space is critical to the accuracy of foreground segmentation. Many different color spaces including RGB, YCbCr, HSV, HSI, lab2000, normalized-RGB (rgb) have been used for background subtraction. Among these color spaces, we focus on the four most widelyused color spaces: RGB, YCbCr, HSV and HSI <ref type="bibr" target="#b32">[34]</ref>  <ref type="bibr" target="#b33">[35]</ref>.</p><p>RGB is a popular choice for a number of reasons: (a) the brightness and color information are equally distributed in all three color channels; (b) it is robust against both environmental and camera noise <ref type="bibr" target="#b32">[34]</ref>; (c) it is the output format of most cameras and its direct usage in BS avoids the computation cost of color conversion <ref type="bibr" target="#b33">[35]</ref>.</p><p>The use of the three other color spaces: YCbCr, HSV and HSI are motivated by human visual system (HVS). The defining color perception in HVS is that it tends to assign a constant color to an object even under changing illumination over time or space <ref type="bibr" target="#b32">[34]</ref>  <ref type="bibr" target="#b34">[36]</ref>. These color spaces segregate the brightness and color information, with YCbCr on Cartesian coordinates whereas HSV and HSI on polar coordinates. While the color constancy makes the BS process more robust against shadow, highlights and illumination changes, the foreground detection is less discriminatory if brightness information is not used <ref type="bibr" target="#b32">[34]</ref>[36][37] <ref type="bibr" target="#b36">[38]</ref>.</p><p>In comparative studies on color spaces <ref type="bibr" target="#b32">[34]</ref>[35] <ref type="bibr" target="#b37">[39]</ref>[37], YCbCr has been shown to outperform RGB, HSI and HSV color spaces and is considered to be the most suitable color space for foreground segmentation <ref type="bibr" target="#b32">[34]</ref>[37] <ref type="bibr" target="#b33">[35]</ref>. Due to its independent color channels, YCbCr is the least sensitive to noise, shadow and illumination changes. RGB is ranked second with HSI and HSV at the bottom as their polar coordinate descriptions are quite prone to noise <ref type="bibr" target="#b32">[34]</ref>. The conversion from RGB to YCbCr is also computationally less expensive than to HSI or HSV.</p><p>Based on the above comparison, YCbCr is a natural choice for segmentation. However, <ref type="bibr" target="#b34">[36]</ref> and <ref type="bibr" target="#b35">[37]</ref> also identify potential problems with the YCbCr color space: when current image contains very dark pixels, the chance of misclassification increases since dark pixels are close to the origin in RGB space. The fact that all chromaticity lines in RGB space meet at the origin makes dark pixels close or similar to any chromaticity line. Such scenario does not occur only when illumination levels are low globally, but also happens when portion of the image becomes darker. This is common especially in indoor scenes with complex illumination sources and scene geometry. Shadows casted by objects is one such example. The exclusive use of YCbCr color space in such situations will result in a decrease in foreground segmentation accuracy.</p><p>Inspired by the HVS, we propose to use two color spaces: RGB and YCbCr to handle different illumination conditions. We then choose the appropriate channels for the scene in question. This is different from all existing techniques that employ all channels and only one color space. RGB and Y channels are used under poor lighting conditions since chromatic information is uniformly distributed across RGB channels and Y represents intensity only. During good lighting conditions, we also employ the color channels (Cb and Cr) of YCbCr color space to increase foreground segmentation accuracy. During intermediate lighting conditions, both RGB and YCbCr color spaces complement each other in providing a robust FG/BG classification To support our claim of using multiple color spaces, a detailed quantitative analysis is presented in section V by comparing segmentation accuracy across 12 different categories using each color space separately, two color spaces combined, and by dynamically choosing color channels.</p><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2695882, IEEE Transactions on Image Processing B. Background Modelling BG modelling is one of most important steps in a BS process and the accuracy of the model used directly impacts the segmentation results. Most BG models use a variant of multi-modal pixel-wise statistical background model. Such an approach has two problems: first, it is difficult to determine the correct number of modes for modelling the pixel probability distribution function. Second, and more importantly, inter-pixel dependencies are overlooked, which leads to poor segmentation results.</p><p>In order to model the BG, we propose Background Model Bank (BMB), which comprises of multiple BG models instead of a single BG model. To form BMB, each background training image is treated as a BG model with selected color channels stacked together as a vector. This initial set of BG models are then merged together into a number of average BG models using an iterative sequential clustering procedure. Two BG mean models (p and q in vector form) with correlation measure greater than the pre-defined parameter are merged and replaced by their average. The correlation measure is defined as <ref type="bibr" target="#b0">(1)</ref> where and are defined as:</p><p>(</p><p>This process continues in an iterative fashion unless there are no more average BG models with . The use of frame-level clustering is motivated by physical laws that govern scene geometry. Typically real-life scenes comprise of different types of objects. The variety in configurations and interactions between different types of matter and objects generate very intricate and infinite scene geometry. Examples include variations caused by illumination changes, dynamic changes, camera shaking, camera movement etc. This diversity makes it difficult to accurately capture and model the scene. The use of multiple BG models allows us to capture scene more accurately while keeping spatial dependencies intact.</p><p>Another advantage of BMB is that it is computationally simpler than other multi-mode approaches -as we will demonstrate, we choose a model at frame level and ignore the rest of the BG models in the BMB. While there is an additional cost on choosing the model at frame level, it incurs minimal cost because of simple comparison with average BG models than those that rely on pixel-based multi-mode distributions.</p><p>As our experimental results in Section V will demonstrate, our multiple BG models can capture scene diversity and camera variations accurately. Comparing to more complex multi-modal or non-parametric techniques, our model obtain equal or better results using only simple binary classifier for pixel classification, resulting in efficient implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Binary Classification</head><p>In this sub-section, we discuss the binary mask generation for each of the selected color channels. It is a four step process: color channel activation/deactivation, pixel-level probability estimation, MP formation and average probability estimation. Fig. <ref type="figure" target="#fig_0">1</ref> depicts the binary mask generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Color-channels Activation/Deactivation</head><p>This step is responsible to activate/deactivate the color channels Cb and Cr. Both color channels are used if the mean intensity of input image is greater than empirically determined parameter channel_th, which otherwise are not employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Pixel-level Probability Estimation</head><p>Pixel-wise error, is calculated between each color channel from both RGB and YCbCr spaces and the chosen BG model as follows.</p><p>(3) where D denotes the color channel in question, is the input image, and is the chosen average BG model.</p><p>Once we have calculated the error for each individual pixel, we estimate an initial probability for each pixel by passing them through a sigmoid function.</p><p>(4)</p><p>The rationale behind this conversion is that the higher the error the more likely that the pixel belongs to the FG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Mega-Pixel Formation</head><p>The primarily goal of this step is to introduce spatial denoising by considering the initial probability estimates and color information of the neighbourhood pixels under the framework of Super-Pixels (SP) <ref type="bibr" target="#b39">[41]</ref>.</p><p>SPs offers advantage in terms of capturing local context and significant reduction in computational complexity. These algorithms combine neighboring pixels into one pixel based on similarity measure such as color, texture, size etc. We use the ERS algorithm in <ref type="bibr" target="#b39">[41]</ref> to segment the input frame into SPs. In <ref type="bibr" target="#b39">[41]</ref>, the SP segmentation is formulated as a graph partitioning problem. For a graph and number of SPs, the goal is to find a subset of edges to approximate a graph with at least connected sub-graphs. The clustering objective function comprises of two terms: the entropy rate of a random walk and a balancing term .</p><p>(</p><formula xml:id="formula_1">)<label>5</label></formula><p>where is the number of connected components in . A large entropy term favors compact and homogeneous clusters, whereas the balancing term encourages clusters with similar size. For more details, we refer readers to <ref type="bibr" target="#b39">[41]</ref>.</p><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. is the pixel index and Y is the total number of pixels in SP . Our implementation of DBSCAN is based on <ref type="bibr" target="#b41">[43]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> depicts the overall MP formation process. Notice the road SPs correctly merged as a single MP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Average Probability Estimation and Labelling</head><p>The next step is to compute the average probability of a MP , denoted as with a total of pixels: <ref type="bibr" target="#b6">(8)</ref> where is the pixel index and is the initial FG/BG probability estimate of each pixel. The is then assigned to each pixel belonging to that MP. Finally, to obtain Binary Mask for each color channel D, the average probability measure is thresholded using an empirically determined parameter prob_th.</p><p>The use of MP and its respective allow us to assign the same probability to each pixel belonging to the same object and therefore increases the segmentation accuracy. For example, all the pixels belonging to the road in Fig. <ref type="figure" target="#fig_1">2</ref> should be BG. Clearly, in Fig. <ref type="figure" target="#fig_1">2</ref>, as we move from left to right, road pixels with erroneous probability estimates would be averaged out using neighbouring pixels via SPs or MP, thereby improving the segmentation accuracy. As MPs respect edge integrity, the average probability of a MP represents the same object or part rather than using FG/BG probability estimates for each individual pixel or SPs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Update</head><p>This section explains model update mechanism of the proposed system. Model update is an essential component of an algorithm to deal with scene changes that take place with the passage of time. The classic approach for model update is to replace old values in the model with new ones after a number of frames or time period. Such updating mechanisms can be problematic since the update rate is difficult to determine. For example, a person sitting idly in a scene may become a part of background if update rate is too fast. Another 1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2695882, IEEE Transactions on Image Processing scenario could be of a forgotten luggage, in which question arises as when should it become a part of background or should it ever become a part of background? An update mechanism should be able to address two questions. First, is there a need for model update at all? Second, what is the appropriate update rate? We argue that rate of change in number of FG pixels can serve as a good measure to trigger model update and to determine an appropriate update rate. In a typical surveillance scene, the number of FG pixels fluctuates in a relatively narrow range and a significant change can serves as a trigger for departure from the old BG model:</p><p>where th is an empirically-determined parameter that signifies a significant enough change for model update. The rateOfChange is calculated based on the deviation of the number of FG pixels in current frame from the running mean. Formally, we define it as: <ref type="bibr" target="#b7">(9)</ref> where is the output binary mask of current input image at time t.</p><p>Once model update mechanism is triggered and rateOfChange is calculated, an update rate function f is used to map rate of change to determine an appropriate update rate and defined as: (10) In order to understand the need for an update rate function f, we must first understand how and what type of changes can occur in a scene. Changes in BG can occur at different rates from slow to abrupt. The gradual illumination change in daylight from sunrise to sunset is a good example of a slowly changing BG and requires a slow update rate. Whereas on the contrary, there can be abrupt changes such as caused by sudden illumination changes in indoor environments or due to a moving camera. Situations such as these require a fast update rate. Failure to determine an appropriate update rate can result in too many false positives. Hence it is necessary for the algorithm to be able to dynamically determine appropriate update rate for changing BG.</p><p>There are different options for choosing an update rate function f ranging from simple linear to complex functions. Two candidates are a linear function or an exponential function based on the simplicity of parameters and their effectiveness. A linear function provides a straightforward direct relationship between the model update rate and the rate of change. Exponential function can be used when a more aggressive response i.e. higher update rate is desired for any small change in BG. Such function may be more suitable for coping sudden illumination changes and PTZ camera movements. In our experiments, we have used a simple linear function: <ref type="bibr" target="#b9">(11)</ref> where m is the slope and can be set by the user to any value between zero to one. For example with m set to 0.75 and a rate of change of 1, the calculated update rate would be 0.75, i.e. less weightage is given to old BG model and current frame is given more weightage in updating the BG model. After determining the update rate, the models are then updated as follows:</p><p>(12) where represents current input frame at time t and is the chosen BG model for current frame and is being updated.</p><p>The dynamic model update mechanism allows to cater for various scenarios in which conventional approaches fail. For example, no model update will be applied when there is no FG in the scene or FG is not changing as the rate of change is close to zero. Lastly, whenever there is a change in BG, it is able to dynamically determine update rate and then update BG model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SYSTEM INTEGRATION</head><p>In this section, we describe how individual components are combined in our system. The proposed system consists of five steps as shown in Fig. <ref type="figure">3</ref>. Each step is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step 1: BG Model Selection</head><p>The first step is to select an appropriate BG Model for the incoming frame. The selection criterion is based on identifying the BG model in BMB that maximizes the correlation with input image : <ref type="bibr" target="#b11">(13)</ref> where, and are vector forms of and respectively. and are defined as: <ref type="bibr" target="#b12">(14)</ref> Step 2: Binary Mask (BM) Generation</p><p>In this step, the input image and the selected BG model are first used to estimate an initial probability estimate for each pixel. The input image is simultaneously passed to the MP module, which segments the image in arbitrary number of MPs. Average probability estimates are calculated for each MP using pixel-level probability estimates and then thresholded to generate Binary Mask(BM) for each color channel. We denote the BM for color channel as . The BM generation is discussed in detail in section III.C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step 3: Binary Masks Aggregation/Fusion</head><p>The BMs are then used to form Foreground Detection (FGD) masks for RGB and YCbCr color spaces: <ref type="bibr" target="#b13">(15)</ref> For YCbCr color space, if Cb and Cr channels are deactivated then will be reduced to the Y channel BM alone. Finally the two FGD masks are combined by taking logical AND between dilated versions of the two to obtain the actual FGD mask: <ref type="bibr" target="#b14">(16)</ref> 1057 The dilated versions are to ensure that all true foreground pixels are captured in the FGD mask.</p><p>Step</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4: Binary Masks Purging</head><p>The FGD mask is then applied to each of the BMs obtained in step 3. This removes all of the falsely detected foreground regions and increases our confidence in classifying FG and BG pixels in the final step. The resulting component masks are defined as follows: <ref type="bibr" target="#b15">(17)</ref> Figure <ref type="figure">3</ref>. Universal Multimode Subtraction System.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step 5: Foreground Mask</head><p>In the final step of the process, FG mask is obtained by the logical OR of all the masks.</p><p>V. EXPERIMENTS AND RESULTS In this section, we compare the proposed system with state of the art algorithms on publicly available test sequences. Two datasets are included; CDnet 2014 <ref type="bibr" target="#b1">[2]</ref> and ESI <ref type="bibr" target="#b42">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CDnet 2014 Dataset</head><p>The CDnet 2014 dataset <ref type="bibr" target="#b1">[2]</ref> is one of the most comprehensive datasets available for evaluating BS </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Evaluation Metrics</head><p>The authors of <ref type="bibr" target="#b1">[2]</ref> use the seven evaluation metrics:</p><p>1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>7.</p><p>An additional metric has been introduced by authors of <ref type="bibr" target="#b1">[2]</ref> for Shadow (SHD) category. This metric determines False Positive Rate in hard-shadow areas (FPR-S). Finally, in order to compare the state of the art algorithms, the authors combine these metrics into an overall average rank (R) and average rank across categories (RC) metrics. For details of these metrics, the readers are referred to <ref type="bibr" target="#b1">[2]</ref>.</p><p>In our evaluation, we primarily use F-Measure (FM) for overall and category-wise comparison purposes for a number of reasons. First, the authors of <ref type="bibr" target="#b1">[2]</ref> indicate strong correlation of FM with ranks on CDnet website and in general is considered as a good indicator for comparison purposes. Second, in <ref type="bibr" target="#b14">[16]</ref>, the authors identifies potential biasness towards "precise" method -change detection is an unbalanced classification problem as there are more BG pixels in comparison to FG pixels. As a result, PWC metric would therefore favour "precise" methods. Furthermore, the ranking relies on two reciprocal metrics, i.e. FPR and Sp, and hence it will favour "precise" method. Third, nonlinearity of overall ranks substantially affect how top methods are ranked and therefore is not a reliable indicator for comparing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Parameter Setting</head><p>One set of parameters are used for the entire dataset: corr_th=0.99, prob_th=0.75, M=300, colorthreshold=3, channel_th=100, th=0.15 and m=0.5. The parameter setting is based on the set that yields overall best results across all categories. For details of parameter used by other techniques, we refer readers to the website at <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Quantitative Evaluation</head><p>In this section, we compare our proposed MBS system with 15 state of the art algorithms: Flux Tensor with Split Gaussian models(FTSG) <ref type="bibr" target="#b12">[14]</ref>, suBSENSE <ref type="bibr" target="#b14">[16]</ref>, CwisarDH <ref type="bibr" target="#b29">[31]</ref>, Spectral-360 <ref type="bibr" target="#b43">[45]</ref>, Bin Wang Apr 2014 <ref type="bibr" target="#b44">[46]</ref>, KNN <ref type="bibr" target="#b11">[13]</ref>, SC_SOBS <ref type="bibr" target="#b28">[30]</ref>, Region-based Mixture of Gaussians (RMoG) <ref type="bibr" target="#b45">[47]</ref>, KDE -ElGammal <ref type="bibr" target="#b9">[11]</ref>, SOBS_CF <ref type="bibr" target="#b46">[48]</ref>, Mahalanobis distance <ref type="bibr" target="#b47">[49]</ref>, GMM-Stauffer &amp; Grimson <ref type="bibr" target="#b8">[10]</ref>, GMM-Zivkovic <ref type="bibr" target="#b48">[50]</ref>, Multiscale Spatio-Temporal BG Model <ref type="bibr" target="#b21">[23]</ref> and Euclidean distance <ref type="bibr" target="#b47">[49]</ref>. Furthermore, we provide three additional configurations of MBS using RGB color space alone, YCbCr color space alone and RGB and YCbCr combined. These are denoted by MBS-RGB, MBS-YCbCr and MBS-Both respectively. The F-Measure based overall and category-wise comparisons for these configurations are presented in Table <ref type="table" target="#tab_3">I</ref>. These additional comparisons serve two purposes: (a) to quantitatively analyse the robustness that is offered by selecting appropriate color spaces and channels in comparison with using a single or combination of color spaces for every scene/test sequence, and (b) to analyse strength and weaknesses of color spaces in different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Discussions</head><p>We first analyse the performance of different MBS configurations namely MBS-RGB, MBS-YCbCr, MBS-Both and MBS (i.e. when appropriate color space and channels are selected). As depicted in Table <ref type="table" target="#tab_3">I</ref>, MBS not only has the highest overall F-Measure but it outperforms other configurations in individual categories as well. This supports the claim that use of appropriate color space and channels are critical for segmentation accuracy. The second best configuration is MBS-Both, which combines the strength of both color spaces. Among the two remaining, YCbCr performs marginally better than RGB. MBS-YCbCr is the most robust in handling shadows with the least FPR-S rate of 0.397, while MBS-RGB has the worst performance with the highest FPR-S of 0.591.</p><p>As for their performances on each category, NV and BW categories are affected by low lighting conditions and potentially has poor color discrimination problem. There are two important observations; (a) as shown in Table <ref type="table" target="#tab_3">I</ref>, F-Measure of MBS-RGB is significantly higher than MBS-YCbCr for both NV and BW categories and, (b) as per Table <ref type="table" target="#tab_3">I</ref>, in NV category, using all channels deteriorates the segmentation accuracy because of poor color discrimination in Cb and Cr channels. On the other hand, significant improvement is achieved when the chroma channels are automatically deactivated. This substantiates our earlier claim that RGB and Y channels are more robust under low lighting conditions or when color discrimination is poor. The BW category has considerably better illumination conditions, the Cb and Cr channels are retained and the use of all channels produce higher segmentation accuracy than using RGB and YCbCr color spaces alone. In all of the categories except NV, TB and TH (where there is zero color information or poor discrimination), the segmentation results are improved with added advantage of color information.</p><p>Next, we compare MBS against other state of the art algorithms. The following seven key points are observed from our comprehensive evaluation and results on different categories.</p><p>1. In six out of eleven categories, the proposed system is among top 3 with 1 st position in two of them. In DB, BL and SHD categories, MBS is not among top 3 but achieves acceptable results with approximately 80% F-Measure (FM). According to <ref type="bibr" target="#b14">[16]</ref>, FM ≥ 80% is considered an acceptable result. 2. In six out of eleven categories, the proposed system is among top 3 with 1 st position in two of them. In DB, BL and SHD categories, MBS is not among top 3 but achieves acceptable results with approximately 80% F-Measure (FM). According to <ref type="bibr" target="#b14">[16]</ref>, FM ≥ 80% is considered an acceptable result. 3. In the NV category, we are ranked at 2 nd position with FM of 0.534. Like other top methods, the performance is affected by halos and reflections caused by strong headlights and low visibility. 4. In the LFR category, MBS has FM of 0.618, which is slightly less than FM of 0.644 of top performing method. The result is poor for all methods in this category. It is important to mention that MBS performs poorly in only one of four test sequences in LFR category. This particular test sequence 'port_0_17fps' is recorded at 0.17fps with wavering lighting conditions and intense dynamic behavior of water and boats causing the overall FM to drop down. 5. It is important to note the marked difference in performance of our algorithm against others in the two moving camera categories; PTZ and CJ. Most of the existing state of the art fail due to static camera assumption, whereas the frame level BG modeling and MP spatial denoising approach of proposed system allows it to handle both static and moving camera video sequences and thus make our system universal. We have FM of 87.4% for CJ and 60.9% for PTZ. Although our FM is significantly higher than others, better results (&gt;80% F-Measure) could be achieved for the PTZ category if sufficient training data, especially for the "continuousPan" and "zoomInZoomout" video sequences are available. 6. IOM category involves objects being placed and removed intermittently, our innovative model update mechanism and MP based spatial denoising allows MBS to achieve FM of 76.36% and is placed at 2 nd position. The MP approach allows to handle intermittently placed objects. Consider the situation where a box is on a sofa and was learnt as a part of BG. As soon as the box is removed the pixel-level estimates would classify those pixels as FG, however all the neighboring pixels belonging to sofa will average those out and therefore will have no effect on segmentation accuracy while model can be gradually updated. None of the methods in this category is able to produce the defined acceptable FM level ≥ 80%. In BW and TH categories, MBS achieves acceptable results and is placed at 3 rd and 2 nd position respectively. Our worst performance is in TB category. In general for all categories, MBS is always placed among top 5 out of 15 methods.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ESI Dataset</head><p>Robustness of BS algorithm against sudden illumination changes is very critical to its success in real life scenarios. This is especially true for indoor environments, where sudden lighting change often occurs during door opening and closing, switching light on and off etc. CDnet 2014 dataset lacks such a category. As a result, we include the ESI dataset and, instead of comparing with general BS algorithms, we compare MBS with algorithms that specialize in dealing with this challenge. In our opinion, ESI dataset <ref type="bibr" target="#b42">[44]</ref> is the most challenging publicly available test dataset in terms of sudden illumination changes.</p><p>ESI dataset comprises of 5 test sequences; sofa, walking, chair, scene1 and scene2 <ref type="bibr" target="#b7">[9]</ref> EB dyn <ref type="bibr" target="#b46">[48]</ref> EB fix <ref type="bibr" target="#b21">[23]</ref> SI <ref type="bibr" target="#b20">[22]</ref> TA <ref type="bibr" target="#b22">[24]</ref> ABMM <ref type="bibr" target="#b47">[49]</ref> BL   <ref type="bibr" target="#b12">[14]</ref> 0.7283 SuBSENSE <ref type="bibr" target="#b14">[16]</ref> 0.7408 CwisarDH <ref type="bibr" target="#b29">[31]</ref> 0.6812 Spectral-360 <ref type="bibr" target="#b43">[45]</ref> 0.6732 Bin Wang Apr 2014 <ref type="bibr" target="#b44">[46]</ref> 0.6577 SC_SOBS <ref type="bibr" target="#b28">[30]</ref> 0.5961 KNN <ref type="bibr" target="#b11">[13]</ref> 0.5937 RMoG <ref type="bibr" target="#b45">[47]</ref> 0.5735 KDE <ref type="bibr" target="#b9">[11]</ref> 0.5688 SOBS_CF <ref type="bibr" target="#b46">[48]</ref> 0.5883 Mahalanobis <ref type="bibr" target="#b47">[49]</ref> 0.2267 GMM <ref type="bibr" target="#b8">[10]</ref> 0.5707 GMM-Zivkovic <ref type="bibr" target="#b48">[50]</ref> 0.5566 Multiscale Spatio-Temporal <ref type="bibr" target="#b21">[23]</ref> 0.5141 Euclidean distance <ref type="bibr" target="#b47">[49]</ref> 0.5161 *Red font is for best, Green for second best and Blue for third best result. and 154 frames respectively. Since the test sequences sofa, chair and walking have the same background scene/model, we combine these three into a single test sequence "House" comprising of 1689 frames. We now discuss the evaluation metrics, parameter setting for all test sequences and also present quantitative and qualitative results.</p><note type="other">CJ DB LFR IOM NV PTZ BW TH TB</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Evaluation Metrics</head><p>For quantitative evaluation of ESI dataset, we use three metrics as defined earlier; precision (Pr), Recall (Re) and Fmeasure. Precision and Recall are calculated for whole of a test sequence as arithmetic mean over all frames. Using this precision and recall, F-measure is calculated for each test sequence. Overall F-Measure is used for comparison purposes, which is simply mean FM of all test sequences in this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Parameter Setting</head><p>One set of parameters are used for the entire dataset: corr_th=0.99, prob_th=0.75, M=300, colorthreshold=3, channel_th=100, th=0.15 and m=0.5. The parameter setting is based on the set that yields the overall best results across all test sequences. Table V reports the number of training images (IMG) used for making BMB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Quantitative Evaluation</head><p>Overall as well as individual results for each test sequence are tabulated in Table <ref type="table" target="#tab_7">V</ref>. A comparison of existing state-ofthe-art techniques with MBS is depicted in Fig. <ref type="figure" target="#fig_5">6</ref>. The techniques include; Eigen background based Statistical Illumination (ESI) <ref type="bibr" target="#b7">[9]</ref>, Statistical Illumination (SI) <ref type="bibr" target="#b23">[25]</ref>, Eigen Background (EB) both dynamic and fixed <ref type="bibr" target="#b49">[52]</ref>[26], Tonal Alignment (TA) <ref type="bibr" target="#b25">[27]</ref> and Adaptive Background Mixture Model (ABMM) <ref type="bibr" target="#b50">[53]</ref>. The results for these techniques are obtained from <ref type="bibr" target="#b7">[9]</ref>.  Clearly, as shown in Fig. <ref type="figure" target="#fig_5">6</ref> MBS outperforms state of the art in handling illumination changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Qualitative Results</head><p>For qualitative results, we choose the ESI technique as benchmark for comparison purposes. Fig. <ref type="figure" target="#fig_6">5</ref> not only presents comparative results of our approach on some of example frames from scene1, scene2 and house test sequences, but also depicts the challenging nature and variation of illumination in these test sequences. Complete comparative video of all test sequences with ground truth and input images can be found at our website <ref type="bibr">[51]</ref>.</p><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Processing Speed</head><p>The proposed system is currently implemented in Matlab and run on an Intel core i5 PC with 8GB RAM. For a typical image resolution of 320 x 240, the current system in its coarse form is able to achieve ~10 fps if both color spaces i.e. all color channels are used. Approximately, 70% of processing time is consumed by SP segmentation algorithm, which is an external component. With code optimization and implementation in C++, the system is expected to meet real time requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this paper, we have presented a universal BG subtraction system that exploits multiple BG models and computationally inexpensive pixel-level comparison to generate initial probability estimates, which undergo spatial denoising by forming MPs. To separate vision tasks based on illumination conditions, we use RGB and Y color channels to for low light vision and CbCr for bright light to provide more accurate foreground segmentation. The introduction of FG dependent model update mechanism eliminates the need to tune parameters for every test sequence.</p><p>Comprehensive evaluations of the proposed system over 12 different challenging categories comprising of 56 video test sequences demonstrate the capability and flexibility of proposed system over wide variety of environmental conditions. In 10 out of 12 categories, MBS ranks among top 3 or achieve acceptable results. MBS is clearly a top performing method that outperforms state of the art especially in the moving camera categories and achieves best results for shadow suppression among top methods.</p><p>The current implementation of our algorithm is in MATLAB. Code optimization and implementation of algorithm are part of our future work. All results have been made available at official CDnet 2014 website [3].</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Binary Classification and Mask GenerationTo mitigate over-segmentation, SPs are combined to form much bigger Mega-Pixels (MPs) using DBSCAN clustering<ref type="bibr" target="#b40">[42]</ref>. DBSCAN is a density based clustering algorithms in which clusters are defined as high density areas, whereas the sparse regions are treated as outliers or borders to separate clusters. Two SPs are merged together into a MP under the following criteria:</figDesc><graphic coords="5,86.45,54.00,418.08,235.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison of segmentation with probability measure of each pixel individually (left), SP based average motion probability estimation (middle), and MP based average motion probability estimation (right).</figDesc><graphic coords="5,313.45,441.97,251.10,124.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>algorithms. It has 11 different categories: Baseline (BL), Dynamic Background (DB), Camera Jitter (CJ), Intermittent Object Motion (IOM), Shadow (SHD), Thermal (TH), Bad Weather (BW), Low Framerate (LFR), Night Videos (NV), Pan Tilt Zoom (PTZ) and Turbulence (TB). Each category has 4 to 6 videos totalling to 53 video test sequences. The authors have clearly identified training and testing data to ensure consistency for comparing state of the art algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. They have 382, 734, 573, 750</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Foreground Segmentation results of MBS on example frames from CDnet 2014 dataset. Input Image (Row 1), Ground Truth (Row 2) and MBS output(Row 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. ESI Dataset F-Measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Foreground Segmentation results of example frames from test sequence scene1 (columns 2-5), house (columns 6-9) and scene2 (column 10). Input Image (Row 1), Ground Truth (Row 2), ESI output (Row 3) and MBS output (Row 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2695882, IEEE Transactions on Image Processing</figDesc><table /><note><p>-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table II presents F-Measure based category-wise comparisons. Table IV provides the overall comparison in terms of FM overall . Table III provides the complete statistics of MBS on the CDnet 2014 dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2695882, IEEE Transactions on Image Processing</figDesc><table /><note><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I MBS</head><label>I</label><figDesc>EVALUATION WITH RGB, YCBCR AND BOTH (RGB &amp; YCBCR) COLOR SPACES ON THE CDNET 2014 DATASET</figDesc><table><row><cell>Method</cell><cell>FMoverall</cell><cell>FMBW</cell><cell>FMLFR</cell><cell>FMNV</cell><cell>FMPTZ</cell><cell>FMTB</cell><cell>FMBL</cell><cell>FMDB</cell><cell>FMCJ</cell><cell>FMIOM</cell><cell>FMSHD</cell><cell>FMTH</cell><cell>FPR-S</cell></row><row><cell>MBS-RGB</cell><cell>0.6708</cell><cell>0.771</cell><cell>0.604</cell><cell>0.473</cell><cell>0.468</cell><cell>0.445</cell><cell>0.858</cell><cell>0.707</cell><cell>0.799</cell><cell>0.735</cell><cell>0.735</cell><cell>0.778</cell><cell>0.591</cell></row><row><cell>MBS-YCbCr</cell><cell>0.6040</cell><cell>0.595</cell><cell>0.470</cell><cell>0.295</cell><cell>0.383</cell><cell>0.453</cell><cell>0.751</cell><cell>0.659</cell><cell>0.802</cell><cell>0.718</cell><cell>0.735</cell><cell>0.777</cell><cell>0.397</cell></row><row><cell>MBS-Both</cell><cell>0.7030</cell><cell>0.787</cell><cell>0.618</cell><cell>0.393</cell><cell>0.609</cell><cell>0.466</cell><cell>0.888</cell><cell>0.783</cell><cell>0.874</cell><cell>0.750</cell><cell>0.773</cell><cell>0.787</cell><cell>0.436</cell></row><row><cell>MBS</cell><cell>0.7179</cell><cell>0.787</cell><cell>0.618</cell><cell>0.534</cell><cell>0.609</cell><cell>0.466</cell><cell>0.888</cell><cell>0.783</cell><cell>0.874</cell><cell>0.763</cell><cell>0.776</cell><cell>0.789</cell><cell>0.465</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Red font is for best, Green Font for second best, and Blue font for third best results. Out of the 12 categories, MBS ranks first in 3 categories, second in 2 and third in 2. MBS is within the top five for all categories.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="7">CATEGORY-WISE COMPARISONS ON THE CDNET 2014 DATASET*</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>FMBL</cell><cell>FMDB</cell><cell>FMCJ</cell><cell cols="2">FMIOM FMSHD</cell><cell>FMLFR</cell><cell>FMNV</cell><cell>FMPTZ</cell><cell>FMBW</cell><cell>FMTB</cell><cell>FMTH</cell><cell>FPR-S</cell></row><row><cell>MBS</cell><cell>0.888</cell><cell>0.783</cell><cell>0.874</cell><cell>0.763</cell><cell>0.776</cell><cell>0.618</cell><cell>0.534</cell><cell>0.609</cell><cell>0.787</cell><cell>0.466</cell><cell>0.789</cell><cell>0.465</cell></row><row><cell>FTSG [14]</cell><cell>0.933</cell><cell>0.879</cell><cell>0.751</cell><cell>0.789</cell><cell>0.883</cell><cell>0.625</cell><cell>0.513</cell><cell>0.324</cell><cell>0.822</cell><cell>0.712</cell><cell>0.776</cell><cell>0.500</cell></row><row><cell>SuBSENSE [16]</cell><cell>0.950</cell><cell>0.817</cell><cell>0.815</cell><cell>0.656</cell><cell>0.898</cell><cell>0.644</cell><cell>0.559</cell><cell>0.347</cell><cell>0.861</cell><cell>0.779</cell><cell>0.817</cell><cell>0.599</cell></row><row><cell>CwisarDH [31]</cell><cell>0.914</cell><cell>0.827</cell><cell>0.788</cell><cell>0.575</cell><cell>0.858</cell><cell>0.640</cell><cell>0.373</cell><cell>0.321</cell><cell>0.683</cell><cell>0.722</cell><cell>0.786</cell><cell>0.554</cell></row><row><cell>Spectral-360 [45]</cell><cell>0.933</cell><cell>0.776</cell><cell>0.714</cell><cell>0.560</cell><cell>0.851</cell><cell>0.643</cell><cell>0.483</cell><cell>0.365</cell><cell>0.756</cell><cell>0.542</cell><cell>0.776</cell><cell>0.581</cell></row><row><cell>Bin Wang Apr 2014 [46]</cell><cell>0.881</cell><cell>0.843</cell><cell>0.710</cell><cell>0.721</cell><cell>0.812</cell><cell>0.468</cell><cell>0.380</cell><cell>0.134</cell><cell>0.767</cell><cell>0.754</cell><cell>0.759</cell><cell>0.465</cell></row><row><cell>SC_SOBS [30]</cell><cell>0.933</cell><cell>0.668</cell><cell>0.705</cell><cell>0.591</cell><cell>0.778</cell><cell>0.546</cell><cell>0.450</cell><cell>0.040</cell><cell>0.662</cell><cell>0.488</cell><cell>0.692</cell><cell>0.603</cell></row><row><cell>KNN [13]</cell><cell>0.841</cell><cell>0.686</cell><cell>0.689</cell><cell>0.502</cell><cell>0.746</cell><cell>0.549</cell><cell>0.420</cell><cell>0.212</cell><cell>0.758</cell><cell>0.519</cell><cell>0.604</cell><cell>0.397</cell></row><row><cell>RMoG [47]</cell><cell>0.784</cell><cell>0.735</cell><cell>0.701</cell><cell>0.543</cell><cell>0.721</cell><cell>0.531</cell><cell>0.426</cell><cell>0.247</cell><cell>0.682</cell><cell>0.457</cell><cell>0.478</cell><cell>0.309</cell></row><row><cell>KDE [11]</cell><cell>0.909</cell><cell>0.596</cell><cell>0.572</cell><cell>0.408</cell><cell>0.803</cell><cell>0.547</cell><cell>0.436</cell><cell>0.036</cell><cell>0.757</cell><cell>0.447</cell><cell>0.742</cell><cell>0.621</cell></row><row><cell>SOBS_CF [48]</cell><cell>0.929</cell><cell>0.651</cell><cell>0.715</cell><cell>0.581</cell><cell>0.772</cell><cell>0.514</cell><cell>0.448</cell><cell>0.036</cell><cell>0.637</cell><cell>0.470</cell><cell>0.714</cell><cell>0.589</cell></row><row><cell>Mahalanobis distance [49]</cell><cell>0.464</cell><cell>0.179</cell><cell>0.335</cell><cell>0.229</cell><cell>0.335</cell><cell>0.079</cell><cell>0.137</cell><cell>0.037</cell><cell>0.221</cell><cell>0.335</cell><cell>0.138</cell><cell>0.064</cell></row><row><cell>GMM [10]</cell><cell>0.824</cell><cell>0.633</cell><cell>0.596</cell><cell>0.520</cell><cell>0.737</cell><cell>0.537</cell><cell>0.409</cell><cell>0.152</cell><cell>0.738</cell><cell>0.466</cell><cell>0.662</cell><cell>0.535</cell></row><row><cell>GMM-Zivkovic [50]</cell><cell>0.838</cell><cell>0.632</cell><cell>0.567</cell><cell>0.532</cell><cell>0.732</cell><cell>0.506</cell><cell>0.396</cell><cell>0.104</cell><cell>0.740</cell><cell>0.416</cell><cell>0.654</cell><cell>0.542</cell></row><row><cell cols="2">Multiscale Spatio-Temporal [23] 0.845</cell><cell>0.595</cell><cell>0.507</cell><cell>0.449</cell><cell>0.791</cell><cell>0.336</cell><cell>0.416</cell><cell>0.036</cell><cell>0.637</cell><cell>0.529</cell><cell>0.510</cell><cell>0.528</cell></row><row><cell>Euclidean distance [49]</cell><cell>0.872</cell><cell>0.508</cell><cell>0.487</cell><cell>0.489</cell><cell>0.678</cell><cell>0.501</cell><cell>0.385</cell><cell>0.039</cell><cell>0.670</cell><cell>0.413</cell><cell>0.631</cell><cell>0.576</cell></row><row><cell>*In each column,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V</head><label>V</label><figDesc>PRECISION, RECALL AND F-MEASURE OF MBS ON ESI DATASET</figDesc><table><row><cell>Sequence</cell><cell cols="2">IMG Precision</cell><cell>Recall</cell><cell>F-Measure</cell></row><row><cell>House</cell><cell cols="2">400 0.7157</cell><cell>0.7541</cell><cell>0.7344</cell></row><row><cell>Scene1</cell><cell cols="2">500 0.8154</cell><cell>0.7236</cell><cell>0.7667</cell></row><row><cell>Scene2</cell><cell cols="2">500 0.7358</cell><cell>0.7988</cell><cell>0.7660</cell></row><row><cell>Overall</cell><cell>-</cell><cell>0.7556</cell><cell>0.7588</cell><cell>0.7557</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Part of this material is based upon work supported by the National Science Foundation under Grant No. 1237134. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wallflower: Principles and practice of background maintenance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brumitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CDnet 2014: An expanded change detection benchmark dataset</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Background subtraction under sudden illumination change</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><forename type="middle">S</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Background subtraction for static and moving camera</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><forename type="middle">S</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust techniques for background subtraction in urban traffic video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Sen-Ching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE 5308, Visual Communications and Image Processing</title>
		<meeting>SPIE 5308, Visual Communications and Image essing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation of background subtraction techniques for video surveillance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recent advanced statistical background modeling for foreground detection-a systematic survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Recent Patents on Computer Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Background subtraction under sudden illumination changes</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Vosters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gritti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for real-time tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-parametric model for background subtraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multiscale region-based motion detection and background subtraction algorithm</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D Z</forename><surname>Varcheie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sills-Lavoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient adaptive density estimation per image pixel for the task of background subtraction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zivkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Der Heijden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Static and moving object detection using flux tensor with split gaussian models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bunyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Palaniappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian modeling of dynamic scenes for object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Subsense: A universal change detection method with local adaptive sensitivity</title>
		<author>
			<persName><forename type="first">P.-L</forename><surname>St-Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Background modeling and subtraction by codebook construction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Chalidabhongse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time foreground-background segmentation using codebook model</title>
	</analytic>
	<monogr>
		<title level="j">Real-time imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical change detection with moments under time-varying illumination</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vibe: A universal background subtraction algorithm for video sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Background subtraction based on cooccurrence of image variations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding background mixture models for foreground segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schoonees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of image and vision computing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multiscale spatio-temporal background model for motion detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Illumination independent change detection for real world image sequences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Skifstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making background subtraction robust to sudden illumination changes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pilet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A bayesian computer vision system for modeling human interactions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust and accurate change detection under sudden illumination variations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Di Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Lisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A robust measure for visual correspondence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Density-based multifeature background subtraction with support vector machine</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The sobs algorithm: what are the limits?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Change detection with weightless neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Gregorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giordano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrating intensity and texture differences for robust change detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mrf-based adaptive approach for foreground segmentation under sudden illumination change</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information, Communications &amp; Signal Processing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Background segmentation beyond rgb</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kristensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Öwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance analysis of lab2000hl color space for background subtraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karabiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sonmez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Innovations in Intelligent Systems and Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A statistical approach for real-time robust background subtraction and shadow detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Horprasert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Background subtraction in video using recursive mixture models, spatio-temporal filtering and shadow removal</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting moving objects, ghosts, and shadows in video streams</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Background extraction and update method based on histogram in ycbcr color space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chengjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huiyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on E-Business and E-Government</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
		<title level="m">Color in computer vision: fundamentals and applications</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Entropy rate superpixel segmentation</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kdd</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">34</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">MATLAB and Octave functions for computer vision and image processing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Kovesi</surname></persName>
		</author>
		<ptr target="&lt;http://www.peterkovesi.com/matlabfns/&gt;" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Esi dataset [online]. available</title>
		<ptr target="https://sites.google.com/site/-tommasogritti/publications/background-subtraction-data" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spectral-360: A physicsbased technique for change detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sedky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moniri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chibelushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A fast self-tuning background subtraction algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial mixture of gaussians for dynamic background modelling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A fuzzy spatial coherence-based approach to background/foreground separation for moving object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maddalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrosino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Comparative study of background subtraction algorithms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Emile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improved adaptive gaussian mixture model for background subtraction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zivkovic</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/hasansajid/research" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Real-time subspace-based background modeling using multi-channel data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An improved adaptive background mixture model for real-time tracking with shadow detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kaewtrakulpong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Video-based surveillance systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
