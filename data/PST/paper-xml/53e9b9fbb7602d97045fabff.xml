<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SHIFT: Shared History Instruction Fetch for Lean-Core Server Processors</title>
				<funder ref="#_kTBakax">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Edinburgh EcoCloud</orgName>
								<orgName type="institution" key="instit2">EPFL EcoCloud</orgName>
								<address>
									<region>EPFL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cansu</forename><surname>Kaynak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Edinburgh EcoCloud</orgName>
								<orgName type="institution" key="instit2">EPFL EcoCloud</orgName>
								<address>
									<region>EPFL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Edinburgh EcoCloud</orgName>
								<orgName type="institution" key="instit2">EPFL EcoCloud</orgName>
								<address>
									<region>EPFL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SHIFT: Shared History Instruction Fetch for Lean-Core Server Processors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2540708.2540732</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>B.3.2 [Memory Structures]: Design Styles -cache memories Design</term>
					<term>Performance Instruction streaming</term>
					<term>prefetching</term>
					<term>caching</term>
					<term>branch prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In server workloads, large instruction working sets result in high L1 instruction cache miss rates. Fast access requirements preclude large instruction caches that can accommodate the deep software stacks prevalent in server applications. Prefetching has been a promising approach to mitigate instruction-fetch stalls by relying on recurring instruction streams of server workloads to predict future instruction misses. By recording and replaying instruction streams from dedicated storage next to each core, stream-based prefetchers have been shown to overcome instruction fetch stalls. Problematically, existing stream-based prefetchers incur high history storage costs resulting from large instruction working sets and complex control flow inherent in server workloads. The high storage requirements of these prefetchers prohibit their use in emerging lean-core server processors.</p><p>We introduce Shared History Instruction Fetch, SHIFT, an instruction prefetcher suitable for lean-core server processors. By sharing the history across cores, SHIFT minimizes the cost per core without sacrificing miss coverage. Moreover, by embedding the shared instruction history in the LLC, SHIFT obviates the need for dedicated instruction history storage, while transparently enabling multiple instruction histories in the presence of workload consolidation. In a 16-core server CMP, SHIFT eliminates 81% (up to 93%) of instruction cache misses, achieving 19% (up to 42%) speedup on average. SHIFT captures 90% of the performance benefit of the state-of-the-art instruction prefetcher at 14x less storage cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Servers power today's information-centric world. Server workloads range from traditional databases that perform business analytics or online transaction processing, to emerging scale-out workloads such as web search and media streaming. A characteristic feature of server workloads is their multi-megabyte instruction working sets that defy the capacities of private first-and secondlevel caches. As a result, server workloads suffer from frequent instruction fetches from the last-level cache that cause the frontend of the processor to stall. Prior work has shown that frontend stalls due to large instruction working sets of server workloads account for up to 40% of execution time in server processors <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>Prefetching is a well-known mechanism for overcoming the instruction stall bottleneck. Because server workloads exhibit highly recurring behavior in processing a large number of similar requests, they are amenable to prefetching as their control flow tends to be predictable at the request level. However, the control flow of each individual request may be quite complex and span multiple layers of the software stack, including the application itself, a database engine, a web server and the OS. As a result, na?ve techniques, such as next-line prefetching, offer only marginal performance benefit.</p><p>State-of-the-art instruction prefetchers for server workloads rely on temporal streaming to record, and subsequently replay, entire sequences of instructions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. As a class, these stream-based prefetchers have been shown to be highly effective at eliminating the vast majority of frontend stalls stemming from instruction cache misses. However, due to the large instruction footprints and small, yet numerous differences in the control flow among the various types of requests of a given workload, stream-based prefetchers require significant storage capacities for high miss coverage. Thus, the state-of-the-art stream-based prefetcher for servers, Proactive Instruction Fetch (PIF), can eliminate an average of 90% of instruction cache misses, but necessitates over 200KB per core for its history storage <ref type="bibr" target="#b13">[14]</ref>.</p><p>Stream-based instruction prefetchers were proposed for conventional fat-core processors, where the prefetcher's high storage cost is acceptable in view of large private cache capacities and big core area footprints. Meanwhile, recent research results <ref type="bibr" target="#b22">[22]</ref> and industry trends point in the direction of server processors with many lean cores, rather than a handful of fat cores, which is a characteristic of conventional server processor designs. For instance, Tilera's Tile-series processors integrate as many as 64 simple cores and have been shown to be highly effective on Facebook's workload <ref type="bibr" target="#b6">[7]</ref>. In general, manycore processors are well-matched to rich request-level parallelism present in server workloads, and achieve high energy efficiency on memory-intensive server applications through their simple core microarchitectures.</p><p>We observe that stream-based prefetchers are poorly matched to lean-core processor designs in which the area footprint of the prefetcher history storage may approach that of the core and its L1 caches. For instance, in 40nm technology, an ARM Cortex-A8 (a dual-issue in-order core) together with its L1 caches occupies an area of 1.3mm 2 , whereas PIF's per-core storage cost is 0.9mm 2 . This work attacks the problem of effective instruction prefetch in lean-core server processors. Individual server workloads are homogeneous, meaning that each core executes the same types of requests as all other cores. Consequently, over time, the various cores of a processor executing a common homogeneous server workload tend to generate similar instruction access sequences. Building on this observation, we make a critical insight that commonality and recurrence in the instruction-level behavior across cores can be exploited to generate a common instruction history, which can then be shared by all of the cores running a given workload. By sharing the instruction history and its associated storage among multiple cores, this work provides an effective approach for mitigating the severe area overhead of existing instruction prefetcher designs, while preserving their performance benefit. As a result, this work proposes a practical instruction prefetcher to mitigate the frontend stalls resulting from instruction cache misses in lean-core server processors.</p><p>The contributions of this work are as follows:</p><p>? We demonstrate significant commonality (over 90%) in the instruction history across multiple cores running a common server workload.</p><p>? We show that a single core chosen at random can generate the instruction history, which can then be shared across other cores running the same workload. In a 16-core CMP, the shared history eliminates 81%, on average, of all instruction cache misses across a variety of traditional and emerging server workloads.</p><p>? We introduce Shared History Instruction Fetch, SHIFT, a new instruction prefetcher design, which combines shared instruction history with light-weight per-core control logic. By sharing the history, SHIFT virtually eliminates the high history storage cost associated with earlier approaches, improving performance per unit of core area by 16% and 59% for two lean-core designs over the state-of-the-art instruction prefetcher <ref type="bibr" target="#b13">[14]</ref>. The absolute performance improvement on a suite of diverse server workloads is 19%, on average.</p><p>? By embedding the history in the memory hierarchy, SHIFT eliminates the need for dedicated storage and provides the flexibility needed to support consolidated workloads via a perworkload history.</p><p>The rest of the paper is organized as follows. We motivate the need for effective and low-cost instruction prefetching for lean-core server processors in Section 2. We quantify the commonality of instruction fetch streams across cores running a server workload in Section 3 and describe the SHIFT design in Section 4. In Section 5, we evaluate SHIFT against prior proposals and study SHIFT's sensitivity to the design parameters. We discuss additional issues and prior work in Section 6 and Section 7. Finally, we conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instruction Stalls in Server Workloads</head><p>Server workloads have vast instruction working sets as a result of the deep software stacks they employ and their heavy reliance on the operating system functionality. The large instruction footprints are beyond the reach of today's practical first-level instruction caches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b35">35]</ref>. Instruction cache misses served from lower levels of the cache hierarchy incur delays that cause server cores to stall up to 40% of the execution time <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39]</ref>, exposing instruction-fetch stalls as a dominant performance bottleneck in servers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b35">35]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the performance improvements that server workloads can achieve as a function of the fraction of instruction cache misses eliminated. The system configuration and the workloads used for the experiment are detailed in Section 5.1. In this experiment, each instruction cache miss is converted into a hit (i.e., the miss latency is not exposed to the core) with some probability, determined based on the desired fraction of instruction misses eliminated. 100% instruction cache miss elimination is equivalent to a perfect instruction cache.</p><p>As Figure <ref type="figure" target="#fig_0">1</ref> shows, both traditional database and web workloads (OLTP, DSS and web frontend), as well as emerging server workloads (media streaming and web search), achieve significant performance improvements as a result of eliminating instruction misses. As indicated by the trend line, performance increases linearly as the fraction of instruction misses decreases, reaching 31% speedup on average, for our workload suite.</p><p>Despite the linear relationship between performance and miss coverage, the correspondence is not one-to-one. This indicates that while high miss coverage is important, perfection is not necessary to realize much of the performance benefit. For instance, improving the miss coverage from 80% to 90% yields an average performance gain of 2.3%. Consequently, the amount of resources dedicated to eliminating some fraction of misses needs to be balanced with the expected performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instruction Prefetching</head><p>Instruction prefetching is an established approach to alleviate instruction-fetch stalls prevalent in server workloads. Next-line prefetcher, a common design choice in today's processors, improves performance by 9% by eliminating 35% of instruction cache misses, on average, for our workload suite. Given the performance potential from eliminating more instruction cache misses,  server processors call for more sophisticated instruction prefetchers.</p><p>To overcome the next-line prefetcher's inability to predict instruction cache misses that are not contiguous, stream-based prefetchers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> exploit the recurring control-flow graph traversals in server workloads. As program control flow recurs, the core frontend generates repeating sequences of fetch addresses. The instruction fetch addresses that appear together and in the same order are temporally correlated and together form a so-called temporal stream. For instance, in the instruction cache access sequence A, B, C, D, X, Y, A, B, C, D, Z, the address sequence A, B, C, D constitutes a temporal instruction stream. Once a temporal stream is recorded, it can be identified by its first address, the stream head (address A in our example) and replayed to issue prefetch requests in advance of the core frontend to hide the instruction cache miss latency from the core. If the actual instruction stream matches the replayed stream (addresses B, C, D), the instruction misses are eliminated by the prefetcher.</p><p>The state-of-the-art stream-based instruction prefetcher is Proactive Instruction Fetch (PIF) <ref type="bibr" target="#b13">[14]</ref>, which extends earlier work on temporal streaming <ref type="bibr" target="#b14">[15]</ref>. PIF's key innovation over prior work is its reliance on access streams instead of miss streams. By recording all accesses to the instruction cache, as opposed to just those that miss, PIF eliminates the dependence on the content of the cache. While cache content can change over time, reference patterns that PIF records remain stable. The drawback of recording access streams, instead of just miss streams, is the high history storage required. For instance, to eliminate an average of 90% of instruction cache misses, thus approaching the performance of a perfect I-cache, PIF requires over 210KB of history storage per core (Section 5.1 details PIF's storage cost).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Toward Lean Cores</head><p>Today's mainstream server processors, such as Intel Xeon and AMD Opteron, feature a handful of powerful cores targeting high single-threaded performance. Recent research has demonstrated a mismatch between these processor organizations and server workloads whose chief characteristic is low instruction-and memorylevel parallelism but high request-level parallelism <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">22]</ref>. Server workloads are best served by processors with many lean cores to maximize throughput and efficiency. A number of processors in today's server space exemplify the lean-core philosophy. These include the Tilera Tile series <ref type="bibr" target="#b45">[45]</ref> and the Calxeda ARM-based Server-on-Chip <ref type="bibr" target="#b8">[9]</ref>. While the performance benefits of streambased instruction prefetching are consistently high across the core microarchitecture spectrum, the relative cost varies greatly. For instance, the 213KB of storage required by the PIF prefetcher described above consumes 0.9mm 2 of the die real-estate in 40nm process technology. Meanwhile, a Xeon Nehalem core along with its private L1 caches has an area footprint of 25mm 2 . The 4% of area overhead that PIF introduces when coupled to a Xeon core is a relative bargain next to the 23% performance gain it delivers. This makes PIF a good design choice for conventional server processors.</p><p>On the opposite end of the microarchitectural spectrum is a very lean core like the ARM Cortex-A8 <ref type="bibr" target="#b5">[6]</ref>. The A8 is a dual-issue inorder design with an area of 1.3mm 2 , including the private L1 caches. In comparison to the A8, PIF's 0.9mm 2 storage overhead is prohibitive given the 17% performance boost that it delivers.</p><p>Given that server workloads have abundant request-level parallelism, making it easy to scale performance with core count, the area occupied by PIF is better spent on another A8 core. The extra core can double the performance over the baseline core, whereas PIF only offers a 17% performance benefit.</p><p>To succinctly capture the performance-area trade-off, we use the metric of performance-density (PD), defined as performance per unit area <ref type="bibr" target="#b22">[22]</ref>. By simply adding cores, server processors can effectively scale performance, while maintaining a constant PD (i.e., twice the performance in twice the area). As a result, the desirable microarchitectural features are those that grow performance-density, as they offer a better-than-linear return on the area investment.</p><p>Figure <ref type="figure">2</ref> shows the relative performance-density merits of three PIF-enabled core microarchitectures over their baseline (without PIF) counterparts. Two of the cores are the Xeon and ARM Cortex-A8 discussed above; the third is an ARM Cortex-A15 <ref type="bibr" target="#b40">[40]</ref>, a lean out-of-order core with an area of 4.5mm 2 . For simplicity, we refer to the three designs as Fat-OoO (Xeon), Lean-OoO (A15), and Lean-IO (A8).</p><p>In the figure, the trend line indicates constant PD (PD = 1), which corresponds to scaling performance by adding cores. The shaded area that falls into the left-hand side of the trend line is the region of PD gain, where the relative performance improvement is greater than the relative area overhead. The right-hand side of the trend line corresponds to PD loss, where the relative performance gain is less than the relative area. In summary, designs that fall in the shaded region improve performance-density over the baseline; those outside strictly diminish it.</p><p>The results in the figure match the intuition. For the Fat-OoO core, PIF improves performance-density. In contrast, for the Lean-OoO core, PIF fails to improve PD, and for the Lean-IO core, PIF actually diminishes PD, providing less-than-linear performance density benefit that cannot compensate for its area overhead. Thus, we conclude that lean-core processors benefit from instruction prefetching nearly as much as fat-core designs, but mandate areaefficient mechanisms to minimize the overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lean-IO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lean-OoO</head><p>Fat-OoO </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INSTRUCTION STREAM COMMONALITY ACROSS CORES</head><p>Our performance density analysis of the state-of-the-art instruction prefetcher shows it to be unsuitable for lean-core server processors due to its high area overhead. This work overcomes the high percore storage cost of existing designs by sharing the cost among all cores executing a common server workload. Our insight is that instruction fetch streams across cores are common and are amenable to sharing.</p><p>To show the potential for sharing the instruction history, we first quantify the similarity between the instruction streams of cores running a homogeneous server workload. For the purposes of this study, only one core picked at random records its instruction cache access stream. All the other cores, upon referencing the first address in a stream, replay the most recent occurrence of that stream in the recorded history. The commonality between two streams is quantified as the number of matching instruction block addresses between the replayed stream and those issued by the core replaying the stream. For this study, we use instruction fetch traces from 16 cores and average the results across all of the cores.</p><p>Figure <ref type="figure">3</ref> illustrates the commonality of instruction streams between cores executing a given homogeneous server workload. More than 90% (up to 96%) of instruction cache accesses (comprised of both application and operating system instructions) from all sixteen cores belong to temporal streams that are recorded by a single core picked at random. This result indicates that program control flow commonality between cores yields temporal instruction stream commonality, suggesting that multiple cores running a homogeneous server workload can benefit from a single shared instruction history for instruction prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SHIFT DESIGN</head><p>SHIFT exploits the commonality of instruction fetch streams across cores running a common homogeneous server workload by enabling a single instruction fetch stream history to be shared by all the cores. We base the SHIFT history storage on the Global History Buffer <ref type="bibr" target="#b26">[26]</ref> prefetcher to record instruction fetch streams in a similar vein to prior stream-based instruction prefetchers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. We augment each core with simple logic to read instruction streams from the shared history buffer and issue prefetch requests.</p><p>In the rest of this section, we first present the baseline SHIFT design with dedicated storage, and then explain how to virtualize the storage (i.e., embed the storage in the LLC). Finally, we demonstrate SHIFT with multiple history buffers to enable support for workload consolidation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline SHIFT Design</head><p>SHIFT employs two microarchitectural components shared by all the cores running a common workload to record and replay the common instruction streams: the history buffer and the index table.</p><p>The history buffer records the history of instruction streams; the index table provides fast lookups for the records stored in the history buffer.</p><p>The per-core private stream address buffer reads instruction streams from the shared history buffer and coordinates prefetch requests in accordance with instruction cache misses.</p><p>Recording. SHIFT's distinguishing feature is to maintain a single shared history buffer and employ only one core, history generator core, running the target workload to generate the instruction fetch stream history.</p><p>The history generator core records retire-order instruction cache access streams to eliminate the microarchitectural noise in streams introduced by the instruction cache replacement policy and branch mispredictions <ref type="bibr" target="#b13">[14]</ref>. To mitigate increased history storage requirements resulting from recording instruction cache accesses rather than instruction cache misses, the history generator core collapses retired instruction addresses by forming spatial regions of instruction cache blocks.</p><p>Step 1 in Figure <ref type="figure" target="#fig_2">4</ref>(a) depicts how a spatial region is generated by recording retire-order instruction cache accesses obtained from the history generator core's backend. In this example, a spatial region consists of five consecutive instruction cache blocks; the trigger block and four adjacent blocks. The first access to the spatial region, an instruction within block A, is the trigger access and defines the new spatial region composed of the instruction blocks between block A and A+4. Subsequent accesses to the same spatial region are recorded by setting the corresponding bits in the bit vector until an access to a block outside the current region occurs.</p><p>Upon an access to a new spatial region, the old spatial region record is sent to the shared history buffer to be recorded. The history buffer, logically organized as a circular buffer, maintains the stream of retired instructions as a queue of spatial region records.</p><p>A new spatial region is recorded in the shared history buffer into the entry pointed by the write pointer, as illustrated in step 2 in Figure <ref type="figure" target="#fig_2">4</ref>(a). The write pointer is incremented by one after every history write operation and wraps around when it reaches the end of the history buffer.</p><p>To enable fast lookup for the most recent occurrence of a trigger address, SHIFT employs an index table for the shared history buffer, where each entry is tagged with a trigger instruction block address and stores a pointer to that block's most recent occurrence in the history buffer. Whenever a new spatial region record is inserted into the history buffer, SHIFT modifies the index table entry for the trigger address of the new record to point to the insertion position (step 3 in Figure <ref type="figure" target="#fig_2">4</ref>(a)).</p><p>Replaying. The per-core stream address buffer maintains a queue of spatial region records and is responsible for reading a small portion of the instruction stream history from the shared history buffer in anticipation of future instruction cache misses. When an instruction block is not found in the instruction cache, the stream address buffer issues an index lookup for the instruction's block address to the index table (step 1 in Figure <ref type="figure" target="#fig_2">4(b)</ref>). If a matching entry is found, it supplies the pointer to the most recent occurrence of the address in the history buffer. Once the spatial region corresponding to the instruction block that triggered the lookup is located in the history buffer (step 2 in Figure <ref type="figure" target="#fig_2">4(b)</ref>), the stream address buffer reads out the record and a number of consecutive records following it as a lookahead optimization. The records are then placed into the stream address buffer (step 3 in Figure <ref type="figure" target="#fig_2">4(b)</ref>).</p><p>Next, the stream address buffer reconstructs the instruction block addresses encoded by the spatial region entries based on the trigger address and the bit vector. Then, the stream address buffer issues prefetch requests for the reconstructed instruction block addresses if they do not exist in the instruction cache (step 4 in Figure <ref type="figure" target="#fig_2">4</ref>(b)).</p><p>The stream address buffer also monitors the retired instructions. A retired instruction that falls into a spatial region maintained by the stream address buffer advances the stream by triggering additional spatial region record reads from the history buffer (step 5 in Figure <ref type="figure" target="#fig_2">4</ref>(b)) and issues prefetch requests for the instruction blocks in the new spatial regions.</p><p>As an optimization, SHIFT employs multiple stream buffers (four in our design) to replay multiple streams, which may arise due to frequent traps and context switches in server workloads. The leastrecently-used stream is evicted upon allocating a new stream. When the active stream address buffer reaches its capacity, the oldest spatial region record is evicted to make space for the incoming record.</p><p>For the actual design parameters we performed the corresponding sensitivity analysis and found that a spatial region size of eight, a lookahead of five and a stream address buffer capacity of twelve achieve the maximum performance (results not shown in the paper due to space limitation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Virtualized SHIFT Design</head><p>The baseline SHIFT design described in Section 4.1 relies on dedicated history storage. The principal advantage of dedicated storage is that it ensures non-interference with the cache hierarchy. However, this design choice carries several drawbacks, including (1) new storage structures for the history buffer and the index table, (2) lack of flexibility with respect to capacity allocation, and (3) considerable storage expense to support multiple histories as required for workload consolidation. To overcome these limitations, we embed the SHIFT history buffer in the LLC leveraging the virtualization framework <ref type="bibr" target="#b7">[8]</ref>.</p><p>History Virtualization. To virtualize the instruction history buffer, SHIFT first allocates a portion of the physical address space for the history buffer. History buffer entries are stored in the LLC along with regular instruction and cache blocks. For the index table entries, SHIFT extends the LLC tag array to augment the existing instruction block tags with pointers to the shared history buffer records.</p><p>SHIFT reserves a small portion of the physical address space that is hidden from the operating system. The reserved address space starts from a physical address called the History Buffer Base (HBBase) and spans a contiguous portion of the physical address space. The size of the reserved physical address space for the history buffer can change based on the instruction working set size of a workload.</p><p>The history buffer is logically a circular buffer; however, the actual storage is organized as cache blocks. To access a spatial region record in the history buffer, the value of the pointer to the spatial region record is added to HBBase to form a physical address and an LLC lookup is performed for that physical address. Each cache block that belongs to the history buffer contains multiple spatial region records, therefore, each history buffer read and write operation spans multiple spatial region records. The LLC blocks that belong to the history buffer are non-evictable, which ensures that the entire history buffer is always present in the LLC. Non-eviction support is provided at the cache controller through trivial logic that compares a block's address to the address range reserved for the history. As an alternative, a cache partitioning scheme (e.g., <ref type="bibr">Van</ref>tage <ref type="bibr" target="#b31">[31]</ref>) can easily guarantee the required cache partition for the history buffer.</p><p>The index table, which contains pointers to the spatial region records in the history buffer, is embedded in the LLC by extending the tag array with pointer bits. This eliminates the need for a dedicated index table and provides an index lookup mechanism for free by coupling index lookups with instruction block requests to LLC (details below). Although each tag is augmented with a pointer, the pointers are used only for instruction blocks. Each instruction block tag in the LLC can point to the most recent occurrence of the corresponding instruction block address in the history buffer. The width of the pointer is a function of the history buffer size and is independent of the LLC capacity. In our design, each pointer is 15 bits allowing for an up to 32K-entry history buffer.</p><p>In Figure <ref type="figure" target="#fig_3">5</ref>, the shaded areas indicate the changes in the LLC due to history virtualization. The LLC tag array is augmented with  pointers and a portion of the LLC blocks are reserved for the history buffer. The LLC blocks reserved for the history buffer are distributed across different sets and banks; however, we show the history buffer as a contiguous space in the LLC to simplify the figure.</p><p>Recording. Figure <ref type="figure" target="#fig_3">5</ref>(a) illustrates how the SHIFT logic next to the history generator core records the retire-order instruction stream in the shared and virtualized history buffer. First, the history generator core forms spatial region records as described in Section 4.1.</p><p>Because the history buffer is accessed at cache-block granularity in virtualized SHIFT, the history generator core accumulates the spatial region records in a cache-block buffer (CBB), instead of sending each spatial region record to the LLC one by one (step 1). However, upon each spatial region record insertion into the CBB, the history generator core issues an index update request to the LLC for the spatial region's trigger address by sending the current value of the write pointer (step 2). The LLC performs a tag lookup for the trigger instruction block address and if the block is found in the LLC, its pointer is set to the write pointer value sent by the history generator core. After sending the index update request, the history generator core increments the write pointer.</p><p>Once the CBB becomes full, its content needs to be inserted into the virtualized history buffer. To accomplish this, the SHIFT logic next to the history generator core computes the write address by adding the value of the write pointer to HBBase (step 3), and then flushes the CBB into the LLC to the computed write address (step 4).</p><p>Replaying. While the history generator core is continuously writing its instruction access history into the LLC-resident history buffer, the rest of the cores executing the workload read the history buffer to anticipate their instruction demands. Figure <ref type="figure" target="#fig_3">5</ref>(b) illustrates how each core replays the shared instruction stream.</p><p>SHIFT starts replaying a new stream when there is a miss in the instruction cache. For every demand request for an instruction block, the LLC sends the instruction block and the index pointer stored next to the tag of the instruction block to the requesting core (step 1). The SHIFT logic next to the core constructs the physical address for the history buffer by adding the index pointer value to HBBase and sends a request for the corresponding history buffer block (step 2). Finally, the LLC sends the history buffer block to the stream address buffer of the core (step 3).</p><p>Upon arrival of a history buffer block, the stream address buffer allocates a new stream and places the spatial region records in the history buffer block into the new stream. The stream address buffer constructs instruction block addresses and issues prefetch requests for them (step 4) as described in Section 4.1. If a retired instruction matches with an address in the stream address buffer, the stream is advanced by issuing a history read request to the LLC following the index pointer maintained as part of the stream in the stream address buffer (i.e., by incrementing the index pointer by the number of history buffer entries in a block and constructing the physical address as in step 2) .</p><p>Hardware cost. Each spatial region record, which spans eight consecutive instruction blocks, maintains the trigger instruction block address (34 bits) and 7 bits in the bit vector (assuming a 40bit physical address space and 64-byte cache blocks). A 64-byte cache block can accommodate 12 such spatial region records. A SHIFT design with 32K history buffer entries (i.e., spatial region records) necessitates 2,731 cache lines for an aggregate LLC footprint of 171KB.</p><p>With history entries stored inside the existing cache lines and given the trivial per-core prefetch control logic, the only source of meaningful area overhead in SHIFT is due to the index table appended to the LLC tag array. The index table augments each LLC tag with a 15-bit pointer into the 32K-entry virtualized history buffer. In an 8MB LLC, these extra bits in the tag array constitute the 240KB storage overhead (accounting for the unused pointers for associated with regular data blocks in the LLC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SHIFT Design for Workload Consolidation</head><p>Multiple server workloads running concurrently on a manycore CMP also benefit from SHIFT, as SHIFT relies on virtualization allowing for a flexible history buffer storage mechanism. SHIFT requires two minor adjustments in the context of workload consolidation. First, a history buffer per workload. should be instantiated in the LLC. SHIFT's 171KB (2% of an 8MB LLC) history buffer size is dwarfed by the LLC capacities of contemporary server processors and the performance degradation due to the LLC capacity reserved for SHIFT is negligible. So, we instantiate one history buffer per workload. Second, the operating system or the hypervi- sor needs to assign one history generator core per workload and set the history buffer base address (HBBase) to the HBBase of the corresponding history buffer for all cores in the system. After these two adjustments, the record and replay of instruction streams work as described in Section 4.2.</p><p>Even with extreme heterogeneity (i.e., a unique workload per core), SHIFT provides a storage advantage over PIF as the history buffers are embedded in the LLC data array and the size of the index table embedded in the LLC tag array does not change.</p><p>Because the size of the index pointers only depends on the size of the corresponding history buffer, it does not change with the number of active per-workload history buffers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methodology</head><p>We evaluate SHIFT and compare it to the state-of-the-art instruction prefetcher with per-core private instruction history, PIF <ref type="bibr" target="#b13">[14]</ref>, using trace-based and cycle-accurate simulations of a 16-core CMP, running server workloads. For our evaluation, we use Flexus <ref type="bibr" target="#b44">[44]</ref>, a Virtutech Simics-based, full-system multiprocessor simulator, which models the SPARC v9 instruction set architecture. We simulate CMPs running the Solaris operating system and executing the server workload suite listed in Table <ref type="table" target="#tab_4">I</ref>.</p><p>We use trace-based experiments for our opportunity study and initial predictor results by using traces 32 billion instructions (two billion per core) in steady state. For the DSS workloads, we collect traces for the entire query execution. Our traces include both the application and the operating system instructions.</p><p>For performance evaluation, we use the SimFlex multiprocessor sampling methodology <ref type="bibr" target="#b44">[44]</ref>, which extends the SMARTS sampling framework <ref type="bibr" target="#b46">[46]</ref>. Our samples are collected over 10-30 seconds of workload execution (for the DSS workloads, they are collected over the entire execution). For each measurement point, we start the cycle-accurate simulation from checkpoints with warmed architectural state and run 100K cycles of cycle-accurate simulation to warm up the queues and the interconnect state, then collect measurements from the subsequent 50K cycles. We use the ratio of the number of application instructions to the total number of cycles (including the cycles spent executing operating system code) to performance; this metric has been shown to accurately reflect overall system throughput <ref type="bibr" target="#b44">[44]</ref>. Performance measurements are computed with an average error of less than 5% at the 95% confidence level.</p><p>We model a tiled SHIFT architecture with a lean-OoO core modeled after an ARM-Cortex A15 <ref type="bibr" target="#b40">[40]</ref>. For the performance density study, we also consider a fat-OoO core (representative of contemporary Xeon-class cores) and a lean in-order core (similar to an ARM Cortex-A8 <ref type="bibr" target="#b5">[6]</ref>), which all operate at 2GHz to simplify the comparison. The design and architectural parameter assumptions are listed in Table <ref type="table" target="#tab_4">I</ref>. For performance density studies, we scale the published area numbers for the target cores to the 40nm technology. Cache parameters, including the SRAMs for PIF's history buffer and index table, are estimated using CACTI <ref type="bibr" target="#b24">[24]</ref>.</p><p>State-of-the-art prefetcher configuration. We compare SHIFT's effectiveness and history storage requirements with the state-ofthe-art instruction prefetcher, PIF <ref type="bibr" target="#b13">[14]</ref>. Like other stream-based instruction and data prefetchers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">43]</ref>, PIF employs a per-core history buffer and index table. PIF records and replays spatial region records with eight instruction blocks. Each spatial region record maintains the trigger instruction block address (34 bits) and 7 bits in the bit vector. Hence, each record in the history buffer contains 41 bits. PIF requires 32K spatial region records in the history buffer targeting 90% instruction miss coverage <ref type="bibr" target="#b13">[14]</ref>, also validated with our experiments. As a result, the history buffer is 164KB for each core in total.</p><p>Each entry in PIF's index PIF design as PIF_2K and the original PIF design as PIF_32K to differentiate between the two design points in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Instruction Miss Coverage</head><p>To show SHIFT's effectiveness, we first compare the fraction of instruction cache misses predicted by SHIFT to PIF <ref type="bibr" target="#b13">[14]</ref>. For the purposes of this study, we only track the predictions that would be made through replaying recurring instruction streams in stream address buffers and do not prefetch or perturb the instruction cache state.</p><p>Figure <ref type="figure">6</ref> shows the fraction of instruction cache misses correctly predicted for all of the cores in the system averaged across all workloads, as the number of spatial region records in the history buffer increases. The history size shown is the aggregate for PIF in the 16-core system evaluated, whereas for SHIFT, it is the overall size of the single shared history buffer.</p><p>Because the history buffer can maintain more recurring temporal instruction streams as its size increases, the prediction capabilities of both designs increase monotonically with the allocated history buffer size. Because the aggregate history buffer capacity is distributed across cores, PIF's coverage always lags behind SHIFT. For relatively small aggregate history buffer sizes, PIF's small percore history buffer can only maintain a small fraction of the instruction working set size. As the history buffer size increases, PIF's per-core history buffer captures a larger fraction of the instruction working set. For all aggregate history buffer sizes, SHIFT can maintain a higher fraction of the instruction working set compared to PIF by employing a single history buffer, rather than distributing it across cores. As all the cores can replay SHIFT's shared instruction stream history, SHIFT's miss coverage is always greater than PIF for equal aggregate storage capacities. Because the history sizes beyond 32K return diminishing performance benefits, for the actual SHIFT design, we pick a history buffer size of 32K records.</p><p>We compare SHIFT's actual miss coverage with PIF for each workload, this time accounting for the mispredictions as well, as mispredictions might evict useful but not-yet-referenced blocks in the cache. For this comparison, we use the two PIF design points described in Section 5.1.</p><p>Figure <ref type="figure">7</ref> shows the instruction cache misses eliminated (covered) and the mispredicted instruction blocks (overpredicted) normalized to the instruction cache misses in the baseline design without any prefetching. On average, SHIFT eliminates 81% of the instruc-tion cache misses with 16% overprediction, while PIF_32K eliminates 92% of the instruction cache misses with 13% overprediction, corroborating prior results <ref type="bibr" target="#b13">[14]</ref>. However, PIF_2K, which has the same aggregate storage overhead as SHIFT, can eliminate only 53% of instruction cache misses on average, with a 20% overprediction ratio. The discrepancy in miss coverage between PIF_2K and SHIFT is caused by the limited instruction stream history stored by each core's smaller history buffer in PIF, which falls short of capturing the instruction working set.</p><p>In conclusion, by sharing the instruction history generated by a single core, all cores running a common server workload attain similar benefits to per-core instruction prefetching, but with a much lower storage overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Comparison</head><p>We compare SHIFT's performance against PIF_32K, PIF_2K and the next-line prefetcher normalized to the performance of the baseline system with lean-OoO cores, where no instruction prefetching mechanism is employed in Figure <ref type="figure">8</ref>.</p><p>The difference in PIF_32K and SHIFT's speedups stems from two main differences. First, SHIFT has slightly lower miss coverage compared to PIF_32K, as shown in Figure <ref type="figure">7</ref>. Second, SHIFT's history buffer is embedded in the LLC resulting in accesses to the LLC for reading the history buffer, which delays the replay of streams until the history buffer block arrives at the core. Moreover, history buffer reads and writes to LLC incur extra LLC traffic as compared to PIF_32K. To compare the performance benefits resulting solely from SHIFT's prediction capabilities, we also plot the performance results achieved by SHIFT assuming a dedicated history buffer with zero access latency (ZeroLat-SHIFT).</p><p>The relative performance improvements of zero-latency SHIFT and PIF_32K match the miss coverages shown in Figure <ref type="figure">7</ref>. On average, zero-latency SHIFT provides 20% performance improvement, while PIF_32K improves performance by 21%. A realistic SHIFT design results in 1.5% speedup loss compared to zerolatency SHIFT, due to the latency of accessing the history buffer in the LLC and the traffic created by transferring history buffer data over the network. Overall, despite its low history storage cost, SHIFT retains over 90% of the performance benefit (98% of the overall absolute performance) that PIF_32K provides.</p><p>In comparison to PIF_2K, SHIFT achieves higher speedups for all the workloads as a result of its higher miss coverage as Figure <ref type="figure">7</ref> shows. Due to its greater effective history buffer capacity, SHIFT outperforms PIF_2K for all the workloads (by 9% on For the workloads with bigger instruction working sets (e.g., OLTP on Oracle), SHIFT outperforms PIF_2K by up to 26%.</p><p>Finally, we compare SHIFT's performance to the next-line prefetcher. Although the next-line prefetcher does not incur any storage overheads, it only provides 9% performance improvement due to its low miss coverage (35%) stemming from its incapability of predicting misses to discontinuous instruction blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">LLC Overheads</head><p>SHIFT introduces two types of LLC overhead. First, the history buffer occupies a portion of the LLC, effectively reducing its capacity. Our results indicate that the performance impact of reduced capacity is negligible. With SHIFT occupying just 171KB of the LLC capacity, the measured performance loss with an 8MB LLC is under 1%.</p><p>The second source of overhead is due to the extra LLC traffic, generated by <ref type="bibr" target="#b0">(1)</ref> read and write requests to the history buffer; (2) useless LLC reads as a result of mispredicted instruction blocks, which are discarded before used by the core; and (3) index updates issued by the history generator core. Figure <ref type="figure" target="#fig_5">9</ref> illustrates the extra LLC traffic generated by SHIFT normalized to the LLC traffic (due to both instruction and data requests) in the baseline system without any prefetching. History buffer reads and writes increase the LLC traffic by 6%, while discards account for the 7% of the baseline LLC traffic on average. The index updates (not shown in the graph) are only 2.5% of the baseline LLC traffic; however, they only increase the traffic in the LLC tag array.</p><p>In general, we note that LLC bandwidth is ample in our system, as server workloads have low ILP and MLP, plus the tiled design provides for a one-to-one ratio of cores to banks, affording very high aggregate LLC bandwidth. With average LLC bandwidth utilization well under 10%, the additional LLC traffic even for the worstcase workload (web frontend) is easily absorbed and has no bearing on performance in our studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Workload Consolidation</head><p>Figure <ref type="figure" target="#fig_6">10</ref> shows the performance improvement attained by SHIFT in comparison to the next-line prefetcher, PIF_2K and PIF_32K, in the presence of multiple workloads running on a server CMP. In this experiment, we use the 16-core Lean-OoO server processor described in Section 5.1. We consolidate two traditional (OLTP on Oracle and web frontend) and two emerging (media streaming and web search) server workloads. Each workload runs on four cores and has its own software stack (i.e., separate OS images). For SHIFT, each workload has a single shared history buffer with 32K records embedded in the LLC.</p><p>We see that the speedup trends for the various design options follow the same trend as the standalone workloads, as shown in Section 5.3. SHIFT delivers 95% of PIF_32K's absolute performance and outperforms PIF_2K by 12% and the next-line prefetcher by 11% on average.</p><p>Zero-latency SHIFT delivers 25% performance improvement over the baseline, while SHIFT achieves 22% speedup on average. The 3% difference mainly results from the extra LLC traffic generated by virtualized SHIFT. The LLC traffic due to log reads remains the same in the case of workload consolidation as all the cores read from their corresponding shared history embedded in the LLC. However, the index updates and log writes increase with the number of workloads, as there is one history generator core per workload. While log writes increase the fetch traffic by 1.1%, index updates, which only increase the traffic in the LLC tag array, correspond to 15% of the baseline fetch accesses.</p><p>Overall, we conclude that in the presence of multiple workloads, SHIFT's benefits are unperturbed and remain comparable to the single-workload case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Performance Density Implications</head><p>To quantify the performance benefits of the different prefetchers as a function of their area cost, we compare SHIFT's performancedensity (PD) with PIF_32K and PIF_2K. We consider the three core designs described in Section 2.3 namely, Fat-OoO, Lean-OoO, and Lean-IO.</p><p>SHIFT improves performance-density over PIF_32K as a result of eliminating the per-core instruction history, while retaining similar performance benefits. Compared to PIF_32K, SHIFT improves the overall performance by 16 to 20%, depending on the core type, at a negligible area cost per core (0.96mm 2 in total, as opposed to PIF_32K's 14.4mm 2 cost in aggregate in a 16-core CMP). While PIF_32K offers higher absolute performance, the relative benefit is diminished due to the high area cost. As a result, SHIFT improves PD over PIF_32K for all three core microarchitectures. As expected, the biggest PD improvement is registered for lean cores (16% and 59% for Lean-OoO and Lean-IO respectively); however, even the fat-core design enjoys a 2% improvement in PD due to SHIFT's low area cost.</p><p>In comparison to PIF_2K, SHIFT achieves higher miss coverage due to the better capacity management of the aggregate history buffer storage. Although PIF_2K occupies the same aggregate area as SHIFT, SHIFT almost doubles the performance improvement for the three core types, as a result of its higher miss coverage. Consequently, SHIFT improves performance density over PIF_2K by around 9% on average for all the core types.</p><p>SHIFT improves the absolute performance-density for both leancore designs and the fat-core design over a no-prefetch system, while providing 98% of performance of the state-of-the-art instruction prefetcher, demonstrating the feasibility of area-efficient highperformance instruction prefetching for servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Power Implications</head><p>SHIFT introduces power overhead to the baseline system due to two factors: (1) history buffer reads and writes to/from the LLC and (2) index reads and writes to/from the LLC. To quantify the overall power overhead induced by SHIFT, we use CACTI <ref type="bibr" target="#b24">[24]</ref> to estimate the LLC power (both for index pointers in the tag array and history buffers in the data array) and custom NoC power models to estimate the link, router switch fabric and buffer power in the NoC <ref type="bibr" target="#b21">[21]</ref>. We find the additional power overhead due to history buffer and index activities in the LLC to be less than 150mW in total for a 16-core CMP. This corresponds to less than 2% power increase per Lean-IO core, which is the lowest-power core evaluated in our studies. We thus conclude that the power consumption due to SHIFT is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSIONS 6.1 Choice of History Generator Core</head><p>We show SHIFT's miss coverage and performance improvement by employing one history generator core picked at random throughout the paper. In our experience, in a sixteen-core system, there is no sensitivity to the choice of the history generator core.</p><p>Although the cores executing a homogeneous server workload exhibit common temporal instruction streams, there are also spontaneous events that might take place both in the core generating the shared instruction history and the cores reading from the shared instruction history, such as the OS scheduler, TLB miss handlers, garbage collector, and hardware interrupts. In our experience, such events are rare and only briefly hurt the instruction cache miss coverage due to the pollution and fragmentation of temporal streams.</p><p>In case of a long-lasting deviation in the program control flow of the history generator core, a sampling mechanism that monitors the instruction miss coverage and changes the history generator core accordingly can overcome the disturbance in the shared instruction history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Virtualized PIF</head><p>Although virtualization could be readily used with prefetchers using per-core history, such designs would induce high capacity and bandwidth pressure in the LLC. For example, virtualizing PIF's per-core history buffers would require 2.7MB of LLC capacity and this requirement grows linearly with the number of cores. Furthermore, as each core records its own history, the bandwidth and power consumption in the LLC also increase linearly with the number of cores. By sharing the instruction prefetcher history, SHIFT not only saves area but also minimizes the pressure on the LLC compared to virtualized per-core instruction prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Instruction fetch stalls have long been recognized as a dominant performance bottleneck in servers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b41">41]</ref>. Simple next-line instruction prefetchers varying in prefetch degree have been ubiquitously employed in commercial processors to eliminate misses to subsequent blocks <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref> and fail to eliminate instruction cache misses due to discontinuities in the program control flow caused by function calls, taken branches and interrupts.</p><p>A class of instruction prefetchers rely on the branch predictor running ahead of the fetch unit to predict instruction cache misses caused by discontinuities <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b42">42]</ref>. To explore future code paths, an idle thread <ref type="bibr" target="#b20">[20]</ref>, a helper thread <ref type="bibr" target="#b0">[1]</ref>, speculative threading mechanisms <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b48">48]</ref> or run-ahead execution <ref type="bibr" target="#b25">[25]</ref> can be employed. Although these techniques do not require additional history, they are limited by the lookahead and accuracy of the branch predictor.</p><p>To overcome the lookahead limitation, the discontinuity prefetcher <ref type="bibr" target="#b35">[35]</ref> maintains the history of discontinuous transitions between two instruction blocks. However, the lookahead of the discontinuity prefetcher is limited to one target instruction block for each source block. TIFS <ref type="bibr" target="#b14">[15]</ref> records streams of discontinuities in its history, enhancing the lookahead of discontinuity prefetching. PIF <ref type="bibr" target="#b13">[14]</ref>, records the complete retire-order instruction cache access history, capturing both discontinuities and next-line misses. SHIFT maintains the retire-order instruction cache access history like PIF. Unlike prior stream-based instruction prefetchers, SHIFT maintains a single shared history, allowing all cores running a common workload to use the shared history to predict future instruction misses.</p><p>The SHIFT design adopts its key history record and replay mechanisms from previously proposed per-core data and instruction prefetchers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b43">43]</ref>. To facilitate sharing the instruction history, SHIFT embeds the history buffer in the LLC as proposed in predictor virtualization <ref type="bibr" target="#b7">[8]</ref>.</p><p>A number of orthogonal studies mitigate instruction cache misses by exploiting code commonality across multiple threads <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. These approaches distribute the code footprint across private instruction caches to leverage the aggregate on-chip instruction cache capacity. In a similar manner, SHIFT relies on the code path commonality, but it does not depend on the aggregate instruction cache capacity, which might be insufficient to accommodate large instruction footprints. Moreover, SHIFT supports multiple workloads running concurrently, while these techniques might lose their effectiveness due to the contention for instruction cache capacity in the presence of multiple workloads.</p><p>Another way to exploit the code commonality across multiple threads is to group similar requests and time-multiplex their execution on a single core, so that the threads in a group can reuse instructions, which are already brought into the instruction cache by the lead thread <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">17]</ref>. Unfortunately, these approaches are likely to hurt response latency of individual threads, as each thread is queued for execution and has to wait for the other threads in the group to execute.</p><p>Prior software-base approaches proposed optimizing the code layout to avoid conflict misses <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b47">47]</ref>, inserting instruction prefetch instructions at compile time <ref type="bibr" target="#b23">[23]</ref>, and exploiting the recurring call-graph history <ref type="bibr" target="#b2">[3]</ref>. These techniques can be used with SHIFT to further improve instruction miss coverage and reduce the storage cost.</p><p>Concurrent with our work, RDIP <ref type="bibr" target="#b19">[19]</ref> correlates instruction cache miss sequences with the call stack content. RDIP associates the history of miss sequences with a signature, which summarizes the return address stack content. In doing so, RDIP reduces the history storage requirements by not recording the entire instruction streams as in stream-based prefetchers. However, RDIP's miss coverage is still limited by the amount of per-core storage. SHIFT, on the other hand, amortizes the cost of the entire instruction stream history across multiple cores, obviating the need for percore history storage reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>Instruction fetch stalls are a well-known cause of performance loss in server processors due to the large instruction working sets of server workloads. Sophisticated instruction prefetch mechanisms developed by researchers specifically for this workload class have been shown to be highly effective in mitigating the instruction stall bottleneck by recording, and subsequently replaying, entire instruction sequences. However, for high miss coverage, existing prefetchers require prohibitive storage for the instruction history due to the large instruction working sets and complex control flow. While high storage overhead is acceptable in fat-core processors whose area requirements dwarf those of the prefetcher, we find the opposite to be true in lean-core server chips.</p><p>This work confronted the problem of high storage overhead in stream-based prefetchers. We observed that the instruction history among all of the cores executing a server workload exhibits significant commonality and showed that it is amenable to sharing. Building on this insight, we introduced SHIFT -a shared history instruction prefetcher. SHIFT records the instruction access history of a single core and shares it among all of the cores running the same workload. In a 16-core CMP, SHIFT delivers over 90% of the performance benefit of PIF, a state-of-the-art instruction prefetcher, while largely eliminating PIF's prohibitive per-core storage overhead. With a lean in-order core microarchitecture, SHIFT improves performance per mm 2 by up to 59% compared to PIF, indicating SHIFT's advantage in lean-core processor designs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Speedup as a function of cache misses eliminated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3.Instruction cache accesses within common temporal streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. SHIFT's logical components and data flow to (a) record and (b) replay temporal instruction streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. SHIFT's virtualized history and data flow to (a) record and (b) replay temporal instruction streams. Virtualized history components (index pointers and shared history buffer) are shaded in the LLC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Percentage of instruction misses predicted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 8. Performance comparison.</figDesc><graphic url="image-27.png" coords="9,101.09,79.76,188.35,91.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Speedup for workload consolidation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Comparison of PIF [14] area overhead and performance improvement for various core types.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>PD Gain</cell><cell>PD Loss</cell></row><row><cell></cell><cell>1.3</cell><cell></cell><cell></cell></row><row><cell>Relative Performance</cell><cell>1.1 1.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>1.25</cell><cell>1.5</cell><cell>1.75</cell></row><row><cell></cell><cell></cell><cell>Relative Area</cell><cell></cell></row><row><cell cols="2">Figure 2.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table contains</head><label>contains</label><figDesc></figDesc><table><row><cell>Processing Nodes</cell><cell>UltraSPARC III ISA, sixteen 2GHz cores Fat-OoO (25mm 2 ): 4-wide dispatch/retire-ment, 128-entry ROB, 32-entry LSQ Lean-OoO (4.5mm 2 ): 3-wide dispatch/retire-ment, 60-entry ROB, 16-entry LSQ Lean-IO (1.3mm 2 ): 2-wide dispatch/retirement</cell><cell cols="2">OLTP -Online Transaction Processing (TPC-C) DB2 IBM DB2 v8 ESE, 100 warehouses (10GB), 64 clients, 2 GB buffer pool Oracle Oracle 10g Enterprise Database Server, 100 warehouses (10 GB), 16 clients, 1.4 GB SGA DSS -Decision Support Systems (TPC-H)</cell></row><row><cell>I-Fetch Unit L1D Caches L2 NUCA Cache Main Memory Interconnect</cell><cell>32KB, 2-way, 64B-blocks, 2-cycle load-to-use L1-I cache Hybrid branch predictor (16K gShare &amp; 16K bimodal) 32KB, 2-way, 64B blocks, 2-cycle load-to-use, 32 MSHRs Unified, 512KB per core, 16-way, 64B blocks, 16 banks, 5-cycle hit latency, 64 MSHRs 45 ns access latency 4x4 2D mesh, 3 cycles/hop</cell><cell>Qry 2, Qry 17 Darwin Apache Nutch</cell><cell>IBM DB2 v8 ESE, 480MB buffer pool, 1GB database Media Streaming Darwin Streaming Server 6.0.3, 7500 clients, 60GB dataset, high bitrates Web Frontend (SPECweb99) Apache HTTP Server v2.0, 16K connections, fastCGI, worker threading model Web Search Nutch 1.2/Lucene 3.0.1, 230 clients, 1.4 GB index, 15 GB data segment</cell></row><row><cell></cell><cell></cell><cell></cell><cell>an instruction block</cell></row><row><cell></cell><cell></cell><cell cols="2">address (34 bits) and a pointer to the history buffer (15 bits) adding</cell></row><row><cell></cell><cell></cell><cell cols="2">up to 49 bits per entry. According to our sensitivity analysis, the</cell></row><row><cell></cell><cell></cell><cell cols="2">index table requires 8K entries for the target 90% instruction miss</cell></row><row><cell></cell><cell></cell><cell cols="2">coverage. The actual storage required for the 8K entry index table</cell></row><row><cell></cell><cell></cell><cell cols="2">is 49KB per core. PIF's per-core history buffer and index table together occupy 0.9mm 2 area.</cell></row><row><cell></cell><cell></cell><cell cols="2">We also evaluate a PIF design with a total storage cost equal to that</cell></row><row><cell></cell><cell></cell><cell cols="2">of SHIFT. Since SHIFT stores the history buffer entries inside</cell></row></table><note><p>existing LLC cache blocks, its only source of storage overhead is the 240KB index table embedded in the LLC tag array. An equalcost PIF design affords 2K spatial region records in the history buffer and 512 entries in the index table per core. We refer to this</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table I . System and application parameters.</head><label>I</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9.">ACKNOWLEDGEMENTS</head><p>The authors would like to thank <rs type="person">Christos Kozyrakis</rs>, <rs type="person">Onur Kocberber</rs>, <rs type="person">Stavros Volos</rs>, <rs type="person">Djordje Jevdjic</rs>, <rs type="person">Javier Picorel</rs>, <rs type="person">Almutaz Adileh</rs>, <rs type="person">Pejman Lotfi-Kamran</rs>, <rs type="person">Sotiria Fytraki</rs>, and the anonymous reviewers for their insightful feedback on earlier drafts of this paper. This work was partially supported by <rs type="funder">Swiss National Science Foundation</rs>, Project No. <rs type="grantNumber">200021_127021</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kTBakax">
					<idno type="grant-number">200021_127021</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hardware support for prescient instruction prefetch</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hammarlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High Performance Computer Architecture</title>
		<meeting>the International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2004-02">Feb. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DBMSs on a modern processor: Where does time go?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Very Large Data Bases</title>
		<meeting>the International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="1999-09">Sept. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Call graph prefetching for database applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SLICC: Self-assembly of instruction cache collectives for OLTP workloads</title>
		<author>
			<persName><forename type="first">I</forename><surname>Atta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>T?z?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
		<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">STREX: Boosting instruction cache reuse in OLTP workloads through stratified execution</title>
		<author>
			<persName><forename type="first">I</forename><surname>Atta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>T?z?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The F1: T1&apos;s 65nm Cortex-A8</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocessor Report</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Many-core key-value store</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berezecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Steele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Green Computing Conference and Workshops</title>
		<meeting>the International Green Computing Conference and Workshops</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predictor virtualization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Burcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="http://www.calxeda.com/" />
		<title level="m">Calxeda</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detailed characterization of a Quad Pentium Pro server running TPC-D</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knighten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Won</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Design</title>
		<imprint>
			<date type="published" when="1999-10">Oct. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Computation spreading: Employing hardware migration to specialize CMP cores on-the-fly</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Instruction prefetching using branch prediction information</title>
		<author>
			<persName><forename type="first">I.-C</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Design</title>
		<meeting>the International Conference on Computer Design</meeting>
		<imprint>
			<date type="published" when="1997-10">Oct. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Proactive instruction fetch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
		<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
		<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mancheril</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Database servers on chip multiprocessors: Limitations and opportunities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Innovative Data Systems Research</title>
		<meeting>the Conference on Innovative Data Systems Research</meeting>
		<imprint>
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">STEPS towards cache-resident transaction processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Harizopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Very Large Data Bases</title>
		<meeting>the International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Performance characterization of a Quad Pentium Pro SMP using OLTP workloads</title>
		<author>
			<persName><forename type="first">K</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RDIP: Return-addressstack directed instruction prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenisch</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
		<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An analysis of database workload performance on simultaneous multithreaded processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Parekh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">NOC-Out: Microarchitecting a scale-out processor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
		<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scale-out processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Idgunji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cooperative prefetching: Compiler and hardware support for effective instruction prefetching in modern processors</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
		<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1998-12">Dec. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
		<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Runahead execution: An effective alternative to large instruction windows</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="2003-12">Nov.-Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High Performance Computer Architecture</title>
		<meeting>the International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2004-02">Feb. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Code layout optimizations for transaction processing workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fetching instruction streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
		<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performance of database workloads on sharedmemory systems with out-of-order processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Adve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
		<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1999-12">Dec. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vantage: Scalable and efficient fine-grain cache partitioning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enlarging instruction streams</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1342" to="1357" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatio-temporal memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Effective instruction prefetching in chip multiprocessors for modern commercial applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High Performance Computer Architecture</title>
		<meeting>the International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Branch history guided instruction prefetching</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High Performance Computer Architecture</title>
		<meeting>the International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-01">Jan. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Slipstream processors: Improving both performance and fault tolerance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Purser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimizing instruction cache performance for operating system intensive workloads</title>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daigle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High Performance Computer Architecture</title>
		<meeting>the International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">From A to E: Analyzing TPC&apos;s OLTP benchmarks: the obsolete, the ubiquitous, the unexplored</title>
		<author>
			<persName><forename type="first">P</forename><surname>T?z?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Extending Database Technology</title>
		<meeting>the International Conference on Extending Database Technology</meeting>
		<imprint>
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cortex-A15</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eagle&quot; flies the coop. Microprocessor Report</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Instruction fetching: Coping with code bloat</title>
		<author>
			<persName><forename type="first">R</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sechrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Instruction cache prefetching using multilevel branch prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High-Performance Computing</title>
		<meeting>the International Symposium on High-Performance Computing</meeting>
		<imprint>
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal streaming of shared memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SimFlex: Statistical sampling of computer system simulation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2006-08">July-Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tilera sees opening in clouds</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wheeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocessor Report</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="13" to="16" />
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SMARTS: Accelerating microarchitecture simulation via rigorous statistical sampling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Instruction prefetching of systems codes with layout optimized for reduced cache misses</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1996-06">June 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Execution-based prediction using speculative slices</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
