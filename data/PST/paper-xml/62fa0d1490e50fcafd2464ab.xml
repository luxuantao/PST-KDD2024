<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoShard: Automated Embedding Table Sharding for Recommender Systems</title>
				<funder ref="#_ubbxfE5">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-12">12 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
							<email>daochen.zha@rice.edu</email>
						</author>
						<author>
							<persName><forename type="first">Louis</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bhargav</forename><surname>Bhushanam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jade</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jay</forename><surname>Chae</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yinbin</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arun</forename><surname>Kejariwal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<email>xia.hu@rice.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Rice University Houston</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Meta Platforms, Inc</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Meta Platforms, Inc</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Meta Platforms, Inc</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Meta Platforms, Inc</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Rice University Houston</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AutoShard: Automated Embedding Table Sharding for Recommender Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-12">12 Aug 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539034</idno>
					<idno type="arXiv">arXiv:2208.06399v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender System</term>
					<term>Reinforcement Learning</term>
					<term>Cost Modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embedding learning is an important technique in deep recommendation models to map categorical features to dense vectors. However, the embedding tables often demand an extremely large number of parameters, which become the storage and efficiency bottlenecks. Distributed training solutions have been adopted to partition the embedding tables into multiple devices. However, the embedding tables can easily lead to imbalances if not carefully partitioned. This is a significant design challenge of distributed systems named embedding table sharding, i.e., how we should partition the embedding tables to balance the costs across devices, which is a non-trivial task because 1) it is hard to efficiently and precisely measure the cost, and 2) the partition problem is known to be NP-hard. In this work, we introduce our novel practice in Meta, namely AutoShard, which uses a neural cost model to directly predict the multi-table costs and leverages deep reinforcement learning to solve the partition problem. Experimental results on an open-sourced large-scale synthetic dataset and Meta's production dataset demonstrate the superiority of AutoShard over the heuristics. Moreover, the learned policy of AutoShard can transfer to sharding tasks with various numbers of tables and different ratios of the unseen tables without any fine-tuning. Furthermore, AutoShard can efficiently shard hundreds of tables in seconds. The effectiveness, transferability, and efficiency of AutoShard make it desirable for production use. Our algorithms have been deployed in Meta production environment. A prototype is available at https://github.com/daochenzha/autoshard</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Reinforcement learning; Machine learning approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Embedding learning has become an important technique for modeling categorical features in deep recommendation models <ref type="bibr" target="#b36">[37]</ref>. It maps sparse categorical features into dense vectors by performing embedding lookup in embedding tables. The learned vectors are then used for complex feature interactions and can greatly help us improve the prediction results (e.g., DeepFM <ref type="bibr" target="#b3">[4]</ref>, AutoInt <ref type="bibr" target="#b28">[29]</ref>, and deep learning recommendation model (DLRM) <ref type="bibr" target="#b24">[25]</ref>).</p><p>However, industrial recommendation models often demand an extremely large number of parameters for embedding tables, which become the storage and efficiency bottlenecks <ref type="bibr" target="#b37">[38]</ref>. A typical example is YouTube Recommendation Systems <ref type="bibr" target="#b4">[5]</ref>, where a single categorical feature contains tens of millions of video IDs, which leads to gigantic embedding tables. The ultra-large embedding tables also result in training efficiency problems. For instance, more than 48% of the operation kernel time is spent on embedding tables in a Meta production model (see Figure <ref type="figure">2</ref> for the breakdown). Similar observations are also reported in <ref type="bibr" target="#b1">[2]</ref>, showing that embedding table sizes have a significant impact on the training throughput. The memory and efficiency requirements motivate the distributed training solutions, where model-parallelism is exploited to partition and feed the embedding tables into multiple devices [1, <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref>. The embedding lookup for a certain index will then be performed by querying the device that actually holds the corresponding table.</p><p>While the model-parallelism enables training models with very large embedding sizes, it poses a significant design challenge named embedding table sharding, i.e., how we should partition the embedding tables across devices. Figure <ref type="figure">1</ref> presents an illustrative example of why optimizing the sharding can significantly accelerate the training. If not carefully partitioned (left-hand side of Figure <ref type="figure">1</ref>), the tables could lead to imbalances among GPUs, where all the GPUs Bal an ced Sh ar di n g Figure <ref type="figure">1</ref>: An illustrative sharding problem of partitioning 9 embedding tables across 3 devices with naive sharding and balanced sharding. Blue blocks are embedding tables, whose numbers indicate the costs measured by the operation execution time. Purple blocks are shards, whose costs are usually smaller than the sum of the table costs within the shard due to parallelism (e.g., 5 &lt; 1+2+3 and 10 &lt; 4+5+6). The slowest shard will become the bottleneck since the other shards have to wait until it finishes. Optimizing the naive sharding in this task can achieve 1.5X speedup (i.e., 15/10 = 1.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0% 31% 64% 100%</head><p>Em bedding Oper ations Other Oper ations Other Cost (com m unication, tensor copy, etc.)</p><p>Figure <ref type="figure">2</ref>: A breakdown of the computation time of one iteration of a Meta production model. Embedding operations account for 31% of the iteration time and 48% of the total operation execution time in GPU (i.e., 0.31 / 0.64 ? 48%).</p><p>are forced to wait for the slowest GPU 1 . In contrast, a balanced sharding (right-hand side of Figure <ref type="figure">1</ref>) can significantly accelerate the embedding operation by reducing the waiting time. Motivated by this, we investigate the following research question: given an arbitrary set of embedding tables, how can we shard the embedding tables to balance the costs across devices?</p><p>It is non-trivial to achieve this goal because of two major challenges. First, we need to efficiently estimate the cost (i.e., the kernel time of the embedding operators), which serves as the optimization objective. Unfortunately, the cost is hard to estimate. Unlike many other partition problem, the total cost of multiple tables in a shard is not the sum of the single table costs within the shard due to parallelism and operator fusion. As a result, it is hard to estimate the cost without actually running the operators; however, running the operators is computationally expensive. Second, we need an efficient algorithm to solve the partition problem, which is known to be NP-hard 2 . The ever-increasing number of embedding tables makes it infeasible to adopt a brute-force approach, i.e., iterating through all the possible sharding plans and outputting the best one. For practical use, the sharding algorithm is expected to propose an effective sharding plan for hundreds of tables in a reasonable time.</p><p>To address the above challenges, we present our novel practice in Meta, namely AutoShard, based on cost modeling and deep reinforcement learning (RL). To efficiently estimate the cost, we develop 1 In this work, we focus on embedding table sharding among GPU devices when the tables fit on the GPU memory. We note that system memory <ref type="bibr" target="#b11">[12]</ref>, non-volatile memory <ref type="bibr" target="#b10">[11]</ref>, and SSD <ref type="bibr" target="#b37">[38]</ref> can also be used to store tables at the cost of decreased throughput <ref type="bibr" target="#b1">[2]</ref>. We defer hybrid sharding strategies for these scenarios to future work. a neural cost model, which is trained with regression to the multitable cost data collected from running micro-benchmarks on GPUs.</p><p>To optimize the partitioning, we formulate the table sharding as a Markov decision process (MDP), where in each step, we allocate one table to a shard. The process ends when all the tables are allocated and we obtain a reward indicating the sharding quality at the final step. Then we leverage deep RL to learn an LSTM policy to optimize the sharding strategy. The cost model and the sharding policy are jointly trained towards convergence. In summary, we make the following contributions:</p><p>? Provide an in-depth analysis of the main influential factors of the cost of embedding operators via a case study on an modern embedding bag implementation from FBGEMM <ref type="bibr" target="#b15">[16]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We start with a background of distributed recommender systems, and then formulate the embedding table sharding problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training Deep Learning Recommendation Models at Scale</head><p>Industrial recommendation models usually require massive memory and high training throughput. Therefore, distributed training is employed in large-scale industrial systems [1, <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>. We take DLRM 3 <ref type="bibr" target="#b23">[24]</ref> as an example to introduce the system design. Figure <ref type="figure" target="#fig_1">3</ref> depicts an overview of DLRM. The system exhibits a combination of data parallelism and model parallelism. To exploit data parallelism, each trainer will hold a copy of MLP layers, which will be trained on its own mini-batch of data. The model parameters are updated in the fully synchronous mode. For embedding tables, model parallelism is adopted to shard the tables into multiple devices. In the forward pass of embedding lookup, each device will perform a lookup for all the indices concerning the tables in the device (including the indices from the other devices' mini-batches). The obtained vectors will be transferred to the corresponding devices via an all-to-all communication. Then each device will receive all the embedding vectors for its own mini-batch, which will be interacted with the dense features, followed by an MLP layer for predictions. In the backward pass, the gradients will be similarly transferred with another all-to-all communication so that each device will receive all the gradients for the tables within the device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Formulation</head><p>In the production environment, the possible embedding tables will often stay unchanged for a period of time since the raw features for a recommendation task are relatively steady. Nevertheless, different subsets of the tables could be used to build recommendation models. For example, a machine learning engineer may conduct feature selection by experimenting with different table combinations.</p><p>Following the above intuition, we formalize the embedding table sharding problem. Let T ? = {? 1 ,? 2 , ...,? ? } be a pool of embedding tables, where ? is the total number of tables in the pool. A sharding task ? can be represented as a triple ? = (T , D, M), where T ? T ? is subset of the tables, D = {1, 2, ..., ? } is a set of shard IDs with ? shards in total, and M = {? 1 , ? 2 , ..., ? ? } is the memory 3 https://github.com/facebookresearch/dlrm constraints for all the shards. A sharding plan ? can be represented as a mapping from each table to a shard. Then each device will get its own shard to process, which leads to a set of actual memory usages M = { M1 , M2 , ..., M? } (which is obtained by summing the tables sizes in each shard) and a set of costs C = {? 1 , ? 2 , ...? ? } in terms of operation execution time. Embedding table sharding aims to optimize the sharding plan ? such that the maximum cost across shards is minimized subject to the memory constraints:</p><formula xml:id="formula_0">min ? max(C) := max ? ? ? s.t. M? ? ? ? , ?? ? D.</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSIS OF EMBEDDING TABLE COST</head><p>This section analyzes the table costs on a modern embedding bag implementation from FBGEMM<ref type="foot" target="#foot_0">4</ref>  <ref type="bibr" target="#b15">[16]</ref> with a 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Analysis of Single-Table Cost</head><p>The cost of a table is mainly determined by the characteristics of the table itself and indices lookup. We first visualize the impact of table characteristics. We fix the pooling factor to be 32 and the indices to be uniformly distributed. Then we vary the hash size and table dimension to plot the kernel time in the left-hand side of Figure <ref type="figure">4</ref>. As expected, we can see that a higher dimension will significantly increase the kernel time. This is because the dimension is positively correlated to the amount of data to be fetched. An interesting observation is that the hash size only has a moderate impact on the cost, which could be partly explained by the ? (1) time complexity for hash table lookup.</p><p>Similarly, we study the impact of indices lookup by fixing hash size to be 10 6 and dimension to be 32. For the indices distribution, some indices could be accessed far more frequently than others <ref type="bibr" target="#b1">[2]</ref>. We simulate this behavior by restricting the indices access to a subset of the indices with a pre-defined accessed indices ratio. For example, a ratio of 1.0 is equivalent to the uniform distribution, while a ratio of 10 -2 suggests only one percent of the indices will be accessed, which means the indices are sparsely distributed and those one percent of the indices are warm indices. The middle of Figure <ref type="figure">4</ref> shows the impact of the pooling factor and accessed indices ratio. A larger pooling factor will significantly increase the time, which is expected since it indicates more indices per lookup. In contrast, a sparse indices distribution tends to decrease the time.</p><p>We speculate that this is caused by the caching mechanism.</p><p>The above analysis motivates a series of greedy sharding algorithms, which will be elaborated in Section 5.1. However, the designed heuristics are sub-optimal since the actual running time has non-linear and complex relationships with these four factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis of Multi-Table Cost</head><p>Since we usually have multiple tables in a shard, we need to estimate the multi-table cost. A naive way is to sum the costs of the single tables within the shard (single-sum for short). However, this is inaccurate due to the parallelism of GPU. The right-hand side of Figure <ref type="figure">4</ref> plots the multi-table cost versus the single-sum of 50 randomly sampled table combinations from MetaSyn (see Section 5.1 for dataset details), where each sample contains 10 tables. First, we observe that the multi-table cost is significantly smaller than the single-sum. This is because the tables can be batched and accelerated with parallelism. Second, while the single-sum is positively correlated with the multi-table cost in general, it is still a poor estimator in many cases. The discrepancy between the multi-table cost and single-sum will amplify the difficulty of cost estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>An overview of AutoShard is shown in Figure <ref type="figure">5</ref>. It consists of four modules: 1) a micro-benchmark that measures actual costs of embedding operators (Section 4.1), 2) a cost model which approximates multi-table costs based on the data collected from microbenchmark (Section 4.2), 3) an environment that formulates the sharding process as a Markov Decision Process (MDP) by allocating one table in each step (Section 4.3), and 4) an RL policy that optimizes the sharding strategies in a trial-and-error fashion (Section 4.4). Finally, we summarize the training procedure in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Micro-Benchmarking Embedding Operators</head><p>This subsection introduces how we efficiently and precisely measure the latency of embedding tables. Since we only need the latency of embedding operators, we design and implement a microbenchmark, which only benchmarks embedding operators alone. Due to space limitation, we introduce the main steps of microbenchmarking here and provide more details in Appendix B.1: 1) initialization: we initialize the operators with the specified arguments of embedding tables and load the indices data. 2) warmup: we run the embedding operator several times to warm up the device to allow the CUDA to complete the necessary preparation for the operator. 3) benchmarking: run embedding operator several times and return the mean latency<ref type="foot" target="#foot_1">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modeling Multi-Table Costs</head><p>While micro-benchmark can accurately and efficiently measure the latency, it still needs to run the operators, which remains computationally expensive for production use. To further accelerate the cost estimation, we develop a neural cost model to predict the operator cost based on the data collected from the micro-benchmark. The neural cost model can be easily deployed since it only requires a forward pass of a shallow neural network to obtain the cost.</p><p>Cost estimation can be formulated as a regression task, which takes as input the features of multiple tables and outputs the latency. The lower right corner of Figure <ref type="figure">5</ref> illustrates the neural architecture of the cost model. For each table, we use a shared MLP (green) to generate the single table representations based on the table features. Then we sum up the single table representations to obtain a multi-table representation (we have tried other reductions, such as max and mean, and found that sum works better), followed by another MLP to predict the cost. This design can flexibly accommodate different numbers of tables and obtain a final representation with a fixed dimension. We empirically use the following features: table dimension, hash size, pooling factor, table size, and indices distributions (17 features). We provide more details of these features in Appendix B.2. Formally, let (X, y) be the collected data. X is the table features where each row represents the features of multiple tables and has variable lengths; y denotes a vector of ground-truth costs obtained by running micro-benchmark on GPUs. We train the cost model ? with mean squared error (MSE) loss ? cost = (y -? (X)) 2 . The performance of the cost model is reported in Table <ref type="table" target="#tab_10">3</ref> of Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Formulating Sharding as MDP</head><p>This subsection describes why and how we formulate the sharding procedure as a sequential decision process. A naive strategy to solve the sharding problem is to treat it as a black-box optimization problem, where we sample and evaluate a sharding plan in each iteration. However, this will lead to an extremely large search space since each table can be possibly assigned to any shard.</p><p>To tackle this problem, we decompose the generation of a sharding plan into multiple steps, where we only assign one table to a shard in each step. Then after scanning all the tables, we can We formulate the above process as an MDP with the state, action, and reward defined as follows. State: The features of the upcoming table and a step-aware feature which is the ratio of the remaining tables to be assigned. Action: The shard IDs with ? actions in total. The multi-table cost features and the predicted costs from the cost model can serve as the action features. Reward: The agent will receive zero rewards for all the intermediate steps and a final reward indicating the quality of the sharding plan. Specifically, if the sharding plan meets the memory constraint, we run the microbenchmark to obtain the shard latencies of all the shards. The reward is calculated by the ratio between the maximum latency and minimum latency, i.e., ???(C)/??? (C), to encourage the agent to balance the costs across shards. The reward is in the range of [0, 1], where a higher reward suggests a better balance. Alternatively, if the sharding plan cannot meet the memory constraints, we penalize this behavior with a negative reward, which is determined by the shard that is the most seriously affected by the memory explosion, i.e., max ? (( M? -? ? )/? ? ). A higher reward indicates that the sharding plan is closer to meeting the memory constraints.</p><formula xml:id="formula_1">1 2 3 4 1 2 1 1 1 1 2 3 2 1 2 3 4 M i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Optimizing Sharding Strategy with RL</head><p>This subsection presents how we solve the above MDP with RL. We will first elaborate on the neural architecture design and then describe how we train the model weights.</p><p>Figure <ref type="figure">6</ref> illustrates the neural architecture and how it makes predictions to shard tables. The model takes as input the state features and actions features, and outputs an action probability vector where  each action corresponds to a shard ID (policy head) and a scalar value indicating the value of the state (value head). The model is instantiated with a two-layer LSTM to process the state and actions with the following procedure. 1) The state representation is obtained by combining the table features and step-aware features (dotted circle in the left).</p><p>2) The state representations will be fed into the LSTM sequentially so that historical state information will be encoded into LSTM output as well.</p><p>3) The multi-table representations obtained in the cost model will be concatenated with the predicted costs from the cost model to construct the action representations (dotted circle in the right). 4) Each action representation will be concatenated with the state representation, followed by an MLP (which is shared for all actions) to produce a confidence score for the action, shown in Figure <ref type="figure" target="#fig_3">7</ref>. 5) The scores for all the actions will be processed by a Softmax layer to obtain the probability vector, where the probabilities sum to one. Similarly, the LSTM output will be fed into another MLP to generate a state value in the value head.</p><p>For an upcoming table, the model will sample a shard ID (action) based on the probabilities from the policy head. The tables will be allocated to the shards one by one following this procedure. We train the model with IMPALA <ref type="bibr" target="#b6">[7]</ref>, a distributed actor-critic algorithm enhanced by V-trace targets to achieve off-policy learning. The V-trace correction can tackle delayed model weights update, which is helpful in our problem in that evaluating a sharding plan is slow and may result in substantial delays. Here, we only briefly introduce IMPALA since RL itself is not our focus; one can adopt other RL algorithms as well under our framework. We first introduce the V-trace targets and then describe the loss for updating the policy and value heads. Let ? ? , ? ? , and ? ? be the state, action, and reward at step ?, respectively. We consider an n-step trajectory (? ? , ? ? , ? ? ) ? =? ? +? ? =? ? . The V-trace target for ? ? ? is defined as</p><formula xml:id="formula_2">? target (? ? ? ) = ? (? ? ? ) + ? ? +?-1 ?? ? =? ? ? ? -? ? (? ? -1 ?=? ??? )? ? ? ,<label>(2)</label></formula><p>where ? (? ? ? ) is the output of the value head for ? ? ? , ? ? ? = ? ? (? ? + ?? (? ? +1 ) -? (? ? )) is the temporal difference, and ? ? and ? ? are truncated importance sampling weights that tackle the delayed update of the model. Then the loss at step ? is defined as</p><formula xml:id="formula_3">? ? = ? ? log ? (? ? |? ? )(? ? +?? target (? ? +1 -? (? ? )) + 1 2 (? ? -? (? ? )) 2 , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where ? (? ? |? ? ) and ? (? ? ) correspond to policy and value heads, respectively. The training can be batched to update the losses for multiple steps at a time in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training of AutoShard</head><p>This subsection summarizes the overall training procedure. To improve the sample efficiency, the cost model and the LSTM policyvalue network are jointly trained with shared table representations and data. Specifically, the MLP for processing the tables features (i.e., the green parts in Figure <ref type="figure">5</ref> and Figure <ref type="figure">6</ref>) is shared. Similarly, the data collected by the RL agent will be reused to generate cost data to train the cost model. The whole training process is summarized in Algorithm 1. In each training iteration, we collect a batch of trajectories by interacting with the micro-benchmark to update the policy-value network (line 4). The collected data will be stored in a buffer and reused to train the cost model (line 8). Since the main bottleneck is data collection, we parallelize line 4 with multiple processes operating on different GPUs. Once the cost model and the policy-value network are trained, they can be directly applied to any new sharding tasks by sequentially predicting the shard IDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>The experiments are conducted on both synthetic datasets and production datasets at Meta. We aim to answer the following questions: Q1: How does AutoShard compare with the heuristic sharding strategies (Section 5.2)? Q2: How large is the search space of Au-toShard and can simple random search be competitive with it (Section 5.3)? Q3: How does each component of AutoShard contribute to the performance (Section 5.4)? Q4: Can AutoShard transfer to unseen tables, and sharding tasks with more tables (Section 5.5)? Q5: How efficient is the training/inference of AutoShard (Section 5.6)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets. The public recommendation datasets often cannot match the scale of the industrial models to enable a valid evaluation of embedding table sharding. For example, Criteo end if 11: end for of embedding tables with very large and diverse hash sizes and pooling factors. Since MetaSyn does not specify table dimensions, we randomly select a dimension for each table from {16, 32} (we purposely make the dimensions small so that our results can be reproduced on GPUs with 10 GB memory). For verification, we also conduct experiments on the Meta production embedding tables (MetaProd), which have a similar scale as MetaSyn except for larger table dimensions. We keep the details of MetaProd confidential. MetaSyn and MetaProd will serve as the table pools, where each sharding task is constructed by a randomly sampled subset of the tables. Intuitively, with more tables, we also need more devices so that the tables can fit on the GPU memory. We empirically set the number of devices to be 1/10 of the total number of tables for all the tasks since this setting can ensure sufficient memory for all the sharding algorithms on both MetaSyn and MetaProd; that is, we use 8 devices for 80 tables, and 16 devices for 160 tables, etc. We provide more details in Appendix A.</p><p>Heuristic Algorithms. We compare AutoShard against several deployed heuristic sharding algorithms. They mainly consist of two steps: (1) cost function: each table will be assigned an estimated cost, and (2) greedy algorithm: the tables are first sorted in descending order based on the costs. Starting from the table with the highest cost, greedy algorithm will assign tables one-by-one to the device with the lowest sum of the costs so far, so that each device will have roughly an equal sum of the costs in the end. We consider the heuristics with the following cost functions, which have been proven to show strong performance in prior work <ref type="bibr" target="#b25">[26]</ref>: the size (the product of dimension and hash size) of the table (size-greedy), the dimension of the table (dim-greedy), the product of the dimension and mean pooling factor of the table (lookup-greedy). We further include a random sharding baseline (rand).</p><p>Metrics. We evaluate the performance with the following metrics. Degree of balance: the ratio between the minimum latency and the maximum latency across shards. 100% suggests perfect balancing where each shard has equal latency, and 0% indicates the worst-case of load balance. Speedup: the speedup over random sharding which is the most naive strategy. Specifically, the speedup   is calculated by ??? (C ?????? )/??? (C), where C ?????? and C are the sets of actual embedding table costs of random sharding strategy and the sharding algorithm hand, respectively.</p><p>Implementation Details. All the hyperparameters are tuned based on MetaSyn and the same set of hyperparameters are used on MetaProd. Specifically, we set ? 1 = 8, ? 2 = 512, ? = 20, and ? = 100. We use the IMPALA implementation in <ref type="bibr" target="#b16">[17]</ref> with the default hyperparameters for RL training. We use 2080 Ti and V100 GPUs for MetaSyn and MetaProd, respectively. We run all the experiments five times with random seeds 0, 1, 2, 3, and 4 and report the means and standard deviations. We provide more details in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with the Heuristics</head><p>To study Q1, we conduct experiments on the tasks of sharding 80 tables to 8 devices. Specifically, we randomly sample 90 training tasks, where each task consists of 80 randomly sampled tables from the pool. Then we sample another 10 different tasks with the same procedure for the testing purpose. AutoShard is trained on the 90 training tasks. We collect the mean result on the same 10 testing tasks for all the algorithms. Note that we have purposely separated the training and testing tasks to test whether AutoShard can generalize to different table combinations from the pool.</p><p>We summarize the results in Figure <ref type="figure" target="#fig_5">8</ref>. We make the following observations. First, all the sharding algorithms outperform the random sharding, which is expected since random sharding may easily result in imbalances. Second, AutoShard performs significantly and consistently better than the heuristics on both synthetic and production data for both metrics, which demonstrates the effectiveness of AutoShard. Third, look-greedy appears to be the strongest heuristic algorithm. This is expected because it considers both table dimensions and pooling factors, which can essentially quantify the workload for the indices lookup. Nevertheless, there is still a clear gap between lookup-greedy and AutoShard. This is because AutoShard can achieve a more accurate estimation of the cost by considering indices distributions and multi-table costs, and leveraging RL to optimize the sharding process.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Random Search</head><p>To answer Q2, we implement a random search algorithm to understand the difficulty of identifying a strong sharding plan in the search space. We choose random search because it is shown to be a strong baseline in neural architecture search when the search space is restricted <ref type="bibr" target="#b18">[19]</ref>. Specifically, we treat the tables as decisions, whose possible choices are the device IDs, and use random search to optimize the degree of balance. We follow the setting in Section 5.2, which results in an extremely large search space. Note that search is infeasible in production because it requires lots of GPU resources. This experiment is designed solely for understanding of the massive search space of AutoShard. Figure <ref type="figure" target="#fig_7">9</ref> plots the performance of random search w.r.t. the number of samples. Although random search can achieve competitive performance with lookup-greedy after hours of searching, it is far behind the AutoShard, which verifies the difficulty of embedding table sharding. In contrast, AutoShard shows clear advantages in terms of both effectiveness and efficiency. It can achieve strong performance with only a forward pass without the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>For Q3, we consider several ablations: 1) we remove the cost model and only use raw features to train RL, 2) instead of sharding with RL, we greedily assign tables like the heuristics with the only difference that we use the cost model to estimate the cost, and 3) we remove each of the table features to study the feature importance.</p><p>Table <ref type="table" target="#tab_8">2</ref> summarizes the results. First, we observe a significant performance drop when removing the cost model, which verifies the necessity of cost modeling. Second, the pooling factor feature is a very important feature, which is expected since pooling factor can indicate the number of lookup indices. Finally, removing either of the features will degrade the performance, which suggests the designed features are complimentary.  Note that when the ratio is 0.0, the results are worse than those shown in Figure <ref type="figure" target="#fig_5">8</ref>. This is because we only use half of the tables in this experiment. A possible reason is that more tables can improve the generalization ability of AutoShard, which leads to better results on the testing tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis of Transferability</head><p>To investigate Q4, we evaluate the transferability of AutoShard on unseen tables and sharding tasks with more tables.</p><p>To test AutoShard on unseen tables, we split the original table pools in half, where the first sub-pool is used to train AutoShard, and the tables in the second sub-pool are unseen in training. Then we randomly mix the tables from the two sub-pools to construct the sharding tasks based on a specified ratio of unseen tables. A ratio of 0.0 suggests that all the tables are from the first sub-pool (i.e., all the tables in the sharding tasks are seen in training), while a ratio of 1.0 means all the tables are from the second sub-pool (i.e., all the tables are unseen). A high ratio will make the transferability task more challenging. We report the performance w.r.t. different ratios of unseen tables in Figure <ref type="figure" target="#fig_8">10</ref>. First, AutoShard only shows a moderate performance decrease when a part of the tables are unseen. Specifically, when the ratio is between 0.0 to 0.8, AutoShard can achieve at least 80% degree of balance and 1.6X speedup. Second, when all the tables are unseen (i.e., the ratio is 1.0), AutoShard can still achieve more than 70% degree of balance and around 1.6X speedup, which significantly outperform the baselines.</p><p>To test whether AutoShard can scale to hundreds of tables, we compare AutoShard with baselines on tasks that shard up to 800 tables to 80 GPU devices. Note that in this experiment we have not trained any new models but instead directly apply the model trained on 80 tables. We plot the results in Figure <ref type="figure" target="#fig_9">11</ref> and make the following observations. First, the degree of balance decreases for all the sharding algorithms. This is expected because the task becomes more challenging with more tables. Second, AutoShard can significantly outperform the baselines in all the settings. In particular, we surprisingly observe an increase in speedup with more tables. This is because random sharding will perform poorly with more tables. This again demonstrates the superiority of AutoShard.</p><p>Overall, we conclude that AutoShard can well transfer to unseen tables and hundreds of tables, making it a desirable choice in handling complex training tasks in the production environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analysis of Training/Inference Efficiency</head><p>We analyze the training and inference time to answer Q5. The lefthand side of Figure <ref type="figure" target="#fig_10">12</ref> plots the training curve of AutoShard on four GPUs. We observe that AutoShard can achieve more than 80% degree of balance within around 150 samples or 454 seconds. This is highly efficient for production use since we only need to train AutoShard offline periodically (e.g. we can run a daily or weekly training job). The right-hand side of Figure <ref type="figure" target="#fig_10">12</ref> shows the inference time with a single CPU core. AutoShard can shard hundreds of tables in seconds. This cost is neglectable in production use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Deep recommendation models. Deep-learning-based recommendation models have shown superior performance in many recommendation scenarios <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37]</ref>. Due to the ultra-large-scale of the data and the features in industrial applications, distributed training solutions have been developed to improve the training efficiency <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>. Despite these efforts, embedding table sharding remains to be a critical challenge for distributed training. AutoShard is the first learning-based sharding algorithm that can optimize the sharding strategy in an end-to-end fashion.</p><p>Tackling large embedding tables. How to deal with the ultralarge embedding tables of recommendation models has been a long-standing challenge. One line of work aims to reduce the embedding table size, such as sharing the embeddings across related features <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>, searching the vocabulary sizes or the table dimensions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>, pruning <ref type="bibr" target="#b21">[22]</ref>, quantilization <ref type="bibr" target="#b13">[14]</ref>, and hashing <ref type="bibr" target="#b14">[15]</ref>. Our work is orthogonal to these methods since AutoShard can be applied to compressed tables as well. A related work explored using the tiered memory hierarchy to store the embedding tables <ref type="bibr" target="#b25">[26]</ref>. They exploited the unequal access patterns of embedding tables to improve the efficiency by placing hot rows in the GPU memory. Our efforts complement <ref type="bibr" target="#b25">[26]</ref> by providing an end-to-end learning-based framework for cost approximation and partitioning optimization.</p><p>Deep RL. Deep RL has shown promise in accomplishing goaloriented tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. Recently, deep RL has been applied to various machine learning model design tasks, such as neural architecture search <ref type="bibr" target="#b40">[41]</ref>, pipeline search <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>, data augmentation/sampling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Our work also falls into this line of studies but we instead focus on optimizing the model efficiency. Our work is also related to applying RL for classical combinational optimization <ref type="bibr" target="#b2">[3]</ref>. Unlike <ref type="bibr" target="#b2">[3]</ref>, we tackle a real-world combinational optimization challenge in industrial recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>This work presents a novel solution for embedding table sharding practiced at Meta. The proposed algorithm, namely AutoShard, uses a cost model to efficiently estimate the table cost and leverages deep RL to solve the partition problem. The empirical results suggest that AutoShard is effective, transferable, and efficient. Through developing AutoShard, we show the promise in applying RL to optimize the industrial-level system designs. We have open-sourced a prototype of AutoShard to motivate and facilitate future exploration in this direction. In the future, we will extend AutoShard to tackle more complex sharding tasks by modeling the cost of the communication and the tiered memory hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DATASET DETAILS</head><p>We will not provide the details of MetaProd for data privacy and only discuss MetaSyn, which is open-sourced 8 and shares similar patterns to Meta production recommendation workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Data Format</head><p>MetaSyn consists of three PyTorch tensors saved in a single file, including an indices tensor, an offsets tensor, and a length tensor. We denote them as indices, offsets, and lengths, respectively. indices is 1-dimensional, where each element is an integer index. The indices are ordered by (table_id, batch_offset); that is, if we scan the tensor from the left to the right, we will first get a batch of indices for the first table, and then obtain a batch of indices for the second table, etc. offsets is also 1-dimensional and specifies the starting position and the ending position for one lookup. It is also ordered by (table_id, batch_offset). For example, suppose the batch size is 65,536. Then start = offsets[65536] and end = offsets[65537] will specify the starting and ending positions of the first indices lookup in the second table. The indices tensor between the starting and ending positions, i.e., indices[start:end] correspond to the first instance in the batch and the second table. lengths is 2-dimensional and is of the shape [num_tables, batch_size], where each element is the pooling factor of the corresponding indices lookup. lengths is provided for correctness validation since it can be inferred from the other two tensors. indices and offsets share the same data format with the batched embedding bag operator from FBGEMM and can be directly fed into it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Distributions</head><p>We visualize the distribution of hash sizes, the distribution of mean pooling factors, the relation between the hash sizes and pooling factors, and the distribution of indices accessing frequency of MetaSyn in Figure <ref type="figure" target="#fig_12">13</ref>. We make the following observations: 1) the hash sizes for most tables are on the scale of millions, while some can reach tens of millions. 2) the pooling factor generally follows a power-law distribution, where the majority is less than 50, while some can be as large as 200. 3) there is no clear relationship between the hash size and pooling factor; 4) most of the indices are accessed less than ten times, while some of them can reach 10 5 . The highly diverse table characteristics and indices patterns will easily result in imbalances if not carefully partitioning the tables. Specifically, the costs of the tables will also be diverse, i.e., some tables will have extremely high costs while some others could have very low costs. As a result, if we do not shard the tables carefully, some tables with high costs can be easily put into the same shard, resulting in a very high cost for the shard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Data Processing</head><p>Recall that the 856 tables in MetaSyn will serve as the table pool in our experiments. For the ease of use, we separate indices and offsets into a list of indices and a list of offsets, respectively, where each element corresponds to one table. The offsets for each table will be re-indexed starting from 0. In training, we merge indices and the offsets of the selected tables to construct the input of Call benchmark_op and append the result to costs 8: end for 9: Sort the costs and remove the ? highest/lowest costs. 10: Return the mean of the costs the operator. For the embedding tables, we randomly choose the table dimension from {16, 32} since the MetaSyn does not provide this information. We purposely make the dimensions small to facilitate reproducibility on GPUs with small memory. Note that, for MetaProd, the table dimension is specified by the production model and is larger than MetaSyn. For the parameters of embedding tables, we randomly initialize them with fp16 precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MODEL DETAILS B.1 Details of Micro-Benchmark</head><p>Precisely measuring the kernel time of the embedding operator requires non-trivial engineering efforts because of the warmup overhead of the GPU, caching mechanism, and variance. Specifically, naively running an operator multiple times and using the mean latency as the cost will not result in an accurate estimation. First, the warmup stage of the CUDA devices will cause significant overhead, leading to a higher estimation of the cost. Second, the L1 and L2 caches will cache the tensors in the previous iterations and result in a lower estimation of the cost. Third, some certain anomalous runs could have very high or low latency due to variance. However, the mean latency will be sensitive to anomalies. Fortunately, we found that some engineering tricks can well tackle the above problems to enable a precise measurement of the latency. The overall procedure is summarized in Algorithm 3 with an inner function in Algorithm 2. Before actually measuring the time, we first run several warmup iterations to warm up the GPU (line 2-3 of Algorithm 3). For each run, we first clear the cache to remove the impact of caching (line 2 of Algorithm 2). To tackle the anomalies, we remove the highest and the lowest costs and return the mean value of the remaining costs. We empirically set ? = 5, ? = 10, and ? = 2. We have performed sanity checks and concluded that the time collected from  the micro-benchmark is consistent with the kernel time obtained by profiling. In essence, the micro-benchmark is general and can be applied to other operators as well. Thus, we have deployed it in the production environment to support general micro-benchmarking.</p><p>It is also open-sourced under PARAM Benchmarks 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Details of Features</head><p>The features are important for learning. Recall that we have used the following features in the cost model and the RL policy: table dimension, hash size, pooling factor, table size, indices distributions, and step-aware feature. We elaborate on these features below.</p><p>? Table dimension: it is the dimension of each embedding vector in the table. This feature is normalized to have a mean of 0 and a standard deviation of 1. ? Hash size: it is the number of rows in a table, which is determined by the feature cardinality. This feature is also normalized. ? Pooling Factor: it is obtained by dividing the total number indices with the batch size. This feature is also normalized. Architecture of the policy-value network. We first use onelayer MLP to map the state-aware feature to a 32-dimensional representation. Then we concatenate the single table 32-dimensional representation (shared with the cost model) of the upcoming table to obtain a 64-dimensional state representation. The state representation is processed by a two-layer LSTM with a hidden size 64. For actions, we similar obtain a multi-table representation for each shard. We use a one-layer MLP to map the shard cost to a 32-dimensional cost representation. The action representation is obtained by concatenating the multi-table representation and the cost representation, and is 64-dimensional. For the policy head, we concatenate the state representation, the action representation, and their dot-product and use a four-layer MLP with size 128-128-64-1 to generate the action confidence. Then SoftMax is applied to obtain the probabilities. For the value head, we use a two-layer MLP with a size 64-1 to produce the state value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Hyperparameters Configuration</head><p>We summarize all the hyperparameters of AutoShard below.</p><p>? Cost model training: batch size ? 2 = 512, the number of update iteration ? = 20, the buffer size is 5000. ? RL training: batch size ? 1 = 8, number of data collection steps ? = 100, number of learning threads is 1, entropy weight is 0.001, baseline weight is 0.5, discounting factor is 1.0, gradient is clipped with threshold 40, and the total number of training steps is 100,000. ? Optimizer: We use Adam optimizer with a learning rate of 0.001. The other hyperparameters are kept as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Hardware</head><p>For MetaSyn, we conduct all the experiments on a server with 48 Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz processors, 188 GB memory, and four NVIDIA GeForce RTX 2080 Ti GPUs. For MetaProd, the server has a similar hardware environment but with NVIDIA V100 GPUs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: A typical recommendation model with dense and sparse features<ref type="bibr" target="#b24">[25]</ref>. The system exploits a combination of model parallelism (i.e., the embedding tables are partitioned into different devices) and data parallelism (i.e., replicating MLPs on each device and partitioning training data into different devices). The embedding vectors obtained from embedding lookup are appropriately sliced and transferred to the target devices through an all-to-all communication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An illustration of the policy head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance of AutoShard against baselines. We report the mean and standard deviation across five runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance of random search with five runs. Note that searching 1000 samples is extremely time-consuming and are often impractical for production use (it takes 9,783 seconds for MetaSyn and 14,599 seconds for MetaProd).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Degree of balance (left) and speedup (right) of Au-toShard on MetaSyn with different ratios of unseen tables.Note that when the ratio is 0.0, the results are worse than those shown in Figure8. This is because we only use half of the tables in this experiment. A possible reason is that more tables can improve the generalization ability of AutoShard, which leads to better results on the testing tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: of balance (left) and speedup (right) of Au-toShard on MetaSyn with up to 800 tables. We directly transfer the model trained on 80 tables without fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Training curve on four 2080 Ti GPUs (left) and inference time with a single CPU core (right).becomes more challenging with more tables. Second, AutoShard can significantly outperform the baselines in all the settings. In particular, we surprisingly observe an increase in speedup with more tables. This is because random sharding will perform poorly with more tables. This again demonstrates the superiority of AutoShard.Overall, we conclude that AutoShard can well transfer to unseen tables and hundreds of tables, making it a desirable choice in handling complex training tasks in the production environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>3 :</head><label>3</label><figDesc>8 https://github.com/facebookresearch/dlrm_datasets Algorithm 2 benchmark_op 1: Input: A PyTorch operator, the arguments of the operator 2: Feed random tensors to GPU to clear the cache 3: Initialize the start and end CUDA events for time measuring 4: Run the forward and backward passes of the operator 5: Collect latency from the CUDA events 6: Return the latency Algorithm 3 Micro-Benchmark 1: Input: A PyTorch operator, the arguments of the operator, the number of warmup iterations ? , the number of measuring iterations ?, the number of removed highest/lowest costs ? 2: for iteration = 1, 2, ... W do Call benchmark_op with the operator and the arguments 4: end for 5: Initialize a Python list costs to store the costs 6: for iteration = 1, 2, ... B do 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Data distributions of MetaSyn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? Propose AutoShard for embedding table sharding. It uses neural cost model to predict the kernel time of the operator and leverages deep RL to solve the partition problem. It can propose an effective sharding plan for hundreds of tables in seconds with a single CPU core.</figDesc><table><row><cell>? Conduct extensive experiments on an open-sourced large-</cell></row><row><cell>scale synthetic dataset (for reproducibility) and Meta's pro-</cell></row><row><cell>duction dataset. AutoShard significantly outperforms the</cell></row><row><cell>heuristic sharding strategies. In particular, AutoShard can</cell></row><row><cell>well transfer to various scenarios. The trained policy of Au-</cell></row><row><cell>toShard can be directly applied to solve a wide range of</cell></row></table><note><p>sharding tasks with various numbers of tables and different ratios of the unseen tables without any fine-tuning, achieving the same level of balance and speedup.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>?</head><label></label><figDesc>Table Costs (ms) Mult-Table Cost = Sum of Single-Table CostsFigure 4: Impact of hash size and dimension on the single table cost (left); impact of indices distribution and pooling factor on the single table cost (middle); multi-table cost versus the sum of single table costs (right).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table characteristics</head><label>characteristics</label><figDesc></figDesc><table><row><cell>include</cell></row><row><cell>hash size, which is defined as the number of entries of the table, and</cell></row><row><cell>dimension, which means the dimension of the embedding vectors.</cell></row><row><cell>Characteristics of indices lookup include pooling factor, which is the</cell></row><row><cell>average number of lookup indices per query, and indices distribution,</cell></row><row><cell>which determines the indices accessing frequencies. We study the</cell></row><row><cell>table characteristics and indices lookup on synthetic tables.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure 6: An illustration of the sharding process of AutoShard. A two-layer LSTM encodes historical state representations and in each step outputs a shard ID (yellow) that allocates the table at hand (blue) to a shard (purple). State representation is obtained by concatenating the table representation and step-aware features (dotted circle in the left). Action representation is the sum of the single tables' representations (dotted circle in the right). A policy head produces a shard ID based on the state/action representations and the cost model. A value head will approximate state values to reduce the variance of RL training.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Pol i cy Head</cell><cell cols="2">Val u e Head</cell></row><row><cell>St at e</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Repr esen t at i on</cell><cell>Cost</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>M odel</cell><cell></cell><cell></cell></row><row><cell cols="2">Con cat en at i on</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="2">Act i on 1</cell><cell>Act i on 2</cell><cell>LSTM Ou t pu t</cell></row><row><cell>h</cell><cell></cell><cell cols="2">El em en t -W i se Su m</cell><cell></cell></row><row><cell></cell><cell></cell><cell>h</cell><cell>h</cell><cell>h</cell></row><row><cell></cell><cell></cell><cell>cr o-</cell><cell></cell><cell></cell></row><row><cell>Tabl e</cell><cell>St ep -Aw ar e</cell><cell>Ben ch m ar k</cell><cell></cell><cell></cell></row><row><cell>Feat u r es</cell><cell>Feat u r es</cell><cell>1</cell><cell>3</cell><cell>2</cell></row></table><note><p>eventually obtain a sharding plan. This formulation has two desirable properties. First, it can significantly decrease the decision space. Specifically, the decision space of the sharding policy is only the number of shards ? in each step. Although the policy needs to perform more steps, the decisions made across steps are very similar (i.e., they all aim to assign a table to achieve load balance) so that the policy in one step may learn to reuse the knowledge learned from other steps and improve learning efficiency. Second, it can implicitly encourage transferable strategies. By associating one table with one step, a model trained on very few tables can easily transfer to more tables by simply adding more steps without re-training. This cannot be achieved by black-box optimization.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>6 , one of the most popular datasets, only has 26 sparse features with a cardinality of at most one million. Thus, our experiments are mainly conducted on an open-sourced large-scale synthetic dataset7 (MetaSyn), which shares similar indices accessing patterns to Meta production embedding tables. As tabulated in Table 1, MetaSyn consists of hundreds Algorithm 1 Training of AutoShard 1: Input: Training tasks S train = {? ? } ? ?=1 , batch size ? 1 for policyvalue network and ? 2 for cost model, number of update iterations ? for cost model, number of data collection steps ?</figDesc><table><row><cell cols="2">2: Initialize the cost model and policy-value network</cell></row><row><cell cols="2">3: for iteration = 1, 2, ... until convergence do</cell></row><row><cell>4:</cell><cell>Collect a set of trajectories with ? steps {? ? , ? ? , ? ? } ? ? =1 from a</cell></row><row><cell></cell><cell>randomly sampled task from S train and store the generated</cell></row><row><cell></cell><cell>cost data into a buffer</cell></row><row><cell>5:</cell><cell>if more than ? 1 sets of new trajectories are collected then</cell></row><row><cell>6:</cell><cell>Update policy-value network with Eq. 3</cell></row><row><cell>7:</cell><cell>for iteration = 1, 2, ..., ? do</cell></row><row><cell>8:</cell><cell>Sample a batch of cost data with size ? 2 from the buffer</cell></row><row><cell></cell><cell>and update the cost model with MSE loss</cell></row><row><cell>9:</cell><cell>end for</cell></row><row><cell>10:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Meta synthetic embedding tables.</figDesc><table><row><cell></cell><cell cols="2">Attribute</cell><cell></cell><cell>Value</cell></row><row><cell></cell><cell cols="3">Number of Tables</cell><cell>856</cell></row><row><cell></cell><cell cols="2">Batch Size</cell><cell></cell><cell>65,536</cell></row><row><cell></cell><cell cols="3">Max/Mean/Min Hash Sizes</cell><cell cols="2">12,543,670 / 4,107,458 / 1</cell></row><row><cell></cell><cell cols="4">Max/Mean/Min Pooling Factors 193 / 15 / 0</cell></row><row><cell></cell><cell cols="2">Au t oSh ar d</cell><cell>Look u p -gr eedy</cell><cell>Di m -gr eedy</cell><cell>Si ze-gr eedy</cell><cell>Ran d</cell></row><row><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Degree of Balance</cell><cell>40% 50% 60% 70% 80%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell>MetaSyn</cell><cell>MetaProd</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of AutoShard on MetaSyn.</figDesc><table><row><cell></cell><cell>Degree of Balance</cell><cell>Speedup</cell></row><row><cell>w/o cost modeling</cell><cell>61.3%?12.9%</cell><cell>1.420?0.203</cell></row><row><cell>w/o dimension feature</cell><cell>87.6%?1.5%</cell><cell>1.706?0.056</cell></row><row><cell>w/o hash size feature</cell><cell>87.8%?0.9%</cell><cell>1.702?0.047</cell></row><row><cell>w/o pooling factor feature</cell><cell>45.9%?1.9%</cell><cell>1.271?0.035</cell></row><row><cell>w/o size feature</cell><cell>87.0%?1.1%</cell><cell>1.683?0.049</cell></row><row><cell>w/o distribution features</cell><cell>84.3%?1.3%</cell><cell>1.688?0.035</cell></row><row><cell>Full version of AutoShard</cell><cell>88.6%?1.2%</cell><cell>1.712?0.027</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Mean absolute error (MAE) and mean squared error (MSE) of the cost model on 100 randomly sampled 10 tables following the unseen tables setting in Section 5.5, where the training tables are the first half of the pool, and the testing tables are the second half. The prediction error of the multitable cost is around 1.3 milliseconds on the training tables and 3 milliseconds on the unseen tables.</figDesc><table><row><cell cols="2">MAE (training) MSE (training)</cell><cell cols="2">MAE (testing) MSE (testing)</cell></row><row><cell>1.321</cell><cell>3.408</cell><cell>3.061</cell><cell>10.835</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>?</head><label></label><figDesc>Table size: it is obtained by calculating the parameter size in GBs. We do not normalize this feature. ? Indices distributions: Since the majority of the indices are visited very few times, we divide frequencies into several bins, where the size of each bin grows exponentially. Specifically, we use the following 17 bins: (0, 1], (1, 2], (2, 4], (4, 8], (8, 16], (16, 32], (32, 64], (64, 128], (128, 256], (256, 512], (512, 1024], (1024, 2048], (2048, 4096], (4096, 8192], (8192, 16384], (16384, 32768], and (32768, ?). We count the number of times each index appears in a batch of indices and put the count into the corresponding bin. Then we calculate the ratio for each bin, which results in 17 features. For example, the first feature is the ratio of the indices for the bin (0, 1]. The fourth feature is the ratio of the indices for the bin (4, 8]. ? Step-aware feature: it is defined as the the ratio of the tables that have already been assigned. Architecture of the cost model. We use a two-layer MLP to obtain the representation for every single table. The input dimension is 21, which is the number of table features. The hidden dimension is 128. The output size is 32; that is, the size of a single table representation is 32. For multi-table cases, we sum the table representations to obtain multi-table representation, whose size is also 32. Finally, we use another two-layer MLP to produce the multitable cost, where the hidden dimension is 64, and the output size is 1.</figDesc><table><row><cell>C IMPLANTATION DETAILS</cell></row><row><cell>C.1 Neural Architecture</cell></row></table><note><p><p>9 </p>https://github.com/facebookresearch/param</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>https://github.com/pytorch/FBGEMM/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>The micro-benchmark is currently maintained as a separate open-source effort for all the PyTorch operators at https://github.com/facebookresearch/param</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://www.kaggle.com/c/avazu-ctr-prediction/data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>https://github.com/facebookresearch/dlrm_datasets</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>The work is, in part, supported by <rs type="funder">NSF</rs> (#IIS-1900990, #<rs type="grantNumber">IIS-1750074</rs>). The views and conclusions in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ubbxfE5">
					<idno type="grant-number">IIS-1750074</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Dsstne</forename><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://github.com/amazon-archives/amazon-dsstne" />
		<title level="m">Deep Scalable Sparse Tensor Network Engine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding training efficiency of deep learning recommendation models at scale</title>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploratory combinatorial optimization with reinforcement learning</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lvovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DLRS Workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The netflix recommender system: Algorithms, business value, and innovation</title>
		<author>
			<persName><forename type="first">Carlos A Gomez-Uribe</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Management Information Systems (TMIS)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bill Jia, et al. 2020. The architectural implications of facebook&apos;s dnn-based personalized recommendation</title>
		<author>
			<persName><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Cottel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<meeting><address><addrLine>Kim Hazelwood, Mark Hempstead</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<title level="m">Neural collaborative filtering</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonvolatile memory materials for neuromorphic intelligent machines</title>
		<author>
			<persName><forename type="first">Doo</forename><surname>Seok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cheol Seong</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Materials</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">1704729</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">XDL: an industrial deep learning framework for high-dimensional sparse data</title>
		<author>
			<persName><forename type="first">Biye</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zelin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural input search for large scale recommendation models</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Manas R Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taibai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multi-granular quantized embeddings for large-vocab categorical features in recommender systems</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to Embed Categorical Features without Embedding Tables for Recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference</title>
		<author>
			<persName><forename type="first">Daya</forename><surname>Khudia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Protonu</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Summer</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05615</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Selvatici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viswanath</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03552</idno>
		<title level="m">Torchbeast: A pytorch platform for distributed rl</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TODS: An Automated Time Series Outlier Detection System</title>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanchu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yile</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Purav</forename><surname>Zumkhawaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated Anomaly Detection via Curiosity-Guided Search and Self-Imitation Learning</title>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Related pins at pinterest: The evolution of a real-world recommender system</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>David C Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Shiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">C</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learnable Embedding sizes for Recommender Systems</title>
		<author>
			<persName><forename type="first">Siyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep learning training in facebook data centers: Design of scale-up and scale-out systems</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitney</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serhat</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ozdal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09518</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisson</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Azzolini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00091</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RecShard: Statistical Feature-Based Memory Optimization for Industry-Scale Neural Recommendation</title>
		<author>
			<persName><forename type="first">Geet</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Trippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compositional embeddings using complementary partitions for memory-efficient recommendation systems</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyan</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Autoint: Automatic feature interaction learning via selfattentive neural networks</title>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RLCard: a platform for reinforcement learning in card games</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanpu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keerthana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-AAD: Active anomaly detection with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Experience Replay Optimization</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenye</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AutoVideo: An Automated Video Action Recognition System</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pervaiz</forename><surname>Zaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Wei</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anmoll</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Qazim</forename><surname>Kumar Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaben</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingru</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenye</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Model size reduction using frequency based double hashing for recommender systems</title>
		<author>
			<persName><forename type="first">Caojin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofia</forename><forename type="middle">Ira</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranay</forename><surname>Kumar Myana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Dilipkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvadip</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ikuhiro</forename><surname>Ihara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronglai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiquan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLSys</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Autoemb: Automated embedding dimensionality search in streaming recommendations</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep interest evolution network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
