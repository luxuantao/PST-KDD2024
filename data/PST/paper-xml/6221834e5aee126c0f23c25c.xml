<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAFE: Learning to Condense Dataset by Aligning Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-03">3 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<email>kai.wang@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
							<email>bo.zhao@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>You</surname></persName>
							<email>youy@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Phigent</forename><surname>Robotics</surname></persName>
						</author>
						<title level="a" type="main">CAFE: Learning to Condense Dataset by Aligning Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-03">3 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.01531v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dataset condensation aims at reducing the network training effort through condensing a cumbersome training set into a compact synthetic one. State-of-the-art approaches largely rely on learning the synthetic data by matching the gradients between the real and synthetic data batches. Despite the intuitive motivation and promising results, such gradient-based methods, by nature, easily overfit to a biased set of samples that produce dominant gradients, and thus lack a global supervision of data distribution. In this paper, we propose a novel scheme to Condense dataset by Aligning FEatures (CAFE), which explicitly attempts to preserve the real-feature distribution as well as the discriminant power of the resulting synthetic set, lending itself to strong generalization capability to various architectures. At the heart of our approach is an effective strategy to align features from the real and synthetic data across various scales, while accounting for the classification of real samples. Our scheme is further backed up by a novel dynamic bi-level optimization, which adaptively adjusts parameter updates to prevent over-/under-fitting. We validate the proposed CAFE across various datasets, and demonstrate that it generally outperforms the state of the art: on the SVHN dataset, for example, the performance gain is up to 11%. Extensive experiments and analysis verify the effectiveness and necessity of proposed designs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) have demonstrated unprecedented results in many if not all applications in computer vision <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref>. These gratifying results, nevertheless, come with costs: the training of DNNs heavily rely on the sheer amount of data, sometimes up to tens of millions of sam-Figure <ref type="figure">1:</ref> (a) At the later training stage, most examples do not contribute meaningful gradients, making the synthetic set learned by gradient matching extremely bias towards those large-gradient samples, which downgrades its generalization to unseen architectures. (b) Compared with gradient-based method <ref type="bibr" target="#b53">[54]</ref>, the synthetic set learned by our approach effectively captures the whole distribution thus generalizes well to other network architectures.</p><p>Numerous research endeavours have, therefore, focused on alleviating the cumbersome training process through constructing small training sets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref>. One classic approach is known as coreset or subset selection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11]</ref>, which aims to obtain a subset of salient data points to represent the original dataset of interest. Nevertheless, coreset selection is typically a NP-hard problem <ref type="bibr" target="#b17">[18]</ref>, making it computationally intractable over large-scale datasets. Most existing approaches have thus resorted to greedy algorithms with heuristics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b32">33]</ref> to speed up the process by trading-off the optimality.</p><p>Recently, dataset condensation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b53">54]</ref> has emerged as a competent alternative with promising results. The goal of dataset condensation is, as its name indicates, to condense a large training set into a small synthetic one, upon which DNNs are trained and expected to preserve the performances. Along this line, the pioneering approach of <ref type="bibr" target="#b37">[38]</ref> proposed a meta-learning-based strategy; however, the nestloop optimization precludes its scaling up to large-scale inthe-wild datasets. The work of <ref type="bibr" target="#b53">[54]</ref> alleviates this issue by enforcing the batch gradients of the synthetic samples to approach those of the original ones, which bypasses the recursive computations and achieves impressive results. The optimization of the synthetic examples is explicitly supervised by minimizing the distance between the gradients produced by the synthetic dataset and the real dataset.</p><p>However, gradient matching method has two potential problems. First, due to the memorization effect of deep neural networks <ref type="bibr" target="#b47">[48]</ref>, only a small number of hard examples or noises produce dominant gradients over the network parameters. Thus, gradient matching may overlook those representative but easy samples, while overfit to those hard samples or noises. Second, these hard examples that produce large gradients may vary across different architectures; relying solely on gradients, therefore, will yield poor generalization performance to unseen architectures. The distributions of gradients and hard examples are illustrated in Fig. <ref type="figure">1a</ref>. The synthetic data learned by gradient matching may be highly biased towards a small number of unrepresentative data points, which is illustrated in Fig. <ref type="figure">1b</ref>.</p><p>To go beyond the learning bias and better capture the whole dataset distribution, in this paper, we propose a novel strategy to Condense dataset by Aligning FEatures, termed as CAFE. Unlike the approach of <ref type="bibr" target="#b53">[54]</ref>, we account for the distribution consistency between synthetic and real datasets by applying distribution-level supervision. Our approach, through matching the features that involve all intermediary layers, expands the attention across all samples and hence provides a much more comprehensive characterization of the distribution while avoiding over-fitting on hard or noisy samples. Such distribution-level supervision will, in turn, endow CAFE with stronger generalization power than gradient-based methods, since the hard examples may easily vary across different architectures.</p><p>Specifically, we impose two complementary losses into the objective of CAFE. The first one concerns capturing the data distribution, in which the layer-wise alignment between the features of the real and synthetic samples is enforced and further the distribution is preserved. The second loss, on the other hand, concerns discrimination. Intuitively, the learned synthetic samples from one class should well represent the corresponding clusters of the real samples. Hence, we may treat each real sample as a testing sample, and classify it based on its affinity to the synthetic clusters. Our second loss is then defined upon the classification result of the real samples, which, effectively, injects the discriminant capabilities into the synthetic samples.</p><p>The proposed CAFE is further backed up by a novel bilevel optimization scheme, which allows our network and synthetic data to be updated through a customized number of SGD steps. Such a dynamic optimization strategy, in practice, largely alleviates the under-and over-fitting issues of prior methods. We conduct experiments on several popular benchmarks and demonstrate that, the results yielded by CAFE are significantly superior to the state of the art: on the SVHN dataset, for example, our method outperforms the runner-up by 11% when learning 1 image/class synthetic set. We also especially prove that synthetic set learned by our method has better generalization ability than that learned by <ref type="bibr" target="#b53">[54]</ref>.</p><p>In summary, our contribution is a novel and effective approach for condensing datasets, achieved through aligning layer-wise features between the real and synthetic data, and meanwhile explicitly encoding the discriminant power into the synthetic clusters. In addition, a new bi-level optimization scheme is introduced, so as to adaptively alter the number of SGD steps. These strategies jointly enable the proposed CAFE to well characterize the distribution of the original samples, yielding state-of-the-art performances with strong generalization and robustness across various learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Dataset Condensation. Several methods have been proposed to improve the performance, scalability and efficiency of dataset condensation. Based on the meta-learning method proposed in <ref type="bibr" target="#b37">[38]</ref>, some works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref> try to simplify the inner-loop optimization of a classification model by training with ridge regression which has a closed-form solution. <ref type="bibr" target="#b31">[32]</ref> trains a generative network to produce the synthetic set. The generative network is trained using the same objective as <ref type="bibr" target="#b37">[38]</ref>. To improve the data efficiency of <ref type="bibr" target="#b53">[54]</ref>, differentiable Siamese augmentation is proposed in <ref type="bibr" target="#b51">[52]</ref>. They enable the synthetic data train neural networks with data augmentation effectively.</p><p>A recent work <ref type="bibr" target="#b52">[53]</ref> also learns synthetic set with feature distribution matching. Our method is different from it in three main aspects: 1) we match layer-wise features while <ref type="bibr" target="#b52">[53]</ref> only uses final-layer features; 2) we further explicitly enable the synthetic images to be discriminative as the classifier (i.e. Sec. 3.3); 3) our method includes a dynamic bilevel optimization which can boost the performance with adaptive SGD steps, while <ref type="bibr" target="#b52">[53]</ref> tries to reduce the training cost by dropping the bi-level optimization.</p><p>Coreset Selection. The classic technique to condense the training set size is coreset or subset selection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39]</ref>. Most of these methods incrementally select important data points based on heuristic selection criteria. For example, <ref type="bibr" target="#b28">[29]</ref> selects data points that can approach the cluster centers. <ref type="bibr" target="#b1">[2]</ref> tries to maximize the diversity of samples in the gradient space. <ref type="bibr" target="#b32">[33]</ref> measures the forgetfulness of trained samples during network training and drops those that are not easy to forget. However, these heuristic selection criteria cannot ensure that the selected subset is optimal for training models, especially for deep neural networks. In addition, greedy sample selection algorithms are unable to guarantee that the selected subset is optimal to satisfy the criterion.</p><p>Generative Models Our work is also closely related to generative model such as auto-encoder <ref type="bibr" target="#b16">[17]</ref> and generative adversarial networks (GANs) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>. The difference is that image generation aims to synthesize real-looking images that can fool human beings, while our goal is to generate informative training samples that can be used to train deep neural networks more efficiently. As shown in <ref type="bibr" target="#b53">[54]</ref>, concerning training models, the efficiency of these images generated by GANs closes to that of randomly sampled real images. In contrast, our method can synthesize better training images that significantly outperform those selected real images in terms of model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first briefly overview the proposed CAFE. Then, we introduce three carefully designed modules: layer-wise feature alignment module, discrimination loss, and dynamic bi-level optimization module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Dataset condensation aims to condense a large-scale dataset</p><formula xml:id="formula_0">T = {(x i , y i )}| |T | i=1 into small (synthetic) dataset S = {(s j , y j )}| |S| j=1</formula><p>while achieving similar generalization performance. Fig. <ref type="figure" target="#fig_0">2</ref> illustrates the proposed method. First, we sample two data batches from the large-scale dataset T and the learnable synthetic set S respectively, and then extract the features using neural network ? ? (?) which is parameterized with ?. To capture the distribution of T accurately, layer-wise feature alignment module is designed, in which we minimize the difference of layer-wise feature maps of real and synthetic images using Mean Square Error (MSE). To enable learning discriminative synthetic images, we use the feature centers of synthetic images of each class to classify the real images by computing their innerproduct and cross-entropy loss. The synthetic images are updated by minimizing the above two losses, which is the outer-loop. Then, we update the network ? ? (?) by mini-mizing the cross-entropy loss on synthetic images, which is the inner-loop. The synthetic images and network are alternatively using a novel dynamic bi-level optimization algorithm which avoids the over-or under-fitting on synthetic dataset and breaks the outer-and inner-loop automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Layer-wise Features Alignment</head><p>As mentioned above, previous works <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b51">52]</ref> compare the differences of gradients between real and synthetic data. Such objective produces samples with large gradients, but these samples fail to capture the distribution of the original dataset (illustrated in Fig. <ref type="figure">5</ref>). Thus, it may have poor performance when generalizing to unseen architectures. To tackle this issue, we design Category-Wise Feature Averaging (CWFA), illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>, to measure the feature difference between T and S at each convolutional layer. Specifically, we sample a batch of real data T k and synthetic data S k with the same label k and the batch size N and M , from T and S respectively. We embed each real and synthetic datum using network ? ? (?) with L layers (except the output layer) and obtain the layerwise features</p><formula xml:id="formula_1">F T k = [f T k,1 , f T k,2 , ..., f T k,L ] = ? ? (T k ) and F S k = [f S k,1 , f S k,2 , ..., f S k,L ] = ? ? (S k ). The l th layer feature f T k,l ? R N ?C is reduced to f T k,l ? R 1?C</formula><p>by averaging the N samples in the real data batch, where C = C ? H ? W and it refers to the feature size of corresponding layer. Similarly, we obtain f S k,l for synthetic data batch. Then, MSE is applied to calculate the feature distribution matching loss L f for every layer, which is formulated as</p><formula xml:id="formula_2">L f = K k=1 L l=1 | f S k,l -f T k,l | 2 , (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where K is the number of categories in a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discrimination Loss</head><p>Though the layer-wise feature alignment can capture the distribution of original dataset, it may overlook the discriminative sample mining. We hold the view that an informative synthetic set could be used as a classifier to classify real samples. Based on this, we calculate the classification loss in the last-layer feature space. we obtain the synthetic feature center f S k,L ? R 1?C of each category k by averaging the batch. We concatenate the feature centers</p><formula xml:id="formula_4">F S L = [ f S 1,L , f S 2,L , ..., f S K,L</formula><p>] and also real data</p><formula xml:id="formula_5">F T L = [f T 1,L , f T 2,L , ..., f T K,L</formula><p>] from all classes. The real data is classified using the inner-product between real data and the synthetic centers</p><formula xml:id="formula_6">O = F T L , ( F S L ) T ,<label>(2)</label></formula><p>Layer-wise Feature Alignment Module </p><formula xml:id="formula_7">? !?#?$?% ? 1 ? ? &amp;?#?$?% N images from one category C W F A Real Images Synthetic Images C N N F ! " Layer-wise Feature Alignment Module D et ac h T ra ns po se CE Loss MSE Loss F ! " [-1] F ! # [-1] F ! # C W F A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrimination Loss</head><p>Selection where O ? R N ?K contains the logists of N = K ? N real data points. The classification loss is</p><formula xml:id="formula_8">L d = - 1 N ? N i=1 log p i ,<label>(3)</label></formula><p>where the probability p i is the softmax value corresponding to its ground-truth label over all classes p i = softmax(O i ).</p><p>The total loss for learning synthetic images is</p><formula xml:id="formula_9">L total = L f + ?L d ,<label>(4)</label></formula><p>where ? is a positive scalar weight of L d . We study the influence of ? in Sec. 4.3. The synthetic set is updated by minimizing L total :</p><formula xml:id="formula_10">S ? arg min S L total (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dynamic Bi-level Optimization</head><p>Similar to previous work <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b53">54]</ref>, we also learn the synthetic set with a bi-level optimization, in which the synthetic set S is updated using Eq. 5 in the outer-loop and network parameters ? is updated using</p><formula xml:id="formula_11">? ? arg min ? J(S, ?)<label>(6)</label></formula><p>in the inner-loop alternatively. J(S, ?) calculates the crossentropy classification loss on the synthetic set S. In this way, the synthetic set can be trained on many different ? so that it can generalize to them. We initialize S and ? from random noise and standard network random initialization <ref type="bibr" target="#b15">[16]</ref>. Previous work <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b51">52]</ref> sets a fixed number of outer-loop and inner-loop optimization steps, which takes too much time to adjust the hyper-parameters and may lead to networks' over-or under-fitting on synthetic set. To address these issues, we design a new bi-level optimization algorithm that can break the outer-and inner-loop automatically. Fig. <ref type="figure" target="#fig_0">2</ref> illustrates the proposed dynamic bi-level optimization module. To monitor the changing of network parameters ?, we randomly sample some images from real training set as a query set to evaluate the network. Then, a queue Q is used to store the performance on the query set.</p><p>We expect to learn synthetic data on more diverse network parameters. Hence, we sample inner-loop networks to optimize synthetic images when remarkable performance improvement is achieved on the query set. The optimization will be stopped when the performance on the query set is converged. ? 1 and ? 2 are two hyper parameters of dynamic bi-level optimization. We implement ablation study to show that the performance is not sensitive to ? 1 and ? 2 . The train-  acc. = get acc( (K, N )); Qout.append(acc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>if |Qout| == ? and div(Qout)&lt; ?1 or l out c &gt; lout then while True do 13: updating ? using Eq. 6. {inner-loop} 14:</p><p>acc. = get acc( (K, N )); Qin.append(acc.) end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>end while 23: end while ing algorithm is summarized in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the used datasets and implementation details. Then, we compare the proposed method to the state-of-the-art methods. After that, we conduct sufficient ablation studies to analyze the significant components and the influence of hyper parameters. Finally, the visualizations of synthetic images and feature distributions are provided to show the superiority of our CAFE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets &amp; Implementation Details</head><p>MNIST <ref type="bibr" target="#b20">[21]</ref>. The MNIST is a handwritten digits dataset that is commonly used for validating image recognition models. It 60,000 training images and 10,000 testing images with the size of 28?28.</p><p>FashionMNIST <ref type="bibr" target="#b40">[41]</ref>. FashionMNIST is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28?28 gray-scale image, associated with a label from 10 classes.</p><p>SVHN <ref type="bibr" target="#b29">[30]</ref>. SVHN is a real-world image dataset for developing machine learning and object recognition algorithms. It consists of over 600,000 digit images coming from real world data. The images are cropped to 32?32. CIFAR10/100 <ref type="bibr" target="#b18">[19]</ref>. The two CIFAR datasets consist of tiny colored natural images with the size of 32?32 from 10 and 100 categories, respectively. In each dataset, 50,000 images are used for training and 10,000 images for testing.</p><p>Implementation Details. We present the experiments details of the outer-loop and inner-loop, respectively. In outerloop, we optimize 1/10/50 Images Per Class (IPC) synthetic sets for all the five datasets using three-layer Convolutional Network (ConvNet) as same as <ref type="bibr" target="#b53">[54]</ref>. The ConvNet includes three repeated "Conv-InstNorm-ReLU-AvgPool" blocks. The channel number of each convolutional layer is 128. The initial learning rate of synthetic images is 0.1, which is divided by 2 in 1,200, 1,400, and 1,800 iterations. We stop training in 2,000 iterations. For inner-loop, we train the ConvNet on synthetic sets for 300 epochs and evaluate the performances on 20 randomly initialized networks. The initial learning rate of network is 0.01. Following <ref type="bibr" target="#b53">[54]</ref>, we perform 5 experiments and report the mean and standard deviation on 100 networks. The default N is 256, ? 1 is 0.05 and ? 2 is 0.05. We assess the sensitiveness of ? 1 and ? 2 in the Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to the State-of-the-art Methods</head><p>We compare our method to four coreset selection methods, namely Random <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>, Herding <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref>, K-Center <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref> and Forgetting <ref type="bibr" target="#b32">[33]</ref>. We also make comparisons to recent state-of-the-art condensation methods, namely Dataset Distillation (DD) <ref type="bibr" target="#b37">[38]</ref>, LD <ref type="bibr" target="#b3">[4]</ref>, Dataset Condensation (DC) <ref type="bibr" target="#b53">[54]</ref> and DSA (adding differentiable Siamese augmentation for DC) <ref type="bibr" target="#b51">[52]</ref>. We report the performances of our method and competitors on five datasets in Tab. 1. When learning 1 image per class, our method achieves the best results on all the 5 datasets. In particular, the improvements on SVHN and FashionMNIST are 11% and 6.5% over other methods. Condensation-based methods outperforms than coreset selection methods with a large margin. Among coreset selection methods, Herding and K-Center outperform Random and Forgetting with a large margin. When learning 10 and 50 images/class, the performance of our method exceeds DC with 0.7%?2.6% on most datasets. Compared with DSA, Our CAFE+DSA achieves comparable results with DSA on most datasets on CIFAR10/100. For 50 images/class learning on CIFAR10, our CAFE+DSA outperforms DSA by 1.7%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>In this subsection, we study ablations using CIFAR10 (IPC = 50) to investigate the effectiveness of each module and the influence of the hyper parameters.</p><p>Evaluation of the three components in CAFE. To explore the effect of each component in our method, we design ablation studies of Discrimination Loss (DL), Layerwise Feature Alignment (LFA) and Dynamic Bi-level Optimization on CIFAR10. As shown in Tab. 2, DL, LFA and Dynamic Bi-level Opt. are complementary with each other. CAFE performs poorly when using DL individually (49.78%), as DL focuses more on classifying the real samples but ignores the distribution consistency with real images. The result of using LFA individually outperforms DL with 4.18%, which implies considering of distribution consistency is more important for dataset condensation. However, utilizing LFA independently means the importance of all the images in real dataset are equal, which may overlook the information from discriminative samples (i.e. samples nearby the decision boundaries). Jointly using DL and LFA can obtain better result than using DC on CIFAR10 testing set. Adding the Dynamic Bi-level Opt. can further improve the performance of DL and LFA, which indicates breaking out-/inner-looper automatically can reduce the over-/underfitting effectively. Using these three components together achieves the highest result. To understand the effect of DL and LFA more intuitively, we also visualize the synthetic images feature distributions of using DL or LFA independently in Sec. 4.4. Exploring the importance of layer-wise feature alignment in each layer. To investigate the importance of feature alignment, we apply the feature alignment operation to each layer individually. As shown in Tab. 3, the performances of different layers vary remarkably. Applying feature alignment operation in layer1 or layer4 obtains better results than in layer2 or layer3, as the supervision in layer2 or layer3 is far from the input and output layers. Applying feature alignment in each layer individually can not obtain promising results. To demonstrate the effectiveness of DL in each layer, we also show the results of adding DL loss. The addition of DL can consistently improve the per-formances in all layers. Exploring the complementarity of layer-wise feature alignment among all layers. After evaluating the importance of the LFA in each layer, exploring the complementarity of LFA in all layers is also very important. We first utilize the feature alignment in the layer1 to update the synthetic images. Then, we apply the same feature alignment to other layers (i.e. layer2, layer3, layer4). Here, we also consider the effect of DL and report the results with and without using DL in Tab. 4. When adding feature alignment to more layers, the performances on testing set become better. Meanwhile, the DL can further improve the performance in all cases. Specifically, the performance difference between using LFA in all layers using in the first layer is about 4% (w/o DL) and 3% (with DL). The average performance boost of adding each layer is about 1% (w/o DL) and 0.7% (with DL), which indicates the strong complementarity of layer-wise feature alignment in all layers.</p><p>Evaluation of ? 1 and ? 2 . ? 1 and ? 2 are the thresholds to control whether break the out-looper and inner-looper or not. As shown in Fig. <ref type="figure" target="#fig_6">3a</ref> and 3b, we study different values of ? 1 and ? 2 ranging from 0.01 to 0.08. Our default ? 1 = 0.05 and ? 2 = 0.05 achieve the best results, which outperforms DC 1.6%. For out-looper, too large ? 1 may reduce the iterations of updating the synthetic images, which leads to the worse results. Too small ? 1 increases the optimization difficulties and even makes the model unable to break the out-looper normally. As for inner-looper, the model diversity is not large enough when ? 2 is too small, whereas it would be tricky to break the inner-looper when ? 2 is very large. Furthermore, it is worth noting that our method outperforms DC with a large margin at almost all settings. Meanwhile, the performance is not sensitive to ? 1 and ? 2 .</p><p>Evaluation of the ratio ?. In Fig. <ref type="figure" target="#fig_6">3c</ref>, we evaluate the effect of different ratios between the L f and L d . We find that setting equal weight for each loss achieves the best results. The performance gets promoted as ? increase from 0.1 to 1, while increasing the weight of L d from 1 to 10 dramatically degrades performance.</p><p>Evaluation of ? and the training time. ? is hyper parameter of the maximum length of queues in dynamic bi-level optimization. We show the performances and training time of different ? in Tab. 5. One can find the default ? = 10 achieves the best result and requires less time than DC. Too small and too large ? may lead to under-or over-fitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of the generalization to unseen architectures</head><p>To evaluate the generalization ability of synthetic data on unseen architectures, we first condense CIFAR10 dataset with ConvNet to generate synthetic images. Then, we train different architectures, including AlexNet, VGG11, ResNet18, and MLP (3 layers), on the synthetic images. As shown in Tab. 6, our method achieves better generalization performance than DC obviously. Specifically, our method outperforms DC with 5.25%, 1.79%, 4.42%, and 7.96% when testing on AlexNet, VGG11, ResNet18, and MLP (3 layers). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualizations</head><p>In this subsection, we visualize the synthetic images as well as data distribution to show the effectiveness of our proposed CAFE. Synthetic images. To make fair comparison, the synthetic set is initialized by the same random noise (IPC = 50). After that, we apply DC and CAFE to optimize the synthetic set on CIFAR10 dataset. Finally, the partial (only show 10 images per class) optimized synthetic images and original images of CIFAR10 are shown in Fig. <ref type="figure">4</ref>. There are several observations can be summarized as follows: 1). It is easy to find that the synthetic images generated by our method is more visually similar to original CIFAR10 images than DC. 2). The synthetic images have more semantic information than DC, which illustrates the effectiveness of LFA and DL modules. 3). A certain ratio of images generated by DC are    Data distribution. To evaluate whether the synthetic images using our method can capture more accurate distribution from original dataset, we utilize t-SNE to visualize the features of real set and synthetic sets generated by DC, DL, LFA and CAFE. As shown in Fig. <ref type="figure">5</ref>, the "points"</p><p>and "stars" represent the real and synthetic features. The synthetic images of DC gather around a small area of decision boundary, which indicates using DC can not capture the original distribution well. Our methods DL, LFA, and CAFE effectively capture useful information across the whole real dataset, which possesses a good generalization among different CNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a novel scheme to Condense dataset by Aligning FEatures (CAFE), which explicitly attempts to preserve the real-feature distribution as well as the discriminant power of the resulting synthetic data, lending itself to strong generalization capability to unseen architectures. The CAFE consists of three carefully designed modules, namely layer-wise feature alignment module, discrimination loss, and dynamic bi-level optimization module. The feature alignment module and discrimination loss con-cern capturing distribution consistency between synthetic real sets, while bi-level optimization enables CAFE to learn customized SGD steps to avoid over-/under-fitting. Experimental results across various datasets demonstrate that, CAFE consistently outperforms the state of the art with less computation cost, making it readily applicable to in-the-wild scenarios. As the future work, we plan to explore the use of dataset condensation on more challenging datasets such as ImageNet <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledge</head><p>This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-08-008) and NUS ARTIC Project (ECT-RP2). We thank Google TFRC for supporting us to get access to the Cloud TPUs. We thank CSCS (Swiss National Supercomputing Centre) for supporting us to get access to the Piz Daint supercomputer. We thank TACC (Texas Advanced Computing Center) for supporting us to get access to the Longhorn supercomputer and the Frontera supercomputer. We thank LuxProvide (Luxembourg national supercomputer HPC organization) for supporting us to get access to the MeluXina supercomputer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the proposed CAFE pipeline. The CAFE consists of a layer-wise feature alignment module to capture the accurate distribution with the original large-scale dataset, a discrimination loss for mining the discriminate samples from real dataset, a dynamic bi-level optimization module to reduce the influence of under-and over-fitting on synthetic images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 :</head><label>2</label><figDesc>randomly initialize ?, Qout = []; Qin = []; l out c =l in c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>15 :</head><label>15</label><figDesc>if |Qin| == ? and div(Qin)&gt; ?2 or l in c &gt; lin then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ? 1 and ? 2 are the hyper parameters in dynamic bi-level optimization module. ? is the ratio between L f and L d .</figDesc><graphic url="image-67.png" coords="8,50.11,253.49,163.06,163.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Visualizations of original images, and synthetic images using CAFE and DC. Both CAFE and DC are initialized from random noise.</figDesc><graphic url="image-68.png" coords="8,215.94,253.49,163.06,163.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Dynamic Bi-level Optimization T S are the real the synthetic datasets. is a random sampling function for selecting N images from K categories. Q out and Q in are the queues to save the performance on real dataset in outer-and inner-loop, respectively. div(?) is a function that calculates the difference between maximum and minimum values of Q out and Q in . ? is the maximum length of queues. The default loop numbers of DC are l out and l in . l ? c represents the loop number of CAFE.</figDesc><table /><note><p>1: while not converged do</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>1 :</head><label>1</label><figDesc>The performance comparison to coreset selection and recent state-of-the-art methods. This table shows the testing accuracies (%) of different methods on five datasets. LD ? and DD ? use LeNet for MNIST and AlexNet for CIFAR10, while the rest use ConvNet for training and testing. IPC: Images Per Class, Ratio (%): the ratio of condensed images to whole training set.</figDesc><table><row><cell></cell><cell cols="2">IPC Ratio %</cell><cell cols="2">Coreset Selection Random Herding K-Center Forgetting</cell><cell>DD  ?</cell><cell>LD  ?</cell><cell cols="2">Condensation DC DSA</cell><cell>CAFE CAFE+DSA</cell><cell>Whole Dataset</cell></row><row><cell></cell><cell>1</cell><cell cols="3">0.017 64.9?3.5 89.2?1.6 89.3?1.5 35.5?5.6</cell><cell>-</cell><cell cols="3">60.9?3.2 91.7?0.5 88.7?0.6 93.1?0.3 90.8?0.5</cell></row><row><cell>MNIST</cell><cell>10</cell><cell cols="7">0.17 95.1?0.9 93.7?0.3 84.4?1.7 68.1?3.3 79.5?8.1 87.3?0.7 97.4?0.2 97.8?0.1 97.2?0.2 97.5?0.1</cell><cell>99.6?0.0</cell></row><row><cell></cell><cell>50</cell><cell cols="3">0.83 97.9?0.2 94.8?0.2 97.4?0.3 88.2?1.2</cell><cell>-</cell><cell cols="3">93.3?0.3 98.8?0.2 99.2?0.1 98.6?0.2 98.9?0.2</cell></row><row><cell></cell><cell>1</cell><cell cols="3">0.017 51.4?3.8 67.0?1.9 66.9?1.8 42.0?5.5</cell><cell>-</cell><cell>-</cell><cell cols="2">70.5?0.6 70.6?0.6 77.1?0.9 73.7?0.7</cell></row><row><cell>FashionMNIST</cell><cell>10</cell><cell cols="3">0.17 73.8?0.7 71.1?0.7 54.7?1.5 53.9?2.0</cell><cell>-</cell><cell>-</cell><cell cols="2">82.3?0.4 84.6?0.3 83.0?0.4 83.0?0.3</cell><cell>93.5?0.1</cell></row><row><cell></cell><cell>50</cell><cell cols="3">0.83 82.5?0.7 71.9?0.8 68.3?0.8 55.0?1.1</cell><cell>-</cell><cell>-</cell><cell cols="2">83.6?0.4 88.7?0.2 84.8?0.4 88.2?0.3</cell></row><row><cell></cell><cell>1</cell><cell cols="3">0.014 14.6?1.6 20.9?1.3 21.0?1.5 12.1?1.7</cell><cell>-</cell><cell>-</cell><cell cols="2">31.2?1.4 27.5?1.4 42.6?3.3 42.9?3.0</cell></row><row><cell>SVHN</cell><cell>10</cell><cell cols="3">0.14 35.1?4.1 50.5?3.3 14.0?1.3 16.8?1.2</cell><cell>-</cell><cell>-</cell><cell cols="2">76.1?0.6 79.2?0.5 75.9?0.6 77.9?0.6</cell><cell>95.4?0.1</cell></row><row><cell></cell><cell>50</cell><cell>0.7</cell><cell cols="2">70.9?0.9 72.6?0.8 20.1?1.4 27.2?1.5</cell><cell>-</cell><cell>-</cell><cell cols="2">82.3?0.3 84.4?0.4 81.3?0.3 82.3?0.4</cell></row><row><cell></cell><cell>1</cell><cell cols="3">0.02 14.4?2.0 21.5?1.2 21.5?1.3 13.5?1.2</cell><cell>-</cell><cell cols="3">25.7?0.7 28.3?0.5 28.8?0.7 30.3?1.1 31.6?0.8</cell></row><row><cell>CIFAR10</cell><cell>10</cell><cell>0.2</cell><cell cols="6">26.0?1.2 31.6?0.7 14.7?0.9 23.3?1.0 36.8?1.2 38.3?0.4 44.9?0.5 52.1?0.5 46.3?0.6 50.9?0.5</cell><cell>84.8?0.1</cell></row><row><cell></cell><cell>50</cell><cell>1</cell><cell cols="2">43.4?1.0 40.4?0.6 27.0?1.4 23.3?1.1</cell><cell>-</cell><cell cols="3">42.5?0.4 53.9?0.5 60.6?0.5 55.5?0.6 62.3?0.4</cell></row><row><cell></cell><cell>1</cell><cell>0.2</cell><cell cols="2">4.2?0.3 8.4?0.3 8.3?0.3 4.5?0.3</cell><cell>-</cell><cell cols="3">11.5?0.4 12.8?0.3 13.9?0.3 12.9?0.3 14.0?0.3</cell></row><row><cell>CIFAR100</cell><cell>10</cell><cell>2</cell><cell cols="2">14.6?0.5 17.3?0.3 7.1?0.2 9.8?0.2</cell><cell>-</cell><cell>-</cell><cell cols="2">25.2?0.3 32.3?0.3 27.8?0.3 31.5?0.2</cell><cell>56.17?0.3</cell></row><row><cell></cell><cell>50</cell><cell>10</cell><cell>30.0?0.4 33.7?0.5 30.5?0.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.8?0.4 37.9?0.3 42.9?0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of the three components in CAFE</figDesc><table><row><cell>DL LFA Dynamic Bi-level Opt. Performance</cell></row><row><cell>49.78</cell></row><row><cell>53.96</cell></row><row><cell>54.53</cell></row><row><cell>50.92</cell></row><row><cell>54.98</cell></row><row><cell>55.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of the importance of layer-wise feature alignment. The layer1 is closest to the output layer while layer4 is closest to input layer. Note that, layer4 represents the last average pooling layer in ConvNet.</figDesc><table><row><cell>Layer1 Layer2 Layer3. Layer4 Performance/+DL</cell></row><row><cell>50.74/52.78</cell></row><row><cell>43.45/49.30</cell></row><row><cell>44.52/49.08</cell></row><row><cell>51.30/52.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>of complementarity of layer-wise feature alignment. The indexes of layers are same as Tab. 3.</figDesc><table><row><cell>Layer1 Layer2 Layer3. Layer4 Performance/+DL</cell></row><row><cell>50.74/52.78</cell></row><row><cell>51.27/53.28</cell></row><row><cell>53.16/53.96</cell></row><row><cell>54.98/55.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of ? and the training time.</figDesc><table><row><cell>?</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>DC</cell></row><row><cell>Accuracy (%)</cell><cell cols="4">53.16 55.50 54.10 53.63</cell><cell>53.9</cell></row><row><cell cols="6">Time (minutes) ?117 ?367 ?463 ?463 ?460</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The testing performance (%) on unseen architectures. The 50 IPC synthetic set is learned on one architecture (C), and then tested on another architecture (T).</figDesc><table><row><cell>C\T</cell><cell>ConvNet AlexNet</cell><cell>VGG11 ResNet18</cell><cell>MLP</cell></row><row><cell cols="4">DC ConvNet 53.9?0.5 28.77?0.7 38.76?1.1 20.85?1.0 28.71?0.7</cell></row><row><cell cols="4">CAFE ConvNet 55.50?0.4 34.02?0.6 40.55?0.8 25.27?0.9 36.67?0.6</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximating extent measures of points</title>
		<author>
			<persName><forename type="first">Sariel</forename><surname>Pankaj K Agarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><surname>Kasturi R Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient based sample selection for online continual learning</title>
		<author>
			<persName><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Goujaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scail: Classifier weights scaling for class incremental learning</title>
		<author>
			<persName><forename type="first">Eden</forename><surname>Belouadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flexible dataset distillation: Learn labels instead of images</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bohdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Francisco M Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicol?s</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Super-samples from kernel herding</title>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UAI</title>
		<imprint>
			<date type="published" when="2005">2010. 1, 2, 3, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The epic-kitchens dataset: Collection, challenges and baselines</title>
		<author>
			<persName><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.1" />
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Facility location: concepts, models, algorithms and case studies</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zanjirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farahani</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Hekmatfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Feldman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09384</idno>
		<title level="m">Introduction to core-sets: an updated survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable training of mixture models via coresets</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A ptas for k-means clustering based on weak coresets</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Monemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoCG</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Turning big data into tiny data: Constant-size coresets for kmeans, pca and projective clustering</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimal continual learning has perfect memory and is np-hard</title>
		<author>
			<persName><forename type="first">Jeremias</forename><surname>Knoblauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisham</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Diethe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gradient-based learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dataset meta-learning from kernel-ridge regression</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Suppressing mislabeled data via grouping and self-attention</title>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="786" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Crafting better contrastive views for siamese representation learning</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03278</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2005">2018. 1, 2, 3, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional neural networks applied to house numbers digit classification</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Small-gan: Speeding up gan training using core-sets</title>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative teaching networks: Accelerating neural architecture search by learning to generate synthetic training data</title>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Petroski Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An empirical study of example forgetting during deep neural network learning</title>
		<author>
			<persName><forename type="first">Mariya</forename><surname>Toneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Tachet Des Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2005">2019. 2, 3, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Suppressing uncertainties for large-scale facial expression recognition</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6897" to="6906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4057" to="4069" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mask aware network for masked face recognition in the wild</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1456" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An efficient training approach for very large scale face recognition</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10375,2021.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10959</idno>
		<imprint>
			<date type="published" when="2005">2018. 2, 4, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">Dataset distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Submodularity in data subset selection and active learning</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Facility location: concepts, models, algorithms and case studies</title>
		<author>
			<persName><forename type="first">G W</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fashionmnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Free lunch for few-shot learning: Distribution calibration</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02687,2021.1</idno>
		<title level="m">Objects in semantic topology</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bridging the gap between few-shot and many-shot learning via distribution calibration</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Single-view 3d object reconstruction from shape priors in memory</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive semantic-visual tree for hierarchical embeddings</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Online coreset selection for rehearsal-based continual learning</title>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divyam</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01085</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Morphmlp: A self-attention free, mlp-like backbone for image and video</title>
		<author>
			<persName><forename type="first">David Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashwat</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12527</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning dynamical human-joint affinity for 3d pose estimation in videos</title>
		<author>
			<persName><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7914" to="7925" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph-based few-shot learning with transformed feature propagation and optimal class allocation</title>
		<author>
			<persName><forename type="first">Ruiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dataset condensation with differentiable siamese augmentation</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005">2021. 2, 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Dataset condensation with distribution matching</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04181</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dataset condensation with gradient matching</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 3, 4, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
