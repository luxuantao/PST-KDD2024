<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian geometric modeling of indoor scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luca</forename><forename type="middle">Del</forename><surname>Pero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Bowdish</surname></persName>
							<email>jbowdish@email.arizona.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
							<email>dfried@email.arizona.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bonnie</forename><surname>Kermgard</surname></persName>
							<email>kermgard@email.arizona.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Hartley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian geometric modeling of indoor scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F79EB02A895278DDB63C28001DDBA9A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method for understanding the 3D geometry of indoor environments (e.g. bedrooms, kitchens) while simultaneously identifying objects in the scene (e.g. beds, couches, doors). We focus on how modeling the geometry and location of specific objects is helpful for indoor scene understanding. For example, beds are shorter than they are wide, and are more likely to be in the center of the room than cabinets, which are tall and narrow. We use a generative statistical model that integrates a camera model, an enclosing room "box", frames (windows, doors, pictures), and objects (beds, tables, couches, cabinets), each with their own prior on size, relative dimensions, and locations. We fit the parameters of this complex, multi-dimensional statistical model using an MCMC sampling approach that combines discrete changes (e.g, adding a bed), and continuous parameter changes (e.g., making the bed larger). We find that introducing object category leads to state-of-theart performance on room layout estimation, while also enabling recognition based only on geometry.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We propose an approach for integrating object recognition with 3D reconstruction from monocular images of indoor scenes, based on 3D reasoning and Bayesian inference. Our goal is to simultaneously estimate the camera, detect and localize in 3D the floor, ceiling, and walls comprising the room "box," and determine the position and identity of the objects (e.g., beds, couches, and tables) and frames (doors, windows, and pictures) in the room. For example, if a room contains a couch, we would like to identify it as such, as well as understand where it is in 3D. Reasoning in terms of specific objects allows us to identify several different object categories, rather than generic bounding boxes or regions of occupied space. This improves the semantic parsing of the scene, as we can now identify objects that are typically found in these environments. In this paper we used couches, beds, tables, cabinets, windows, doors, and picture frames.</p><p>We are motivated by the recent advancements in the task of recovering the 3D geometry of indoor scenes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Specifically, there has been much interest in estimating the 3D layout of rooms from single images (position of walls, floor and ceiling), as this provides crucial information on the scene geometric context, which allows to reason about objects <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> and human activities <ref type="bibr" target="#b3">[4]</ref>. For example, current approaches can estimate what part of the 3D space is free and what part is occupied by objects, modeled either in terms of clutter <ref type="bibr" target="#b4">[5]</ref> or bounding boxes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. Also, Hedau et al. <ref type="bibr" target="#b5">[6]</ref> identified beds by combining image appearance and the 3D reasoning made possible by the estimate of the room layout. To our knowledge, this was one of first attempts to provide a semantic parsing of the objects in indoor scenes based on 3D reasoning, followed by Gupta et al. <ref type="bibr" target="#b3">[4]</ref>, who managed to label surfaces and surface configurations in terms of affordances (i.e., the opportunities for human interaction provided by the environment, such as where a person can sit or reach). Finally, Hoiem et al. <ref type="bibr" target="#b7">[8]</ref> did significant work on combining 3D geometry and semantics in the scope of outdoor scenes.</p><p>In this paper we focus on exploiting the geometric properties and typical locations of specific objects for better indoor scene understanding. Simultaneously identifying objects while fitting their geometry and location improves both, and also improves the global room layout and the estimate of the camera. A key intuition is that we can discriminate between many indoor scene object categories using only gross geometry (size, relative dimensions, and position). For example, beds are much wider than they are tall, while wardrobes are the other way around (Figure <ref type="figure">2</ref>). Similarly, position and size with respect to the room box also provide hints to object identity. For example, the height of a door is usually very close to the height of the room (Figure <ref type="figure">3</ref>), but this is usually not the case for picture frames.</p><p>To integrate all these factors we use a Bayesian generative statistical model for the geometry of indoor scenes and entities within them. We set rough priors on object dimensions and their typical location from a held out image data set and from text in on-line Ikea and Home Depot catalogs ( §3.2). These priors are combined with an edge like-lihood model similar to one we used in previous work <ref type="bibr" target="#b11">[12]</ref>. Since we are focused on exploring the use of geometry, we use only edge information and do not consider color or texture. We fit the scene model using an MCMC approach that combines sampling over both discrete and continuous variables, and uses multiple threads to speed up convergence ( §4).</p><p>We find that the room layout estimation benefits from using specific objects coming from realistic categories, rather than plain bounding boxes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> or voxel occupation <ref type="bibr" target="#b4">[5]</ref>. Using a model where every component has specific semantics associated to it (e.g. beds, windows, etc.) is a key contribution of our approach, and allows us to achieve stateof-art room layout results by using geometric information only, and with minimal training ( §5). In addition, our object category recognition results, based on minimal geometric information, are promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview</head><p>As in previous work in this domain <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, we model an indoor scene as a collection of right-angled parallelepipeds (blocks), parametrized in terms of the 3D position of their center and size. A single block is used to model the room box. Objects in the scene are also approximated using blocks, which provide reasonable bounding-boxes for most furniture, for example beds and tables. Block objects have to lie on the room floor or be attached to a wall in case of doors and windows, and cannot overlap. This is a suitable scenario for incorporating priors on object size and position in 3D, as all this information is encoded in the model.</p><p>To integrate this prior information with evidence coming from the image data D, we rely on a generative Bayesian framework. We define θ as the model parameters, which comprise block parameters and camera parameters, assuming that the image data is generated by the projection of the blocks in the scene under the given camera. We then introduce the posterior distribution</p><formula xml:id="formula_0">p(θ) ∝ p(D|θ)π(θ) ,<label>(1)</label></formula><p>where p(D|θ) is the likelihood function and π(θ) the prior over model parameters. We use category-dependent priors, that inform both where objects in a specific category tend to be, and the size of each dimension (e.g. beds are usually quite short and wide, and against a wall). While we approximate all objects with simple bounding boxes, these category-dependent priors allow us to estimate the most likely class for a given object, based on its position and size. During inference the likelihood and the prior can act as competing forces, as the former will force the objects to change in order to better fit the image data, while the latter will prevent them from changing to positions or sizes that are unlikely for that specific class. Intuitively, a good solution is when an object of the right category fits the image data well and, at the same time, its parameters will be in a region of high probability for the prior distribution for that category.</p><p>In our approach, the room box and the objects within it are fit to image data simultaneously for two main reasons: 1) one cannot robustly identify the room box and the camera without adding objects in it, since the layout can be estimated correctly only if we take occlusions into account <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>; and 2) an individual object can be identified more effectively if we take into account the contextual information provided by its position and size with respect to the room box and the other objects in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A geometric model for indoor scenes</head><p>The model parameters θ = (s, c) comprise scene s and camera parameters c. As explained above, the scene consists of a collection of blocks, parametrized in terms of the 3D position of the center, their width, height, length, and the amount of rotation γ around their y axis</p><formula xml:id="formula_1">b i = (x i , y i , z i , w i , l i , γ i ) .</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>The room box itself is approximated with one of such blocks r = b r = (x r , y r , z r , w r , l r , γ r ) .</p><p>These structures are also used to model objects inside the room, since they can provide a reasonable approximation for pieces of furniture such as couches and beds (one can think of them as bounding boxes), or for objects on the walls, which we call frames. Windows and doors are an example, and are approximated with very thin blocks. We define each object in the room as</p><formula xml:id="formula_4">o i = (b i , t i ) ,<label>(4)</label></formula><p>where t i defines the object type. The whole scene is then modeled as a room box containing an unknown number of objects n s = (r, o 1 , ..., o n ) .</p><p>We parametrize the camera as in our previous work <ref type="bibr" target="#b11">[12]</ref> </p><formula xml:id="formula_6">c = (f, φ, ψ) ,<label>(6)</label></formula><p>where f, φ and ψ are, respectively, the focal length, the pitch and the roll angle. Since we cannot determine absolute position when reconstructing from single images, we can arbitrarily position the camera at the world origin, looking down the negative z axis. This, together with φ, ψ and the rotation angle of the room γ r , fully determine the camera extrinsic parameters <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Finally, we assume that the principal point is in the image center, and that there is no skew.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The image model</head><p>Our image model is similar to the one used by Schlecht and Barnard <ref type="bibr" target="#b12">[13]</ref>. Specifically, we assume that given an instance of the model parameters θ i = (s i , c i ), image features D = (f 1 , ..., f s ) are generated by the projection of the 3D scene s i under the given camera. We use two features that proved useful in this domain: edges <ref type="bibr" target="#b11">[12]</ref> and orientation surfaces <ref type="bibr" target="#b8">[9]</ref>.</p><p>Image edges. We assume image edges to be generated by the blocks in the scene. We measure the quality of a fit by comparing the set of edges E d detected on the image plane to the set of edges E m generated by projecting the model. As Schlecht et al. <ref type="bibr" target="#b12">[13]</ref>, we define a likelihood function p(E d |E m ), which we approximate using the following intuitions:</p><p>• An edge point e dj ∈ E d detected in the image plane should be matched to an edge point e mk ∈ E m generated by the model. If the match is good the two points should be very close to each other, and the difference in orientation between the two should be minimal. We approximate p(e</p><formula xml:id="formula_7">dj |e mk ) = N (d jk , 0, σ d )N (φ jk , 0, σ φ )</formula><p>, where d jk is the distance between the points, and φ jk the difference in orientation between the edges.</p><p>• We penalize a detected edge point that is not matched to any model edge (noise). We define p n as the probability of such an event occurring</p><p>• We explain points in E m not matched to any point in E d as missing detections, and define probabilities p hmiss and p smiss . The former is used for "hard" edges arising from occlusion boundaries, such as the edges that belong to the silhouette of an object. The latter is used for "soft" edges that are less likely to be found by the edge detector, such as the room edges and non-silhouette edges from objects. Notice that the detector missing a "hard" edge is less likely than a "soft" edge. One of the advantages of using a full 3D model, is that we can determine whether edge points in E m are soft or hard.</p><p>We then have</p><formula xml:id="formula_8">p(E d |E m ) = p Nn n p Nsmiss smiss p N hmiss hmiss (j,k)∈matches p(e dj |e mk ) ,<label>(7)</label></formula><p>where N n is the number of edge points labeled as noise, and N smiss (N hmiss ) the number of missed soft (hard) edges. We match points in a greedy fashion by finding the closest point e m to a data edge e d along the edge gradient, provided that this distance is smaller than 40 pixels. We further adjust this likelihood function, in order to make it independent of the number of edge points, which we found makes it more stable over a larger variety of input data. Specifically, we use</p><formula xml:id="formula_9">p(E d |E m ) ≈ p(E d |E m ) (N hmiss +Nsmiss+Nn+N matches ) -1 . (8)</formula><p>Orientation surfaces. Indoor environments typically satisfy the Manhattan World assumption, since most surfaces in the scene are aligned along three orthogonal directions. Thus, most pixels in the scene are generated by a plane aligned with one of these directions, and we can estimate which one using the approach by Lee et al. <ref type="bibr" target="#b9">[10]</ref>. We compare the pixel orientation O d detected from the image plane with the orientation surfaces O m generated by projecting our model. We approximate p(O d |O m ) as the ratio between the number of pixels such that the orientation detected on the image plane agrees with the orientation predicted by the model, and the total number of pixels. Notice that this is a number between 0 and 1.</p><p>Combining the two features. Assuming independence between the edges and the orientation features, we define our likelihood function</p><formula xml:id="formula_10">p(D|θ) = p(E d |E m )p(O d |O m ) α , (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>where α is used to weigh the importance of the orientation likelihood (experiments on the Hedau test set suggest that 6 is a good value for this parameter). In Figure <ref type="figure" target="#fig_0">1</ref> we show that these two features work very well together, as the errors in the edge detection process can be fixed using orientation surfaces, and vice versa. Using edges also helps improving the camera fit when starting from a wrong estimate of the vanishing points, which are detected at the beginning and used to initialize the camera parameters <ref type="bibr" target="#b11">[12]</ref>. In fact, since the algorithm for computing orientation maps depends on the initial vanishing point estimation, this feature is compromised by this initial error, whereas edges are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model priors</head><p>A major novelty of our approach is that priors on scene elements help global scene understanding, and are also key for identifying objects based on geometry cues, such as size and location, alone. For example, wardrobe cabinets are tall and narrow and typically against the wall, while tables are usually shorter, wider, and in the middle of the room. Notice that, since we are reconstructing from a single image, we have one overall scale ambiguity, and thus priors on object "size" and camera height are always relative to the overall room box size.</p><p>We start by introducing priors over the room box π(r), the camera parameters π(c), and each object o i inside the room π(o i ). Assuming independence, we compute the prior over the model parameters as where we take the geometric means of the object priors, so that we can compare models with a different number of objects. e is a stabilizing factor, which we set to 0.</p><formula xml:id="formula_12">π(θ) = π(r)π(c) n i=1 π(o i ) e n ,<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>In what follows, we describe each of these components, followed by how we set their parameters from training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Prior on room box</head><p>The room box is defined in terms of the center position in 3D (x r , y r , z r ) and its width, height and length (w r , h r , l r ). First, we define a prior over the ratio between width and length</p><formula xml:id="formula_13">r r1 = max(w r , l r ) min(w r , l r ) .<label>(11)</label></formula><p>We use this formulation since we do not know in advance which dimension is the largest. We are also interested in the average ratio between room width (length) and height</p><formula xml:id="formula_14">r r2 = max(w r , l r ) h r . (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>As explained in further detailed in the Section 3.3, these two quantities have very little impact on the quality of the final solution. However, they reduce time spent in regions of the sampling space with low probability, especially during the early stages of the inference process. Further, these two components prevent the sides of the room that are not visible from expanding arbitrarily, and this also makes the inference more efficient. We then have</p><formula xml:id="formula_16">π(r b ) = N (r r1 , µ r1 , σ r1 )N (r r2 , µ r2 , σ r2 ) ,<label>(13)</label></formula><p>where we assumed that the two quantities are normally distributed and independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Prior on camera parameters</head><p>We found that the camera height from the floor c h is a particularly discriminative feature in indoor scenes. Small variations in this quantity result in major changes in the image plane. For this reason, we introduce a prior on the ratio between camera height and room height (again, we cannot use absolute sizes)</p><formula xml:id="formula_17">π(c) = N (c h , µ ch , σ ch ) . (<label>14</label></formula><formula xml:id="formula_18">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Prior on objects.</head><p>Several categories of furniture and frames have a very distinctive size (Figure <ref type="figure">2</ref>). In this section, we introduce a general formulation for a prior for a specific object category τ that exploits this intuition. Given an object o i defined in terms of its size (w i , h i , l i ), and a room with dimensions (w r , h r , l r ), we are interested in the following quantities</p><p>• ratio between object height and largest dimension r i1 = h i /max(w i , l i ) (Figure <ref type="figure">2</ref>)</p><p>• ratio between object width and length r i2 = max(w i , l i )/min(w i , l i ) (Figure <ref type="figure">2</ref>)</p><p>• ratio between room height and object height r i3 = h r /h i (Figure <ref type="figure">3</ref>)</p><p>The first two ratios carry information on the object structure, and do not depend on the scene. As shown in Figure <ref type="figure">2</ref>, both features can help distinguish between different object classes. Notice that we do not use the second component for frames, since these objects are very thin blocks attached to a wall, and it would thus not make sense to use this measure.</p><p>The third quantity encodes information on the relative size of an object with respect to the room. Intuitively, the height of a bed is quite small with respect to the room height, whereas the height of a wardrobe or of a door is quite large (Figure <ref type="figure">3</ref>). Assuming that these quantities are normally distributed, we introduce prior distributions</p><formula xml:id="formula_19">π j (o i |t i = τ ) = N (r ij ; µ τ j , σ τ j ) ,<label>(15)</label></formula><p>for j = 1, 2, 3. Each category τ has different (µ τ j , σ τ j ).</p><p>For object o i , we use the prior distribution for the category it belongs to, denoted by t i . Notice that from now on we will use the shorthand π j (o i ) for π j (o i |t i = τ ).</p><p>Last, we introduce a fourth component that relates the position of an object to that of the room box. We use a discrete variable d i that takes two possible values depending on whether o i is against a wall or not, based on the intuition that some objects tend to be against a wall (e.g. beds) more than others (tables). For frames, we use the position with respect to the floor instead. For example, doors touch the floor, while windows typically do not. For each category, we introduce distribution p τ (d i ) over these two possible values.</p><p>Last, given an object o i , we combine the components of its prior probability as follows</p><formula xml:id="formula_20">π(o i ) = p ti (d i ) 3 j=1 π j (o i ) . (<label>16</label></formula><formula xml:id="formula_21">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Setting prior probabilities from data</head><p>Setting prior probabilities for objects categories. As mentioned above, the first two components of the object prior are independent of the scene. For each category τ , we set (µ τ 1 , σ τ 1 , µ τ 2 , σ τ 2 ) using fifty random examples selected from online furniture and appliances catalogs. We recorded their dimensions, provided in the text description, and the means and variances of the relevant ratios. We used the Ikea catalog<ref type="foot" target="#foot_0">1</ref> for beds, couches, cabinets and tables, and the Home Depot catalog<ref type="foot" target="#foot_1">2</ref> for windows, doors and picture frames.</p><p>Setting the parameters for the remaining two priors is more challenging, since they relate the size of an object category to that of the room, and this information is not available in furniture catalogs. In this case, we rely on image data, and set (µ c3 , σ c3 ) as explained in Figure <ref type="figure">3</ref>. We also use images to set p τ (d), which we approximate as the frequency at which an instance of an object of category τ is against a wall, or floor if it is a frame. For training, we used the images in the test split of the Hedau dataset <ref type="bibr" target="#b4">[5]</ref>. We did not use images with ambiguous examples, where we could not tell whether a piece of furniture was against the wall or not.</p><p>Setting prior probabilities for camera and room parameters. We use training images in order to set these parameters. Following the ground truth procedure we introduced in previous work <ref type="bibr" target="#b11">[12]</ref>, we manually fit an empty room box and camera to the images in the training set. From this data, we can set the camera height parameters µ ch , σ ch . Setting parameters (µ r1 , σ r1 , µ r2 , σ r2 ) for the room box Figure <ref type="figure">2</ref>. In indoor environments, we can distinguish object classes based on their size. However, when reconstructing from single images we cannot determine absolute sizes, and we thus have to use size ratios. For example, the ratio between the height of an object and the largest between its width and length varies considerably between beds and cabinets. Similarly, the ratio between width and length can be quite discriminative too (to avoid ambiguities, we use the ratio of the largest of the two to the smallest). These two quantities define a prior on object size within a category.</p><p>Figure <ref type="figure">3</ref>. The relative size of an object with respect to the room is a very discriminative feature. We are interested in the ratio between the room height (yellow arrow) and the object height (red arrow). This can be estimated from image data by dividing the length of the yellow arrow (in pixel) by the red arrow, provided that the object is against or close to a wall. Notice that ratios of lengths of collinear segments are normally not preserved by projective transformations (only affine). However, in this domain the vanishing point for vertical segments is usually at infinity, and this method provides a reasonable approximation.</p><p>prior is more challenging, since walls are completely visible only in a few images. For example, we can use the right image in Figure <ref type="figure">3</ref> to compare the lenght of the back wall to the room height, but we cannot use the left image, since each wall is partly outside the image plane. We then use images like the former to set mean µ r2 and variance σ r2 of the ratio between room width and height. Since the main purpose of these two components is to prevent the room box from expanding too much, we can assume roughly square rooms and set µ r1 = 1. We use a large variance σ r1 to account for non square rooms and corridors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inference</head><p>We fit our model to images by sampling from the posterior distribution p(θ|D) using a reversible jump MCMC strategy. We alternate "jump" moves to add/remove objects to the scene (e.g. add a couch, or remove a door), and "diffusion" moves to sample over continuous parameters (e.g., changing the position and size of an object, of the room box or of the camera).</p><p>Diffusion moves. As in our previous work <ref type="bibr" target="#b11">[12]</ref>, we use Stochastic Dynamics <ref type="bibr" target="#b10">[11]</ref> for sampling over subsets of the continuous parameters. We alternatively sample over</p><p>• object size and position. Basically, objects are shifted around the room and stretched, and the room box and other objects have to adjust to avoid collisions • room size and position. Here, we change the room box.</p><p>When needed, objects must shrink or move in order to remain fully inside the room • camera parameters Jump moves. These moves are used to change the discrete structure of the model by adding and removing objects, since the number of objects in the room is not known a priori. Each jump move is accepted or rejected using the Metropolis Hastings acceptance formula. Here, we need a mechanism to propose objects of the right size at the right place, otherwise the acceptance ratio will be extremely low. Specifically, we use a data-driven <ref type="bibr" target="#b13">[14]</ref> strategy for adding blocks to the scene, relying on orthogonal corners detected onto the image plane <ref type="bibr" target="#b11">[12]</ref>, which proved very effective in this domain. Further, instead of proposing an object of random size, we randomly select a category τ from the set of furniture and frame categories available, and draw a sample from the size prior for τ (e.g. we propose adding a bed or a cabinet, rather than adding a generic block). If a proposal gets accepted, the object just added will be considered as an instance of class τ . We also introduce a jump move for proposing a category change for a given object (e.g. we propose to turn a "bed" into a "table"). To summarize, we use the following set of jump moves</p><p>• adding an object of a specific type to the scene from a randomly selected orthogonal corner. • removing an object • changing the category of an object • proposing a different room box from a corner to replace the current one Once an object is added to the scene, diffusion moves will try to change its size and position. As already mentioned, the prior and the likelihood will act as competing forces in this process. The latter will try to change the object parameters to better explain image edges and orientation surfaces, while the former will constrain these changes to regions of parameter spaces that are likely for the class the object belongs to. This prevents objects from assuming unnatural sizes to satisfy the likelihood function, as shown in Figure <ref type="figure" target="#fig_1">4</ref>, where we ran the sampler without using the prior. Not using the prior has two negative effects. First, it slows down the sampler, since most of the time is spent exploring regions of parameters space that would be unlikely according to the priors. Second, it also has a negative impact on the final solution, since "objects" that are good fits for noise are clearly not reasonable according to the prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Initializing and running the sampler.</head><p>We use a multi-threaded sampler to efficiently explore more of the space on modern multi-core workstations. Some details on how we initialize the sampler follow.</p><p>Finding the most promising room boxes. We start by proposing a room box from each of the detected orthogonal corners, and we initialize the camera parameters from a triplet of orthogonal vanishing points detected onto the image plane, which is a relatively standard procedure <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>. We sample briefly over the room box and camera parameters of the most promising proposals, and keep the 20 room boxes with the highest posterior value. We then use a multi-threaded strategy, where we run samplers in parallel, each initialized with one of the 20 best room boxes found so far. Notice that we are not exclusively committing to these 20 boxes, since each thread can change the parameters of the room box during execution.</p><p>Finding the most promising corners. As a last part of the initialization process, each thread iterates over the orthogonal corners, and generates object proposals for each category τ , relying on the best room hypotheses found in the previous step. We keep track of the corners that generated the object proposals with the highest posterior, and make sure that they will be used more frequently to propose objects during the sampling process itself. This initialization considerably increases the acceptance ratio of the sampler, since likely objects are proposed more often. We emphasize that we cannot iteratively add the most likely object to the scene, since early commitment to partial configurations leads to error <ref type="bibr" target="#b9">[10]</ref>.</p><p>After initialization, each thread randomly alternates the sampling moves described in the previous section. At the end of the sampling process, we join the threads and return the best global solution. The whole process takes, on average, 12 minutes per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We start by evaluating the performances of the various components of our algorithm in terms of room layout estimation, which is a standard measure in this field. This measure relies on ground truth data where each pixel in the image was labeled according to the room face it belongs to (i.e. 1= ceiling, 2= floor, 3 = right wall, etc.). The error is measured by comparing the projection of the estimated room layout against the ground truth, and computing the ratio of misclassified pixels. In Table <ref type="table" target="#tab_0">1</ref>, we can see the benefits of integrating the camera and room prior in the model, as well as the orientation component of the likelihood. We then compare the scenario where blocks of random size and no prior are used to our full algorithm, where we add specific objects such as beds and tables. We ran the two approaches for the same number of iterations, and we can see that the latter performs much better. In this case, the acceptance ratio of the sampler is much higher, since we are proposing realistic objects in likely positions, and this helps converging to a good solution. Notice that priors on objects also solve the problem illustrated in Figure <ref type="figure" target="#fig_1">4</ref>. In Table <ref type="table">2</ref>, we can see that our results are comparable to the state-of-the-art on two standard datasets.</p><p>Then, we measured performances on object recognition. We ground truthed the 340 images in the UCB dataset <ref type="bibr" target="#b15">[16]</ref> by manually identifying the seven object classes we experimented with (we did not consider objects occupying less than 1% of the image). We believe this dataset is harder than the Hedau test split <ref type="bibr" target="#b4">[5]</ref>, and we hope that this ground truth, which we made available online <ref type="bibr" target="#b0">[1]</ref>, will stimulate further experiments on this data.</p><p>To evaluate detection, we project the 3D object hypothesized by our model onto the image plane, and compare this with the ground truth object position. If the intersection of the two areas is more than 50% of their union, we consider it a correct detection. In Table <ref type="table" target="#tab_1">3</ref>, we first report precision and recall for the four furniture classes (beds, couches, cabinets and tables) and for the frame frame classes (door, window, picture frame). Here we consider a piece of furniture as correctly detected even if we confused, say, a couch for a table, and similarly for frames. We also report a confusion matrix for the instances of furniture (and frames) that were correctly identified.</p><p>We believe that these results are promising, considering that only 3D geometry and edge features are used. Visual  <ref type="table">2</ref>. Average error on room layout estimation on two standard datasets. Our approach is comparable to the state-of-the-art. inspection (Figures <ref type="figure">5</ref> and<ref type="figure" target="#fig_2">6</ref>) suggests that adding realistic objects to the scene improves the semantic understanding of the scene. The fact that results look consistent across two different datasets is promising too, as reconstructions in Figure <ref type="figure">5</ref> come from both datasets. The experiments also suggest that appearance models are needed to achieve better recognition. We can see that it is easy to make confusion between beds, couches and tables, which are quite similar in size compared to cabinets, which get confused less often. The same problem occurs between windows and picture frames. This and other limitations of our algorithm are shown in Figure <ref type="figure" target="#fig_3">7</ref>. Visual inspection showed that in many of these cases, the posterior of the right solution is very close to the false positive. This suggests that adding appearance or geometric context information <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> could improve results considerably.  . Current limitations of our approach. Top: the block over the bed seems a good fit, but is actually in the middle of the room, and is labeled as a table. We can see this in the birdview on the right, where the white rays indicate the camera field of view. In the birdview we render the full model that was fit to the image, and this includes parts of the room that are not visible in the image plane. Despite being wrong, the table block explains image features very well, and this is a major source of confusion. Bottom left: objects facing the camera directly can be both interpreted as furniture or frames. We think both problems can be solved by incorporating image appearance, which is likely to fix also mistakes like the ones in the second image of the bottom row. Last, using only priors on size generates false positives (bottom right). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Fitting specific objects, characterized with sensible priors, significantly helps scene understanding. In particular, room layout performance increased, and we had some sense of object identity. Our investigations only considered block edges; we expect that training appearance models would help correctly tag blocks. However, using only geometry, we are able to get state-of-the-art scene layout results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Advantages of integrating edges and surface orientation in the likelihood function. Faint wall edges are often missed by the edge detector (top left), and the edge likelihood alone would provide the wrong solution, by "latching" the wall edge to the window (top right). However, in this case the orientation surfaces help converge to the right solution (second row). Conversely, mistakes in the orientation map estimation can be overcome by relying on edge information (third and fourth rows).</figDesc><graphic coords="4,85.55,72.00,165.38,263.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.If we run the sampler without a prior on object size, significant time is wasted exploring regions of space that do not correspond to realistic configurations. Here, we can see a sample with a very high likelihood, since the long edges of the frame at the bottom happen to match image edges very well (especially the one formed by the pillows), and the room floor is nicely "latched" to the edge generated by the back of the couch. Without a component in the prior penalizing the unlikely size of the frame, it would take a long time before the sampler gets out of this deep local minimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Some objects correctly identified. Best viewed in color.</figDesc><graphic coords="8,181.49,182.13,73.24,74.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7</head><label>7</label><figDesc>Figure 7. Current limitations of our approach. Top: the block over the bed seems a good fit, but is actually in the middle of the room, and is labeled as a table. We can see this in the birdview on the right, where the white rays indicate the camera field of view. In the birdview we render the full model that was fit to the image, and this includes parts of the room that are not visible in the image plane. Despite being wrong, the table block explains image features very well, and this is a major source of confusion. Bottom left: objects facing the camera directly can be both interpreted as furniture or frames. We think both problems can be solved by incorporating image appearance, which is likely to fix also mistakes like the ones in the second image of the bottom row. Last, using only priors on size generates false positives (bottom right).</figDesc><graphic coords="8,130.44,264.15,75.60,50.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average error on room layout estimation on the test split of the Hedau dataset. We can see the benefit of adding each component discussed so far (see text for details).</figDesc><table><row><cell>Method</cell><cell>Error on Hedau (test) [5]</cell></row><row><cell>only edge likelihood (no blocks)</cell><cell>26.0 %</cell></row><row><cell>+ camera and room prior (no blocks)</cell><cell>24.7 %</cell></row><row><cell>+ orientation likelihood (no blocks)</cell><cell>21.3 %</cell></row><row><cell>+ random blocks</cell><cell>19.7 %</cell></row><row><cell>+ objects</cell><cell>16.3 %</cell></row></table><note><p>Figure 5. Full scene reconstructions. Best viewed in color.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Top. Detection accuracy of furniture and frames considered as groups. Bottom two tables. Confusion matrices for identified furniture and frames. Out of the pieces of furniture detected by our approach, 38% are correctly classified. The ratio of correct classifications for frames is 60%.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Precision Recall</cell><cell></cell></row><row><cell cols="2">Furniture</cell><cell>31.0 %</cell><cell>20.1 %</cell><cell></cell></row><row><cell cols="2">Frames</cell><cell>27.7 %</cell><cell>19.7 %</cell><cell></cell></row><row><cell cols="5">Bed Cabinet Couch Table</cell></row><row><cell>Bed</cell><cell>26</cell><cell>1</cell><cell>12</cell><cell>6</cell></row><row><cell>Cabinet</cell><cell>1</cell><cell>11</cell><cell>0</cell><cell>3</cell></row><row><cell>Couch</cell><cell>16</cell><cell>3</cell><cell>6</cell><cell>14</cell></row><row><cell>Table</cell><cell>13</cell><cell>4</cell><cell>5</cell><cell>5</cell></row><row><cell></cell><cell cols="4">Door Picture Window</cell></row><row><cell>Door</cell><cell>6</cell><cell>1</cell><cell>9</cell><cell></cell></row><row><cell>Picture</cell><cell>0</cell><cell>23</cell><cell>22</cell><cell></cell></row><row><cell>Window</cell><cell>7</cell><cell>14</cell><cell>49</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.ikea.com/us/en/catalog/categories/departments/bedroom/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www6.homedepot.com/cyber-monday/index.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the National Science Foundation under Grant No. 0747511.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ground truth object masks for the UCB dataset</title>
		<ptr target="http://kobus.ca/research/data/CVPR12room.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A dynamic bayesian network model for autonomous 3d reconstruction from a single indoor image</title>
		<author>
			<persName><forename type="first">E</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="2418" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic single-image 3d reconstructions of indoor manhattan world scenes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISRR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From 3d scene geometry to human workspace</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1961" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2009. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Thinking inside the box: Using appearance models and context based on room geometry</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Closing the loop on scene interpretation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2010. 1, 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Probabilistic inference using markov chain monte carlo methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sampling bedrooms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Pero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2011. 1, 2, 3, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning models of object structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image segmentation by data-driven markov chain monte-carlo</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="657" to="673" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative learning with latent variables for cluttered indoor scene understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inferring spatial layout from a single image via depth-ordered grouping</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POCV Workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
