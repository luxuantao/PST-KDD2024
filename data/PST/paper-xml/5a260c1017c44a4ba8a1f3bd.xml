<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Shot Temporal Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xu</forename><surname>Zhao</surname></persName>
							<email>zhaoxu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cooperative Medianet Innovation Center (CMIC)</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Shou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single Shot Temporal Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A307C3845A2398A04B70A6BF6B5F1C7C</idno>
					<idno type="DOI">10.1145/3123266.3123343</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Temporal Action Detection</term>
					<term>Untrimmed Video</term>
					<term>SSAD network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances. This problem requires not only recognizing action categories but also detecting start time and end time of each action instance. Many state-of-the-art methods adopt the "detection by classi cation" framework: rst do proposal, and then classify proposals. The main drawback of this framework is that the boundaries of action instance proposals have been xed during the classi cation step. To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. On pursuit of designing a particular SSAD network that can work e ectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted. Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy. We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD signi cantly outperforms other state-of-the-art systems by increasing mAP from 19.0% to 24.6% on THUMOS 2014 and from 7.4% to 11.0% on MEXaction2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Due to the continuously booming of videos on the internet, video content analysis has attracted wide attention from both industry and academic eld in recently years. An important branch of video content analysis is action recognition, which usually aims at classifying the categories of manually trimmed video clips. Substantial progress has been reported for this task in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. However, most videos in real world are untrimmed and may contain multiple action instances with irrelevant background scenes or activities. This problem motivates the academic community to put attention to another challenging task -temporal action detection. This task aims to detect action instances in untrimmed video, including temporal boundaries and categories of instances. Methods proposed for this task can be used in many areas such as surveillance video analysis and intelligent home care.</p><p>Temporal action detection can be regarded as a temporal version of object detection in image, since both of the tasks aim to determine the boundaries and categories of multiple instances (actions in time/ objects in space). A popular series of models in object detection are R-CNN and its variants <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27]</ref>, which adopt the "detect by classifying region proposals" framework. Inspired by R-CNN, recently many temporal action detection approaches adopt similar framework and classify temporal action instances generated by proposal method <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref> or simple sliding windows method <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref>. This framework may has some major drawbacks: <ref type="bibr" target="#b0">(1)</ref> proposal generation and classi cation procedures are separate and have to be trained separately, but ideally we want to train them in a joint manner to obtain an optimal model; (2) the proposal generation method or sliding windows method requires additional time consumption; (3) the temporal boundaries of action instances generated by the sliding windows method are usually approximative rather than precise and left to be xed during classi cation. Also, since the scales of sliding windows are pre-determined, it is not exible to predict instances with various scales.</p><p>To address these issues, we propose the Single Shot Action Detector (SSAD) network, which is a temporal convolutional network conducted on feature sequence with multiple granularities. Inspired by another set of object detection methods -single shot detection models such as SSD <ref type="bibr" target="#b19">[20]</ref> and YOLO <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, our SSAD network skips the proposal generation step and directly predicts temporal boundaries and con dence scores for multiple action categories, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. SSAD network contains three sub-modules:</p><p>(1) base layers read in feature sequence and shorten its temporal length; <ref type="bibr" target="#b1">(2)</ref> anchor layers output temporal feature maps, which are associated with anchor action instances; (3) prediction layers generate categories probabilities, location o sets and overlap scores of these anchor action instances.</p><p>For better encoding of both spatial and temporal information in video, we adopt multiple action recognition models (action classiers) to extract multiple granularities features. We concatenate the output categories probabilities from all action classi ers in snippetlevel and form the Snippet-level Action Score (SAS) feature. The sequences of SAS features are used as input of SSAD network.</p><p>Note that it is non-trivial to adapt the single shot detection model from object detection to temporal action detection. Firstly, unlike VGGNet <ref type="bibr" target="#b30">[31]</ref> being used in 2D ConvNet models, there is no existing widely used pre-trained temporal convolutional network. Thus in this work, we search multiple network architectures to nd the best one. Secondly, we integrate key advantages in di erent single shot detection models to make our SSAD network work the best. On one hand, similar to YOLO9000 <ref type="bibr" target="#b25">[26]</ref>, we simultaneously predict location o sets, categories probabilities and overlap score of each anchor action instance. On the other hand, like SSD <ref type="bibr" target="#b19">[20]</ref>, we use anchor instances of multiple scale ratios from multiple scales feature maps, which allow network exible to handle action instance with various scales. Finally, to further improve performance, we fuse the prediction categories probability with temporal pooled snippetlevel action scores during prediction.</p><p>The main contributions of our work are summarized as follows:</p><p>(1) To the best of our knowledge, our work is the rst Single Shot Action Detector (SSAD) for video, which can e ectively predict both the boundaries and con dence score of multiple action categories in untrimmed video without the proposal generation step.</p><p>(2) In this work, we explore many con gurations of SSAD network such as input features type, network architectures and postprocessing strategy. Proper con gurations are adopted to achieve better performance for temporal action detection task.</p><p>(3) We conduct extensive experiments on two challenging benchmark datasets: THUMOS'14 <ref type="bibr" target="#b13">[14]</ref> and MEXaction2 <ref type="bibr" target="#b0">[1]</ref>. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD signi cantly outperforms other state-of-the-art systems by increasing mAP from 19.0% to 24.6% on THUMOS'14 and from 7.4% to 11.0% on MEXaction2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Action recognition. Action recognition is an important research topic for video content analysis. Just as image classi cation network can be used in image object detection, action recognition models can be used in temporal action detection for feature extraction. We mainly review the following methods which can be used in temporal action detection. Improved Dense Trajectory (iDT) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> feature is consisted of MBH, HOF and HOG features extracted along dense trajectories. iDT method uses SIFT and optical ow to eliminate the in uence of camera motion. Two-stream network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref> learns both spatial and temporal features by operating network on single frame and stacked optical ow eld respectively using 2D Convolutional Neural Network (CNN) such as GoogleNet <ref type="bibr" target="#b34">[35]</ref>, VGGNet <ref type="bibr" target="#b30">[31]</ref> and ResNet <ref type="bibr" target="#b11">[12]</ref>. C3D network <ref type="bibr" target="#b35">[36]</ref> uses 3D convolution to capture both spatial and temporal information directly from raw video frames volume, and is very e cient. Feature encoding methods such as Fisher Vector <ref type="bibr" target="#b37">[38]</ref> and VAE <ref type="bibr" target="#b23">[24]</ref> are widely used in action recognition task to improve performance. And there are many widely used action recognition benchmark such as UCF101 <ref type="bibr" target="#b33">[34]</ref>, HMDB51 <ref type="bibr" target="#b17">[18]</ref> and Sports-1M <ref type="bibr" target="#b15">[16]</ref>.</p><p>Temporal action detection. This task focuses on learning how to detect action instances in untrimmed videos where the boundaries and categories of action instances have been annotated. Typical datasets such as THUMOS 2014 <ref type="bibr" target="#b13">[14]</ref> and MEXaction2 <ref type="bibr" target="#b0">[1]</ref> include large amount of untrimmed videos with multiple action categories and complex background information.</p><p>Recently, many approaches adopt "detection by classi cation" framework. For examples, many approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> use extracted feature such as iDT feature to train SVM classi ers, and then classify the categories of segment proposals or sliding windows using SVM classi ers. And there are some approaches specially proposed for temporal action proposal <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref>. Our SSAD network di ers from these methods mainly in containing no proposal generation step.</p><p>Recurrent Neural Network (RNN) is widely used in many action detection approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> to encode feature sequence and make per-frame prediction of action categories. However, it is di cult for RNNs to keep a long time period memory in practice <ref type="bibr" target="#b31">[32]</ref>. An alternative choice is temporal convolution. For example, Lea et al. <ref type="bibr" target="#b18">[19]</ref> proposes Temporal Convolutional Networks (TCN) for temporal action segmentation. We also adopt temporal convolutional layers, which makes our SSAD network can handle action instances with a much longer time period.</p><p>Object detection. Deep learning approaches have shown salient performance in object detection. We will review two main set of object detection methods proposed in recent years. The representative methods in rst set are R-CNN <ref type="bibr" target="#b8">[9]</ref> and its variations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. R-CNN uses selective search to generate multiple region proposals then apply CNN in these proposals separately to classify their categories; Fast R-CNN <ref type="bibr" target="#b7">[8]</ref> uses a 2D RoI pooling layer which makes feature map be shared among proposals and reduces the time consumption. Faster RCNN <ref type="bibr" target="#b26">[27]</ref> adopts a RPN network to generate region proposal instead of selective search.</p><p>Another set of object detection methods are single shot detection methods, which means detecting objects directly without generating proposals. There are two well known models. YOLO <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> uses the whole topmost feature map to predict probabilities of multiple categories and corresponding con dence scores and location o sets. SSD <ref type="bibr" target="#b19">[20]</ref>   multiple scales default boxes. In our work, we combine the characteristics of these single shot detection methods and embed them into the proposed SSAD network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head><p>In this section, we will introduce our approach in details. The framework of our approach is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem De nition</head><p>We denote a video as X = {x t } T t =1 where T is the number of frames in X and x t is the t-th frame in X . Each untrimmed video X is annotated with a set of temporal action instances</p><formula xml:id="formula_0">Φ = ϕ n = φ n ,φ n ,k n N n=1</formula><p>, where N is the number of temporal action instances in X , and φ n ,φ n ,k n are starting time, ending time and category of action instance ϕ n respectively. k n ∈ {1, ...,K } where K is the number of action categories. Φ is given during training procedure and need to be predicted during prediction procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extracting of Snippet-level Action Scores</head><p>To apply SSAD model, rst we need to make snippet-level action classi cation and get Snippet-level Action Score (SAS) features. Given a video X , a snippet s t = (x t , F t ,X t ) is composed by three parts:</p><formula xml:id="formula_1">x t is the t-th frame in X , F t = f t t +5 t =t -4 is stacked optical ow eld derived around x t and X t = {x t } t +8</formula><p>t =t -7 is video frames volume. So given a video X , we can get a sequence of snippets S = {s t } T t =1 . We pad the video X in head and tail with rst and last frame separately to make S have the same length as X .</p><p>Action classi er. To evaluate categories probability of each snippet, we use multiple action classi ers with commendable performance in action recognition task: two-stream network <ref type="bibr" target="#b29">[30]</ref> and C3D network <ref type="bibr" target="#b35">[36]</ref>. Two-stream network includes spatial and temporal networks which operate on single video frame x t and stacked optical ow eld F t respectively. We use the same two-stream network architecture as described in <ref type="bibr" target="#b39">[40]</ref>, which adopts VGGNet-16 network architecture. C3D network is proposed in <ref type="bibr" target="#b35">[36]</ref>, including multiple 3D convolution layers and 3D pooling layers. C3D network operates on short video frames volume X t with length l, where l is the length of video clip and is set to 16 in C3D. So there are totally three individual action classi ers, in which spatial network measures the spatial information, temporal network measures temporal consistency and C3D network measures both. In section 4.3, we evaluate the e ect of each action classi er and their combinations.</p><p>SAS feature. As shown in Figure <ref type="figure" target="#fig_1">2</ref>(a), given a snippet s t , each action classi er can generate a score vector p t with length K = K + 1, where K includes K action categories and one background category. Then we concatenate output scores of each classi ers to form the Snippet-level Action Score (SAS) feature p sas,t = p S,t ,p T ,t ,p C,t , where p S,t , p T ,t , p C,t are output score of spatial, temporal and C3D network separately. So given a snippets sequence S with length T , we can extract a SAS feature sequence P = p sas,t</p><formula xml:id="formula_2">T t =1</formula><p>. Since the number of frames in video is uncertain and may be very large, we use a large observation window with length T w to truncate the feature sequence. We denote a window as ω = φ ω ,φ ω ,P ω , Φ ω , where φ ω and φ ω are starting and ending time of ω, P ω and Φ ω are SAS feature sequence and corresponding ground truth action instances separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SSAD Network</head><p>Temporal action detection is quite di erent from object detection in 2D image. In SSAD we adopt two main characteristics from single shot object detection models such as SSD <ref type="bibr" target="#b19">[20]</ref> and YOLO <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>: 1) unlike "detection by classi cation" approaches, SSAD directly predicts categories and location o sets of action instances in untrimmed video using convolutional prediction layers; 2) SSAD combine temporal feature maps from di erent convolution layers for prediction, making it possible to handle action instances with various length. We rst introduce the network architecture.</p><p>Network architecture. The architecture of SSAD network is presented in Figure <ref type="figure" target="#fig_1">2</ref> In SSAD network, we adopt 1D temporal convolution and pooling to capture temporal information. We conduct Recti ed Linear Units (ReLu) activation function <ref type="bibr" target="#b10">[11]</ref> to output temporal feature map except for the convolutional prediction layers. And we adopt temporal max pooling since max pooling can enhance the invariance of small input change.</p><p>Base layers. Since there are no widely used pre-trained 1D ConvNet models such as the VGGNet <ref type="bibr" target="#b30">[31]</ref> used in 2D ConvNet models, we search many di erent network architectures for SSAD network. These architectures only di er in base layers while we keep same architecture of anchor layers and prediction layers. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, we totally design 5 architectures of base layers. In these architectures, we mainly explore three aspects: 1) whether use convolution or pooling layer to shorten the temporal dimension and increase the size of receptive elds; 2) number of layers of network and 3) size of convolution layer's kernel. Notice that we set the number of convolutional lter in all base layers to 256. Evaluation results of these architectures are shown in section 4.3, and nally we adopt architecture B which achieves the best performance.</p><p>Multi-scale anchor layers. After processing SAS feature sequence using base layers, we stack three anchor convolutional layers (Conv-A1, Conv-A2 and Conv-A3) on them. These layers have same con guration: kernel size 3, stride size 2 and 512 convolutional lters. The output anchor feature maps of anchor layers are f A1 , f A2 and f A3 with size (T w /32 × 512), (T w /64 × 512) and (T w /128 × 512) separately. Multiple anchor layers decrease temporal dimension of feature map progressively and allow SSAD get predictions from multiple resolution feature map.</p><p>For each temporal feature map of anchor layers, we associate a set of multiple scale anchor action instances with each feature map cell as shown in Figure <ref type="figure" target="#fig_4">4</ref>. For each anchor instance, we use convolutional prediction layers to predict overlap score, classi cation score and location o sets, which will be introduced later.</p><p>In term of the details of multi-scale anchor instances, the lower anchor feature map has higher resolution and smaller receptive eld than the top anchor feature map. So we let the lower anchor layers detect short action instances and the top anchor layers detect long action instances. For a temporal feature map f of anchor layer with length M, we de ne base scale s f = 1</p><p>M and a set of scale ratios R f = r d D f d=1 , where D f is the number of scale ratios. We use {1, 1.5, 2} for f A1 and {0.5, 0.75, 1, 1.5, 2} for f A2 and f A3 . For each ratio r d , we calculate µ w = s f • r d as anchor instance's default width. And all anchor instances associated with the m-th feature map cell share the same default center location µ c = m+0. 5  M . So for an anchor feature map f with length M f and D f scale ratios, the number of associated anchor instances is</p><formula xml:id="formula_3">M f • D f .</formula><p>Prediction layers. We use a set of convolutional lters to predict classi cation scores, overlap scores and location o sets of anchor instances associated with each feature map cell. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, for an anchor feature map f with length M f and D f scale ratios, we use D f • (K + 3) temporal convolutional lters with kernel size 3, stride size 1 for prediction. The output of prediction layer has size M f × D f • (K + 3) and can be reshaped into M f • D f × (K + 3) . Each anchor instance gets a prediction score vector p pr ed = p c l ass ,p o er , ∆c, ∆w with length (K + 3), where p c l ass is classi cation score vector with length K , p o er is overlap score and ∆c, ∆w are location o sets. Classi cation score p cl ass is used to predict anchor instance's category. Overlap score p o er is used to estimate the overlap between anchor instance and ground truth instances and should have value between [0, 1], so it is normalized by using sigmoid function: (1) And location o sets ∆c, ∆w are used for adjusting the default location of anchor instance. The adjusted location is de ned as:</p><formula xml:id="formula_4">φ c = µ c + α 1 • µ w • ∆c φ w = µ w • exp(α 2 • ∆w ),<label>(2)</label></formula><p>where φ c and φ w are center location and width of anchor instance respectively. α 1 and α 2 are used for controlling the e ect of location o sets to make prediction stable. We set both α 1 and α 2 to 0.1. The starting and ending time of action instance are φ = φ c -1 2 • φ w and φ = φ c + 1 2 •φ w respectively. So for a anchor feature map f , we can get a anchor instances set Φ f = ϕ n = φ c ,φ w ,p cl ass ,p o er N f n=1 , where N f = M f • D f is the number of anchor instances. And the total prediction instances set is</p><formula xml:id="formula_5">Φ p = Φ f A1 , Φ f A2 , Φ f A3 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training of SSAD network</head><p>Training data construction. As described in Section 3.2, for an untrimmed video X with length T , we get SAS features sequence P with same length. Then we slide window of length T w in feature sequence with 75% overlap. The overlap of sliding window is aim to handle the situation where action instances locate in boundary of window and also used to increase the amount of training data.</p><p>During training, we only keep windows containing at least one ground-truth instance. So given a set of untrimmed training videos, we get a training set Ω = {ω n } N ω n=1 , where N ω is the number of windows. We randomly shu e the data order in training set to make the network converge faster, where same random seed is used during evaluation.</p><p>Label assignment. During training, given a window ω, we can get prediction instances set Φ p via SSAD network. We need to match them with ground truth set Φ ω for label assignment. For an anchor instance ϕ n in Φ p , we calculate it's IoU overlap with all ground truth instances in Φ ω . If the highest IoU overlap is higher than 0.5, we match ϕ n with corresponding ground truth instance ϕ and regard it as positive, otherwise negative. We expand ϕ n with matching information as ϕ n = φ c ,φ w ,p c l ass ,p o er ,k , iou , c , w , where k is the category of ϕ and is set to 0 for negative instance, iou is the IoU overlap between ϕ n and ϕ , c and w are center location and width of ϕ respectively. So a ground truth instance can match multiple anchor instances while a anchor instance can only match one ground truth instance at most.</p><p>Hard negative mining. During label assignment, only a small part of anchor instances match the ground truth instances, causing an imbalanced data ratio between the positive and negative instances. Thus we adopt the hard negative mining strategy to reduce the number of negative instances. Here, the hard negative instances are de ned as negative instances with larger overlap score than 0.5. We take all hard negative instances and randomly sampled negative instances in remaining part to make the ratio between positive and negative instances be nearly 1:1. This ratio is chosen by empirical validation. So after label assignment and hard negative mining, we get Φ p = ϕ n N t r ain n=1 as the input set during training, where N t r ain is the number of total training instances and is the sum of the number of positives N pos and negatives N ne .</p><p>Objective for training. The training objective of the SSAD network is to solve a multi-task optimization problem. The overall loss function is a weighted sum of the classi cation loss (class), the overlap loss (conf), the detection loss (loc) and L2 loss for regularization:</p><formula xml:id="formula_6">L = L cl ass + α • L o er + β • L loc + λ • L 2 (Θ),<label>(3)</label></formula><p>where α, β and λ are the weight terms used for balancing each part of loss function. Both α and β are set to 10 and λ is set to 0.0001 by empirical validation. For the classi cation loss, we use conventional softmax loss over multiple categories, which is e ective for training classi cation model and can be de ned as:</p><formula xml:id="formula_7">L cl ass = L sof tmax = 1 N t r ain N t r ain i=1 (-lo (P (k ) i )),<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">P (k ) i = exp (p (k ) cl as s,i ) j exp (p (k j ) cl as s,i )</formula><p>and k is the label of this instance.</p><p>L o er is used to make a precise prediction of anchor instances' overlap IoU score, which helps the procedure of NMS. The overlap loss adopts the mean square error (MSE) loss and be de ned as:</p><formula xml:id="formula_9">L o er = 1 N t r ain N t r ain i=1 (p o er,i -iou,i ). (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>L loc is the Smooth L1 loss <ref type="bibr" target="#b7">[8]</ref> for location o sets. We regress the center (ϕ c ) and width (ϕ w ) of predicted instance: <ref type="bibr" target="#b5">(6)</ref> where c,i and w,i is the center location and width of ground truth instance. L 2 (Θ) is the L2 regularization loss where Θ stands for the parameter of the whole SSAD network.</p><formula xml:id="formula_11">L loc = 1 N pos N pos i=1 (SL 1 (ϕ c,i -c,i ) + SL 1 (ϕ w,i -w,i )),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Prediction and post-processing</head><p>During prediction, we follow the aforementioned data preparation method during the training procedure to prepare test data, with the following two changes: (1) the overlap ratio of window is reduced to 25% to increase the prediction speed and reduce the redundant predictions; (2) instead of removing windows without annotation, we keep all windows during prediction because the removing operation is actually a leak of annotation information. If the length of input video is shorter than T w , we will pad SAS feature sequence to T w so that there is at least one window for prediction. Given a video X , we can get a set of Ω = {ω n } N ω n=1 . Then we use SSAD network to get prediction anchors of each window and merge these prediction as Φ p = ϕ n N p n=1 , where N p is the number of prediction instances. For a prediction anchor instance ϕ n in Φ p , we calculate the mean Snippet-level Action Score psas among the temporal range of instance and multiple action classi ers.</p><formula xml:id="formula_12">psas = 1 3 • (φ -φ) φ t =φ p S,t + p T ,t + p C,t ,<label>(7)</label></formula><p>where φ and φ are starting and ending time of prediction anchor instance respectively. Then we fuse categories scores psas and p cl ass with multiplication factor p conf and get the p f inal :</p><formula xml:id="formula_13">p f inal = p o er • (p cl ass + psas ) .<label>(8)</label></formula><p>We choose the maximum dimension k p in p f inal as the category of ϕ n and corresponding score p conf as the con dence score. We expand ϕ n as ϕ n = φ c ,φ w ,p conf ,k p and get prediction set Φ p = ϕ n N p n=1 . Then we conduct non-maximum suppress (NMS) in these prediction results to remove redundant predictions with con dence score p conf and get the nal prediction instances set Φ p = ϕ n N p n=1 , where N p is the number of the nal prediction anchors. Since there are little overlap between action instances of same category in temporal action detection task, we take a strict threshold in NMS, which is set to 0.1 by empirical validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Dataset and setup</head><p>THUMOS 2014 <ref type="bibr" target="#b13">[14]</ref>. The temporal action detection task of THU-MOS 2014 dataset is challenging and widely used. The training set is the UCF-101 <ref type="bibr" target="#b33">[34]</ref> dataset including 13320 trimmed videos of 101 categories. The validation and test set contain 1010 and 1574 untrimmed videos separately. In temporal action detection task, only 20 action categories are involved and annotated temporally. We only use 200 validation set videos (including 3007 action instances) and 213 test set videos (including 3358 action instances) with temporal annotation to train and evaluate SSAD network.</p><p>MEXaction2 <ref type="bibr" target="#b0">[1]</ref>. There are two action categories in MEXac-tion2 dataset: "HorseRiding" and "BullChargeCape". This dataset is consisted of three subsets: YouTube clips, UCF101 Horse Riding clips and INA videos. YouTube and UCF101 Horse Riding clips are trimmed and used for training set, whereas INA videos are untrimmed with approximately 77 hours in total and are divided into training, validation and testing set. Regarding to temporal annotated action instances, there are 1336 instances in training set, 310 instances in validation set and 329 instances in testing set.</p><p>Evaluation metrics. For both datasets, we follow the conventional metrics used in THUMOS'14, which evaluate Average Precision (AP) for each action categories and calculate mean Average Precision (mAP) for evaluation. A prediction instance is correct if it gets same category as ground truth instance and its temporal IoU with this ground truth instance is larger than IoU threshold θ . Various IoU thresholds are used during evaluation. Furthermore, redundant detections for the same ground truth are forbidden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Action classi ers. To extract SAS features, action classi ers should be trained rst, including two-stream networks <ref type="bibr" target="#b39">[40]</ref> and C3D network <ref type="bibr" target="#b35">[36]</ref>. We implement both networks based on Ca e <ref type="bibr" target="#b12">[13]</ref>. For both MEXaction and THUMOS'14 datasets, we use trimmed videos in training set to train action classi er.</p><p>For spatial and temporal network, we follow the same training strategy described in <ref type="bibr" target="#b39">[40]</ref> which uses the VGGNet-16 pre-trained on ImageNet <ref type="bibr" target="#b3">[4]</ref> to intialize the network and ne-tunes it on training set. And we follow <ref type="bibr" target="#b35">[36]</ref> to train the C3D network, which is pretrained on Sports-1M <ref type="bibr" target="#b15">[16]</ref> and then is ne-turned on training set.</p><p>SSAD optimization. For training of the SSAD network, we use the adaptive moment estimation (Adam) algorithm <ref type="bibr" target="#b16">[17]</ref> with the aforementioned multi-task loss function. Our implementation is based on Tensor ow <ref type="bibr" target="#b1">[2]</ref>. We adopt the Xavier method <ref type="bibr" target="#b9">[10]</ref> to randomly initialize parameters of whole SSAD network because there are no suitable pre-trained temporal convolutional network. Even so, the SSAD network can be easily trained with quick convergence since it has a small amount of parameters (20 MB totally) and the input of SSAD network -SAS features are concise high-level feature.</p><p>The training procedure takes nearly 1 hour on THUMOS'14 dataset.   For training of SSAD model, we use 200 annotated untrimmed video in THUMOS'14 validation set as training set. The window length L w is set to 512, which means approximately 20 seconds of video with 25 fps. This choice is based on the fact that 99.3% action instances in the training set have smaller length than 20 seconds. We train SSAD network for 30 epochs with learning rate of 0.0001.</p><p>The comparison results between our SSAD and other state-ofthe-art systems are shown in Table <ref type="table" target="#tab_1">1</ref> with multiple overlap IoU thresholds varied from 0.1 to 0.5. These results show that SSAD signi cantly outperforms the compared state-of-the-art methods. While the IoU threshold used in evaluation is set to 0.5, our SSAD network improves the state-of-the-art mAP result from 19.0% to 24.6%. The Average Precision (AP) results of all categories with overlap threshold 0.5 are shown in Figure <ref type="figure" target="#fig_5">5</ref>, the SSAD network outperforms other state-of-the-art methods for 7 out of 20 action categories. Qualitative results are shown in Figure <ref type="figure" target="#fig_7">6</ref>.</p><p>Results on MEXaction2. For training of action classi ers, we use all 1336 trimmed video clips in training set. And we randomly sample 1300 background video clips in untrimmed training videos. The prediction categories of action classi ers are "HorseRiding", "BullChargeCape" and "Background". So the dimension of SAS features equals to 9 in MEXaction2.</p><p>For SSAD model, we use all 38 untrimmed video in MEXaction2 training set training set. Since the distribution of action instances' length in MEXaction2 is similar with THUMOS'14, we also set the interval of snippets to zero and the window length T w to 512. We train all layers of SSAD for 10 epochs with learning rate of 0.0001.</p><p>We compare SSAD with SCNN <ref type="bibr" target="#b28">[29]</ref> and typical dense trajectory features (DTF) based method <ref type="bibr" target="#b0">[1]</ref>. Both results are provided by <ref type="bibr" target="#b28">[29]</ref>. Comparison results are shown in Table <ref type="table" target="#tab_2">2</ref>, our SSAD network achieve signi cant performance gain in all action categories of MEXaction2 and the mAP is increased from 7.4% to 11.0% with overlap threshold 0.5. Figure <ref type="figure" target="#fig_7">6</ref> shows the visualization of prediction results for two action categories respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Analysis</head><p>We evaluate SSAD network with di erent variants in THUMOS'14 to study their e ects, including action classi ers, architectures of SSAD network and post-processing strategy. Action classi ers. Action classi ers are used to extract SAS feature. To study the contribution of di erent action classi ers, we evaluate them individually and coherently with IoU threshold 0.5. As shown in Table <ref type="table">3</ref>, two-stream networks show better performance than C3D network and the combination of two-stream and C3D network lead to the best performance. In action recognition task such as UCF101, two-stream network <ref type="bibr" target="#b39">[40]</ref> achieve 91.4%, which is better than 85.2% of C3D <ref type="bibr" target="#b35">[36]</ref> network (without combining with other method such as iDT <ref type="bibr" target="#b37">[38]</ref>). So two-stream network can predict action categories more precisely than C3D in snippet-level, which leads to a better performance of the SSAD network. Furthermore, the SAS feature extracted by two-stream network and C3D network are complementary and can achieve better result if used together.   Architectures of SSAD network. In section 3.3, we discuss several architectures used for base network of SSAD. These architectures have same input and output size. So we can evaluate them fairly without other changes of SSAD. The comparison results are shown in Table <ref type="table">4</ref>. Architecture B achieves best performance among these con gurations and is adopted for SSAD network. We can draw two conclusions from these results: (1) it is better to use max pooling layer instead of temporal convolutional layer to shorten the length of feature map; (2) convolutional layers with kernel size 9 have better performance than other sizes.</p><p>Post-processing strategy. We evaluate multiple post-processing strategies. These strategies di er in the way of late fusion to generate p f inal and are shown in Table <ref type="table" target="#tab_3">5</ref>. For example, p c l ass is used for generate p f inal if it is ticked in table. Evaluation results are shown in Table <ref type="table" target="#tab_3">5</ref>. For the categories score, we can nd that p c l ass has better performance than psas . And using the multiplication factor p o er can further improve the performance. SSAD network achieves the best performance with the complete post-processing strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose the Single Shot Action Detector (SSAD) network for temporal action detection task. Our SSAD network drops the proposal generation step and can directly predict action instances in untrimmed video. Also, we have explored many con gurations of SSAD network to make SSAD network work better for temporal action detection. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD signi cantly outperforms other state-of-the-art systems by increasing mAP from 19.0% to 24.6% on THUMOS'14 and from 7.4% to 11.0% on MEXaction2. In our approach, we conduct feature extraction and action detection separately, which makes SSAD network can handle concise high-level features and be easily trained. A promising future direction is to combine feature extraction procedure and SSAD network together to form an end-to-end framework, so that the whole framework can be trained from raw video directly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our system. Given an untrimmed long video, (1) we extract Snippet-level Action Score features sequence with multiple action classi ers; (2) SSAD network takes feature sequence as input and directly predicts multiple scales action instances without proposal generation step.</figDesc><graphic coords="1,330.36,167.97,215.43,172.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of our approach. (a) Multiple action classi ers are used to extract Snippet-level Action Scores (SAS) feature. (b) The architecture of SSAD network: base layers are used to reduce the temporal dimension of input data; anchor layers output multiple scale feature map associated with anchor instances and prediction layers are used for predicting categories, location and con dence of anchor instances. (c) The training and prediction procedures: during training, we match anchor instances with ground truth instances and calculate loss function for optimization. During prediction, post-processing and NMS procedure are conducted on anchor instances to make nal prediction.</figDesc><graphic coords="3,65.06,83.68,481.89,261.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b), which mainly contains three sub-modules: base layers, anchor layers and prediction layers. Base layers handle the input SAS feature sequence, and use both convolution and pooling layer to shorten the temporal length of feature map and increase the size of receptive elds. Then anchor layers use temporal convolution to continually shorten the feature map and output anchor feature map for action instances prediction. Each cell of anchor layers is associated with anchor instances of multiple scales. Finally, we use prediction layers to get classi cation score, overlap score and location o sets of each anchor instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Multiple architectures of base layers. Input and output sizes are same for each architecture. Parameter of layer is shown with the format of kernel/stride. All convolutional layers have 512 convolutional lters. Evaluation results of these architectures are shown in section 4.3, and we adopt architecture B which achieves the best performance.</figDesc><graphic coords="4,319.02,83.69,238.10,120.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Anchor instances and prediction layer in temporal feature map. In feature map of a anchor layer, we associate a set of multiple scale anchor instances with each feature map cell. We use convolutional prediction layer to predict location o set, con dence and classi cation scores simultaneously for each anchor instance.</figDesc><graphic coords="5,57.70,83.68,232.44,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Detection AP over di erent action categories with overlap threshold 0.5 in THUMOS'14.</figDesc><graphic coords="7,60.80,83.69,490.39,143.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Table 3 :Table 4 : 3 .</head><label>343</label><figDesc>Comparisons between di erent action classi ers used in SSAD on THUMOS'14, where two-stream network includes both spatial and temporal networks. Action Classi er used for SAS Feature mAP (θ = 0.5) Comparisons among multiple base layers con gurations on THUMOS'14. A, B, C, D, E are base layers con gurations which presented in Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of prediction action instances by SSAD network. Figure (a) shows prediction results for two action categories in THUMOS'14 dataset. Figure (b) shows prediction results for two action categories in MEXaction2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 6: Visualization of prediction action instances by SSAD network. Figure (a) shows prediction results for two action categories in THUMOS'14 dataset. Figure (b) shows prediction results for two action categories in MEXaction2 dataset.</figDesc><graphic coords="8,55.13,83.69,501.72,316.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>makes prediction from multiple feature map with Session: Fast Forward 4 MM'17, October 23-27, 2017, Mountain View, CA, USA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>mAP results on THUMOS'14 with various IoU threshold θ used in evaluation.</figDesc><table><row><cell>θ</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell>Karaman et al. [15]</cell><cell>0.2</cell><cell>0.3</cell><cell>0.5</cell><cell>0.9</cell><cell>1.5</cell></row><row><cell>Wang et al. [39]</cell><cell>8.5</cell><cell>12.1</cell><cell>14.6</cell><cell>17.8</cell><cell>19.2</cell></row><row><cell>Oneata et al. [23]</cell><cell>15.0</cell><cell>21.8</cell><cell>28.8</cell><cell>36.2</cell><cell>39.8</cell></row><row><cell>Richard et al. [28]</cell><cell>15.2</cell><cell>23.2</cell><cell>30.0</cell><cell>35.7</cell><cell>39.7</cell></row><row><cell>Yeung et al. [42]</cell><cell>17.1</cell><cell>26.4</cell><cell>36.0</cell><cell>44.0</cell><cell>48.9</cell></row><row><cell>Yuan et al. [44]</cell><cell>18.8</cell><cell>26.1</cell><cell>33.6</cell><cell>42.6</cell><cell>51.4</cell></row><row><cell>Shou et al. [29]</cell><cell>19.0</cell><cell>28.7</cell><cell>36.3</cell><cell>43.5</cell><cell>47.7</cell></row><row><cell>Zhu et al. [45]</cell><cell>19.0</cell><cell>28.9</cell><cell>36.2</cell><cell>43.6</cell><cell>47.7</cell></row><row><cell>SSAD</cell><cell>24.6</cell><cell>35.0</cell><cell>43.0</cell><cell>47.8</cell><cell>50.1</cell></row><row><cell cols="6">4.3 Comparison with state-of-the-art systems</cell></row><row><cell cols="6">Results on THUMOS 2014. To train action classi ers, we use full</cell></row><row><cell cols="6">UCF-101 dataset. Instead of using one background category, here</cell></row><row><cell cols="6">we form background categories using 81 action categories which</cell></row><row><cell cols="6">are un-annotated in detection task. Using two-stream and C3D</cell></row><row><cell cols="6">networks as action classi ers, the dimension of SAS features is 303.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on MEXaction2 dataset with overlap threshold 0.5. Results for<ref type="bibr" target="#b0">[1]</ref> are taken from<ref type="bibr" target="#b28">[29]</ref>.</figDesc><table><row><cell>AP(%)</cell><cell cols="2">BullCHargeCape HorseRiding</cell><cell>mAP(%)</cell></row><row><cell>DTF [1]</cell><cell>0.3</cell><cell>3.1</cell><cell>1.7</cell></row><row><cell>SCNN [29]</cell><cell>11.6</cell><cell>3.1</cell><cell>7.4</cell></row><row><cell>SSAD</cell><cell>16.5</cell><cell>5.5</cell><cell>11.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Evaluation on di erent post-processing strategy on THUMOS'14.</figDesc><table><row><cell>p c l ass p sas p o er</cell><cell>!</cell><cell cols="4">! ! ! ! ! ! ! ! ! !</cell></row><row><cell>mAP (θ = 0.5)</cell><cell>22.8</cell><cell>13.4</cell><cell>24.3</cell><cell>19.8</cell><cell>23.3 24.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>MM'17, October 23-27, 2017, Mountain View, CA, USA</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research has been supported by the funding from NSFC (61673269, 61273285) and the Cooperative Medianet Innovation Center</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">MEXaction2</title>
		<ptr target="http://mexculture.cnam.fr/xwiki/bin/view/Datasets/Mex+action+dataset" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tensor ow: Largescale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for e cient detection of human actions in untrimmed videos</title>
		<author>
			<persName><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ImageNet: A largescale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Apt: Action localization proposals from dense trajectories</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>BMVA Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the di culty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep Sparse Recti er Neural Networks.. In Aistats</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">275</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ca e: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of sher encoded dense trajectories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classi cation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing in Science and Engineering &apos;12</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal Convolutional Networks: A Uni ed Approach to Action Segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning activity progression in LSTMs for activity detection and early detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1942">2016. 1942-1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="437" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<title level="m">The LEAR submission at Thumos 2014. ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09502</idno>
		<title level="m">Deep Quantization: Encoding Convolutional Activations with Deep Generative Model</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Uni ed, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m">YOLO9000: Better, Faster, Stronger</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for ne-grained action detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1961">2016. 1961-1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01979</idno>
		<title level="m">Untrimmed Video Classi cation for Activity Detection: submission to ActivityNet Challenge</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<title level="m">A dataset of 101 human actions classes from videos in the wild</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">THUMOS14 Action Recognition Challenge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<title level="m">Towards good practices for very deep two-stream convnets</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">UTS at activitynet 2016. AcitivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2016</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1302" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal Action Localization with Pyramid of Score Distribution Features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07403</idno>
		<title level="m">E cient Action Detection in Untrimmed Videos via Multi-Task Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
