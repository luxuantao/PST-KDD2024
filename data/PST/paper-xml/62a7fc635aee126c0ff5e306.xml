<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Learning of Non-Autoregressive Transformers</title>
				<funder ref="#_zzPyd6S">
					<orgName type="full">National Science Foundation for Distinguished Young Scholars</orgName>
				</funder>
				<funder ref="#_Hj5E5rb #_sAJha83">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_MtysMdR">
					<orgName type="full">Guoqiang Institute of Tsinghua University</orgName>
				</funder>
				<funder ref="#_e4v5VFE">
					<orgName type="full">Tsinghua-Toyota Joint Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>huang@tsinghua.edu.cn&gt;</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelli-gent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">DCST</orgName>
								<orgName type="institution">Tsinghua Univer-sity. Institute for Artificial Intelligence</orgName>
								<address>
									<country>Tsinghua Uni-versity</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianhua</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelli-gent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">DCST</orgName>
								<orgName type="institution">Tsinghua Univer-sity. Institute for Artificial Intelligence</orgName>
								<address>
									<country>Tsinghua Uni-versity</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelli-gent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">DCST</orgName>
								<orgName type="institution">Tsinghua Univer-sity. Institute for Artificial Intelligence</orgName>
								<address>
									<country>Tsinghua Uni-versity</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Institute for AI Industry Research</orgName>
								<orgName type="institution" key="instit2">Tsinghua Univer-sity</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelli-gent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">DCST</orgName>
								<orgName type="institution">Tsinghua Univer-sity. Institute for Artificial Intelligence</orgName>
								<address>
									<country>Tsinghua Uni-versity</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Institute for AI Industry Research</orgName>
								<orgName type="institution" key="instit2">Tsinghua Univer-sity</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelli-gent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">DCST</orgName>
								<orgName type="institution">Tsinghua Univer-sity. Institute for Artificial Intelligence</orgName>
								<address>
									<country>Tsinghua Uni-versity</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Institute for AI Industry Research</orgName>
								<orgName type="institution" key="instit2">Tsinghua Univer-sity</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of California Santa Barbara</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelli-gent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">DCST</orgName>
								<orgName type="institution">Tsinghua Univer-sity. Institute for Artificial Intelligence</orgName>
								<address>
									<country>Tsinghua Uni-versity</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Institute for AI Industry Research</orgName>
								<orgName type="institution" key="instit2">Tsinghua Univer-sity</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><surname>?1</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelli-gent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">DCST</orgName>
								<orgName type="institution">Tsinghua Univer-sity. Institute for Artificial Intelligence</orgName>
								<address>
									<country>Tsinghua Uni-versity</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Institute for AI Industry Research</orgName>
								<orgName type="institution" key="instit2">Tsinghua Univer-sity</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prob</forename><surname>Sample</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelli-gent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">DCST</orgName>
								<orgName type="institution">Tsinghua Univer-sity. Institute for Artificial Intelligence</orgName>
								<address>
									<country>Tsinghua Uni-versity</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Institute for AI Industry Research</orgName>
								<orgName type="institution" key="instit2">Tsinghua Univer-sity</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the Learning of Non-Autoregressive Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Non-autoregressive Transformer (NAT) is a family of text generation models, which aims to reduce the decoding latency by predicting the whole sentences in parallel. However, such latency reduction sacrifices the ability to capture left-toright dependencies, thereby making NAT learning very challenging. In this paper, we present theoretical and empirical analyses to reveal the challenges of NAT learning and propose a unified perspective to understand existing successes. First, we show that simply training NAT by maximizing the likelihood can lead to an approximation of marginal distributions but drops all dependencies between tokens, where the dropped information can be measured by the dataset's conditional total correlation. Second, we formalize many previous objectives in a unified framework and show that their success can be concluded as maximizing the likelihood on a proxy distribution, leading to a reduced information loss. Empirical studies show that our perspective can explain the phenomena in NAT learning and guide the design of new training methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Non-Autoregressive Transformers (NATs, <ref type="bibr" target="#b11">Gu et al., 2018;</ref><ref type="bibr" target="#b12">Gu et al., 2019;</ref><ref type="bibr" target="#b23">Ma et al., 2019;</ref><ref type="bibr">Ding et al., 2021a;</ref><ref type="bibr" target="#b10">Gu &amp; Kong, 2021)</ref> have received growing attention due to their significantly lower decoding latency and approaching accuracy compared to the autoregressive Transformers (ATs)</p><p>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). The log-likelihood and BLEU score on a 2D section in NAT's weight space. GLAT+KD ( ) and MLE ( ) are trained via different objectives starting from an initial checkpoint ( ) for 10k steps, where MLE achieve higher log-likelihood but lower BLEU, and GLAT is the opposite. Each point of the contour map is linearly interpolated from the three checkpoints. All values are evaluated on one translation benchmark, the validation set of WMT14 En-De. in text generation <ref type="bibr">(Qian et al., 2021b;</ref><ref type="bibr">Huang et al., 2022b)</ref>. NATs generate the whole sequence in parallel based on the assumption that each token can be predicted independently. However, unlike ATs that can be easily trained via Maximum Likelihood Estimation (MLE), NAT learning is very challenging because it drops the left-to-right dependencies. <ref type="bibr" target="#b11">Gu et al. (2018)</ref> show that directly training NATs via MLE leads to implausible outputs with repeated tokens, revealing their inability to preserve the consistency in generated texts.</p><p>To address the problem, many training methods have been proposed. For example, knowledge distillation (KD, <ref type="bibr" target="#b20">Kim &amp; Rush, 2016;</ref><ref type="bibr" target="#b11">Gu et al., 2018)</ref> supervises NATs with target sentences distilled from an AT teacher model. GLAT <ref type="bibr">(Qian et al., 2021a)</ref> improves the training by utilizing a masked language model objective. These methods only change the training objectives without modifying the model, but they demonstrate significant improvements in generation quality.</p><p>Despite the empirical successes in NAT learning, there still exists a surprising characteristic not well studied: the objectives leading to a good generation quality actually result in a very low likelihood. <ref type="foot" target="#foot_0">1</ref> As shown in Fig. <ref type="figure" target="#fig_6">1</ref>, we finetune two NATs with different objectives from an initial checkpoint and track the changes in the log-likelihood and the BLEU score. The optimal training directions under the two metrics Figure <ref type="figure">2</ref>: An example explaining why directly training NATs towards higher likelihood does not lead to better generation quality. We illustrate continuous distributions with two variables, i.e., P (y 1 , y 2 ), which are analogous to two-token sentences shown in the table. Given (a) a two-mode oracle distribution, (b) P ?1 achieves higher likelihood than (c) P ?2 but generates undesired outputs that mix the two modes. NATs have to follow the independent assumption, i.e., the joint distribution satisfies P ? (y 1 , y 2 ) = P ? (y 1 )P ? (y 2 ). All distributions are conditioned on a same X, which is omitted.</p><p>are inconsistent, where GLAT+KD improves the generation quality despite that its perplexity is about 10 times of that of the counterpart trained via MLE.</p><p>Based on the phenomenons, we raise two questions:</p><p>? Q1: Why is NAT learning so challenging that the MLE training does not work well? ? Q2: Why are previously proposed objectives successful despite they lead to a low likelihood?</p><p>In this paper, we present theoretical and empirical analyses to answer the two questions. For Q1, we investigate the challenges of NAT learning from intuitive and theoretical perspectives. Specifically, we show that directly training NATs towards high likelihood prevents them from learning correct dependencies between target tokens, thereby degrading the generation performance. The lost information can be measured by a property of the data distribution, namely, the conditional total correlation (Conditional TC, C), which also measures the difficulties of NAT learning.</p><p>For Q2, we revisit many previous training objectives and explain their success in a unified framework. Generally, we find that previous success on NAT can be concluded as adopting a training objective different from the vanilla MLE. Specifically, instead of maximizing the likelihood on the original dataset X, Y , they in fact approximate a revised distribution, namely, the proxy distribution X + Z, T , where Z and T are designed to enhance the inputs and simplify the targets. With carefully designed Z and T , the proxy distribution has a lower C than the original distribution, thereby alleviating the information loss in the NAT learning. Based on the above analysis, we formulate previous training objectives in a unified framework, named Maximum Proxy-Likelihood Estimation (MPLE). We further derive a general objective to reveal the connections between the proxy distribution and the real distribution, which empirically correlates well with the generation performance and further guides the design of new training methods.</p><p>Our contributions are as follows:</p><p>? We present empirical and theoretical analyses showing that NAT learning is challenging due to the information loss in dependencies, which can be measured by a dataset's property, conditional total correlation C. ? We revisit the existing successes in NAT learning and propose to understand previous training objectives in a unified perspective. We reveal that these objectives construct a proxy distribution with a reduced C, thereby alleviating the information loss.  <ref type="bibr" target="#b0">(Akaike, 1998)</ref>. Given a source sentence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Challenges of NAT Learning</head><formula xml:id="formula_0">X = [x 1 , x 2 , ? ? ? , x N ] and a target sentence Y = [y 1 , y 2 , ? ? ? , y M ], MLE training minimizes L MLE = D KL [P data (Y |X)||P ? (Y |X)] (1) = -H data (Y |X) -E Pdata(Y |X) [log P ? (Y |X)] ,</formula><p>where H data is a constant representing the Shannon Entropy, and the second term is the log-likelihood. For autoregressive Transformers (ATs), the log-likelihood is defined as</p><formula xml:id="formula_1">log P AT ? (Y |X) = M i=1 log P AT ? (y i |y &lt;i , X),<label>(2)</label></formula><p>where y i is predicted based on the prefix y &lt;i .</p><p>The vanilla NAT makes a conditional independent assumption where each token is independent of each other when X is given. Formally, we have</p><formula xml:id="formula_2">log P NAT ? (Y |X) = M i=1 log P NAT ? (y i |X).<label>(3)</label></formula><p>Such assumption makes the NAT a poor approximator of the real data distribution, thereby bringing many challenges in NAT learning. In the next sections, we present an intuitive explanation of the challenges and provide a quantitative method to evaluate the difficulties of NAT learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Challenges from Intuitive Perspective</head><p>A major challenge in NAT learning is that directly training NATs towards higher likelihood cannot lead to good generation performance. We show an intuitive example in Fig. <ref type="figure">2</ref>, which contains a continuous distribution with two variables as the real distribution (analogous to a sentence with two tokens) and two NATs with different parameters.</p><p>Comparing P ?1 and P ?2 , we find that P ?1 perfectly approximates the marginal distributions P (y 1 ) and P (y 2 ), thereby achieving a higher likelihood. However, P ?1 drops the dependency between y 1 and y 2 , leading to wrong outputs by mixing two sentences, previously known as the multimodality problem <ref type="bibr" target="#b11">(Gu et al., 2018)</ref>. In contrast, although P ?2 has a low likelihood due to the poor approximations of the marginal distributions, it captures one of the real modes while preserving the correct lexical collocation, i.e., no is followed by problem but not course.</p><p>This example intuitively shows that directly training NAT to maximize the likelihood cannot capture correct lexical collocation due to the severe dependency dropping in target tokens. In the next section, we quantify the dropped dependencies based on information theory and further evaluate the difficulties of NAT learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Challenges from Theoretical Perspective</head><p>With the autoregressive decomposition of Eq.2, ATs can achieve zero KL divergence theoretically.<ref type="foot" target="#foot_1">2</ref> However, we show that NATs' KL divergence is bounded by a nonnegative constant, which corresponds to the information loss in approximating the data distribution.</p><p>Theorem 1. For a NAT model P ? (Y |X), we have</p><formula xml:id="formula_3">min ? D KL [P data (Y |X)||P ? (Y |X)] ? C, where C = M i=1 H data (y i |X) -H data (Y |X), and H data (?|X) is the Shannon Entropy. Proof. DKL[Pdata(Y |X)||P ? (Y |X)] = -Hdata(Y |X) -E P data (Y |X) M i=1 log P ? (yi|X) (Conditional Independent Assumption of Eq.3) = -Hdata(Y |X) - M i=1 E P data (y i |X) [log P ? (yi|X)] ? -Hdata(Y |X) + M i=1 Hdata(yi|X) (Gibbs' Inequality)</formula><p>The equality is achieved when P ? (y i |X) = P data (y i |X). Note that C is a non-negative constant called conditional total correlation (Conditional TC, <ref type="bibr" target="#b39">Watanabe, 1960)</ref> or multiinformation <ref type="bibr" target="#b35">(Studen? &amp; Vejnarov?, 1998)</ref>, which measures the information of dependencies between the target tokens </p><formula xml:id="formula_4">L = D KL (Q||P ? ) + R(Q, P data ).<label>(4)</label></formula><p>The first term is similar to the MLE objective, which trains the model towards the proxy distribution Q instead of P data .</p><p>The second term is a regularizer controlling the distortion between Q and P data .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting Previous Successes</head><p>Considering the severe challenges in NAT learning, many training methods are proposed to improve the generation performance. For example, Aligned Cross Entropy (AXE, <ref type="bibr" target="#b9">Ghazvininejad et al., 2020)</ref> finds that the cross-entropy loss highly penalizes small shifts in word order, which deviates from the evaluation of generation quality and thus hinders the NAT training. They propose an aligned-based objective that allows small target shifts to alleviate the problem. GLAT <ref type="bibr">(Qian et al., 2021a)</ref> proposes to promote representation learning by utilizing curriculum learning. Specifically, they train NATs similar to the masked language model, which feeds a masked target as the decoder input and adjusts the training difficulties by annealing the masking ratio.</p><p>Although these methods are proposed with different motivations, we find that they generally share a similar objective that can be interpreted as the MLE training. Specifically, they still use the cross-entropy loss between the NAT predictions and the target tokens, except that the target labels or model inputs are changed. 4 Then we can interpret the loss 4 They also do not change the inference process, e.g., no extra inputs are introduced for decoder in generation.</p><p>as an objective of MLE, but the target distribution is actually replaced by a new distribution with their new inputs and outputs, where we call it a proxy distribution Q. In these methods, NATs are trained on the proxy distribution Q to maximize the likelihood, which explains why they have a low likelihood on the original validation set.</p><p>By examining these methods closely, we find that the proxy distribution Q is an essential key to their success. Specifically, we divide existing methods into two categories: Modifying Targets or Enhancing Inputs. As shown in Fig. <ref type="figure">3</ref>, both approaches try to preserve a one-to-one mapping between the new inputs and outputs, which intuitively reduces C by limiting the possible modes in the proxy distribution,<ref type="foot" target="#foot_3">5</ref> thereby alleviating the information loss in NAT learning.</p><p>Formally, we denote the proxy distribution by Q(T |Z, X), where the original Y is replaced by a proxy target T , and the original X is enhanced with a proxy input Z. Next, we revisit existing methods of NAT learning to study how they construct the proxy distribution Q.</p><p>Constructing Q by Modifying Targets (Y ? T ) Sequence-level knowledge distillation (KD, <ref type="bibr" target="#b11">Gu et al., 2018)</ref> is a direct method to simplify the targets. For a given input X, an autoregressive teacher generates the proxy target T by beam search, which replaces the diverse references and thus reduces the possible outputs in the data distribution. The KD data are usually generated in advance and does not change during NAT training. Some methods construct T through the training, which are adaptively adjusted according to the NAT model. AXE <ref type="bibr" target="#b9">(Ghazvininejad et al., 2020)</ref> and OaXE <ref type="bibr" target="#b6">(Du et al., 2021)</ref> use alignment-based objectives, which match each prediction with a reference token and calculate the crossentropy loss. The two losses are equivalent to obtaining the MLE objective with a new target T , where T is a permutation of Y but closer to the model prediction.</p><p>Constructing Q by Enhancing Inputs (X ? Z, X) CMLM<ref type="foot" target="#foot_4">6</ref>  <ref type="bibr" target="#b8">(Ghazvininejad et al., 2019)</ref> uses a masked language model objective, where a randomly masked target sentence is fed into the NAT decoder. Intuitively, if P (Y |X) has multiple possible outputs, Q(Y |Z, X) can reduce the number of candidates with the constraint of Z, which again leads to a simplified distribution with reduced C.</p><p>Unlike CMLM that samples Z from a predefined distribution by random masking, GLAT <ref type="bibr">(Qian et al., 2021a)</ref> proposes to sample Z adaptively according to the NAT performance. Specifically, if the NAT well approximates P (Y |X) without Z, GLAT will mask most tokens in the proxy input. Since the NAT uses full masks in inference, GLAT improves CMLM by reducing the training and inference gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A Unified Objective of MPLE</head><p>Existing methods simply train NAT by maximizing the likelihood on the proxy distribution Q(T |Z, X). However, they do not answer when the performance on the proxy distribution can generalize to the real distribution. For example, a good approximation of Q may not guarantee good generation performance on P data since there can be a substantial distortion between the two distributions.</p><p>In MPLE, we propose considering C and the data distortion together in a unified objective. Specifically, we regard T and Z as latent variables and build a latent variable model that connects Z, T and X, Y , as shown in Fig. <ref type="figure">4</ref>. Formally,</p><formula xml:id="formula_5">P ? (Y |X) = Z T P ? (Y |T )P ? (T |Z, X)P ? (Z|X),<label>(5)</label></formula><p>where P ? (T |Z, X) is the NAT decoder, and the other two modules bridge Z with X, and T with Y , respectively. Then we derive our objective from the likelihood on P data (Y |X):</p><formula xml:id="formula_6">-E P data (Y |X) log P ? (Y |X) = -E P data (Y |X) log E Q(T,Z|X) P ? (Y, T, Z|X) Q(T, Z|X) ? -E P data (Y |X) E Q(T,Z|X) log P ? (Y, T, Z|X) Q(T, Z|X) (6) = -E P data (Y |X) E Q(T,Z|X) log P ? (Y |T )+ log P ? (T |Z, X) Q(T |Z, X) + log P ? (Z|X) Q(Z|X)<label>(7)</label></formula><p>In Eq.6, we apply variational principle <ref type="bibr" target="#b7">(Fox &amp; Roberts, 2012)</ref> by introducing Q(T, Z|X), which specifies how we obtain Z, T and can be decomposed into the proxy distribution Q(T |Z, X) and Q(Z|X).</p><p>Eq.7 is our unified objective L MPLE , which can be simplified and recovers our intuition in Eq.4:</p><formula xml:id="formula_7">LMPLE = LNAT D KL (Q||P ? ) + Ltarget + Linput R(Q,P data ) ,<label>(8)</label></formula><formula xml:id="formula_8">LNAT = E Q(Z|X) DKL [Q(T |Z, X)||P ? (T |Z, X)] , (9) Ltarget = E P data (Y |X) E Q(T |X) [-log P ? (Y |T )] , (10) Linput = DKL [Q(Z|X)||P ? (Z|X)] .<label>(11)</label></formula><p>In Eq.8, L NAT supervises the decoder P ? (T |Z, X) to maximize the likelihood on the proxy distribution. L target and L input measure the cost in bridging T, Z with X, Y , and act as regularizers to avoid large distortions between the proxy and original variables.</p><p>Moreover, since P ? (T |Z, X) still follows the independent assumption, we can derive a lower bound of L NAT in a similar way of Theorem 1. Specifically, we have</p><formula xml:id="formula_9">C := E Q(Z|X) L i=1 HQ(ti|Z, X) -HQ(T |Z, X) ,<label>(12)</label></formula><p>that satisfies L NAT ? C , where C is the Conditional TC of the proxy distribution Q(T |Z, X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Understanding Existing Methods in MPLE</head><p>L MPLE seems a bit complex because it includes both the likelihood term to train the NAT model and the objective for selecting the proxy inputs and targets. To understand previous methods in MPLE, we describe the training process as an Expectation Maximization algorithm including two steps: (1) find optimal proxy distribution Q by adjusting the proxy variables Z and T ;</p><p>(2) optimize model parameter ?.</p><p>In E-step, we fix the model parameter ? and update proxy variables to reduce L MPLE , which aims to find good proxy distribution Q to balance L NAT and the data distortion. Since ? is fixed, adjusting proxy variables for lower L NAT does not affect the NAT model but actually optimizes C , where L NAT is the upper bound of C defined in Eq.12.</p><p>However, such optimization is non-trivial, where existing methods utilize some heuristic rules. For example, KD obtains T by distilling sentences from a pre-trained AT teacher, which efficiently alleviates the information loss by reducing the modes in the dataset. AXE and OaXE obtain T by aligning the NAT prediction with Y , where they have hyper-parameters for controlling the distortion within an acceptable range. Please refer to Appendix E for more details about the heuristic rules in existing methods.</p><p>As introduced in Sec.3.1, these heuristic rules utilize either fixed or adaptive strategies. Fixed strategies obtain the proxy distribution before the training, where C is lower than the original C, but not further optimized. In contrast, adaptive strategies adjust the proxy distribution through the training, which usually outperforms the fixed ones.</p><p>In M-step, we fix the proxy distribution Q and optimize the model ?. Since all Q's entropies are constants and thus ignored, the three losses in Eq.8 can be easily calculated based on Z and T previously obtained in the E-step. Specially, L NAT recovers the objectives of existing methods by maximizing the likelihood on the proxy distribution.</p><p>Quantifying Data Distortion Existing methods heuristically obtain Z and T to balance the training difficulties and the data distortion, which does not involve a measurement of the distortion. MPLE provides a method to quantify the distortion, allowing for comparisons between different methods in constructing the proxy distribution.</p><p>Specifically, we use the output paraphraser P ? (Y |T ) and the input predictor P ? (Z|X) to measure the data distortion L target and L input , respectively. For L target , we define the output paraphraser as a simple non-trainable distribution related to the similarity between Y and T :</p><formula xml:id="formula_10">P ? (Y |T ) = exp(?S(Y, T ))/?, (<label>13</label></formula><formula xml:id="formula_11">)</formula><p>where ? is a hyper-parameter, S(Y, T ) is the sentence BLEU, and ? = Y exp(?S(Y, T )). However, the normalization term ? is intractable, so we drop it and empirically use Ltarget instead:</p><formula xml:id="formula_12">Ltarget = E Pdata(Y |X) E Q(T |X) [-?S(Y, T )] .<label>(14)</label></formula><p>Intuitively, Eq.14 measures the distortion between proxy and real targets by the average BLEU score.</p><p>For L input , we design a trainable input predictor specially for GLAT and CMLM, where the other methods without an extra input Z always have L input = 0. Specifically, we define the input predictor as a classifier, which predicts z i from the vocabulary including a special mask token. We predict Z non-autoregressively (See Appendix E.6 for details):</p><formula xml:id="formula_13">log P ? (Z|X) = M i=1 log P ? (z i |X).<label>(15)</label></formula><p>Then, L input can be calculated according to Eq.11.</p><p>Discussing More Work from MPLE Perspective Besides the methods discussed above, MPLE can also explain many other objectives proposed for NAT learning, including (1) the methods introducing continuous or discrete latent variables <ref type="bibr" target="#b17">(Kaiser et al., 2018;</ref><ref type="bibr" target="#b34">Shu et al., 2020;</ref><ref type="bibr" target="#b2">Bao et al., 2021;</ref><ref type="bibr">2022)</ref>; (2) enhancing NAT decoder with order information <ref type="bibr" target="#b1">(Bao et al., 2019;</ref><ref type="bibr" target="#b29">Ran et al., 2021)</ref>, POS taggings <ref type="bibr" target="#b43">(Yang et al., 2021)</ref>, or tokens sampled from target sentences <ref type="bibr">(Huang et al., 2022a)</ref>; (3) KD variants like reverse distillation <ref type="bibr">(Ding et al., 2021b)</ref> or repeated distillation <ref type="bibr" target="#b44">(Zhou et al., 2020;</ref><ref type="bibr" target="#b36">Sun &amp; Yang, 2020)</ref>.</p><p>Notably, CTC-based methods <ref type="bibr" target="#b22">(Libovick? &amp; Helcl, 2018;</ref><ref type="bibr" target="#b30">Saharia et al., 2020)</ref>  Finally, MPLE also connects with iterative NATs <ref type="bibr" target="#b21">(Lee et al., 2018;</ref><ref type="bibr" target="#b8">Ghazvininejad et al., 2019;</ref><ref type="bibr" target="#b18">Kasai et al., 2020;</ref><ref type="bibr" target="#b13">Guo et al., 2020)</ref>. Although iterative NATs do not satisfy the independent assumption in Eq.3, they still predict tokens independently in each iterative step. Specifically, we point out that (1) C measures the information loss of iterative NAT in each refinement step;</p><p>(2) some iterative NATs are special cases of MPLE with parameter sharing in Input Predictor and NAT decoder. Please refer to Appendix A for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">A KD Variant from MPLE</head><p>Existing methods heuristically obtain Z and T to construct the proxy distribution. We propose a new variant of KD that improves the proxy distribution by explicitly balancing L NAT and the data distortion, named dynamic KD.</p><p>For a source sentence X, we obtain a target candidate set ?, which contains the raw data and distilled data from AT teachers of different sizes, i.e., Transformer-tiny/small/base/big. Then we choose a best target T ? ? that minimizes L NAT + Ltarget . Noticing that Eq.14 is intractable due to the sampling from P data , we use the pairwise BLEU between the candidates instead. More details are presented in Appendix F.</p><p>Previous work <ref type="bibr" target="#b44">(Zhou et al., 2020)</ref> finds that the KD data from a larger AT teacher is closer to the real data but more difficult to predict, where they suggest choosing the teacher size according to NAT's capacity. Our method dynamically selects the best proxy target from multiple KD candidates for each sample, which achieves substantial improvement over NATs trained on any single KD data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset Implementation Details We implement Raw Data, KD, AXE, OaXE for obtaining proxy targets, and Vanilla (no extra input), CMLM, GLAT for obtaining proxy inputs. We generally follow the hyper-parameters in <ref type="bibr">Qian et al. (2021a)</ref>.</p><p>For fair comparisons, we only modify the heuristic rules to obtain Z and T , which may be different from their original implementations. For example, we do not use iterative refinement for CMLM, or combine OaXE with CMLM. Unless otherwise specified, we do not utilize reranking methods or other decoding tricks. More details are in Appendix G.</p><p>Metrics We utilize tokenized BLEU <ref type="bibr" target="#b26">(Papineni et al., 2002)</ref> to evaluate the translation performance. L input and L NAT are averaged per token on validation set. Ltarget in Eq.14 requires multiple real targets Y from P data , so we utilize multi-reference annotations from <ref type="bibr" target="#b24">Ott et al. (2018)</ref>; <ref type="bibr" target="#b14">Hassan et al. (2018)</ref>, where each sample has 10(2) extra humanannotated references for En-De(Zh-En). To calculate Ltarget by Eq.14, we use ? = 0.2 for En-De, ? = 0.25 for Zh-En. LMPLE := L NAT + L input + Ltarget . We measure the speedup of decoding latency on WMT14 En-De with batch size 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Verification of Reduced Information Loss</head><p>Theorem 1 implies that any NAT approximating the real distribution cannot achieve less information loss than dataset's C. We argue that existing methods approximate a proxy distribution Q instead, thereby achieving reduced information loss. To verify the proposition, we compare L NAT of different methods against the the original dataset's C.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">5</ref>, most methods except Raw Data achieve lower L NAT than the dataset's C. Note that L NAT evaluates the information loss in approximating the proxy distribution, which is the upper bound of Q's Conditional TC, i.e., C defined Eq.12. The results empirically verify that (1) training on the proxy distribution alleviates the information loss in NAT learning;</p><p>(2) the proxy distribution has a reduced Conditional TC.</p><p>Raw KD AXE OaXE GLAT CMLM 0  However, lower L NAT does not promise higher BLEU because they do not control the data distortion. In the next sections, we will analyze how different methods affect the performance by balancing L NAT and the data distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effects of Proxy Target</head><p>In this section, we compare different methods of obtaining proxy targets with varying hyper-parameters. We present the results on En-De in Table <ref type="table" target="#tab_4">2</ref> and Zh-En in Appendix C.</p><p>Strong Correlation. LMPLE is strongly correlated with BLEU, where L NAT and Ltarget are both important. For example, AXE(? =1) achieves low L NAT with high Ltarget , indicating that T is easy to predict but heavily distorted from the real target. On the contrary, KD's proxy target is less distorted but hard to predict. OaXE(300k) balances the two losses well and thus achieves the best BLEU.</p><p>? in Eq.14 will affect the scale of Ltarget , where we choose ? = 0.2 to maximize the correlation. However, the choose of ? is not sensitive that |r| ? 0.8 for all ? ? [0.1, 0.5].</p><p>Secret Advantage of KD. Previous work <ref type="bibr" target="#b11">(Gu et al., 2018)</ref> has shown that KD can simplify the training data and thus reduce L NAT . However, our results show a secret advantage that KD also achieves the lowest Ltarget , indicating that the KD data are even closer to the multiple human references on average than the raw data. This result is caused by the diversity of human annotations, as shown in Fig. <ref type="figure" target="#fig_4">6</ref>.  Although the KD data may not belong to any modes of the data distribution, it still has higher similarity on average.</p><p>Hyper-parameters and Trade-off. AXE and OaXE utilize tricks to avoid large distortion between the proxy target and the real target. For example, AXE tunes the skip penalty ? , and OaXE tunes the pre-training step. MPLE provides a quantifiable method to measure the trade-off between the likelihood loss L NAT and the distortion Ltarget , which improves the interpretability of hyper-parameter selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effects of Proxy Input</head><p>We compare methods that obtain proxy inputs including several variants of CMLM and GLAT, which are also used in <ref type="bibr">Qian et al. (2021a)</ref>. These variants use different strategies for masking, whose details are presented in Appendix E.6.</p><p>In the inference of CMLM and GLAT, they use a full mask as the proxy input by default (Default Decoding), leading to a large gap between train and inference. We propose to sample the latent input (Input Sampling) based our latent variable model in Eq.5: We first sample Z according to the input predictor P ? (Z|X),<ref type="foot" target="#foot_6">8</ref> and then choose the most likely tokens predicted by the NAT decoder. We present the results on En-De in Table <ref type="table" target="#tab_5">3</ref> and Zh-En in Appendix C.</p><p>Strong Correlation with Sample BLEU. Our objective is strongly correlated with BLEU when using Input Sampling, where L input and L NAT should be balanced to achieve the best performance. For example, Vanilla NAT does not introduce extra inputs, leading to large L NAT ; CMLM introduces too many tokens in Z, bringing a large distortion from the original input. However, LMPLE is less correlated with BLEU of Default Decoding, which can be caused by the decoding strategy as discussed below. Potentials for Decoding Strategies. Previous work <ref type="bibr">(Qian et al., 2021a)</ref> showed that CMLM performs poorly with a full masked decoder input, but we find that it can be improved by utilizing the input predictor to generate a better proxy input Z in inference. Specifically, Input Sampling brings about 4 BLEU points improvement on CMLM. This idea is connected with the iterative NATs, where their refined sentence can be interpreted as a proxy input to improve the generation quality.</p><p>We also find that CMLM and GLAT prefer different decoding strategies, which can be explained by the decoding confidence P ? (Z|X) and P ? (T |Z, X). As shown in Fig. <ref type="figure" target="#fig_5">7</ref>, CMLM is more confident with Input Sampling than Default Decoding, whereas GLAT is the opposite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results of Dynamic KD</head><p>We combine Dynamic KD with GLAT and further utilize a reranking method following our baselines. As shown in Table 4, Dynamic KD brings about 0.6 ? 0.7 BLEU improvement against the single KD distilled from Transformer-base. Moreover, our best results achieve competitive translation quality with ATs with the modest cost in reranking.</p><p>We further compare Dynamic KD against single KD data distilled from different AT teachers. As shown in Fig. <ref type="figure">8</ref>,  <ref type="bibr">(2021)</ref>. LPD <ref type="bibr">(Wei et al., 2019a)</ref> and NPD <ref type="bibr" target="#b11">(Gu et al., 2018)</ref> indicate reranking methods with the number of candidates. NPD are slower than LPD due to the use of an external AT reranker. Please see Table <ref type="table" target="#tab_12">9</ref> for more results. The results suggest that our perspective effectively guides the design of new training methods. Explicitly optimizing L MPLE provides a promising way to find better proxy distributions, which outperforms existing heuristic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>NATs are proposed to reduce the decoding latency but suffer from poor generation quality. Many studies are devoted to solving the problem. Besides the methods discussed in our analysis, some studies are also helpful in improving the NAT performance, mainly including (1) objectives not based on cross-entropy <ref type="bibr">(Wei et al., 2019a;</ref><ref type="bibr" target="#b37">Sun et al., 2019;</ref><ref type="bibr" target="#b32">Shao et al., 2020;</ref><ref type="bibr">2021)</ref>; (2) iteratively refining the generated outputs <ref type="bibr" target="#b21">(Lee et al., 2018;</ref><ref type="bibr" target="#b8">Ghazvininejad et al., 2019;</ref><ref type="bibr" target="#b12">Gu et al., 2019;</ref><ref type="bibr" target="#b18">Kasai et al., 2020;</ref><ref type="bibr" target="#b13">Guo et al., 2020)</ref>. Although the iterative approaches usually lead to better quality, <ref type="bibr" target="#b19">Kasai et al. (2021)</ref> find that the these models are much slower and may not have advantages against ATs. Moreover, recent works show that the non-iterative methods can also achieve competitive quality with AT models and have substantial lower latency than iterative methods <ref type="bibr" target="#b10">(Gu &amp; Kong, 2021;</ref><ref type="bibr">Qian et al., 2021b;</ref><ref type="bibr">Huang et al., 2022b)</ref>.</p><p>Notably, a previous study <ref type="bibr" target="#b44">(Zhou et al., 2020</ref>) also analyzes the NAT learning but mainly focuses on the KD method.</p><p>They propose metrics to evaluate the complexity of the KD data and explain how KD improves NAT generation.</p><p>Unlike their analysis that only considers the KD data, our perspective is more general in understanding many SoTA methods and better supported by the information theory.<ref type="foot" target="#foot_7">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we investigate the challenges in NAT learning.</p><p>From intuitive and theoretical perspectives, we show that the problem roots in the large information loss in capturing dependencies between tokens, where the dropped information can be measured by the dataset's conditional total correlation C. Furthermore, we revisit the existing successes in NAT learning and find that many previous studies alleviate the problem by maximizing the likelihood on a proxy distribution, which is designed to have a lower C. Based on the analysis, we propose a unified framework named Maximum Proxy-Likelihood Estimation (MPLE), which provides a unified objective revealing how the choice of proxy distribution contributes to the final performance. This framework improves our understanding of a wide range of NAT learning methods, including the SOTA ones like alignment-based objectives and glancing training. Empirical analyses show that our perspective can well explain the phenomena in NAT learning, where the proposed objective highly correlates with the generation performance and can further guide the design of better training methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relation to Iterative NATs</head><p>Although MPLE provides a unified perspective to understand many previous methods, we do not discuss an important branch of the NAT model, i.e., the iterative NATs. The reason lies in the conditional independent assumption of Eq.3, which is the basic assumption of our analyses but not satisfied in the iterative methods. However, our perspective can also improve the understanding of iterative NATs, and we point out some important relations.</p><p>C measures the information loss in each iterative step. Although iterative NATs do not satisfy Eq.3, they still predict tokens independently in each refinement step, so the minimum information loss can be measured by C. For example, in some popular iterative NATs <ref type="bibr" target="#b21">(Lee et al., 2018;</ref><ref type="bibr" target="#b8">Ghazvininejad et al., 2019)</ref>, the i-th refinement step's log-likelihood is defined as</p><formula xml:id="formula_14">log P NAT ? (Y |X, Y (i-1) ) = M k=1 log P NAT ? (y k |X, Y (i-1) ), (<label>16</label></formula><formula xml:id="formula_15">)</formula><p>where Y is the target sentence, Y (i-1) is the refined result of previous steps, and M is the target length. Similar to Theorem 1, we can prove that the minimal KL divergence is C i , i.e., the conditional TC of the target distribution when Y (i-1) is given, where</p><formula xml:id="formula_16">C i = M k=1 H data (y k |X, Y (i-1) ) -H data (Y |X, Y (i-1) ),<label>(17)</label></formula><p>The result implies that iterative NATs also suffer from the information loss due to the dependency dropping and explains why they can benefit from methods that reduces C, e.g., knowledge distillation.</p><p>Iterative Masked Prediction are special cases of MPLE with shared Input Predictor and NAT decoder. If we regard the output of the (i -1)-th refinement as the proxy input, iterative NATs actually construct a proxy distribution P data (Y |X, Y (i-1) ), which reduces C by providing an extra decoder input. Unlike the input predictor defined in our MPLE framework, iterative NATs predict the proxy input Y (i-1) by the NAT decoder itself with i -1 refinement steps. In Table .2, we propose Input Sampling method that generates Z from the input predictor, which is similar to a single step refinement but with a separate NAT decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Conditional TC and Performance Gap</head><p>Table <ref type="table" target="#tab_1">1</ref> aims to show that the large C is the main obstacle in NAT learning, and we provide more details here.</p><p>Dataset We first choose WMT14 En-De and WMT16 En-Ro, which contains 4.5M pairs and 610k pairs in the training set, respectively. Since natural datasets usually have a large C, we further construct two synthetic datasets for comparison. Both synthetic datasets use the English corpus in WMT14 En-De as targets, and the source sentences are modified from the targets by word replacement or word dropping. In other words, the synthetic dataset trains the NAT to generate clean English sentences from corrupted English inputs. Specifically, Synthetic A replaces 50% of tokens by randomly sampled tokens from the vocabulary. Synthetic B further drops 10% of tokens in the source sentences based on Synthetic A.</p><p>Estimation of Conditional TC To estimate C, we use V-entropy <ref type="bibr" target="#b42">(Xu et al., 2020)</ref> instead of the Shannon entropy because the latter is intractable due to the unknown data distribution. The V-entropy is comparable only when the function family for estimation is fixed. In our implementation, we use Transformers-base as the function family.</p><p>More Rigorous Comparison Since BLEU is not strictly comparable across datasets, we present a more rigorous comparison by estimating the parameter size required for an autoregressive Transformer to achieve a similar performance with NAT. The comparison is based on the assumption that a smaller AT will suffer from more information loss than a larger AT. We use Transformer-base for NATs, and the AT architecture is choose from Table <ref type="table" target="#tab_9">6</ref>.</p><p>As shown in Table .5, we find that an AT only requires about 2.1% ? 3.2% of parameters to achieve similar performance with the NAT on WMT14 En-De. However, on Synthetic A, an AT requires at least 27.3% of parameters to compete with the NAT. The results verify that large C brings much information loss, making AT easily outperform NAT with much less parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on WMT17 Zh-En</head><p>We repeat the experiments in Sec.4.2 and Sec.4.3 on WMT17 Zh-En. As shown in Table <ref type="table" target="#tab_10">7</ref> and Table <ref type="table" target="#tab_11">8</ref>, our objective is strongly correlated with the translation quality, which supports our claim well. 1.93 -5.00 -3.07 18.39 + AXE(? =10) 2.    <ref type="table" target="#tab_6">4</ref>, we apply some decoding tricks for the results on the last row:</p><p>? We use length parallel decoding (LPD, <ref type="bibr">Wei et al., 2019b)</ref>. We use a candidate set of 3. Since all candidates can be generated simultaneously, LPD is still much fast in inference. It is worth noting that LPD is faster than NPD <ref type="bibr" target="#b11">(Gu et al., 2018)</ref> since it does not need an external reranker. ? We use the de-duplication trick <ref type="bibr" target="#b21">(Lee et al., 2018)</ref>, i.e., removing the repeated tokens in generated sentences.</p><p>? We adjust the predicted length by a factor ? <ref type="bibr" target="#b9">(Ghazvininejad et al., 2020)</ref>. The factor is tuned on the validation set. We use ? = 1 (i.e., the predicted length is not changed) for WMT14 En-De, and ? = 1.05 for WMT17 Zh-En.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Results</head><p>In Table <ref type="table" target="#tab_12">9</ref>, we compare Dynamic KD against strong baselines including non-iterative and iterative NATs. Moreover, we justify the necessity of the regularizer in Eq.8 by removing Ltarget in choosing the proxy target (i.e., T * = arg min T L NAT ) as an ablation study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Formalization of Existing Methods in MPLE</head><p>In the main paper, we briefly describe how existing methods obtain Z and T . In this section, we present detailed formalization of these methods by describing the heuristic rules and their objectives in the framework of MPLE. Specifically, we formalize each method in two steps:</p><p>First, we define the variational distribution following their heuristic rule. Existing methods use heuristic rule to obtain Z and T , which builds the variational distribution Q(T, Z|X) used in the derivation of <ref type="bibr">MPLE (Eq.6)</ref>. For all methods in our analysis, their variational distribution is defined by</p><formula xml:id="formula_17">Q(T, Z|X) := Q(T |X)Q(Z|T, X),<label>(18)</label></formula><p>where Q(T |X) is defined by the methods that obtain the proxy target (including Raw Data, KD, AXE, and OaXE), and Q(Z|T, X) is defined by the methods that obtain the proxy input (including Vanilla, CMLM, and GLAT).</p><p>Second, we prove that their original objective is equivalent to minimizing L NAT of Eq.9. Notably, in M-step, Q is unchanged when optimizing the model ? as discussed in Sec.3.3, so we only prove that their objective is equivalent to minimizing the NLL:</p><formula xml:id="formula_18">L NAT = -E Q(Z,T |X) [log P ? (T |Z, X)] + Constant (19)</formula><p>For some methods, the proof is trivial and thus omitted in the following sections. Original Objective AXE introduces a monotonic alignment ? ?</p><formula xml:id="formula_19">? = [? 1 , ? ? ? , ? L ],</formula><p>where the i-th token of the reference R is aligned to the ? i -th token of the NAT prediction. Formally, the AXE loss is defined as</p><formula xml:id="formula_20">L AXE = min ? ? ? - L i=1 log P ?i (r i ) - k / ?? log P k ( ) , s.t. 1 ? ? 1 ? ? ? ? ? ? L ? L.</formula><p>The first term is the cross entropy between aligned targets and predictions, and the second term is a penalty for unaligned predictions.</p><p>In the AXE loss, a single prediction may be aligned to multiple target tokens. In their original paper, aligning the prediction to the first target token is called the "align" operation, and aligning the prediction to later tokens is called the "skip target" operation. However, a one-to-many alignment will damage the performance, so they penalize the "skip target" operations with a factor ?. This trick is called the skip penalty.</p><p>Proof of Equivalence To connect their definition with ours, we convert the alignment to an adjacency list, as shown in Figure <ref type="figure" target="#fig_7">9</ref>, where ? ? ? i is a list containing all aligned tokens for the i-th prediction. Specially, if the i-th prediction is not aligned, we set ? ? ? i = [0] and r 0 = . Then, L AXE can be reformulated as</p><formula xml:id="formula_21">min ? ? ? ? ? - L i=1 log P i (r ?i,1 ) -? L i=1 |? ? ?i| j=2 log P i (r ?i,j ) ? ? ,<label>(20)</label></formula><p>where ? i,j indicates the j-th element of ? ? ? i . The first term is the cross entropy between the prediction and a new target</p><formula xml:id="formula_22">T * = [r ?1,1 , ? ? ? , r ? L,1 ],</formula><p>and the second term is the penalty for "skipping target" operations.</p><p>When ? = 0, Eq.20 is equivalent to finding an optimal T * to minimize L NAT in Eq.19. Since ? ? ? is a monotonic alignment, T * is constrained and should be a subsequence of R with some empty tokens inserted, which recover our definition. When ? = 0, the second term can be regarded as a regularizer to control the distortion between proxy targets and real targets.</p><p>E.4. Order-agnostic Cross Entropy (OaXE, <ref type="bibr" target="#b6">Du et al., 2021)</ref> OaXE is similar to AXE despite the constraint S(R). Any T ? S(R) is a permutation of R. An example is shown in Fig. <ref type="figure" target="#fig_7">9</ref>. Original Objective Different from AXE, OaXE's ? ? ? is a non-monotonic alignment, and each predicted token can only be used once. The loss is defined as</p><formula xml:id="formula_23">L OaXE = min ? ? ??Perm(L) - L i=1 log P ?i (r i ) ,</formula><p>where Perm(L) indicates the permutations of sequences containing 1 to L.</p><p>Proof of Equivalence Similar to the derivation for AXE, we can reformulate L OaXE as</p><formula xml:id="formula_24">min ? ? ? - L i=1 log P i (r ?i,1 ) .</formula><p>The above formulation recovers our definition: It finds an optimal T * to minimize L NAT in Eq.19, where T * can be an arbitrary permutation of R.</p><p>However, without the monotonic constraints, T * in OaXE may be heavily distorted from the real target Y . To alleviate the problem, OaXE first pretrains the NAT using the vanilla MLE and then finetunes it to minimize L OaXE . This trick is based on an intuition that the optimal T * in a well-trained NAT will be close to the real target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Vanilla</head><p>Many NATs use a full masked sequence as Z or predict Z by Uniform Copy <ref type="bibr" target="#b11">(Gu et al., 2018)</ref> or attention <ref type="bibr">(Qian et al., 2021a)</ref>. We regard them as vanilla decoder inputs because they do not introduce any hints from the target. Formally, they can be formulated as a one-point distribution Q(Z = Z * |T, X) = 1, where Z * is obtained from a deterministic function f (X).</p><p>E.6. CMLM <ref type="bibr" target="#b8">(Ghazvininejad et al., 2019)</ref> / GLAT <ref type="bibr">(Qian et al., 2021a)</ref> CMLM and GLAT sample the proxy input by randomly masking the target sentence. Specifically, they first sample l ? [1, L] as the number of unmasked tokens, and then obtain the proxy input by randomly masking Ll tokens.</p><p>In Table <ref type="table" target="#tab_5">3</ref>, we compare CMLM, GLAT and their variants. Here we list their differences:</p><p>? CMLM sets the number of unmasked tokens l = ?L, where ? is uniformly sampled from 0 to 1.</p><p>? CMLM + fixed masking ratio uses l = 0.2L instead of random sampling.</p><p>? GLAT uses an adaptive sampling strategy according to the NAT prediction accuracy. Specifically, l = ? L i=1 [T i = arg max(t i |X)]. We follow their original paper and anneal ? from 0.5 to 0.3. ? GLAT + mask by P ref use the same l as GLAT, but chooses the unmasked tokens according to the difficulties in predicting them, where the probability of an unmasked z i is proportional to the prediction probability P ? (t i |X). ? GLAT + mask by 1 -P ref chooses the unmasked tokens proportional to 1 -P ? (t i |X).</p><p>Implementation Details of Input Predictor In Eq.15, we mention that Z is predicted non-autoregressively. Concretely, P ? (z i |X) is composed of two modules: P ? (z i is masked|X) predicts whether z i is a masked token, and P ? (t i |X) predicts the target token t i from the vocabulary if z i is not masked. Formally,</p><formula xml:id="formula_25">P ? (z i |X) = ? ? ? P ? (z i is masked|X),</formula><p>if z i is masked; (1 -P ? (z i is masked|X))P ? (t i |X), if z i = t i ; 0, otherwise.</p><p>Therefore, L input can be formulated as</p><formula xml:id="formula_26">L input = E Q(Z|X) - L i=1 log P ? (z i |X) + log Q(Z|X) ,</formula><p>where Q(Z|X) can be obtained according to the definition of heuristic rules.</p><p>For the first module P ? (z i is masked|X), we reuse the Transformer encoder and the NAT decoder and further add a binary classification layer on top of the NAT decoder. For the second module P ? (t i |X), we use a pre-trained vanilla NAT and freeze its parameters during the training of CMLM or GLAT. In this way, P ? (t i |X) can be computed offline to speed up the training.</p><p>For Input Sampling used in Table <ref type="table" target="#tab_5">3</ref>, we only do sampling from P ? (z i is masked|X). If z i is not masked, we directly use z i = arg max ti P ? (t i |X) because it empirically leads to better performance.</p><p>Original Objective In the original implementation, CMLM and GLAT use a masked language model objective, where the unmasked tokens are not included in the loss L NAT . Formally,</p><formula xml:id="formula_27">L NAT = E Q(Z,T |X) - i / ?G log P ? (t i |Z, X) ,</formula><p>where G is the set of the unmasked token.</p><p>Proof of Equivalence To reach a same formulation of Eq.19, we add a copy mechanism in the NAT decoder. The decoder directly copies the unmasked token as the prediction if available. As a result, for an unmasked token t i , log P ? (t i |Z, X) = 0 because the prediction of t i is always correct. Therefore, the masked language model objective recovers our objective: Note that the copy mechanism does not require modifications to the network architecture.</p><formula xml:id="formula_28">L NAT = E Q(Z,</formula><p>E.7. VAE <ref type="bibr" target="#b34">(Shu et al., 2020)</ref> Although not discussed in our main analysis, VAE and its variants <ref type="bibr" target="#b17">(Kaiser et al., 2018;</ref><ref type="bibr" target="#b2">Bao et al., 2021;</ref><ref type="bibr">2022)</ref> can also be formulated as a method to provide proxy input in MPLE. VAE uses two trainable networks, the prior and posterior networks, to model P ? (Z|X) and Q(Z|T, X), respectively. Specially, the posterior network Q(Z|T, X) can be trained together with ?.</p><p>F. Implementation Details of Dynamic KD Candidate Generation. Dynamic KD chooses the proxy target from a candidate set ?, which contains Raw Data and four distilled targets. We generate the distilled targets with beam size 5 from four AT teachers, whose hyper-parameters and performance are shown in Table <ref type="table" target="#tab_1">10</ref>. For WMT14 En-De, we train the AT teachers for 100k updates with a batch of approximately 64k tokens. For WMT17 Zh-En, we raise the step to 300k to match the size of training data, and tune the length penalty in the beam search on the validation set.</p><p>For fair comparisons in Table <ref type="table" target="#tab_4">2</ref> and 3, we do not use any decoding tricks and only modify the methods for obtaining Z and T . Taking OaXE as an example, our implementation differs from their original paper <ref type="bibr" target="#b6">(Du et al., 2021)</ref> in: (1) Our OaXE is finetuned on a vanilla NAT, not a CMLM. (2) They use Transformer-big for KD whereas we use Transformer-base. (3) They use Length Parallel Decoding <ref type="bibr">(Wei et al., 2019a)</ref> of beam 5 and the de-duplication trick <ref type="bibr" target="#b21">(Lee et al., 2018)</ref> for decoding. We do not use any reranking methods here. (4) We do not use the truncation trick because it is incompatible with L NAT in our formulation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1: The log-likelihood and BLEU score on a 2D section in NAT's weight space. GLAT+KD ( ) and MLE ( ) are trained via different objectives starting from an initial checkpoint ( ) for 10k steps, where MLE achieve higher log-likelihood but lower BLEU, and GLAT is the opposite. Each point of the contour map is linearly interpolated from the three checkpoints. All values are evaluated on one translation benchmark, the validation set of WMT14 En-De.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>with parameter ? ? (c) NAT with parameter ? ? Log-Likelihood =-16.83</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>We use two translation benchmarks, WMT14 En-De (4.5M) and WMT17 Zh-En (20M), and follow Zhou et al. (2020); Kasai et al. (2020) for preprocessing.Knowlegde Distillation We use Transformer-base with the same settings as<ref type="bibr" target="#b38">Vaswani et al. (2017)</ref> and generate the distilled data with beam size 5. All models are based on KD unless otherwise specified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: L NAT of different methods on WMT14 En-De. All methods except Raw Data achieve lower L NAT than the dataset's C, verifying that training on the proxy distribution alleviates the information loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: KD data are even closer to the multiple references on average than Raw Data on WMT14 En-De, thereby achieving lower Ltarget and improving the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Decoding Confidence I and the BLEU score with two decoding strategies. CMLM with Input Sampling is more confident and thus achieves better BLEU than Default Decoding. GLAT is the opposite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>E. 1</head><label>1</label><figDesc>. Raw Data Raw Data uses the original target sentence as the proxy target. Formally, it defines Q(T |X) as a one-point distribution that Q(T = Y * |X) = 1, where Y * is the original target in the dataset. E.2. Knowledge Distillation (KD, Gu et al., 2018) KD first trains an autoregressive model P AR on the raw data, and then uses beam search to obtain T * = arg max Y P AR (Y |X). Formally, Q(T |X) is defined as a one-point distribution at T * . E.3. Aligned Cross Entropy (AXE, Ghazvininejad et al., 2020) Q(T |X) is defined as a one-point distribution at T * , where T * = arg min T ?S(R) L NAT . The reference R = [r 1 , ? ? ? , r L ] is picked from Raw Data or KD. Any T ? S(R) is a subsequence of R with empty tokens inserted. 10 An example is shown in Fig.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Examples of the alignment ? ? ?, the adjacency list ? ? ?, and the proxy target T * in AXE and OaXE. Red dotted line in AXE indicates the skip target operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>T |X)i / ?G log P ? (t i |Z, X) + 0 = E Q(Z,T |X)i / ?G log P ? (t i |Z, X) -i?G log P ? (t i |Z, X) = E Q(Z,T |X) [-log P ? (T |Z, X)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Estimated C and the gap of BLEU between AT and NAT on various datasets. 3 A large C leads to significant performance gap between AT and NAT. All models are trained via MLE. ?BLEU = BLEU AT -BLEU NAT . The dataset details are presented in Appendix B.</figDesc><table><row><cell cols="2">Dataset WMT14 En-De 2.50 C</cell><cell cols="3">?BLEU BLEU AT BLEU NAT 15.32 27.11 11.79</cell></row><row><cell cols="2">WMT16 En-Ro 2.20</cell><cell>9.98</cell><cell>33.70</cell><cell>23.72</cell></row><row><cell>Synthetic B</cell><cell>1.51</cell><cell>5.66</cell><cell>20.97</cell><cell>15.31</cell></row><row><cell>Synthetic A</cell><cell>0.92</cell><cell>0.35</cell><cell>26.96</cell><cell>26.61</cell></row><row><cell cols="5">when X is known. We make two remarks on Theorem 1:</cell></row><row><cell cols="5">Remark 1. A well-trained NAT (in terms of KL divergence)</cell></row><row><cell cols="5">achieves perfect approximations on marginal distributions</cell></row><row><cell cols="5">but drops all the dependency information between target</cell></row><row><cell cols="3">tokens, which can be measured by C.</cell><cell></cell><cell></cell></row><row><cell cols="5">Remark 2. C is a property of data distribution representing the difficulties in NAT learning. Given the data distribution,</cell></row><row><cell cols="5">an NAT cannot achieve an information loss less than C regardless of its parameters or training methods.</cell></row><row><cell cols="5">Conditional TC and Performance Gap To better under-</cell></row><row><cell cols="5">stand how C affects the NAT performance, we estimate C and compare the generation performance of AT and NAT</cell></row><row><cell cols="5">models trained via MLE on four datasets. Since C is usually high for most datasets, besides two translation benchmarks,</cell></row><row><cell cols="5">we further construct two synthetic datasets that have a lower</cell></row><row><cell cols="4">C. Please refer to Appendix B for more details.</cell><cell></cell></row><row><cell cols="5">As shown in Table 1, large C indicates strong dependencies between target tokens, leading to a serious performance gap</cell></row><row><cell cols="5">between NAT and AT models. When C is small, NAT can achieve a similar performance with AT, verifying that the</cell></row><row><cell cols="5">large Conditional TC is the main obstacle in NAT learning.</cell></row><row><cell cols="4">3. Understanding NAT Learning via</cell><cell></cell></row><row><cell cols="5">Maximum Proxy-Likelihood Estimation</cell></row><row><cell cols="5">Sec.2 shows that MLE-trained NAT drops the dependen-</cell></row><row><cell cols="5">cies between tokens, where C measures the difficulties in NAT learning. In this section, we investigate previous suc-</cell></row><row><cell cols="5">cesses in training NATs and propose a unified perspective</cell></row><row><cell>to understand them.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Specifically, we revisit existing training objectives and find</cell></row><row><cell cols="5">that many of them improve the MLE training by simplifying</cell></row><row><cell cols="5">the target sentences or enhancing the training inputs. Such</cell></row><row><cell cols="5">modifications significantly change the training directions,</cell></row><row><cell cols="5">where they actually maximize the likelihood on a proxy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>NAT Encoder Proxy Input ? Proxy Target ? Input Predictor</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Real Target ?</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>? 1</cell><cell>? 2</cell><cell>?</cell><cell>? ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Output Paraphraser</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">? ? (?|?)</cell><cell></cell></row><row><cell></cell><cell cols="2">? ? (?|?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? 1</cell><cell>? 2</cell><cell>?</cell><cell>? ?</cell><cell>? 1</cell><cell>? 2</cell><cell>?</cell><cell>? ?</cell></row><row><cell cols="4">Source Sentence ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Figure 4: The latent variable model used in the</cell></row><row><cell cols="8">derivation. The source sentence X and the real</cell></row><row><cell cols="8">target Y are observable, whereas the proxy</cell></row><row><cell cols="8">input Z and the proxy target T are latent.</cell></row><row><cell>distribution instead of the original distribution. The proxy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>distribution with modified targets or inputs usually has a low</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C, thereby reducing the information loss in NAT learning. Based on the above analysis, we formulate these methods in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a unified framework, named Maximum Proxy-Likelihood</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Estimation (MPLE). Intuitively, MPLE's objective can be</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>expressed as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and DA-Transformer(Huang et al.,   2022b)  have been shown very effective in NAT learning, where they also utilize alignment-based objectives. Unlike AXE, these methods predict a sequence longer than the real target, and then remove useless tokens by rules or model predictions. Their success show that the proxy target T does not necessarily have similar length with Y , where a longer T can be more flexible and efficient in reducing the token dependencies. Moreover, they introduce a different P ? (Y |T ) from Eq.13, which predicts Y from a longer T with reconstruction of dependency information, e.g., by transitions predicted inDA-Transformer. 7   </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of proxy targets on WMT14 En-De. All methods use Vanilla for Z and L input = 0. LMPLE and BLEU are strongly correlated (Pearson's |r|=0.99). AXE's ? and OaXE's numbers indicate the skip penalty and pretraining steps, which are hyper-parameters in choosing T .</figDesc><table><row><cell>Models Raw Data</cell><cell>L NAT Ltarget LMPLE BLEU 4.41 -6.42 -2.01 11.79</cell></row><row><cell>KD</cell><cell>2.42 -7.08 -4.66 20.87</cell></row><row><cell>+ AXE (? =1)</cell><cell>0.78 -5.13 -4.35 18.56</cell></row><row><cell>+ AXE (? =5)</cell><cell>1.09 -6.34 -5.25 22.22</cell></row><row><cell cols="2">+ AXE (? =10) 1.25 -6.50 -5.26 22.35</cell></row><row><cell cols="2">+ OaXE (10k) 1.03 -4.41 -3.38 15.00</cell></row><row><cell cols="2">+ OaXE (50k) 0.79 -5.84 -5.06 21.37</cell></row><row><cell cols="2">+ OaXE (300k) 0.83 -6.28 -5.44 22.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of proxy inputs on WMT14 En-De. All methods use KD for T and Ltarget = -7.08. Sample and Default indicate the BLEU score with Input Sampling and Default Decoding. LMPLE is strongly correlated with Sample BLEU (|r|=0.99) but less correlated with Default BLEU (|r|=0.37). The variants of CMLM and GLAT use different strategies for masking, detailed in Appendix E.6.</figDesc><table><row><cell>Models Vanilla</cell><cell>L input L NAT LMPLE Sample Default 0 2.42 -4.66 20.87 20.87</cell></row><row><cell>CMLM</cell><cell>0.99 0.46 -5.63 23.48 19.39</cell></row><row><cell cols="2">+ fixed masking ratio 0.48 0.55 -6.05 24.28 19.35</cell></row><row><cell>GLAT</cell><cell>0.45 0.66 -5.96 23.98 25.12</cell></row><row><cell>+ Levenshtein dist.</cell><cell>0.41 0.73 -5.94 24.03 24.84</cell></row><row><cell>+ mask by P ref</cell><cell>0.25 1.24 -5.59 22.98 24.22</cell></row><row><cell>+ mask by 1 -P ref</cell><cell>0.57 0.50 -6.01 24.35 25.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparing Dynamic KD against AT and previous NATs. ?: Reported by Qian et al. (2021a) and Du et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Estimated C and parameter size for an AT to achieve similar performance with NAT on various dataset. Parameter Ratio is the ratio of AT parameter size and NAT parameter size. For example, ATs with 2.1% ? 3.2% of parameters achieve 4.60 ? 11.88 BLEU on WMT14 En-De, which is similar to the NAT performance (11.79). The large C leads to serious information loss in NAT learning, therefore a small AT can easily outperforms the NAT.</figDesc><table><row><cell>Dataset WMT14 En-De 2.50 C WMT16 En-Ro 2.20 Synthetic B 1.51 Synthetic A 0.92</cell><cell>Parameter Ratio BLEU NAT 2.1% ? 3.2% 11.79 3.2% ? 4.3% 23.72 4.3% ? 7.0% 15.31 27.3% ? 100% 26.61</cell><cell>BLEU AT 4.60 ? 11.88 20.50 ? 24.75 14.10 ? 16.10 23.99 ? 26.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>AT Architectures used in searching the parameter size.</figDesc><table><row><cell></cell><cell cols="6">d model d hidden n layers n heads # Param Parameter Ratio</cell></row><row><cell>1</cell><cell>32</cell><cell>128</cell><cell>2</cell><cell>2</cell><cell>1.3M</cell><cell>2.1%</cell></row><row><cell>2</cell><cell>48</cell><cell>192</cell><cell>2</cell><cell>2</cell><cell>2.0M</cell><cell>3.2%</cell></row><row><cell>3</cell><cell>64</cell><cell>256</cell><cell>2</cell><cell>2</cell><cell>2.7M</cell><cell>4.3%</cell></row><row><cell>4</cell><cell>96</cell><cell>384</cell><cell>2</cell><cell>2</cell><cell>4.4M</cell><cell>7.0%</cell></row><row><cell>5</cell><cell>128</cell><cell>512</cell><cell>3</cell><cell>4</cell><cell>6.6M</cell><cell>10.5%</cell></row><row><cell>6</cell><cell>256</cell><cell>1024</cell><cell>3</cell><cell>4</cell><cell>17.1M</cell><cell>27.3%</cell></row><row><cell>7</cell><cell>512</cell><cell>2048</cell><cell>6</cell><cell>8</cell><cell>62.6M</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison of methods that obtain proxy targets on WMT17 Zh-En. All methods use Vanilla and L input =0. LMPLE and BLEU are strongly correlated (Pearson's |r|=0.96). AXE's ? and OaXE's numbers indicate the skip penalty and the pre-training step, which are hyper-parameters in choosing proxy targets.</figDesc><table><row><cell>Models Raw Data</cell><cell>L NAT Ltarget LMPLE BLEU 4.43 -6.25 -1.82 8.69</cell></row><row><cell>KD</cell><cell>2.85 -5.72 -2.87 15.53</cell></row><row><cell>+ AXE(? =1)</cell><cell>1.02 -2.95 -1.93 9.68</cell></row><row><cell>+ AXE(? =5)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Comparison of methods and variants that obtain proxy inputs on WMT17 Zh-En. All methods use KD and Ltarget = -5.72. Sample and Default indicate the BLEU score in Input Sampling and Default Decoding. LMPLE is strongly correlated with Sample BLEU (Pearson's |r|=0.99) but less correlated with Default BLEU (|r|=0.35).</figDesc><table><row><cell>Models Vanilla</cell><cell>L input L NAT LMPLE Sample Default 0 2.85 -2.87 15.53 15.53</cell></row><row><cell>CMLM</cell><cell>1.13 0.76 -3.83 19.74 14.12</cell></row><row><cell>+ Fixed</cell><cell>0.29 1.13 -4.30 20.81 14.49</cell></row><row><cell>GLAT</cell><cell>0.33 1.26 -4.13 20.73 22.51</cell></row><row><cell cols="2">+ Levenshtein 1.08 0.44 -4.19 20.71 21.70</cell></row><row><cell>+ P ref</cell><cell>0.40 1.60 -3.73 18.98 21.22</cell></row><row><cell>+ 1 -P ref</cell><cell>0.73 0.81 -4.17 20.79 21.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Full results of comparing Dynamic KD against AT and previous NATs. The best results of Non-iterative NATs are bolded. NPD<ref type="bibr" target="#b11">(Gu et al., 2018)</ref> and LPD(Wei et al., 2019a)  indicate reranking methods with the number of candidates. NPD are slower than LPD due to the use of an external AT reranker. The ablation of regularizer in Dynamic KD indicates removing Ltarget in choosing the proxy target. ?: Results reported by previous studies. Speed up of Imputer are re-evaluated in our implementation. ?: Use adaptive iteration numbers.</figDesc><table><row><cell></cell><cell>Models</cell><cell cols="4"># Iters En-De Zh-En Speedup</cell></row><row><cell>AT</cell><cell>Transformer</cell><cell>L</cell><cell cols="2">27.11 23.89</cell><cell>1.0x</cell></row><row><cell></cell><cell>CMLM ? (Ghazvininejad et al., 2019)</cell><cell>4 10</cell><cell cols="2">25.94 21.90 27.03 23.21</cell><cell>3.0x 1.3x</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell cols="2">25.83 22.42</cell><cell>4.3x</cell></row><row><cell>Iterative</cell><cell>DisCo ? (Kasai et al., 2020)</cell><cell>10</cell><cell cols="2">27.06 23.68</cell><cell>3.2x</cell></row><row><cell>NATs</cell><cell></cell><cell cols="3">? 4  ? 27.34 23.83 1 25.8 /</cell><cell>/ 14.9x</cell></row><row><cell></cell><cell>Imputer ? (Saharia et al., 2020)</cell><cell>2</cell><cell>27.5</cell><cell>/</cell><cell>7.5x</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell>28.2</cell><cell>/</cell><cell>2.7x</cell></row><row><cell>Non-iterative NATs</cell><cell>MLE GLAT (NPD=7) ? (Qian et al., 2021a) OaXE (LPD=5) ? (Du et al., 2021)</cell><cell>1 1 1</cell><cell>11.79 26.55 26.1</cell><cell>8.69 / 22.1</cell><cell>15.3x 7.9x 14.2x</cell></row><row><cell></cell><cell>Vanilla + KD</cell><cell>1</cell><cell cols="2">20.98 15.53</cell><cell>15.3x</cell></row><row><cell></cell><cell>+ Dynamic KD w/o Regularizer</cell><cell>1</cell><cell cols="2">20.51 18.10</cell><cell>15.3x</cell></row><row><cell></cell><cell>+ Dynamic KD</cell><cell>1</cell><cell cols="2">22.82 18.29</cell><cell>15.3x</cell></row><row><cell>Ours</cell><cell>+ LPD=3 + Decoding Tricks GLAT + KD</cell><cell>1 1</cell><cell cols="2">24.83 19.97 25.12 22.51</cell><cell>14.6x 15.3x</cell></row><row><cell></cell><cell>+ Dynamic KD w/o Regularizer</cell><cell>1</cell><cell cols="2">22.66 22.14</cell><cell>15.3x</cell></row><row><cell></cell><cell>+ Dynamic KD</cell><cell>1</cell><cell cols="2">25.88 23.07</cell><cell>15.3x</cell></row><row><cell></cell><cell>+ LPD=3 + Decoding Tricks</cell><cell>1</cell><cell cols="2">26.89 24.42</cell><cell>14.6x</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this paper, we mainly discuss the NATs that use the same architecture and without iterative refinement unless otherwise specified. The likelihood is obtained on the validation set.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>It is achieved when P ? (yi|y&lt;i, X) = Pdata(yi|y&lt;i, X).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We report C with the V-entropy<ref type="bibr" target="#b42">(Xu et al., 2020)</ref> instead of the Shannon entropy because the latter is intractable due to the unknown data distribution. Note that C measures the amount of information, whose unit is bit, which is comparable across datasets. However, ?BLEU is not strictly comparable, where we present a more rigorous comparison in Appendix B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>A distribution with multiple modes requires dependency information to recover the joint distribution, as shown in Fig.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We discuss the non-iterative version of CMLM here, following<ref type="bibr" target="#b9">Ghazvininejad et al. (2020)</ref>;<ref type="bibr" target="#b6">Du et al. (2021)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Both methods do not directly fit in Eq.8 because they maximize the logarithm of probability sum on all alignments instead of a single T . However, we refer the reader to Sec.3.2 ofHuang  et al. (2022b), which shows that their objectives can be regarded as utilizing multiple proxy targets with different weights.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>More precisely, we first decide whether a token is masked according to the predicted distribution. If it is not masked, we directly use the most likely non-mask token, which empirically leads to better performance. Please see Appendix E.6 for details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Specifically, their proposed metric may not correctly reflect the difficulties of NAT learning in some cases. For a dataset</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to thank <rs type="person">Yuxuan Song</rs> for the help in the derivation of Theorem 1 and the MPLE framework. This work was supported by the <rs type="funder">National Science Foundation for Distinguished Young Scholars</rs> (with No. <rs type="grantNumber">62125604</rs>) and the <rs type="funder">NSFC</rs> projects (<rs type="projectName">Key</rs> project with No. <rs type="grantNumber">61936010</rs> and regular project with No. <rs type="grantNumber">61876096</rs>). This work was also supported by the <rs type="funder">Guoqiang Institute of Tsinghua University</rs>, with Grant No. <rs type="grantNumber">2019GQG1</rs> and <rs type="grantNumber">2020GQG0005</rs>, and sponsored by <rs type="funder">Tsinghua-Toyota Joint Research Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zzPyd6S">
					<idno type="grant-number">62125604</idno>
				</org>
				<org type="funded-project" xml:id="_Hj5E5rb">
					<idno type="grant-number">61936010</idno>
					<orgName type="project" subtype="full">Key</orgName>
				</org>
				<org type="funding" xml:id="_sAJha83">
					<idno type="grant-number">61876096</idno>
				</org>
				<org type="funding" xml:id="_MtysMdR">
					<idno type="grant-number">2019GQG1</idno>
				</org>
				<org type="funding" xml:id="_e4v5VFE">
					<idno type="grant-number">2020GQG0005</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where S is the sentence BLEU, ? i is the target distilled from the i-th teacher model, and ? i is hyper-parameters to bias the candidates from different teachers (i = 5 indicates Raw Data). We use L target instead of Ltarget in selecting the proxy target.</p><p>Hyper-parameter Selection. To find the optimal value of ? i , we introduce the multi-reference dataset <ref type="bibr" target="#b24">(Ott et al., 2018;</ref><ref type="bibr" target="#b14">Hassan et al., 2018)</ref>, making it possible to adjust the value of L target according to the real Ltarget . Intuitively, if L NAT is the same for all target candidates, we should choose a proxy target that minimize the data distortion. Therefore, we obtain T = arg min L target as the current proxy target with a specific ? i , and then evaluate the real data distortion Ltarget . We tune ? i to minimize Ltarget . Notably, tuning ? i only involves calculating the BLEU score, which does not need to train a NAT model. We do a manual search from 1 to 3 with the step of 0.1 and finally choose ? = [0.7, 0.6, 1.3, 1.5, 2.3] for WMT14 En-De and ? = [0.9, 1.4, 1.3, 0.9, 2.0] for WMT17 Zh-En. Then we train a NAT with the dynamic KD and further tune ? according to the generation performance on the validation set, where we finally choose ? = 0.2 for WMT14 En-De and ? = 0.1 for WMT17 Zh-En.</p><p>Applying Dynamic KD to GLAT. When combining GLAT with Dynamic KD, L NAT may suffer from high variance because Z is sampled from Q(Z|T ) following the rule of GLAT. In our implementation, we simply ignore Z when choosing the proxy target. Specifically, we obtain the proxy target by T * = arg min T LNAT + L target , where</p><p>and Z * is a full masked sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Details of Experiment Settings</head><p>For WMT14 En-De, we follow <ref type="bibr" target="#b44">Zhou et al. (2020)</ref> to use a joint BPE <ref type="bibr" target="#b31">(Sennrich et al., 2016)</ref> with 32K merge operations, which leads to a vocabulary of 40k tokens. For WMT17 Zh-En, we follow <ref type="bibr" target="#b18">Kasai et al. (2020)</ref> to use a BPE with 32K merge operations, which leads to vocabularies of 48k tokens in Chinese and 33k tokens in English.</p><p>All our models are implemented with Fairseq <ref type="bibr" target="#b25">(Ott et al., 2019)</ref> and generally follow the hyper-parameter of transformer-base <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>. For regularization, we set dropout to 0.1, weight decay to 0.01, and label smoothing to 0.1. Except for OaXE, all models are trained for 300k updates with a batch of approximately 64k tokens. The learning rate warms up to 5 ? 10 -4 within 10k steps and then decays with the inverse square-root schedule. For OaXE, we choose a pre-trained vanilla NAT and finetune the model for 100k steps with a fixed learning rate of 10 -5 . We evaluate the BLEU scores on the validation set every epoch and average the best 5 checkpoints for the final model. All models are trained with mixed precision floating point arithmetic on 8 Nvidia V100-32G GPUs. It costs approximately 20 hours for a vanilla NAT and 30 hours for Dynamic KD + GLAT.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information theory and an extension of the maximum likelihood principle</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected papers of hirotugu akaike</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="199" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Non-autoregressive transformer by position learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1911.10677</idno>
		<ptr target="http://arxiv.org/abs/1911.10677" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">their proposed metric is high if p(?|X) has a large entropy. However, our C = 0, correctly showing that the data satisfy the independent assumption</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.458</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.naacl-main.458" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of y1, ? ? ? , yM iid ? p(?|X)</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting>the 2021 Conference of the North American Chapter of y1, ? ? ? , yM iid ? p(?|X)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">June 6-11, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">latent-glat: Glancing at latent variables for parallel text generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.02030</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2204.02030" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding and improving lexical choice in non-autoregressive translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ZTFeSBIX9C" />
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rejuvenating low-frequency words: Making the most of parallel data in non-autoregressive translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.266</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.acl-long.266" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021">August 1-6, 2021. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3441" />
		</imprint>
	</monogr>
	<note>Long Papers), Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Order-agnostic cross entropy for non-autoregressive machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/du21c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2849" to="2859" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A tutorial on variational bayesian inference</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-011-9236-8</idno>
		<ptr target="https://doi.org/10.1007/s10462-011-9236-8" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1633</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1633" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wan</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="6111" to="6120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aligned cross entropy for non-autoregressive machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/ghazvininejad20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3515" to="3523" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully non-autoregressive neural machine translation: Tricks of the trade</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.11</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.findings-acl.11" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1l8BtlCb" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Levenshtein transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/675" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Garnett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="11179" to="11189" />
		</imprint>
	</monogr>
	<note>f9820626f5bc0afb47b57890b466e-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jointly masked sequence-tosequence model for non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.36</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.36" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">July 5-10, 2020. 2020</date>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1803.05567</idno>
		<ptr target="http://arxiv.org/abs/1803.05567" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonautoregressive translation with layer-wise prediction and deep supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Za?ane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.07515" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, 2022a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Directed acyclic transformer for non-autoregressive machine translation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.07459" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning, ICML 2022, 2022b</title>
		<meeting>the 39th International Conference on Machine Learning, ICML 2022, 2022b</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/kaiser18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan, Stockholm, Sweden</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan, Stockholm, Sweden</meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2395" to="2404" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonautoregressive machine translation with disentangled context transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/kasai20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="5144" to="5155" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=KpfasTaLUpq" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1139</idno>
		<ptr target="https://doi.org/10.18653/v1/d16-1139" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</editor>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01">2016. November 1-4, 2016. 2016</date>
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deterministic nonautoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1149</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1149" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04">October 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end non-autoregressive neural machine translation with connectionist temporal classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Libovick?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Helcl</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1336</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1336" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04">October 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="3016" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-autoregressive conditional sequence generation with generative flow</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><surname>Flowseq</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1437</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1437" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wan</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="4281" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analyzing uncertainty in neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/ott18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3953" to="3962" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-4009</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-4009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Ammar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Louis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://aclanthology.org/P02-1040/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">July 6-12, 2002. 2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glancing transformer for nonautoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.acl-long.155" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1993" to="2003" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.11247" />
		<title level="m">The volctrans GLAT system: Non-autoregressive translation meets WMT21. CoRR, abs/2109.11247, 2021b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Guiding nonautoregressive neural machine translation decoding with reordering information</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/17618" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">February 2-9, 2021. 2021</date>
			<biblScope unit="page" from="13727" to="13735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nonautoregressive machine translation with latent alignments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.83</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.83" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="1098" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1162</idno>
		<ptr target="https://doi.org/10.18653/v1/p16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers. The Association for Computer Linguistics</publisher>
			<date type="published" when="2016-08-07">2016. August 7-12, 2016. 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Minimizing the bag-of-ngrams difference for nonautoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/5351" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="198" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence-level training for non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00421</idno>
		<ptr target="https://doi.org/10.1162/coli_a_00421" />
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="891" to="925" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Latentvariable non-autoregressive neural machine translation with deterministic inference using a delta posterior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/6413" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8846" to="8853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The multiinformation function as a tool for measuring stochastic dependence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Studen?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vejnarov?</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-011-5014-9_10</idno>
		<ptr target="https://doi.org/10.1007/978-94-011-5014-9_10" />
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<title level="s">NATO ASI Series</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</editor>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="261" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An EM approach to non-autoregressive conditional sequence generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/sun20c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">18 July 2020. 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="9249" to="9258" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast structured decoding for sequence models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/74563" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="3011" to="3020" />
		</imprint>
	</monogr>
	<note>ba21a90da13dacf2a73e3ddefa7-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Information theoretical analysis of multivariate correlation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.41.0066</idno>
		<ptr target="https://doi.org/10.1147/rd.41.0066" />
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="82" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imitation learning for non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1125</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>M?rquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1304" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imitation learning for non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1125</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>M?rquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1304" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A theory of usable information under computational constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1eBeyHFDH" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Posconstrained parallel decoding for non-autoregressive generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.467</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.acl-long.467" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Understanding knowledge distillation in non-autoregressive machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BygFVAEKDH" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
