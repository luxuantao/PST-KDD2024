<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Secure or Insure? A Game-Theoretic Analysis of Information Security Games</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jens</forename><surname>Grossklags</surname></persName>
							<email>jensg@ischool.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Christin</surname></persName>
							<email>nicolasc@cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Chuang</surname></persName>
							<email>chuang@ischool.berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">UC Berkeley School of Information Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University INI</orgName>
								<address>
									<country>CyLab Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Kobe</orgName>
								<address>
									<postCode>650-0044</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">UC Berkeley School of Information Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Secure or Insure? A Game-Theoretic Analysis of Information Security Games</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1FE9C10F1F826A82231D5A54F55BBDBA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.2 [Computer Systems Organization]: Computer-Communication Networks</term>
					<term>J.4 [Computer Applications]: Social and Behavioral Sciences-Economics</term>
					<term>K.4.4 [Computers and Society]: Electronic Commerce-Security Economics, Reliability, Security Economics of the Internet, Game Theory, Public Goods, Incentive-Centered Design and Engineering, Security, Protection, Self-Insurance WWW 2008 / Refereed Track: Internet Monetization -Recommendation &amp; Security Beijing, China</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite general awareness of the importance of keeping one's system secure, and widespread availability of consumer security technologies, actual investment in security remains highly variable across the Internet population, allowing attacks such as distributed denialof-service (DDoS) and spam distribution to continue unabated. By modeling security investment decision-making in established (e.g., weakest-link, best-shot) and novel games (e.g., weakest-target), and allowing expenditures in self-protection versus self-insurance technologies, we can examine how incentives may shift between investment in a public good (protection) and a private good (insurance), subject to factors such as network size, type of attack, loss probability, loss magnitude, and cost of technology. We can also characterize Nash equilibria and social optima for different classes of attacks and defenses. In the weakest-target game, an interesting result is that, for almost all parameter settings, more effort is exerted at Nash equilibrium than at the social optimum. We may attribute this to the "strategic uncertainty" of players seeking to self-protect at just slightly above the lowest protection level.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Internet has opened new and attractive channels to publicize and market products, to communicate with friends and colleagues, and to access information from spatially distributed resources. Though it has grown significantly, the network's architecture still reflects the cooperative spirit of its original designers <ref type="bibr" target="#b32">[32]</ref>. Unfortunately, today's network users are no longer held together by that same sense of camaraderie and common purpose. For instance, concrete evidence of the tragedy of the commons <ref type="bibr" target="#b21">[21]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>occurring in</head><p>Copyright is held by the International World Wide Web Conference Committee (IW3C2). Distribution of these papers is limited to classroom use, and personal use by others.</p><p>WWW 2008, <ref type="bibr">April 21-25, 2008</ref>, Beijing, China.</p><p>ACM 978-1-60558-085-2/08/04. peer-to-peer filesharing networks has been documented for a long time <ref type="bibr" target="#b2">[2]</ref>. Accordingly, studies of networking protocols and user interaction have been assuming users to be selfish and to act strategically <ref type="bibr" target="#b37">[37]</ref>.</p><p>Selfish users are one thing, but the expansion of the Internet has also attracted individuals and groups with often destructive motivations; these "attackers" intend to improve on their perceived utility by exploiting or creating security weaknesses and harming or inconveniencing other network users. Some malicious entities are motivated by peer recognition, or curiosity, and are often undecided regarding the ethical legitimacy of their behavior <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>. Others have clearly demonstrated financial goals <ref type="bibr" target="#b17">[17]</ref>. Problematic behaviors and threats include attacks on the network as a whole, attacks on selected end-points, undesirable forms of interactions such as spam e-mail, and annoyances such as Web pages that are unavailable or defaced. As a result, users cannot rely and trust other network participants <ref type="bibr" target="#b13">[13]</ref>.</p><p>When asked in surveys, network users say they are interested in preventing attacks and mitigating the damages from computer and information security breaches <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b40">40]</ref>. Researchers and industry have responded by developing numerous security technologies to alleviate many of the aforementioned problems <ref type="bibr" target="#b4">[4]</ref>, thereby expecting to help improving individual security practices.</p><p>Nevertheless, security breaches are common, widespread and highly damaging. The "I Love You" virus <ref type="bibr" target="#b27">[27]</ref>, Code Red <ref type="bibr" target="#b29">[29]</ref> and Slammer worms <ref type="bibr" target="#b28">[28]</ref>, to cite the most famous cases, have infected hundreds of thousands of machines and caused, all together, billions of dollars in damages. Underground markets for processor time on compromised end-systems are developing <ref type="bibr" target="#b17">[17]</ref> thanks to large population of home computers that can be easily commandeered by third-parties. The high financial impact of security failures is explained by user surveys <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b10">10]</ref>, which show strong evidence that comprehensive security precautions, be they patching, spyware-removal tools, or even sound backup strategies, are missing from a vast majority of systems surveyed.</p><p>In other words, despite a self-professed interest in security, most individuals do not implement effective security on their systems, even though necessary technologies and methods are (by and large) readily available. We propose to investigate the root causes of the disconnect between users' actions and their intentions.</p><p>In practice, there is a large variety of situations in which users face security threats, and an equally large number of possible responses to threats. However, we postulate in this paper that one can model most security interactions through a handful of "security games," and with a small number of decision parameters upon which each user can act.</p><p>More precisely, building upon public goods literature <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b43">43]</ref>, we consider the classical best shot, total effort, and weakest-link games, and will analyze them in a security context. We complement these three games with a novel model, called the "weakesttarget" game, which allows us to describe a whole class of attacks ranging from insider threats to very aggressive worms. Furthermore, while most research on the economics of security focuses on security investments as a problem with a single variable (e.g., amount of money spent on security), our analysis is the first to decouple protection investments (e.g., setting up a firewall) from insurance coverage (e.g., archiving data as back up). This decoupling allows us to explain a number of inefficiencies in the observed user behaviors.</p><p>This paper is only a first step toward a more comprehensive modeling of user attitudes toward security issues. Indeed, the present study relies on game theory, mostly using Nash equilibrium and social optima concepts. As such, we primarily view this study as a theoretical basis for follow up experimental work using laboratory experiments with human participants. We nevertheless show that the models and results derived here provide for considerable insights.</p><p>The rest of this paper is organized as follows. We elaborate in Section 2 the relationship of our work with related research, before introducing our game-theoretic models in Section 3. We present an analysis of the Nash equilibria (Section 4) and social optima (Section 5) for all of these games, and discuss our findings in Section 6. We conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The economics of information security is a growing research area with a diverse set of participating researchers from various disciplines. Important common anchors are the observations that misaligned incentives and positive and negative externalities play significant roles in the strategies used by each party in the battle between attackers and potential victims <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>.</p><p>Economics as a tool for security analysis has gained in importance since the economy of attackers has become increasingly rational (e.g., motivated by greed), over the last years <ref type="bibr" target="#b17">[17]</ref>. This increasingly rational behavior stands in contrast to that exhibited by the hacker communities of the 1980s and 1990s, who valued reputation, intellectual achievement, and even entertainment above financial incentives <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>.</p><p>Most of the initial results obtained in security economics research concern the analysis of optimal security investments. For example, Gordon and Loeb <ref type="bibr" target="#b18">[18]</ref> as well as Hausken <ref type="bibr" target="#b22">[22]</ref> focus on the impact of different security breach functions and degrees of vulnerability on an entity's investment strategy. More specialized models have been proposed to analyze a subset of important security management problems. For instance, August and Tunca scrutinize optimal system update strategies when patching a system against security vulnerabilities is costly <ref type="bibr" target="#b7">[7]</ref>. Rescorla investigates the impact of code quality control on vulnerability of software <ref type="bibr" target="#b31">[31]</ref>. Böhme and Kataria study individual security investment decisions and market insurance offers when correlation of cyber-risks within a single firm and across multiple firms differ <ref type="bibr" target="#b8">[8]</ref>.</p><p>From a policy standpoint, Bull et al. <ref type="bibr" target="#b11">[11]</ref> argue that given the state of heterogeneous networks no single security policy will be applicable to all circumstances. They suggest that, for a system to be viable from a security standpoint, individuals need to be empowered to control their own resources and to make customized security trade-offs. This stands in contrast to the traditional centralized structure where all security decision are made by a central planner (e.g., the IT department). Nevertheless, as Anderson suggests, organizational and structural dependencies have to be considered in individual security decision making <ref type="bibr" target="#b3">[3]</ref>.</p><p>While many models prescribe behavior in individual choice situations, the focus of our work is to model and study strategic interaction with respect to security decisions in networked systems, in an effort to understand the impact of individual choices on a larger group. Such interaction usually involves common as well as conflicting interests. (Pure conflict, in which the interests of the two antagonists are completely opposed, is a special case.) This mutual dependence as well as opposition guarantees for a much richer scenario for analysis <ref type="bibr" target="#b35">[35]</ref>.</p><p>To better understand the implications of this mutual dependence, Varian <ref type="bibr" target="#b43">[43]</ref> conducts an analysis of system reliability within a public goods game-theoretical framework. He discusses the best effort, weakest-link and total effort games, as originally analyzed by Hirshleifer <ref type="bibr" target="#b23">[23]</ref>. The main difference from classical public goods theory is that within the framework of computer reliability "considerations of costs, benefits, and probability of failure become paramount, with income effects being a secondary concern." <ref type="bibr" target="#b43">[43]</ref> Varian focuses on two-player games with heterogeneous effort costs and benefits from reliability. <ref type="foot" target="#foot_0">1</ref> He also adds an inquiry into the role of taxes and fines, and differences between simultaneous and sequential moves.</p><p>Our work generalizes <ref type="bibr" target="#b43">[43]</ref> in several aspects. First, instead of considering security decisions to be determined by a single "security" variable, we identify two key components of a security strategy: self-protection (e.g., patching system vulnerabilities) and selfinsurance (e.g., having good backups). More precisely, we allow agents to self-protect and/or self-insure their resources in N -player games. We also contrast the three canonical games discussed by Varian with two more complex "weakest-target" games that represent a more complicated incentive structure, which we believe applies to a whole class of security issues.</p><p>Outside the information security context, the dual role of selfprotection and self-insurance was first recognized by <ref type="bibr" target="#b15">[15]</ref>. To provide a more precise definition, self-protection stands for the ability to reduce the probability of a loss -for example, by installing a firewall application which limits the amount of traffic allowed to communicate with one's network. Self-insurance, on the other hand, denotes a reduction in the magnitude of a loss, e.g., by performing regular backups on existing data. Some technologies and practices such as disconnecting a computer from a network do both. Ehrlich and Becker <ref type="bibr" target="#b15">[15]</ref> focus in their analysis on the comparison of selfprotection and self-insurance to market insurance. They find that, for rare loss events, there is less incentive to self-insure losses than to use market insurance. This is due to their assumption, that the price of self-insurance is independent of the probability of the loss. An additional result is that the demand for self-insurance grows with the base loss of a security threat. As an outcome of their work, they characterize self-insurance and market insurance as substitutes, and self-protection and market insurance as complements. Our analysis complements the work in <ref type="bibr" target="#b15">[15]</ref> by extending the concepts of self-protection and self-insurance to the public goods and security context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DESCRIPTION OF SECURITY GAMES</head><p>We define a security game as a game-theoretic model that captures essential characteristics of decision making to protect and self-insure resources within a network. Varian <ref type="bibr" target="#b43">[43]</ref> observed that frequently the success of security (or reliability) decision making depends on a joint protection level determined by all participants of a network. The computation of the protection level will often take the form of a public goods contribution function with nonrival and nonexcludable benefits or consequences. A main observation is that dependent on the contribution function individuals may be able to freeride on others' efforts. However, individuals may also suffer from inadequate protection efforts by other members if those have a decisive impact on the overall protection level. Following Varian's exposition, we analyze three canonical contribution functions that determine a global protection level. Different from Varian's work however, here network members have a second action available: They can decide to self-insure themselves from harm. The success of insurance decisions is completely independent of protection choices made by the individual and others. Consequently, the games we consider share qualities of private (on the insurance side) and public (on the protection side) goods. We further add to the research literature by studying two additional games with a more complex determination of protection levels.</p><p>Security games share the following key assumptions: (i) all entities in the network share a single purely public protection output, (ii) a single individual decides on protection efforts for each entity (so we do not assume a second layer of organizational decision making), (iii) protection costs per unit are identical for each entity, and (iv) all decisions are made simultaneously. These assumptions are commonly made also in models on decision making of partners in military alliances <ref type="bibr" target="#b33">[33]</ref>. We add to these main assumptions that individuals are able to self-insure resources at a homogeneous cost with self-insurance being a purely private good.</p><p>Formally, the basic model from which we develop the security games has the following payoff structure. Each of N ∈ N players receives an endowment M . If she is attacked and compromised successfully she faces a loss L. Attacks arrive with an exogenous probability of p (0 ≤ p ≤ 1). Players have two security actions at their disposition. Player i chooses an insurance level 0 ≤ si ≤ 1 and a protection level 0 ≤ ei ≤ 1. Finally, b ≥ 0 and c ≥ 0 denote the unit cost of protection and insurance, respectively. The generic utility function has the following structure:</p><formula xml:id="formula_0">Ui = M -pL(1 -si)(1 -H(ei, e-i)) -bei -csi ,<label>(1)</label></formula><p>where following usual game-theoretic notation, e-i denotes the set of protection levels chosen by players other than i. H is a "contribution" function that characterizes the effect of ei on Ui, subject to the protection levels chosen (contributed) by all other players. We require that H be defined for all values over (0, 1) N . However, we do not place, for now, any further restrictions on the contribution function (e.g., continuity). From Eqn. (1), the magnitude of a loss depends on three factors: i) whether an attack takes place (p), ii) whether the individual invested in self-insurance (1 -si), and iii) the magnitude of the joint protection level (1 -H(ei, e-i)). Selfinsurance always lowers the loss that an individual incurs when compromised by an attack. Protection probabilistically determines whether an attack is successful. Eqn. (1) therefore yields an expected utility. We introduce five games in the following discussion. In selecting and modeling these games we paid attention to comparability of our security games to prior research (e.g., <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b43">43]</ref>). The first three specifications for H represent important baseline cases recognized in the public goods literature. To allow us to cover most security dilemmas, we add two novel games, for which we could not find a formal representation in the literature. All games are easy to interpret within and outside the online security context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total effort security game:</head><p>The global protection level of the network depends on the sum of contributions normalized over the number of all participants. That is, we define H(ei, e-i) = 1</p><formula xml:id="formula_1">N P i ei, so that Eqn. (1) becomes Ui = M -pL(1 -si)(1 - 1 N X k e k ) -bei -csi . (2)</formula><p>Economists identified the sum of efforts (or total effort) contribution function long before the remaining cases included in this paper <ref type="bibr" target="#b23">[23]</ref>. We consider a slight variation of this game to normalize it to the desired parameter range. A typical parable for the total sum function is that the effectiveness of a dam or city wall depends on its strength that is contributed to by all players. In terms of security the average contributions matters if an attacker wants to successfully conquer the majority of machines in a network one-by-one.</p><p>For instance, consider a building plan for a new technology that is spread across a company's network and which is considerably more valuable to an attacker, if obtained in its entirety.</p><p>As another example, maybe more related to Internet security, consider parallelized file transfers, as in the BitTorrent peer-to-peer service. It may be the case that an attacker wants to slow down transfer of a given piece of information; but the transfer speed itself is a function of the aggregate effort of the machines participating in the transfer. Note that, the attacker in that case is merely trying to slow down a transfer, and is not concerned with completely removing the piece of information from the network: censorship actually results in a different, "best shot" game, as we discuss later.</p><p>Weakest-link security game: The overall protection level depends on the minimum contribution offered over all entities. That is, we have H(ei, e-i) = min(ei, e-i), and Eqn. (1) takes the form:</p><formula xml:id="formula_2">Ui = M -pL(1 -si)(1 -min(ei, e-i)) -bei -csi . (3)</formula><p>This game describes the situation where a levee or city wall that is too low at any point leads to a negative payoff to all players in the event of a flood or attack. The weakest link game is easily the most recognized public goods problem in computer security by business professionals and researchers alike. <ref type="foot" target="#foot_1">2</ref> Once the perimeter of an organization is breached it is often possible for attackers to leverage this advantage. This initial compromise can be the result of a weak password, an inconsistent security policy, or some malicious code infiltrating a single client computer.</p><p>Best shot security game: In this game, the overall protection level depends on the maximum contribution offered over all entities. Hence, we have H(ei, e-i) = max(ei, e-i), so that Eqn. <ref type="bibr" target="#b1">(1)</ref> becomes</p><formula xml:id="formula_3">Ui = M -pL(1 -si)(1 -max(ei, e-i)) -bei -csi . (4)</formula><p>As an example of a best shot game, consider a set of walls of which the highest sets the effectiveness benchmark. Among information systems, networks with built-in redundancy, such as peerto-peer, sensor networks, or even Internet backbone routes, share resilience qualities with the best shot security game; for instance, to completely take down communications between two (presumably highly connected) backbone nodes on the Internet, one has to shut down all possible routes between these two nodes. Censorshipresistant networks are another example of best shot games. A piece of information will remain available to the public domain as long as a single node serving that piece of information can remain unharmed <ref type="bibr" target="#b14">[14]</ref>.</p><p>Weakest-target security game (without mitigation): Here, an attacker will always be able to compromise the entity (or entities) with the lowest protection level, but will leave other entities unharmed. This game derives from the security game presented in <ref type="bibr" target="#b12">[12]</ref>. Formally, we can describe the game as follows:</p><formula xml:id="formula_4">H(ei, e-i) =  0 if ei = min(ei, e-i), 1 otherwise,<label>(5)</label></formula><p>which leads to</p><formula xml:id="formula_5">Ui =  M -pL(1 -si) -bei -csi if ei = min(ei, e-i), M -bei -csi otherwise. (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>The weakest-target game markedly differs from the weakest link. There is still a decisive security level that sets the benchmark for all individuals. It is determined by the individual(s) with the lowest chosen effort level. However, in this game all entities with a protection effort strictly larger than the minimum will remain unharmed.</p><p>In information security, this game captures the situation in which an attacker is interested in securing access to an arbitrary set of entities with the lowest possible effort. Accordingly, she will select the machines with the lowest security level. An attacker might be interested in such a strategy if the return on attack effort is relatively low, for example, if the attacker uses a compromised machine to distribute spam. Such a strategy is also relevant to an attacker with limited skills, a case getting more and more frequent with the availability of automated attack toolboxes <ref type="bibr" target="#b41">[41]</ref>; or, when the attacker's goal is to commandeer the largest number of machines using the smallest investment possible <ref type="bibr" target="#b17">[17]</ref>. Likewise, this game can be useful in modeling insider attacks -a disgruntled employee may for instance very easily determine how to maximize the amount of damage to her corporate network while minimizing her effort.</p><p>Weakest-target security game (with mitigation): This game is a variation on the above weakest-target game. The difference is that, the probability that the attack on the weakest protected player(s) is successful is now dependent on the security level min ei chosen. That is,</p><formula xml:id="formula_7">H(ei, e-i) =  1 -ei if ei = min(ei, e-i), 1 otherwise,<label>(7)</label></formula><p>so that</p><formula xml:id="formula_8">U i =  M -pL(1 -s i )(1 -e i ) -be i -cs i if e i = min(e i , e -i ), M -be i -cs i otherwise. (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>This game represents a nuanced version of the weakest-target game.</p><p>Here, an an attacker is not necessarily assured of success. In fact, if all individuals invest in full protection, not a single machine will be compromised. This variation allows us to capture scenarios where, for instance, an attacker targets a specific vulnerability, for which an easily deployable countermeasure exists.</p><p>Limitations: With the analysis in this paper we aim for a more thorough understanding of the ecology of security threats and defense functions an individual or organization faces and has to respond to. We have generalized and newly developed models that represent vastly different security scenarios and will call for different actions. As Hirshleifer observed <ref type="bibr" target="#b23">[23]</ref> a security practitioner will be presented with "all kinds of intermediate cases and combinations," e.g., social composition functions involving all of these five rules as well as other not identified yet. Some minor variations would be the "location of the top decile, or the total of the best three shots, or the average of the best and worst shots, or the variance or skewness" etc. See also <ref type="bibr" target="#b7">[7]</ref> and <ref type="bibr" target="#b26">[26]</ref> for variations in which the likelihood of a compromise depends on the number of unprotected players.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NASH EQUILIBRIUM ANALYSIS</head><p>We next determine the equilibrium outcomes where each individual chooses protection effort and self-insurance investments unilaterally, in an effort to maximize her own utility. In Section 5, we then compare these results to the protection efforts and selfinsurance levels chosen if coordinated by a social planner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Total effort</head><p>Let us focus on player i, and consider e k for k = i as exogenous. Then, Ui is a function of two variables, ei and si. From Eqn. (2), Ui is twice differentiable in ei and si, with ∂ 2 Ui/∂s 2 i = 0 and ∂ 2 Ui/∂e 2 i = 0. Hence, according to the second derivative test, only (ei, si) ∈ {(0, 0), (0, 1), (1, 0), (1, 1)} can be an extremumthat is, possible Nash equilibria are limited to these four values (or to strategies yielding a payoff constant regardless of ei and/or si).</p><p>As long as at least one of b or c is strictly positive, (ei, si) = (1, 1) is always dominated by either (ei, si) = (1, 0) or (ei, si) = (0, 1) and cannot define a Nash equilibrium. Let us analyze the three other cases:</p><formula xml:id="formula_10">• (ei, si) = (0, 0). Replacing in Eqn. (2), we get Ui = M -pL 0 @ 1 - 1 N X k =i e k 1 A .<label>(9)</label></formula><p>• (ei, si) = (0, 1). Replacing in Eqn.</p><p>(2), we get</p><formula xml:id="formula_11">Ui = M -c .<label>(10)</label></formula><p>• (ei, si) = (1, 0). Replacing in Eqn.</p><p>(2), we get</p><formula xml:id="formula_12">Ui = M -pL 0 @ 1 - 1 N - 1 N X k =i e k 1 A -b . (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>Result 1: After investigating Eqs. <ref type="bibr">(9)</ref><ref type="bibr" target="#b10">(10)</ref><ref type="bibr" target="#b11">(11)</ref> we can identify three Nash equilibrium strategies.</p><p>• Full protection eq.: If pL &gt; bN and c &gt; b + pL N -1 N , meaning that protection is cheap, potential losses are high, and insurance is extremely overpriced, then the (only) Nash equilibrium is defined by everybody protecting but not insuring, that is, (ei, si) = (1, 0).</p><p>• Full self-insurance eq.: In the other cases where pL &gt; bN , (ei, si) = (0, 1) is a Nash equilibrium. Also, if c &lt; pL &lt; bN (expected losses above insurance costs), then (ei, si) = (0, 1), is a Nash equilibrium.</p><p>• Passivity eq.: If pL &lt; bN and pL &lt; c, then the expected losses are small enough so that complete passivity, defined by (ei, si) = (0, 0) for all players, is a Nash equilibrium.</p><p>Increasing number of players N: As the number of players increases, protection equilibria become more and more unlikely to occur. Indeed, in a total effort scenario, "revenues" yielded by a player's investment in security have to be shared with all of the other participants, making it an increasingly uninteresting strategy for the player as the network grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weakest-link</head><p>Let e0 = mini(ei). From Eqn. (3), we have Ui = M -pL(1si)(1 -e0) -bei -csi, so that ∂U i ∂s i = pL(1 -e0) -c, and, for all i,</p><formula xml:id="formula_14">Ui ≤ M -pL(1 -si)(1 -e0) -be0 -csi ,</formula><p>which is reached for ei = e0. So, in a Nash equilibrium, everybody picks the same ei = e0. It follows that Nash equilibria are of the form (e0, 0) or (0, 1).</p><p>Result 2: In the weakest link security game, we can identify three types of Nash equilibrium strategies. However, there exist multiple pure protection equilibria.</p><p>Denote by ê0 the minimum of the protection levels initially chosen by all players. We have</p><formula xml:id="formula_15">• Multiple protection equilibria: If pL &gt; b and {(ê0 &gt; (pL - c)/(pL -b) for c &lt; pL) ∪ (pL ≥ c)}, then (ei, si) = (ê0, 0</formula><p>) for all i is a Nash equilibrium: everybody picks the same minimal security level, but no one has any incentive to lower it further down. This equilibrium can only exist for b ≤ c, and may be inefficient, as it could be in the best interest of all parties to converge to ei = 1, as we discuss later in Section 5.</p><p>• Full self-insurance eq.: If pL &gt; c and {ê0 &lt; (pL-c)/(pLb) ∪ b &gt; pL}, then (ei, si) = (0, 1) for all i is a Nash equilibrium: essentially, if the system is not initially secured well enough (by having all parties above a fixed level), players prefer to self-insure.</p><p>• Passivity eq.: If pL &lt; b and pL &lt; c, then (ei, si) = (0, 0) is the only Nash equilibrium -both insurance and protection are too expensive.</p><p>Notice that if ê0 = (pL -c)/(pL -b), then both full selfinsurance ((ei, si) = (0, 1) for all i) and protection ((ei, si) = (ê0, 0) for all i) form a Nash equilibrium. In particular, if b = c (b &lt; pL and c &lt; pl) full protection (ei, si) = (1, 0) and full self-insurance (ei, si) = (0, 1) are Nash strategies.</p><p>Increasing number of players N: The weakest link security game, much like the tacit coordination game of <ref type="bibr" target="#b42">[42]</ref> has highly volatile protection equilibria when the number of players increase. In fact, any protection equilibrium has to contend with the strategic certainty of a self-insurance equilibrium. To view this, consider the cumulative distribution function F (ei) over the protection strategies ei of a given player i. From what precedes, with pure strategies, in the Pareto-optimum, F (1) = 1 and F (ei) = 0 for ei &lt; 1. Assuming all N players use the same c.d.f. F , then the c.d.f. of e0 = mini{ei} is given by Fmin(e0) = 1 -(1 -F (e0)) N <ref type="bibr" target="#b42">[42]</ref>. So, Fmin(1) = 1 and Fmin(e0) = 0 for e0 &lt; 1 as well. Now, assume there is an arbitrarily small probability ε &gt; 0 that one player will defect, that is F (0) = ε. Then, Fmin(0) converges quickly to 1 as N grows large. That is, it only takes the slightest rumor that one player may defect for the whole game to collapse to the (ei, si) = (0, 1) equilibrium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Best shot</head><p>Let e * = maxi(ei). Eqn. <ref type="bibr" target="#b4">(4)</ref> gives</p><formula xml:id="formula_16">Ui = M -pL(1 -si)(1 -e * ) -bei -csi .</formula><p>Clearly, (ei, si) = (1, 1) is suboptimal, so that three strategies may yield the highest payoff to user i.</p><p>• Selecting (ei, si) = (0, 0) yields Ui = M -pL(1 -e * ).</p><p>• Selecting (ei, si) = (1, 0) yields Ui = M -b.</p><p>• Selecting (ei, si) = (0, 1) yields Ui = M -c.</p><p>Result 3: From the above relationships, we can identify the following pure Nash equilibrium strategies.</p><p>• Full self-insurance eq.: If b &gt; c we find that the self-insurance equilibrium (∀i, (ei, si) = (0, 1)) is the only possible Nash equilibrium.</p><p>• Passivity eq.: If pL &lt; b and pL &lt; c agents prefer to abstain from security actions (∀i, (ei, si) = (0, 0)).</p><p>In particular, there is no protection equilibrium in this game. For one protection equilibrium to exist, we would need b &lt; c and pL &gt; b. But even assuming that this is the case, as long as the game is synchronized, players endlessly oscillate between securing as much as possible (ei = 1) and free-riding (ei = 0). This is due to the fact that as soon as one player secures, all others have an incentive to free-ride. Conversely, if everybody free-rides, all players have an incentive to deviate and secure as much as possible.</p><p>Increasing number of players N: In the absence of coordination between players, the outcome of this game is globally independent of the number of players N , as there is no protection equilibrium, and the insurance equilibrium is independent of the number of players. However, the game may be stabilized by using player coordination (e.g., side payments) for low values of N , something harder to do as N grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Weakest-target (without mitigation)</head><p>Fix the strategy point and let ε &lt; pL 2b . Let e0 be the minimum effort level of any player. Then no player selects a higher effort than e0 + ε because it dominates all higher effort levels. However, any player at e0 would prefer to switch to e0 + 2ε. Then the change in her payoff is greater than pL -2 pL 2b b = 0. Because this deviation is profitable this strategy point is not an equilibrium. <ref type="foot" target="#foot_2">3</ref>Result 4: In the weakest-target game with an attacker of infinite strength we find that pure Nash equilibria for non trivial values of b, p, L and c do not exist.</p><p>Mixed strategy equilibria. While no pure Nash equilibria exist, let us explore the existence of a mixed strategy equilibrium. We use the shorthand notation ei = e, si = s here, and consider mixed strategies for choosing e. There are two cases to consider.</p><p>Case c &gt; pL: If c &gt; pL then dominance arguments immediately lead to s = 0 meaning that nobody buys any self-insurance.</p><p>An equilibrium strategy may be parametrized by e. For a given player, the utility function U becomes a function of a single variable e. Let f (e) be the probability distribution function of effort in the weakest-target game and let F (e) be the cumulative distribution function of effort. Assuming only one player is at the minimum protection level, shall an attack occur, the probability of being the victim is then (1 -F (e)) N -1 . (All N players choose protection levels greater than e.)</p><p>Then the utility is given by</p><formula xml:id="formula_17">U = M -pL(1 -F (e)) N -1 -be .<label>(12)</label></formula><p>In a Nash equilibrium, the first-order condition dU /de = 0 must hold, so that:</p><formula xml:id="formula_18">(N -1)pLf (e)[1 -F (e)] N -2 -b = 0</formula><p>If we substitute G = (1 -F (e)) and g = -f we can write G N -2 dG/de = -b/p(N -1)L, which, by integration yields</p><formula xml:id="formula_19">Z G(0) G(e) G N -2 dG = Z 0 e -b p(N -1)L dê ,</formula><p>that is</p><formula xml:id="formula_20">G N -1 ˛G(0) G(e) = -b pL e .<label>(13)</label></formula><p>With G(0) = 1,</p><formula xml:id="formula_21">G(e) = " 1 - b pL e « 1 N -1</formula><p>.</p><p>Differentiating, we get</p><formula xml:id="formula_22">g(e) = - 1 N -1 b pL " 1 - b pL e « -N -2 N -1</formula><p>, and, replacing g = -f we find,</p><formula xml:id="formula_23">f (e) = 1 N -1 b pL " 1 - b pL e « -N -2 N -1 ,<label>(14)</label></formula><p>as the probability distribution function of self-protection in a mixed Nash equilibrium.</p><p>Case c ≤ pL: Now let us consider a game with insurance under the more reasonable assumption c ≤ pL; that is, insurance is not overpriced compared to expected losses. Dominance arguments indicate that a Nash strategy must be of the form (e, s) ∈ {(e, 0), e ≥ 0} ∪ {(0, 1)}.</p><p>Let q be the probability that a player chooses strategy (e, s) = (0, 1). That is, F (0) = q. Because insurance is independent of protection, we can reuse Eqn. <ref type="bibr" target="#b13">(13)</ref> with the new boundary</p><formula xml:id="formula_24">G(0) = 1 -q: G(e) = " (1 -q) N -1 - b pL e « 1 N -1<label>(15)</label></formula><p>However, since we are now including self-insurance, a second condition must hold. The payoff for strategy (e, s) = (0, 1) must equal the payoff for all other strategies. Specifically, we may compare payoffs for strategies (e, s) = (ε, 0) and (e, s) = (0, 1) which gives, by continuity as ε → 0,</p><formula xml:id="formula_25">pL(1 -q) N -1 = c .<label>(16)</label></formula><p>Together Eqs. ( <ref type="formula" target="#formula_24">15</ref>) and ( <ref type="formula" target="#formula_25">16</ref>) yield:</p><formula xml:id="formula_26">F (e) = 1 -G(e) = 1 - " c -be pL « 1 N -1</formula><p>, which, differentiating, gives</p><formula xml:id="formula_27">f (e) = 1 N -1 b pL " c -be pL « 1 N -1 -1 . (<label>17</label></formula><formula xml:id="formula_28">)</formula><p>This allows us to compute how often strategy (e, s) = (0, 1) is played:</p><formula xml:id="formula_29">q = F (0) = 1 - " c pL « 1 N -1 . (<label>18</label></formula><formula xml:id="formula_30">)</formula><p>Result 5: In the weakest-target game with an attacker of infinite strength, a mixed Nash equilibrium strategy exists. The individual's strategy is given by Eqs. ( <ref type="formula" target="#formula_27">17</ref>) and <ref type="bibr" target="#b18">(18)</ref>. Also note that, per Eqn. ( <ref type="formula" target="#formula_27">17</ref>) and continuity arguments, the upper bound for protection effort is given by emax = c/b, which can be less than 1 when protection costs dominate insurance costs b &gt; c.</p><p>Increasing number of players N: From Eqn. <ref type="bibr" target="#b18">(18)</ref>, we can directly infer that an increase in the number of participating players decreases the probability that a full self-insurance strategy is chosen. When N grows large, q tends to zero, which means that players increasingly prefer to gamble in order to find a protection level that leaves them unharmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Weakest target (with mitigation)</head><p>Let us assume that there exists a Nash equilibrium where 0 &lt; K &lt; N players who satisfy ei = e0 = min(ei, e-i), while (N -K &gt; 0) players satisfy ei &gt; e0. We can show that such an equilibrium does not exist and that players rather congregate at the highest protection level if certain conditions are met. Due to space constraints, we will only sketch the analysis of this equilibrium. By computing the partial derivatives ∂Ui/∂si and ∂Ui/∂ei, and discriminating among values for ei and si, we get the following results.</p><p>Result 6: In contrast to the infinite strength weakest-target game we find that a pure Nash equilibrium may exist.</p><p>• Full protection eq.: If b ≤ c we find that the full protection equilibrium (∀i, (ei, si) = (1, 0)) is the only possible pure Nash equilibrium.</p><p>• For b &gt; c we can show that no pure Nash equilibrium exists.</p><p>• There are no pure self-insurance equilibria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed strategy equilibrium</head><p>To complement this analysis we also present the mixed strategy equilibrium. The derivation is similar to the one given by Eqs. <ref type="bibr" target="#b12">(12)</ref><ref type="bibr" target="#b13">(13)</ref><ref type="bibr" target="#b14">(14)</ref><ref type="bibr" target="#b15">(15)</ref><ref type="bibr" target="#b16">(16)</ref><ref type="bibr" target="#b17">(17)</ref><ref type="bibr" target="#b18">(18)</ref>, however, with an additional substitution step. This gives the resulting distribution,</p><formula xml:id="formula_31">F (e) = 1 - " c -be pL(1 -e) « 1 N -1 ,<label>(19)</label></formula><p>so that</p><formula xml:id="formula_32">f (e) = 1 N -1 " (b -c)pL pL 2 (1 -e) 2 « " c -be pL(1 -e) « -N -2 N -1 .</formula><p>Interestingly, the probability of playing (e, s) = (0, 1) remains</p><formula xml:id="formula_33">q = F (0) = 1 - " c pL « 1 N -1<label>(20)</label></formula><p>Note that if c &lt; b there is a zero probability that e = 1 will be chosen by any player. The upper bound for protection effort is given by emax = c/b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result 7:</head><p>In the weakest-target game with an attacker of finite strength we find that a mixed Nash equilibrium strategy exists. The relevant equations are given in Eqs. <ref type="bibr" target="#b19">(19)</ref><ref type="bibr" target="#b20">(20)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">IDENTIFICATION OF SOCIAL OPTIMA</head><p>Organizations and public policy actors frequently attempt to identify policies that provide the highest utility for the largest number of people. This idea has been operationalized with the social optimum analysis. It states that a system has reached the optimum when the sum of all players' utilities is maximized. That is, the social optimum is defined by the set of strategies that maximize P i Ui. Consider N players, and denote by Φ(e1, s1, . . . , eN , sN ) the aggregate utility, Φ(e1, s1, . . . , eN , sN ) = P i Ui(ei, si). The social optimum maximizes Φ(si, ei) over all possible (si, ei) ∈ [0, 1] 2N . Because enforcing a social optimum may at times be conflicting with the optimal strategy for a given (set of) individual(s), to enforce a social optimum in practice, we may need to assume the existence of a "social planner" who essentially decides, unopposed, the strategy each player has to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Total effort game</head><p>Summing the utility given by Eqn. ( <ref type="formula">2</ref>) over i, we realize that Φ((ei, si) i∈{1,...,N } ) can be expressed as a function of two variables, E = P i ei and S = P i si. Φ is continuous and twice differentiable in E and S, and the second derivative test tells us that the only possible extrema of Φ are reached for the boundary values of E and S, that is (E, S) ∈ {0, N } 2 . In other words, the only possible social optima are 1) passivity (for all i, (ei, si) = (0, 0)), 2) full protection (for all i, (ei, si) = (1, 0)), or 3) full insurance (for all i, (ei, si) = (0, 1)). As long as one of b or c is strictly positive, a social planner will never advise agents to invest into protection and self-insurance at the same time.</p><p>By comparing the values of Φ in all three cases, we find that if b &lt; pL and b &lt; c then all agents are required to exercise maximum protection effort (ei, si) = (1, 0). With c &lt; pL and c &lt; b all agents will self-insure at the maximum possible (ei, si) = (0, 1). A social planner will not encourage players to invest in security measures if they are too expensive (c &gt; pL and b &gt; pL).</p><p>Result 8: In the total effort security game we observe that in the Nash equilibrium there is almost always too little protection effort exerted compared to the social optimum. In fact, for a wide range of parameter settings no protection equilibria exist while the social optimum prescribes protection at a very low threshold.</p><p>• Protection: Except for very unbalanced parameter settings (i.e., pL &gt; bN and c &gt; b + pL N -1 N ) agents refrained from full protection. Now full protection by all agents is a viable alternative.</p><p>• Self-insurance: Full self-insurance now has to compete with full protection effort under a wider range of parameters.</p><p>• Passivity: Agents remain passive if self-insurance is too expensive (c &gt; pL). However, we find a substantial difference with respect to protection behavior. Agents would selfishly refrain from protection efforts if pL &lt; bN since they would only be guaranteed the N -th part of their investments as returns. Now the social planner can ensure that all agents protect equally so that it is beneficial to protect up until b &lt; pL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Weakest link game</head><p>In the weakest link game agents are required to protect at a common effort level to be socially efficient. We compute Φ by summing Eqn. (3) over i, and can express Φ as a function of ei, si and e0 = mini(ei). In particular, for all i, we obtain ∂Φ/∂si = pL(1 -e0) -c. Studying the sign of ∂Φ/∂si as a function of e0 tells us that, if b &lt; c and b &lt; pL the social planner requires all agents to protect with maximum effort (ei, si) = (1, 0). If c &lt; b and c &lt; pL the social planner requires all agents to self-insure (ei, si) = (0, 1). Finally, the Nash equilibrium and social optimum coincide when security costs are high. Agents do not invest in protection or self-insurance if b &gt; pL or c &gt; pL.</p><p>Result 9: The availability of self-insurance lowers the risk of below-optimal security in the Nash equilibrium since agents have an alternative to the unstable Pareto-optimal protection equilibrium. From the analysis of the weakest link game with many agents we know that deviation from the Pareto-optimal highest protection level is very likely. A social planner can overcome these coordination problems.</p><p>• Protection: The Pareto-optimal Nash equilibrium coincides with socially optimal protection. However, the protection level would likely be lower in the Nash case due to coordination problems.</p><p>• Self-insurance: The self-insurance equilibria are equivalent for the Nash and social optimum analysis.</p><p>• Passivity: A social planner cannot expand the range of parameter values at which it would be socially beneficial to protect or self-insure while passivity would be prescribed in the Nash equilibrium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Best shot game</head><p>We compute the social optimum by summing Ui given in Eqn. (4) over i, yielding that Φ can be expressed as a function of ei, si, and e * . It is immediate that, to maximize Φ, one should pick ei = 0 for all i, except for one participant j, where ej = e * ≥ 0. We then get ∂P hi/∂si = pL(1 -e * ) -c, which tells us under which conditions on e * (and consequently on b, c, and pL) self-insurance is desirable.</p><p>We find that if b/c &lt; N (i.e., protection is not at a prohibitive cost compared to insurance and/or there is a reasonably large number of players), the social optimum is to have one player protect as much as possible, the others not protect at all, and no one insures. In practice, this may describe a situation where all participants are safely protected behind an extremely secure firewall. If, on the other hand b/c &gt; N , which means there are either few players, insurance is very cheap compared to protection, then the best strategy is to simply insure all players as much as possible.</p><p>Result 10: In the best shot security Nash outcome there is almost always too little effort exerted compared to the social optimum. Exceptions are few points in which full self-insurance remains desirable for the social planner and all agents remain passive.</p><p>• Protection: Surprisingly, while protection is not even a Nash strategy we find that a social planner would elect an individual to exercise full protection effort.</p><p>• Self-insurance: Full self-insurance by every player is only desirable if protection costs are large. Therefore, for most cases the strategy of a social planner will not coincide with the only Nash equilibrium strategy.</p><p>• Passivity: In the Nash equilibrium agents are also too inactive. Passivity is highly undesirable from a social planner's perspective. Only if N pL &lt; b no agent will be selected to exercise maximum protection effort (while self-insurance might remain an option).</p><p>It is important to note that the social optimum variation that requires full protection by one individual results in the whole population being unharmed, since one highly secure individual is enough to thwart all attacks. Therefore, it is easy to see that protection is extremely desirable from a planners perspective. Out of the three classical public goods games with homogeneous agents the best shot game can benefit the most from a guiding hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Weakest-target security game (without mitigation)</head><p>We compute the social optimum by using Eqn. <ref type="bibr" target="#b8">(8)</ref>, assuming that 1 ≤ K ≤ N players pick e0 = mini(ei). By studying the variations on Φ as a function as ei, as a function of K, and as a function of si (for both the K players picking e0 and the remainder of the players), we find that in the weakest-target game without mitigation a social planner would direct a single player to exacerbate no protection effort.</p><p>Essentially, this player serves as a direct target for a potential attacker. However, as long as c &lt; pL the player would be directed to maximize self-insurance (ei, si) = (0, 1). If insurance is too expensive (c &gt; pL) then the social planner would prefer to leave the player uninsured (ei, si) = (0, 0). This strategy is independent of the cost of protection. The remaining N -1 players have to select their protection effort as ei = ε &gt; 0 (as small as possible). These players will not be attacked, and therefore will set their selfinsurance to the possible minimum (ε, 0). Passivity by all players is never an option in the social optimum.</p><p>Result 11: A social planner can easily devise a strategy to overcome the coordination problems observed in the Nash analysis for the weakest-target game with mitigation. We found that no pure Nash strategy exists and, therefore, had to rely on the increased rationality requirement for entities to play a mixed strategy. 4 The average payoff for each player in the social optimum is considerably higher compared to the mixed Nash equilibrium.</p><p>Understandably, without side-payments the node with the lowest protection effort is worse off compared to his peers. However, the social planner could choose to devise a so-called "honeypot" system with the sole goal of attracting the attacker while only suffering a marginal loss. A honeypot is a computer system (or another device) that is explicitly designed to attract and to be compromised by attackers. It serves usually a double purpose. First, it will detract attention from more valuable targets on the same network. Second, if carefully monitored it allows gathering of information about attacker strategies and behaviors, e.g., early warnings about new attack and exploitation trends <ref type="bibr" target="#b30">[30]</ref>.</p><p>An interesting aspect of the social optimum solution is the question how the individual is selected (if a honeypot system cannot be devised). Obviously, a social planner might be able to direct an individual to serve as a target (in particular, if c &lt; pL). However, if insurance costs are large being a target requires an almost certain sacrifice (dependent on the value of p). In anthropology and economics there are several theories that relate to an individuals willingness to serve as a sacrificial lamb. Most prominently, altruism and heroism come to mind. Simon also introduced the concept of docility. This theory refers to an individual's willingness to be taught or to defer to the superior knowledge of others <ref type="bibr" target="#b39">[39]</ref>. 4 Economists are generally cautious regarding the assumption that individuals can detect and adequately respond to mixed strategy play by opponents <ref type="bibr" target="#b36">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Weakest-target security game (with mitigation)</head><p>We adopt the same strategy for finding Φ's maximum as in the unmitigated case -that is, summing Eqn. ( <ref type="formula" target="#formula_5">6</ref>) over i, and then studying the variations of Φ over K, si and e0.</p><p>The first observation is that the social planner might prescribe the same strategy as in the case of the weakest-target game without mitigation. However, now the planner has a second alternative. Since an attacker will not be able to compromise players if they are fully protected we find that (ei, si) = (1, 0) for all N players is a feasible strategy. The tipping point between the two strategies is at N b &lt; c. If this condition holds the social planner would elect to protect all machines in favor of offering one node as honeypot and investing in its self-insurance. Note that again we find that if protection and self-insurance are extremely costly the planner will elect to sacrifice one entity without insurance. Passivity is not a preferable option.</p><p>Result 12: Compared to the weakest-target game without mitigation the social planner is better off if protection is cheap. Otherwise the planner has to sacrifice a node with or without self-insurance. Interestingly, while compared to the pure Nash equilibrium outcome the social planner can increase the overall utility in the network we find that security expenditures are lowered. In the Nash equilibrium agents were willing to fully protect against threats as long as (b ≤ c).</p><p>The last observation also holds for the mixed strategy case in both weakest-target games (with or without mitigation). That is, agents exert more effort in the Nash equilibrium (except when N b &lt; c for the game with mitigation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION OF RESULTS</head><p>The results we obtained, and notably the disconnect between social optima and Nash equilibria we observed, lead to a number of remarks that may prove relevant to organizational strategy. However, we want to preface this discussion by pointing out that our analysis is a first comparison of different security games with two security options under common, but restrictive assumptions.</p><p>Most notably, we assume agents to be risk-neutral providers of the public protection good. In our game formulation we also simplified cost of protection (and insurance) to be linear. Including different risk preferences, as well as uncertainty and limited information about important parameters of the game would be important steps towards a sensitivity analysis of our results. Shogren found, for example, that risk-averse agents will increase their contributions if information about other agents actions is suppressed <ref type="bibr" target="#b38">[38]</ref>. Others, e.g., <ref type="bibr" target="#b34">[34]</ref>, have obtained more nuanced results. We defer a more extensive analysis of such phenomena to future work, but believe that the main trends and differentiating features between security games we observed remain largely unchanged.</p><p>Security scenario identification: We find that security predictions vary widely between the five different games. Similarly, policies set by a social planner do not only yield different contribution levels but may also switch the recommended security action from protection to self-insurance and vice versa. Chief Security Officers' tasks involve a careful assessment of threat models the company is faced with.</p><p>We want to emphasize that an integral part of the threat model should be an assessment of the organizational structure including system resources and employees. Similarly important is a detailed consideration whether resources are protected independently or by an overarching system policy. For example, replication, redundancy and failover systems (that automatically switch to a standby database, server or network if the primary system fails or is temporarily shut down for servicing) should most likely not be treated as independent resources.</p><p>Managers should consider how the organizational structure of resources matches potentially existing policies. For example, we can see that a policy that requires full protection by every individual is sub-optimal if the most likely threat and organizational structure fits the description of a best shot game. Contributions resources are squandered and are likely to deteriorate. Not to mention that employees may simply ignore the policy over time. See, for example, recent survey results that highlight that 35% of white-collar employees admit to violations of security policies <ref type="bibr" target="#b25">[25]</ref>.</p><p>Security scenario selection: A security professional might be faced with a unidentifiable organization and system-policy structure. However, we want to highlight that our research allows a more careful choice between security options if managers can redesign organizations and policies. For example, the choice between a system-wide firewall and intrusion detection system versus an individual alternative has important implications on how incentives drive security-relevant behavior over time. Individual systems will better preserve incentives, however, might have negative cost implications. The same choice applies between the availability of backup tools and protective measures.</p><p>Leveraging strategic uncertainty: The example of the weakesttarget game shows the importance of the degree of dependency between agents. We show that in larger organizations a much lower average level of self-insurance investments will be achieved because the strategic dependence between actors is reduced. However, in turn more agents will elect to protect their resources (ei &gt; 0 for more players). contrast, agents in small groups will respond to the increasing strategic uncertainty caused by the increased interdependency by self-insuring their resources more often.</p><p>Introducing a social planner into the weakest-target game completely removes strategic uncertainty and leads to both reduced selfinsurance and protection investments. This apparent paradox emphasizes that higher security investments do not necessarily translate in higher security -but instead that how the investments are made are crucial to the returns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>We consider the problem of decision-making with respect to information security investments. To that effect, we model security interactions through a careful selection of games, some established (weakest-link, best-shot, and total effort) and some novel (weakesttarget, with or without mitigation). All of these games offer players two independent decision parameters: a protection level, e, which determines the level of security a player chooses for his resources; and a self-insurance level, s, which mitigates losses, shall a successful attack occur. We postulate that the five games considered cover a vast majority of practical security situations, and study them both from a rational agent's perspective (Nash equilibrium analysis) and from a central planner's view (social optimum analysis).</p><p>Our main findings are that the effects of central planning compared to laissez-faire considerably differ according to the game considered. While in a number of traditional cases borrowed from the public good literature, we observe that a central planner may increase the average protection level of the network, we also note that strategic decisions are highly impacted by the level of interdependency between the actions of different players.</p><p>In particular, we found that the common wisdom that having a central planner who decides upon security implementation always yields higher protection contributions by individual players does not hold. Indeed, it may at times be much more advantageous from an economic standpoint to invest in self-insurance instead of protecting systems, or to select a few, unprotected, sacrificial lambs in order to divert the attention of potential attackers. This is particularly the case in situations which exhibit a "strategic uncertainty" due to a very strong correlation between the actions of different agents, for instance, in our weakest-target game where the least secure player is always the one attacked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Future research directions</head><p>The work presented here opens a number of avenues for future research. First, we have looked at homogeneous populations of users, where all participants have the same utility function. In practice, this homogeneity assumption is reasonable in a number of important cases, particularly when dealing with very large systems where a large majority of the population have the same aspirations. For instance, most Internet home users are expected to have vastly similar expectations and identical technological resources at their disposal; likewise, modern distributed systems, e.g., peer-to-peer or sensor networks generally treat their larger user base as equals. It will be nevertheless prudent to study whether considering heterogeneous user populations impacts the results we obtained here and in what way. Varian <ref type="bibr" target="#b43">[43]</ref>, for instance, evidences important asymmetric user behaviors due to heterogeneity in his reliability games. We also plan to extend our work to explore the impact of fines and liability rules on security investments <ref type="bibr">[9,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>Second, this paper assumes that execution of a player's strategy is always perfect, and that all players are perfectly rational. As has been discussed elsewhere, e.g., <ref type="bibr" target="#b12">[12]</ref>, this assumption generally leads to idealized models, which deserve to be complemented by empirical studies. In that respect, we are currently developing a set of laboratory experiments to conduct user studies and attempt to measure the differences between perfectly rational behavior and actual strategies played. Our preliminary investigations in the field notably evidence that players often experiment with different strategies to try to gain a better understanding of the game they are playing.</p><p>Reconciling observed experimental behavior and theoretical analysis with the design of meaningful security policies is a very challenging goal. We do hope that the present paper will encourage research in this area.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A distinction between reliability and security, in terms of consequences, may exist<ref type="bibr" target="#b24">[24]</ref>. In this study, we do not follow this distinction and consider reliability as a key component of security.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>See, for example, see a recent interview with a security company CEO. New York Times (September 12, 2007), "Who needs hackers," available at http://www.nytimes.com/2007/09/ 12/technology/techspecial/12threat.html. Stating that: "As computer networks are cobbled together [...] the Law of the Weakest Link always seems to prevail."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>While this proof assumes the player is initially at (ei, si) = (e0, 0), it can be trivially extended to the case (ei, si) = (e0, s) with s &gt; 0 by picking ε &lt; pL 2b (1 -s) + cs 2b for any s in (0, 1].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>WWW 2008 / Refereed Track: Internet Monetization -Recommendation &amp; SecurityBeijing, China</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their valuable comments and editorial guidance. Paul Laskowski greatly improved this manuscript with his tremendously helpful feedback. This work is supported in part by the National Science Foundation under ITR award ANI-0331659. Jens Grossklags' research is also partially funded by TRUST (Team for Research in Ubiquitous Secure Technology), under support from the NSF (award CCF-0424422) and the following organizations: BT, Cisco, ESCHER, HP, IBM, iCAST, Intel, Microsoft, ORNL, Pirelli, Qualcomm, Sun, Symantec, Telecom Italia, United Technologies, and AFOSR (#FA 9550-06-1-0244).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Privacy and rationality in individual decision making</title>
		<author>
			<persName><forename type="first">A</forename><surname>Acquisti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grossklags</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security &amp; Privacy</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="33" />
			<date type="published" when="2005-02">January-February 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Free riding on Gnutella. First Monday</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2000-10">Oct. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why cryptosystems fail</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CCS&apos;93</title>
		<meeting>ACM CCS&apos;93<address><addrLine>Fairfax, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-11">Nov. 1993</date>
			<biblScope unit="page" from="215" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Why information security is hard -an economic perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACSAC&apos;01</title>
		<meeting>ACSAC&apos;01<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The economics of information security</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">314</biblScope>
			<biblScope unit="issue">5799</biblScope>
			<biblScope unit="page" from="610" to="613" />
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Online safety study</title>
		<author>
			<persName><surname>Aol/Nsca</surname></persName>
		</author>
		<ptr target="http://www.staysafeonline.org/pdf/safety_study_2005.pdf" />
		<imprint>
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Network software security and user incentives</title>
		<author>
			<persName><forename type="first">T</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tunca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mgmt. Science</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1703" to="1720" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Models and measures for correlation in cyber-insurance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Böhme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kataria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. (online) WEIS&apos;06</title>
		<meeting>(online) WEIS&apos;06<address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward an economic theory of liability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Legal Studies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="323" to="349" />
			<date type="published" when="1973-06">June 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Nearly one in four computer users have lost content to blackouts, viruses and hackers according to new national survey</title>
		<author>
			<persName><forename type="first">Bruskin</forename><surname>Research</surname></persName>
		</author>
		<ptr target="http://www.corporate-ir.net/ireye/ir_site.zhtml?ticker=iom&amp;script=410&amp;layout=-6&amp;item_id=163653" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards security in an open systems federation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sollins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ESORICS&apos;92</title>
		<meeting>ESORICS&apos;92<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS No</publisher>
			<date type="published" when="1992-11">Nov. 1992</date>
			<biblScope unit="volume">648</biblScope>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Near rationality and competitive equilibria in networked systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Christin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grossklags</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM&apos;04 PINS Workshop</title>
		<meeting>ACM SIGCOMM&apos;04 PINS Workshop<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
			<biblScope unit="page" from="213" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tussle in cyberspace: defining tomorrow&apos;s Internet</title>
		<author>
			<persName><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wroclawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sollins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Braden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM&apos;02</title>
		<meeting>ACM SIGCOMM&apos;02<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">Aug. 2002</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The economics of resisting censorship</title>
		<author>
			<persName><forename type="first">G</forename><surname>Danezis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security &amp; Privacy</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="50" />
			<date type="published" when="2005-02">January-February 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Market insurance, self-insurance, and self-protection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Political Economy</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="648" />
			<date type="published" when="1972-07">July 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cooperation and punishment in public goods experiments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gaechter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="980" to="994" />
			<date type="published" when="2000-09">Sept. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An inquiry into the nature and causes of the wealth of Internet miscreants</title>
		<author>
			<persName><forename type="first">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Paxson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CCS&apos;07</title>
		<meeting>ACM CCS&apos;07<address><addrLine>Alexandria, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11">Oct./Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The economics of information security investment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information and System Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="438" to="4572" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The generic virus writer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Virus Bulletin Conf</title>
		<meeting>Intl. Virus Bulletin Conf<address><addrLine>Jersey, Channel Islands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="121" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Virus writers -the end of the innocence?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gordon</surname></persName>
		</author>
		<ptr target="http://www.research.ibm.com/antivirus/SciPapers/VB2000SG.htm" />
	</analytic>
	<monogr>
		<title level="m">10th Annual Virus Bulletin Conference (VB2000)</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-09">Sept. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The tragedy of the commons</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">3859</biblScope>
			<biblScope unit="page" from="1243" to="1248" />
			<date type="published" when="1968-12">Dec. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Returns to information security investment: The effect of alternative information security breach functions on optimal investment and sensitivity to vulnerability</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hausken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems Frontiers</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="338" to="349" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From weakest-link to best-shot: the voluntary provision of public goods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hirshleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Choice</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="386" />
			<date type="published" when="1983-01">Jan. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interdependence of reliability and security</title>
		<author>
			<persName><forename type="first">P</forename><surname>Honeyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Assche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. (online) WEIS&apos;07</title>
		<meeting>(online) WEIS&apos;07<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Telephone survey conducted by MARC Research</title>
		<ptr target="http://biz.yahoo.com/bw/071031/20071031005079.html?.v=1" />
	</analytic>
	<monogr>
		<title level="m">Information Systems Audit and Control Association</title>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interdependent security</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kunreuther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Risk and Uncertainty</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="231" to="249" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The &quot;I Love You&quot; computer virus and the financial services industry</title>
		<author>
			<persName><forename type="first">S</forename><surname>Malphrus</surname></persName>
		</author>
		<ptr target="http://www.federalreserve.gov/BoardDocs/testimony/2000/20000518.htm" />
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inside the Slammer worm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Paxson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staniford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security and Privacy</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="33" to="39" />
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Code-Red: a case study on the spread and victims of an internet worm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/USENIX IMW&apos;02</title>
		<meeting>ACM/USENIX IMW&apos;02<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11">Nov. 2002</date>
			<biblScope unit="page" from="273" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A virtual honeypot framework</title>
		<author>
			<persName><forename type="first">N</forename><surname>Provos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Security&apos;04</title>
		<meeting>USENIX Security&apos;04<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Security holes... who cares?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rescorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Security&apos;03</title>
		<meeting>USENIX Security&apos;03<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08">Aug. 2003</date>
			<biblScope unit="page" from="75" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end arguments in system design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="288" />
			<date type="published" when="1984-11">Nov. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Economics of alliances: The lessons for collective action</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Literature</title>
		<imprint>
			<biblScope unit="volume">XXXIX</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="869" to="896" />
			<date type="published" when="2001-09">Sept. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Free riding and uncertainty</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sterbenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Posnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1605" to="1617" />
			<date type="published" when="1987-12">Dec. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Schelling</surname></persName>
		</author>
		<title level="m">The Strategy of Conflict</title>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Do we detect and exploit mixed strategy play by opponents?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shachat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Swarthout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Methods of Operations Research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Making greed work in networks: A game-theoretic analysis of switch service disciplines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Networking</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="831" />
			<date type="published" when="1995-12">Dec. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On increased risk and the voluntary provision of public goods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shogren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Choice and Welfare</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="229" />
			<date type="published" when="1990-09">Sept. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Altruism and economics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="161" />
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">E-privacy in 2nd generation e-commerce: privacy preferences versus actual behavior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Spiekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grossklags</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berendt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM EC&apos;01</title>
		<meeting>ACM EC&apos;01<address><addrLine>Tampa, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10">Oct. 2001</date>
			<biblScope unit="page" from="38" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Know your enemy: the tools and methodologies of the script-kiddie</title>
		<ptr target="http://project.honeynet.org/papers/enemy/" />
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
		<respStmt>
			<orgName>The Honeynet Project</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tacit coordination games, strategic uncertainty, and coordination failure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Huyck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Battallio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="234" to="248" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">System reliability and free riding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Varian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Economics of Information Security (Advances in Information Security</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Camp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lewis</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
