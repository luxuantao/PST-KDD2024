<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Top-Down Method for Performance Analysis and Counters Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ahmad</forename><surname>Yasin</surname></persName>
							<email>ahmad.yasin@intel.com</email>
						</author>
						<title level="a" type="main">A Top-Down Method for Performance Analysis and Counters Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optimizing an application's performance for a given microarchitecture has become painfully difficult. Increasing microarchitecture complexity, workload diversity, and the unmanageable volume of data produced by performance tools increase the optimization challenges. At the same time resource and time constraints get tougher with recently emerged segments. This further calls for accurate and prompt analysis methods.</p><p>In this paper a Top-Down Analysis is developed -a practical method to quickly identify true bottlenecks in out-oforder processors. The developed method uses designated performance counters in a structured hierarchical approach to quickly and, more importantly, correctly identify dominant performance bottlenecks. The developed method is adopted by multiple in-production tools including VTune. Feedback from VTune average users suggests that the analysis is made easier thanks to the simplified hierarchy which avoids the highlearning curve associated with microarchitecture details. Characterization results of this method are reported for the SPEC CPU2006 benchmarks as well as key enterprise workloads. Field case studies where the method guides software optimization are included, in addition to architectural exploration study for most recent generations of Intel Core™ products.</p><p>The insights from this method guide a proposal for a novel performance counters architecture that can determine the true bottlenecks of a general out-of-order processor. Unlike other approaches, our analysis method is low-cost and already featured in in-production systems -it requires just eight simple new performance events to be added to a traditional PMU. It is comprehensive -no restriction to predefined set of performance issues. It accounts for granular bottlenecks in super-scalar cores, missed by earlier approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The primary aim of performance monitoring units (PMUs) is to enable software developers to effectively tune their workload for maximum performance on a given system. Modern processors expose hundreds of performance events, any of which may or may not relate to the bottlenecks of a particular workload. Confronted with a huge volume of data, it is a challenge to determine the true bottlenecks out of these events. A main contributor to this, is the fact that these performance events were historically defined in an ad-doc bottom-up fashion, where PMU designers attempted to cover key issues via "dedicated miss events" <ref type="bibr" target="#b0">[1]</ref>. Yet, how does one pin-point performance issues that were not explicitly foreseen at design time?</p><p>Bottleneck identification has many applications: computer architects can better understand resource demands of emerging workloads. Workload characterization often uses data of raw event counts. Such unprocessed data may not necessary point to the right bottlenecks the architects should tackle. Compiler writers can determine what Profile Guided Optimization (PGO) suit a workload more effectively and with less overhead. Monitors of virtual systems can improve resource utilization and minimize energy.</p><p>In this paper, we present a Top-Down Analysis -a feasible, fast method that identifies critical bottlenecks in outof-order CPUs. The idea is simple -a structured drill down in a hierarchical manner, guides the user towards the right area to investigate. Weights are assigned to nodes in the tree to guide users to focus their analysis efforts on issues that indeed matter and disregard insignificant issues. For instance, say a given application is significantly hurt by instruction fetch issues; the method categorizes it as Frontend Bound at the uppermost level of the tree. A user/tool is expected to drill down (only) on the Frontend sub-tree of the hierarchy. The drill down is recursively performed until a tree-leaf is reached. A leaf can point to a specific stall of the workload, or it can denote a subset of issues with a common micro-architectural symptom which are likely to limit the application's performance.</p><p>We have featured our method with the Intel 3 rd generation Core™ codenamed Ivy Bridge. Combined with the hierarchical approach, a small set of Top-Down oriented counters are used to overcome bottleneck identification challenges (detailed in next section). Multiple tools have adopted our method including VTune <ref type="bibr" target="#b1">[2]</ref> and an add-on package to the standard Linux perf utility <ref type="bibr" target="#b2">[3]</ref>. Field experience with the method has revealed some performance issues that used to be underestimated by traditional methods. Finally, the insights from this method are used to propose a novel performance counters architecture that can determine the true bottlenecks of general out-of-order architecture, in a top down approach.</p><p>The rest of this paper is organized as follows. Section 2 provides a background and discusses the challenges with bottleneck identification in out-of-order CPUs. The Top-Down Analysis method and its abstracted metrics are introduced in Section 3. In Section 4, novel low-cost counters architecture is proposed to obtain these metrics. Results on popular workloads as well as sample use-cases are presented in Section 5. Related work is discussed in Section 6 and finally, Section 7 concludes and outlines future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Modern high-performance CPUs go to great lengths to keep their execution pipelines busy, applying techniques such as large-window out-of-order execution, predictive speculation, and hardware prefetching. Across a broad range of traditional workloads, these high-performance architectures have been largely successful at executing arbitrary code at a high rate of instructions-per-cycle (IPC). However, with these sophisticated super-scalar out-of-order machines attempting to operate so "close to the edge", even small performance hiccups can limit a workload to perform far below its potential. Unfortunately, identifying true performance limiters from among the many inconsequential issues that can be tolerated by these CPUs has remained an open problem in the field.</p><p>From a bird's eye view, the pipeline of modern out-oforder CPU has two main portions: a frontend and a backend. The frontend is responsible for fetching instructions from memory and translating them into micro-operations (uops). These uops are fed to the backend portion. The backend is responsible to schedule, execute and commit (retire) these uops per original program's order. So as to keep the machine balanced, delivered uops are typically buffered in some "ready-uops-queue" once ready for consumption by the backend. An example block diagram for the Ivy Bridge microarchitecture, with underlying functional units is depicted in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Out-of-order CPU block diagram -Intel Core™</head><p>Traditional methods <ref type="bibr" target="#b3">[4]</ref>[5] do simple estimations of stalls. E.g. the numbers of misses of some cache are multiplied by a pre-defined latency:</p><formula xml:id="formula_0">Stall_Cycles = Σ Penalty i * MissEvent i</formula><p>While this "naïve-approach" might work for an in-order CPU, surely it is not suitable for modern out-of-order CPUs due to numerous reasons: (1) Stalls overlap, where many units work in parallel. E.g. a data cache miss can be handled, while some future instruction is missing the instruction cache. (2) Speculative execution, when CPU follows an incorrect control-path. Events from incorrect path are less critical than those from correct-path. (3) Penalties are workloaddependent, while naïve-approach assumes a fixed penalty for all workloads. E.g. the distance between branches may add to a misprediction cost. ( <ref type="formula">4</ref>) Restriction to a pre-defined set of miss-events, these sophisticated microarchitectures have so many possible hiccups and only the most common subset is covered by dedicated events. ( <ref type="formula">5</ref>) Superscalar inaccuracy, a CPU can issue, execute and retire multiple operations in a cycle. Some (e.g. client) applications become limited by the pipeline's bandwidth as latency is mitigated with more and more techniques.</p><p>We address those gaps as follows. A major category named "Bad Speculation" (defined later) is placed at the top of the hierarchy. It accounts for stalls due to incorrect predictions as well as resources wasted by execution of incorrect paths. Not only does this bring the issue to user's first attention, but it also simplifies requisites from hardware counters used elsewhere in the hierarchy. We introduce a dozen truly Top-Down designated counters to let us deal with other points. We found that determining what pipeline stage to look at and "to count when matters", play a critical role in addressing ( <ref type="formula">1</ref>) and (3). For example, instead of total memory access duration, we examine just the sub-duration when execution units are underutilized as a result of pending memory access. Calling for generic events, not tied to "dedicated miss events" let us deal with (4). Some of these are occupancy events<ref type="foot" target="#foot_1">1</ref> in order to deal with (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Top-Down Analysis</head><p>Top-Down Analysis methodology aims to determine performance bottlenecks correctly and quickly. It guides users to focus on issues that really matter during the performance optimization phase. This phase is typically performed within the time and resources constraints of the overall application development process. Thus, it becomes more important to quickly identify the bottlenecks.</p><p>The approach itself is straightforward: Categorize CPU execution time at a high level first. This step flags (reports high fraction value) some domain(s) for possible investigation. Next, the user can drill down into those flagged domains, and can safely ignore all non-flagged domains. The process is repeated in a hierarchical manner until a specific performance issue is determined or at least a small subset of candidate issues is identified for potential investigation.</p><p>In this section we first overview the hierarchy structure, and then present the heuristics behind the higher levels of the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Hierarchy</head><p>The hierarchy is depicted in Figure <ref type="figure">2</ref>. First, we assume the user has predefined criteria for analysis. For example, a user might choose to look at an application's hotspot where at least 20% of execution time is spent. Another example is to analyze why a given hotspot does not show expected speedup from one hardware generation to another. Hotspot can be a software module, function, loop, or a sequence of instructions across basic blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: The Top-Down Analysis Hierarchy</head><p>Top-Down breakdown is applied to the interesting hotspots where available pipeline slots are split into four basic categories: Retiring, Bad Speculation, Frontend Bound and Backend Bound. These terms are defined in the following subsections. The best way to illustrate this methodology is through an example. Take a workload that is limited by the data cache performance. The method flags Backend Bound, and Frontend Bound will not be flagged. This means the user needs to drill down at the Backend Bound category as next step, leaving alone all Frontend related issues. When drilling down at the Backend, the Memory Bound category would be flagged as the application was assumed cache-sensitive. Similarly, the user can skip looking at non-memory related issues at this point. Next, a drill down inside Memory Bound is performed. L1, L2 and L3-Bound naturally break down the Memory Bound category. Each of them indicates the portion the workload is limited by that cache-level. L1 Bound should be flagged there. Lastly, Loads block due to overlap with earlier stores or cache line split loads might be specific performance issues underneath L1 Bound. The method would eventually recommend the user to focus on this area.</p><p>Note that the hierarchical structure adds a natural safety net when looking at counter values. A value of an inner node should be disregarded unless nodes on the path from the root to that particular node are all flagged. For example, a simple code doing some divide operations on a memory-resident buffer may show high values for both Ext. Memory Bound and Divider nodes in Figure <ref type="figure">2</ref>. Even though the Divider node itself may have high fraction value, it should be ignored assuming the workload is truly memory bound. This is assured as Backend.CoreBound will not be flagged. We refer to this as hierarchical-safety property. Note also that only weights of sibling nodes are comparable. This is due to the fact they are calculated at same pipeline stage. Comparing fractions of nonsibling nodes is not recommended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Top Level breakdown</head><p>There is a need for first-order classification of pipeline activity. Given the highly sophisticated microarchitecture, the first interesting question is how and where to do the first level breakdown? We choose the issue point, marked by the asterisk in Figure <ref type="figure">1</ref>, as it is the natural border that splits the frontend and backend portions of machine. It enables a highly accurate Top-Level classification.</p><p>At issue point we classify each pipeline-slot into one of four base categories: Frontend Bound, Backend Bound, Bad Speculation and Retiring, as illustrated by Figure <ref type="figure">3</ref>. If a uop is issued in a given cycle, it would eventually either get retired or cancelled. Thus it can be attributed to either Retiring or Bad Speculation respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Top Level breakdown flowchart</head><p>Otherwise it can be split into whether there was a backendstall or not. A backend-stall is a backpressure mechanism the Backend asserts upon resource unavailability (e.g. lack of load buffer entries). In such a case we attribute the stall to the Backend, since even if the Frontend was ready with more uops it would not be able to pass them down the pipeline. If there was no backend-stall, it means the Frontend should have delivered some uops while the Backend was ready to accept them; hence we tag it with Frontend Bound. This backendstall condition is a key one as we outline in FetchBubbles definition in next section.</p><p>In fact the classification is done at pipeline slots granularity as a superscalar CPU is capable of issuing multiple uops per cycle. This makes the breakdown very accurate and robust which is a necessity at the hierarchy's top level. This accurate classification distinguishes our method from previous approaches in <ref type="bibr" target="#b0">[1]</ref>[5][6].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Frontend Bound category</head><p>Recall that Frontend denotes the first portion of the pipeline where the branch predictor predicts the next address to fetch, cache lines are fetched, parsed into instructions, and decoded into micro-ops that can be executed later by the Backend. Frontend Bound denotes when the frontend of the CPU undersupplies the backend. That is, the latter would have been willing to accept uops.</p><p>Dealing with Frontend issues is a bit tricky as they occur at the very beginning of the long and buffered pipeline. This means in many cases transient issues will not dominate the actual performance. Hence, it is rather important to dig into this area only when Frontend Bound is flagged at the Top-Level. With that said, we observe in numerous cases the Frontend supply bandwidth can dominate the performance, especially when high IPC applies. This has led to the addition of dedicated units to hide the fetch pipeline latency and sustain required bandwidth. The Loop Stream Detector as well as Decoded I-cache (i.e. DSB, the Decoded-uop Stream Buffer introduced in Sandy Bridge) are a couple examples from Intel Core <ref type="bibr" target="#b6">[7]</ref>.</p><p>Top-Down further distinguishes between latency and bandwidth stalls. An i-cache miss will be classified under Frontend Latency Bound, while inefficiency in the instruction decoders will be classified under Frontend Bandwidth Bound. Ultimately, we would want these to account for only when the rest of pipeline is likely to get impacted, as discussed earlier.</p><p>Note that these metrics are defined in Top-Down approach; Frontend Latency accounts for cases that lead to fetch starvation (the symptom of no uop delivery) regardless of what has caused that. Familiar i-cache and i-TLB misses fit here, but not only these. For example, <ref type="bibr" target="#b3">[4]</ref> has flagged Instruction Length Decoding as a fetch bottleneck. It is CPUspecific, hence not shown in Figure <ref type="figure">2</ref>. Branch Resteers accounts for delays in the shadow of pipeline flushes e.g. due to branch misprediction. It is tightly coupled with Bad Speculation (where we elaborate on misprediction costs).</p><p>The methodology further classifies bandwidth issues per fetch-unit inserting uops to the uops-ready-queue. Instruction Decoders are commonly used to translate mainstream instructions into uops the rest of machine understands -That would be one fetch unit. Also sophisticated instruction, like CPUID, typically have dedicated unit to supply long uop flows. That would be 2 nd fetch unit and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Bad Speculation category</head><p>Bad Speculation reflects slots wasted due to incorrect speculations. These include two portions: slots used to issue uops that do not eventually retire; as well as slots in which the issue pipeline was blocked due to recovery from earlier missspeculations. For example, uops issued in the shadow of a mispredicted branch would be accounted in this category. Note third portion of a misprediction penalty deals with how quick is the fetch from the correct target. This is accounted in Branch Resteers as it may overlap with other frontend stalls.</p><p>Having Bad Speculation category at the Top-Level is a key principle in our Top-Down Analysis. It determines the fraction of the workload under analysis that is affected by incorrect execution paths, which in turn dictates the accuracy of observations listed in other categories. Furthermore, this permits nodes at lower levels to make use of some of the many traditional counters, given that most counters in out-of-order CPUs count speculatively. Hence, a high value in Bad Speculation would be interpreted by the user as a "red flag" that need to be investigated first, before looking at other categories. In other words, assuring Bad Speculation is minor not only improves utilization of the available resources, but also increases confidence in metrics reported throughout the hierarchy.</p><p>The methodology classifies the Bad Speculation slots into Branch Misspredict and Machine Clears. While the former is pretty famous, the latter results in similar symptom where the pipeline is flushed. For example, incorrect data speculation generated Memory Ordering Nukes <ref type="bibr" target="#b6">[7]</ref> -a subset of Machine Clears. We make this distinction as the next steps to analyze these issues can be completely different. The first deals with how to make the program control flow friendlier to the branch predictor, while the latter points to typically unexpected situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Retiring category</head><p>This category reflects slots utilized by "good uops"issued uops that eventually get retired. Ideally, we would want to see all slots attributed to the Retiring category; that is Retiring of 100% corresponds to hitting the maximal uops retired per cycle of the given microarchitecture. For example, assuming one instruction is decoded into one uop, Retiring of 50% means an IPC of 2 was achieved in a four-wide machine . Hence maximizing Retiring increases IPC.</p><p>Nevertheless, a high Retiring value does not necessary mean there is no room for more performance. Microcode sequences such as Floating Point (FP) assists typically hurt performance and can be avoided <ref type="bibr" target="#b6">[7]</ref>. They are isolated under Micro Sequencer metric in order to bring it to user's attention.</p><p>A high Retiring value for non-vectorized code may be a good hint for user to vectorize the code. Doing so essentially lets more operations to be completed by single instruction/uop; hence improve performance. For more details see Matrix-Multiply use-case in Section 5. Since FP performance is of special interest in HPC land, we further breakdown the base retiring category into FP Arithmetic with Scalar and Vector operations distinction. Note that this is an informative field-originated expansion. Other styles of breakdown on the distribution of retired operations may apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Backend Bound category</head><p>Backend Bound reflects slots no uops are being delivered at the issue pipeline, due to lack of required resources for accepting them in the backend. Examples of issues attributed in this category include data-cache misses or stalls due to divider being overloaded.</p><p>Backend Bound is split into Memory Bound and Core Bound. This is achieved by breaking down backend stalls based on execution units' occupation at every cycle. Naturally, in order to sustain a maximum IPC, it is necessary to keep execution units busy. For example, in a four-wide machine, if three or less uops are executed in a steady state of some code, this would prevent it to achieve a optimal IPC of 4. These suboptimal cycles are called ExecutionStalls.</p><p>Memory Bound corresponds to execution stalls related to the memory subsystem. These stalls usually manifest with execution units getting starved after a short while, like in the case of a load missing all caches.</p><p>Core Bound on the other hand, is a bit trickier. Its stalls can manifest either with short execution starvation periods, or with sub-optimal execution ports utilization: A long latency divide operation might serialize execution, while pressure on execution port that serves specific types of uops, might manifest as small number of ports utilized in a cycle. Actual metric calculations is described in Section 4.</p><p>Core Bound issues often can be mitigated with better code generation. E.g., a sequence of dependent arithmetic operations would be classified as Core Bound. A compiler may relieve that with better instruction scheduling. Vectorization can mitigate Core Bound issues as well; as demonstrated in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Memory Bound breakdown (within Backend)</head><p>Modern CPUs implement three levels of cache hierarchy to hide latency of external memory. In the Intel Core case, the first level has a data cache (L1D). L2 is the second level shared instruction and data cache, which is private to each core. L3 is the last level cache, which is shared among sibling cores. We assume hereby a three-cache-level hierarchy with a unified external memory; even though the metrics are genericenough to accommodate other cache-and memoryorganizations, including NUMA.</p><p>To deal with the overlapping artifact, we introduce novel heuristic to determine the actual penalty of memory accesses. A good out-of-order scheduler should be able to hide some of the memory access stalls by keeping the execution units busy with useful uops that do not depends on pending memory accesses. Thus the true penalty for a memory access is when the scheduler has nothing ready to feed the execution units. It is likely that further uops are either waiting for the pending memory access, or depend on other unready uops. Significant ExecutionStalls while no demand-load 2 is missing some cache-level, hints execution is likely limited by up to that level itself. Figure <ref type="figure" target="#fig_0">4</ref> also illustrates how to break ExecutionStalls per cache-level.</p><p>For example, L1D cache often has short latency which is comparable to ALU stalls. Yet in certain scenarios, like load blocked to forward data from earlier store to an overlapping address, a load might suffer high latency while eventually being satisfied by L1D. In such scenario, the in-flight load will last for a long period without missing L1D. Hence, it gets tagged under L1 Bound per flowchart in Figure <ref type="figure" target="#fig_0">4</ref>. Load blocks due to 4K Aliasing <ref type="bibr" target="#b6">[7]</ref> is another scenario with same symptom. Such scenarios of L1 hits and near caches' misses, are not handled by some approaches <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b4">[5]</ref>.</p><p>Note performance hiccups, as the mentioned L1 Bound scenarios, would appear as leaf-nodes in the hierarchy in Figure <ref type="figure">2</ref>. We skipped listing them due to scope limitation. 2 Hardware prefetchers are of special treatment. We disregard them as long as they were able to hide the latency from the demand requests. For the most part they have small impact on performance (as shown in results section); they cannot be completely neglected though. Top-Down defined Stores Bound metric, as fraction of cycles with low execution ports utilization and high number of stores are buffered. In case both load and store issues apply we will prioritize the loads nodes given the mentioned insight.</p><p>Data TLB misses can be categorized under Memory Bound sub-nodes. For example, if a TLB translation is satisfied by L1D, it would be tagged under L1 Bound.</p><p>Lastly, a simplistic heuristic is used to distinguish MEM Bandwidth and MEM Latency under Ext. Memory Bound. We measure occupancy of requests pending on data return from memory controller. Whenever the occupancy exceeds a certain threshold, say 70% of max number of requests the memory controller can serve simultaneously, we flag that as potentially limited by the memory bandwidth. The remainder fraction will be attributed to memory latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Counters Architecture</head><p>This section describes the hardware support required to feature the described Top-Down Analysis. We assume a baseline PMU commonly available in modern CPU (e.g. x86 or ARM). Such a PMU offers a small set of general counters capable of counting performance events. Nearly a dozen of events are sufficient to feature the key nodes of the hierarchy. In fact, only eight designated new events are required. The rest can be found in the PMU already today -these are marked with asterisk in Table <ref type="table" target="#tab_0">1</ref>. For example, TotalSlots event can be calculated with the basic Clockticks event. Additional PMU legacy events may be used to further expand the hierarchy, thanks to the hierarchical-safety property described in Section 3.</p><p>It is noteworthy that a low-cost hardware support is required. The eight new events are easily implementable. They rely on design local signals, possibly masked with a stall indication. Neither at-retirement tagging is required as in IBM POWER5 <ref type="bibr" target="#b5">[6]</ref>, nor complex structures with latency counters as in Accurate CPI Stacks proposals <ref type="bibr" target="#b0">[1]</ref>[8] <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Top-Down Events</head><p>The basic Top-Down generic events are summarized in Table <ref type="table" target="#tab_0">1</ref>. Please refer to Appendix 1 for the Intel implementation of these events. Notice there, an implementation can provide simpler events and yet get fairly good results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Top-Down Metrics</head><p>The events in Table <ref type="table" target="#tab_0">1</ref> can be directly used to calculate the metrics using formulas shown in Table <ref type="table" target="#tab_1">2</ref>. In certain cases, a flavor of the baseline hardware event is used 3 . Italic #-prefixed metric denotes an auxiliary expression. Note ExecutionStall denotes sub-optimal cycles in which no or few uops are executed. A workload is unlikely to hit max IPC in such case. While these thresholds are implementationspecific, our data suggests cycles with 0, 1 or 2 uops executed are well-representing Core Bound scenarios at least for Sandy Bridge-like cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we present Top-Down Analysis results for the SPEC CPU2006 benchmarks in single-thread (1C) and multi-copy (4C) modes with setup described in Table <ref type="table" target="#tab_2">3</ref>. Then, an across-CPUs study demonstrates an architecture exploration use-case. As Frontend Bound tends to be less of a bottleneck in CPU2006, results for key server workloads are included. Lastly, we share a few use-cases where performance issues are tuned using Top-Down Analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">SPEC CPU2006 1C</head><p>At the Top Level, Figure <ref type="figure" target="#fig_2">5a</ref> suggests diverse breakdown of the benchmark's applications. Performance wise, the Retiring category is close to 50% which aligns with aggregate Instruction-Per-Cycle (IPC) of ~1.7 measured for same set of runs. Recall 100% Retiring means four retired uops-per-cycle while for SPEC CPU2006 an instruction is decoded into slightly more than one uop on average. Note how Retiring correlates well with IPC, included to cross-validate with an established metric.</p><p>Overall Backend Bound is dominant. So we drill down into it in next diagrams in Figure <ref type="figure" target="#fig_2">5</ref>. The Backend Level diagram guides the user whether to look at Core or Memory issues next. For example, 456.hmmer is flagged as Backend.CoreBound. Close check of the top hotspots with VTune, indeed points to loops with tight data-dependent arithmetic instructions.  2006 benchmarks in single-thread mode The Integer applications are more sensitive to Frontend Bound and Bad Speculation than the FP applications. This aligns with simulations data using a propriety cycle-accurate simulator, as well as prior analysis by Jaleel <ref type="bibr" target="#b10">[11]</ref>. For example, Jaleel's analysis reported that gcc, perlbench, xalancbmk, gobmk, and sjeng have code footprint bigger than 32KB. They are classified as most Frontend Bound workloads. Note how the breakdown eases to assess the relative significance of bottlenecks should multiple apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">SPEC CPU2006 4C</head><p>Results running 4-copies of these applications are shown in Figure <ref type="figure" target="#fig_4">6</ref>. Top Level shows similarity to 1-copy. At a closer look, some applications do exhibit much increased Backend Bound. These are memory-sensitive applications as suggested by bigger Memory Bound fractions in Figure <ref type="figure" target="#fig_4">6b</ref>. This is expected as L3 cache is "shared" among cores. Since an identical thread is running alone inside each physical core and given CPU2006 has minor i-cache misses, Frontend Bound and Bad Speculation in 4-copy roughly did not changed over 1-copy.  2006 benchmarks in multi-core mode (4-copy) For the less-scalable applications, Memory Bound breakdown points to off-core contention when comparing Figure <ref type="figure" target="#fig_4">6c</ref> to 5c <ref type="foot" target="#foot_3">4</ref> . The key differences occur in applications that are either (I) sensitive to available memory bandwidth, or (II) impacted by shared cache competition between threads. An example of (I) is 470.lbm which is known for its high memory bandwidth requirements <ref type="bibr" target="#b11">[12]</ref>. Its large MEM Bound is the primary change between 1-and 4-copy.</p><p>A key example of (II) is 482.sphinx3. A close look at Memory Bound breakdown indicates the 4-copy sees reduced L3 Bound, and a greatly increased MEM Bound; capacity contention between threads in the shared L3 cache has forced many more L3 misses. This conclusion can be validated by consulting the working-set of this workload <ref type="bibr" target="#b10">[11]</ref>: a single copy demands 8MB (same as LLC capacity) in 1-copy, vs 2MB effective per-core LLC share in 4-copy runs.</p><p>Figure <ref type="figure">7</ref> shows how off-chip resources are utilized for some FP applications, with 1-and 4-copy side-by-side. The bars' height indicates fraction of run time where the memory controller is serving some request. "MEM Bandwidth" is the relative portion where many requests are being serviced simultanously. Note we could plot these metrics at their native local units, thanks to the hierarchical-safety property. We should consider them carefully though.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7: Off-chip comparison of memory-sensitive FP apps</head><p>The available 25GB/s bandwidth clearly satisfies demand of 1-copy. The picture changes in 4-copy in different ways. 435.gromacs, 447.dealII, 454.calculix and 465.tonto now spend more memory cycles due to increase of 1.3-3.6x in L3 misses per-kilo instructions as measured by distinct set of performance counters. Note however, they showed on-par Memory-and Core-Bound stall fractions in Figure <ref type="figure" target="#fig_4">6b</ref>, likely because the out-of-order could mitigate most of these memory cycles. This aligns with measured IPC in range of 1.7-2.3 in 4copy. In contrast, 410.bwaves, 433.milc, 437.leslie3d and 470.lbm become much more MEM Bound in 4-copy per Figure <ref type="figure" target="#fig_4">6c</ref>. Figure <ref type="figure">7</ref> tells us that was due to memory latency in 1-copy which turns into memory bandwidth in 4-copy (4x data demand). Top-Down correctly classifies 470.lbm as MEM Bandwidth limited <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Microarchitectures comparison</head><p>So far we have shown results for the same system. This section demonstrates how Top-Down can assist hardware architects. Figure <ref type="figure" target="#fig_5">8</ref> shows Top Level for Intel Core 3 rd and 4 th generation CPUs, side-by-side for a subset of CPU2006 integer benchmarks. The newer Intel Core has improved frontend where speculative iTLB and i-cache accesses are supported with better timing to improve the benefits of prefetching <ref type="bibr" target="#b6">[7]</ref>. This can be clearly noticed for the benefiting benchmarks with reduction in Frontend Bound. This validation adds to the confidence of underlying heuristics invented two generations earlier.  It is interesting to see that the characterization of DBMS workloads generally conforms to <ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data cache misses and 1 st level i-cache misses a while back. Within the Frontend, Latency issues are dominant across all server workloads. This is due to more i-cache and i-TLB misses as expected there, in contrast to client workloads whose Frontend Bound was almost evenly split between Latency and Bandwidth issues (not shown due to paper scope limitation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Case Study 1: Matrix-Multiply</head><p>A matrix-multiply textbook kernel is analyzed with Top-Down. It demos the iterative nature of performance tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4: Results of tuning Matrix-Multiply case</head><p>The initial code in multiply1() is extremely MEM Bound as big matrices are traversed in cache-unfriendly manner. Loop Interchange optimization, applied in multiply2()gives big speedup. The optimized code continues to be Backend Bound though now it shifts from Memory Bound to become Core Bound.</p><p>Next in multiply3(), Vectorization is attempted as it reduces the port utilization with less net instructions. Another speedup is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Case Study 2: False Sharing</head><p>A university class educates students on multithreading pitfalls through an example to parallelize a serial computebound code. First attempt has no speedup (or, a slowdown) due to False Sharing. False Sharing is a multithreading hiccup, where multiple threads contend on different data-elements mapped into the same cache line. It can be easily avoided by padding to make threads access different lines.</p><p>Table <ref type="table">5</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>: Results of tuning False Sharing case</head><p>The single-thread code has modest IPC. Top-Down correctly classifies the first multithreaded code attempt as Backend.Memory.StoresBound (False Sharing must have one thread writing to memory, i.e. a store, to apply). Stores Bound was eliminated in the fixed multithreaded version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Case Study 3: Software Prefetch</head><p>A customer propriety object-recognition real application is analyzed with Top-Down. The workload is classified as Backend.Memory.ExtMemory.LatencyBound at application scope. Ditto for biggest hotspot function; though the metric fractions are sharper there. This is a symptom of more nonmemory bottlenecks in other hotspots.</p><p>Table <ref type="table">6</ref>: Results of tuning Software Prefetch case Software Prefetches <ref type="bibr" target="#b9">[10]</ref> are planted in the algorithm's critical loop to prefetch data of next iteration. A speedup of 35% per the algorithm-score is achieved, which is translated to 1.21x at workload scope. Note the optimized version shows higher memory-bandwidth utilization and has become more Backend.CoreBound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>The widely-used naïve-approach is adopted by <ref type="bibr" target="#b3">[4]</ref>[5] to name a few. While this might work for in-order CPUs, it is far from being accurate for out-of-order CPUs due to: stalls overlap, speculative misses and workload-dependent penalties as elaborated in Sections 2.</p><p>IBM POWER5 <ref type="bibr" target="#b5">[6]</ref> has dedicated PMU events to aid compute CPI breakdown at retirement (commit) stage. Stall periods with no retirement are counted per type of the next instruction to retire and possibly a miss-event tagged to it. Again this is a predefined set of fixed events picked in a bottom-up way. While a good improvement over naïveapproach, it underestimates frontend misses' cost as they get accounted after the point where the scheduler's queue gets emptied. Levinthal <ref type="bibr" target="#b4">[5]</ref> presents a Cycle Accounting method for earlier Intel Core implementations. A flat breakdown is performed at execution-stage, to decompose total cycles into retired, non-retired and stall components. Decomposition of stall components then uses the inadequate naïve-approach as author himself indicates.</p><p>In contrast, Top-Down does breakdown at issue-stage, at finer granularity (slots) and avoids summing-up all penalties into one flat breakdown. Rather it drills down stalls in a hierarchical manner, where each level zooms into the appropriate portion of the pipeline. Further, designated Top-Down events are utilized; sampling (as opposed to counting) on frontend issues is enabled, as well as breakdown when HT is on. None of these is featured by <ref type="bibr" target="#b4">[5]</ref>.</p><p>Some researchers have attempted to accurately classify performance impacts on out-of-order architectures. Eyerman et al. in <ref type="bibr" target="#b0">[1]</ref>[9] use a simulation-based interval analysis model in order to propose a counter architecture for building accurate CPI stacks. The presented results show improvements over naïve-approach and IBM POWER5 in terms of being closer to the reference simulation-based model. A key drawback of this approach (and its reference model) is that it restricts all stalls to a fixed set of eight predefined miss events. In <ref type="bibr" target="#b0">[1]</ref>[4] <ref type="bibr" target="#b4">[5]</ref> there is no consideration of (fetch) bandwidth issues, and short-latency bottlenecks like L1 Bound. Additionally, high hardware cost is implied due to fairly complex tracking structures as authors themselves later state in <ref type="bibr" target="#b7">[8]</ref>. While <ref type="bibr" target="#b7">[8]</ref> replaces the original structure with smaller FIFO; extra logic is required for penalty calculation and aggregation to new dedicated counters. This is in comparison with the simple events adopted by our method with no additional counters/logic. We have pointed to more drawbacks in previous sections.</p><p>More recently, <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b11">[12]</ref> proposed instrumentationbased tools to analyze data-locality and scalability bottlenecks, respectively. In <ref type="bibr" target="#b12">[13]</ref>, average memory latency is sampled with a PMU and coupled with reuse distance obtained through combination of Pin and a cache simulator, in order to prioritize optimization efforts. An offline analyzer maps these metrics back to source code and enables the user to explore the data in hierarchal manner starting from main function. <ref type="bibr" target="#b11">[12]</ref> presents a method to obtain speedup stacks for a specific type of parallel programs, while accounting for three bottlenecks: cache capacity, external memory bandwidth and synchronization.</p><p>These can be seen as advanced optimization-specific techniques that may be invoked from Top-Down once Backend.MemoryBound is flagged. Furthermore, better metrics based on our MemStalls.L3Miss event e.g. can be used instead of raw latency value in <ref type="bibr" target="#b12">[13]</ref> to quantify when speedup may apply. Examining metrics at higher program scope first, may be applied to our method as already done in VTune's General Exploration view <ref type="bibr" target="#b1">[2]</ref>. While <ref type="bibr" target="#b11">[12]</ref> estimates speedups (our method does not), it accounts for subset of scalability bottlenecks. For example, the case in 5.6 is not be covered by their three bottlenecks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Summary and Future Work</head><p>This paper presented Top-Down Analysis method -a comprehensive, systematic in-production analysis methodology to identify critical performance bottlenecks in out-of-order CPUs. Using designated PMU events in commodity multi-cores, the method adopts a hierarchical classification, enabling the user to zero-in on issues that directly lead to sub-optimal performance. The method was demonstrated to classify critical bottlenecks, across variety of client and server workloads, with multiple microarchitectures' generations, and targeting both single-threaded and multi-core scenarios.</p><p>The insights from this method are used to propose a novel low-cost performance counters architecture that can determine the true bottlenecks of a general out-of-order processor. Only eight simple new events are required.</p><p>The presented method raises few points on PMU architecture and tools front. Breakdown of few levels require multiple events to be collected simultaneously. Some techniques might tolerate this; such as Sandy Bridge's support of up to eight general-purpose counters <ref type="bibr" target="#b9">[10]</ref>, or eventmultiplexing in the tools <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b2">[3]</ref>. Still a better hardware support is desired. Additionally, the ability to pinpoint an identified issue back to the user code can benefit much software developers. While PMU precise mechanisms are a promising direction, some microarchitecture areas are under-covered. Yet, enterprise-class applications impose additional challenges with flat long-tail profiles.</p><p>Correctly classifying bottlenecks in the context of hardware hyper-threading (HT) is definitely a challenging front. While it was beyond the scope of this paper, the design of some Top Down events, does take HT into account, letting the Top Level works when HT is enabled; but that is just the start. Lastly, While the goal of our method was to identify critical bottlenecks, it does not gauge the speedup should underlying issues be fixed. Generally, even to determine whether an issue-fix will be translated into speedup (at all) is tricky. A workload often moves to the next critical bottleneck. <ref type="bibr" target="#b11">[12]</ref> has done nice progress to that end in scalability bottlenecks context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Memory Bound breakdown flowchart So far, load operations of the memory subsystem were treated. Store operations are buffered and executed postretirement (completion) in out-of-order CPUs due to memory ordering requirements of x86 architecture.For the most part they have small impact on performance (as shown in results section); they cannot be completely neglected though. Top-Down defined Stores Bound metric, as fraction of cycles with low execution ports utilization and high number of stores are buffered. In case both load and store issues apply we will prioritize the loads nodes given the mentioned insight.Data TLB misses can be categorized under Memory Bound sub-nodes. For example, if a TLB translation is satisfied by L1D, it would be tagged under L1 Bound.Lastly, a simplistic heuristic is used to distinguish MEM Bandwidth and MEM Latency under Ext. Memory Bound. We measure occupancy of requests pending on data return from memory controller. Whenever the occupancy exceeds a certain threshold, say 70% of max number of requests the memory controller can serve simultaneously, we flag that as potentially limited by the memory bandwidth. The remainder fraction will be attributed to memory latency.</figDesc><graphic url="image-4.png" coords="5,350.92,54.00,185.00,206.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Top-Down Analysis breakdown for SPEC CPU2006 benchmarks in single-thread mode The Integer applications are more sensitive to Frontend Bound and Bad Speculation than the FP applications. This aligns with simulations data using a propriety cycle-accurate simulator, as well as prior analysis by Jaleel<ref type="bibr" target="#b10">[11]</ref>. For example, Jaleel's analysis reported that gcc, perlbench, xalancbmk, gobmk, and sjeng have code footprint bigger than 32KB. They are classified as most Frontend Bound workloads. Note how the breakdown eases to assess the relative significance of bottlenecks should multiple apply.</figDesc><graphic url="image-7.png" coords="7,45.03,312.55,251.98,131.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Top-Down Analysis breakdown for SPEC CPU2006 benchmarks in multi-core mode (4-copy) For the less-scalable applications, Memory Bound breakdown points to off-core contention when comparing Figure6cto 5c4 . The key differences occur in applications that are either (I) sensitive to available memory bandwidth, or (II) impacted by shared cache competition between threads. An example of (I) is 470.lbm which is known for its high memory bandwidth requirements<ref type="bibr" target="#b11">[12]</ref>. Its large MEM Bound is the primary change between 1-and 4-copy.A key example of (II) is 482.sphinx3. A close look at Memory Bound breakdown indicates the 4-copy sees reduced L3 Bound, and a greatly increased MEM Bound; capacity contention between threads in the shared L3 cache has forced many more L3 misses. This conclusion can be validated by consulting the working-set of this workload<ref type="bibr" target="#b10">[11]</ref>: a single copy demands 8MB (same as LLC capacity) in 1-copy, vs 2MB effective per-core LLC share in 4-copy runs.Figure7shows how off-chip resources are utilized for some FP applications, with 1-and 4-copy side-by-side. The</figDesc><graphic url="image-10.png" coords="7,314.92,312.55,251.98,131.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Top Down across-microarchitectures</figDesc><graphic url="image-12.png" coords="8,62.28,590.47,217.37,127.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Top and Frontend levels for server workloadsIt is interesting to see that the characterization of DBMS workloads generally conforms to<ref type="bibr" target="#b3">[4]</ref> who reported these workloads are limited by last-level data cache misses and 1 st level i-cache misses a while back. Within the Frontend, Latency issues are dominant across all server workloads. This is due to more i-cache and i-TLB misses as expected there, in contrast to client workloads whose Frontend Bound was almost evenly split between Latency and Bandwidth issues (not shown due to paper scope limitation).</figDesc><graphic url="image-13.png" coords="8,332.17,130.49,217.16,103.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Definitions of Top-Down performance events</figDesc><table><row><cell>Event</cell><cell>Definition</cell></row><row><cell>TotalSlots*</cell><cell>Total number of issue-pipeline slots.</cell></row><row><cell>SlotsIssued*</cell><cell>Utilized issue-pipeline slots to issue operations</cell></row><row><cell>SlotsRetired*</cell><cell>Utilized issue-pipeline slots to retire (complete)</cell></row><row><cell></cell><cell>operations</cell></row><row><cell>FetchBubbles</cell><cell>Unutilized issue-pipeline slots while there is no</cell></row><row><cell></cell><cell>backend-stall</cell></row><row><cell>RecoveryBubbles</cell><cell>Unutilized issue-pipeline slots due to recovery</cell></row><row><cell></cell><cell>from earlier miss-speculation</cell></row><row><cell>BrMispredRetired*</cell><cell>Retired miss-predicted branch instructions</cell></row><row><cell>MachineClears*</cell><cell>Machine clear events (pipeline is flushed)</cell></row><row><cell>MsSlotsRetired*</cell><cell>Retired pipeline slots supplied by the micro-</cell></row><row><cell></cell><cell>sequencer fetch-unit</cell></row><row><cell>OpsExecuted*</cell><cell>Number of operations executed in a cycle</cell></row><row><cell>MemStalls.AnyLoad</cell><cell>Cycles with no uops executed and at least 1 in-</cell></row><row><cell></cell><cell>flight load that is not completed yet</cell></row><row><cell>MemStalls.L1miss</cell><cell>Cycles with no uops executed and at least 1 in-</cell></row><row><cell></cell><cell>flight load that has missed the L1-cache</cell></row><row><cell>MemStalls.L2miss</cell><cell>Cycles with no uops executed and at least 1 in-</cell></row><row><cell></cell><cell>flight load that has missed the L2-cache</cell></row><row><cell>MemStalls.L3miss</cell><cell>Cycles with no uops executed and at least 1 in-</cell></row><row><cell></cell><cell>flight load that has missed the L3-cache</cell></row><row><cell>MemStalls.Stores</cell><cell>Cycles with few uops executed and no more</cell></row><row><cell></cell><cell>stores can be issued</cell></row><row><cell>ExtMemOutstanding</cell><cell>Number of outstanding requests to the memory</cell></row><row><cell></cell><cell>controller every cycle</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Formulas for Top-Down Metrics</figDesc><table><row><cell>Metric Name</cell><cell>Formula</cell></row><row><cell>Frontend Bound</cell><cell>FetchBubbles / TotalSlots</cell></row><row><cell>Bad Speculation</cell><cell>(SlotsIssued -SlotsRetired + RecoveryBubbles) /</cell></row><row><cell></cell><cell>TotalSlots</cell></row><row><cell>Retiring</cell><cell>SlotsRetired / TotalSlots</cell></row><row><cell>Backend Bound</cell><cell>1 -(Frontend Bound + Bad Speculation + Retiring)</cell></row><row><cell>Fetch Latency</cell><cell>FetchBubbles[≥ #MIW] / Clocks</cell></row><row><cell>Bound</cell><cell></cell></row><row><cell>Fetch Bandwidth</cell><cell>Frontend Bound -Fetch Latency Bound</cell></row><row><cell>Bound</cell><cell></cell></row><row><cell cols="2">#BrMispredFraction BrMispredRetired / (BrMispredRetired +</cell></row><row><cell></cell><cell>MachineClears)</cell></row><row><cell>Branch Mispredicts</cell><cell>#BrMispredFraction * Bad Speculation</cell></row><row><cell>Machine Clears</cell><cell>Bad Speculation -Branch Mispredicts</cell></row></table><note>3 For example, the FetchBubbles[≥ MIW] notation tells to count cycles in which number of fetch bubbles exceed Machine Issue Width (MIW). This capability is called Counter Mask ever available in x86 PMU<ref type="bibr" target="#b9">[10]</ref>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Baseline system setup parameters</figDesc><table><row><cell>Processor</cell><cell>Intel® Core™ i7-3940XM (Ivy Bridge). 3 GHz</cell></row><row><cell></cell><cell>fixed frequency. A quadcore with 8MB L3 cache.</cell></row><row><cell></cell><cell>Hardware prefetchers enabled.</cell></row><row><cell>Memory</cell><cell>8GB DDR3 @1600 MHz</cell></row><row><cell>OS</cell><cell>Windows 8 64-bit</cell></row><row><cell cols="2">Benchmark SPEC CPU 2006 v1.2 (base/rate mode)</cell></row><row><cell>Compiler</cell><cell>Intel Compiler 14 (SSE4.2 ISA)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="978" xml:id="foot_0">-1-4799-3606-9/14/$31.00 ©2014 IEEE Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:11:08 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">An occupancy event is capable to increment by more than 1 in a given cycle when a certain condition is met for multiple entities</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:11:08 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Negative L2 Bound is due to PMU erratum on L1 prefetchers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>The author would like to thank the anonymous reviewers at Intel; Joseph Nuzman and Vish Viswanathan in particular for their insightful comments, Tal Katz for data collection and Mashor Housh for a close technical write-up review.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Appendix 1</head><p>Intel Core™ microarchitecture is a 4-wide issue machine. Table <ref type="table">7</ref> summarizes the metrics implementation using the Ivy Bridge PMU event names. Some of the Top-Down designated events are not directly available in hardware; instead a formula is supplied to approximate metric from available events. Note metrics that do not appear in the table have nothing specific to the Intel implementation and can be used as is from Table <ref type="table">2</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A performance counter architecture for computing accurate CPI components</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intel® VTune TM Amplifier XE 2013</title>
		<imprint/>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The New Linux &apos;perf&apos; tools</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>presented at the Linux Kongress</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DBMSs on a Modern Processor: Where Does Time Go?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Very Large Data Bases</title>
				<meeting>the 25th International Conference on Very Large Data Bases<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="266" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Performance Analysis Guide for Intel Core i7 Processor and Intel® Xeon 5500 processors</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Intel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Mericas</surname></persName>
		</author>
		<author>
			<persName><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bret</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName><surname>Olszewski</surname></persName>
		</author>
		<title level="m">CPI analysis on POWER5, Part 2: Introducing the CPI breakdown model</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Intel® 64 and IA-32 Architectures Optimization Reference Manual</title>
		<imprint/>
	</monogr>
	<note>Intel Corporation. Intel. [Online</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient CPI stack counter architecture for superscalar processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Allam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GLSVLSI</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A top-down approach to architecting CPI component performance counters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="84" to="93" />
			<pubPlace>Micro, IEEE</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Intel® 64 and IA-32 Architectures Software Developer Manuals</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Memory Characterization of Workloads Using Instrumentation-Driven Simulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<ptr target="http://www.jaleels.org/ajaleel/workload/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Profiling Method for Analyzing Scalability Bottlenecks on Multicores</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eklov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pinpointing data locality bottlenecks with low overhead</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Performance Analysis of Systems and Software (ISPASS), 2013 IEEE International Symposium on</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="183" to="193" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
