<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization</title>
				<funder ref="#_PSJeBUa #_7ZQAGnj #_82PtHYh">
					<orgName type="full">National Natural Science Foundation of China</orgName>
					<orgName type="abbreviated">NSFC</orgName>
				</funder>
				<funder ref="#_kXKe7GS">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-30">30 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cong</forename><surname>Guo</surname></persName>
							<email>guocong@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingwen</forename><surname>Leng</surname></persName>
							<email>leng-jw@cs.sjtu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>fanyang@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
							<email>liuyunxin@air.tsinghua.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute for AI Industry Research (AIR)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
							<email>guo-my@cs.sjtu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
							<email>yzhu@rochester.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Cn</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-30">30 Aug 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2208.14286v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Quantization is a technique to reduce the computation and memory cost of DNN models, which are getting increasingly large. Existing quantization solutions use fixed-point integer or floating-point types, which have limited benefits, as both require more bits to maintain the accuracy of original models. On the other hand, variable-length quantization uses low-bit quantization for normal values and high-precision for a fraction of outlier values. Even though this line of work brings algorithmic benefits, it also introduces significant hardware overheads due to variable-length encoding and decoding.</p><p>In this work, we propose a fixed-length adaptive numerical data type called ANT to achieve low-bit quantization with tiny hardware overheads. Our data type ANT leverages two key innovations to exploit the intra-tensor and inter-tensor adaptive opportunities in DNN models. First, we propose a particular data type, flint, that combines the advantages of float and int for adapting to the importance of different values within a tensor. Second, we propose an adaptive framework that selects the best type for each tensor according to its distribution characteristics. We design a unified processing element architecture for ANT and show its ease of integration with existing DNN accelerators. Our design results in 2.8? speedup and 2.5? energy efficiency improvement over the stateof-the-art quantization accelerators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep neural networks (DNNs) have achieved great success in a variety of application domains, including computer vision <ref type="bibr" target="#b20">[21]</ref> and natural language processing <ref type="bibr" target="#b17">[18]</ref>. With the tensor-based computations as the dominant patterns, specialized tensor accelerators have been introduced for DNN inference <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b87">[88]</ref> and training <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b68">[69]</ref>. However, the size of DNN models increases by 240? every two years, significantly exceeding the hardware improvement rate (3.1? every two years) <ref type="bibr" target="#b28">[29]</ref>. For instance, the recent large Transformer-based GPT-3 <ref type="bibr" target="#b4">[5]</ref> model has 175 billion parameters, whose single inference needs to take around 740 TOPs (tera operations).</p><p>Exploiting DNN model sparsity and redundancy through algorithm and hardware co-design is a promising way to overcome the widening computation gap between model and 1 This work started during his internship at Microsoft Research. 2 Jingwen Leng and Minyi Guo are corresponding authors of this paper.</p><p>hardware. There are generally two approaches, model pruning and model quantization, for reducing the computation and memory costs of DNN models. First, pruning away the unimportant elements results in sparse DNN models with reduced parameter counts. For example, NVIDIA has introduced the sparse tensor core since its Ampere architecture <ref type="bibr" target="#b63">[64]</ref>. Second, model quantization uses narrow bit length to represent values to save memory and computation. For example, Google's first generation TPU <ref type="bibr" target="#b45">[46]</ref> uses an 8-bit integer type for inference, while other commercial accelerators use the floating-point types with reduced precisions, such as FP16 <ref type="bibr" target="#b63">[64]</ref>, TF32 <ref type="bibr" target="#b63">[64]</ref>, and BF16 <ref type="bibr" target="#b45">[46]</ref>, for accelerating the DNN training.</p><p>All the above algorithm and hardware co-design works for DNN models leverage the traditional numerical data types such as int and float, and these types are inherently inefficient for DNN models. The reason is that the values in DNNs' tensors have both a non-uniform distribution and non-uniform importance. For instance, many weight tensors in DNNs follow the Gaussian-like distribution, with many values around zero, which, according to many DNN pruning works <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b46">[47]</ref>, are not important and hence can be pruned. However, the float type has the highest resolution (called rigid resolution <ref type="bibr" target="#b51">[52]</ref>) for these small values, wasting its bit length. On the other hand, the Gaussian-like distribution also has a long tail, whose range is critical for the DNN model accuracy <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b85">[86]</ref>. The int type needs a long bit length to represent the large values. Given the above reasons, both int and float data types usually need more bits to maintain the original model accuracy <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b77">[78]</ref>.</p><p>To achieve even higher quantization benefits (i.e., lower bit length), prior works have proposed the outlier-aware quantization method and designed special hardware support <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b85">[86]</ref>. The basic idea is to employ the low precision 4-bit int for the small values with a high appearance frequency and high precision 32/16-bit float or int for large values with an extremely low frequency. However, this method results in variable-length encoding and hence unaligned memory access, which are incompatible with the existing DNN accelerators and require a complex and high-cost hardware design.</p><p>In this work, we present an adaptive numeric data type called ANT, which can adapt to the importance of different value intervals within a tensor (i.e., intra-tensor adaptivity) and the distribution of different tensors (i.e., inter-tensor adaptivity). More importantly, ANT has aligned memory accesses and efficient low-bit computation, leading to large quantization benefits and low hardware overheads. We first propose a novel data type primitive called flint that combines the advantages of float with a large range and int with high precision for important value intervals. We leverage the variable-length first-one coding technique to encode the exponent field. As a result, the overall encoding has a fixed length, which is friendly for hardware decoding.</p><p>We then design a general framework that adapts to different distributions of weight and activation tensors, which include not only the above Gaussian distribution and also Laplace and uniform-like distributions. We build upon the previous works like PoT (i.e., power of two) type <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b93">[94]</ref>, and show how to integrate them into our ANT framework that chooses the best-fit numerical type to minimize the quantization error. All those primitive types have the fixed-length, and a tensor can only have a fixed primitive type. Compared to previous works that only exploit the intra-tensor adaptivity <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b85">[86]</ref> or inter-tensor adaptivity <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b77">[78]</ref>, the ANT framework can achieve both with high hardware efficiency.</p><p>We further propose a unified TypeFusion processing element (PE) design that can handle the case when the input tensor and weight tensor have different primitive types. The TypeFusion PE can be implemented on top of the original float or int multiply-accumulate (MAC) unit. The required modification is a simple type decoder, which decodes different primitive types in ANT to a unified format for computing on the underlying float or int MAC unit.</p><p>The proposed ANT framework targets to solve the problem of the low-bit (i.e., 4-bit) quantization, which could still degrade the original model accuracy. We show that ANT is compatible with mixed-precision quantization <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b94">[95]</ref>. In specific, our 4-bit ANT PE design can naturally support 8-bit int PE with little modification. Owing to the mixed-precision support, ANT can use the 4-bit representation for over 90% tensors and still maintain the same level of accuracy as the original full-precision models.</p><p>We describe how to integrate the above ANT PEs into existing DNN accelerator architectures such as systolic array and tensor core <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b63">[64]</ref>. We present several optimizations to minimize the overhead of using ANT's TypeFusion PE. In particular, we show that ANT only imposes a simple type extension for the multiply-accumulate instruction, leaving the original programming model unmodified. Our evaluation results show that the ANT-based accelerator surpasses the existing mixed-precision accelerator BitFusion <ref type="bibr" target="#b72">[73]</ref> by 2.8? performance improvement and 2.5? energy reduction.</p><p>We make the following contributions in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>This section presents relevant background on DNN quantization, which has been widely studied to reduce the memory and computation cost of DNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantization Metric</head><p>Many prior studies <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b88">[89]</ref> have shown that the optimization target of quantization is to reduce the MSE (Mean Square Error) between the original model and quantized model. In the digital image processing field <ref type="bibr" target="#b44">[45]</ref>, the MSE metric is formally defined by the following equaiton:</p><formula xml:id="formula_0">MSE = E[(x -x) 2 ] = (x -x) 2 p(x) dx,</formula><p>where x and x are the original and quantized value, respectively, and p(x) is the the probability density function.</p><p>To reduce the quantization MSE, it is natural to choose a numeric type whose quantization resolution distribution is similar to the tensor distribution <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b88">[89]</ref>. Many researchers have proposed the distribution-aware quantization to address the non-uniform distribution, e.g., the Huffman encoding <ref type="bibr" target="#b38">[39]</ref> and outlier-aware quantization <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b85">[86]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fixed-length Quantization</head><p>Fixed-length quantization usually uses a narrow bit length representation based on int type or float type. For example, there are works using 4-bit or 8-bit int types (called int4 and int8, respectively) to quantize the weight tensors or activation tensors in DNN models. The floatbased types can be represented by the following equation, with a varying exponent and mantissa filed.</p><formula xml:id="formula_1">Real value = sign ? 2 exponent -bias ? 1.mantissa<label>(1)</label></formula><p>For example, FP16 <ref type="bibr" target="#b47">[48]</ref> uses a 5-bit exponent and 10-bit mantissa (5E10M), while BF16 <ref type="bibr" target="#b45">[46]</ref> and TF32 <ref type="bibr" target="#b63">[64]</ref> use the configuration of 8E7M and 8E10M, respectively. There are also other more aggressive numerical types as follows.</p><p>PoT type <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b93">[94]</ref> (power of two) can be viewed as a special format of float type with only the exponent field and no mantissa field. As a result, it can represent a large value range and its multiplication can be simplified to addition.</p><p>AdaptiveFloat type <ref type="bibr" target="#b77">[78]</ref> extends the basic float type to reduce the quantization errors for tensors with a non-uniform distribution (e.g., Gaussian-like distribution). Its quantization framework adaptively sets the tensor-wise exponent bias to match the Gaussian-like distribution and reduce the MSE. The quantization and dequantization function for a quantized element w can be generalized to the following equation:</p><formula xml:id="formula_2">w = s ? Dequant[Clamp(Quant( w s ), min, max)] (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where s is the quantization scale factor and, min and max are the lower and upper thresholds for the clipping function Clamp(?). The operator Quant represents the quantization function. For example, the quantization function for int is a simple rounding-to-nearest function. After that, the dequantization operator Dequant can decode the quantized number to the original type (e.g., 32-bit float or FP32), which is required for the quantization-aware training <ref type="bibr" target="#b41">[42]</ref>.</p><p>For memory-aligned quantization, we follow the common practice of per-channel weight quantization <ref type="bibr" target="#b60">[61]</ref>, which applies a separate scale factor for each output channel without additional hardware overhead. For the input activations, we use the per-tensor scale factors because the per-channel activation quantization is challenging to implement <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b60">[61]</ref>. These quantization granularities are widely used and supported by the DNN quantization frameworks such as TensorRT <ref type="bibr" target="#b62">[63]</ref>. Moreover, we use the unsigned type to quantize activation tensors after ReLU efficiently <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b77">[78]</ref>, as its outputs are all non-negative. However, note that our framework supports both the signed numerical types and unsigned numerical types as we describe later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mixed-precision Quantization</head><p>Mixed-precision DNN quantization method uses different numbers of bits for a given data type to represent values in DNN tensors. Many works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b94">[95]</ref> have shown that the mixedprecision method is efficient for quantizing DNN layers that have different importance and sensitiveness for the bit length.</p><p>The most widely used approach is tensor-wise mixedprecision, such as Bit Fusion <ref type="bibr" target="#b72">[73]</ref> and NVIDIA's latest tensor core <ref type="bibr" target="#b63">[64]</ref>. In the tensor-wise quantization, all elements in each tensor use the same fixed-length numerical type. Thus, the tensor-wise mixed-precision is hardware-friendly without incurring much overhead, which has two kinds of implementation, i.e., temporal or spatial mixed-precision. For example, a temporal design needs four cycles to perform an</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-18 activation tensor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-Base weight tensor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-Base activation tensor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Int PoT</head><p>Uniform-like Gaussian-like Laplace-like</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wide range High precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-tensor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-tensor</head><p>Figure <ref type="figure" target="#fig_3">1</ref>: Intra-tensor and inter-tensor adaptivity.</p><p>8-bit ? 8-bit multiplication using a 4-bit PE <ref type="bibr" target="#b75">[76]</ref>, while a spatial design needs only one cycle using four 4-bit PEs <ref type="bibr" target="#b72">[73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Outlier-aware Quantization</head><p>Outlier-aware quantization (OLAccel) <ref type="bibr" target="#b65">[66]</ref> is tailored for Gaussian (non-uniform) distribution, which is common in DNN models. It divides the values in a tensor into two regions, i.e., outliers and non-outlier (or normal) values. The outlier with a low probability can be represented by high precision (such as FP32 or FP16), and normal values with a high probability can be compressed with fewer bits. GOBO <ref type="bibr" target="#b85">[86]</ref> is similar to OLAccel but has fewer outliers. However, they exploit variable-length data encoding, which leads to the non-alignment in the memory sub-system. As a result, these kinds of design increase the hardware complexity and have a non-negligible area overhead as we would show later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTIVATION: ADAPTIVE DATA TYPE</head><p>In this section, we first analyze the distributions of values in weight tensors and activation tensors from existing DNN models. Prior works have proposed the accelerator microarchitecture with adaptive bit length to achieve low-bit quantization <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b85">[86]</ref>, which requires a significant amount of hardware resources to deal with the variable length. In contrast, our work proposes the idea of adaptive numerical data type (ANT) to fulfill the potential of low-bit quantization. We present a qualitative framework to demonstrate the advantage of ANT, which is extremely hardware-friendly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Opportunities for Adaptive Type</head><p>We identify two opportunities: the inter-tensor adaptivity to the specific distribution of each tensor and the intra-tensor adaptivity to the importance of values in each tensor. These two opportunities lay the foundation for constructing the hardware-friendly adaptive quantization scheme.</p><p>We first analyze the diversity of value distributions for various tensors in popular DNN models. Most previous works focus on Gaussian-based distribution and propose different techniques to mitigate the accuracy loss of quantization <ref type="bibr" target="#b51">[52]</ref>,  [66], <ref type="bibr" target="#b85">[86]</ref>. However, tensors in DNN models exhibit different distributions as shown in Fig. <ref type="figure" target="#fig_3">1</ref>. For example, the activation tensor in the first layer of ResNet18 <ref type="bibr" target="#b40">[41]</ref> is closer to a uniform distribution, while an activation tensor in BERT <ref type="bibr" target="#b21">[22]</ref> has a long tail that is close to Laplace distribution <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Inter-tensor Adaptivity Given the diverse distribution of various tensors in DNN models, there naturally exists the opportunity called inter-tensor adaptivity. Intuitively, inspired by the non-uniform quantization of digital image processing <ref type="bibr" target="#b44">[45]</ref>, if we adaptively choose the most suitable numerical type to quantize a tensor according to its distribution, it may achieve a lower quantization error, e.g., MSE (mean square error). For instance, in the left part of Fig. <ref type="figure" target="#fig_3">1</ref>, the four-bit int type would lead to a smaller quantization error than the four-bit float type for the uniform-like distribution with a narrow range. In contrast, the PoT type, which can represent a large dynamic range under the same bit length, is more suitable than int and float for the Laplace-like distribution with a long tail, as shown in the right part.</p><p>Intra-tensor Adaptivity Complementary to the inter-tensor adaptivity, the opportunity to reduce the quantization error also exists within a tensor. Specifically, extremely small and large values in a tensor do not require a high precision. First, the key premise of DNN model pruning is that <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b46">[47]</ref> small close-to-zero values are less important so can be pruned. Second, many quantization works have shown that large values can be clipped to a threshold <ref type="bibr" target="#b14">[15]</ref>, which, however, should be large enough. In other words, the exact numerical value of extremely large values is unimportant too as long as its rough numerical range is captured. To exploit such an intra-tensor adaptivity opportunity, the quantization scheme should allocate fewer precisions for very small and large values while capturing a large enough range.</p><p>In the next part, we show that existing works cannot exploit the aforementioned opportunities, leading to marginal quantization benefits or significant hardware overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantization Architecture Analysis</head><p>We first present a unified qualitative framework in Fig. <ref type="figure" target="#fig_1">2</ref> to analyze the hardware overhead introduced by previous quantization works. We consider the three main components in the baseline DNN accelerator, which include the offchip memory, on-chip memory, and computation unit. For example, Google's TPU architecture <ref type="bibr" target="#b45">[46]</ref> has the off-chip HBM, large unified on-chip buffers, and weight-stationarybased systolic array as the computation unit. On top of the baseline architecture, a quantization scheme would generally introduce three extra components, which are off-chip data decoder, on-chip data decoder, and compute controller. We analyze five state-of-the-art quantization schemes, which are int, AdaptiveFloat <ref type="bibr" target="#b77">[78]</ref>, BitFusion <ref type="bibr" target="#b72">[73]</ref>, BiScaled <ref type="bibr" target="#b42">[43]</ref>, OLAccel <ref type="bibr" target="#b65">[66]</ref>, and GOBO <ref type="bibr" target="#b85">[86]</ref>. Tbl. I compares their area overhead and quantization benefits with the metrics including averaged off-chip data width, averaged on-chip data width, and averaged compute data width. For a fair comparison, we collect the statistics from over ten models including CNNs <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b76">[77]</ref>, ViT (vision transformer) <ref type="bibr" target="#b24">[25]</ref>, and BERT <ref type="bibr" target="#b21">[22]</ref>. We report numbers for all schemes when all models are close to their original FP32 accuracy (CNN with &lt; 0.1% loss and Transformer with &lt; 1% loss) except BiScaled. We take the results from the BiScaled paper <ref type="bibr" target="#b42">[43]</ref>. Sec. VII provides more experimental details.</p><p>The conventional int-based quantization stores tensors with the same bit width in both off-chip and on-chip memory, making them all access aligned. Thus, it does not require any additional decoder logics and compute controller (i.e., its area overhead is zero). On the other hand, it does not exploit the inter-and intra-tensor adaptivity. A low-bit int can only represent a narrow range, which may clip away few but important large values <ref type="bibr" target="#b14">[15]</ref>. As such, it often requires 8-bit int type to retain original model's accuracy. Hence its quantization benefits are limited to 8 bit for off-chip memory, on-chip memory, and computation resources.</p><p>AdaptiveFloat <ref type="bibr" target="#b77">[78]</ref> (shorted as AdaFloat) extends the float type with a tensor-wise exponent bias. It has aligned off-chip and on-chip memory accesses but requires an exponent bias decoder for controlling the bias offset, whose area is 14.5% larger than the fixed-point (int). Although floating-point allows AdaFloat to represent a greater value range, it gradually increases quantization resolution as values' magnitude decreases logarithmically, leading to excessively high resolution (called rigid resolution <ref type="bibr" target="#b51">[52]</ref>) for much smaller values. Because smaller values are usually less important, the rigid resolution wastes much numerical representation space, rendering its overall quantization benefits to 8 bits. BitFusion <ref type="bibr" target="#b72">[73]</ref> exploits the inter-tensor adaptivity by choosing different bit lengths (or precisions) for different tensors. It incurs an almost zero hardware overhead because the high precision data type (e.g., 8-bit int) can reuse all the low-precision data type (e.g., 4-bit int) components without extra overhead. However, its underlying primitive data type is still int type, which limits its quantization benefits to 7.07 memory bits and computation bits on average.</p><p>OLAccel <ref type="bibr" target="#b65">[66]</ref> and GOBO <ref type="bibr" target="#b85">[86]</ref> leverage the outlier-aware quantization scheme. They store a tensor using a variablelength compressed form (e.g., 4 bits for normal values and 16 bits for outliers with relatively large values) in the offchip memory, which requires a dedicated data decoder. They also require an additional outlier controller to orchestrate the computation between normal values and outliers. Note that GOBO <ref type="bibr" target="#b85">[86]</ref> only supports weight quantization, it requires high precision (i.e., 16-bit) floating-point computations for activation tensors. Even though both designs have a large quantization benefit with low memory bits, their associated hardware complexity and overhead are also significant.</p><p>BiScaled <ref type="bibr" target="#b42">[43]</ref> also adopts outlier-aware quantization but with a fixed-length compressed form. However, it requires an extra bit mask for indicating different scale factors, which leads to a more considerable area overhead. Moreover, it only considers two value ranges for the intra-tensor adaptivity, leading to the benefits of 6.16 memory and computation bits.</p><p>To balance the hardware overhead and quantization benefits, we propose the adaptive numerical data type (ANT) framework that exploits both inter-tensor and intra-tensor adaptivity. ANT further supports mixed-precision. As the last row in Tbl. I shows, ANT achieves the lowest average bit for memory and computation for both activation and weight tensors with a negligible area overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ADAPTIVE NUMERIC DATA TYPE</head><p>In this section, we present our adaptive numeric data type ANT that can exploit both the intra-and inter-tensor adaptive opportunities in a hardware-friendly fashion. We first present a novel primitive data type called flint that combines the advantages of float and int for adapting to the importance of different values within a tensor. We then propose a general framework that adapts to each tensor's distribution by selecting different primitive types, including int, float, flint, and PoT. As a result, ANT has aligned memory accesses and efficient low-bit computation, translating to significant benefits with low hardware overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Intra-tensor ANT: Flint</head><p>Main Idea As shown in Sec. III-A, extremely small and large values in a tensor do not need high precision. A naive approach is to divide a value range into multiple intervals and assign fewer bits to intervals with small or large values. However, this leads to variable-length for different elements in the tensor and requires expensive hardware logic to handle the incurred unaligned accesses as mentioned in Sec. III-B.</p><p>To provide a fixed-length data type overcome while exploiting intra-tensor adaptivity, we propose a new primitive data type called flint. Our main idea is to start with a fixedlength, and allocate fewer mantissa bits (i.e., more exponent bits) to extremely small and large values (as they do not require a high precision) while allocating more mantissa bits (i.e., fewer exponent bits) to middle-range values to preserve their precision. Using more exponents bits for large values also allows us to capture the range of very large values.</p><p>To mark the boundary between the exponent and mantissa field, we use the first appearance of bit '1' after the most significant bit (or sign bit). We call this encoding first-one encoding. While other strategies exist to split the exponent and mantissa fields, this encoding has the critical advantage of simplicity: the decoder for this encoding only requires a simple leading zero detector as we would show later.</p><p>An Example We use the example of four-bit flint in Fig. <ref type="figure" target="#fig_2">3</ref> to illustrate our design. Without loss of generality, we assume the case of unsigned values that have been scaled with the per-channel (or per-tensor) granularity for the weight (activation) tensor as described in Sec. II-B.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> left shows a four-bit unsigned binary number using our flint encoding, which can represent 16 distinctive binary values with the maximum value of 64. We divide this value range to eight intervals corresponding into the eight columns in Fig. <ref type="figure" target="#fig_2">3</ref>    fields of 0000 2 , 0001 2 , 001 2 , and 01 2 . Under the four-bit fixed-length encoding, the number of mantissa bits for these intervals are 0, 0, 1, and 2, respectively. This mantissa bit allocation scheme is adaptive to the value importance as the first two intervals are closer to zero and hence have the least number of precisions. The exponent-mantissa bit allocation for the last four intervals is inverse to the first four intervals.</p><formula xml:id="formula_4">- 0 0 0001 1 -1 = 0 1 2 0 ? 1 = 1 001x 2 -1 = 1 1, 1.5 2, 3 01xx 3 -1 = 2 1, 1.</formula><p>In specific, the greatest interval 1000 2 has no mantissa bit, which is also desirable because the range is more important than the precision for large values. Tbl. II shows the value table for the above 4-bit unsigned flint with the exponent bias of -1. Each row refers to the divided interval (i.e., column) in Fig. <ref type="figure" target="#fig_2">3</ref> left. The equivalent exponent value for a given flint encoding is the interval number plus the bias. The final decimal value equals the fraction value multiplied by the exponent value raised by the power of two as in Equation <ref type="formula" target="#formula_1">1</ref>. For example, the flint encoded number 1110 2 has the exponent value of 4 -1 = 3 and mantissa bit 10 2 , which corresponds to the fraction value of 1.5. As such, its decimal value is 2 3 ? 1.5 = 12 <ref type="bibr" target="#b9">10</ref> .</p><p>Essentially, our proposed flint is a mixture of int, float (and its variants), and PoT at different intervals. The first four intervals (i.e., rows) in Tbl. II have the binary encoding of 0000 2 , 0001 2 , ..., 0111 2 and represent the integer value of 0, 1, . . . , 7, respectively. In other words, the four-bit flint type is equivalent to int in the first four intervals. The 5th and 6th intervals have the 2 and 1 mantissa bits, making them equivalent to the float with 2 and 1 mantissa bits (i.e., 2 and 3 exponent bits), respectively. The last two intervals have zero mantissa bits, which are equivalent to the PoT type.</p><p>Given these above insights, we call the proposed type flint, which is able to combine the advantages of float and int. Note that in the right of Fig. <ref type="figure" target="#fig_2">3</ref>, the divided eight intervals can be coalesced into four intervals according to their typeequivalence. We can see that the mantissa precision allocation in flint highly matches the Gaussian distribution, meaning values with a higher frequency also with more mantissa bits.  As most tensors in DNN models are Gaussian-like, we show later that this behavior leads to fewer quantization errors.</p><p>Flint Encoding Algorithm To recover the accuracy loss, it is generally required to perform the fine-tuning with the quantization in the training loop. As such, flint needs an encoding algorithm to convert the original high precision values, such as FP32, to the low precision flint. The software can use this encoding algorithm to mimic the flint behavior during fine-tuning. Meanwhile, as we target both weight and activation quantization, the encoding needs to be performed dynamically during inference, which requires a lightweight and hardware-efficient encoding algorithm.</p><p>Algo. 1 details the hardware-efficient flint encoding algorithm for each tensor element. First, a b-bit flint number has 2 ? b possible first-one codes for exponents (Line 2), so its value interval is [0, 2 2b-2 ]. We first use int quantization with the scale factor s to quantize the input value e to its integer value with the value range [0, 2 2b-2 ] (Line 3). We then calculate its value interval index according to the interval boundary in Tbl. II (Line 7), and derive the exponent and mantissa filed correspondingly (Line 8 -12).</p><p>For example, the 4-bit unsigned flint type has the value range of [0, 2 2?4-2 = 64] (Line 2). For a decimal number 11 <ref type="bibr" target="#b9">10</ref> , it has been quantized by the int quantization with the scale factor s (Line 3). We then calculate its value interval index i = 4 (Line 7), for which the encoded exponent is 11 2 (Line 8). After deriving the exponent field, we know its mantissa bit-width is mb = 4 -2 = 2 (Line 9), with value of m = (11/8 -1) ? 2 2 = 1.5 10 and rounded to m = 2 10 (Line 10). This binary code of 2 10 is 10 2 (Line 11), which is concatenated with exponent exp to get the final flint encoded number q = 1110 2 (Line 12). Note that after the  The above flint encoding algorithm is an element-wise function that can be implemented efficiently in both hardware and software. The exponent and mantissa bit settings are constants when the quantization bit-width is given. The quantization of weight tensors can be done offline, while activation quantization needs hardware support. Owing to the simplicity of our encoding algorithm, we can implement it in the hardware by augmenting the hardware's element-wise computation unit, such as the activation unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inter-tensor ANT</head><p>To exploit the inter-tensor adaptivity and balance the hardware complexity, we propose to select the data type for a tensor according to its distribution. As we have shown previously, flint is equivalent to int, float, and PoT in certain value intervals. ANT is natural to support these four primitive data types for inter-tensor adaptivity.</p><p>As previously shown in the right of Fig. <ref type="figure" target="#fig_3">1</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>, the int is most suitable for the uniform-like distribution. The float or PoT are most suitable for Laplace-like distributions. The flint data type is most suitable for Gaussian distribution because flint has the highest resolution (most mantissa bits) for the values with the highest frequency. In our work, we propose an automatic algorithm to determine the data type for tensors in a trained DNN model that we describe later. Meanwhile, it is hardware-efficient to support the above four data types. Even with different data types, a tensor is stored in a fix-length format. As such, the memory accesses of ANT are aligned and hence efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ANT-based Quantization Framework</head><p>To apply ANT for quantizing DNN models, we first need to select the specific type for a given tensor since ANT contains multiple primitive data types. After that, we then perform  the fine-tuning to recover the accuracy loss. We describe the details in each step and explain how to use the ANT-quantized model for inference in the end.</p><p>Type Selection Algo. 2 shows the type selection algorithm for ANT, which chooses the primitive data type with minimum mean squared error (MSE) out of the candidate list L (e.g., flint/int/float/PoT). We first get the quantization function for each candidate type (Line 3-4). The flint encoding algorithm is described previously in Sec. IV-A, and the de-quantization algorithm can be derived by inverting the process. We use the original quantization function for the other primitive types. For a given data type, we also need to determine its range (i.e., the scaling factor). We employ a widely-used range clipping method <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref> that determines the clipping range by minimizing the MSE (Line 5). We then determine the most suitable data type for each tensor with minimum MSE from the candidate list (Line 6-7).</p><p>We only execute the above type selection algorithm once per tensor before fine-tuning. The reason is that the distribution of tensors of a well-trained model remains roughly similar even during the fine-tuning stage <ref type="bibr" target="#b1">[2]</ref>, which has been exploited by many other quantization methods <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b72">[73]</ref>. For the weight tensor quantization, we do not require any training samples and directly use the weight tensors from the original, trained DNN models to determine each weight tensor's data type. For the activation tensor quantization, we need about 100 training samples to collect the statistical information for determining the types.</p><p>Mixed Precision ANT is also compatible with the mixedprecision quantization method to achieve the same level of accuracy as the original DNN model. We leverage a layerwise precision selection method <ref type="bibr" target="#b72">[73]</ref>. In the beginning, we use the 4-bit ANT type for all layers and perform fine-tuning. We then collect and sort the MSE of all layers in descending order. We enlarge the bit width of a layer with the greatest MSE to 8 bits and then perform another fine-tuning. We repeat the above process until the accuracy of the quantized model is within the preset threshold of the original model. is two-fold. First, the accumulation in these layers needs to maintain a high precision <ref type="bibr" target="#b41">[42]</ref>. Second, their following layers are usually activation layers such as SoftMax and GeLU, which also require high-precision numbers <ref type="bibr" target="#b86">[87]</ref>. The output tensors of activation layers can be quantized to low-bit values, which can be completed in the hardware by augmenting the activation units (or their equivalence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANT-based Inference</head><p>V. TYPE-FUSION PROCESSING ELEMENT The ANT data type introduces unique challenges for the design of the processing element because the PE now needs to handle different primitive types (flint/int/ float/PoT). Moreover, the input activation tensor and weight tensor for the same layer may have different data types. To address these challenges, we propose the TypeFusion processing element architecture that supports the multiply-accumulate (MAC) operation between different primitive types. We describe the two cases where we build TypeFusion PE on top of the original float-based PE and int-based PE, respectively. For convenience, we simplify the description below with the focus on unsigned numbers and it is straightforward to adapt the described design to support signed numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Float-based PE</head><p>We first describe how to augment the original floatbased MAC unit to support int, PoT, and flint. As we have described previously in Sec. IV-C, the accumulation needs to be performed in high precision, which is sufficient to cover the ranges of low-precision ANT. As such, we focus on how to augment the multiplication component.</p><p>Multiplier For the int and PoT, we can regard them as two special float formats. Int has no exponent and is full of mantissa with the subnormal number. PoT has no mantissa and is full of exponent bits with extreme dynamic range. For each type, we need to identify its exponent bitlength and mantissa bit-length and send them to exponent and mantissa decoders, respectively. Therefore, we need a float multiplier with an n-bit exponent and an n-bit mantissa for n-bit int and PoT. Meanwhile, those exponent and mantissa bits are sufficient for the n-bit flint, which is equivalent to float, int, and PoT in different value intervals.</p><p>Decoder To decode int (PoT) to float, we can set the exponent (mantissa) to zero and copy all bits to mantissa (exponent). The decoding of flint is more complicated because its decoding is value-dependent. Thus, we design an efficient float-based flint decoder to address this issue.</p><p>The 4-bit float-based unsigned flint design is illustrated in Fig. <ref type="figure" target="#fig_8">5</ref>, and an arbitrary n-bit flint decoder can be designed in a similar way. The decoder uses a leading-zero detector (LZD) <ref type="bibr" target="#b64">[65]</ref> and shifters, which are well-known hardware components and both have lightweight implementations. We use the following equations to extract the exponent and mantissa field from flint type:</p><formula xml:id="formula_5">Exponent = 3 -LZD(b 2 b 1 b 0 ), b 3 = 0 4 + LZD(b 2 b 1 b 0 ), b 3 = 1 ,<label>(3)</label></formula><formula xml:id="formula_6">Mantissa = b 2 b 1 b 0 &lt;&lt; (LZD(b 2 b 1 b 0 ) + 1)<label>(4)</label></formula><p>where the LZD is the leading zero number function and &lt;&lt; represents left shift. We can decode flint to the original exponent and mantissa. Finally, as shown in Tbl. II, the float decoder will continue to transfer them to real values. For example, a flint number 1110 2 is 12 10 . Its exponent is 4+LZD(110) = 4. Its mantissa is 110 &lt;&lt; (0+1) = 100 2 = 0.5 10 . Therefore, 1110 2 is 2 4-1 ? 1.5 = 12 10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Integer-based PE</head><p>For DNN inference, it is more common to use int-based PE which is simpler and more area efficient than floatbased PE. Because of the incompatibility between int and float, we remove the latter from the ANT primitive data types, which now include flint, int, and PoT. To support ANT on the integer-based PE, we first introduce a unified representation that is based on two int values and its corresponding decoder design. We then present the lightweight modification of the original int MAC to support other primitive types in ANT such as flint and PoT. Decoder For a given integer i, we decompose it to a base integer b i and an exponent integer e, such that i = b i &lt;&lt; e. Tbl. III shows such a decomposition for 4-bit flint type. When the most significant bit (MSB) of flint is 0, the base integer value matches the value in int format and the exponent is zero. When the most significant bit is 1, the base integer value matches the value of remaining bits in int format left-shifted by one, and the exponent can be derived by using the leading-zero detection function/logic.  The following equations describe the base integer and exponent integer for a f lint number x = b 3 b 2 b 1 b 0 . Fig. <ref type="figure" target="#fig_9">6</ref> illustrates the corresponding decoder design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Integer</head><formula xml:id="formula_7">= ? ? ? b 2 b 1 b 0 , b 3 = 0 b 2 b 1 b 0 &lt;&lt; 1 , b 3 = 1 1 , x = 1000 2 ,<label>(5)</label></formula><formula xml:id="formula_8">Exponent = 0 , b 3 = 0 2 ? LZD(b 2 b 1 b 0 ) , b 3 = 1 ,<label>(6)</label></formula><p>This representation also works for int and PoT. The int type has a zero exponent value, while the PoT type has the base integer of one and the exponent value from its binary. In summary, the flint type expands the value range if int type by using a simple left shifter instead of the complicated hardware logics in float type. It can be decoded to two integer numbers instead of a fraction number. Combined with a proper scale factor, the flint can be coupled with int quantization without extra overhead. The decoder for flint with an arbitrary bit-width can be generated in a similar way. The sign bit can be easily combined with the decoded numbers to fit the int multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiplier and Accumulator</head><p>The decoded flint is not directly compatible with the original int-based MAC because of the extra exponent and shift operations. Therefore, we need an adder and shifter for flint computation shown in Fig. <ref type="figure" target="#fig_10">7</ref>. Assume we have two flint numbers, f a and f b , with exponent e a and e b and base integer i a and i b , respectively. The integer multiplication is the same as original int, including the sign bit, i.e., i c = i a ? i b . The exponent need an add operation e c = e a + e b . Then, we get the final result i d = i c &lt;&lt; e c , which can be represented by a 16-bit int number. As we have explained in Sec. IV-C, low-bit int MAC usually adopts a high precision accumulator to achieve the precise accumulation results <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b63">[64]</ref>. The flint type produces a 16-bit int result and is compatible with the original 16-bit accumulator with i f = i e + i d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Signed Number Support</head><p>The flint decoder function and hardware design are generally extensible for arbitrary bit-width with specific constant settings. In particular, we show that the signed  number decoder can reuse most of the components in the unsigned number decoder without affecting its critical path.</p><p>For example, assume that we have a 4-bit signed flint number. The most significant bit b 3 is the sign, and the last three bits are b 2 b 1 b 0 . For int-based flint, the following equations are the base integer and exponent decoder for 3-bit flint. Obviously, we can easily reuse the 4-bit unsigned flint decoder function shown in Equation ( <ref type="formula" target="#formula_7">5</ref>) and ( <ref type="formula" target="#formula_8">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Integer</head><formula xml:id="formula_9">= ? ? ? b 1 b 0 , b 2 = 0 b 1 b 0 &lt;&lt; 1 , b 2 = 1 1 , x = 100 2 ,<label>(7)</label></formula><formula xml:id="formula_10">Exponent = 0 , b 2 = 0 2 ? LZD(b 1 b 0 ) , b 2 = 1 ,<label>(8)</label></formula><p>To maintain the compatibility with the signed integer MAC unit, we need to convert the base integer to its two's complement form as shown in Fig. <ref type="figure" target="#fig_9">6</ref>. However, this conversion process does not affect the critical path of the unsigned flint decoder as the critical path still lies in the leading zero detector unit. For the float-based flint, we can attach the sign bit to the decoded exponent and mantissa based on the original unsigned float-based decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Mixed-precision Support</head><p>In this work, we propose to couple our ANT with the mixedprecision quantization to achieve the same accuracy of the original high-precision DNN models. According to many prior works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b94">[95]</ref>, the 8-bit int is sufficient to maintain the original model accuracy. We explain how our 4-bit ANT PE design can naturally support 8-bit int PE.</p><p>Fig. <ref type="figure" target="#fig_12">8</ref> shows how to use four 4-bit ANT PEs to multiply two 8-bit int numbers. First, we decode the two 8-bit numbers &lt; a, b &gt; and &lt; c, d &gt; to four numbers in our base integer and exponent representation, which are &lt; a, 4 &gt;, &lt; b, 0 &gt;, &lt; c, 4 &gt;, and &lt; d, 0 &gt;. Then, we perform four parallel multiplication for those four numbers, as illustrated in Fig. <ref type="figure" target="#fig_12">8</ref>, each using a 4-bit ANT PE. Finally, we sum the results of four multiplication using an extra adder tree. In summary, our ANT PE is a good fit for supporting the mixedprecision DNN inference. In the later evaluation, we show </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8-bit int</head><p>Exp. that most tensors (up to 91%) would use 4-bit ANT while only a fraction of tensors would use 8-bit int.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-bit 4-bit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ARCHITECTURE INTEGRATION</head><p>In this section, we describe how to integrate the aforementioned ANT processing element into existing DNN accelerator architectures such as systolic array and tensor core <ref type="bibr" target="#b63">[64]</ref>. We present our optimizations to minimize the overhead of using ANT's TypeFusion PE. In the end, we describe the convenience of extending the instruction set for our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ANT and Dataflow Co-design</head><p>We first describe the architectural optimizations for applying ANT to the systolic array, which is also adopted by commercial DNN accelerators like Google's TPU <ref type="bibr" target="#b45">[46]</ref>. As we have explained in Sec. IV-C, our design follows the common practice in which the input and weight tensor have low-bit quantization while the output tensor has high-bit quantization. As such, we find that our design achieves the best benefits on the systolic array with the output-stationary dataflow <ref type="bibr" target="#b72">[73]</ref>. Our evaluation results show that the weightstationary systolic has close benefits as well. Fig. <ref type="figure" target="#fig_14">9</ref> depicts an output stationary systolic array with ANT decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder Placement</head><p>We place ANT decoders between the on-chip memory buffer and systolic array. This means that quantized tensors are stored with low-bit precision in both off-chip and on-chip buffers. Meanwhile, there is no special hardware requirement for off-chip memory accesses because ANT numbers are decoded before they enter the systolic array. This design decision improves both the performance and energy efficiency because BERT-like models are bounded by the off-chip memory bandwidth <ref type="bibr" target="#b79">[80]</ref> while CNN models spend the most energy on on-chip buffer accesses <ref type="bibr" target="#b12">[13]</ref>.</p><p>As Fig. <ref type="figure" target="#fig_14">9</ref> shows, only the boundary PEs in the systolic array access the on-chip buffer. As such, we only place the decoders along the boundary to mitigate the area overhead. For the output-stationary systolic array, the input and weight elements are sent to the PE (process element) array from the  top and left, respectively. Assuming the array size of n ? n, we only need 2n instead of n 2 decoders, which amortizes the hardware area overhead of our design. The weight-stationary systolic array only needs n decoders for the input tensor as the output tensor is stored with high precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PE Connection</head><p>To use ANT PEs in the systolic array, we need extra wires connecting neighbouring PEs. The reason is that after decoding, an n-bit ANT type has two n-bit binary numbers. For example, a float-based ANT type has an n-bit exponent number and an n-bit mantissa number, while an int-based ANT type has an n-bit exponent number and an n-bit base integer number. However, our evaluation results show that the extra overhead for those wires is negligible due to the extremely short distance between PEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component Reuse</head><p>The 4-bit int-based ANT MAC unit uses a 4-bit adder and shifter for adding exponent values and shifting the multiplier result, respectively. Those extra hardware overheads can be mitigated in the mixed-precision design. As we have explained in Sec. V-D, the 8-bit int MAC requires four 4-bit ANT PE and a 16-bit adder. In the 8-bit mode, the n ? n systolic array with 4-bit ANT PEs would transform to n/2 ? n/2 systolic array with 8-bit int PEs. In this sense, we claim that ANT does not introduce new components for the PE of the mixed-precision systolic array, except for decoders outside the systolic array.</p><p>Weight Stationary Similar to output stationary, we move the input decoder to the top instead of the inner PE, as shown in Fig. <ref type="figure" target="#fig_14">9</ref>. Because weight elements are preloaded to the PEs, the weight can be decoded before the preloading. Therefore, the weight decoders only need to decode and store the decoded exponent and integer within each PE. Other optimizations for output stationary can also be used similarly as well.</p><p>Tensor Core Tensor core already supports mixed precision. For example, the A100 GPU with Ampere architecture <ref type="bibr" target="#b63">[64]</ref> provides 624 and 1248 TOPS (tera operations per second) for 8-bit int and 4-bit int, respectively. Meanwhile, the accumulator width for those MAC units is 32-bit int. As such, the existing tensor core can easily adopt the ANT type by augmenting its MAC units and adding decoders for the two multiplication operands. Moreover, the tensor core-based ANT has aligned memory accesses and does not require any modification of GPUs' memory hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Instruction Set Extension</head><p>The ANT framework introduces new data types for the multiply-accumulate instructions. For int-based ANT, we have two new data types, i.e., PoT and flint. They have the fixed bit-width so that the original load/store instructions are still applicable and hence remain unchanged. Thus, there is no modification for the memory sub-system.</p><p>Obviously, ANT does also not break the original programming model for convolutional (CONV) and fully connected (FC) layers. The specific type for each CONV and FC layer are determined after the quantization, and we can replace the original int-based version with flint or PoT to generate the corresponding codes. Thus, our ANT framework has a broad applicability owing to its ease of integration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EVALUATION</head><p>We evaluate ANT in the aspect of model accuracy, performance, area overhead, and energy efficiency in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methodology</head><p>Baselines We implement the ANT quantization framework in PyTorch <ref type="bibr" target="#b66">[67]</ref>. We evaluate four baselines compared against ANT, including BitFusion <ref type="bibr" target="#b72">[73]</ref>, OLAccel <ref type="bibr" target="#b65">[66]</ref>, BiScaled <ref type="bibr" target="#b42">[43]</ref>, AdaFloat <ref type="bibr" target="#b77">[78]</ref>, and GOBO <ref type="bibr" target="#b85">[86]</ref>. BitFusion <ref type="bibr" target="#b72">[73]</ref> uses the mixed-precision of 4-bit and 8-bit int types. BiScaled <ref type="bibr" target="#b42">[43]</ref> quantizes the tensors with two scale factors to address different ranges. We take the accuracy results from BiScaled paper and only synthesize the 6-bit BiScaled BPE. AdaFloat <ref type="bibr" target="#b77">[78]</ref> requires an 8-bit float to maintain the original model accuracy. OLAccel and GOBO are both outlier-aware quantization. We extend OLAccel <ref type="bibr" target="#b65">[66]</ref> to the Transformer-based models with weight &amp; activation quantization. Note that according to the original paper, the first and last layer require 8-bit instead of 4-bit for normal  values. GOBO <ref type="bibr" target="#b85">[86]</ref> only quantizes weights, so we only compare ANT against it in the metrics of area and accuracy.</p><p>Benchmark We use both CNN and Transformer-based models, including computer vision and natural language processing tasks listed in Tbl. IV. We exploit the SOTA checkpoint from PyTorch official repository <ref type="bibr" target="#b66">[67]</ref>. We report the top-1 accuracies with FP32 in Tbl. IV. The evaluated CNN models with the ImageNet dataset <ref type="bibr" target="#b20">[21]</ref> include VGG-16 <ref type="bibr" target="#b74">[75]</ref>, ResNet-18 <ref type="bibr" target="#b40">[41]</ref>, ResNet-50 <ref type="bibr" target="#b40">[41]</ref>, and Inception-V3 <ref type="bibr" target="#b76">[77]</ref>. For Transformer-based models, we evaluate BERT-Base <ref type="bibr" target="#b21">[22]</ref> with eight datasets of the GLUE dataset suite <ref type="bibr" target="#b78">[79]</ref>.</p><p>Owing to the space limitation, we only present the results on three datasets (MNLI, CoLA, and SST-2), while the other datasets have similar results. We also evaluate ViT (vision transformer) <ref type="bibr" target="#b24">[25]</ref>, which is a recent Transformer-based model and has achieved excellent results for vision tasks.</p><p>Fine-tuning Our ANT along with other baselines except BiScaled <ref type="bibr" target="#b42">[43]</ref> are compatible with quantization-aware training for better accuracies. To conduct a fair comparison, we strictly set the same hyper-parameters, including number of finetuning epochs and learning rate, for all types. All variables use 32-bit floating-point (FP32) arithmetic operations to simulate quantization effects <ref type="bibr" target="#b41">[42]</ref>. We generate and inject trainable weights and activation quantization parameters into the computation graph to fine-tune the quantized weights and activations. To optimize the clipping ranges (i.e., scale factors in Equation ( <ref type="formula" target="#formula_2">2</ref>)), we also employ the straight-through estimator (STE) <ref type="bibr" target="#b2">[3]</ref> method in the backward propagation based on the quantization framework PACT <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b51">[52]</ref>.</p><p>Accelerator Implementation We implement the ANT decoder and PE described in Sec. V with the Verilog RTL. We use Synopsys Design Compiler <ref type="bibr" target="#b49">[50]</ref> to synthesize those components with the 28 nm TSMC process, which reports area and static/dynamic power estimation. We use CACTI <ref type="bibr" target="#b58">[59]</ref> to estimate the area, latency, and power of memory structures.</p><p>For the end-to-end performance evaluation of ANT and other baselines, we develop a cycle-accurate simulator based on the DnnWeaver <ref type="bibr" target="#b71">[72]</ref>. We use DeepScaleTool <ref type="bibr" target="#b70">[71]</ref> to scale all designs to the 28 nm process for the iso-area comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantization Accuracy</head><p>Since ANT uses multiple primitive data types (flint/int/float/PoT), we first study the contribution of each primitive for improving the quantization accuracy.</p><p>Primitive Combination We study six combinations of the four primitive data types. Int is the combination with only a single data type. Two combinations int-PoT (IP) and float-int-PoT (FIP) excludes flint and hence only exploit the inter-tensor adaptivity. Correspondingly, we evaluate these two combinations that add flint and exploit the intra-and inter-tensor adaptivity. They are int-PoT-flint (IP-F) and float-int-PoT-flint (FIP-F). ANT4-8 uses the mixed-precision of 4-bit int-based ANT (i.e., IP-F) and 8-bit int for accuracy comparison. All types use 4-bit quantization except ANT4-8. For the quantization metrics, we use the MSE and model accuracy loss against the original FP32 model. Fig. <ref type="figure" target="#fig_15">10</ref> plots the quantization MSE of these combinations on eight DNN models. Fig. <ref type="figure" target="#fig_3">11</ref> and Fig. <ref type="figure" target="#fig_3">12</ref> demonstrate their accuracy loss compared to original highprecision models before and after fine-tuning, respectively.</p><p>Quantization MSE For quantizing each tensor in DNN models, we employ the ANT algorithm described in in Sec. IV-C to choose the primitive data type with minimum MSE. From the results in Fig. <ref type="figure" target="#fig_15">10</ref>, we find that adding more primitive data types generally lets us decrease the accuracy loss owing to quantization errors. In specific, adding the PoT type is critical for Transformer-based models on NLP datasets (MNLI, CoLA, and SST2), since they have large activation values. The benefit of the PoT type is smaller for the vision tasks including ViT. Adding the flint type is important for both vision and NLP tasks. Finally, we observe that adding the float has the least impact on the quantization errors, whose role is replaced by other primitive types.</p><p>Accuracy Comparing Fig. <ref type="figure" target="#fig_15">10</ref>, Fig. <ref type="figure" target="#fig_3">11</ref>, and Fig. <ref type="figure" target="#fig_3">12</ref>, we find that the model accuracy loss correlates well with the quantization MSE, and the fine-tuning plays an essential role in recovering the accuracy to the original values before quantization.   configuration (i.e., int-PoT-flint) as the final ANT for the rest of evaluation. Note that the 4-bit ANT type is still not able to maintain the original model accuracy, which justifies the choice of mixed-precision in our work. The mixed-precision ANT4-8 type can achieve original model accuracy in CNN models and less than 1% accuracy loss for ViT and BERT. We also observe that our proposed flint data type is important for the accuracies of both vision and NLP tasks. Meanwhile, the PoT type is more important for Transformer-based models on NLP tasks than vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison against BiScaled</head><p>We first compare the accuracy of IP-F configuration (i.e., int-PoT-flint) of ANT against the BiScaled <ref type="bibr" target="#b72">[73]</ref> without fine-tuning. Tbl. V shows the 6-bit quantization without fine-tuning results for ANT and BiScaled. We find that ANT offers much better accuracy than BiScaled because ANT can exploit inter-tensor adaptivity and intra-tensor adaptivity with more exponent domains.</p><p>Comparison against GOBO We compare the accuracy of ANT against the prior outlier-aware quantization work GOBO <ref type="bibr" target="#b85">[86]</ref>. Unlike ANT that performs both weight and activation quantization, GOBO only performs weight quantization. For a fair comparison, Tbl. VI shows that the weightonly quantization using ANT achieves a similar accuracy, while ANT's fixed-length feature is more hardware-friendly than the GOBO's variable-length encoding scheme.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Area</head><p>According to our evaluation, the float-based PE has about 3? area of int-based PE. Given their similar accuracies, we choose to use the int-based decoder and PE for ANT accelerator. We compare the accelerator area breakdown in Tbl. VII. Overall, the int-decoder overhead is about 0.2% for the systolic array. In the rest of evaluation, we scale other accelerators to 28 nm and perform an iso-area comparison. All accelerators have the same on-chip buffer configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance and Energy</head><p>We implement ANT with output-stationary (ANT-OS) and weight-stationary (ANT-WS). We adjust the mixed-precision ratio to make all models close to their original accuracy (CNN with &lt; 0.1% loss and Transformer with &lt; 1% loss) for the iso-accuracy and iso-area comparison except BiScaled. We only compare BiScaled on VGG16 and ResNet50, which have unignorable (&gt; 5%) accuracy loss, as shown in Tbl. V. Since AdaFloat <ref type="bibr" target="#b77">[78]</ref> does not support mixed-precision, we only conduct 8-bit quantization based on AdaFloat. Fig. <ref type="figure" target="#fig_17">13</ref> compares ANT design against various baselines with the metrics including the ratio of tensor types, normalized latency, and energy. The batch size is 64 for all experiments.</p><p>Tensor Type Ratio The top plot of Fig. <ref type="figure" target="#fig_17">13</ref> compares the ratio of 4-bit (flint, PoT, and int) and 8-bit (int) tensors in different designs. ANT-OS and ANT-WS use the same quantization algorithm but different microarchitectures, so that they have the same ratio of various data types. By inspecting the tensor ratio, we find that CNN models and vision transformer model ViT choose to use a significant portion of 4-bit flint type, while NLP Transformer models use a roughly same portion of 4-bit flint and PoT type. Compared to the prior mixed-precision work BitFusion <ref type="bibr" target="#b72">[73]</ref>, ANT has a much greater ratio of 4-bit tensors because its inter-tensor and intra-tensor adaptivity make the 4-bit ANT achieve much lower quantization errors. Especially for BERT on SST-2, ANT can get the original accuracy with 100% 4-bit quantization. OLAccel <ref type="bibr" target="#b65">[66]</ref> is not tensor-wise quantization as it uses variable-length encoding for different values within a tensor. We show its element-wise ratio of 4-bit and 8-bit values in the plot. Owing to its fine-grained element-wise quantization, it has a slightly higher proportion of 4-bit values than ANT, but also incurs a much greater hardware overhead with low end-to-end latency.</p><p>Performance The middle plot of Fig. <ref type="figure" target="#fig_17">13</ref> compares the normalized execution time of different designs, which shows that ANT achieves the best latency performance. We also find that ANT-OS and ANT-WS have very similar performances because their architectural differences can be mitigated through tiling optimizations <ref type="bibr" target="#b72">[73]</ref>. BitFusion has more 8-bit tensors, which lead to its worse performance. Even though OLAccel has a higher proportion of 4-bit tensors, it needs the additional outlier controller with significant overhead to orchestrate the computation among normal values and outliers. In the end, ANT achieves averaged 2.8?, 3.24?, 1.48?, and 4? speedup over BitFusion <ref type="bibr" target="#b72">[73]</ref>, OLAccel <ref type="bibr" target="#b65">[66]</ref>, BiScaled <ref type="bibr" target="#b72">[73]</ref>, and AdaFloat <ref type="bibr" target="#b77">[78]</ref>, respectively.</p><p>Energy The bottom plot of Fig. <ref type="figure" target="#fig_17">13</ref> compares the normalized energy consumption of different designs, which includes the static energy and dynamic energy (DRAM, on-chip buffer, and core). ANT-OS and ANT-WS have the lowest and second-lowest energy, respectively. Even though ANT-WS has a similar performance to ANT-OS, ANT-WS needs more buffer accesses for the high-precision output activation. Thus, ANT-WS spends more energy on accessing on-chip buffers. OLAccel consumes less energy than BitFusion because it has more 4-bit values, which reduces the energy of DRAM and on-chip buffer. In the end, ANT-OS achieves averaged 2.53?, 1.93?, 1.6?, and 3.33? energy reduction over BitFusion, OLAccel, BiScaled, and AdaFloat, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ANT Type Selection Analysis</head><p>In this subsection, we study the effectiveness of the data type selection algorithm in ANT, as previously described in Sec. IV-C. Specifically, we present the MSE for all weight and activation tensors in DNN models with diverse distributions.</p><p>We collect weight and activation tensors from ResNet-18 (CNN model) on ImageNet and BERT-Base (Transformer model) on MNLI dataset. Fig. <ref type="figure" target="#fig_18">14</ref> shows the MSE values of different 4-bit data types that are all normalized to flint. We adopt the unsigned numerical type for ResNet-18 activation tensors because of the ReLU function, which is a common practice for CNN quantizations <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b77">[78]</ref>. We use signed types for ResNet-18 weight tensors and all tensors of BERT. Fig. <ref type="figure" target="#fig_18">14</ref> shows that ANT always chooses the most appropriate data type, i.e., the type with the minimum MSE. Note that signed 4-bit float and PoT are identical so that they overlap in ResNet-18 weight MSE and BERT-Base weight and activation MSE.</p><p>We also justify the choice of data types by inspecting the distribution of different tensors. Recall that in Sec. II-A, it is natural to choose a numeric type whose quantization resolution distribution is similar to the tensor distribution to reduce the quantization error. For CNN models, int has pretty low MSEs with the first convolutional layer. Our manual inspection shows that the first layer is more like a uniform distribution than Gaussian. This is especially so for the activation tensor, which is the original image and not the featured map. After the first layer, tensors in CNN models are closer to Gaussian distribution so that flint almost dominates these layers with very low MSE values.</p><p>For BERT, we only collect the former two Transformer blocks as the representative, which have a similar trend to the rest Transformer blocks. BERT model has relatively more complex tensor distributions. The weight tensors show both uniform-like and Gaussian-like distributions so both int and flint are chosen. On the other hand, activation tensors have significant outliers so that they prefer PoT or float.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. RELATED WORK</head><p>This section presents related work on DNN acceleration, sparse accelerators, and low-bit quantization accelerators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN Acceleration</head><p>To accelerate the DNN models efficiently, researchers proposed both various hardware and software solutions. For hardware acceleration, the proposed architectures are tailored to fit the computation characteristics of DNN models which leverage the regular access pattern, high data reuse and tremendous parallelism to save the area and latency from control logic <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b96">[97]</ref>. In these hardware accelerators, weight or weight data flow through multiple stages to maximize reuse, with example like systolic array <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b45">[46]</ref> and other spatial architectures <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b90">[91]</ref>. Modern GPUs have already deployed SIMD-friendly matrix-matrix multiplication (GEMM) accelerator like tensor core <ref type="bibr" target="#b63">[64]</ref>.</p><p>For software acceleration, the efforts are mainly put into the compilation and scheduling optimizations. To fully utilize the hardware resources, various automated compilers or graph optimizers are proposed to find optimal implementations on different hardware <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b92">[92]</ref>, <ref type="bibr" target="#b91">[93]</ref>, <ref type="bibr" target="#b97">[98]</ref>. Researchers proposed various scheduling techniques <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b83">[84]</ref> to manage resource usage, task queuing, runtime batching, and so on.</p><p>Sparse DNN Accelerators Given the increasing computation demand of DNN models, it is of paramount importance to leverage the algorithm and hardware co-design. Researchers have proposed pruning and quantization methods to exploit the redundancy property of DNNs for such a purpose. Pruning means removing part of the weight, input, or even output of DNN layers, which leads to a sparse model with a portion of model size. However, sparse models contain irregular memory accesses, which could negate the benefits of sparsity. To overcome this challenge, it is important to design sparsityoptimized algorithms and hardware architectures <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b98">[99]</ref>.</p><p>Quantization Accelerators The DNN model quantization exploits the insight that DNN inference does not need high-precision representations like FP32, and is orthogonal to model pruning. It uses a narrow bit width to reduce memory and computation requirement. The fixed-length value encoding is convenient for architectural integration because it only requires processing element design, such as FP16, INT8, or even INT4 <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b85">[86]</ref>, and BF16 <ref type="bibr" target="#b45">[46]</ref>, TF32 <ref type="bibr" target="#b63">[64]</ref>, and Posit <ref type="bibr" target="#b37">[38]</ref>. Posit is a general data type and a potential replacement for IEEE 754 <ref type="bibr" target="#b47">[48]</ref>. It uses variable length encoding for the regime bits to extend the exponent range. Our proposed flint is different from Posit in the aspect that flint has no regime bit and an efficient encoding/decoding process based on float or int type.</p><p>BitFusion <ref type="bibr" target="#b72">[73]</ref> and DRQ <ref type="bibr" target="#b75">[76]</ref> can support different bitwidth via a spatial and temporal combination of low-bit PEs, respectively. There are also outlier-aware quantization accelerator designs, such as OLAccel <ref type="bibr" target="#b65">[66]</ref>, DRQ <ref type="bibr" target="#b75">[76]</ref>, and GOBO <ref type="bibr" target="#b85">[86]</ref>, which are more aggressive and require heavy architectural modifications. Moreover, these outlier-aware quantization accelerators have unaligned computation and memory accesses, resulting in their limited benefits. In contrast, our work provides the adaptive numerical data type, which provides low-bit fixed-length value presentations and hence is also easy for architectural integration. Quantization Methods In our work, we use two popular quantization methods, i.e., quantization-aware training (QAT) <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b99">[100]</ref> and post-training quantization (PTQ) <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b99">[100]</ref>. The QAT requires finetuning to restore the model accuracy, while the latter leverages heuristics such as constraint optimization to avoid fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>In this work, we present a novel, composite data type called ANT to achieve low-bit quantization for accelerating DNN models. The key insight is adapting the data type to value importance within a tensor and different tensors' value distributions. For the intra-tensor adaptivity, we propose flint, a new data type that combines the advantages of int (maintaining a high precision for important value ranges) and float (maintaining a large value range). For the intertensor adaptivity, we propose the composite ANT type, which selects a data type (e.g., int/flint/PoT) for each tensor according to its distribution. We design a unified processing element architecture for ANT and show its ease of integration to existing DNN accelerators. Our design demonstrates 2.8? latency reduction and 2.5? energy improvement over the state-of-the-art quantization accelerators. 2) Hardware dependencies: We fine-tune the DNN models with two types of server configuration: A server is equipped with a single NVIDIA A100 (40GB) GPU, and a server is equipped with four NVIDIA A10 (24GB) GPUs for distributed fine-tuning.</p><p>3) Software dependencies: The experiments rely on the following software components.</p><p>? Ubuntu 18.04. The evaluated image classification models with the ImageNet dataset <ref type="bibr" target="#b20">[21]</ref> include VGG-16 <ref type="bibr" target="#b74">[75]</ref>, ResNet-18 <ref type="bibr" target="#b40">[41]</ref>, ResNet-50 <ref type="bibr" target="#b40">[41]</ref>, Inception-V3 <ref type="bibr" target="#b76">[77]</ref>, and ViT (vision transformer) <ref type="bibr" target="#b24">[25]</ref>. For NLP models, we evaluate BERT-Base <ref type="bibr" target="#b21">[22]</ref> with the GLUE dataset suite <ref type="bibr" target="#b78">[79]</ref>. Owing to the space limitation, we only present the results on three datasets (MNLI, CoLA, and SST-2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Installation</head><p>We have well-documented README files to detail the installation instruction for each experiment at https://github. com/clevercool/ANT Micro22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation and expected results</head><p>Our experiments have two major parts: the evaluation of DNN model accuracy and the performance of the ANT simulator.</p><p>? The directory ant_quantization contains the ANT framework based on PyTorch for the DNN model accuracy evaluation. ? The directory ant_simulator contains the performance and energy evaluation of the ANT simulator. To evaluate the experiments, you can utilize the scripts in each directory according to the README files. We also release all expected results in the README files for Figure <ref type="figure" target="#fig_3">12</ref>, Figure <ref type="figure" target="#fig_17">13</ref>, and Table V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Methodology</head><p>Submission, reviewing and badging methodology:</p><p>? https://www.acm.org/publications/policies/artifactreview-badging ? http://cTuning.org/ae/submission-20201122.html ? http://cTuning.org/ae/reviewing-20201122.html</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The qualitative framework for describing the hardware overhead for leveraging DNN quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The 4-bit unsigned flint type, with "x" as either 0 or 1. The exponent and mantissa bits are marked with green and blue color, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Element-wise flint encoding algorithm.Input: Element, e; Bit-width, b; Scale factor, s. Output: Quantized Element, q. 1 def FlintQuant(e, b, s):2 en = 2 ? b; // Exponent number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 e 7 i</head><label>37</label><figDesc>= IntQuantization(e, s, 0, 2 en-2 ); = log 2 (e) + 1; 8 exp = GetExponent(b, i); // First-one exponent. 9 mb = blen(exp); // Mantissa bit. 10 m = Round[(e/2 i-1 -1) ? 2 mb ]; 11 m = Binary(m); 12 q = Concat(exp, m); 13 return q;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 2 : 2 minMSE = 10 9 ; 3 foreach l ? L do 4 F</head><label>2234</label><figDesc>ANT data type selection algorithm. Input: Tensor, T ; Candidate list of numeric types, L. Output: Quantization function, F Q . 1 def ANT(T , L): = GetQuantFunc(l); // Get the quantization method of l.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5 m 6 if m &lt; minMSE then 7 F</head><label>567</label><figDesc>= ArgminMSE(T , F); // Search the minimum MSE with range clipping. Q = F; 8 return F Q above quantization process, the original value 11 10 is now rounded to 12 10 in flint representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ANT-based quantized inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The 4-bit unsigned float-based flint decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The 4-bit int-based flint decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The 4-bit int-based flint MAC unit. "IF Decoder" is the int-based flint decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The 8-bit int MAC implementation via four 4-bit ANT MACs, which reuses most components except the adder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Architectural optimizations for integrating ANT data type to the output-stationary systolic array.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The quantization MSE with the combination of four different primitive types, all of which use 4-bit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: The accuracy loss without fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Comparison of the tensor type ratios (top), normalized latency (middle), and energy in different designs (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Numerical type (4-bit) mean square error (MSE) results that are normalized to flint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the intra-tensor level and the intertensor level, for which we present a unified qualitative framework to analyze the hardware overhead introduced by previous low-bit quantization works.? We propose a composite adaptive numeric data type framework called ANT that can exploit the adaptive opportunities at both the intra-and inter-tensor levels in a hardware-friendly fashion. ? We propose a unified TypeFusion processing element (PE) design that can handle the case when the input tensor and weight tensor have different primitive types. ? We describe how to integrate the above ANT PEs to existing DNN accelerator architectures such as systolic array, which achieves the same level of accuracy as original full-precision models and significantly outperforms existing quantization accelerator under the same area.</figDesc><table /><note><p>? We demonstrate the opportunities for adaptive quan-tization at both</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>left, and highlight the exponent fields in green color. The first four intervals have the encoded exponent</figDesc><table><row><cell>Bits</cell><cell>Exponent Value</cell><cell>Fraction Value</cell><cell>Value in Decimal</cell></row><row><cell>0000</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table II :</head><label>II</label><figDesc>The value table of 4-bit unsigned flint with the exponent bias of -1. The blue numbers are the first-oneencoded exponent and "x" is mantissa with value of 0 or 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table IV :</head><label>IV</label><figDesc>Details of evaluated model and dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The 4-bit IP-F and FIP-F provide more numerical types for selection, and both achieve the minimum accuracy loss. The former only requires the int-based PE while the latter requires the float-based PE. We show later that the float-based PE for ANT consumes almost 3? area of int-based PE. As such, we choose the IP-F</figDesc><table><row><cell>Model</cell><cell>ANT</cell><cell>BiScaled</cell><cell>Source</cell></row><row><cell>AlexNet [49]</cell><cell>55.85%</cell><cell>54.90%</cell><cell>56.56%</cell></row><row><cell>VGG16</cell><cell>72.80%</cell><cell>66.56%</cell><cell>73.48%</cell></row><row><cell>ResNet50</cell><cell>75.08%</cell><cell>70.46%</cell><cell>75.97%</cell></row><row><cell>ResNet152</cell><cell>77.30%</cell><cell>73.41%</cell><cell>78.25%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table V :</head><label>V</label><figDesc>Accuracy comparison between ANT and BiScaled without fine-tuning under 6-bit quantization.</figDesc><table><row><cell></cell><cell>Int-4bit</cell><cell>IP-4bit</cell><cell>FIP-4bit</cell><cell>IP-F-4bit</cell><cell>FIP-F-4bit</cell></row><row><cell>Accuracy Loss</cell><cell>10 20 30 40 50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">VGG16 Res.18 Res.50 Incep.V3 ViT</cell><cell cols="2">MNLI CoLA SST2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table VI :</head><label>VI</label><figDesc>Accuracy comparison between weight-only quantization using ANT and GOBO for BERT on MNLI dataset.</figDesc><table><row><cell cols="2">100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type Ratio</cell><cell>25% 50% 75%</cell><cell></cell><cell>Int4 PoT Flint Int8 FxP6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Norm. Cycle</cell><cell>0.2 0.4 0.6 0.8 0% 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ANT-OS</cell><cell></cell><cell></cell><cell cols="2">ANT-WS</cell><cell></cell><cell></cell><cell cols="2">BitFusion</cell><cell></cell><cell cols="3">OLAccel</cell><cell></cell><cell></cell><cell cols="3">BiScaled</cell><cell></cell><cell></cell><cell cols="3">AdaFloat</cell><cell></cell><cell>0.25 0.25 0.7 0.81 0.37</cell><cell></cell></row><row><cell>Norm. Energy</cell><cell>0 0.2 0.4 0.6 0.8 0 1</cell><cell>ANT-OS</cell><cell>ANT-WS</cell><cell>BitFusion</cell><cell>OLAccel</cell><cell>BiScaled</cell><cell>AdaFloat</cell><cell>ANT-OS</cell><cell>ANT-WS</cell><cell>BitFusion</cell><cell>OLAccel</cell><cell>AdaFloat</cell><cell>ANT-OS</cell><cell>ANT-WS Static BitFusion</cell><cell>OLAccel</cell><cell>BiScaled</cell><cell>AdaFloat</cell><cell>ANT-OS</cell><cell>ANT-WS DRAM BitFusion OLAccel</cell><cell>AdaFloat</cell><cell>ANT-OS</cell><cell>ANT-WS Buffer BitFusion</cell><cell>OLAccel</cell><cell>AdaFloat</cell><cell>ANT-OS</cell><cell>ANT-WS Core BitFusion</cell><cell>OLAccel</cell><cell>AdaFloat</cell><cell>ANT-OS</cell><cell>ANT-WS</cell><cell>BitFusion</cell><cell>OLAccel</cell><cell>AdaFloat</cell><cell>ANT-OS</cell><cell>ANT-WS</cell><cell>BitFusion</cell><cell>OLAccel</cell><cell>AdaFloat</cell><cell>0.3 0.4 0.76 0.58 0.48 ANT-OS ANT-WS BitFusion OLAccel BiScaled</cell><cell>AdaFloat</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">VGG16</cell><cell></cell><cell></cell><cell></cell><cell cols="3">ResNet18</cell><cell></cell><cell></cell><cell cols="3">ResNet50</cell><cell></cell><cell cols="3">InceptionV3</cell><cell></cell><cell>ViT</cell><cell></cell><cell></cell><cell cols="14">BERT-MNLI BERT-CoLA BERT-SST-2</cell><cell>Geomean</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table VII :</head><label>VII</label><figDesc>The configuration and area breakdown of ANT and other baselines under 28 nm process.</figDesc><table><row><cell>Architecture</cell><cell>Component</cell><cell>Core Number</cell><cell>Area (mm 2 )</cell><cell>Buffer</cell></row><row><cell>ANT</cell><cell>Decoder (4.9?m 2 ) 4-bit PE (79.57?m 2 )</cell><cell>128 4096</cell><cell>0.327</cell><cell></cell></row><row><cell>BitFusion</cell><cell>4-bit PE</cell><cell>4096</cell><cell>0.326</cell><cell>512 KB</cell></row><row><cell>OLAccel8</cell><cell>4-bit &amp; 8-bit PE</cell><cell>1152</cell><cell>0.320</cell><cell>4.2 mm 2</cell></row><row><cell>BiScaled</cell><cell>6-bit BPE</cell><cell>2560</cell><cell>0.328</cell><cell></cell></row><row><cell>AdaFloat</cell><cell>8-bit PE</cell><cell>896</cell><cell>0.327</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>?</head><label></label><figDesc>How much time is needed to complete experiments (approximately)?: It takes approximately 50 hours to execute all experiments using the server equipped with GPUs. The fast evaluation can take only one hour with the checkpoints. ? Publicly available: Our framework is publicly available on GitHub https://github.com/clevercool/ANT Micro22. ? Code licenses: Apache-2.0 license. ? Data licenses: The datasets are publicly available through their original licensing terms. ? Archived: https://doi.org/10.5281/zenodo.7002114. We archive the source code at https:// doi.org/10.5281/zenodo.7002114. We recommend you access our GitHub repository: https://github.com/clevercool/ANT Micro22 for the latest version.</figDesc><table><row><cell>C. Description</cell></row><row><cell>1) How to access:</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant <rs type="grantNumber">2021ZD0110104</rs>, the <rs type="funder">National Natural Science Foundation of China (NSFC)</rs> grant (<rs type="grantNumber">U21B2017</rs>, <rs type="grantNumber">62072297</rs>, and <rs type="grantNumber">61832006</rs>). The authors would like to thank the anonymous reviewers for their constructive feedback for improving the work. We also thank <rs type="person">Tailong Wangliu</rs>, <rs type="person">Weiming Hu</rs>, and <rs type="person">Yuxian Qiu</rs> for their technical supports and beneficial discussions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kXKe7GS">
					<idno type="grant-number">2021ZD0110104</idno>
				</org>
				<org type="funding" xml:id="_PSJeBUa">
					<idno type="grant-number">U21B2017</idno>
				</org>
				<org type="funding" xml:id="_7ZQAGnj">
					<idno type="grant-number">62072297</idno>
				</org>
				<org type="funding" xml:id="_82PtHYh">
					<idno type="grant-number">61832006</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Abstract</head><p>Our experiments have two major parts: the evaluation of DNN model accuracy and the performance of the ANT simulator.</p><p>We evaluate the results with models in image classification and NLP. The image classification tasks include five models, i.e., VGG16, ResNet18, ResNet50, Inception-V3, and ViT. We adopt the BERT model for the NLP task with three datasets, MNLI, CoLA, and SST-2. We provide the finetuning source code for all models to measure the accuracy. However, that may need dozens of hours to complete the finetuning process. Therefore, we provide the checkpoints for the fast evaluation of image classification models, which can finish in one hour. For measuring the performance, we evaluate all models with six simulator configurations. In all experiments, we run those models according to the experiment setup on a Ubuntu server that equips an NVIDIA A100 GPU and multiple servers with four NVIDIA A10 GPUs for distributed fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Artifact check-list (meta-information)</head><p>? Compilation: NVCC 11.3, GCC 7.5.0. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnvlutin: Ineffectual-neuron-free deep neural network computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Post training 4-bit quantization of convolutional networks for rapid-deployment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient 8-bit quantization of transformer neural machine language translation model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhandare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saletore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00532</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zeroq: A novel zero shot quantization framework</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">178</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking differentiable search for mixed-precision neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A dynamically configurable coprocessor for convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sankaradas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jakkula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cadambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual international symposium on Computer architecture</title>
		<meeting>the 37th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prophet: Precise qos prediction on non-preemptive accelerators to improve utilization in warehouse-scale computers</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-04-08">2017. April 8-12, 2017. 2017</date>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Baymax: Qos awareness and increased utilization for non-preemptive accelerators in warehouse scale computers</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2016</title>
		<meeting>the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2016<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">April 2-6, 2016. 2016</date>
			<biblScope unit="page" from="681" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TVM: an automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Carlsbad, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-10-08">2018. October 8-10, 2018. 2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="284" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of solid-state circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dadiannao: A machinelearning supercomputer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="609" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pact: Parameterized clipping activation for quantized neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lazy batching: An slaaware batching system for cloud machine learning inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High-Performance Computer Architecture, HPCA 2021</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-03-03">February 27 -March 3, 2021. 2021</date>
			<biblScope unit="page" from="493" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lowbit quantization of neural networks for efficient inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choukroun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kravchik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kisilev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3009" to="3018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Natural language processing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chowdhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fundamentals of artificial intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="603" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ebird: Elastic batch for improving responsiveness and throughput of deep learning services</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCD46524.2019.00075</idno>
		<ptr target="https://doi.org/10.1109/ICCD46524.2019.00075" />
	</analytic>
	<monogr>
		<title level="m">37th IEEE International Conference on Computer Design</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-17">2019. November 17-20, 2019. IEEE, 2019</date>
			<biblScope unit="page" from="497" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DVABatch: Diversity-aware Multi-Entry Multi-Exit batching for efficient processing of DNN services on GPUs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 USENIX Annual Technical Conference (USENIX ATC</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="183" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Ieee</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hawq-v2: Hessian aware trace-weighted quantization of neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arfeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hawq: Hessian aware quantization of neural networks with mixed-precision</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shidiannao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lowlatency proactive continuous vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the ACM International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="329" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ptolemy: Architecture support for robust deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="241" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ai and memory wall</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RiseLab Medium Post</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A 240 g-ops/s mobile coprocessor for deep neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">How far does bert look at: Distance-based clustering and analysis of bert s attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00943</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transkimmer: Transformer learns to layer-wise skim</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07324</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Block-skim: Efficient question answering for transformer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">719</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Accelerating sparse dnn models without hardware-support via tile-wise sparsity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Hsueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SQuant: On-the-fly data-free quantization via diagonal hessian approximation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=JXhROKNZzOc" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Balancing Efficiency and Flexibility for DNN Acceleration via Temporal GPU-Systolic Array Integration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>in 2020 57th ACM</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beating floating point at its own game: Posit arithmetic</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Yonemoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing frontiers and innovations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="71" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02626</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Biscaled-dnn: Quantizing longtailed datastructures with two scale factors for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 56th ACM/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TASO: optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Digital image processing: An algorithmic approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PHI Learning Pvt. Ltd</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
		<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to quantize deep networks by optimizing quantization intervals with task loss</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4350" to="4359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ieee standard 754 for binary floating-point arithmetic</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes on the Status of IEEE</title>
		<imprint>
			<biblScope unit="volume">754</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Logic synthesis using Synopsys?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kurup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abbasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Asymmetric resilience: Exploiting task-level idempotency for transient error recovery in accelerator-based systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bertran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13144</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.03759</idno>
		<title level="m">Mqbench: Towards reproducible and deployable model quantization benchmark</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">VELTAIR: towards high-performance multi-tenant deep learning services via adaptive compilation and scheduling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3503222.3507752</idno>
		<ptr target="https://doi.org/10.1145/3503222.3507752" />
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;22: 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</editor>
		<meeting><address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-02-28">28 February 2022 -4 March 2022. 2022</date>
			<biblScope unit="page" from="388" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Heracles: improving resource efficiency at scale</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bubble-up: increasing utilization in modern warehouse scale computers via sensible co-locations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Miyashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Murmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01025</idno>
		<title level="m">Convolutional neural networks using logarithmic data representation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cacti 6.0: A tool to model large caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP laboratories</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Up or down? adaptive rounding for post-training quantization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Amjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Baalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7197" to="7206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A white paper on neural network quantization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fournarakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Amjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Baalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08295</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reduct: Keep it close, keep it cool!: Efficient scaling of dnn inference on multi-core cpus with near-cache compute</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rakshit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abuhatzera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuttanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="167" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Tensorrt: A c++ library for high performance inference on nvidia gpus and deep learning accelerators</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/TensorRT" />
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Nvidia a100 tensor core architecture</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NVIDIA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An algorithmic and novel design of a leading zero detector circuit: Comparison with logic synthesis</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Oklobdzija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="124" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Energy-efficient neural network accelerator based on outlier-aware low-precision computation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="688" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Memory-centric accelerator design for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peemen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Corporaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 31st International Conference on Computer Design (ICCD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sigma: A sparse and irregular gemm accelerator with flexible interconnects for dnn training</title>
		<author>
			<persName><forename type="first">E</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nadella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="58" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adversarial defense through network profiling based path extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deepscaletool: A tool for the accurate estimation of technology scaling in the deepsubmicron era</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">From highlevel deep neural models to fpgas</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="764" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Q-bert: Hessian based ultra low precision quantization of bert</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8815" to="8821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Drq: dynamic region-based quantization for deep neural network acceleration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1010" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Algorithm-hardware co-design of adaptive floating-point encodings for resilient deep learning inference</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 57th ACM/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Spatten: Efficient sparse attention architecture with cascade token and head pruning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Haq: Hardwareaware automated quantization with mixed precision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8612" to="8620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Dual-side sparse tensor core</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1083" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning channel-wise interactions for binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="568" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Bubble-flux: precise online qos management for increased utilization in warehouse scale computers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Breslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 40th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Forms: finegrained polarized reram-based in-situ computation for mixedsignal dnn accelerator</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Behnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Bojnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="265" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Edo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="811" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Q8bert: Quantized 8bit bert</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wasserblat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Fifth Workshop on Energy Efficient Machine Learning and Computing-NeurIPS Edition (EMC2-NIPS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="36" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Optimizing fpga-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM/SIGDA international symposium on field-programmable gate arrays</title>
		<meeting>the 2015 ACM/SIGDA international symposium on field-programmable gate arrays</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Cambricon-x: An accelerator for sparse neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Cambricon-f: machine learning computers with</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Architectural Support for Programming Languages and Operating Systems, Lausanne (ASPLOS), 2020. fractal von neumann architecture</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="788" to="801" />
		</imprint>
	</monogr>
	<note>Proceedings of the 46th International Symposium on Computer Architecture</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Ansor: Generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with lowprecision weights</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Dorefanet: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Cambricon-s: Addressing irregularity in sparse neural networks through a cooperative software/hardware approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 51st Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Characterizing and demystifying the implicit convolution algorithm on commercial matrixmultiplication accelerators</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="214" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">ROLLER: Fast and efficient tensor compilation for deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="233" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Sparse tensor core: Algorithm and hardware co-design for vector-wise sparse neural networks on modern gpus</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="359" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Effective training of convolutional neural networks with low-bitwidth weights and activations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
