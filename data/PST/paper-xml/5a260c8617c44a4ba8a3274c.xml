<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COUNTERING ADVERSARIAL IMAGES USING INPUT TRANSFORMATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rana</forename><surname>Mayank</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COUNTERING ADVERSARIAL IMAGES USING INPUT TRANSFORMATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As the use of machine intelligence increases in security-sensitive applications <ref type="bibr" target="#b2">(Bojarski et al., 2016;</ref><ref type="bibr" target="#b0">Amodei et al., 2015)</ref>, robustness has become a critical feature to guarantee the reliability of deployed machine-learning systems. Unfortunately, recent research has demonstrated that existing models are not robust to small, adversarially designed perturbations of the input <ref type="bibr" target="#b1">(Biggio et al., 2013;</ref><ref type="bibr" target="#b29">Szegedy et al., 2014;</ref><ref type="bibr" target="#b13">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b18">Kurakin et al., 2016a;</ref><ref type="bibr" target="#b5">Cisse et al., 2017a)</ref>. Adversarially perturbed examples have been deployed to attack image classification services <ref type="bibr" target="#b20">(Liu et al., 2016)</ref>, speech recognition systems <ref type="bibr" target="#b5">(Cisse et al., 2017a)</ref>, and robot vision <ref type="bibr" target="#b23">(Melis et al., 2017)</ref>. The existence of these adversarial examples has motivated proposals for approaches that increase the robustness of learning systems to such examples <ref type="bibr" target="#b26">(Papernot et al., 2016;</ref><ref type="bibr" target="#b18">Kurakin et al., 2016a;</ref><ref type="bibr" target="#b6">Cisse et al., 2017b)</ref>.</p><p>The robustness of machine learning models to adversarial examples depends both on the properties of the model (i.e., Lipschitzness) and on the nature of the problem considered, e.g., on the input dimensionality and the Bayes error of the problem <ref type="bibr" target="#b10">(Fawzi et al., 2015;</ref><ref type="bibr">2016)</ref>. Consequently, defenses that aim to increase robustness against adversarial examples fall in one of two main categories. The first category comprises model-specific strategies that enforce model properties such as invariance and smoothness via the learning algorithm or regularization scheme <ref type="bibr" target="#b28">(Shaham et al., 2015;</ref><ref type="bibr" target="#b18">Kurakin et al., 2016a;</ref><ref type="bibr" target="#b6">Cisse et al., 2017b)</ref>, potentially exploiting knowledge about the adversary's attack strategy <ref type="bibr" target="#b13">(Goodfellow et al., 2015)</ref>. The second category of defenses are model-agnostic: they try to remove adversarial perturbations from the input. For example, in the context of image classification, adversarial perturbations can be partly removed via JPEG compression <ref type="bibr" target="#b8">(Dziugaite et al., 2016)</ref> or image re-scaling <ref type="bibr" target="#b21">(Lu et al., 2017)</ref>. Hitherto, none of these defenses has been shown to be very effective. Specifically, model-agnostic defenses appear too simple to sufficiently remove adversarial perturbations from input images. By contrast, model-specific defenses make strong assumptions about the nature of the adversary (e.g., on the norm that the adversary minimizes or on the number of iterations it uses to generate the perturbation). Consequently, they do not satisfy <ref type="bibr" target="#b17">Kerckhoffs (1883)</ref> principle: the adversary can alter its attack to circumvent such model-specific defenses.</p><p>In this paper, we focus on increasing the effectiveness of model-agnostic defense strategies by developing approaches that (1) remove the adversarial perturbations from input images, (2) maintain sufficient information in input images to correctly classify them, and (3) are still effective in settings in which the adversary has information on the defense strategy being used. We explore transformations based on image cropping and rescaling <ref type="bibr" target="#b14">(Graese et al., 2016)</ref>, bit-depth reduction <ref type="bibr" target="#b35">(Xu et al., 2017</ref><ref type="bibr">), JPEG compression (Dziugaite et al., 2016)</ref>, total variance minimization <ref type="bibr" target="#b27">(Rudin et al., 1992)</ref>, and image quilting <ref type="bibr" target="#b9">(Efros &amp; Freeman, 2001)</ref>. We show that these defenses can be surprisingly effective against existing attacks, in particular, when the convolutional network is trained on images that are transformed in a similar way. The image transformations are good at countering the (iterative) fast gradient sign method <ref type="bibr" target="#b18">(Kurakin et al., 2016a</ref><ref type="bibr">), Deepfool (Moosavi-Dezfooli et al., 2016)</ref>, and the Carlini &amp; Wagner (2017) attack, even in gray-box settings in which the model architecture and parameters are public. Our strongest defenses are based on total variation minimization and image quilting: these defenses are non-differentiable and inherently random, which makes it difficult for an adversary to get around them. Our best defenses eliminate 60% of gray-box attacks and 90% of black-box attacks by four major attack methods that perturb pixel values by 8% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><p>We study defenses against non-targeted adversarial examples for image-recognition systems. Let X = [0, 1] H×W ×C be the image space. Given an image classifier h(•) and a source image x ∈ X , a non-targeted<ref type="foot" target="#foot_0">1</ref> adversarial example of x is a perturbed image x ∈ X such that h(x) = h(x ) and d(x, x ) ≤ ρ for some dissimilarity function d(•, •) and ρ ≥ 0. Ideally, d(•, •) measures the perceptual difference between x and x but, in practice, the Euclidean distance d</p><formula xml:id="formula_0">(x, x ) = x−x 2 or the Chebyshev distance d(x, x ) = x − x ∞ is most commonly used.</formula><p>Given a set of N images {x 1 , . . . , x N } and a target classifier h(•), an adversarial attack aims to generate {x 1 , . . . , x N } such that each x n is an adversarial example for x n . The success rate of an attack is measured by the proportion of predictions that was altered by an attack:</p><formula xml:id="formula_1">1 N N n=1 1[h(x n ) = h(x n )].</formula><p>The success rate is generally measured as a function of the magnitude of the perturbations performed by the attack, using the normalized L 2 -dissimilarity:</p><formula xml:id="formula_2">1 N N n=1 x n − x n 2 x n 2 .<label>(1)</label></formula><p>A strong adversarial attack has a high success rate whilst its normalized L 2 -dissimilarity is low.</p><p>In most practical settings, an adversary does not have direct access to the model h(•) and has to do a black-box attack. However, prior work has shown successful attacks by transferring adversarial examples generated for a separately-trained model to an unknown target model <ref type="bibr" target="#b20">(Liu et al., 2016)</ref>. Therefore, we investigate both the black-box and a more difficult gray-box attack setting: in our gray-box setting, the adversary has access to the model architecture and the model parameters, but is unaware of the defense strategy that is being used.</p><p>A defense is an approach that aims make the prediction on an adversarial example h(x ) equal to the prediction on the corresponding clean example h(x). In this study, we focus on imagetransformation defenses g(x) that perform prediction via h(g(x )). Ideally, g(•) is a complex, nondifferentiable, and potentially stochastic function: this makes it difficult for an adversary to attack the prediction model h(g(x)) even when the adversary knows both h(•) and g(•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ADVERSARIAL ATTACKS</head><p>One of the first successful attack methods is the fast gradient sign method (FGSM; <ref type="bibr" target="#b13">Goodfellow et al. (2015)</ref>). Let (•, •) be the differentiable loss function that was used to train the classifier h(•), e.g., the cross-entropy loss. The FGSM adversarial example corresponding to a source input x and true label y is:</p><formula xml:id="formula_3">x = x + • sign (∇ x (x, y)) ,<label>(2)</label></formula><p>for some &gt; 0 that governs the perturbation magnitude. A stronger variant of this attack, called iterative FGSM (I-FGSM; <ref type="bibr" target="#b19">Kurakin et al. (2016b)</ref>), iteratively applies the FGSM update: where m = 1, . . . , M ; x (0) = x; and x = x (M ) . The number of iterations M is set such that h(x ) = h(x). Both FGSM and I-FGSM approximately minimize the Chebyshev distance between the inputs and the adversarial examples they generate.</p><formula xml:id="formula_4">x (m) = x (m−1) + • sign ∇ x (m−1) (x (m−1) , y) ,<label>(3)</label></formula><p>Alternative attacks aim to minimize the Euclidean distance between the input and the adversarial example instead. For instance, assuming h(•) is a binary classifier, DeepFool (Moosavi-Dezfooli et al., 2016) projects x onto a linearization of the decision boundary defined by h(•) for M iterations:</p><formula xml:id="formula_5">x (m) = x (m−1) − • h(x (m−1) ) ∇ x (m−1) h(x (m−1) ) 2 2 ∇ x (m−1) h x (m−1) ,<label>(4)</label></formula><p>where x (0) and x are defined as in I-FGSM. The multi-class variant of DeepFool performs the projection onto the nearest class boundaries. The linearization performed in DeepFool is particularly well suited for ReLU-networks, as these represent piecewise linear class boundaries.</p><p>Carlini-Wagner's L 2 attack (CW-L2; Carlini &amp; Wagner (2017)) is an optimization-based attack that combines a differentiable surrogate for the model's classification accuracy with an L 2 -penalty term. Let Z(x) be the operation that computes the logit vector (i.e., the output before the softmax layer) for an input x, and Z(x) k be the logit value corresponding to class k. The untargeted variant of CW-L2 finds a solution to the unconstrained optimization problem min</p><formula xml:id="formula_6">x x − x 2 2 + λ f max −κ, Z(x ) h(x) − max{Z(x ) k : k = h(x)} ,<label>(5)</label></formula><p>where κ denotes a margin parameter, and where the parameter λ f trades off the perturbation norm and the hinge loss of predicting a different class. We perform the minimization over x using the Adam optimizer (Kingma &amp; Ba, 2014) for 100 iterations with an initial learning rate of 0.001.</p><p>All of the aforementioned attacks enforce that x ∈ X by clipping values between 0 and 1. Figure <ref type="figure" target="#fig_0">1</ref> shows adversarial images produced by all four attacks at five normalized L 2 -dissimilarity levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEFENSES</head><p>Adversarial attacks alter particular statistics of the input image in order to change the model prediction. Indeed, adversarial perturbations x−x have a particular structure, as illustrated by Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>We design and experiment with image transformations that alter the structure of these perturbations, and investigate whether the alterations undo the effects of the adversarial attack. We investigate five image transformations: (1) image cropping and rescaling, (2) bit-depth reduction, (3) JPEG compression, (4) total variance minimization, and (5) image quilting. We first introduce three simple image transformations: image cropping-rescaling <ref type="bibr" target="#b14">(Graese et al., 2016)</ref>, bit-depth reduction <ref type="bibr" target="#b35">(Xu et al., 2017), and</ref><ref type="bibr">JPEG compression and</ref><ref type="bibr">decompression (Dziugaite et al., 2016)</ref>. Image croppingrescaling has the effect of altering the spatial positioning of the adversarial perturbation, which is important in making attacks successful. Following <ref type="bibr" target="#b15">He et al. (2016)</ref>, we crop and rescale images at training time as part of the data augmentation. At test time, we average predictions over random image crops. Bitdepth reduction <ref type="bibr" target="#b35">(Xu et al., 2017)</ref> perform a simple type of quantization that can removes small (adversarial) variations in pixel values from an image; we reduce images to 3 bits in our experiments. JPEG compression <ref type="bibr" target="#b8">(Dziugaite et al., 2016)</ref> removes small perturbations in a similar way; we perform compression at quality level 75 (out of 100).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TOTAL VARIANCE MINIMIZATION</head><p>An alternative way of removing adversarial perturbations is via a compressed sensing approach that combines pixel dropout with total variation minimization <ref type="bibr" target="#b27">(Rudin et al., 1992)</ref>. This approach randomly selects a small set of pixels, and reconstructs the "simplest" image that is consistent with the selected pixels. The reconstructed image does not contain the adversarial perturbations because these perturbations tend to be small and localized. Specifically, we first select a random set of pixels by sampling a Bernoulli random variable X(i, j, k) for each pixel location (i, j, k); we maintain a pixel when X(i, j, k) = 1. Next, we use total variation minimization to constructs an image z that is similar to the (perturbed) input image x for the selected set of pixels, whilst also being "simple" in terms of total variation by solving:</p><formula xml:id="formula_7">min z (1 − X) (z − x) 2 + λ TV • TV p (z).<label>(6)</label></formula><p>Herein, denotes element-wise multiplication, and TV p (z) represents the L p -total variation of z:</p><formula xml:id="formula_8">TV p (z) = K k=1   N i=2 z(i, :, k) − z(i − 1, :, k) p + N j=2 z(:, j, k) − z(:, j − 1, k) p   . (7)</formula><p>The total variation (TV) measures the amount of fine-scale variation in the image z, as a result of which TV minimization encourages removal of small (adversarial) perturbations in the image. The objective function ( <ref type="formula" target="#formula_7">6</ref>) is convex in z, which makes solving for z straightforward. In our implementation, we set p = 2 and employ a special-purpose solver based on the split Bregman method <ref type="bibr" target="#b12">(Goldstein &amp; Osher, 2009)</ref> to perform total variance minimization efficiently.</p><p>The effectiveness of TV minimization is illustrated by the images in the middle column of Figure <ref type="figure" target="#fig_1">2</ref>: in particular, note that the adversarial perturbations that were present in the background for the nontransformed image (see bottom-left image) have nearly completely disappeared in the TV-minimized adversarial image (bottom-center image). As expected, TV minimization also changes image structure in non-homogeneous regions of the image, but as these perturbations were not adversarially designed we expect the negative effect of these changes to be limited. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">IMAGE QUILTING</head><p>Image quilting <ref type="bibr" target="#b9">(Efros &amp; Freeman, 2001</ref>) is a non-parametric technique that synthesizes images by piecing together small patches that are taken from a database of image patches. The algorithm places appropriate patches in the database for a predefined set of grid points, and computes minimum graph cuts <ref type="bibr" target="#b3">(Boykov et al., 2001)</ref> in all overlapping boundary regions to remove edge artifacts.</p><p>Image quilting can be used to remove adversarial perturbations by constructing a patch database that only contains patches from "clean" images (without adversarial perturbations); the patches used to create the synthesized image are selected by finding the K nearest neighbors (in pixel space) of the corresponding patch from the adversarial image in the patch database, and picking one of these neighbors uniformly at random. The motivation for this defense is that the resulting image only consists of pixels that were not modified by the adversary -the database of real patches is unlikely to contain the structures that appear in adversarial images.</p><p>The right-most column of Figure <ref type="figure" target="#fig_1">2</ref> illustrates the effect of image quilting on adversarial images. Whilst interpretation of these images is more complicated due to the quantization errors that image quilting introduces, it is interesting to note that the absolute differences between quilted original and the quilted adversarial image appear to be smaller in non-homogeneous regions of the image. This suggests that TV minimization and image quilting lead to inherently different defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We performed five experiments to test the efficacy of our defenses. The experiment in Section 5.2 considers gray-box attacks: it applies the defenses on adversarial images before using them as input into a convolutional network trained to classify "clean" images. In this setting, the adversary has access to the model architecture and parameters but is unaware of the defense strategy. The experiment in Section 5.3 focuses on a black-box setting: it replaces the convolutional network by networks that were trained on images with a particular input-transformation. The experiment in Section 5.4 combines our defenses with ensembling and model transfer. The experiment in Section 5.5 investigates to what extent networks trained on image-transformations can be attacked in a gray-box setting. The experiment in Section 5.6 compares our defenses with prior work. The setup of our gray-box and black-box experiments is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. Code to reproduce our results is available at https://github.com/facebookresearch/adversarial_image_defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENTAL SETUP</head><p>We performed experiments on the ImageNet image classification dataset. The dataset comprises 1.2 million training images and 50, 000 test images that correspond to one of 1, 000 classes. Our adversarial images are produced by attacking a ResNet-50 model <ref type="bibr" target="#b15">(He et al., 2016)</ref>. We evaluate our defense strategies against the four adversarial attacks presented in Section 3. We measure the strength of an adversary in terms of its normalized L 2 -dissimilarity and report classification accu- racies as a function of the normalized L 2 -dissimilarity. To produce adversarial images like those in Figure <ref type="figure" target="#fig_0">1</ref>, we set the normalized L 2 -dissimilarity for each of the attacks as follows:</p><p>• FGSM. Increasing the step size increases the normalized L 2 -dissimilarity.</p><p>• I-FGSM. We fix M = 10, and increase to increase the normalized L 2 -dissimilarity.</p><p>• DeepFool. We fix M = 5, and increase to increase the normalized L 2 -dissimilarity.</p><p>• CW-L2. We fix κ = 0 and λ f = 10, and multiply the resulting perturbation by an appropriately chosen ≥ 1 to alter the normalized L 2 -dissimilarity.</p><p>We fixed the hyperparameters of our defenses in all experiments: specifically, we set pixel dropout probability p = 0.5 and the regularization parameter of the total variation minimizer λ TV = 0.03. We use a quilting patch size of 5×5 and a database of 1, 000, 000 patches that were randomly selected from the ImageNet training set. We use the nearest neighbor patch (i.e., K = 1) for experiments in Sections 5.2 and 5.3, and randomly select a patch from one of K = 10 nearest neighbors in all other experiments. In the cropping defense, we sample 30 crops of size 90×90 from the 224×224 input image, rescale the crops to 224×224, and average the model predictions over all crops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GRAY BOX: IMAGE TRANSFORMATIONS AT TEST TIME</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the top-1 accuracy of a ResNet-50 tested on transformed adversarial images as a function of the adversary strength for each of the four attacks. Each plot shows results for five different transformations we apply to the images at test time (viz., image cropping-rescaling, bitdepth reduction, JPEG compression, total variation minimization, and image quilting). The dotted line shows the classification error of the ResNet-50 model on images that are not adversarially perturbed, i.e., it gives an upper bound on the accuracy that defenses can achieve.</p><p>In line with the results reported in the literature, the four adversaries successfully attack the ResNet-50 model in nearly all cases (FGSM has a slightly lower favorable attack rate of 80−90%) when the input images are not transformed. The results also show that the proposed image transformations are capable of partly eliminating the effect of the attacks. In particular, ensembling 30 predictions over different, random image crops is very efficient: these predictions are correct for 40−60% of the images (note that 76% is the highest accuracy that one can expect to achieve). This result suggests that adversarial examples are susceptible to changes in the location and scale of the adversarial perturbations. While not as effective, image transformations based on total variation minimization and image quilting also successfully defend against adversarial examples from all four attacks: applying these transformations allows us to classify 30−40% of the images correctly. This result suggests that total variation minimization and image quilting can successfully remove part of the perturbations from adversarial images. In particular, the accuracy of the image-quilting defense hardly deteriorates as the strength of the adversary increases. However, the quilting transformation does severely impact the model's accuracy on non-adversarial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">BLACK BOX: IMAGE TRANSFORMATIONS AT TRAINING AND TEST TIME</head><p>The high relative performance of image cropping-rescaling in 5.2 may be partly explained by the fact that the convolutional network was trained on randomly cropped-rescaled images<ref type="foot" target="#foot_1">2</ref> , but not on any of the other transformations. This implies that independent of whether an image is adversarial or not, the network is more robust to image cropping-rescaling than it is to those transformations. The results in Figure <ref type="figure" target="#fig_3">4</ref> suggest that this negatively affects the effectiveness of these defenses, even if the defenses are successful in removing the adversarial perturbation. To investigate this, we trained ResNet-50 models on transformed ImageNet training images. We adopt the standard data augmentation from <ref type="bibr" target="#b15">He et al. (2016)</ref>, but apply bit-depth reduction, JPEG compression, TV minimization, or image quilting on the resized image crop before feeding it to the network. We measure the classification accuracy of the resulting networks on the same adversarial images as before. Note that this implies that we assume a black-box setting in this experiment. We present the results of these experiments in Figure <ref type="figure" target="#fig_4">5</ref>. Training convolutional networks on images that are transformed in the same way as at test time, indeed, dramatically improves the effectiveness of all transformation defenses. In our experiments, the image-quilting defense is particularly effective against strong attacks: it successfully defends against 80−90% of all four attacks, even when the normalized L 2 -dissimilarity of the attack approaches 0.08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">BLACK BOX: ENSEMBLING AND MODEL TRANSFER</head><p>We evaluate the efficacy of (1) ensembling different defenses and ( <ref type="formula" target="#formula_3">2</ref>) "transferring" attacks to different network architectures (in a black-box setting). Specifically, we measured the accuracy of four networks using ensembles of defenses on adversarial images generated to attack a ResNet-50; the four networks we consider are ResNet-50, ResNet-101, DenseNet-169 <ref type="bibr" target="#b16">(Huang et al., 2017)</ref>, and Inception-v4 <ref type="bibr" target="#b31">(Szegedy et al., 2017)</ref>. To ensemble the image quilting and TVM defenses, we average the image-quilting prediction (using a weight of 0.5) with model predictions for 10 different TVM reconstructions (with a weight of 0.05 each), re-sampling the pixels used to measure the reconstruction error each time. To combine cropping with other transformations, we first apply those transformations and average predictions over 10 random crops from the transformed images.</p><p>The results of our ensembling experiments are presented in Table <ref type="table" target="#tab_0">1</ref>. The results show that gains of 1 − 2% in classification accuracy can be achieved by ensembling different defenses, whereas transferring attacks to different convolutional network architectures can lead to an improvement of 2−3%. Inception-v4 performs best in our experiments, but this may be partly due to that network having a higher accuracy even in non-adversarial settings. Our best black-box defense achieves an accuracy of about 71% against all four defenses: the attacks deteriorate the accuracy of our best classifier (which combines cropping, TVM, image quilting, and model transfer) by at most 6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">GRAY BOX: IMAGE TRANSFORMATIONS AT TRAINING AND TEST TIME</head><p>The previous experiments demonstrated the effectiveness of image transformations against adversarial images, in particular, when convolutional networks are re-trained to be robust to those image transformations. In this experiment, we investigate to what extent the resulting networks can be attacked in a gray-box setting in which the adversary has access to those networks (but does not have access to the input transformations applied at test time). We use the four attack methods to generate novel adversarial images against the transformation-robust networks trained in 5.3, and measure the accuracy of the networks on these novel adversarial images in Figure <ref type="figure" target="#fig_5">6</ref>.</p><p>The results show that bit-depth reduction and JPEG compression are weak defenses in such a graybox setting. Whilst their relative ordering varies between attack methods, image cropping and rescaling, total variation minimization, and image quilting are fairly robust defenses in the white-box setting. Specifically, networks using these defenses classify up to 50% of adversarial images correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">COMPARISON WITH PRIOR WORK</head><p>In our final set of experiments, we compare our defenses with the state-of-the-art ensemble adversarial training approach proposed by <ref type="bibr" target="#b32">Tramèr et al. (2017)</ref>. Ensemble adversarial training fits the parameters of a convolutional network on adversarial examples that were generated to attack an ensemble of pre-trained models. These adversarial examples are very diverse, which makes the convolutional network being trained robust to a variety of adversarial perturbation. In our experiments, we used the model released by <ref type="bibr" target="#b32">Tramèr et al. (2017)</ref>: an Inception-Resnet-v2 <ref type="bibr" target="#b30">(Szegedy et al., 2016)</ref> trained on adversarial examples generated by FGSM against Inception-Resnet-v2 and Inception-v3 models. We compare the model to our ResNet-50 models with image cropping, total variance minimization, and image quilting defenses. We note that there are two small differences in terms of the assumptions that ensemble adversarial training makes and the assumptions our defenses make: (1) in contrast to ensemble adversarial training, our defenses assume that part of the defense strategy (viz., the input transformation) is unknown to the adversary, and (2) in contrast to ensemble adversarial training, our defenses assume no prior knowledge of the attacks being used. The former difference is advantageous to our defenses, whereas the latter difference gives our defenses a disadvantage compared to ensemble adversarial training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>The results from this study suggest there exists a range of image transformations that have the potential to remove adversarial perturbations while preserving the visual content of the image: one merely has to train the convolutional network on images that were transformed in the same way. A critical property that governs which image transformations are most effective in practice is whether Table <ref type="table" target="#tab_1">2</ref>: Top-1 classification accuracy on images perturbed using attacks against ResNet-50 models trained on input-transformed images, and an Inception-v4 model trained using ensemble adversarial. Adversarial images are generated by running attacks against the models, aiming for an average normalized L 2 -dissimilarity of 0.06. The best defense against each attack is typeset in boldface.</p><p>an adversary can incorporate the transformation in its attack. For instance, median filtering likely is a weak remedy because one can backpropagate through the median filter, which is sufficient to perform any of the attacks described in Section 3. A strong input-transformation defense should, therefore, be non-differentiable and randomized, a strategy has been previously shown to be effective <ref type="bibr" target="#b33">(Wang et al., 2016a;</ref><ref type="bibr">b)</ref>. Two of our top defenses possess both properties:</p><p>1. Both total variation minimization and image quilting are difficult to differentiate through. Specifically, total variation minimization involves solving a complex minimization of a function that is inherently random. Image quilting involves a discrete variable that selects the patch from the database, which is a non-differentiable operation, and the graph-cut optimization complicates the use of differentiable approximations <ref type="bibr" target="#b22">(Maddison et al., 2017)</ref>.</p><p>2. Both total variation minimization and image quilting give rise to randomized defenses. Total variation minimization randomly selects the pixels it uses to measure reconstruction error on when creating the denoised image. Image quilting randomly selects one of the K nearest neighbors uniformly at random. The inherent randomness of our defenses makes it difficult to attack the model: it implies the adversary has to find a perturbation that alters the prediction for the entire distribution of images that could be used as input, which is harder than perturbing a single image <ref type="bibr" target="#b25">(Moosavi-Dezfooli et al., 2017)</ref>.</p><p>Our results with gray-box attacks suggest that randomness is particularly important in developing strong defenses. Therefore, we surmise that total variation minimization, image quilting, and related methods <ref type="bibr" target="#b7">(Dong et al., 2011)</ref> are stronger defenses than deterministic denoising procedures such as bit-depth reduction, JPEG compression, or non-local means <ref type="bibr" target="#b4">(Buades, 2005)</ref>. Defenses based on total variation minimization and image quilting also have an advantage over adversarial-training approaches <ref type="bibr" target="#b18">(Kurakin et al., 2016a)</ref>: an adversarially trained network is differentiable, which implies that it can be attacked using the methods in Section 3. An additional disadvantage of adversarial training is that it focuses on a particular attack; by contrast, transformation-based defenses generalize well across attack methods because they are model-agnostic.</p><p>While our study focuses exclusively on image classification, we expect similar defenses to be useful in other domains for which successful attacks have been developed, such as semantic segmentation and speech recognition <ref type="bibr" target="#b5">(Cisse et al., 2017a;</ref><ref type="bibr" target="#b36">Zhang et al., 2017)</ref>. In speech recognition, for example, total variance minimization can be used to remove perturbations from waveforms, and one could develop "spectrogram quilting" techniques that reconstruct a spectrogram by concatenating "spectrogram patches" along the temporal dimension. We leave such extensions to future work. In future work, we also intend to study combinations of our input-transformation defenses with ensemble adversarial training <ref type="bibr" target="#b32">(Tramèr et al., 2017)</ref>, and we intend to investigate new attack methods that are specifically designed to circumvent our input-transformation defenses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FGSMFigure 1 :</head><label>1</label><figDesc>Figure 1: Adversarial images and corresponding perturbations at five levels of normalized L 2dissimilarity for all four attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of total variance minimization and image quilting applied to an original and an adversarial image (produced using I-FGSM with = 0.03, corresponding to a normalized L 2dissimilarity of 0.075). From left to right, the columns correspond to: (1) no transformation, (2) total variance minimization, and (3) image quilting. From top to bottom, rows correspond to: (1) the original image, (2) the corresponding adversarial image produced by I-FGSM, and (3) the absolute difference between the two images above. Difference images were multiplied by a constant scaling factor to increase visibility.</figDesc><graphic url="image-47.png" coords="4,319.16,235.12,60.18,60.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Block diagram detailing the differences between the experimental setups in Section 5.2, 5.3, and 5.4. We train networks (a) on regular images or (b) on transformed images; we test the networks on transformed adversarial images. For each of the three setups, dashed arrows indicate which model is used by the adversary and which model is used by the classification model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FGSMFigure 4 :</head><label>4</label><figDesc>Figure 4: Top-1 classification accuracy of ResNet-50 tested on transformed adversarial images produced by four attacks using five image transformations in a gray-box setting: (1) cropping-rescaling, (2) bit-depth reduction, (3) JPEG compression, (4) total variance minimization, and (5) image quilting. The dotted line shows the top-1 accuracy of the ResNet-50 model on non-adversarial images, providing an upper bound on the effectiveness of a defense. An L 2 -dissimilarity of 0.00 corresponds to the classification accuracy on non-adversarial images. Higher is better.</figDesc><graphic url="image-62.png" coords="6,313.24,204.41,141.26,105.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FGSMFigure 5 :</head><label>5</label><figDesc>Figure 5: Top-1 classification accuracy of ResNet-50 trained and tested on transformed adversarial images produced by four attacks using five image transformations in a black-box setting: (1) cropping-rescaling, (2) bit-depth reduction, (3) JPEG compression, (4) total variance minimization, and (5) image quilting. The dotted line represents the top-1 accuracy of the ResNet-50 model on nonadversarial images, providing an upper bound on the effectiveness of a defense. An L 2 -dissimilarity of 0.00 corresponds to the classification accuracy on non-adversarial images. Higher is better.</figDesc><graphic url="image-66.png" coords="7,313.24,204.41,141.26,105.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FGSMFigure 6 :</head><label>6</label><figDesc>Figure 6: Top-1 classification accuracy of ResNet-50 trained and tested on transformed adversarial images produced by four attacks using five image transformations in a gray-box setting: (1) cropping-rescaling, (2) bit-depth reduction, (3) JPEG compression, (4) total variance minimization, and (5) image quilting. The dotted line represents the top-1 accuracy of the ResNet-50 model on non-adversarial images, providing an upper bound on the effectiveness of a defense. L 2 -dissimilarity of 0 corresponds to clean image accuracy. Higher is better.</figDesc><graphic url="image-70.png" coords="9,313.24,204.41,141.26,105.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Top-1 classification accuracy of ensemble and model transfer defenses (columns) against four black-box attacks (rows). The four networks we use to classify images are ResNet-50 (RN50), ResNet-101 (RN101), DenseNet-169 (DN169), and Inception-v4 (Iv4). Adversarial images are generated by running attacks against the ResNet-50 model, aiming for an average normalized L 2dissimilarity of 0.06. Higher is better. The best defense against each attack is typeset in boldface.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Quilting</cell><cell></cell><cell></cell><cell cols="2">TVM + Quilting</cell><cell></cell><cell cols="4">Cropping + TVM + Quilting</cell></row><row><cell></cell><cell cols="3">RN50 RN101 DN169</cell><cell>Iv4</cell><cell cols="3">RN50 RN101 DN169</cell><cell>Iv4</cell><cell cols="3">RN50 RN101 DN169</cell><cell>Iv4</cell></row><row><cell>No Attack</cell><cell>70.07</cell><cell>72.56</cell><cell>70.18</cell><cell>73.01</cell><cell>72.38</cell><cell>74.74</cell><cell>73.10</cell><cell>75.55</cell><cell>72.14</cell><cell>74.53</cell><cell>72.92</cell><cell>75.10</cell></row><row><cell>FGSM</cell><cell>65.45</cell><cell>68.50</cell><cell>65.96</cell><cell>67.53</cell><cell>65.70</cell><cell>68.77</cell><cell>67.09</cell><cell>69.19</cell><cell>66.65</cell><cell>69.75</cell><cell>67.86</cell><cell>70.37</cell></row><row><cell>I-FGSM</cell><cell>65.59</cell><cell>68.72</cell><cell>66.16</cell><cell>69.29</cell><cell>65.84</cell><cell>69.10</cell><cell>67.32</cell><cell>71.05</cell><cell>67.03</cell><cell>70.14</cell><cell>68.20</cell><cell>71.52</cell></row><row><cell>DeepFool</cell><cell>65.20</cell><cell>68.73</cell><cell>65.86</cell><cell>68.70</cell><cell>65.80</cell><cell>69.34</cell><cell>67.40</cell><cell>71.03</cell><cell>67.11</cell><cell>70.49</cell><cell>68.62</cell><cell>71.47</cell></row><row><cell>CW-L2</cell><cell>64.11</cell><cell>67.72</cell><cell>65.00</cell><cell>68.14</cell><cell>63.99</cell><cell>68.20</cell><cell>66.08</cell><cell>70.13</cell><cell>65.31</cell><cell>69.14</cell><cell>66.96</cell><cell>70.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>compares the classification accuracies of the defense strategief on adversarial examples with a normalized L 2 -dissimilarity of 0.06. The results show that ensemble adversarial training works better on FGSM attacks (which it uses at training time), but is outperformed by each of the transformation-based defenses all other attacks. Input transformations particularly outperform ensemble adversarial training against the iterative attacks: our defense are are 18−24× more robust than ensemble adversarial training against DeepFool attacks. Combining cropping, TVM, and quilting increases the accuracy of our defenses against DeepFool gray-box attacks to 51.51% (compared to 1.84% for ensemble adversarial training).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Given a target class c, a targeted adversarial example x is an example that satisfies h(x ) = c. We do not consider targeted attacks in this study.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We trained the ResNet-50 model using the data-augmentation scheme of<ref type="bibr" target="#b15">He et al. (2016)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Kilian Weinberger, Iasonas Kokkinos, Changhan Wang, and the entire Facebook AI Research team for helpful discussions and code support. Chuan Guo is supported in part by NSF grant IIS-1618134.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work was performed whilst Chuan Guo was at Facebook AI Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Speech 2: End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<idno>CoRR, abs/1512.02595</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECML</title>
				<meeting>ECML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end learning for self-driving cars</title>
		<author>
			<persName><forename type="first">Mariusz</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><forename type="middle">Del</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beat</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasoon</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urs</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiakai</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/1604.07316</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2005">2005. 2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Houdini: Fooling deep structured prediction models</title>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
		<idno>CoRR, abs/1707.05373</idno>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno>CoRR, abs/1704.08847</idno>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centralized sparse representation for image restoration</title>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1259" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A study of the effect of JPG compression on adversarial images</title>
		<author>
			<persName><forename type="first">Gintare</forename><surname>Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Roy</surname></persName>
		</author>
		<idno>CoRR, abs/1608.00853</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH</title>
				<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Analysis of classifiers&apos; robustness to adversarial perturbations</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno>CoRR, abs/1502.02590</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robustness of classifiers: From adversarial to random noise</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1632" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The split Bregman method for L1-regularized problems</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Imaging Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="323" to="343" />
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Assessing threat of adversarial examples on deep neural networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>Graese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andras</forename><surname>Rozsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<idno>CoRR, abs/1610.04256</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Auguste</forename><surname>Kerckhoffs</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Diederik Kingma and Jimmy Ba</title>
				<meeting><address><addrLine>IX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1883">1883. 2014</date>
			<biblScope unit="page" from="161" to="191" />
		</imprint>
	</monogr>
	<note>La cryptographie militaire</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1611.01236</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno>CoRR, abs/1611.02770</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">No need to worry about adversarial examples in object detection in autonomous vehicles</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Sibai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Fabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<idno>CoRR, abs/1707.03501</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Is deep learning safe for robot vision? adversarial examples against the icub humanoid</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambra</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
		<idno>CoRR, abs/1708.06939</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="86" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Understanding adversarial training: Increasing local stability of neural nets through robust optimization</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
		<idno>CoRR, abs/1511.05432</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno>CoRR, abs/1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adversary resistant deep neural networks with an application to malware detection</title>
		<author>
			<persName><forename type="first">Qinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1610.01239</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning adversary-resistant deep neural networks</title>
		<author>
			<persName><forename type="first">Qinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1612.01401</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<idno>CoRR, abs/1704.01155</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dolphinatack: Inaudible voice commands</title>
		<author>
			<persName><forename type="first">Guoming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Xu</surname></persName>
		</author>
		<idno>CoRR, abs/1708.09537</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
