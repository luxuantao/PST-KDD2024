<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Overfitting in Wrapper-Based Feature Subset Selection: The Harder You Try the Worse it Gets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><surname>Loughrey</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<addrLine>Dublin 2</addrLine>
									<settlement>College Green</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Padraig</forename><surname>Cunningham</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<addrLine>Dublin 2</addrLine>
									<settlement>College Green</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Overfitting in Wrapper-Based Feature Subset Selection: The Harder You Try the Worse it Gets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">841EAD1021949CF9464772E8D0511271</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>{John</term>
					<term>Loughrey, Padraig</term>
					<term>Cunningham}@ cs</term>
					<term>ted</term>
					<term>ie</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Wrapper based feature selection, the more states that are visited during the search phase of the algorithm the greater the likelihood of finding a feature subset that has a high internal accuracy while generalizing poorly. When this occurs, we say that the algorithm has overfitted to the training data. We outline a set of experiments to show this and we introduce a modified genetic algorithm to address this overfitting problem by stopping the search before overfitting occurs. This new algorithm called GAWES (Genetic Algorithm With Early Stopping) reduces the level of overfitting and yields feature subsets that have a better generalization accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The benefits of wrapper-based techniques for feature selection are well established <ref type="bibr" target="#b0">[ 1,</ref><ref type="bibr" target="#b14">15]</ref>. However, it has recently been recognized that wrapper-based techniques have the potential to overfit the training data <ref type="bibr" target="#b1">[2]</ref>. That is, feature subsets that perform well on the training data may not perform as well on data not used in the training process. Furthermore, the extent of the overfitting is related to the depth of the search. Reunanen <ref type="bibr" target="#b1">[2]</ref> shows that, whereas Sequential Forward Floating Selection (SFFS) beats Sequential Forward Selection on the data used in the training process, the reverse is true on hold-out data. He argues that this is because SFFS is a more intensive search process i.e. it explores more states.</p><p>In this paper we present further evidence of this and explore the use of the number of states explored in the search as an indicator of the depth of the search and thus as a predictor of overfitting. Clearly this metric does not tell the whole story since for example a lengthy random search will not overfit at all.</p><p>We also explore a solution to this overfitting problem. Techniques from Machine Learning research for tackling overfitting include: -Post-Pruning: Overfitting can be eliminated by pruning as is done in the construction of Decision Trees <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Wrapper-Based Feature Subset Selection</head><p>Feature selection is defined as the selection of a subset of features to describe a phenomenon from a larger set that may contain irrelevant or redundant features. Improving classifier performance and accuracy are usually the motivating factors behind this, as the accuracy is degraded by the presence of these irrelevant features.</p><p>The curse of dimensionality is the term given to the phenomenon when there are too many features in the model and not enough instances to completely describe the target concept. Feature selection attempts to identify and eliminate unnecessary features, thereby reducing the dimensionality of the data, and hopefully resulting in an increase in accuracy.</p><p>The two common approaches to feature selection are the use of filters and the wrapper method. Filtering techniques attempt to identify features that are related to or predictive of the outcome of interest: they operate independently of the learning algorithm. An example is Information Gain, which was originally introduced to Machine Learning research by Quinlan as a criterion for building concise decision trees <ref type="bibr" target="#b5">[6]</ref> but it is now widely used for feature selection in general. The wrapper approach differs in that it evaluates subsets based upon the accuracy estimates provided by a classifier built with that feature subset. Thus wrappers are much more computationally expensive that filters but can produce better results because they take the bias of the classifier into account and evaluate features in context. A detailed presentation of the wrapper approach can be found in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search Algorithms</head><p>The wrapper method can be viewed as a search optimization process and therefore can incur a high computational cost. From n features, the number of possible feature subsets is T, so it is impractical to search the whole state space except in situations with a small number of features. The search strategies available can be classed into three categories; randomized, sequential and exhaustive, depending on the order in which they evaluate the subsets. In this research we only experiment with randomized and sequential techniques as an exhaustive search is infeasible in most domains. The algorithms we use are forward selection, backward elimination, hill climbing and a genetic algorithm as these tend to be quite popular and are easily implemented strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Problem of Overfitting</head><p>A classifier is said to overfit to a dataset if it models the training data too closely and gives poor predictions on new data. This occurs when there is insufficient data to train the classifier and the data does not fully cover the concept being learned. Such models are said to have a high variance, meaning that small changes in this data will have a significant influence on the resulting model <ref type="bibr" target="#b7">[8]</ref>. This is a problem for many real world situations where the data available may be quite noisy. Overfitting in feature selection appears to be exacerbated by the intensity of the search since the more feature subsets that are visited the more likely the search is to find a subset that overfits <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. hi <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> this problem is described, although little is said on how it can be addressed. However, we believe that limiting the extent of search will help combat overfitting. Kohavi et a/ <ref type="bibr" target="#b9">[10]</ref> describe the feature weighting algorithm DIET, in which the set of possible feature weights can be restricted. Their experiments show that when DIET is restricted to two non-zero weights the resultant models perform better than when the algorithm allows for a larger set of feature weights, in situations when the training data is limited. This restriction on the possible set of values in turn restricts the extent to which the algorithm can search. However, in feature selection we only have two possible weights, a feature can only have a value of T or '0' i.e. be turned 'on' or 'off, so we cannot restrict this aspect any further. Perhaps counter-intuitively, restricting the number of nodes visited by the feature selection algorithm should help further.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows accuracies obtained during a feature selection search using a genetic algorithm for the hand dataset (see Table <ref type="table" target="#tab_0">1</ref>). We could expect the search to suffer from overfitting at any point after generation 17 in the search. In this example, we see a typical demonstration of overfitting where we see a peak in the generalization performance early on with a gradual deterioration in performance after that. Our experiments begin with an initial investigation into the correlation between the depth of search and the associated level of overfitting. We compare the algorithms mentioned in Section 2.1 using a 10-fold Cross Validation Accuracy on a 3-Nearest Neighbor classifier.</p><p>The graphs in Figure <ref type="figure" target="#fig_1">2</ref> supports the hypothesis that the more nodes that are evaluated in the subspace search the more likely it is to find a subset that overfits and performs poorly on the test set. Hill Climbing is the least intensive search in each example and as a result has the poorest internal and test set accuracy in most cases. This shows this algorithm's tendency to under-fit the training data, probably getting stuck in a local maximum. The FS and BE searches perform quite similarly over all datasets, and it is interesting to note that they examine a similar number of nodes in most cases. The research into the comparative performances of these strategies have been inconclusive <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Our results are not much different. BE tends to be a little more intensive but on the datasets we show here, this does not result in it overfitting to a greater extent. One could expect that any difference in these strategies is dependent on the dataset used. In five of the seven datasets the GA explores the most states and is outperformed by both FS and BE in all of these cases. While one may have expected this more intensive strategy to yield higher generalization accuracies, the graphs show that this is clearly not the case. Moreover, on the two datasets that the GA evaluates fewer nodes, it performance is more competitive with the FS and BE algorithms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Early Stopping in Stochastic Search</head><p>The idea of implementing early stopping in our search is an appealing one. The method is widely understood, and easy to implement. In neural networks the training process is stopped once the generalization accuracy starts to drop. This generalization performance is obtained by withholding a sample of the data (the validation set). A major drawback of withholding data from the training process for use in early stopping is that overfitting arises in situations where the data available provides inadequate coverage of the phenomenon. In such situations, we can ill afford to withhold data from the training process.</p><p>The strategy we adopt here (see Figure <ref type="figure">4</ref>) is to start with a cross validation process to determine when overfitting occurs <ref type="bibr" target="#b8">[9]</ref>. Then all the training data is used to guide the search, with the search stopping at the point determined in the cross-validation. In order to determine if this actually does address overfitting, our evaluation involves wrapping this process in an outer cross validation that gives a good assessment of the overall generalization accuracy. The overall evaluation process is shown in Figure <ref type="figure">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">AETjig] i-accuracy of mask Mj[g] on training data ETj {i.e. fitness} Step 1.3. AEGjig] &lt;-accuracy of mask Mig] on validation data E, using ETj as training set Step AET[g] i-Ayer3Qe{AETJ[g]) {Accuracy on training data at each gen.} AEGfg] &lt;-AyerageiAEGlg]) {Accuracy on validation data at each gen.} Step 3. sg 4-generation with highest AEG[g] {the stopping point}</head><p>Step 4.</p><p>Using GA and Ff, find best feature mask Mlsg] for generation sg Step 5.</p><p>Return Mlsg] Fig. <ref type="figure">4</ref> Inner cross validation, determining the generation for early stopping.</p><p>From this evaluation we can estimate when overfitting will occur once the generalization performance starts to fall off Once we have this estimate we can then rerun the algorithm with new parameters that will stop the search before overfitting starts.</p><p>Deciding when to stop is not such a straightforward task. In <ref type="bibr" target="#b13">[14]</ref> a number of different criteria for early stopping are discussed and it is suggested that allowing the condition to be biased towards the latter stages of the search will yield small improvements in generalization accuracy. This said however, if we delay too much we run the risk of overfitting once again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The GAWES Approach Using a Genetic Algorithm</head><p>The GAWES algorithm was developed using the FIONN workbench <ref type="bibr" target="#b12">[13]</ref>. The algorithm is based upon the standard GA and the fitness of each individual is calculated from a 10-fold Cross Validation measure. Once the fitness has been calculated, the evolutionary strategy is based upon the Roulette Wheel technique, where the probability of an individual being selected for the new generation is related to its fitness. We use a two point crossover operation and the probability of a mutation occurring is 0.05 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>After a series of preliminary experiments we decided to fix the population size for the GA to 20, with the number of generations set to 100. We arrived at this after taking into consideration the length of time it took to execute the algorithm, the performance of the end mask along with the rate at which the population converged. The purpose of the experiments was to determine the gen limit for each dataset -the generation after which the genetic algorithm should be stopped.  Fig. <ref type="figure">5</ref> The graphs above show the results of running GAWES on 9 datasets. The x axis represents the generation count, while the&gt;^ axis is the accuracy.</p><p>The results obtained are shown in Figure <ref type="figure">5</ref>. The graphs represent 90% of the total data available, where 81% was used in the internal accuracy measure and 9% was used for the test set accuracy. The remaining 10% of data was withheld for the evaluation of the GAWES algorithm. All graphs are averaged over 100 runs of the genetic algorithm, where each run is performed on a different sample of the data. From these run we were able to generate a trend line of the test set accuracy, based upon of measure of a nine-point moving average. Using a smoothed average we have a more reliable indication of the best point for early stopping. The stopping point was chosen as the point at which this test set trend line is at the maximum value.</p><p>The graphs in Fig <ref type="figure">5</ref> show classical overfitting to different extents in five of the seven datasets; that is, there is an increase in test set accuracy followed by a gradual deterioration. In the breast dataset we see that the hold-out test set accuracy starts to deteriorate from the first generation and never seems to recover. From this behavior we assume that feature selection in these datasets will not lead to an increase in accuracy. The test set accuracy of ionosphere degrades in a similar manner but starts to improve in fitness after the 8^ generation and peaks at around the 11^^ before overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Having obtained an estimated genjimit from Figure <ref type="figure">5</ref> we can re-run our GA to evaluate the GAWES performance. As with other early stopping techniques, GAWES is only successful if the generalization of the end point is higher than the result that would otherwise be obtained. Another characteristic of these techniques is that the internal accuracy will be lower because the potential to overfit has been constrained. Figure <ref type="figure" target="#fig_4">6</ref> and Table <ref type="table" target="#tab_3">2</ref> show the results.</p><p>The results are much as we expected. The number of states evaluated by the classifier is greatly reduced as indicated by the line in the graphs. It is also shown that our algorithm does not suffer from overfitting as much as the standard GA and in six of the seven datasets our GAWES algorithm beats the longer, more intensive search. We believe that in the case where GAWES failed (zoo), this failure was due to a small number of cases per class in the dataset. Dividing smaller datasets further, as is required in our algorithm leads to a high variance between successive training and test sets which makes is more difficult to get an accurate estimate of when overfitting occurs.</p><p>These results are consistent with our suggestion that the harder you try in wrapperbased feature subset selection, the worse it gets when the number of training cases is limited. By reducing the length of time that the GA is allowed to run, we limit the number of subsets it can evaluate, thus reducing the depth of the search. Our results provide clear evidence that early stopping can help to reduce the amount of overfitting. The improvements in some of the results could probably be increased further if work were done on other aspects of the GA.  </p><note type="other">5 Future Work</note><p>At this stage we feel we have estabhshed the principle that early stopping can be effective in addressing overfitting in feature subset selection. The next stage of this research is to perform experiments on many more datasets to get a clear picture of the performance of the early stopping algorithm. Our experiments so far have been done with a one-size-fits-all GA and it seems clear that the parameters of the GA need to be tuned to the characteristics of the data. Some of the results shown here might have been improved if we had chosen other parameters for the GA, as our better results were on datasets that had fewer features. This was probably due to our choice of population size. The population size remained constant across the experiments so that the effect of early stopping could be examined under equal conditions. It would be interesting to look into this further, whether it means working with the GA more or indeed moving to another stochastic technique such as Simulated Annealing (SA). Simulated annealing has been inspired by statistical mechanics and is similar to the standard Hill Climbing search, but differs in that it is able to accept decreases in the fitness. The search is modeled on the cooling of metals and so the probability of accepting a decrease in fitness is based upon the current temperature of the system (an artificial variable). The temperature of the system is high at the beginning but slowly cools as the search progresses, therefore significant decreases in fitness are more likely to be accepted early in the search process when the temperature of the system is high, but are less likely as the search progresses and the temperature gradually cools <ref type="bibr" target="#b16">[17]</ref>. This gives the search the ability to escape from local maxima that it would otherwise get trapped in early in the search. We feel implementing Early Stopping in the SA has promise as there are many ways in which one can restrict the length of the search e.g. by increasing the cooling rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Reunanen <ref type="bibr" target="#b1">[2]</ref> shows that overfitting is a problem in wrapper based feature selection.</p><p>Our preliminary experiments support this finding. We have proposed a mechanism for early stopping in stochastic search as a solution. Early stopping is a widely known and well understood method of avoiding overfitting in neural network training, and we are unaware of any other research that applies it to the feature selection. Genetic algorithms are often used in feature selection, although one major difficulty associated with them is parameter selection. The population size, generation limit, evolutionary technique, crossover and mutation values all have to be set, as these values are all dependent on the dataset being explored. It has been shown that the more the feature subspace is search the greater the chance there is of overfitting. By reducing the length of time that the GA is allowed to run, we limit the number of subsets it can evaluate, thus reducing the depth of the search. However, more work is needed to make the algorithm more competitive with existing feature-selection techniques. It is important to mention that overfitting does not always occur and finding datasets that demonstrated the effects of early stopping was difficult. We have an issue with the datasets available to us in that sometimes feature selection is not always necessary and as a result determining when to stop based upon a marginal increase in test set accuracy is not always reliable. Increasing the number of datasets is a major issue for future research. Moreover, the computational requirements of GAWES resulted in many searches taking days to execute which limited somewhat the number of results we could show.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 A</head><label>1</label><figDesc>Fig. 1 A comparison of the Internal and Test Set accuracy on the 'hand' dataset. A trend line is shown for the Test Set accuracy (dashed line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 The graphs above show the results for the preliminary experiments. FS -Forward Selection; BE -Backward Elimination; HC -Hill Climbing; GA -Genetic Algorithm. The left-hand side y' axis represents the classification accuracy. The right-hand side 'y' axis represents the number of states visited in the subspace search.</figDesc><graphic coords="5,52.09,170.03,334.32,259.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 . 2 . 1 . 3 .Fig. 3</head><label>12133</label><figDesc>Fig. 3 Outer cross-validation, determining the accuracy on training data AT and the generalization accuracy AG. Step 0. Divide the data set FT, into 10 folds, Ei... Eio Define ETj &lt;-E \ Ej {training set corresponding to validation set Ej} Stepl. For each fold y Step 1.1. Using GA and ETj find best feature mask Mjlg] for each generation g Step 1.2. AETjig] i-accuracy of mask Mj[g] on training data ETj {i.e. fitness} Step 1.3. AEGjig] &lt;-accuracy of mask Mig] on validation data E, using ETj as training set Step 2. AET[g] i-Ayer3Qe{AETJ[g]) {Accuracy on training data at each gen.} AEGfg] &lt;-AyerageiAEGlg]) {Accuracy on validation data at each gen.} Step 3. sg 4-generation with highest AEG[g] {the stopping point} Step 4.Using GA and Ff, find best feature mask Mlsg] for generation sg Step 5.Return Mlsg]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 The graphs above show the results for the GAWES algorithm. The line and the right side V axis represent the number of states visited during each search.</figDesc><graphic coords="9,53.74,56.02,332.88,268.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets used:</figDesc><table><row><cell>Name</cell><cell>Instances</cell><cell>Features</cell><cell>Source</cell></row><row><cell>Hand</cell><cell>63</cell><cell>13 (+1 Class)</cell><cell>http://www.cs.tcd.ie/research groups/mlg</cell></row><row><cell>Breast</cell><cell>273</cell><cell>9 (+1 Class)</cell><cell>UCI Repository</cell></row><row><cell>Sonar</cell><cell>208</cell><cell>60 (+ 1 Class)</cell><cell>UCI Repository</cell></row><row><cell>Ionosphere</cell><cell>351</cell><cell>35 (+ 1 Class)</cell><cell>UCI Repository</cell></row><row><cell>Diabetes</cell><cell>768</cell><cell>8 (+ 1 Class)</cell><cell>UCI Repository</cell></row><row><cell>Zoo</cell><cell>101</cell><cell>16 (+1 Class)</cell><cell>UCI Repository</cell></row><row><cell>Glass</cell><cell>214</cell><cell>9 (+ 1 Class)</cell><cell>UCI Repository</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>4 7 10 13 16 19 22 25 28 3134 37 40 4346 49</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Glass</cell></row><row><cell></cell><cell></cell><cell></cell><cell>77</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>76-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>75-</cell><cell>.. -.</cell><cell>...••</cell><cell>.</cell><cell>.•''•..-•-•</cell><cell>....</cell></row><row><cell></cell><cell></cell><cell></cell><cell>74-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>73-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>72-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>71 -</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>70.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>69 i</cell><cell></cell><cell></cell></row><row><cell>77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>76.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>73-72 H</cell><cell>V</cell><cell>^^</cell><cell>'-^</cell><cell></cell><cell></cell></row><row><cell>71 -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">1 4 7 10 13 16 19 22 25 28 31 34 37</cell><cell></cell><cell></cell></row></table><note><p>Breast v^^ -r r-^^ -^^^^ A.. • ^'\j^\n'^'^^isr'*^</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>V 1 5 9 13 17 21 25 29 33 37 41 45 49 53 1 4 7 10 13 16 19 22 25 28 31 34 37 Internal Accuracy Test Set Accuracy Trend Line (Test Set)</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Summary of results for the GAWES algorithm.</figDesc><table><row><cell></cell><cell>GA</cell><cell></cell><cell cols="2">GAWES</cell><cell>1</cell></row><row><cell></cell><cell>Internal</cell><cell>Test Set</cell><cell>Internal</cell><cell cols="2">Test Set 1</cell></row><row><cell>Hand</cell><cell>98.04</cell><cell>84.03</cell><cell>96.47</cell><cell>88.57</cell></row><row><cell>Breast</cell><cell>78.42</cell><cell>70.39</cell><cell>78.26</cell><cell cols="2">73.64 1</cell></row><row><cell>Sonar</cell><cell>90.70</cell><cell>83.7</cell><cell>90.12</cell><cell>84.12</cell></row><row><cell>ionosphere</cell><cell>93.634</cell><cell>89.74</cell><cell>92.81</cell><cell cols="2">90.32 1</cell></row><row><cell>diabetes</cell><cell>75.14</cell><cell>70.84</cell><cell>74.87</cell><cell>73.69</cell></row><row><cell>1 Zoo</cell><cell>96.37</cell><cell>93</cell><cell>94.38</cell><cell>91.18</cell></row><row><cell>Glass</cell><cell>77.51</cell><cell>71.42</cell><cell>77.36</cell><cell>72.87</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This research was funded by Science Foundation Ireland Grant No. SFI-02 IN. Ill 11</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overfitting in making comparisons between variable selection methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reunanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="371" to="1382" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature Selection: Evaluation, Application and Small Sample Performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zongker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature Subset Selection Using the Wrapper Method: Overfitting and E)&gt;&apos;namic Search Space Topology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sommerfield</surname></persName>
		</author>
		<idno>KDD-95</idno>
	</analytic>
	<monogr>
		<title level="m">First International Conference on Knowledge Discovery and Data Mining</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Feature Extraction, Construction and Selection: A Data Mining Perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
		<editor>H. Liu and H. Motoda</editor>
		<imprint>
			<publisher>Kluwer Academic Pubhshers</publisher>
			<biblScope unit="page" from="117" to="136" />
			<pubPlace>Massachusetts</pubPlace>
		</imprint>
	</monogr>
	<note>Feature Subset Selection using a genetic algorithm</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">L: Fundamentals of Neural Networks : architectures, algorithms,and applications</title>
		<author>
			<persName><surname>Fausett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Overfitting and Diversity in Classification Ensembles based on Feature Selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<idno>No. TCD-CS-2000-07</idno>
		<imprint/>
		<respStmt>
			<orgName>Department of Computer Science, Trinity College Dublin -</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The utility of feature weighting in nearest-neighbor algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning (ECML-97)</title>
		<meeting>the European Conference on Machine Learning (ECML-97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An Introduction to Genetic Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kernel regression and backpropagation training with noise</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koistinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Holmstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippman</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1033" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">FIONN: A Framework for Developing CBR Systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loughrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Coyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>to appear in Expert Update</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic Early Stopping Using Cross Validation: Quantifying the criteria</title>
		<author>
			<persName><forename type="first">L</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Comparative Evaluation of Sequential Feature Selection Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">;</forename><surname>Bankert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Articial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An evaluation of feature selection methods and their application to computer security</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doak</surname></persName>
		</author>
		<idno>CSE-92-18</idno>
		<imprint>
			<pubPlace>Davis, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimization by Simulated Annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">4598</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
