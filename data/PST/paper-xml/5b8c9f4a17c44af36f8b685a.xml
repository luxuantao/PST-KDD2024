<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-based Incentive</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiasi</forename><surname>Weng</surname></persName>
							<email>wengjiasi@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jian</forename><surname>Weng</surname></persName>
							<email>tjweng@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jilian</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiqi</forename><forename type="middle">Q</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology in Jinan University, and Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-based Incentive</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">31A9B799C6B304C84E64307CBB856B11</idno>
					<idno type="DOI">10.1109/TDSC.2019.2952332</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2019.2952332, IEEE Transactions on Dependable and Secure Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Privacy-preserving training</term>
					<term>Blockchain</term>
					<term>Incentive</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning can achieve higher accuracy than traditional machine learning algorithms in a variety of machine learning tasks. Recently, privacy-preserving deep learning has drawn tremendous attention from information security community, in which neither training data nor the training model is expected to be exposed. Federated learning is a popular learning mechanism, where multiple parties upload local gradients to a server and the server updates model parameters with the collected gradients. However, there are many security problems neglected in federated learning, for example, the participants may behave incorrectly in gradient collecting or parameter updating, and the server may be malicious as well. In this paper, we present a distributed, secure, and fair deep learning framework named DeepChain to solve these problems. DeepChain provides a value-driven incentive mechanism based on Blockchain to force the participants to behave correctly. Meanwhile, DeepChain guarantees data privacy for each participant and provides auditability for the whole training process. We implement a DeepChain prototype and conduct experiments on a real dataset for different settings, and the results show that our DeepChain is promising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R ECENT advances in deep learning based on artificial neural networks have witnessed unprecedented accuracy in various tasks, e.g., speech recognition <ref type="bibr" target="#b0">[1]</ref>, image recognition <ref type="bibr" target="#b1">[2]</ref>, drug discovery <ref type="bibr" target="#b2">[3]</ref> and gene analysis for cancer research <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. In order to achieve even higher accuracy, huge amount of data must be fed to deep learning models, incurring excessively high computational overhead <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. This problem, however, can be solved by employing distributed deep learning technique that has been investigated extensively in recent years. Unfortunately, privacy issue worsens in the context of distributed deep learning, as compared to conventional standalone deep learning scenario.</p><p>Privacy-preserving deep learning thus arises to deal with privacy concerns in deep learning, and various models have been around in the past few years <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Among these existing work, federated learning is the widely adopted system context. Federated learning, also known as collaborative learning, distributed learning, is essentially the combination of deep learning and distributed computation, where there is a server, called parameter server, maintaining a deep learning model to train and multiple parties that take part in the distributed training process. First, the training data is partitioned and stored at each of the parties. Then, each party trains a deep learning model (the same one as maintained at the parameter server) on her local data individually, and uploads intermediate gradients to the parameter server. Upon receipt of the gradients from all the parties, the parameter server aggregates those gradients and updates the learning model parameters accordingly, after which each of the parties downloads the updated parameters from the server and continues to train her model on the same local data again with the downloaded parameters. This training process repeats until the training errors are smaller than pre-specified thresholds.</p><p>This federated learning framework, however, cannot protect the privacy of the training data, even the training data is divided and stored separately. For example, some researchers show that the intermediate gradients can be used to infer important information about the training data <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Shokri et. al <ref type="bibr" target="#b10">[11]</ref> applied differential privacy technique by adding noises in the gradients to upload, achieving a trade-off between data privacy and training accuracy. Hitaj et. al <ref type="bibr" target="#b18">[19]</ref> pointed out that Shokri's work failed to protect data privacy and demonstrated that a curious parameter server can learn private data through GAN (Generative Adversarial Network) learning. Orekondy et. al <ref type="bibr" target="#b19">[20]</ref> exploited the intermediate gradients to launch linkability attack on training data, since the gradients contain sufficient data features.</p><p>Phong et. al <ref type="bibr" target="#b15">[16]</ref> proposed to use homomorphic encryption technique to protect training data privacy from curious parameter server. The drawback of their scheme is that they assumed the collaborative participants are honest but not curious, hence their scheme may fail in scenario where some participants are curious. To prevent curious participants, Bonawitz et. al <ref type="bibr" target="#b13">[14]</ref> employed a secret sharing and symmetric encryption mechanism to ensure confidentiality of the gradients of participants. They assumed that (1) participants and parameter server cannot collude at all, and (2) the aggregated gradients in plain text reveal nothing about the participants' local data. The second assumption, unfortunately, is no longer valid since membership inference attack on aggregated location data is now available <ref type="bibr" target="#b20">[21]</ref>.</p><p>Despite extensive research is underway on distributed deep learning, there are two serious problems that receive less attention so far. The first one is that existing work generally considered privacy threats from curious parameter server, neglecting the fact that there exist other security threats from dishonest behaviors in gradient collecting and parameter update that may disrupt the collaborative training process. For example, the parameter server may drop gradients of some parties deliberately, or wrongly update model parameters on purpose. Recently, Bagdasaryan et. al <ref type="bibr" target="#b21">[22]</ref> demonstrated the existence of this problem that dishonest parties can poison the collaborative model by replacing the updating model with its exquisitely designed one. Therefore, it is crucial for distributed deep learning framework to guarantee not only confidentiality of gradients, but also auditability of the correctness of gradient collecting and parameter update.</p><p>The second problem is that in existing schemes those parties are assumed to have enough local data for training and are willing to cooperate in the first place, which are not always true in real applications. For example, in healthcare applications, companies or research institutes are usually facing the difficulty in collecting enough personal medical data, due to privacy regulations such as HIPAA <ref type="bibr" target="#b22">[23]</ref>, people's unwillingness to share and malicious attacks like identifying inference attacks against HCUPnet <ref type="bibr" target="#b23">[24]</ref>. As a consequence, lack of training data will result in poor deep learning models in general <ref type="bibr" target="#b24">[25]</ref>. On the other hand, in business applications some companies may be reluctant to participate in collaborative training, because they are very concerned about possible disclosure of their valuable data during distributed training <ref type="bibr" target="#b10">[11]</ref>. Obviously, it is vital to ensure data privacy and bring in some incentive mechanism for distributed deep learning, so that more parties can actively involved in collaborating training.</p><p>Traditional incentive mechanisms can be categorized into three types: reputation-based <ref type="bibr" target="#b25">[26]</ref>, tit-for-tat <ref type="bibr" target="#b26">[27]</ref> and payment-based mechanism <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Usually, these mechanisms, except for tit-for-tat, need a trusted centralized authority to audit participant behaviors and arbitrate their payoff. Unfortunately, they fail to provide public auditability and decision fairness. Although there is no trusted centralized party in tit-for-tat, it is not suitable for our setting, because a party's contribution is not symmetric to that of her counterparts. It is worth noting that Blockchain, originated from decentralized currencysystem, enables distrustful nodes to share a common transaction ledger without the need of a trusted third party, by employing a consensus protocol and financial incentives. This motivates us to introduce a payment-based incentive mechanism that guarantees public authority and fairness.</p><p>In this paper, we propose DeepChain, a secure and decentralized framework based on Blockchain-based incentive mechanism and cryptographic primitives for privacypreserving distributed deep learning, which can provide data confidentiality, computation auditability, and incentives for parties to participate in collaborative training. The system models of traditional distributed deep learning and our DeepChain are given in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, DeepChain can securely aggregate local intermediate gradients from un- • DeepChain preserves the privacy of local gradients and guarantees auditability of the training process. By employing incentive mechanism and transactions, participants are pushed to behave honestly, particularly in gradient collecting and parameter update, thus maintaining fairness during collaboration training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We implement DeepChain prototype and evaluate its performance in terms of cipher size, throughput, training accuracy and training time. We believe that DeepChain can benefit AI and machine learning communities, for example, it can audit collaborative training process and the trained model, which represents the learned knowledge. Well-trained models can be used for paid services when the model-based pricing market is mature. In addition, making the best use of this learned knowledge by combining transfer learning technique can improve both the learning efficiency and accuracy.</p><p>The rest of the paper is organized as follows. In Section 2, we give a brief introduction of Blockchain and deep learning model training. Then, we describe the threat model and security requirements in Section 3. In Section 4, we present our DeepChain, a framework for auditable and privacypreserving deep learning, and analyze security properties of DeepChain in Section 5. We give implementation details of DeepChain in Section 6, and conduct extensive experiments to evaluate its performance. Finally, we conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Our work is closely related to Blockchain and deep learning training, and we give background knowledge in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Blockchain technology</head><p>Blockchain has arisen a surge of interests both in research community and industry <ref type="bibr" target="#b29">[30]</ref>. It becomes an emerging technology as a decentralized, immutable, sharing and timeordered ledger. Transactions are stored in blocks that contain timestamps and references (i.e., the hash of previous block), which are maintained as a chain of blocks. In Bitcoin, transactions are created by pseudonymous participants and competitively collected to build a new block by an entity called worker. The worker who generates a new and valid block can gain some amount of rewards, hence the chain is continuously lengthened by workers. To achieve this, proof of work (PoW)-based consensus protocol and incentive mechanism are required.</p><p>There are a wide variety of consensus protocols, such as proof of stake (PoS)-based, byzantine fault tolerance (BFT)based and hybrid protocols. In general, when introducing a new consensus protocol for a Blockchain setting, one needs to consider six problems: (1) leader selection, i.e., how to select a new block leader in each round, (2) network model, i.e., the message communication mode, such as asynchronous, synchronous, and semi-synchronous, (3) system model, i.e., permissioned or permissionless system model, explaining whether a party can join the system freely, (4) communication complexity, reflecting the communication cost to propagate a new block to all parties in the system in each round, <ref type="bibr" target="#b4">(5)</ref> adversary assumption, defining the probability of tolerating fault parties in the system, and (6) consensus property, corresponding to the Agreement-Validity-Termination properties defined in classic consensus protocols <ref type="bibr" target="#b30">[31]</ref>.</p><p>The latest Algorand protocol <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> is a hybrid consensus protocol based on PoS and BFT. Different from PoWbased consensus protocol, Algorand can guarantee consensus finality with overwhelming probability in terms of consensus property. Here, consensus finality means that a valid block appended to the chain will never be removed in the future, which is especially suitable for our problem. Without block data abandonment, we avoid spending excessive time and computation power to retrain a huge model. Also, Algorand protocol works in permissioned environment with the assumption of a synchronous network, which can be adapted to our setting. Some latest Blockchain techniques, such as Ethereum and Hyperledger, introduce smart contract that supports Turing-complete programmability. Other researchers use these techniques to solve specific security issues in different application scenarios such as software-update management <ref type="bibr" target="#b33">[34]</ref>, cloud storage <ref type="bibr" target="#b34">[35]</ref> and machine learning <ref type="bibr" target="#b35">[36]</ref>. On the other hand, a series of work on transaction privacy apply cryptographic tools in Blockchain, such as Zerocash <ref type="bibr" target="#b36">[37]</ref>, Zerocoin <ref type="bibr" target="#b37">[38]</ref> and Hawk <ref type="bibr" target="#b38">[39]</ref>. In general, consensus protocol and incentive mechanism in Blockchain are key ingredients for us to solve our problems, i.e., absence of incentive function and collaboration fairness guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep learning and distributed deep learning</head><p>A typical deep learning model consists of three layers, namely input layer, hidden layer and output layer. A deep learning model can contain multiple hidden layers, where the number of layers is called depth of the model. Each hidden layer can have certain number of neurons, and neurons at different layers can learn hierarchical features of the input training data, which represent different levels of abstraction. Each neuron has multiple inputs and a single output. Generally, the output of neuron i at layer l -1 connects to the input of each neuron at layer l. For the connection between two neurons, there is a weight assigned to it. For example, w i,j is a weight assigned to the connection between neuron i at layer l -1 and neuron j at layer l. Each neuron i also has a bias b i . These weights and bias are called model parameters, which need to be learned during the training.</p><p>Back-Propagation (BP) <ref type="bibr" target="#b39">[40]</ref> is the most popular learning method for deep learning, which consists of feed forward step and back-propagation step. Specifically, in feed forward step, the outputs at each layer are calculated based on parameters at previous layer and current layer, respectively.</p><p>A key component in deep neural network training is called activation, which is the output of each neuron. Activation is used to learn non-linear features of inputs via function Act(•). To compute the output value of a neuron i at layer l, Act(•) takes all the n inputs of i from layer l -1 as the input. In addition, we assume that weight w j,i is associated with the connection between neurons j at layer l -1 and neurons i at layer l, and b i is the bias of neuron i. Then, the value of neuron i at layer l can be obtained by Act i (l) = Act i (Σ n j=1 (w j,i * Act j (l -1)) + b i ). The back-propagation step employs gradient descent method, which gradually reduces the model error E total , i.e., the gap between model output value V output and the target value V target . Assume that there are n output units at the output layer. Then, the gap can be calculated by</p><formula xml:id="formula_0">E total = 1 2 Σ n i=1 (V targeti -V outputi ) 2</formula><p>. Once E total is available, weights w j,i can be updated through w j,i = w j,i -η * ∂E total ∂wj,i , where η is the learning rate and ∂E total ∂wj,i is the partial derivative of E total with respect to w j,i . This is the main idea of gradient descent method. The learning process repeats until the prespecified number of iterations to train is reached.</p><p>When training a complex and multi-layer deep learning model, the aforementioned training procedure requires high computational overhead. To alleviate this problem, distributed deep learning training has been proposed recently, and some research work <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> and system implementations have been around, such as DistBelief <ref type="bibr" target="#b45">[46]</ref>, Torch <ref type="bibr" target="#b46">[47]</ref>, DeepImage <ref type="bibr" target="#b47">[48]</ref> and Purine <ref type="bibr" target="#b48">[49]</ref>. Generally, there are two approaches for distributed training, namely, model parallelism and data parallelism, where the former partitions a training model among multiple machines and the latter splits up the whole training dataset.</p><p>Our work focuses on the data parallelism approach, i.e., we have multiple machines and each machine maintains a copy of the training model while keeps a subset of the whole dataset as model input. These machines share the same parameters of the training model, by uploading/downloading parameters to/from a centralized parameter server. Then, machines upload their local training gradients, based on which the training model is updated by using SGD (Stochastic Gradient Descent). They download updated parameters from the parameter server and continue to train the local model. This process repeats until machines obtain the final trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THREATS AND SECURITY GOALS</head><p>In this section, we discuss threats to collaborative learning, and security goals that DeepChain can achieve to tackle those threats.</p><p>Threat 1: Disclosure of local data and model. Although in distributed deep training each party only uploads her local gradients to the parameter server, adversaries still can infer through those gradients important information about the party's local data by initiating an inference attack or membership attack <ref type="bibr" target="#b17">[18]</ref>. On the other hand, based on the gradients, adversaries may also launch parameter inferring attack to obtain sensitive information of the model <ref type="bibr" target="#b18">[19]</ref>.</p><p>Security Goal: Confidentiality of local gradients. Assume that participants do not expose their own data and at least t participants are honest (i.e., no more than t participants colluded to disclose parameters). Then each party's local gradients cannot be exposed to anyone else, unless at least t participants collude. In addition, if in any circumstance participants do not disclose the downloaded parameters from the collaborative model, then adversaries could not gain any information about the parameters. To achieve this goal, in DeepChain each participant individually encrypts and then uploads gradients obtained from her local model. All gradients are used to update parameters of the collaborative model encrypted collaboratively by all participants, who then obtain updated parameters via collaborative decryption in each iteration. Here, collaborative decryption means that at least t participants provide their secret shares to decrypt a cipher.</p><p>Threat 2: Participants with inappropriate behaviors. Consider a situation that participants may have malicious behaviors during collaborative training. They may choose their inputs at will and thus generate incorrect gradients, aiming to mislead the collaborative training process. As a consequence, when updating parameters of collaborative model using the uploaded gradients, it is inevitable that we will get erroneous results. On the other hand, in collaborative decryption phase dishonest participants may give a problematic decryption share and they may be selfish, aborting local training process early to save their cost for training. In addition, dishonest participants may delay trading or terminate a contract for her own benefit, which makes the honest ones suffer losses. All these malicious behaviors may fail the collaborative training task.</p><p>Security Goal 1: Auditability of gradient collecting and parameter update. In DeepChain, assume that majority of the participants and at least 2  3 of the workers are honest in gradient collecting and parameter update, respectively. During gradient collecting, participants' transactions contain encrypted gradients and correctness proofs, allowing the third party to audit whether a participant gives a correctly encrypted construction of gradients. For parameter update, on the other hand, workers claim computation results through transactions that will be recorded in DeepChain. These transactions are auditable as well, and computation results are guaranteed to be correct only if at least 2  3 workers are honest. After parameters are updated, participants download and collaboratively decrypt the parameters by providing their decryption shares and corresponding proofs for correctness verification. Again, any third party can audit whether the decryption shares are correct or not.</p><p>Security Goal 2: Fairness guarantee for participants. DeepChain provides fairness for participants through timeout-checking and monetary penalty mechanism. Specifically, for each function with smart contracts DeepChain defines a time point for it. At the time point after function execution, results of the function are verified. If the verification failed, it means that (1) there exist participants not being punctual by the time point, and (2) some participants may incorrectly execute the function. For either of the two cases, DeepChain applies the monetary penalty mechanism, revoking the pre-frozen deposit of dishonest participants and re-allocating it to the honest participants. Therefore, fairness can be achieved, because penalty will never be imposed on honest participants behaved punctually and correctly, and they will be compensated if there exist dishonest participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE DEEPCHAIN MODEL</head><p>In this section, we present DeepChain, a secure and decentralized framework for privacy-preserving deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System overview</head><p>Before introducing DeepChain, we give definitions of related concepts and terms used in DeepChain.</p><p>Party: In DeepChain, a party is the same entity as defined in traditional distributed deep learning model, who has similar needs but unable to perform the whole training task alone due to resource constraints such as insufficient computational power or limited data.</p><p>Trading: When a party gets her local gradients, she sends out the gradients by launching a transaction to a smart contract called trading contract to DeepChain. This process is called trading. Those contracts can be downloaded to process by worker (an entity in DeepChain that will be defined shortly).</p><p>Cooperative group: A cooperative group is a set of parties who have the same deep learning model to train.</p><p>Local model training: Each party trains her local model independently, and at the end of a local iteration the party generates a transaction by attaching her local gradients to the contract.</p><p>Collaborative model training: Parties of a cooperative group train a deep learning model collaboratively. Specifically, after deciding a same deep learning model and parameter initialization, the model is trained in an iterative manner. In each iteration, all parties trade their gradients, and workers download and process the gradients. The processed gradients are then sent out by workers to the smart contract called processing contract. These correctly processed gradients are used to update parameters of the collaborative model by the leader selected from the workers. Parties download the updated parameters of the collaborative model and update their local models accordingly. After that parties begin next iteration of model training.</p><p>Worker: Similar to miners in BitCoin, workers are incented to process transactions that contain training weights for collaborative model update. Workers compete to work on a block, and the first one finishes the job is a leader. The leader will gain block rewards that can be consumed in the future, for example, she may use rewards to pay for usage fee of trained models in DeepChain.</p><p>Iteration: Deep learning model training consists of multiple steps called iterations, where at the end of each iteration all the weights of neurons of the model are updated once.</p><p>Round: In DeepChain, a round refers to the process of the creation of a new block.</p><p>DeepCoin: DeepCoin, denoted as $Coin, is a kind of asset on DeepChain. In particular, for each newly generated block DeepChain will generate certain amount of $Coin as rewards. Participants in DeepChain consist of parties and workers, where the former gain $Coin for their contributions to local model training, and the latter are rewarded with $Coin for helping parties update training models. Meanwhile, a well-trained model will cost $Coin for those who have no capability to train the model by themselves and want to use the model. This setting is reasonable because recent work on model-based pricing for machine learning has found applications in some scenarios <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b35">[36]</ref>. We define a validity value for $Coin, which essentially is the time interval of a round. Validity value is related to consensus mechanism in DeepChain, and we will discuss it in detail in Section 4.2.5.</p><p>DeepChain combines together Blockchain techniques and cryptographic primitives to achieve secure, distributed, and privacy-preserving deep learning. Suppose there are N parties P j , j ∈ {1, ..., N }, and they agree on some pre-defined information such as a concrete collaborative model and initial parameters of the collaborative model. The information is attached to a transaction T x 0 co signed by all parties. Assume the address corresponding to transaction T x 0 co is pk it0 , where it 0 is the initial iteration. At the end of iteration i, the updated model in T x i co is attached to a new address pk iti . All addresses are known to the parties.</p><p>Intermediate gradients from party P j are enveloped in transaction T x i Pj , and all those transactions are collected by a trading contract at round i. Note that intermediate gradients are local weights C Pj ( W i,j ), where C is a cipher used by party P j to encrypt the weights. When all transactions {T x i Pj } at round i have been collected, trading contract uploads them to DeepChain. After that, workers download those transactions {T x i Pj } to process via processing contract. Specifically, workers update the weights by computing</p><formula xml:id="formula_1">C(W i+1 ) = 1 N • C(W i ) • N j=1 C Pj ( W i,j</formula><p>), where C(W i ) is the weight at round i in T x i co , and C(W i+1 ) is the updated weights that will be attached to T x i+1 co for updating the local models in next round i + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Components of DeepChain</head><p>DeepChain consists of five building blocks that collectively achieve distributed and privacy-preserving deep learning, namely, DeepChain bootstrapping, incentive mechanism, asset statement, cooperative training and consensus protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">DeepChain bootstrapping</head><p>DeepChain bootstrapping consists of two steps, i.e., Deep-Coin distribution and genesis block generation. Assume that all parties and workers have registered (i.e., have a valid account) in DeepChain, where each one uses an address pk that corresponds to a DeepCoin unit for launching a transaction.</p><p>In the first step, DeepCoin distribution realizes Deep-Coin allocation among parties and workers, and initially each party or worker is allocated with the same amount of DeepCoins. Then in the second step, a genesis block is generated at round 0, which contains initial transactions recording ownership statements for each DeepCoin.</p><p>After the genesis block is created, a random seed seed 0 is also publicly known, which is randomly chosen by registered users through a routine for distributed random number generation. When DeepChain keeps running, at round i, seed i-1 is used for generating seed i . It is worth mentioning that these random seeds are crucial for DeepChain, because they guarantee randomness when selecting a leader to create a new block at each round. The idea of introducing random seeds is motivated by Algorand's cryptographic sortition <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, and details will be given in Section 4.2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Incentive mechanism</head><p>An incentive can act as a driving force for participants to actively and honestly take part in a collaborative training task, and the goal of incentive mechanism is to produce and distribute value, so that a participant gets rewards or penalties based on her contribution. The introduction of incentive mechanism is crucial for collaborative deep learning, due to the following reasons. First, for those parties who want a deep learning model but have insufficient data to train the model on their own, incentive can motivate them to join the collaborative training with their local data. Second, with reward and penalty, incentive mechanism ensures that (1) parties are honest in local model training and gradient trading, and (2) workers are honest in processing parties' transactions.</p><p>For ease of understanding the incentive mechanism, we give an example consisting of two parties. These two parties contribute their data to collaborative training via launching transactions. Suppose the data possessed by the two parties is not equal in quantity. Each party can launch transactions and pay transaction fee based on the amount of data she owned. Generally, the large amount of data a party has, the less fee she will pay. The two parties agree on the total amount of fees for collaboratively training the model. The worker who successfully creates a new block when processing transactions can be the leader and earn the rewards. Note that transaction issuing and processing are verifiable, meaning that if some party poses an invalid transaction, the party would be punished. On the other hand, if a leader incorrectly processes a transaction, she will be punished accordingly. When collaborative training finished, parties themselves can benefit from the trained model that can bring revenue for them through charged services to those users who want to use the trained model.</p><p>To give a formal description of the incentive mechanism, we first introduce two properties, i.e., compatibility and liveness of the incentive mechanism for participants. Then, we further explain that parties and workers have incentive to behave honestly. Assume that we guarantee data privacy and security of the consensus protocol (explained in Section 4.2.5). We use v c and v i to denote the value of the trained collaboratively model and the trained individual model i, respectively, and we assume that v c is greater than v i . First, we say the incentive mechanism exhibits compatibility if each participant can obtain the best result according to their contributions. Meanwhile, it has liveness only if each party is willing to update her local training model with value v i by continuously launching transactions and each worker also has incentive to update the parameters of the collaborative training model with value v c . Below we describe the importance of these two properties with respect to participant's true contribution and the corresponding payoff. Let ω P and ω W be the contributions of a party and a worker to the final trained model, respectively, and π P and π W be their corresponding payoffs, respectively. At first, we assume that participant's contribution originates from her correct behaviors with a high probability, and later we will explain that this assumption is reasonable.</p><p>Liveness: both the party and the worker have the same common interest to obtain a trained collaborative model. Because if a party costs v i during the whole training process, then she would gain v c in the end, which is attractive for her because v c is greater than v i . On the other hand, a worker will process transactions for collaboratively constructing the training model in order to earn rewards with probability, with which she could pay for the deep learning services in DeepChain. Note that the probability a worker obtains reward depends on the quantity of rewards she has already earned. The larger the quantity, the higher probability she can get reward. As a result, both the party and the worker are incented to build the collaborative training model.</p><p>Compatibility: the more a party contributes ω P , the more she will gain π P . This holds for a worker too. During the collaborative training process, both party and worker are incentivized to do their best to contribute to building a training model M ax(ω P ) M ax(ω W ), where the maximum total payoff is M ax(π P ) + M ax(π W ). If any participant did not perform well, i.e., (ω P = 0) (ω W = 0), then there is no reward, i.e., (π P = 0) (π W = 0). Here, means 'and' and means 'or'. So we have Payoff=</p><formula xml:id="formula_2">M ax(π P ) + M ax(π W ) If M ax(ω P ) M ax(ω W ) (π P = 0) (π W = 0) If (ω P = 0) (ω W = 0)</formula><p>Next, we explain the assumption that participant's contribution originates from her correct behaviors with a high probability. We show that each party or worker is valuedriven to behave correctly in each round so that she could obtain the highest payoff <ref type="bibr" target="#b50">[51]</ref>. If the probability that a party's behavior is correct is P r c (P ), then the corresponding value is V alue(P r c (P )). Clearly, if the party's behavior is correct with probability P r c (P )=1, then she will obtain the highest value, i.e., V alue <ref type="bibr" target="#b0">(1)</ref>. Similarly, a worker can get value V alue(P r c (W )) if she behaves correctly with probability P r c (W ). Assume that a method verifies a party's malicious behavior to be correct with probability P r v (P ), then the probability that a dishonest party is caught is P r vc (P ) = P r v (P ) * (1-P r c (P )). Once the dishonest party is caught, she is punished by forfeiting her deposit and the loss is denoted as f P .</p><p>Thus, the final value according to the party's correct behavior can be computed as where P r vc (P ) = P r v (P ) * (1 -P r c (P )). The above value reaches maximum only when the party behaves honestly, i.e., P r c (P ) = 1. Therefore, V alue(1) = π P -ω P (1) holds. This indicates the importance of the incentive mechanism. Specifically, the values of P r v (P ), π P , and f P can be determined through the following theorems.</p><p>Theorem 1. If f P /π P &gt; (1 -P r vc (P ))/P r vc (P ), where P r vc (P ) = P r v (P ) * (1 -θ), then a party is honest at least with probability θ.</p><p>Proof. We need to prove that for any P r c (P ) &lt; θ, V alue(P r c (P )) is smaller than V alue(θ). Without the loss of generality, we prove that for any P r c (P ) &lt; θ, we have V alue(P r c (P )) &lt; 0. In other words, we have V alue(P r c (P )) = π P * (1 -P r vc (P )) -f P * P r vc (P )ω P (P r c (P )) &lt; 0. When we set f P /π P &gt; 1/P r vc (P ) -1, then we have π P * (1-P r vc (P ))-f P * P r vc (P ) &lt; 0. Thus, V alue(P r c (P )) &lt; 0 holds.</p><p>For a worker, analysis of the incentive mechanism is similar to the above analysis for a party, expect that the worker's payoff is obtained with probability. We denote this probability by P r leader , then we could determine the relationship between the four values P r leader , P r v (W ), π W , and f W by the following theorem, so as to encourage a worker to be honest.</p><formula xml:id="formula_3">Theorem 2. If f W /π W * P r leader &gt; (1-P r vc (W ))/P r vc (W ),</formula><p>where P r vc (W ) = P r v (W ) * (1 -), then a worker will be honest at least with probability .</p><p>Proof. The proof is similar to the proof of Theorem 1, so we omit it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Asset statement</head><p>For ease of presentation, we list related cryptographic notations used in this section in Table <ref type="table" target="#tab_0">1</ref>. A party needs to state her asset, which enables her to find cooperators and accomplish her deep learning task. Asset statement does not reveal the content of asset, since it is simply some description of the asset, e.g., what kind of deep learning tasks the asset can be used for. Specifically, party P states an asset by sending an asset transaction, which will be introduced later.</p><p>We recall the formation of a transaction. Note that a transaction is launched by a pseudo public key address pk psu P generated by P according to her wish in the following form. Here, n is an integer. P selects a secret key sk P ∈ Z * q and generates n public keys</p><formula xml:id="formula_4">g sk P i ∈ G 1 , i ∈ [1, n]</formula><p>. q and g are pre-specified parameters, and g i equals to g ri , where r i is a random element in Z * q . Suppose that party P 1 sends a transaction with her address pk psu P1 to state her asset data P 1 as follows</p><formula xml:id="formula_5">T ranP 1 = pk psu P 1 → pk data P 1 = g H 1 (data P 1 ) , σj P 1 = (H2(j) • g H 1 (data j P 1</formula><p>) ) H 1 (data P 1 ) , "Keywords" .</p><p>In this transaction, the first part in the braces consists of pk data P1 and σ j P1 , which is the statement proof that party P 1 indeed possesses asset H 1 (data P 1 ) without leaking the content of data P 1 . In particular, σ j P1 contains l components, where data P 1 is divided into l blocks represented by data j P1 , j ∈ [1, l]. The second part "Keywords" is the description of the asset data P 1 . In our implementation, "Keywords" is in JSON form that includes four fields, i.e., data size, data format, data topic and data description. With this transaction T ran P1 , P 1 can fulfill her asset statement.</p><p>We assume that the first stated asset is authentic, which is reasonable in Blockchain. Collaborative group establishment. According to similar "Keywords", parties can establish a collaborative group. It is worth noting that parties may get more detailed information about "Keywords" through off-line interactions and this is not the focus of our paper. Before forming a collaborative group, parties can audit cooperator's asset to ensure authenticity of the asset ownership. The auditing process can be done by using the method in <ref type="bibr" target="#b51">[52]</ref>, and we omit the details for brevity.</p><p>Suppose there are N (N &gt; 3) parties P 1 , P 2 , ..., P N that constitute a group with pseudonymity, i.e., pseudo public keys pk psu P1 , pk psu P2 , ..., pk psu P N and their corresponding secret keys sk P1 , sk P2 , ..., sk P N are privately kept, respec- tively. Since different party launches transactions using her own pseudo public key pk psu Pi , transactions signed by the corresponding secret key sk Pi can be verified to ensure that those transactions are from the same cooperative party P i .</p><p>Collaborative information commitment. After the collaborative group is formed, parties agree on the information for securely training a deep learning model. In this step, we assume that a trusted component (e.g., a trusted hardware like Intel SGX <ref type="bibr" target="#b52">[53]</ref>) only takes part in the setup phase in Threshold Paillier algorithm <ref type="bibr" target="#b53">[54]</ref>, and it is not involved in any other process. If there does not exist such a trusted component, we can accomplish the setup phase by using a distributed method such as the one in <ref type="bibr" target="#b54">[55]</ref>. Parties agree on the following information.</p><p>(1) Number of cooperative parties, N .</p><p>(2) Index of the current round, r.</p><p>(3) Parameters of Threshold Paillier algorithm. We have the following equation</p><formula xml:id="formula_6">P K model = (n model , g model , θ = as, V = (v, {v i } i∈[1,...,N ] ))</formula><p>where modulus n model is the product of two selected safe primes, and</p><formula xml:id="formula_7">g model ∈ Z * n 2 model , a, s, θ, v, v i ∈ Z * n model .</formula><p>And SK model = s is randomly divided into N parts, where s = f (s 1 + ... + s N ) and f is a function of secret sharing protocol (i.e., Shamir's secret sharing protocol <ref type="bibr" target="#b55">[56]</ref>). Each party owns a proportion of secure key s i as well as v and {v i }, i ∈ [1, ..., N ] are public verification information, where v i corresponds to s i . A threshold t ∈ { N 3 + 1, ..., N } is set as such that at least t parties together can decrypt a cipher. Specifically, we give the threshold configuration with respect to the number of adversaries in Table <ref type="table" target="#tab_1">2</ref>.</p><p>Note that training gradients to be encrypted are vectors with multiple elements, i.e., W i,j = (w 1 i,j , ..., w l i,j ) where the length of W i,j is l, i is the index of current training iteration, and j ∈ {1, ..., N }. Due to the problem of cipher expansion, we encrypt a vector into one cipher instead of multiple ciphers with respect to multiple elements. Suppose that each value w 1 i,j , ..., w l i,j is no larger than integer d, d &gt; 0. We choose a l-length super increasing sequence α = (α 1 = 1, ..., α l ) that simultaneously meets conditions (1) , so that we generate a cipher for weight vector W 0 = (w 1 0 , ..., w l 0 ). (5) A commitment on SK model = s, with respect to P K model .</p><formula xml:id="formula_8">i-1 l=1 α l • N • d &lt; α i , i = 2, ...,</formula><p>Commitment commit SK model is obtained by combining parties' commitments on their secret shares s i . Recall that r is the index number of the current round. We have Each cooperative party is required to commit some amount of deposits for secure computation. During collaborative training, if a party misbehaves on purpose, her deposit d($Coin) would be forfeited and compensated for other honest parties. Otherwise, those deposits would be refunded after the training process finished.</p><p>All the above collaborative information are recorded in a transaction T ran co that is uploaded to DeepChain. Specifically, T ran co is in the following form and is attached to a commonly coordinated address pk psu co .</p><p>T ranco = pk psu co → N, r, P K model , d, α, modelco, commitSK model , C(W0,j), d($Coin) .</p><p>In addition, two roles called trader and manager are defined for parties in a collaborative group, which will be explained shortly. Next we introduce how collaborative training is securely accomplished through the remaining two steps, namely, Gradient collecting via Trading Contract and Parameter updating via Processing Contract.</p><p>First of all, parties iteratively trade their gradients through Trading Contracts that are executed by a manager selected from cooperative parties. The trading gradients are honestly encrypted by each trader and meanwhile the correct proofs of encryption are attached that indicate two security requirements, i.e., confidentiality and auditability. Herein, we say gradient transactions are generated. In terms of confidentiality, if a trader does not disclose her gradients, then no one can gain information about the gradients. In addition, traders (at most t parties) need to cooperatively decrypt the updated parameters. Similar to <ref type="bibr" target="#b38">[39]</ref>, we assume that the manager does not disclose what she knows. In terms of auditability, there exist proofs of correct encryption which can be auditable. When cooperatively decrypting, each trader presents her own decryption proof. Those proofs are generated non-interactively and publicly auditable by any party on DeepChain.</p><p>Through timeout-checking and monetary penalty mechanism, behaviors of the traders and the manager are forced to be authentic and fair. Even if the manager colludes with traders, the outcome of Trading Contract cannot be modified <ref type="bibr" target="#b38">[39]</ref>. In addition to Trading Contract, Processing Contract is responsible for parameter updating. Workers process transactions by adding up gradients, and send computation results to Processing Contract. Processing Contract verifies correct computation results and updates model parameters for the group. Note that once smart contract is defined, it can be automatically executed in response to some trigger event. In this setting, 'computation results sent to Processing Contract' is the trigger event, and Processing Contract has a pre-defined function to verify those computation results by the rule of majority voting. These two contracts are iteratively invoked, so as to accomplish the whole training process. Details of the two steps are given below.</p><p>Gradient collecting via Trading Contract. As shown in Algorithm 1, Trading Contract invokes six functions, i.e., line 1, 4, 7, 10, 13 and 16 of Algorithm 1, for training model co . At the end of each of the functions, we declare a time point T ti to check time-out events, and these six time points satisfy T ti &lt; T ti+1 , i = 1, 2, ..., 5. We set up the time points according to Greenwich Mean Time. The time interval between T t1 and T t6 can be determined according to the time interval between two consecutive training iterations, e.g., for iteration i and i + 1, we have</p><formula xml:id="formula_9">|T t6 -T t1 | ≤ |T i+1 -T i |.</formula><p>By the end of a time point T ti , function checkT imeout checks whether the parties finish the events or not by T ti . If some party is caught, the monetary penalty mechanism will be performed to forfeit deposit of the party, and the failed step is re-executed. During collaborative training, the six time points are updated accordingly with iterations, e.g.,</p><formula xml:id="formula_10">T t1 = T t1 + |T i+1 -T i |.</formula><p>Algorithm 1 works as follows. As shown in line 1, at the i-th iteration each party P j , j ∈ {1, ..., N } sends a gradient transaction T ran i Pj to receiveGradientT X(). A publicly auditable proof P roof P Ki,j is also attached to the transaction to guarantee encryption correctness. We have T ran i Pj = {pk psu Pj : (C( W i,j ), P roof P Ki,j ) → pk psu co } P roof P Ki,j = f sprove 1 (Σ P K ; C( W i,j ); W i,j , k j ; pk psu Pj )</p><p>Then in line 4, function verif yGradientT X() veri- </p><formula xml:id="formula_11">// T t6 = T t6 + |T i+1 -T i |</formula><p>fies correctness of the encrypted gradients via function f sver 1 (Σ P K ; C( W i,j ); P roof P Ki,j ; pk psu Pj ). Specifically, it verifies whether C( W i,j ) is indeed the encryption of W i,j with random number k j . Here, pk psu Pj can be regarded as the identity information attached to the proof, avoiding replay attack by a malicious party. In line 7, function uploadGradientT X() uploads the transactions that have been verified successfully. When model parameter update finished, downloadU pdatedP aram() retrieves the latest parameters, as can be seen in line 10. Recall that Processing Contract computes gradients N j=1 W i,j for model model co . Suppose that the latest iteration is i, the cipher of the latest parameters is C(W i ) and we denote it as C i for brevity. Then decryptU pdatedP aram() collects parties' decryption shares on C i for collaborative decryption, which generates C i,j , j ∈ 1, ..., N . Meanwhile, the corresponding proofs for correct shares P roof CDi,j are also provided, as follows.</p><formula xml:id="formula_12">C i,j = C 2∆sj i P roof CDi,j = f sprove 2 (Σ CD ; (C i , C i,j , v, v j ); ∆s j ; pk psu Pj )</formula><p>The proof P roof CDi,j is used to verify validity of the decryption shares, i.e., ∆s j = log C 4 i (C 2 i,j ) = log v (v j ), through function f sver 2 (Σ CD ; (C i , C i,j , v, v j ); P roof CDi,j ; pk psu Pj ). If majority of the parties are honest, then C i can be correctly recovered to plaintext by</p><formula xml:id="formula_13">((Π j∈H C 2µj i,j -1)/n model )(4∆ 2 θ) -1 mod n model</formula><p>where µ j is the Lagrange interpolation coefficient with respect to P j , and the plaintext is pushed to parties by function return() in line 16.</p><p>• Parameter updating via Processing Contract. Algorithm 2 summarizes the process of Processing Contract, which contains three functions, as shown in line 1, 4, and 7. Suppose that at the i-th iteration of collaborative training, local Algorithm 2: Processing()</p><formula xml:id="formula_14">1 updateTX() 2 checkTimeout(T t7 ) 3 updateTime() // T t7 = T t7 + T r 4 verifyTX() 5 checkTimeout(T t8 ) 6 updateTime() // T t8 = T t8 + T r 7 appendTX() 8 checkTimeout(T t9 ) 9 updateTime() // T t9 = T t9 + T r</formula><p>gradients C( W i,j ), j ∈ {1, ..., N }, have been uploaded, then workers competitively execute update operations by</p><formula xml:id="formula_15">C(Wi) = C(Wi-1) • 1 N •(C(-Wi,1) • C(-Wi,2)• ... • C(-Wi,N )).</formula><p>Once update operation finished, workers then send the updated results through transactions to function updateT X() in Processing Contract, as shown in line 1.</p><p>At the meantime, a leader is randomly chosen from the workers by using the consensus protocol of DeepChain (we will discuss it in Section 4.2.5). Note that at this moment we defer the reward to the leader until her computational work is verified by using function verif yT X() as shown in line 4, that employs majority voting policy. In other words, the leader's computational result C(W i ) will be compared against those of the other workers, and her result is admitted only if the majority of the workers produce the same result. Otherwise, the leader would be punished according the monetary penalty mechanism and she gains no reward. In such case, we repeat the procedure to chosen a new leader from the remaining workers. The more often a worker is punished, the lower probability she will be chosen as a leader. Once we get a legitimate leader, her block with correctly updated result is appended to DeepChain through appendT X(), as shown in line 7. In Processing Contract, time points T t7 , T t8 and T t9 will be updated to T t7 = T t7 + T r , T t8 = T t8 + T r and T t9 = T t9 + T r , respectively, where T r is the time needed to create a new block between consecutive rounds in DeepChain. Figure <ref type="figure" target="#fig_5">3</ref> gives an example of time point configuration scheme to illustrate relationship of time points of the trading and processing contracts. Suppose that at the i-th iteration, the time points are set as such that T t1 &lt; T t2 &lt; T t3 ≤ T t7 &lt; T t8 &lt; T t9 ≤ T t4 &lt; T t5 &lt; T t6 . At the meantime, the relationship between the three time intervals is In addition to the above configuration scheme for time points, we employ secure monetary penalty mechanism to guarantee fairness in gradient collecting and collaborative decryption. Specifically, enlightened by the penalty mechanism proposed by Bentov et al <ref type="bibr" target="#b56">[57]</ref> and Kumaresan et. al <ref type="bibr" target="#b57">[58]</ref>, we design our secure monetary penalty mechanism based on Trading Contract, presented in Algorithm 3 and 4.</p><formula xml:id="formula_16">T r ≤ |T t6 -T t1 | ≤ |T i+1 -T i |.</formula><p>In particular, in Gradient collecting (Algorithm 3) fairness is guaranteed due to (1) honest collaborative parties must launch gradient transactions to be correctly verified before the pre-specified time point, and (2) dishonest parties who launch incorrect transactions or delayed transactions will be penalized, and the honest ones will be compensated for. In line 1, Trading Contract waits to receive a input message from pk psu Pj for all j = 1, ..., N before time T t1 . By defining C ⊆ {1, ..., N } as adversarial parties S in the input step, the contract also waits an input message from S. Here, sid is session identifier, d($Coin) is deposit, and H means the set of the remaining honest parties, where |H | = h . In line 2, the contract verifies the ciphertext for all pk psu Pj ∈ H , and records the correct parties {1, ..., N } \ C , where C refers to corrupted parties in this step. In line 3, the contract sends return messages to pk psu Pj for j ∈ {1, ..., N } \ C . In line 4, we wait for a return message from S. If the returned message is continue, then the contract outputs normally to all pk psu Pj (j ∈ {1, ..., N }), by sending payback message to S and extrapay to pk psu Pj in H , where H = H \ C and |H | = h ; otherwise, the contract sends penalty to pk psu Pj , j ∈ {1, ..., N }.</p><p>Similarly, fairness is also achieved in Collaborative decryption (Algorithm 4), since (1) a party who gives a correct decryption share no later than the pre-defined time point receives no penalty, and (2) If an adversary successfully decrypts the cipher but a legitimate party fails to do so, then the party should be compensated for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Consensus protocol</head><p>Consensus protocol is essential in DeepChain, since it enables all participants to make a consensus upon some event in a decentralized environment. In this section, we introduce blockwise-BA protocol of DeepChain, based on the work of Algorand <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. The blockwise-BA protocol includes three main steps -(1) A leader who creates a new block is randomly selected by using cryptographic sortition, (2) A committee, consisting of participants whose transactions are included in the new block, verifies and agrees on the new block by executing a Byzantine agreement protocol <ref type="bibr" target="#b30">[31]</ref>, and (3) Each verifier in the committee tells neighbors the new block by using a gossip protocol <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, so that the new block is known to all participants in DeepChain.</p><p>Our consensus protocol possesses three properties, i.e., safety, correctness, and liveness. In particular, safety means that all honest parties agree on a same transaction history in DeepChain, whereas correctness requires that any transaction agreed by a honest party comes from a honest party. Liveness says that parties and workers are willing to continuously perform activities in DeepChain, hence keeping DeepChain alive. Based on these three properties, we assume that message transmission is synchronous and there are no more than 1  3 malicious parties. In this setting, all parties agree on a chain with the largest amount of assets. We give details of the three steps of our consensus protocol below. Suppose block block i is created at round r i .</p><p>Leader selection. At round r i , a leader leader i is randomly chosen from workers who collect transactions and put them into block block i . To choose a leader, we invoke the sortition function of Algorand <ref type="bibr" target="#b31">[32]</ref>, which includes two functions leader selection and leader verification, as follows.</p><p>Sortition(sk, seedi, τ = 1, role = worker, w, w total ) → hash, π, j V erif ySort(pk, hash, π, seedi, τ, role = worker, w, w total ) → j Here, sk and pk are owned by worker, and seed i is a random seed selected based on seed i-1 , i.e., seed i = H(seed i-1 ||r i ), where H is an hash function. τ = 1 means that only one leader is selected from workers role = worker. w represents the amount of $Coins that the participant possesses. Parameter w total is the total amount of $Coins in DeepChain. It is worth mentioning that w is crucial, because it is used to control the probability that worker can gain reward according to the amount of rewards she has already earned (see Section 4.2.2).</p><p>Our definition of w is different from that of Alogrand, in that in our Leader Selection w only contains $Coins that have available validity value, while those without validity value are not considered. In this way, we can eliminate the phenomenon of wealth accumulation, in which a rich participant may become richer because she has a higher probability than her peers to be chosen as the leader. Through the two functions, we can randomly select a leader and all participants can also verify whether the selected leader leader i is legitimate.</p><p>Committee agreement. After leader verification, the selected block block i is sent to the committee. Each participant in the committee verifies the transactions processed by leader i , i.e., to verify whether weight update operations are correct or not. If the committee admits that block i is right based on a majority voting policy, then participants sign block i on behalf of the committee; otherwise, block i is rejected. Note that block i is valid only if more than 2  3 of the committee members signed and agreed on it. If block i is valid, then leader i gains $Coins from block reward and transaction coins of block i ; otherwise, block i is discarded and a new empty block is created to replace block i in DeepChain. This process repeats until the committee agrees on block i .</p><p>Neighbor gossip. Suppose block i has been agreed on by the committee, then participants in the committee are responsible for telling their neighbors block i , by using the popular gossip protocol <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Therefore, after this step all participants arrive at a consensus in DeepChain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SECURITY ANALYSIS</head><p>In this section, we revisit our security goals of DeepChain presented in section 3 and give security analysis for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Confidentiality Guarantee for Gradients</head><p>To achieve this goal, DeepChain employs Threshold Paillier algorithm that provide additive homomorphic property. We assume there exists a trusted setup (refer to Section 4.2.4) and the secret key cannot leak without collaboration of at least t participants. We also assume that at least t participants are honest. Without loss of generality, both local gradients and model parameters W are encrypted with the Threshold Paillier algorithm, C(W) = g W model (k) n model . Based on the following lemma that is derived from the work <ref type="bibr" target="#b53">[54]</ref>'s Theorem 1, we can guarantee confidentiality of local gradients and model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1. With the Decisional Composite Residuosity</head><p>Assumption (DCRA) <ref type="bibr" target="#b60">[61]</ref> and the random oracle model S, Threshold Paillier algorithm is t-robust semantically secure against active non-adaptive adversaries A with polynomial time power to attack, if the following are satisfied</p><formula xml:id="formula_17">Pr (w 0 , w 1 ) ← A(1 λ , F t (•)); b ← {0, 1}; C ← S(1 λ , w b ) : A(C, 1 λ , F t (•)) = b ≤ negl(1 λ ) + 1 2</formula><p>which indicates that the probability for adversaries to distinguish w 0 or w 1 is negligible, in system security parameter λ. Here, F t (•) means that A has at most t corrupted parties and A learns their information including public parameters, secret shares of the corrupted parties, public verification keys, all decryption shares and validity of those shares. In addition, t-robust means that a Threshold Paillier ciphertext can be correctly decrypted, even in the case that A can have up to t corrupted parties. Semantic security is a general security proof methodology to measure the security of an encryption algorithm and in our context it measures confidentiality of the encrypted information by using the Threshold Paillier algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Auditability of Gradient Collecting and Parameter</head><p>Update.</p><p>Auditability ensures that any third party can audit correctness of encrypted gradients and decryption shares in gradient collecting stage and parameter updating stage, receptively. We achieve auditability by following universally verifiable CDN (UVCDN) protocol <ref type="bibr" target="#b61">[62]</ref>. Specifically, correctness proof provided in UVCDN protocol is based on -protocols, where a possible malicious prover proves correctness to a honest verifier that he indeed knows a witness w for a certain statement v leading to (v; w) ∈ R , here R is a binary relation. Generally, a correctness proof consists of three protocols, namely, announcement (denoted as .ann), response (denoted as .res) and verification (denoted as .ver), which are defined according to the purpose of correctness proof (i.e., R). We will give the correctness proof by using two procedures f sprove and f sver for .ann, .res and .ver.</p><formula xml:id="formula_18">Algorithm 5: f sprove 1 (Σ P K ; C( W i,j ); W i,j , k j ; pk psu Pj ) #announcement 1 Σ P K .ann(C( W i,j ); W i,j , k j ) := a 1 ∈ R Z n model , b 1 ∈ R Z * n model , a = g a1 model b n model 1 ; (a; s) = (a; a 1 , b 1 ) #challenge 2 c = H(C( W i,j )||a||pk psu Pj ) #response 3 Σ P K .res(C( W i,j ); W i,j , k j ; a; s; c) := t = (a1+c Wi,j ) n model , d = a 1 + c W i,j , e = b 1 k c j g t model ; r = (d, e)<label>4</label></formula><p>return P roof P Ki,j := (a; c; r) In Section 4.2.4, to prove the correctness of encrypted gradients R = {(C( W i,j ); W i,j , k j )} where C( W i,j ) = g Wi,j model (k j ) n model , a publicly auditable proof P roof P Ki,j is generated by procedure f sprove 1 . Then, any party can execute procedure f sver 1 , by taking P roof P Ki,j as input, to verify whether C( W i,j ) is indeed the encryption of W i,j with random number k j under public key P K j (i ∈ {1, ..., #iteration}, j ∈ {1, ..., N }). The concrete procedures of f sprove 1 and f sver 1 for -protocols Algorithm 6: f sver 1 (Σ P K ; C( W i,j ); P roof P Ki,j ; pk psu Pj ) #verification #P roof P Ki,j := (a; c; r), r = (d, e) 1 Σ P K .ver(C( W i,j ); a; c; r) := (c == H(C( W i,j )||a||pk psu Pj )) (g d model e n model == a(C( W i,j )) c ) 2 return Yes or No are given in Algorithm 5 and Algorithm 6, respectively. In addition, P K refers to the -protocols achieved by f sprove 1 and f sver 1 . Correspondingly, CD is for theprotocols realized by f sprove 2 (Algorithm 7) and f sver 2 (Algorithm 8) for proving decryption correctness. Note that in Algorithm 6 and Algorithm 8, the sign '==' is used to judge whether two values between '==' are equal.  Lemma 2 and Lemma 3 refer to the property of zeroknowledgeness and soundness in UVCDN protocol's Definition 1, respectively. The concrete proofs can be found in Section 3.2 of <ref type="bibr" target="#b62">[63]</ref>. As described in Lemma 2, a simulator without any knowledge of witness of an honest party can provide a proof of encryption correctness, which has statistically indistinguishable distribution compared with a real one. Lemma 3 means that the probability that the extractor E fails to extract the witness (x, r) of an adversary is negligible, with respect to system security parameter λ.</p><formula xml:id="formula_19">Algorithm 7: f sprove 2 (Σ CD ; C i , C i,j , v, v j ; s j ; pk psu Pj ) #announcement 1 Σ CD .ann(C i , C i,j , v, v j ; s j ) := u ∈ R [0, 2 2k+2k2 ], a = C 4u i , b = v u #k = log 2 n model , k 2 is the security param. #challenge 2 c = H(C i ||C i,j ||v||v j ||a||b||pk psu Pj ) #response 3 Σ P K .res(C i , C i,j , v, v j ;</formula><p>In terms of CD , the corresponding properties of zeroknowledge and soundness for public auditability of correctness decryption are described in Lemma 4 and Lemma 5, respectively. Also, the concrete proofs can refer to Section 4 of reference <ref type="bibr" target="#b63">[64]</ref>. Lemma 4. Given (C i , C i,j , v, v j ), and c ∈ C where C is a finite set called the challenge space, then we have</p><formula xml:id="formula_20">{r ∈ R [0, 2 2k+2k2 ]; a = C 4r i (C i,j ) -2c , b = v r (v j ) -c : (a, b; c; r)} ≈ {u ∈ R [0, 2 2k+2k2 ], a = C 4u i , b = v u ; r = u + c s j : (a, b; c; r)}</formula><p>where symbol ≈ means that the two distributions are statistically indistinguishable.</p><p>The above formula means that there exists a simulator without knowledge of s j can provide a proof that has a statistically indistinguishable distribution compared with a real one. Lemma 5. Given (C i , C i,j , v, v j ) and (a, b; u) is generated by the announcement Σ CD .ann. Malicious prover provides two different challenges c, c with respect to the announcement. There exists an extractor E that can extract the witness of an adversary A, if A can present two conversations r and r for (a, b; u), that is,</p><formula xml:id="formula_21">|1 -Pr[A(C i , C i,j , v, v j ; s j ; a, b; u; c; c ) → (r; r ); E(C i , C i,j , v, v j ; a, b; c; c ; r; r ) → s j ]| ≤ negl(1 λ )</formula><p>Lemma 5 shows that extractor E can extract the witness s j of A with overwhelming probability, with respect to system security parameter λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Fairness Guarantee for Collaborative Training.</head><p>Recall that we employ two security mechanisms in Blockchain, namely, the trusted time clock mechanism and secure monetary penalty mechanism, to enhance fairness during collaborative training, by following the work <ref type="bibr" target="#b56">[57]</ref>. With the trusted time clock mechanism, operations in a contract are forced to finish before the respective time point, as shown in function checkTimeout() in Algorithm 1 and 2. On the other hand, we also define two secure monetary penalty functions for gradient collecting and collaborative decryption, respectively.</p><p>In order to prove the property of fairness, Bentov et. al <ref type="bibr" target="#b56">[57]</ref> introduced the definition of secure computation with coins (SCC security) in the multi-party setting in a hybrid model that not only involves standard secure computation <ref type="bibr" target="#b64">[65]</ref>, but also special secure computation dealing with coins. Here, the goal of security refers to fairness presented in their paper. Also, they considered universally composable (UC) security proof for SCC security. In particular, compared to the initial definition of UC security, the view of environment in SCC security additionally indicates the distribution of coins because of the added functionality of monetary penalty.</p><p>In DeepChain setting, based on the tutorial in Bentov et. al's work, the property of fairness for gradient collecting and collaborative decryption is claimed in Section 4.2.4. Our work only replaces the general computation with the special computation to realize functionalities of gradient collecting and collaborative decryption. Other components based on Blockchain, including trusted time clock and monetary penalty exchange, remain unchanged. Thus, the UCstyle SCC security defined in Bentov et. al's work can be guaranteed for the specialized functionalities in DeepChain setting, only if SCC security has been proved according to UC composition theorem (refer it to Section 5 of reference <ref type="bibr" target="#b65">[66]</ref>). This is demonstrated by Lemma 6, where the environment Z becomes a distinguisher, by following the UC-style proof. If Z with non-uniform probabilistic polynomial-time computation could not distinguish the distribution in the ideal model from that of the hybrid model, then a protocol π SCC realizes a functionality f . Lemma 6. Given an input z, security parameter λ, a distinguisher Z, an ideal process IDEAL, an ideal adversary S in IDEAL, an ideal function f , and a protocol π that interacts with ideal function g in a model with adversary A, then we have</p><formula xml:id="formula_22">{IDEAL f,S,Z (λ, z)} λ ∈ N, z ∈ 0, 1 * ≡ c {HYBRID g,π,A,Z (λ, z)} λ ∈ N, z ∈ 0, 1 *</formula><p>where ≡ c means that the distributions are computationally indistinguishable.</p><p>Lemma 7. Let π be a protocol and f a multiparty function.</p><p>We say that π securely computes f with penalties if π SCC-realizes the functionality f * .</p><p>Furthermore, based on Lemma 7 where f is a multiparty function, the security defined for fairness is extended to the multi-party setting as shown in Lemma 6 (as shown by Definition 2 of the work <ref type="bibr" target="#b56">[57]</ref>). With protocol π, F is SSCrealized as F * GradientCollecting and F * CollaborativeDecryption , meaning that they achieve secure gradient collecting and collaborative decryption with penalties, respectively. With these two functionalities and the trusted time clock mechanism, we can guarantee fairness in gradient collecting and collaborative decryption, as shown in Algorithm 3 and 4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">IMPLEMENTATION AND EVALUATION</head><p>In this section, we implement our DeepChain prototype. First, we build a Blockchain to simulate DeepChain. Blockchain nodes are regarded as parties and workers, and they participate in trading and interact with two pre-defined smart contracts, i.e., Trading Contract and Processing Contract. Generated transactions are serialized in the Blockchain.</p><p>We use Corda V3.0 <ref type="bibr" target="#b66">[67]</ref> to simulate DeepChain for its adaptability and simplification. Specifically, Corda project is created by R3CEV and has been widely used in banks and financial institutes. It is a decentralized ledger that has some features of Bitcoin and Ethereum <ref type="bibr" target="#b67">[68]</ref>, such as data sharing based on need-to-know basis and deconflicting transactions with pluggable notaries. A Corda network contains multiple notaries, and our consensus protocol introduced in section 4.2.5 can be executed on them. We build nodes and divide them into parties and workers. Specifically, we set up two CorDapps which agree on Blockchain. The nodes of one CorDapp serve as parties, and the nodes of the other CorDapp play the role of workers. According to the application program interface (API) of Corda, we implement our business logic by integrating three main components, namely, State, Contract, and Flow. In particular, an instance of State is used to represent a fact of a kind of data, and it is immutable once an instance of State is known by all nodes at a specific time point. Contract is used to instantiate some rules on transactions. A transaction is considered to be contractually valid if it follows every rule of the contract. An instance of Flow defines a sequence of steps for ledger updates, e.g., how to launch a transaction from a node to another node.</p><p>We build the deep learning environment with Python (version 3.6.4), Numpy (version 1.14.0), and Tensorflow (version 1.7.0). We select the popular MNIST dataset <ref type="bibr" target="#b68">[69]</ref> which contains 55, 000 training samples, 5, 000 verification samples and 10, 000 test samples. Then, we split randomly this dataset into 10 equi-sized subsets, i.e., each contains 55, 000/10 = 5, 500 samples. Then, we conduct multiple training experiments with 4, 5, 6, 7, 8, 9, and 10 parties, denoted as E-4, E-5, E-6, E-7, E-8, E-9, and E-10, respectively. In each experiment, each party possesses one subset of the dataset.  <ref type="table" target="#tab_5">3</ref>.</p><p>Threshold Paillier algorithm is implemented in JAVA. We set the number of bits of modulus n model to 1024 bits, which corresponds to security level of 80 bits. It is worth noting that before executing the encryption algorithm, the weight matrices are assembled into a vector, so that only one cipher is generated for a party.</p><p>We implement the above building blocks to form three modules, i.e., CordaDeepChain, TrainAlgorithm, and Cryp-toSystem. We evaluate the feasibility of model training on  DeepChain in a multi-party setting by using 4 metrics, that is cipher size, throughput, training accuracy and total cost of time. In particular, we evaluate DeepChain on a desktop computer with 3.3GHz Intel(R) Xeon(R) CPU and 16GB memory. Then, for each metric we average the final results over 10 trails. As can be seen in Figure <ref type="figure" target="#fig_9">4</ref>, the size of cipher remains constant when we encrypt different amounts of gradients. On the other hand, as the number of gradients increases, the throughput decrease steadily, as shown in Figure <ref type="figure" target="#fig_10">5</ref>.</p><p>In terms of training accuracy, we show that the more parties participate in collaborative training, the higher the training accuracy. For this purpose, we create 7 experimental scenarios with 4, 5, 6, 7, 8, 9, and 10 parties, respectively. Each party trains the local model with her training dataset that contains 5,500 samples. Obviously, the more parties in a scenario, the larger the size of the total dataset, for example, the size of the total dataset is 5,500*4 for E-4, and 5,500*10 for E-10.</p><p>By sharing gradients on DeepChain, each individual party obtains updated parameters contributed by the gradients from other parties. Specifically, the updated parameters are    As shown in Fig. <ref type="figure" target="#fig_11">6</ref> and<ref type="figure" target="#fig_12">7</ref>, we can see that for both cases collaborative parties achieve higher training accuracy than the baseline party. Specifically, in Fig. <ref type="figure" target="#fig_11">6</ref> the baseline party has an accuracy of 96.75%, whereas Party 1, Party 2, Party 3, and Party 4 achieve 97.08%, 96.96%, 97.20% and 97.32% accuracy, respectively. Similarly, in Fig. <ref type="figure" target="#fig_12">7</ref>, the accuracy of the baseline party is 96.51%, while the accuracy numbers of Party 1 to Party 10 are 96.72%, 97.02%, 96.63%, 97.12%, 97.14%, 96.62%, 96.75%, 97.14%, 96.90%, 97.02%, respectively.</p><p>Next, we investigate the time costs of our framework. Note that we use the same training model on MNIST dataset  for each experiment E-4, E-5, E-6, E-7, E-8, E-9 and E-10. The total execution time is the time spent by a party in the entire collaborative training process on DeepChain. We depict the training process in Fig. <ref type="figure" target="#fig_13">8</ref> by using an interaction diagram. It is worth noting that for efficiency, parties only share gradients with 100 times, i.e., every 15 iterations, instead of No. of iterations in each experiment. This assumption follows some researcher's suggestion that training accuracy is still acceptable when averaging gradients every 10 to 20 iterations <ref type="bibr" target="#b69">[70]</ref>.</p><p>Let the frequency of sharing gradient be #share, the number of iteration be #iteration, the period of sharing be #period ∈ <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> and the number of parties be N . Then, the total time cost can be computed as T ime ≈ #iteration #period × (t 15 iteration ) + #share × (t encrypt + t uploadByP arty + N × t downloadByW orker + N × t average + t uploadByW orker + ×t downloadByP arty + t decrypt ). For ease of explanation, we summarize all the time-related variables in Table <ref type="table" target="#tab_6">4</ref>.</p><p>We assume that in the above formula all the timerelated variables achieve their corresponding time costs when N = 1. When number of parties N increases, the total time cost also increases slightly, since t downloadByW orker and t average become greater after multiplication. On the other hand, t uploadByP arty , t uploadByW orker and t downloadByP arty grow a little bit with N , but their growth rates are negligible. The reason is that the time used to synchronize State in Corda platform is very short, so that t uploadByP arty increases slightly with N . For t uploadByW orker and t downloadByP arty , no matter how large N is, all parties' ciphers are aggregated into one cipher that is uploaded/downloaded by workers/parties by using relatively constant time. It is worth noting that the time cost is also determined by the size of the training model. Hence, both the number of iterations and the time for cryptographic operations increase with the size of the training model.</p><p>From Fig. <ref type="figure" target="#fig_14">9</ref> we can see that the total time cost grows steadily with the number of parties. Specifically, the total time costs for E-4, E-5, E-6, E-7, E-8, E-9, and E-10 are 391, 861 s, 392, 359 s, 394, 533 s, 394, 287 s, 395, 938 s, 397, 252 s and 398, 079 s, respectively. The reasons for this trend are that (1) when the number of parties increases, t uploadByP arty , t downloadByW orker , t average , t uploadByW orker and t downloadByP arty also increase, but the increments are not obvious in the total time, (2) more parties need longer waiting time to synchronize when sharing gradients, and (3) t encrypt and t decrypt dominate the large proportion of the total time cost that depends on the size of the training model instead of on the number of parties. In addition, based on our experiments it is expected that when the number of parties is more than 1306, the total time would increase significantly with N , because the time N × (t downloadByW orker + t average ) is greater than time (t encrypt + t decrypt ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper we present DeepChain, a robust and fair decentralized platform based on Blockchain for secure collaborative deep training. We introduce an incentive mechanism and achieve three security goals, namely confidentiality, auditability, and fairness. Specifically, we formalize the incentive mechanism based on Blockchain, which possesses compatibility and liveness properties. Through our incentive mechanism we demonstrate that participants are incentive to behave correctly with high probability.</p><p>For confidentiality of local gradients, we employ Threshold Paillier algorithm to protect the data. In particular, by combining a carefully designed component into the encryption algorithm, we achieve the goal that only one cipher is generated for a party. In addition to confidentiality, our DeepChain provides auditability and fairness. We use non-interactive zero-knowledge to prove auditability of the collaborative training process. We also design timeoutchecking and monetary penalty mechanisms to guarantee fairness among the participants. We implement a prototype of DeepChain and evaluate it on real dataset, in terms of cipher size, throughput, training accuracy and training time.</p><p>Our DeepChain may also have some impact on modelbased pricing market. Since DeepChain stores not only the training parameters, but also the trained models, which means that the trained models can be used for paid deep learning services when the model-based pricing market is mature. Participants who possess the trained models may have long-term financial benefits, since they can provide deep learning services to those who are unable to build the model by themselves but willing to pay for the services. On the other hand, all training processes and the model parameters are recorded, which could be used in transfer learning, for example, some information of the trained models could be re-used to train a new similar model. Of course, for transfer learning case, the security problems should be re-defined and analyzed, which are left in our future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .•</head><label>1</label><figDesc>Fig. 1. The left corresponds to traditional distributed deep training framework, while the right is our DeepChain. Here, Trading Contract and Processing Contract are smart contract in DeepChain, together guiding the secure training process, while Tx refers to transaction.</figDesc><graphic coords="2,312.00,43.70,251.99,122.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The incentive mechanism of DeepChain, where ω P and ω W represent the contribution of a party and a worker for maintaining vc, respectively, and π P and π W represent their payoffs, respectively.</figDesc><graphic coords="6,48.00,164.05,251.99,122.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>V alue(P rc(P )) =πP * (1 -P rvc(P )) -fP * P rvc(P )-ωP * P rc(P )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 , g sk P 2 ,</head><label>12</label><figDesc>..., g sk P n }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>commitSK model = (Enc(s1||r||Sign(s1||r)), ..., Enc(sN ||r||Sign(sN ||r))) here, || denotes concatenation. (6) The initial weights W 0,j of local model of party j. Each party provides her local model's initial weights that are encrypted by Paillier.Encrypt algorithm, i.e., C(W 0,j ) = g W0,j model • (k j ) n model , where k j ∈ Z * n model , j ∈ {1, ..., N }. (7) An amount of deposits d($Coin).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Configuration of time points in Processing Contract. From top to bottom: (1) the timeline of collaborative training, (2) the timeline of trading (in Trading Contract), (3) the timeline of block creation. Here, the vertical orange bar refers to the interval between verified gradient transaction being uploaded and updated parameters to be downloaded. The vertical green bar refers to the interval between worker's transactions being sent and final updated result being uploaded.</figDesc><graphic coords="9,312.00,498.61,251.99,116.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>s j ; a, b; u, c) := r = u + c s j 4 return P roof CDi,j := (a, b; c; r) Algorithm 8: f sver 2 (Σ CD ; (C i , C i,j , v, v j ); P roof CDi,j ; pk psu Pj ) #verification #P roof CDi,j := (a, b;c; r) 1 Σ CD .ver(C i , C i,j , v, v j ; a, b; c; r) := (C 4r i == a(C i,j ) 2c ) (v r == b(v j ) c ) 2 return Yes or NoUnder the framework of UVCDN protocol, P K guarantees public auditability if there exist a simulator that can simulate correctness proofs of honest parties, and an extractor that can extract witnesses of corrupted parties which are illustrated by Lemma 2 and Lemma 3, respectively. Similarly, CD also guarantees public auditability shown by Lemma 4 and Lemma 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Lemma 2 .n model 1 ;Lemma 3 .</head><label>213</label><figDesc>Given X = C(x) = g x model r n model , x = W, r = k j , and c ∈ C where C is a finite set called the challenge space, then we have{d ∈ R Z n model ; e ∈ R Z * n model ; a := g d model e n model X -c : (a; c; d, e)} ≈ {a 1 ∈ R Z n model ; b 1 ∈ R Z * n model ; a := g a1 model b t := (a 1 + cx)/n model ; d := a 1 + cx; e := b 1 k c j g t model : (a; c; d, e)}where symbol ≈ means that the two distributions are statistically indistinguishable. Let X = C(x) = g x model r n model , where x = W and r = k. Given (a; s) that is generated by the announcement Σ P K .ann and two different challenges c, c with respect to the announcement, there exists an extractor E that can extract the witness of an adversary A, if A can present two conversations (d, e) and (d , e ) for (a; s), that is, |1 -Pr[A(X; x, r; a; s; c; c ) → (d, e; d , e ); E(X; a; c; c d, e; d , e ) → (x , r ) = (x, r)]| ≤ negl(1 λ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Our training model derives from Convolution Neural Network (CNN) with structure: Input → Conv → Maxpool → Fully Connected → Output. The weights and bias parameters in Conv layer, Fully Connected layer and Output layer are w 1 = (10, 1, 3, 3) and b 1 = (10, 1), w 2 = (1960, 128) and b 2 = (1, 128), w 3 = (128, 10) and b 3 = (1, 10), respectively. We summarize other training parameters in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Impact of No. of gradients on cipher size.</figDesc><graphic coords="14,60.61,136.95,226.77,132.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Impact of No. of gradients on throughput.</figDesc><graphic coords="14,60.61,589.27,226.77,132.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Training accuracy for the case of four parties.</figDesc><graphic coords="14,324.62,225.79,226.77,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Training accuracy for the case of ten parties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Interaction in the entire collaborative training process.</figDesc><graphic coords="15,48.00,43.70,252.00,145.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Total time cost with increasing number of parties.</figDesc><graphic coords="15,60.61,226.47,226.77,151.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Jilian</head><label></label><figDesc>Zhang received his M.S. degree from Guangxi Normal University, China in 2006, and Ph.D degree from Singapore Management University in 2014. He is currently an associate professor with College of Cyber Security, Jinan University, Guangzhou China. Jilian Zhang's research interests include data management, query processing, and database security. His work has been published on international journals and conferences, including IEEE TKDE, ACM SIGMOD, VLDB, and IJCAI. Ming Li received his B.S. in electronic information engineering from University of South China in 2009, and M.S. in information processing from Northwestern Polytechnical University in 2012. From 2016, he started his Ph. D. at Jinan University. His research interests include crowdsourcing, blockchain and its privacy and security. Yue Zhang received his M.S. degree and B.S. degree from Xi'an University of Posts &amp; Telecommunications in 2014 and 2016, respectively. Since September 2016, he is a Ph.D. student with School of Information Science and Technology in Jinan University. His research interests include Blockchain, system security, android security, etc. Weiqi Luo received his B.S. degree and M.S. degree from Jinan University in 1982 and 1985 respectively, and Ph.D. degree from South China University of Technology in 1999. Currently, he is a professor with School of Information Science and Technology in Jinan University. His research interests include network security, big data, artificial intelligence, etc. He has published more than 100 high-quality papers in international journals and conferences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Summary of notations</cell></row><row><cell cols="2">Notation Description pk psu P a pseudo-generated public key of party P</cell></row><row><cell>sk P</cell><cell>a secret key of the party P</cell></row><row><cell>q</cell><cell>a randomly selected big prime</cell></row><row><cell>G 1</cell><cell>cyclic multiplicative cyclic groups of prime order q</cell></row><row><cell>g</cell><cell>a generator of group G 1</cell></row><row><cell>Z  *  q</cell><cell>{1,2, ..., q-1}</cell></row><row><cell>H 1</cell><cell>a collision-resistant hash function mapping any string into an element in Z  *  q</cell></row><row><cell>H 2</cell><cell>a collision-resistant hash function mapping</cell></row><row><cell></cell><cell>any string into an element in G 1</cell></row><row><cell>C()</cell><cell>a cipher generated by Paillier.Encrypt algorithm</cell></row><row><cell>Enc()</cell><cell>the encryption by individual parties</cell></row><row><cell>modelco</cell><cell>collaborative deep learning model</cell></row><row><cell></cell><cell>(collaborative model for short) to train</cell></row><row><cell>Σ P K</cell><cell>correctness proof for a ciphertex that</cell></row><row><cell></cell><cell>is indeed encrypted by a party's public key</cell></row><row><cell>Σ CD</cell><cell>correctness proof for a decryption share</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Example of Threshold Configuration for #adversaries</figDesc><table><row><cell cols="2">Number of parties Maximum number</cell><cell>Threshold</cell></row><row><cell cols="2">of adversaries</cell><cell></cell></row><row><cell>n = 4</cell><cell>1</cell><cell>t ∈ {2, 3, 4}</cell></row><row><cell>n = 5</cell><cell>1</cell><cell>t ∈ {2, 3, 4, 5}</cell></row><row><cell>n = 6</cell><cell>1</cell><cell>t ∈ {2, 3, 4, 5, 6}</cell></row><row><cell>n = 7</cell><cell>2</cell><cell>t ∈ {3, 4, 5, 6, 7}</cell></row><row><cell>n = 8</cell><cell>2</cell><cell>t ∈ {3, 4, 5, 6, 7, 8}</cell></row><row><cell>4.2.4 Collaborative training</cell><cell></cell><cell></cell></row><row><cell cols="3">Based on stated assets, parties who have similar deep</cell></row><row><cell cols="3">learning task can constitute a collaborative group, and the</cell></row><row><cell cols="3">collaborative training process consists of the following four</cell></row><row><cell>steps.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>A collaborative model model co to be trained. For a collaborative model model co , parties agree on the training neural network, the training algorithms, and configurations of the network such as number of network layers, number of neurons per layer, size of mini-batch and number of iterations. Beside those information, they also reach a consensus on initial weights W 0 of model co . Note that weights W i would be updated to W i+1 after the i-th iteration of training. They protect W 0 by applying Paillier.Encrypt algorithm, i.e., C(W 0 ) = g W0 model •(k 0 ) n model , where k 0 is randomly selected from Z * n model . Note that we compute g W0 model with the help of the chosen super increasing sequence, i.e., g W0 model = g</figDesc><table><row><cell>l,</cell></row><row><cell>and (2) (g 1 model , ..., g l l i=1 α i • N • d &lt; n model . We then compute model ) = (g α1 model , ..., g α l model ).</cell></row><row><cell>(4) α1•w 1 0 +...+α l •w l 0</cell></row><row><cell>model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>} . Assert time T t &lt; T t1 . Receive (input, sid, T t , pk psu Pj∈C , C( W), P roof P Kj , H , h × d($Coin)) from S. Assert time T t &lt; T t1 . 2 Compute f sver 1 (C( W), P roof P Kj ) for pk psu P j∈{1,...,N } , and record {1, ..., N } \ C . 3 Send(return, d($Coin)) to pk psu P j∈{1,...,N }\C after T t1 . 4 If S returns (continue, H ), then send (output, Y es or N o) to pk psu P j∈{1,...,N } , and send (payback, (h -h )d($Coin)) to S, and send (extrapay, d($Coin)) to pk psu P j∈H , else if S returns (abort), send (penalty, d($Coin)) to pk psu P j∈{1,...,N } . Compute f sver 2 (C, C j , P roof CDj ) for pk psu P j∈{1,...,N } and record {1, ..., N } \ C .</figDesc><table><row><cell>Algorithm 3: F  *  GradientCollecting</cell></row><row><cell>1 Receive (input, sid, T t , pk psu Pj , C( W), P roof P Kj , d($Coin)) from pk psu P j∈{1,...,N Algorithm 4: F</cell></row></table><note><p>* CollaborativeDecryption 1 Receive (input, sid, T t , pk psu Pj , C, C j , P roof CDj , d($Coin)) from pk psu P j∈{1,...,N } . Assert time T t &lt; T t5 . Receive (input, sid, T t , pk psu Pj ∈ C, C, C j , P roof CDj , H , h * d($Coin)) from S. Assert time T t &lt; T t5 . 2 3 Send(return, d($Coin)) to pk psu P j∈{1,...,N }\C after T t5 ; 4 If S returns (continue, H ), then send (output, Y es or N o) to pk psu P j∈{1,...,N } , and send (payback, (h -h )d($Coin)) to S, and send (extrapay, d($Coin)) to pk psu P j∈H , else if S returns (abort), send (penalty, d($Coin)) to pk psu P j∈{1,...,N } .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 Training</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">configuration</cell></row><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>No. of iterations</cell><cell>1500</cell></row><row><cell>No. of epochs</cell><cell>1</cell></row><row><cell>Learning rate</cell><cell>0.5</cell></row><row><cell cols="2">Minimal batch size 64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Interpretations of time variants</figDesc><table><row><cell>Variant</cell><cell>Interpretation</cell></row><row><cell>t 15 iteration</cell><cell>time of 15 iterations</cell></row><row><cell>tencrypt</cell><cell>time to encrypt gradients</cell></row><row><cell>t uploadByP arty</cell><cell>time to upload gradients</cell></row><row><cell>t downloadByW orker</cell><cell>time to download gradients from</cell></row><row><cell></cell><cell>all parties</cell></row><row><cell>taverage</cell><cell>time to average all gradients</cell></row><row><cell>t uploadByW orker</cell><cell>time to upload updated parameters</cell></row><row><cell>t downloadByP arty</cell><cell>time to download parameters</cell></row><row><cell>t decrypt</cell><cell>time to decrypt parameters</cell></row><row><cell cols="2">calculated by taking the average of the gradients of all</cell></row><row><cell cols="2">parties. We also create an external party, denoted as baseline</cell></row><row><cell cols="2">party, who only trains the local model on her dataset with</cell></row><row><cell cols="2">5,500 samples, without taking into account the gradients</cell></row><row><cell cols="2">from other parties. For space limitation, we only give the</cell></row><row><cell>results for E-4 and E-10.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work was supported by National Key R&amp;D Plan of China (Grant No. 2017YFB0802203 and 2018YFB1003701), National Natural Science Foundation of China (Grant Nos. U1736203, 61732021, 61472165 and 61373158), Guangdong Provincial Engineering Technology Research Center on Network Security Detection and Defence (Grant No. 2014B090904067), Guangdong Provincial Special Funds for Applied Technology Research and Development and Transformation of Important Scientific and Technological Achieve (Grant No. 2016B010124009), the Zhuhai Top Discipline-Information Security, Guangzhou Key Laboratory of Data Security and Privacy Preserving, Guangdong Key Laboratory of Data Security and Privacy Preserving, National Joint Engineering Research Center of Network Security Detection and Protection Technology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pcanet: A simple deep learning baseline for image classification</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning in drug discovery</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gawehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular informatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A deep learning approach for cancer detection and relevant gene identification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Danaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hendrix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACIFIC SYMPOSIUM ON BIOCOMPUTING</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model accuracy and runtime tradeoff in distributed deep learning: A systematic study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Project adam: building an efficient and scalable deep learning training system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalyanaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Usenix Conference on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Privacy-preserving backpropagation neural network learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1554</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Privacy preserving backpropagation neural network learning over arbitrarily partitioned data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Privacy preserving back-propagation learning made practical with cloud computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="212" to="221" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Allerton Conference on Communication, Control, and Computing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="909" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Privacypreserving outsourced classification in cloud computing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cluster Computing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Privacy preserving deep computation model on cloud for big data feature learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1351" to="1362" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Practical secure aggregation for privacy-preserving machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kreuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcedone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1175" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Secureml: A system for scalable privacy-preserving machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mohassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="19" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning via additively homomorphic encryption</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moriai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1333" to="1345" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine learning models that remember too much</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="587" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Inference attacks against collaborative learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Cristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno>arX- iv:1805.04049</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep models under the gan: information leakage from collaborative deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hitaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez-Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="603" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding and controlling user linkability in decentralized learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Orekondy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05838</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Knock knock, who&apos;s there? membership inference on aggregate location data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pyrgelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Troncoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Cristofaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06145</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How to backdoor federated learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Estrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00459</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Health insurance portability and accountability act</title>
		<ptr target="http-s://www.hhs.gov/hipaa/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Identifying inference attacks against healthcare data repositories</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ohno-Machado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Summits on Translational Science Proceedings</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page">262</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multilingual acoustic models using distributed deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="8619" to="8623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An incentive compatible reputation mechanism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jurca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EEE International Conference on E-Commerce</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003. 2003</date>
			<biblScope unit="page" from="285" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incentive-aware routing in dtns</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Network Protocols</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sprite: A simple, cheatproof, credit-based system for mobile ad-hoc networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2003. Twenty-second Annual Joint Conference of the IEEE Computer and Communications Societies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1987" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mobicent: a credit-based incentive system for disruption tolerant network</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 Proceedings IEEE INFOCOM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bitcoin: A peer-to-peer electronic cash system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nakamoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast quantum byzantine agreement</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hassidim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-seventh annual ACM symposium on Theory of computing</title>
		<meeting>the thirty-seventh annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="481" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Algorand: The efficient and democratic ledger</title>
		<author>
			<persName><forename type="first">S</forename><surname>Micali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01341</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Algorand: Scaling byzantine agreements for cryptocurrencies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gilad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hemo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Micali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles</title>
		<meeting>the 26th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="51" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">{CHAINIAC}: Proactive software-update transparency via collectively signed skipchains and verified builds</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nikitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kokoris-Kogias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jovanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Khoffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cappos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">26th {USENIX} Security Symposium ({USENIX} Security</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1271" to="1287" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Searching an encrypted cloud meets blockchain: A decentralized, reliable and fair realization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2018-IEEE Conference on Computer Communications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="792" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Trustless machine learning contracts; evaluating and exchanging machine learning models on the ethereum blockchain</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Kurtulmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10185</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zerocash: Decentralized anonymous payments from bitcoin</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Sasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Miers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tromer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Virza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2014 IEEE Symposium</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Zerocoin: Anonymous distributed e-cash from bitcoin</title>
		<author>
			<persName><forename type="first">I</forename><surname>Miers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2013 IEEE Symposium</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="397" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hawk: The blockchain model of cryptography and privacy-preserving smart contracts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kosba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papamanthou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="839" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Neural networks: a comprehensive foundation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Prentice Hall PTR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Scalable deep learning on distributed gpus with a gpu-specialized parameter server</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Theano-mpi: A theano-based distributed training framework</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
			<biblScope unit="page" from="800" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters</title>
		<author>
			<persName><surname>Poseidon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distributed deep learning models for wireless signal classification with low-cost spectrum sensors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Giustiniano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lenders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pollin</surname></persName>
		</author>
		<idno>abs/1707.08908</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Distributed deep learning on edge-devices: Feasibility via adaptive compression</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monga</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piantino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7580</idno>
		<title level="m">Fast convolutional nets with fbfft: A gpu performance evaluation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep image: Scaling up image recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02876</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Purine: A bi-graph based deep learning framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6249</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Model-based pricing for machine learning in a data marketplace</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koutris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno>arX- iv:1805.11450</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Incentivizing outsourced computation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belenkiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Erway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lysyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd international workshop on Economics of networked systems</title>
		<meeting>the 3rd international workshop on Economics of networked systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deepchain: Auditable and privacy-preserving deep learning with blockchainbased incentive</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<ptr target="https://eprint.iacr.org/2018/679" />
	</analytic>
	<monogr>
		<title level="j">Cryptology ePrint Archive</title>
		<imprint>
			<date type="published" when="2018">2018/679. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Innovative instructions and software model for isolated execution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mckeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Alexandrovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berenzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Rozas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Savagaonkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HASP@ ISCA</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sharing decryption in the context of voting or lotteries</title>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Fouque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poupard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Financial Cryptography</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="90" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distributed paillier cryptosystem without trusted dealer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nishide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Information Security Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="44" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">How to share a secret</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="612" to="613" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">How to use bitcoin to design fair protocols</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bentov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Cryptology Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="421" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">How to use bitcoin to incentivize correct computations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bentov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2014 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="30" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Epidemic algorithms for replicated database maintenance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Irish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sturgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Swinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth annual ACM Symposium on Principles of distributed computing</title>
		<meeting>the sixth annual ACM Symposium on Principles of distributed computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Tendermint: Byzantine fault tolerance in the age of blockchains</title>
		<author>
			<persName><forename type="first">E</forename><surname>Buchman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Public-key cryptosystems based on composite degree residuosity classes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paillier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on the Theory and Applications of Cryptographic Techniques</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="223" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Universally verifiable multiparty computation from threshold homomorphic cryptosystems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schoenmakers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veeningen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Applied Cryptography and Network Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient protocols based on probabilistic encryption using composite degree residue classes</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Damgård</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jurik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BRICS Report Series</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Practical threshold signatures</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shoup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Foundations of cryptography: volume 2, basic applications</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Universally composable security: A new paradigm for cryptographic protocols</title>
		<author>
			<persName><forename type="first">R</forename><surname>Canetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2001 IEEE International Conference on Cluster Computing</title>
		<meeting>2001 IEEE International Conference on Cluster Computing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Corda: an open source distributed ledger platform</title>
		<ptr target="http-s://docs.corda.net/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Ethereum: A secure decentralised generalised transaction ledger</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gavin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Ethereum Project Yellow Paper</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J B</forename><surname>Yann Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Jiasi Weng received the B.S. degree in software engineering from South China Agriculture University</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Currently, she is a Ph.D. student with School of Information Science and Technology in Jinan University. Her research interests include applied cryptography</title>
		<imprint>
			<date type="published" when="2015-06">2015. June 2016</date>
		</imprint>
	</monogr>
	<note>Experiments on parallel training of deep neural network using model averaging. Blockchain, network security</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
