<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OneFlow: Redesign the Distributed Deep Learning Framework from Scratch</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-28">28 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinhui</forename><surname>Yuan</surname></persName>
							<email>&lt;yuanjinhui@oneflow.org&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">OneFlow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinqi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OneFlow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OneFlow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juncheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OneFlow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ran</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OneFlow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shenghang</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OneFlow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OneFlow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Yang</surname></persName>
							<email>yangf@zhejianglab.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Zhejiang Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Yi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhao</surname></persName>
						</author>
						<title level="a" type="main">OneFlow: Redesign the Distributed Deep Learning Framework from Scratch</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-28">28 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.15032v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning frameworks such as TensorFlow and PyTorch provide a productive interface for expressing and training a deep neural network (DNN) model on a single device or using data parallelism. Still, they may not be flexible or efficient enough in training emerging large models on distributed devices, which require more sophisticated parallelism beyond data parallelism. Plugins or wrappers have been developed to strengthen these frameworks for model or pipeline parallelism, but they complicate the usage and implementation of distributed deep learning. Aiming at a simple, neat redesign of distributed deep learning frameworks for various parallelism paradigms, we present OneFlow, a novel distributed training framework based on an SBP (split, broadcast and partial-value) abstraction and the actor model. SBP enables much easier programming of data parallelism and model parallelism than existing frameworks, and the actor model provides a succinct runtime mechanism to manage the complex dependencies imposed by resource constraints, data movement and computation in distributed deep learning. We demonstrate the general applicability and efficiency of OneFlow for training various large DNN models with case studies and extensive experiments. The results show that OneFlow outperforms many well-known customized libraries built on top of the state-of-the-art frameworks. The code of OneFlow is available at: https://github.com/Oneflow-Inc/oneflow.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning (DL) models have become increasingly complicated and large <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr" target="#b3">Brown et al., 2020;</ref><ref type="bibr" target="#b9">Fedus et al., 2021;</ref><ref type="bibr" target="#b18">Kaplan et al., 2020)</ref>. Severe challenges arise for existing DL frameworks such as TensorFlow <ref type="bibr" target="#b1">(Abadi et al., 2016)</ref> and PyTorch <ref type="bibr" target="#b33">(Paszke et al., 2019)</ref> for training large-scale DL models, which were designed in the early days without initially foreseeing the emerging requirements, e.g., model/pipeline parallelism of large models <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b14">Huang et al., 2019;</ref><ref type="bibr" target="#b40">Wang et al., 2019)</ref>.</p><p>Depending on the structure of neural networks (NN) and hardware configuration, various parallelism schemes find their best usage <ref type="bibr" target="#b2">(Ben-Nun &amp; Hoefler, 2019)</ref>. Data parallelism is especially suitable for DL models with a relatively small set of parameters (usually less than tens of millions of parameters), where near-linear speed-up can be achieved once back propagation maximally overlaps with gradient/parameter communication <ref type="bibr">(jea, 2021;</ref><ref type="bibr" target="#b11">Hashemi et al., 2019</ref>; and pipeline parallelism are for models with a more significant number of parameters, which probably cannot fit into a single device or the communication cost is too high for data parallelism. Stanza <ref type="bibr" target="#b42">(Wu et al., 2018)</ref> and DLPlacer <ref type="bibr" target="#b32">(Pal et al., 2019)</ref> adopt data parallelism for training the convolutional layers and model parallelism for other layers in convolutional neural network (CNN) models. OptCNN <ref type="bibr" target="#b15">(Jia et al., 2018)</ref> parallelizes CNN model training by splitting operations along batch and channel dimensions on homogeneous devices. Tofu <ref type="bibr" target="#b40">(Wang et al., 2019)</ref> utilizes a partition-n-reduce method to split a single operation into sub-operations and deploy partitions on multiple GPUs. FlexFlow <ref type="bibr" target="#b16">(Jia et al., 2019)</ref> searches the SOAP (sample, operation, attribute, parameter) space to exploit parallelism within and across operations.</p><p>In the best case, a distributed DL framework should be able to automatically generate the physical execution plan for any chosen parallelism scheme, minimizing manual programming efforts of users. Then a more advanced requirement is that the framework should be able to find the most appropriate parallelism strategy for any combination of NN structure and hardware configuration <ref type="bibr" target="#b38">(Shazeer et al., 2018)</ref>. However, existing DL frameworks cannot even accomplish the first goal, i.e., flexibly supporting various parallelism strategies. This is the exact problem we aim to address in this paper, with a novel redesign of distributed training framework.</p><p>Some emerging open-source projects develop dedicated systems or customized libraries for better support of model or pipeline parallelism. For example, HugeCTR <ref type="bibr" target="#b30">(Oldridge et al., 2020)</ref> enables model parallelism for large-scale click-through rate estimation. Megatron-LMs <ref type="bibr" target="#b39">(Shoeybi et al., 2020;</ref><ref type="bibr" target="#b29">Narayanan et al., 2021)</ref> and <ref type="bibr">DeepSpeed (dee, 2021;</ref><ref type="bibr" target="#b36">Rajbhandari et al., 2021;</ref><ref type="bibr">2020)</ref> support model parallelism for pre-training large NLP models. InsightFace (ins, 2021) trains large-scale face recognition models with model parallelism. However, these systems are customized for specific applications, and cannot be assembled together to constitute a general solution due to compatibility issues.</p><p>Wrappers or plugins have also been proposed to enhance some mainstream DL frameworks (e.g., TensorFlow, Py-Torch) for better support of more complex parallelism schemes. Mesh-TensorFlow <ref type="bibr" target="#b38">(Shazeer et al., 2018)</ref> and GShard <ref type="bibr" target="#b22">(Lepikhin et al., 2020)</ref> provide APIs for developers to express a wide range of parallel computation patterns of DNNs on top of TensorFlow. GPipe <ref type="bibr" target="#b14">(Huang et al., 2019)</ref> and PipeDream <ref type="bibr" target="#b28">(Narayanan et al., 2019)</ref> use pipelining across distributed devices to address the limited memory capacity on each device for training large DNNs on TensorFlow and PyTorch respectively. FairScale (fairscale) integrates techniques from Megatron-LM and DeepSpeed to enable PyTorch with model parallelism and pipeline parallelism. Since the existing training frameworks were initially designed without forseeing such complicated parallelism, incremental improvements over the frameworks often yield non-negligible system overhead and require substantial engineering efforts from users.</p><p>What would a generic design and efficient implementation of distributed DL frameworks be if we could know the rapidly evolving large AI models and demand for various parallelism schemes in advance? Could the system be simpler and neater? In this paper, we explore such possibilities and present OneFlow, a novel DNN training framework built from scratch. OneFlow includes a holistic design from the compiler to the runtime based on the actor model. It adopts an SBP (split, broadcast and partial-value) abstraction, enabling various hybrids of data parallelism and model parallelism in a much easier manner than existing frameworks. The actor model provides a succinct runtime mechanism to manage complex dependencies imposed by resource constraints, data movement and computation in distributed training.</p><p>We demonstrate the general applicability and efficiency of OneFlow for training various large DNN models with extensive experiments, comparing to many representative state-of-the-art systems. The results show that, with a much simpler and more generic implementation, OneFlow achieves performance comparable to or slightly better than that of the major customized libraries which are built on top of the state-of-the-art frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>A DNN is typically expressed as a logical computation graph of operators (abbreviated as op) in DL frameworks, which is manually programmed or automatically converted by a compiler into a physical graph composed of optimized kernels for execution at runtime <ref type="bibr" target="#b1">(Abadi et al., 2016)</ref>. Distributed training involves mandatory communication ops for data (gradient, parameters, or activations) exchange among devices <ref type="bibr" target="#b23">(Li et al., 2014;</ref><ref type="bibr" target="#b10">Goyal et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2016a)</ref>. The inter-device bandwidth is still one or two orders of magnitude lower than that of data access within a device <ref type="bibr" target="#b17">(Jiang et al., 2020;</ref><ref type="bibr" target="#b28">Narayanan et al., 2019)</ref>. Therefore, a distributed DL framework should treat data movement as a first-class citizen as computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributing the Workload in Spatial Domain</head><p>Spatial Scheduling specifies how to spread the ops across multiple devices. Figure <ref type="figure">1</ref>  Manually arranging the communication ops in such hybrid parallelism case by case is labor-intensive, incurring significant obstacles in applying complex parallelism to new DL models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributing the Workload in Temporal Domain</head><p>Temporal Scheduling of dataflow in a DL job refers to scheduling execution of ops in a particular order to maxi- mize hardware utilization and system throughput. The best opportunity for performance improvement usually comes from overlapping communication and computation whenever possible. Execution dependencies are enforced within and across different instances (each mini-batch corresponds to an instance) on a physical graph when using synchronous stochastic gradient descent training <ref type="bibr" target="#b4">(Chen et al., 2016a)</ref>. In Figure <ref type="figure">1</ref>, for example, forward ops f 31 and f 41 cannot be scheduled ahead of the all-reduce op r 1 . On the other hand, data loading and pre-processing ops c 31 and c 41 can be performed simultaneously while the devices are processing the previous batch of data; back-propagation {b 11 , b 21 } and the all-reduce op r 2 can be executed in parallel, without hampering the correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Managing the Complex Dependencies</head><p>In mainstream DL frameworks, both data and control dependencies are represented with edges in the execution graph <ref type="bibr" target="#b1">(Abadi et al., 2016;</ref><ref type="bibr" target="#b33">Paszke et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2015)</ref>. Upon the completion of each op, the scheduler updates dependencies of the remaining ops and identifies ops that are ready to run (whose dependencies have all been resolved). Distributed DL often experiences increased complexity of execution dependencies and resource constraints <ref type="bibr" target="#b35">(Rajbhandari et al., 2020;</ref><ref type="bibr" target="#b14">Huang et al., 2019)</ref>.</p><p>Dependencies caused by resource sharing. The scheduler has to decide an appropriate execution order to avoid out-of-memory (OOM) errors or deadlocks when multiple ops share the same resource. Consider a simple example in Figure <ref type="figure">2</ref>. M 1 and M 2 are two data movement ops serving two computing ops O 1 and O 2 on the same device, respectively. O 1 and O 2 do not depend on each other and O 1 requires more device memory to execute than O 2 . M 1 and M 2 also need some device memory to store the output data. After M 1 and M 2 have occupied their memory, the free memory capacity can only satisfy O 2 but not O 1 , while both O 1 and O 2 are in the ready set of the scheduler (as in TensorFlow's) at the same time. If O 1 is scheduled first, the memory is insufficient; the system may either report an OOM error or block the scheduling thread, and the latter may cause a deadlock. To avoid this risk, it is better for the framework to specify an appropriate execution order in advance (e.g., adding control dependencies between ops in TensorFlow). If the system leverages pipelining to overlap data movement and computation, the issue becomes even more severe, as M 1 can execute simultaneously while O 1 waiting list and will be scheduled in the future when other dependencies are resolved.</p><p>In the above example, the framework has to expose the internal scheduler to users so that the inserted callback functions can correctly interact with the scheduler. However, substantial engineering efforts are required to modify the existing DL frameworks to achieve this, as none of the existing DL frameworks expose the underlying scheduler to users yet. Ideally, the framework should represent all the dependencies among all the ops (including data movement) explicitly in the graph. Once this is achieved, the graph executor at runtime can also be greatly simplified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Summary</head><p>We design OneFlow, with a compiler that can automatically generate a physical graph for data parallelism, model parallelism and pipeline parallelism. The compiler supports a full analysis of all types of dependencies (e.g., resource, data movement and computation) at compile-time. Furthermore, we design a succinct runtime for OneFlow based on actor model, which instantiates all types of dependencies with a unified approach of message passing among actors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE COMPILER</head><p>OneFlow's compiler takes a logical computation graph and the assigned hardware configuration as inputs and generates a physical graph describing the actual execution procedure. We assume each logical op is already assigned with an attribute placement, indicating on which nodes (i.e., physical machines) and devices the logical op will be deployed. Consequently, a logical tensor (i.e., the input or the output of a logical op) is also mapped to multiple physical tensors (i.e., the multiple correspondences on the devices where the logical op is placed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Specifying Parallelism of Each Tensor and Each</head><p>Operator among Assigned Devices We design SBP, a mathematical abstraction specifying the mapping between a logical tensor and the corresponding physical tensors, including split (S in short), broadcast (B) and partial-value (P). The example in Figure <ref type="figure" target="#fig_2">4</ref> demonstrates how a logical tensor with a shape of 2 × 2 is mapped to 2 physical tensors under 4 types of SBP mappings (each referred to as an SBP signature), namely split(0), split(1), broadcast, and partial-sum. split indicates that the physical tensors are obtained by splitting the logical tensor along a certain axis in a balanced manner. For example, the two tensors in the first column in Figure <ref type="figure" target="#fig_2">4</ref> are obtained by splitting the logical 2 × 2 tensor by row axis, while the two tensors in the second column are resulted in by splitting the logical tensor by column axis. As shown by the third column of Figure <ref type="figure" target="#fig_2">4</ref>, broadcast means that each physical tensor is an exact copy of the logical tensor. As demonstrated by the last column of Figure <ref type="figure" target="#fig_2">4</ref>, partial-value indicates that the physical tensors have the same shape as the logical tensor, and the logical tensor can be obtained by performing an element-wise reduction operation (e.g., sum, max, etc.) over all the physical tensors.</p><p>When SBP signatures of the input tensors of an op are given, SBP signature of its output tensor can also be determined. Take M atM ul as an example. Given a data tensor X and a weight tensor W , SBP signature of their product Y = XW can be inferred from those of X and W , as given in Table <ref type="table" target="#tab_2">1</ref>. For most operators, the rule for inferring the SBP of output tensor from the SBP of input tensors is straightforward. Take the first case in Table <ref type="table" target="#tab_2">1</ref> as an example, if X is split by  row (i.e., S(0)) and W is broadcast, the result Y will also be split by row (i.e., S(0)). Currently, we provide the SBP deduction rule for all the operators case by case and expect to automate the process in the future. With SBP signatures of an op's inputs and outputs, the parallelism strategy of the op is fully specified. For example, S(0), B for X, W in the first row of Table <ref type="table" target="#tab_2">1</ref> correspond to data parallelism, and B, S(1) for X, W in the second row indicates model parallelism.</p><formula xml:id="formula_0">P (sum) B B B A0 B0 M0 Y0 B1 M1 Y1 Logical View Device0 Device1 A0 split(0) B0 M0 Y0 split(0) A0 split(0) B0 M0 Y0 split(0) Boxing Y0 broadcast B1 split(1) M1 Y1 split(1) Y0 broadcast B1 split(1) M1 Y1 split(1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Data Routing</head><p>Producer and consumer of the same logical tensor may prefer different SBP signatures for the tensor. As illustrated in Figure <ref type="figure" target="#fig_3">5</ref>, two M atM ul ops are connected by a logical tensor Y 0 . S(0) is Y 0 's inferred SBP signature by M atM ul 0 ; however, M atM ul 1 expects its SBP signature to be B. In this case, a data-routing op for re-arranging or transforming the physical tensors of Y 0 is required between M atM ul 0 and M atM ul 1 . In distributed DL, the data-routing op for automatically transforming the intermediate physical tensors is usually one of the common collective communication primitives such as all2all, broadcast, reduce-scatter, all-reduce, all-gather, etc. We unify all such ops as a type of boxing ops. In the example of Figure <ref type="figure" target="#fig_3">5</ref>, the boxing op performs an all-gather operation internally.</p><p>The inserted boxing op may or may not incur communication cost. Table <ref type="table">2</ref> lists the data size transferred between successive SBP signatures, when the input tensors and the output tensors of the boxing op are on the same set or disjoint sets of devices, respectively. Tensor transformation across disjoint sets of devices always incurs communication costs, while tensor transformation within the same set of devices may not necessarily lead to data movement (e.g., B → S in Table <ref type="table">2</ref>, since the output tensor can be directly obtained Table <ref type="table">2</ref>. Data size transferred between successive SBP signatures. p1 (p2) is the number of devices where input (output) tensors are placed. |T | is the size of the logical tensor T .</p><formula xml:id="formula_1">SBP 1 → SBP2 Cost (same) Cost (disjoint) S(i) → S(i) 0 |T | S(i) → S(j) (i = j) p 1 −1 p 1 |T | all2all |T | S → B (p1 − 1) • |T | all-gather p2 • |T | S → P 0 |T | B → S 0 |T | B → B 0 p2 • |T | B → P 0 |T | P → S (p1 − 1) • |T | reduce-scatter p1 • |T | P → B 2(p1 − 1)•|T | all-reduce (p1 +p2 − 1) • |T | P → P 0 p1 • |T |</formula><p>from the input tensor located at the same device). This is useful for deciding the optimal parallelism strategy, that is, by selecting SBP signatures incurring the lowest communication costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Difference from GShard's Abstractions</head><p>Our SBP abstractions bear some similarities to those in GShard <ref type="bibr" target="#b22">(Lepikhin et al., 2020</ref>), 1 i.e., split (split in GShard) and broadcast (replicate in GShard). GShard further adds a shard annotation to generalize split to multi-dimensional split. In OneFlow, we use multi-dimensional split that unifies the split and shard in GShard. Besides split, we also generalize all other SBP signatures to multi-dimension. For example, a matrix can has an SBP signature as (S(0), B), in which S(0) specifies the parallelism strategy at the level of nodes while B indicates the parallelism strategy among devices inside the same node. As the deduction rule shown in Figure <ref type="figure">3</ref>, with multi-dimensional SBP, more advanced distributed matrix multiplication such as 2D SUMMA algorithm <ref type="bibr" target="#b43">(Xu et al., 2021)</ref> can be conveniently supported.</p><p>Further, we create the partial-value signature which GShard does not consider, but is necessary to make the annotation system complete. For example, Table <ref type="table" target="#tab_2">1</ref> lists all the valid SBP signatures for a matrix multiplication op (Y = XW ). If X uses S(1) and W uses S(0), the signature of Y will be P(sum), which cannot be described by either split (i.e., split and shard in GShard) or broadcast (i.e., replicate in GShard). GShard suggests performing reduce to combine the partial data to obtain the final result immediately after the un-reduced data are generated. However, sometime, maintaining the intermediate result as the partial-value is more efficient than immediately reducing 1 SBP and GShard are independently developed being unaware of each other, which can be proved by tracking the commit logs of OneFlow in GitHub.  the partial results. With partial-value, OneFlow allows the system to choose the optimal timing of inserting a boxing op (i.e., a reduce or all-reduce op). Take Y = U × V × W as an example. Suppose SBP signatures of U , V and W are S(1), S(0) and B, respectively. According to Table <ref type="table" target="#tab_2">1</ref>, SBP signature of the result of U ×V is P(sum). The partial result can be multiplied by W , since the product of P (sum) and B is valid and the resulting signature is P (sum). Without partial-value signature, a boxing op, which incurs additional communication cost, must be inserted before performing the second matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Programming Interface</head><p>The design objective of the programming interface is to keep the operator APIs and the model description the same between a single device version and a distributed one. For different distributed strategies, users only need to specify the placement and SBP signatures of some tensors. Consider the example in Figure <ref type="figure" target="#fig_3">5</ref> where M atM ul 0 and M atM ul 1 use data and model parallelism, respectively. The code snippet in Table <ref type="table" target="#tab_4">4</ref> illustrates how One-Flow achieves the respective parallelism. Two different placements are created in line 2 and line 3, where cuda indicates NVIDIA GPGPUs as accelerators, and {0 : [0, 1]} and {1 : [0, 1]} denote node and device placements (the number before the colon is the node ID and numbers in square brackets are device IDs). SBP signatures are created in lines 4-7. Lines 9, 10 and 14 specify the placement and SBP attribute of tensor A 0 , B 0 and B 1 , respectively. In line 11, SBP signature of Y 0 is then inferred (as split(0)). However, the M atM ul 1 at line 15 expects the SBP signature of Y 0 to be broadcast. Therefore, in line 13, the to consistent() method is used to add a boxing op between M atM ul 0 and M atM ul 1 as described in Section 3.2, which explicitly transforms the placement and SBP signatures of tensor Y 0 . In line 13, the to consistent() method transforms the placement and SBP signature of tensor Y 0 from split(0) to broadcast. We note that, since the placements of input tensors of M atM ul 0 and M atM ul 1 are different, i.e., P 0 and P 1, respectively, the two ops actually work with pipeline parallelism.</p><p>With its APIs, OneFlow does not require a user to program with various low-level communication primitives, but the user may need to specify appropriate placements and SBP signatures for each tensor. Placement and parallelism strategy making entails separate in-depth investigation, as studied in <ref type="bibr" target="#b16">(Jia et al., 2019;</ref><ref type="bibr" target="#b22">Lepikhin et al., 2020;</ref><ref type="bibr" target="#b40">Wang et al., 2019;</ref><ref type="bibr" target="#b28">Narayanan et al., 2019;</ref><ref type="bibr" target="#b14">Huang et al., 2019)</ref>. After OneFlow integrates those strategies to automatically infer optimal placement and parallelism strategy, users will no longer manually specify the attributes of tensors or explicitly call to consistent method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE RUNTIME</head><p>We adopt the actor model <ref type="bibr" target="#b13">(Hewitt et al., 1973)</ref> in runtime design. We use an actor as a thin wrapper for each op and abstract the dependencies and resources dedicated to the op as the actor's state. Actors interact with each other through message passing instead of function invocation. An actor's state is updated whenever it receives a message from others. We show that the actor model can elegantly solve various issues complicated to existing DL frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Actor Model</head><p>An actor in our runtime is associated with 4 components: • Registers. A register is simply a container holding memory addresses of tensors. An actor is usually associated with two types of registers: in register, used for tensors consumed by the actor, and out register, for tensors produced by the actor.</p><p>• Messages. Actor communicate with others by exchanging messages: a req message from a producer (i.e., the actor generating an output) to a consumer (i.e., the actor utilizing the output), notifying the consumer a register containing newly generated tensor can be read, and an ack message from a consumer to a producer indicating that the particular register is no longer required by the consumer.</p><p>• Actions. An action corresponds to the execution of an op that an actor is bound to (e.g., launching a GPU kernel or performing data movement).</p><p>• A state machine. Each actor keeps track of whether all the dependencies are resolved.</p><p>We next discuss the mechanism inside each actor's state machine and the message passing protocol. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Explicit Representation of Resource Dependency</head><p>Counters for both in and out registers. Each actor allocates a pre-determined number of out registers in the beginning, amounting to a fixed memory quota for each actor. If an actor has used up its quota, the next action will not be scheduled even all its input tensors have been ready, until some memory previously allocated to the actor can be recycled. To achieve such goal, we associate a counter with each register. The zero initialized in counter records the number of the tensors held by an in register which is ready to be consumed, while the non-zero initialized out counter represents free memory quota. Each action results in a decrease of some out counter. Only when the in counter equals to an expected non-zero values and the out counter is non-zero (indicating it has free memory to use), can the actor trigger an action.</p><p>In existing DL frameworks, the scheduler considers an op can start once its input tensors are ready, without taking into account whether it can later successfully acquire memory for the output. After the op is scheduled and only just before executing the action, the runtime tries to allocate memory for the op on the fly, which, however, may succeed or not. With in counter and out counter, OneFlow represents resource availability as an explicit dependency for the scheduler to decide whether an op is ready to execute. Consequently, the resource planning at compile-time and flow control at runtime are made possible.</p><p>Reference counting with message passing. Besides the in counter and out counter, we introduce an additional zeroinitialized reference counter for each out register recording the number of consumers who are referencing its content. A non-zero value of a reference counter for an out register indicates the register is in use and the content can not be modified. Therefore, the out counter depends on the reference counter. It turns out that the reference counter can be updated according to a message passing protocol:</p><p>• A producer sends a req message to a consumer and increases the reference counter of the out register relating to the message by one. A change from zero to non-zero of a reference counter results in the decrease of an out counter.</p><p>• On receiving a req message, the consumer knows an in register becomes available and increases the in counter by one.</p><p>• After using data from the in register, the consumer decreases the in counter by one and sends an ack message to the producer.</p><p>• On receiving an ack message from the consumer, the producer decreases the reference counter of the out register relating to the ack message, indicating the elimination of a reference on the out register. If the reference counter becomes zero again, the corresponding out counter increases by one, indicating the corresponding out register can be recycled for the future use.</p><p>In the above protocol, if an out register is being consumed by some consumer, its reference counter must be non-zero and it will be no longer used by the producer to put newly generated tensors. Such a mutual exclusion property safely enables a zero-copy mechanism: if a pair of producer and consumer reside on the same device, the consumer can just directly use the producer's output as input, without making another copy of the content as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Applications: pipelining and back pressure</head><p>Allowing the initial value of an out counter for a particular register to be larger than one facilitates the processing of different versions of data in parallel. actor runs independently, acting as a natural stage in a pipeline. Multiple versions of the same register can be deemed as a generalization of the double buffering technique used in traditional DL frameworks <ref type="bibr">(nvi, 2021)</ref> In Figure <ref type="figure">6</ref>, actor 1 has 3 out registers; actor 2 and actor 3 have 2 out registers respectively.</p><p>• At time 0 , actor 1 produces a register r 11 , while actor 2 and actor 3 are idle because their in counters are zero.</p><p>• At time 1 , actor 2 triggers an action because both its in counter and out counter are non-zeros. At the same time, actor 1 and trigger an action again (on a different microbatch) because its out counter is still non-zero.</p><p>• At time 2 , actions of all 3 actors can be triggered since all their requirements on registers are fulfilled.</p><p>Essentially, the actor-based protocol is equivalent to the credit-based flow control method in asynchronous transfer mode networks <ref type="bibr" target="#b21">(Kung et al., 1994)</ref>. It naturally enables back pressure for resource preservation. If all its out registers are in use, a producer stops processing due to out counter becoming zero and no available free out register to hold the new output tensor. Without this back pressure mechanism (as in existing frameworks), a producer may run out of memory quickly if the consumer blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE IMPLEMENTATION</head><p>We implement OneFlow using around 26K LoC in Python, 120K LoC in C++, and 10K LoC in CUDA. The actor runtime uses 3K LoC of C++, and the compiler module is implemented in 20K LoC of C++. 2 In the following, we present some implementation details of actor system.</p><p>Actor addressing and message routing. Similar to CUDA stream in Nvidia GPGPUs, we also abstract other hardware resources (e.g., network and CPUs) as FIFO queues. We ensure no implicit dependency is brought by sharing resources. For example, two separate CUDA streams are created for copy engine and compute engine. To minimize device context switch, OneFlow creates a dedicated OS thread for each hardware queue and the actors using the same queue (or hardware resource) are bound to the same OS thread (e.g., actor a and actor b in Figure <ref type="figure">7</ref>). With static binding among actor, device, OS thread and node, OneFlow assigns a unique and hierarchically organized 64bit address (or equivalently, ID) for each actor as shown in Figure <ref type="figure">8</ref>; IDs of the device, OS thread and the node (where the actor resides) can be parsed from some specific fields of an actor ID. With this ID translation mechanism, attaching the receiver actor's ID with the message suffices to route the message to its destination.</p><p>In OneFlow, actors running on the same OS thread share a FIFO message queue. For an actor to receive a message, the message is first put in the message queue of the corresponding OS thread, which polls the queue repeatedly, fetches the message and routes it to the intended receiver (e.g., case 3  in Figure <ref type="figure">7</ref>). There is also a local message queue on each OS thread. The message sent to a receiver on the same OS thread as the sender is put into a local message queue and is directly processed by the receiver without being polled by the OS thread (case 1 in Figure <ref type="figure">7</ref>).</p><p>Unifying the intra-and inter-node actor systems. We introduce an abstraction layer, the actor message bus, that provides a unified interface to route a message to its receiver no matter whether the receiver is on the same or another node. In Figure <ref type="figure">7</ref>, the message from actor a to actor d travels along the logical path { 2 , 4 }, while its actual path is { 2 , 5 , 6 , 7 }. Such abstraction hides low-level communication across networks.</p><p>Different from existing frameworks and libraries which insert Send and Recv ops at both sides of inter-node communication, OneFlow's compiler only inserts a networking actor at the consumer's side for pulling data from the producer's node to the consumer's node, once inter-node communication is detected. In Figure <ref type="figure">7</ref>, suppose actor e on node 1 requires the output of actor a on node 0; when generating the physical graph, the compiler creates actor d at node 1 whose sole responsibility is to pull the output of actor a from node 0 to node 1, so that actor e can consume the data as if the producer was on the same node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We demonstrate OneFlow's generality, flexibility and efficiency by implementing representative parallelisms and comparing with state-of-the-art libraries in various cases. Unless stated otherwise, we conduct experiments on a cluster of 4 machines inter-connected by a 100Gbps RoCE network. Each machine is equipped with 8 Nvidia Tesla V100 16G GPUs interconnected with NVLink.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data-preprocessing Pipeline</head><p>In many scenarios such as training small DL models in mixed precision mode with high-end GPGPUs, feeding data to computation renders a bottleneck in DNN training <ref type="bibr" target="#b20">(Kumar et al., 2020)</ref>. Figure <ref type="figure" target="#fig_6">9</ref> compares the throughput achieved by OneFlow and mainstream frameworks with various data loaders. DALI is a plugin developed by Nvidia for optimizing data loading for DL frameworks (nvi, 2021). In "synthetic data" cases, we use fake data generated in memory without the need for data loading from disks, representing the respective ideal cases. Tensorflow and Py-Torch's data loaders are able to overlap data loading and computation but perform much worse than using Nvidia DALI. Unlike using customized plugin such as DALI, One-Flow supports pipelining by just allocating two out registers for data loading, pre-processing and copying host to device ops as described in Section 4.3. Performance of One-Flow's data loader is close to that of the synthetic data case, indicating perfect piplelining between data loading actors and pre-processing actors. OneFlow achieves this without additional engineering efforts like DALI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data Parallelism</head><p>The existing DL frameworks have carried out the most extensive optimization on data-parallel training.</p><p>In the experiments of Figure <ref type="figure" target="#fig_7">10</ref>, MXNet is based on Horovod <ref type="bibr" target="#b37">(Sergeev &amp; Balso, 2018)</ref>; Tensorflow and Py-Torch use their native communication strategies, which lead to better performance than using Horovod. We observe that in the case of ResNet <ref type="bibr" target="#b12">(He et al., 2016)</ref>, One-Flow not only outperforms the official TensorFlow, Py-Torch and MXNet by 23%-31% with FP32 and 71%-213% with FP16 <ref type="bibr" target="#b26">(Micikevicius et al., 2018)</ref>, but also outperforms the highly optimized versions of these frameworks (those prefixed by NGC, using the same script as submitted by NVIDIA to MLPerf <ref type="bibr" target="#b25">(Mattson et al., 2020)</ref>) by 9%-30% with FP32 and 8%-47% with FP16. In terms of BERT <ref type="bibr">(Devlin et al., 2019)</ref>, OneFlow also achieves higher training throughput than NGC versions by 9%-47% with FP32 and around 55% with FP16. For each model, we carry out a lot of performance optimization to ensure the throughput of OneFlow on a single device comparable to or slightly better than that of other frameworks. In this way, the scalability of different frameworks can be compared based on almost the same baseline. Note that the BERT implementation in MXNet does not perform gradient clipping, which hence involves fewer computation. To perform a fair comparison between MXNet and OneFlow, we implement two versions of BERT on OneFlow, with and without gradient clipping, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Parallelism</head><p>We compare OneFlow with two customized DL libraries supporting model parallelism training, as official versions of TensorFlow and PyTorch do not support model parallelism.  <ref type="formula">1</ref>) Split( <ref type="formula">1</ref>) Split( <ref type="formula">1</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">InsightFace</head><p>Split( <ref type="formula">1</ref>)</p><p>Split( <ref type="formula">1</ref>)</p><p>Split( <ref type="formula">1</ref>)</p><p>Split( <ref type="formula">1</ref>)</p><p>Split( <ref type="formula">1</ref>)</p><p>Split( <ref type="formula">1</ref>)</p><p>Split( <ref type="formula">1</ref>  to configure appropriate SBP signatures for M atM ul and softmax ops that require model parallelism. Figure <ref type="figure" target="#fig_9">11a</ref> illustrates the transformation of physical tensors on four GPUs after setting SBP signature of the weight matrix as S(1). Figure <ref type="figure" target="#fig_9">11b</ref> demonstrates the details of a softmax op in the physical graph generated by the compiler. Note that, there are two reduce calculations within the softmax op. To minimize the communication cost incurred by global reduction, OneFlow first carries out local reduction within a device while performing the max and sum ops. In Figure 12, we observe that OneFlow's throughput slightly outperforms InsightFace's when training face recognition models with ResNet and MobileFaceNet as backbone networks respectively <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>. The physical execution plans used by both frameworks are essentially the same. However, the plan in InsightFace is generated with manual programming, while the plan in OneFlow is automatically produced by the compiler. OneFlow significantly eases the programming burden of model parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">HugeCTR</head><p>Wide &amp; Deep Leaning <ref type="bibr" target="#b8">(Cheng et al., 2016)</ref> is widely used in recommender systems, e.g., for click-through rates estimation. In production, to support click-through rates estimation for billions of IDs, the embedding matrices become too large for a single GPU's memory to hold </p><p>0) for splitting the vocabulary IDs and S(1) for splitting the hidden dimension), and our compiler will automatically insert collective communication ops where necessary for model parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Parallelizing the Optimizer</head><p>Memory redundancy of model states (such as gradients, parameters, momentum and variances in Adam <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref>) in data parallelism can  be significantly reduced by sharding them across devices. ZeRO-DP <ref type="bibr" target="#b35">(Rajbhandari et al., 2020)</ref> leverages it to support distributed training of large models on devices with limited memory, with each device only holding part of the sharded model states. When the full model states are required, an all-gather communication primitive can be used. OneFlow is able to implement the same idea with less engineering efforts. Figure <ref type="figure" target="#fig_12">14</ref> illustrates the procedure of generating the physical graph on two devices by OneFlow, while implementing the same techniques as in ZeRO-DP with mixed precision enabled <ref type="bibr" target="#b26">(Micikevicius et al., 2018)</ref>. First, a conversion op (such as fp16 cast) is inserted. Second, our framework configures SBP signatures of the input of the cast op as S(0) and the output of the cast op as B.</p><p>Our compiler automatically generates the physical graph for both forward pass (Figure <ref type="figure" target="#fig_12">14a</ref>) and backward pass (Figure <ref type="figure" target="#fig_12">14b</ref>). Data routing ops are automatically inserted where appropriate. ZeRO-DP's implementation is based on PyTorch, using about 2K LoC. OneFlow implements the idea with 300 LoC, which is much simpler.</p><p>Figure <ref type="figure" target="#fig_3">15</ref> compares per-device memory footprint and throughput when training GPT-2, with the activation checkpoint <ref type="bibr" target="#b7">(Chen et al., 2016b</ref>) on (i.e., opt on) or off (i.e., opt off). We observe that OneFlow consumes less device memory but achieves higher throughput than ZeRO-DP, with or without the activation checkpointing optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Hybrid Parallelism</head><p>Megatron-LM <ref type="bibr" target="#b39">(Shoeybi et al., 2020</ref>) is a customized library for pre-training large models such as GPT-3 based on PyTorch. It supports data parallelism, model parallelism and hybrid parallelism which combines data and model parallelism (amounting to the two-dimensional SBP described in Section 3.3). It also implements activation checkpointing and synchronous pipeline with 1F1B pipeline sched- ule. We compare OneFlow and Megatron-LM for training GPT-2 under representative configurations in Figure <ref type="figure">16</ref>. The four sub-figures demonstrates the experiment results for pure data parallelism, pure model parallelism, hybrid of data parallelism and model parallelism, a combination of data, model and pipeline parallelism. As a generic framework, OneFlow implements all features that Megatron-LM supports, such as the activation checkpointing and 1F1B pipeline schedule techniques and align all the hyper-parameters. The physical execution plans of two frameworks are essentially the same. However, OneFlow performs more kernel fusions than Megatron-LM does. In the result, OneFlow outperforms Megatron-LM even with a single device. This is the major reason why OneFlow achieves higher training efficiency in distributed cases over the customized library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND DISCUSSIONS</head><p>We propose a new distributed deep learning framework OneFlow based on the concept of SBP and the actor model. OneFlow overcomes the complexity and efficiency issues of existing frameworks in supporting various parallelisms for training large DL models. The compiler uses the con-cise abstraction of SBP for automatically generating an effective execution plan for actors with both spatial and temporal scheduling enabled. The actor model unifies various dependencies as message passing and naturally supports pipelining, serving a novel mechanism for runtime of distributed DL frameworks. Finally, we show experiment results from a wide range of challenging tasks on real datasets to demonstrate that the design presented in this paper is more flexible and efficient than the existing ones.</p><p>Even though both OneFlow and Ray <ref type="bibr" target="#b27">(Moritz et al., 2018)</ref> use the concept of the actor, the granularities are different. In Ray, a single actor is used to manage a complete neural network while performing deep learning training. So far, Ray can only act as a plugin to enable data-parallelism to TensorFlow and PyTorch. It does not support model parallelism and pipeline parallelism.</p><p>There are still a number of areas that we are actively working on to improve OneFlow, including: (1) to enable OneFlow with elastic scaling <ref type="bibr" target="#b24">(Mai et al., 2020;</ref><ref type="bibr" target="#b31">Or et al., 2020)</ref> and fine-grained fault resilience <ref type="bibr" target="#b41">(Wang et al., 2021;</ref><ref type="bibr" target="#b44">Zaharia et al., 2013)</ref> besides the naive global checkpointing; (2) to implement auto placement and auto parallelism by designing a more efficient cost model, thus making it easier to use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1. A typical DL framework which translates the logical graph of a three-layer NN to a physical graph (or execution plan) on 4 inter-connected devices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>illustrates a training job with three computation ops f 1 , f 2 , f 3 scheduled onto four interconnected devices d 1 , d 2 , d 3 , d 4 . f 1 and f 2 are executed on d 1 and d 2 with data parallelism, and f 3 runs on d 3 and d 4 with model parallelism. An all-gather communication op g is inserted between {f 12 , f 22 } and {f 13 , f 23 } in the forward pass, while a reduce-scatter communication op s is required between {b 13 , b 23 } and {b 12 , b 22 } in the backward pass. Two all-reduce collective communication ops r 1 and r 2 are used to synchronize model parameters of f 1 and f 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Example of 4 SBP signatures to map a 2 × 2 logical tensor to two devices. Each block in the figure indicates an entry of a tensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Example showing data movement with a boxing op inserted, when translating a logical graph into a physical graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 5. 1 i m p o r t x f l o w a s F 2 P0 = F . p l a c e m e n t ( " c u d a " , { 0 : [ 0 , 1 ] } ) 3 P1 = F . p l a c e m e n t ( " c u d a " , { 1 : [ 0 , 1 ] } ) 4 a 0 s b p = F . s b p . s p l i t ( 0 ) 5 b 0 s b p = F . s b p . b r o a d c a s t 6 y 0 s b p = F . s b p . b r o a d c a s t 7 b 1 s b p = F . s b p . s p l i t ( 1 ) 8 9 A0 = F . r a n d n ( 4 , 5 , p l a c e m e n t =P0 , s b p = a 0 s b p ) 10 B0 = F . r a n d n ( 5 , 8 , p l a c e m e n t =P0 , s b p = b 0 s b p ) 11 Y0 = F . matmul ( A0 , B0 ) 12 13 Y0 . t o c o n s i s t e n t ( p l a c e m e n t =P1 , s b p = y 0 s b p ) 14 B1 = F . r a n d n ( 8 , 6 , p l a c e m e n t =P1 , s b p = b 1 s b p ) 15 Y2 = F . matmul ( Y0 , B1 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. An illustration of 3 message routing cases: sending message to an actor on the same thread, sending message to an actor on another thread in the same node, and sending message to an actor on another node. The CommNet in the figure indicates the low-level networking module in OneFlow. 1 11 22 43 63 node thread hardware queue actor Figure 8. Encoding of an actor's address.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Throughput comparison with various frameworks and data loaders: training ResNet50-V1.5 with mixed precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Data parallelism training of ResNet and BERT-base.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>The details of softmax op in the physical graph generated by compiler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Implementing model parallelism in InsightFace on four GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Model parallelism training: OneFlow vs. HugeCTR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Parallelizing the optimizer in OneFlow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Figure 15. Performance of optimizer sharding: OneFlow vs. ZeRO-DP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Valid SBP signatures for MatMul</figDesc><table><row><cell>X</cell><cell>W</cell><cell>Y = XW</cell></row><row><cell>S(0)</cell><cell>B</cell><cell>S(0)</cell></row><row><cell>B</cell><cell>S(1)</cell><cell>S(1)</cell></row><row><cell>S(1)</cell><cell>S(0)</cell><cell>P (sum)</cell></row><row><cell>P (sum)</cell><cell>B</cell><cell>P (sum)</cell></row><row><cell>B</cell><cell>P (sum)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Two valid two-dimensional SBP signatures for MatMul</figDesc><table><row><cell>X</cell><cell>W</cell><cell>Y = XW</cell></row><row><cell>(S(0), B)</cell><cell cols="2">(B, S(1)) (S(0), S(1))</cell></row><row><cell cols="2">(S(0), S(1)) (B, S(0))</cell><cell>(S(0), P )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Example program for implementing SBP signatures/parallelism of M atM ul0 and M atM ul1 in</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank the anonymous reviewers of OSDI 2021 and SOSP 2021 for their helpful comments on the paper. Developing a deep learning framework such as OneFlow involves a large amount of engineering efforts. We gratefully acknowledge contributions from our colleagues within One-Flow Inc. and Zhejiang Lab., and from the users of One-Flow. In particular, Wenxiao Zhang, Xiaoyu Zhang, Binbin Han, Jianhao Zhang, Houjiang Chen, Luyang Zhao, Yu Ouyang, Zekang Zheng, Xuan Xie, Yinggang Wang, Fengwei Liu, Shijie Wang, Xiaoyu Xu, Depeng Liang, Mingyang Liu, Shiyuan Shangguan, Jing Qiao, Chong Niu, Wei Zhang, Xuefei Jiang contribute a lot of code to One-Flow.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://developer.nvidia.com/DALI,2021" />
		<title level="m">NVIDIA Data Loading Library (DALI)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-scale Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation</title>
				<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACM Computing Surveys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00981</idno>
		<title level="m">Revisiting Distributed Synchronous SGD</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mobilefacenets: Efficient CNNs for Accurate Real-time Face Verification on Mobile Devices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Chinese Conference on Biometric Recognition</title>
				<meeting>Chinese Conference on Biometric Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed System</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LearningSys</title>
				<meeting>LearningSys</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">Training Deep Nets with Sublinear Memory Cost. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
				<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bert ; Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<ptr target="https://github.com/facebookresearch/fairscale" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019. fairscale. Facebook Fairscale project</title>
				<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019. fairscale. Facebook Fairscale project</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll ¢r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Accurate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Large Minibatch SGD: Training ImageNet in 1 Hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accelerating Distributed Deep Learning with Communication Scheduling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><surname>Tictac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Universal Modular ACTOR Formalism for Artificial Intelligence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Steiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Joint Conference on Artificial Intelligence</title>
				<meeting>the 3rd International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond Data and Model Parallelism for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation</title>
				<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling Laws for Neural Language Models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deveci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03641</idno>
		<title level="m">Exploring the Limits of Concurrency in ML Training on Google TPUs</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Creditbased flow control for atm networks: Credit update protocol, adaptive credit allocation and statistical multiplexing</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Rev</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="101" to="114" />
			<date type="published" when="1994-10">October 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scaling Distributed Machine Learning with the Parameter Server</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation</title>
				<meeting>the 11th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making Training in Distributed Machine Learning Adaptive</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagenländer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fertakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-O</forename><surname>Brabete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pietzuch</surname></persName>
		</author>
		<author>
			<persName><surname>Kungfu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation</title>
				<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bittorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pentecost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Janapa Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Robie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Training</forename><surname>Mlperf</surname></persName>
		</author>
		<author>
			<persName><surname>Benchmark</surname></persName>
		</author>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mixed Precision Training</title>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Distributed Framework for Emerging AI Applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation</title>
				<meeting>the 13th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PipeDream: Generalized Pipeline Parallelism for DNN Training</title>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
				<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04473</idno>
		<title level="m">Phanishayee, A., and Zaharia, M. Efficient Large-Scale Language Model Training on GPU Clusters</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Oldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frederickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koumchatzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zamora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yılmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Gunny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Merlin</surname></persName>
		</author>
		<title level="m">A GPU Accelerated Recommendation Framework</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Resource Elasticity in Distributed Deep Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Optimizing Multi-GPU Parallelization Strategies for Deep Learning Training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zulfiqar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Migacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
			<pubPlace>Micro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Generic Communication Scheduler for Distributed DNN Training Acceleration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
				<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Memory Optimizations Toward Training Trillion Parameter Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07857</idno>
		<title level="m">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Balso</surname></persName>
		</author>
		<author>
			<persName><surname>Horovod</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<title level="m">Fast and Easy Distributed Deep Learning in TensorFlow</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Supporting Very Large Models Using Automatic Dataflow Graph Partitioning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference</title>
				<meeting>the Fourteenth EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ownership: A Distributed Futures System for Fine-Grained Tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th USENIX Symposium on Networked Systems Design and Implementation</title>
				<meeting>the 18th USENIX Symposium on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Stanza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10624</idno>
		<title level="m">Distributed Deep Learning with Small Communication Footprint</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">An efficient 2d method for training super-large deep learning models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discretized streams: Fault-tolerant streaming computation at scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentyfourth ACM Symposium on Operating Systems Principles</title>
				<meeting>the twentyfourth ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
