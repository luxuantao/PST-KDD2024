<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNBERT: User-News Matching BERT for News Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<email>zhangqi193@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingjie</forename><surname>Li</surname></persName>
							<email>lijingjie1@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qinglin</forename><surname>Jia</surname></persName>
							<email>jiaqinglin2@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuyuan</forename><surname>Wang</surname></persName>
							<email>wangchuyuan@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaowei</forename><surname>Wang</surname></persName>
							<email>wangzhaowei3@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
							<email>hexiuqiang1@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UNBERT: User-News Matching BERT for News Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, news recommendation has become a popular channel for users to access news of their interests. How to represent rich textual contents of news and precisely match users' interests and candidate news lies in the core of news recommendation. However, existing recommendation methods merely learn textual representations from indomain news data, which limits their generalization ability to new news that are common in coldstart scenarios. Meanwhile, many of these methods represent each user by aggregating the historically browsed news into a single vector and then compute the matching score with the candidate news vector, which may lose the low-level matching signals. In this paper, we explore the use of the successful BERT pre-training technique in NLP for news recommendation and propose a BERT-based user-news matching model, called UNBERT. In contrast to existing research, our UNBERT model not only leverages the pre-trained model with rich language knowledge to enhance textual representation, but also captures multi-grained user-news matching signals at both word-level and newslevel. Extensive experiments on the Microsoft News Dataset (MIND) demonstrate that our approach consistently outperforms the state-of-the-art methods. * Corresponding Author 1. Florida 's favorite Halloween movie is what? 2. Without help from US, UN climate fund struggles. 3. The Best American Movies in 2020. 4. How to Sell a House in California : Make Movies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online news platforms such as Google News<ref type="foot" target="#foot_0">1</ref> and MSN News<ref type="foot" target="#foot_1">2</ref> have become a prevalent way for users to access news information <ref type="bibr" target="#b1">[Das et al., 2007;</ref><ref type="bibr" target="#b4">Lavie et al., 2010]</ref>. The large quantity of news articles generated everyday makes it hard for users to hunt for their interested news contents. Therefore, recommendation systems become necessary to provide personalized news recommendations to users according to their preferences <ref type="bibr" target="#b4">[Phelan et al., 2011;</ref><ref type="bibr" target="#b3">IJntema et al., 2010;</ref><ref type="bibr" target="#b2">De Francisci Morales et al., 2012]</ref>. The quality of news recommendation depends heavily on the understanding of the rich textual contents in news articles. However, traditional recommendation methods often use categorical features (e.g., news ids, news categories) or bag-of-words (tokens or n-grams) to model the news contents. Typical examples include LibFM <ref type="bibr">[Rendle, 2012]</ref> and DeepFM <ref type="bibr" target="#b3">[Guo et al., 2017]</ref>, which are classic recommendation models based on factorization machines. Many recent studies <ref type="bibr" target="#b5">[Wang et al., 2018;</ref><ref type="bibr" target="#b0">An et al., 2019;</ref><ref type="bibr" target="#b6">Wu et al., 2019a;</ref><ref type="bibr" target="#b7">Wu et al., 2019c;</ref><ref type="bibr" target="#b5">Wang et al., 2020]</ref> are devoted to the direction of neural news recommendation and propose to learn news representations in an end-to-end fashion. In these models, words are first embedded to low-dimensional embedding vectors, and then they leverage popular network architectures (e.g., CNNs and attention mechanisms) to learn hidden news representations for recommendation. Neural news recommendation models have made surprising performance improvements, but they still suffer from the cold-start problem. As we know, the cold-start problem is severe in new recommendation due to the rapid updates and short timeliness of news articles. To alleviate this issue, several recent studies <ref type="bibr" target="#b7">[Wu et al., 2019c;</ref><ref type="bibr" target="#b5">Wang et al., 2020]</ref> propose the use of pre-trained word embeddings (e.g., <ref type="bibr">Word2Vec [Mikolov et al., 2013]</ref> and <ref type="bibr">Glove [Pennington et al., 2014]</ref>) to initialize the embedding layers of their models, which has shown nonnegligible improvements. However, these pre-trained word embeddings are mostly context-independent and their effectiveness may be further weaken during training with a randomly initialized downstream model. Instead, in this paper, we take one step further to apply a pre-trained BERT model <ref type="bibr" target="#b2">[Devlin et al., 2018]</ref> in news recommendation. BERT is one of the most successful pre-trained language models in natural language processing (NLP), and has been widely used </p><formula xml:id="formula_0">E A E A E A E A E B E B E B E B E B E B E B E B E 0 E 1 E 2 E 3 E 4 E 5 E 6 E 7 E 8 E 9 E 10 E 11 E N1 E N1 E N1 E N1 E N2 E N2 E N2 E N3 E N3 E N3 E N4 E N4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Token Embedding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segment Embedding</head><p>Position Embedding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>News segment Embedding</head><p>News Sentence User Sentence</p><p>Figure <ref type="figure">2</ref>: UNBERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings, the position embeddings, and the News segmentation embeddings.</p><p>for various NLP tasks. We argue that BERT-based models could enhance the deep semantic modeling of news to better mitigate the cold-start problem, since both word embeddings and model parameters are pre-trained on the Web-scale textual data full of general-domain knowledge. Meanwhile, we adopt the pretain-finetune strategy to fine-tune the entire model in our in-domain news data for recommendation. How to precisely match users' interests and candidate news is another key factor for news recommendation <ref type="bibr" target="#b5">[Wang et al., 2020]</ref>. A lot of research has been done toward this goal. For instance, <ref type="bibr">DKN [Wang et al., 2018]</ref> and NPA <ref type="bibr" target="#b6">[Wu et al., 2019b]</ref> learn user representations based on the similarity between candidate news and previously clicked news. LSTUR <ref type="bibr" target="#b0">[An et al., 2019</ref>] models short-term and long-term user interests from the clicked news using GRUs. NAML <ref type="bibr" target="#b6">[Wu et al., 2019a]</ref> and NRMS <ref type="bibr" target="#b7">[Wu et al., 2019c]</ref> apply the attention networks for learning user representations. After obtaining each user vector and item vector separately, these models then match these two vectors to predict click probability. However, treating each news as a whole vector and encoding users and items separately may ignore some low-level matching signals (e.g., word-level relations) between users' interests and candidate news. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the candidate news has a strong semantic similarity to the 1st and 3rd browsed news because all of them are related to movies at the news level. But at the words level, some words such as "Florida" and "American" in the browsed news may mismatch the "Japan" in candidate news, and the user actually does not click the news. As such, the word-level matching signal which tells a user's location is not well utilized for news recommendation. In this paper, we propose a User-News matching BERT (namely UNBERT) model for news recommendation, which not only leverages the powerful pre-trained BERT model but also captures user-news matching signals at both news-level and word-level. The contributions of our work are summarized as follows:</p><p>• To the best of our knowledge, UNBERT is the first work to introduce the pre-trained BERT to capture user-news matching signals for news recommendation that takes full advantage of the out-domain knowledge.</p><p>• UNBERT proposes the idea of representing user by raw text of the browsed news directly, and learns user-news matching representation at both word-level and newslevel to capture multi-grained user-news matching signals through two matching modules. • Extensive experiments on the real-world dataset show that our approach can effectively improve the performance of news recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional methods utilize manual feature engineering to represent news and user for matching <ref type="bibr">[Son et al., 2013;</ref><ref type="bibr" target="#b0">Bansal et al., 2015]</ref>. For example, CCTM <ref type="bibr" target="#b0">[Bansal et al., 2015]</ref> leverages article and comment content through topic modeling and the co-commenting pattern of users through collaborative filtering to build news and user representations.</p><p>In recent years, several deep learning methods are proposed for news and user representations, and achieve better performance than traditional methods. For example, <ref type="bibr">DKN [Wang et al., 2018]</ref> and NPA <ref type="bibr" target="#b6">[Wu et al., 2019b]</ref> use CNN and personalized attention mechanism respectively to learn news representations, LSTUR <ref type="bibr" target="#b0">[An et al., 2019]</ref>, NAML <ref type="bibr" target="#b6">[Wu et al., 2019a]</ref> and NRMS <ref type="bibr" target="#b7">[Wu et al., 2019c]</ref> apply the attention networks in news representation, and these methods then learn the user representation by aggregating user's browsed news. In addition, matching-based method FIM <ref type="bibr" target="#b5">[Wang et al., 2020]</ref> performs fine-grained matching between segment pairs of each browsed news and the candidate news at each semantic level via stacked dilated convolutions. Different from these methods, our method introduces a capable pre-trained BERT model to alleviate the cold-start problem by introducing out-domain knowledge, learns the matching representations at both the word-level and news-level via self-attention to capture the multi-grained user-news matching signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>In this section, we first formulate the problem of news recommendation, and then elaborate on our UNBERT including the input and output as well as the model architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Given a user u and a set of candidate news V u = v 1 , v 2 , . . . , v |Vu| where u ∈ U , v ∈ V, U and V denote the set of user and news. Formally, model aims to generate scores for all candidate news by predicting their click probability of u. The score of the i-th candidate news v i for user u is denoted by ŷi = f (u, v i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input and Output</head><p>For a given user u and a candidate news v, we denote clicked news of user u as [n u 1 , n u 2 , . . . , n u |nu| ], where n u j (j = 1, . . . , |n u |) is the j-th clicked news by user u, and n v represents the candidate news v. In this study, each news is represented by the title<ref type="foot" target="#foot_3">3</ref> which is composed of a sequence of words, i.e., n u = [w 1 , w 2 , . . .], and w i represent the i-th word.</p><p>For news representation, its sequence of words is used as "News Sentence" as shown in Figure <ref type="figure">2</ref>. And for user representation, we simply concatenate the words of user's clicked news into a whole sequence as "User Sentence", where a segment token [NSEP] is added at begin of each news as the separated signal. As shown in Figure <ref type="figure">2</ref>, we add the special classification token [CLS] followed by these two sentences and a special token [SEP] to separate them. Finally, the "Input" token sequence to UNBERT is a combination of "News Sentence" and "User Sentence" with a set of special symbols.</p><p>The output of UNBERT is a matching score that stands for the probability that the user u will click the candidate news v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Architecture</head><p>In this section, we present the architecture details of our UN-BERT approach as shown in Figure <ref type="figure" target="#fig_1">3</ref>, including Embedding Layer, Word-Level Module, News-Level Module and Click Predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Layer</head><p>For a given input token, the corresponding embeddings including token, segment, position and news segment are generated and the input representation is constructed by summing these embeddings in our study. A visualization of this construction can be seen in Figure <ref type="figure">2</ref>.</p><p>The token, segment and position embeddings are pretrained using masked LM by masking 15% of all word-piece tokens in each sequence at random and predicting the masked words. The news segment embedding is randomly initialized and further updated in our fine-tuning task. The final input token representation E t in UNBERT is constructed by summing its corresponding token, segment, position and news segment embedding<ref type="foot" target="#foot_4">4</ref> :</p><formula xml:id="formula_1">E t = E token + E seg + E pos + E nseg (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word-Level Module</head><p>As illustrated in Figure <ref type="figure" target="#fig_1">3</ref>, the Word-Level Module (WLM) applies multiple Transformer Layers (TL) iteratively to compute the hidden representations at each layer for each word and propagate the matching signal at word-level simultaneously. Each TL contains two sub-layers: a Multi-Head Self- Multi-Head Self-Attention applies Scaled Dot-Product Attention as the attention function to learn the combination weights of the output:</p><formula xml:id="formula_2">Attention(Q, K, V ) = softmax QK T √ d k V (2)</formula><p>where Q, K and V represents the query, key and value matrix correspondingly, which are projected from E t matrix with different learned projection matrices as in Eq.( <ref type="formula">3</ref>), and 1</p><formula xml:id="formula_3">√ d k</formula><p>is a scaling factor to avoid extremely small gradients by producing a softer attention distribution. Multi-Head Self-Attention applies h attention functions in parallel to produce the output representations which are concatenated:</p><formula xml:id="formula_4">head i = Attention E t W Q i , E t W K i , E t W V i (3) M ultiHead(Q, K, V ) = [head 1 ; . . . ; head h ] W O (4)</formula><p>where W O , W Q i , W K i and W V i are learnable parameters, i represents i-th head. With the multi-head, the information from different representation subspaces at different positions of word-level is jointly learnable which would be very helpful for capturing the matching signal between different words.</p><p>Position-Wise Feed-Forward network is a fully connected feed-forward network (FFN) applied to each position separately and identically in order to endow the model with nonlinearity and interactions between different dimensions. This consists of two linear transformations with a ReLU activation in between:</p><formula xml:id="formula_5">F F (x) = ReLU (xW 1 + b 1 ) W 2 + b 2</formula><p>(5) where W 1 , W 2 and b 1 , b 2 are learnable parameters and shared across all positions.</p><p>In addition, the residual connection is introduced into each of the two sub-layers, as shown in Figure <ref type="figure" target="#fig_1">3</ref>, in order to make the model deeper and reduce training difficulty. Moreover, the layer normalization is applied to each sub-layer to normalize the inputs over all the hidden units in the same layer for stabilizing and accelerating the network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>News-Level Module</head><p>We aggregate the word's hidden representation of each news from WLM to the news representation, and then implement the other multiple Transformer Layers to capture the news-level matching signal in News-Level Module (NLM).</p><p>Denote the hidden representation for the i-th word obtained from WLM as w i , n j is the j-th news representation aggregated from its sequence of words S j where i ∈ S j . Here, we obtain the news representation n j from the word representation w i using the following three types of aggregators:</p><p>• NSEP Aggregator directly uses the embedding of the special token [NSEP] for news representation, due to the final hidden state corresponding to this token is designed to represent the token sequence <ref type="bibr" target="#b2">[Devlin et al., 2018]</ref>:</p><formula xml:id="formula_6">n j = w i where i = [NSEP] j<label>(6)</label></formula><p>• Mean Aggregator averages the words' embedding directly to form the news embedding:</p><formula xml:id="formula_7">n j = i ∈ Sj w i / |S j |<label>(7)</label></formula><p>• Attention Aggregator uses a light-weight attention network <ref type="bibr" target="#b0">[Bakhtin et al., 2018]</ref> to learn the combination weights of the word embedding matrix w. It applies a fully connected neural network with W h and b h , tanh as hidden layer activation function, and then another fully connected neural network with W o and b o to learn combination weights f defined in Eq.( <ref type="formula">8</ref>) and finally computes the linear combination of words embedding in Eq.( <ref type="formula">9</ref>).</p><formula xml:id="formula_8">f = tanh (wW h + b h ) W o + b o (8) n j = i ∈ Sj f i w i / i ∈ Sj f i (9)</formula><p>We then stack the same transformer layers as WLM to gather the information propagated from these news embeddings, and explore the news-level matching representation in NLM,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Click Predictor</head><p>The click predictor module is used to predict the probability of a user clicking a candidate news. Denote the word-level matching representation e w and the news-level matching representation e n as shown in Figure <ref type="figure" target="#fig_1">3</ref>, we concatenate these two representation vectors as multi-grained matching representation before applying a full connection layer:</p><formula xml:id="formula_9">y = softmax([e w ; e n ] W c + b c ) (10</formula><p>) where e w and e n refer to the word-level and new-level matching representation correspondingly, W c and b c are the learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on a real-world news recommendation dataset MIND<ref type="foot" target="#foot_5">5</ref>  <ref type="bibr" target="#b8">[Wu et al., 2020]</ref>  News<ref type="foot" target="#foot_6">6</ref> logs. There are two versions of the MIND datasets named MIND-large and MIND-small, where the MINDsmall is a small version of the MIND-large by randomly sampling the daily behavior logs with equal probability from the MIND-large. The detailed statistics of the datasets are shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Several standard evaluation metrics in the recommendation field are used including: AUC, MRR and nDCG@K with K = 5, 10. The performance is the average of these metrics on all impression logs. Since test set labels of MIND-large are not provided, the test performance is obtained through submission on the MIND News Recommendation Competition<ref type="foot" target="#foot_7">7</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Models and Training Details</head><p>In our experiments, MIND-small dataset is used to determine the parameter settings, then we train and evaluate on both small and large dataset. The bert-base-uncased is used as the pre-trained model to initialize the word-level module. We apply negative sampling with ratio 4 in consideration of being consistent with other baselines as well as the training efficiency, Adam [Kingma and Ba, 2014] is used for model optimization. The batch size is set to 128, the learning rate is set to 2e −5 , and 2 epochs are trained. All the hyper-parameters are tuned on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Evaluation</head><p>The performance of our approach is evaluated by comparing with the following methods: (1) LibFM [Rendle, 2012], which extracts TF-IDF <ref type="bibr" target="#b1">[Beel et al., 2016]</ref>  a fine-grained interest matching method for neural news recommendation; (9) UNBERT, our approach. For fair comparison, only title of news is adopted for all methods, in particular, we only use the TF-IDF features extracted from all news titles for LibFM and DeepFM. The results of these methods on two datasets are summarized in Table <ref type="table" target="#tab_2">2</ref>.</p><p>We have several observations from Table <ref type="table" target="#tab_2">2</ref>. First, the methods which learn word representations in an end-to-end manner such as DKN, NPA, NAML, LSTUR and NRMS outperform the neural recommendation methods that craft the features manually such as LibFM and DeepFM, which tells that these end-to-end methods are more suitable for news representation learning than the methods based on manual feature engineering.</p><p>Second, the methods of learning news representation using the attention mechanism (e.g., NPA, NAML, LSTUR and NRMS) outperform DKN. This is probably because the attention mechanism can model the interaction between the words, and learn the news representations more accurately by capturing relative importance of the interaction. Particularly, NRMS outperforms other attention-based methods because it adopts the useful multi-head self-attention to capture the relatedness between users' browsed news and the candidate news in news-level.</p><p>Third, FIM outperforms other methods because its pair- wise multi-level matching architecture can detect fine-grained matching signals not just news-level matching information. Finally, our approach can consistently outperform other baseline methods in terms of all metrics. The significant improvement indicates that the leverage of out-domain data through the pre-trained model can introduce rich language knowledge and enhance the textual representation. Moreover, this also validates advantage of the multi-grained user-news matching signals at both word-level and news-level via selfattention to predict the probability of a user clicking a candidate news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study of WLM and NLM</head><p>In order to study the effectiveness of word-level module and news-level module which aims to capture the multi-grained user-news matching signal, we test our model by disabling one part to evaluate another, yielding UNBERT with wordlevel signal (UNBERT word ), UNBERT with news-level signal (UNBERT news ) and the complete UNBERT, as shown in Table 3. The experiment is conducted on the MIND-small due to the submission limit on the MIND-large.</p><p>Firstly, we observe that UNBERT word and UNBERT news achieve a pretty good performance close to the full version of UNBERT, which confirms the effectiveness of these two level matching signals. Secondly, UNBERT news outperforms UNBERT word , which proves that the word-level is insufficient for its weakness on capturing news structure. Finally, the full version of UNBERT performs best, which tells that the multigrained matching signal is necessary for news recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Impact of Aggregator Type</head><p>We also study the impact of different aggregator type in NLM of UNBERT, on the final ranking performance in Table <ref type="table" target="#tab_5">4</ref> .</p><p>We observe that Attention Aggregator achieves the best performance, Mean Aggregator follows and NSEG Aggregator is the worst. Since NSEG Aggregator uses the embedding of a special token that can represent news from word embeddings to some extent, while Mean Aggregator aggregates word representation to news representation by mean reducing explicitly, which leads to a better performance of Mean Aggregator. Attention Aggregator applies attention mechanism and further improves the performance, which indicates that the relatedness between the words is a vital factor for news representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Effectiveness on Cold Start</head><p>This experiment studies the performance of different news recommendation methods on unseen news. Recall that the large quantity of unseen news are generated everyday, and effective method should be able to alleviate this problem. Therefore, we evaluate UNBERT on the test set of different days, and compare it with other models. We divide the MIND-small dataset into 7 groups by day, train and validate models on the first 3 days, and then test models on each day of the remaining 4 days.</p><p>As shown in figure <ref type="figure" target="#fig_2">4</ref>, our method consistently outperforms other methods in a large margin on different days, which proves the effectiveness of UNBERT. In addition, with the cold news growing over time, UNBERT performs stably, while the performances of other methods drop significantly, especially from 11/13/2019 to 11/14/2019. This illustrate the advantage of out-domain knowledge on alleviating the coldstart problem. The abnormal that the overall performance increases while the percentage of unseen news increases from 11/12/2019 to 11/13/2019 is due to the token distribution has not changed much between these two days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel method named User-News matching BERT (UNBERT) for news recommendation. Our method introduces a pre-trained model on the out-domain data to alleviate the cold-start problem, and captures multigrained user-news matching signals at both word-level and news-level through WLM and NLM. The experimental results show that UNBERT outperforms the state-of-the-art methods on the real-world dataset in terms of AUC, MRR, nDCG@5 and nDCG@10, and UNBERT ranks the first in the leaderboard of MIND competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A negative example: several news browsed by a user (upper box) and a candidate news (lower box). Orange bars represent the important signals related with green bar that should be captured.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall architecture of our UNBERT approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance trends of different methods on different days.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Attention sub-layer and a Position-wise Feed-Forward network[Vaswani et al., 2017]. Statistics of the datasets.</figDesc><table><row><cell></cell><cell></cell><cell>MIND-small</cell><cell></cell><cell></cell><cell>MIND-large</cell><cell></cell></row><row><cell></cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell># User</cell><cell>162,898</cell><cell>47,187</cell><cell>88,898</cell><cell>711,222</cell><cell>255,990</cell><cell>702,005</cell></row><row><cell># News</cell><cell>76,904</cell><cell>53,897</cell><cell>57,856</cell><cell>101,527</cell><cell>72,023</cell><cell>120,961</cell></row><row><cell># Impressions</cell><cell>199,998</cell><cell>50,002</cell><cell>100,000</cell><cell>2,232,748</cell><cell cols="2">376,471 2,370,727</cell></row><row><cell># Positive samples</cell><cell>300,357</cell><cell>75,183</cell><cell>153,963</cell><cell>3,383,656</cell><cell>574,845</cell><cell>-</cell></row><row><cell cols="6"># Negative samples 7,060,083 1,779,492 3,740,561 80,123,718 13,510,712</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Boldface indicates the best results (the higher, the better), while the second best is underlined. UNBERT-en represents the ensemble score based on UNBERT which is at the top of https://msnews.github.io/#leaderboard. The overall performance of different methods on MIND.</figDesc><table><row><cell>collected from MSN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>features from users' browsed news and candidate news, and concatenates them as the input; (2) DeepFM [Guo et al., 2017]: a deep factorization machine with the same features as LibFM; (3) DKN [Wang et al., 2018], a deep news recommendation method based on knowledge-aware CNN; (4) NPA [Wu et al.,</figDesc><table><row><cell>2019c], a neural news rec-</cell></row><row><cell>ommendation method using multi-head self-attention to learn</cell></row><row><cell>user and news representations; (8) FIM [Wang et al., 2020],</cell></row></table><note>2019b], a neural news recommendation method with personalized attention mechanism; (5) NAML<ref type="bibr" target="#b6">[Wu et al., 2019a]</ref>, a neural news recommendation approach with attentive multiview learning; (6) LSTUR<ref type="bibr" target="#b0">[An et al., 2019]</ref>, a neural news recommendation method using GRU to learn user representations; (7) NRMS[Wu et al.,  </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of UNBERT single word-level signal and news-level signal.</figDesc><table><row><cell></cell><cell>AUC</cell><cell>MRR</cell><cell cols="2">nDCG@5 nDCG@10</cell></row><row><cell cols="2">UNBERT word 0.6733</cell><cell>0.3138</cell><cell>0.3429</cell><cell>0.4058</cell></row><row><cell cols="2">UNBERT news 0.6735</cell><cell>0.3147</cell><cell>0.3455</cell><cell>0.4079</cell></row><row><cell>UNBERT</cell><cell>0.6762</cell><cell>0.3172</cell><cell>0.3475</cell><cell>0.4102</cell></row><row><cell></cell><cell>AUC</cell><cell>MRR</cell><cell cols="2">nDCG@5 nDCG@10</cell></row><row><cell>UNBERT nseg</cell><cell>0.6746</cell><cell>0.3162</cell><cell>0.3460</cell><cell>0.4088</cell></row><row><cell cols="2">UNBERT mean 0.6758</cell><cell>0.3151</cell><cell>0.3453</cell><cell>0.4079</cell></row><row><cell>UNBERT att</cell><cell>0.6762</cell><cell>0.3172</cell><cell>0.3475</cell><cell>0.4102</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The ranking performance with different aggregator.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://news.google.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.msn.com/en-us/news</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">In this paper, we only adopt news titles as input, since title is the decisive factor affecting users' choice of reading. But note that our approach can be easily generalized to any sort of news-related texts like the abstract.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">The position embeddings is not used in our approach because the performance will be worse when using it.Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">https://msnews.github.io Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">https://www.msn.com/en-us/news</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7">https://competitions.codalab.org/competitions/24122# participate</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Content driven user profiling for comment-worthy recommendations of news and blog articles</title>
		<author>
			<persName><forename type="first">An</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07705</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015">2019. 2019. 2018. 2018. 2015. 2015</date>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 9th ACM Conference on Recommender Systems</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Google news personalization: scalable online collaborative filtering</title>
		<author>
			<persName><surname>Beel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
				<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2016. 2016. 2007. 2007</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
	<note>Research-paper recommender systems : a literature survey</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From chatter to headlines: harnessing the real-time web for personalized news recommendation</title>
		<author>
			<persName><forename type="first">De</forename><surname>Francisci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morales</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM international conference on Web search and data mining</title>
				<editor>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</editor>
		<meeting>the fifth ACM international conference on Web search and data mining<address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<publisher>Gianmarco De Francisci Morales</publisher>
			<date type="published" when="2012">2012. 2012. 2018. 2018</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Pre-training of deep bidirectional transformers for language understanding</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepfm: a factorization-machine based neural network for ctr prediction</title>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<idno>arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Frank Goossen, Flavius Frasincar, and Frederik Hogenboom</title>
				<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<publisher>Wouter IJntema</publisher>
			<date type="published" when="2010">2017. 2017. 2010. 2010. 2014. 2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A location-based news article recommendation with explicit localized semantic analysis</title>
		<author>
			<persName><surname>Lavie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</title>
				<editor>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</editor>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010. 2010. 2013. 2013. 2014. 2014. 2011. 2011. 2012. 2013. 2017. 2017</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zheng Liu, and Xing Xie. Fine-grained interest matching for neural news recommendation</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2020. 2020</date>
			<biblScope unit="page" from="836" to="845" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 world wide web conference</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural news recommendation with attentive multi-view learning</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05576</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019a. 2019. 2019b. 2019</date>
			<biblScope unit="page" from="2576" to="2584" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural news recommendation with multi-head self-attention</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019c. 2019</date>
			<biblScope unit="page" from="6390" to="6395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mind: A largescale dataset for news recommendation</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="3597" to="3606" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
