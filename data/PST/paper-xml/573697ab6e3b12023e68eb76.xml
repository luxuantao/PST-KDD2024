<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Classification and Retrieval are ONE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
							<email>hongrc@hfut.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer and Information</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<postCode>230009</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Sci&amp;Tech</orgName>
								<orgName type="laboratory">LITS</orgName>
								<orgName type="institution" key="instit1">TNLIST</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>qitian@cs.utsa.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at San Antonio</orgName>
								<address>
									<postCode>78249</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Classification and Retrieval are ONE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B4CA1046C6ECE1F08DDDEA7BF35BEDF</idno>
					<idno type="DOI">10.1145/2671188.2749289</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.4.10 [Image Processing and Computer Vision]: Image Representation-Statistical</term>
					<term>I.4.7 [Image Processing and Computer Vision]: Feature Measurement-Feature representation Algorithms, Experiments, Performance Image Classification</term>
					<term>Image Retrieval</term>
					<term>ONE</term>
					<term>CNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we demonstrate that the essentials of image classification and retrieval are the same, since both tasks could be tackled by measuring the similarity between images. To this end, we propose ONE (Online Nearest-neighbor Estimation), a unified algorithm for both image classification and retrieval. ONE is surprisingly simple, which only involves manual object definition, regional description and nearest-neighbor search. We take advantage of PCA and PQ approximation and GPU parallelization to scale our algorithm up to large-scale image search. Experimental results verify that ONE achieves state-of-the-art accuracy in a wide range of image classification and retrieval benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Past decades have witnessed an impressive bloom of multimedia applications based on image understanding. For example, the number of categories in image classification has grown from a few to tens of thousands <ref type="bibr" target="#b13">[13]</ref>, and deep Convolutional Neural Networks (CNN) have been verified efficient in large-scale learning <ref type="bibr" target="#b25">[25]</ref>. Meanwhile, image retrieval has been transplanted from toy programs to commercial search engines indexing billions of images, and new user intentions such as fine-grained concept search <ref type="bibr" target="#b62">[62]</ref> are realized and proposed in this research field. . On a query image, it is possible to find a number of semantic objects. Searching for nearest neighbors with one object might not capture the exact query intention, but fusing yields satisfying results.</p><p>A yellow circle with the word TP indicates a true-positive image. Images are collected from the Holiday dataset <ref type="bibr" target="#b20">[20]</ref>.</p><p>Both image classification and retrieval receive a query image at a time. Classification tasks aim at determining the class or category of the query, for which a number of training samples are provided and an extra training process is often required. For retrieval, the goal is to rank a large number of candidates according to their relevance to the query, and candidates are considered as independent units, i.e., without explicit relationship between them. Both image classification and retrieval tasks could be tackled by the Bagof-Visual-Words (BoVW) model. However, the ways of performing classification <ref type="bibr" target="#b10">[10]</ref> <ref type="bibr" target="#b26">[26]</ref> and retrieval <ref type="bibr" target="#b46">[46]</ref> <ref type="bibr" target="#b38">[38]</ref> are, most often, very different. Although all the above algorithms start from extracting patch or regional descriptors, the subsequent modules, including feature encoding, indexing/training and online querying, are almost distinct.</p><p>In this paper, we suggest using only ONE (Online Nearestneighbor Estimation) algorithm for both image classification and retrieval. This is achieved by computing similarity between the query and each category or image candidate. Inspired by <ref type="bibr" target="#b4">[4]</ref>, we detect multiple object proposals on the query and each indexed image, and extract high-quality features on each object to provide better image description.</p><p>On the online querying stage, the query's relevance to a category or candidate image is estimated by the averaged nearest distance from querying objects to the objects in that category or candidate image. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, extracting more objects helps to find various visual clues and obtain better results. To improve efficiency, we leverage the idea of approximate nearest-neighbor search, and take advantage of GPU parallelization for fast computation. Experiments are performed on a wide range of image classification/retrieval datasets. Our algorithm achieves state-of-the-art accuracy with reasonable computational overheads.</p><p>The major contribution of this paper is summarized in three aspects. First, we reveal the possibility of unifying image classification and retrieval systems into ONE. Second, ONE achieves the state-of-the-art accuracy on a wide range of image classification and retrieval tasks, defending both training-free models for image recognition and regional features for near-duplicate object retrieval. Finally, we make full use of GPU parallelization to alleviate heavy online computational overheads, which might inspire various multimedia applications and research efforts in the future.</p><p>The remainder for this paper is organized as follows. Section 2 briefly introduces related works. Section 3 illustrates the ONE algorithm and key acceleration techniques. After showing experiments in Section 4, we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Classification</head><p>Image classification is a fundamental task which is aimed at categorizing images according to their semantic contents. Recent years, researchers propose to extend conventional tasks <ref type="bibr" target="#b26">[26]</ref> <ref type="bibr" target="#b16">[16]</ref> in two aspects, i.e., from coarse-grained to finegrained <ref type="bibr" target="#b33">[33]</ref>[49] <ref type="bibr" target="#b36">[36]</ref>, and from small-scale to large-scale <ref type="bibr" target="#b13">[13]</ref>.</p><p>The Bag-of-Visual-Words (BoVW) model is widely adopted to represent images with high-dimensional vectors. It often consists of three stages, i.e., descriptor extraction, feature encoding and feature summarization. Due to the limited descriptive power of raw pixels, local descriptors such as SIFT <ref type="bibr" target="#b28">[28]</ref>[54] and HOG <ref type="bibr" target="#b11">[11]</ref> are extracted. A visual vocabulary or codebook is then built to capture data distribution in the feature space. Descriptors are thereafter quantized onto the codebook as compact feature vectors <ref type="bibr" target="#b63">[63]</ref>[51] <ref type="bibr" target="#b37">[37]</ref> <ref type="bibr" target="#b55">[55]</ref>, and summarized as an image-level representation <ref type="bibr">[26][17]</ref>[57] <ref type="bibr" target="#b59">[59]</ref>. These feature vectors are normalized <ref type="bibr" target="#b56">[56]</ref>, and fed into generalized machine learning algorithms <ref type="bibr" target="#b15">[15]</ref> <ref type="bibr" target="#b41">[41]</ref> for training and testing. Besides, there are also efforts on designing trainingfree classification systems <ref type="bibr" target="#b4">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Retrieval</head><p>Image retrieval is closely related to a number of real-world multimedia applications. Given an image database and a query, it requires finding relative candidates to the query in a short time. As the rapid growth of the Internet <ref type="bibr" target="#b40">[40]</ref>, large-scale image search attracts more and more attentions in both academic and industrial fields <ref type="bibr" target="#b62">[62]</ref>.</p><p>The BoVW model is also widely adopted for image retrieval <ref type="bibr" target="#b34">[34]</ref>. Handcrafted descriptors such as SIFT <ref type="bibr" target="#b28">[28]</ref> <ref type="bibr" target="#b58">[58]</ref> and SURF <ref type="bibr" target="#b3">[3]</ref> are extracted on the detected regions-of-interest (ROI) of an image <ref type="bibr" target="#b29">[29]</ref> <ref type="bibr" target="#b30">[30]</ref>. A codebook, often with a large size, is trained in an approximate manner <ref type="bibr" target="#b38">[38]</ref> to capture data distribution. Descriptors are quantized onto the codebook in an either hard <ref type="bibr" target="#b38">[38]</ref> or soft manner <ref type="bibr" target="#b39">[39]</ref>. Codebook training-free methods <ref type="bibr" target="#b73">[73]</ref> are also suggested for feature encoding. Then the flowchart differs from classification in the way of organizing a huge number of features. An inverted index <ref type="bibr" target="#b46">[46]</ref> is constructed to store the relationship between images and features. When a query image comes, descriptors are extracted and quantized accordingly to look up the inverted index. When the initial search results are available, they are often sent into a post-processing stage for higher precision and recall. Popular post-processing approaches include geometric verification <ref type="bibr" target="#b8">[8]</ref>, query expansion <ref type="bibr" target="#b9">[9]</ref> and diffusion-based algorithms <ref type="bibr" target="#b23">[23]</ref>[60].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Convolutional Neural Networks</head><p>Convolutional Neural Networks (CNN) are based on the theory that a multi-layer network is able to fit any complicated functions. In the early years, neural networks are verified efficient on simple recognition tasks <ref type="bibr" target="#b27">[27]</ref>. Recent years, the available of large-scale training data (e.g., ImageNet <ref type="bibr" target="#b13">[13]</ref>) and powerful GPUs make it possible to train deep CNNs which significantly outperform the BoVW models <ref type="bibr" target="#b25">[25]</ref>.</p><p>A CNN is often composed of a number of layers. In each layer, responses from the previous layer are convoluted and activated by a differentiable function. Thus, the network could be formulated as a composed function, and the error signals produced by the difference between supervised and predicted labels could be back-propagated. Some recently proposed tricks are also crucial to help CNNs converge faster and prevent over-fitting, such as the ReLU activation function and the dropout technique <ref type="bibr" target="#b25">[25]</ref>. It is suggested that deeper networks produce better recognition results <ref type="bibr" target="#b45">[45]</ref> <ref type="bibr" target="#b47">[47]</ref>, and intermediate responses of CNNs could also be transplanted onto other image applications <ref type="bibr" target="#b14">[14]</ref> <ref type="bibr" target="#b43">[43]</ref>. The discussion of different configurations in CNNs is available in <ref type="bibr" target="#b6">[6]</ref> <ref type="bibr" target="#b65">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Approximate Nearest-neighbor Search</head><p>It is a common demand that finding the nearest neighbor of a D-dimensional query vector in a large database of N candidates. Since an exhaustive search algorithm requires as many as O(N D) computations which is often intractable, approximate algorithms are adopted to accelerate this process with reasonable accuracy loss.</p><p>One family of solutions involve constructing a data structure for efficient lookup. Based on the k-d tree <ref type="bibr" target="#b18">[18]</ref> or other similar structures <ref type="bibr" target="#b50">[50]</ref>, efficient Approximate Nearest Neighbor (ANN) search algorithm is proposed <ref type="bibr" target="#b31">[31]</ref>, and further applied on other applications such as large-scale vector clustering <ref type="bibr" target="#b38">[38]</ref>. A notable drawback of such methods is the unavailability of distance between the query and each candidate, which makes it impossible to provide a ranklist for retrieval and related tasks.</p><p>There are also research efforts on encoding high-dimensional data into distance-preserving compact codes. A typical example is quantization, in which the distance between two vectors is estimated by the distance between codewords. Product Quantization (PQ) <ref type="bibr" target="#b21">[21]</ref> trains a large codebook with the Cartesian product of small sub-codebooks. PQ then traverses each candidate and computes its distance to the query quickly with a fast pre-computation on codewords. Improvements of PQ include Cartesian K-Means <ref type="bibr" target="#b35">[35]</ref> and Composite Quantization <ref type="bibr" target="#b69">[69]</ref>. Another efficient approach, Locality Sensitive Hashing (LSH) <ref type="bibr" target="#b19">[19]</ref>, measures the distance by the Hamming distance and could be embedded to improve image search performance <ref type="bibr" target="#b70">[70]</ref>. • standing people • sparse books • square tables</p><formula xml:id="formula_0">• cashier • various styles • square tables • ladder • pictures • square tables • dense books • tidy shelves • square tables • sitting people • tidy shelves • chessboard • arches • open spaces • square table • tidy shelves • sitting people • laptops • dense books • tidy shelves • square tables QUERY Q (library)</formula><p>Figure <ref type="figure">2</ref>: The difference between image classification and retrieval with the same query (green diamond). Red squares and blue circles indicate samples of the bookstore and library classes, respectively. For each image, three most significant visual attributes are listed, with the colors indicating their bias in visual concepts (red for bookstore, blue for library, and green for neutral). Candidates are ranked according to their relevance (distance) to the query. The dashed line illustrates the optimal linear classifier between two categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ALGORITHM 3.1 A Unified Framework</head><p>Since we aim at designing a unified framework for image classification and retrieval, we first make a brief discussion on the essential difference between these two tasks.</p><p>Both classification and retrieval involve measuring the similarity between the query and training or candidate images. The only difference lies in that a class label is provided for each training sample in classification. Therefore, we are actually computing the similarity between the query and a category, i.e., a set of images. As observed in <ref type="bibr" target="#b4">[4]</ref>, image-toclass distance is much more stable than image-to-image distance. On the other hand, candidates in retrieval are often considered independently, i.e., without explicit relationship between them. An intuitive example is shown in Figure <ref type="figure">2</ref>. The query (a library image) is closest to a bookstore image in the feature space, and the outlier (#1) significantly harms the retrieval performance. However, when more labeled training samples are available, we obtain the optimal linear classifier between library and bookstore which predicts the correct label of the query.</p><p>In summary, classification tasks benefit from the class labels of images, but such information is not available in retrieval datasets. A direct solution is to augment the database by extracting multiple object proposals for each image, and consider each object as an individual image sample (with a "class label"). After each object is equipped with a high-quality regional descriptor, it is possible to deal with both image classification and retrieval problems by computing image-to-class distance <ref type="bibr" target="#b4">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Online Nearest-neighbor Estimation</head><p>This part formulates the previous ideas as the ONE (Online Nearest-neighbor Estimation) algorithm. We start from an image classification/retrieval dataset of N images,</p><formula xml:id="formula_1">I = {(I1, y1) , (I2, y2) , . . . , (IN , yN )} (1)</formula><p>In which, In and yn are the image data and label of the n-th image, respectively. For image classification, yn ∈ {1, 2, . . . , C} is pre-defined by the dataset, while for image retrieval we simply set C = N and yn = n, indicating that each instance belongs to an independent "category".</p><p>For each image In, a set of object proposals Pn is constructed,</p><formula xml:id="formula_2">Pn = {pn,1, pn,2, . . . , pn,K n } (2)</formula><p>Here, p n,k = x min n,k , y min n,k , W n,k , H n,k , θ n,k is the k-th object of the n-th image, denoted by the coordinate of its upper-left corner, its width and height, and θ n,k ∈ [0 • , 360 • ) indicating the angle by which the image is rotated before feature extraction. The designation of Pn will be discussed in detail later. Cropping and rotating In according to p n,k yields a subimage I n,k , on which we compute a regional vector representation f n,k . Specifically, we extract 4096-D deep conv-net features which are the intermediate responses of a CNN <ref type="bibr" target="#b45">[45]</ref>. Although other features such as VLAD <ref type="bibr" target="#b22">[22]</ref> could also be adopted, we choose deep conv-net features because the excellent descriptive ability. According to a simple experiment based on NN feature search on the Holiday dataset <ref type="bibr" target="#b20">[20]</ref>, VLAD obtains a mAP score of 0.526 <ref type="bibr" target="#b20">[20]</ref> while deep conv-net features get 0.642 <ref type="bibr" target="#b43">[43]</ref>.</p><p>Next we follow <ref type="bibr" target="#b4">[4]</ref> to perform a Naive-Bayes Nearest-Neighbor (NBNN) search. We first define the feature set Fc, c = 1, 2, . . . , C, which is composed of all the features extracted from the objects that belong to the c-th category,</p><formula xml:id="formula_3">Fc = {f n,k | yn = c ∧ 1 k Kn} (3)</formula><p>For a query image I0, we compute its distance to each category c ∈ {1, 2, . . . , C}, which is the averaged distance between each object of In and its nearest neighbor in Fc,</p><formula xml:id="formula_4">dist(I0, c) . = dist(I0, Fc) (4) = 1 K0 K 0 k=1 dist(f 0,k , Fc) (5) = 1 K0 K 0 k=1 min f ∈Fc f 0,k -f 2 2 (6)</formula><p>When all the image-to-category distances are available, it is convenient to analyze them for specified purposes, such as finding the minimum one for category prediction, or sorting them for a ranklist of candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Object Proposals</head><p>It remains to construct an object proposal set Pn for each image In. For this purpose, we might refer to unsupervised object detectors, including Objectness <ref type="bibr" target="#b1">[1]</ref>, Selective Search <ref type="bibr" target="#b48">[48]</ref> and Binarized Normed Gradients (BING) <ref type="bibr">[7]</ref>. These algorithms often provide a number of bounding boxes which are the possible locations of objects. By sorting the confidence scores of the boxes, it is possible to obtain an arbitrary number KD of top-ranked object proposals 1 . 1 Due to the limited space, please refer to the publications above for examples of object detection.</p><p>In comparison, we also adopt an alternative approach which extracts manually defined objects on an image. For this, we first define the number of object proposal layers LO, and then enforce that objects within one layer have the same size meanwhile are distributed as dispersive as possible. Denote the width and height of image In as Wn and Hn. In the l-th layer, there are r l × r l objects with a fixed size Wn s l × Hn s l , where r l and s l are the object density and scale parameters which will be discussed later. The comparison between automatically detected and manually defined object proposals is similar to that between detected and densely sampled descriptors which is well discussed in the conventional Bag-of-Visual-Words (BoVW) models. We will show in Section 4.2 that both strategies obtain satisfying classification and retrieval results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Approximate Nearest-neighbor Search</head><p>Since ONE requires a huge number of NN queries, linear exhaustive NN search might be computationally expensive. We adopt both Principal Component Analysis (PCA) and Product Quantization (PQ) <ref type="bibr" target="#b21">[21]</ref> to reduce time complexity. Deep conv-net features <ref type="bibr" target="#b14">[14]</ref> (4096 dimensions) of each region are first reduced to D dimensions by PCA. Then it is partitioned by PQ into M segments and each subvector is quantized onto one of T codewords.</p><p>It is worth noting that vector dimensions after PCA reduction are ranked by decreasing energy, whereas PQ works better in the case that each segment contains vectors with approximately equal information. Therefore, we perform dimension rearrangement to make PCA and PQ cooperate better, in which the m-th PQ segment is composed of the (m, M + m, 2M + m, . . . , D -M + m)-th PCA-reduced dimensions. In practise, this strategy consistently boosts the classification/retrieval accuracy by about 2%. It is also instructive to whiten PCA features for similar effects.</p><p>Assume there are N training/candidate images in a classification/retrieval task. For each image, K object windows are proposed, on each of which we extract a 4096dimensional feature. Therefore for each query, we need to process K queries in a database of KN vectors. An exhaustive NN search takes O 4096K 2 N time and O(4096KN ) memory. In an approximate NN search, features are PCAreduced to D dimensions, and quantized with a PQ of M segments and T codewords for each segment. According to PQ <ref type="bibr" target="#b21">[21]</ref>, the time complexity is O K 2 N M + KDT , meanwhile storing quantized vectors and a codebook requires KN M log 2 T bits and DT floating numbers in total. Since M D 4096 and T N , both time and memory costs are greatly reduced. When the parameters are fixed, the computational complexity grows linearly with the dataset size N , which guarantees that our algorithm scales up well. Section 4.4 provides time/memory costs in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">GPU Acceleration</head><p>Despite the approximation by PCA and PQ, the computational cost on the online querying stage is still very high, e.g., about 5 × 10 11 floating operations to search one query among one million candidates (see Section 4.4). Fortunately, most of the heavy computation comes from the linear search stage of PQ. We can take advantage of GPU parallelization for efficient acceleration.</p><p>Graphics Processing Unit (GPU) is a specialized electronic device designed to rapidly manipulate and alter memory to accelerate image processing in a frame buffer intended for output to a display. It often contains a large number of stream processors in which large data blocks are processed in parallel. The highly parallel structure of GPUs makes them more effective than CPUs for large-scale simple-arithmetic operations. Recent years, GPUs have been widely adopted for accelerating deep CNN training <ref type="bibr" target="#b25">[25]</ref>.</p><p>It is worth noting that the storage of a GPU is often limited, e.g., an NVIDIA GeForce GTX Titan has only 6GB memory. We shall carefully design the parameters discussed above, so that quantized vectors could be stored in a GPU. An extensive study on the proper parameters are provided in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Implementation Details</head><p>For image classification, we use the LandUse-21 dataset <ref type="bibr" target="#b64">[64]</ref>, the Indoor-67 dataset <ref type="bibr" target="#b42">[42]</ref> and the SUN-397 dataset <ref type="bibr" target="#b53">[53]</ref> for scene classification. 80, 80 and 50 images per category are randomly selected for training. We also use the Oxford Pet-37 dataset <ref type="bibr" target="#b36">[36]</ref>, the Oxford Flower-102 dataset <ref type="bibr" target="#b33">[33]</ref> and the Caltech-UCSD Bird-200-2011 dataset <ref type="bibr" target="#b49">[49]</ref> for finegrained object recognition. 100, 20 and 30 images per category are randomly selected for training. The SUN-397 dataset <ref type="bibr" target="#b53">[53]</ref> is one of the largest available datasets for scene understanding, which provides an evidence that our algorithm scales up well. For each dataset, the random training/testing splits are repeated for 10 rounds and averaged classification accuracy is reported.</p><p>For image retrieval, we use the UKBench dataset <ref type="bibr" target="#b34">[34]</ref> and the Holiday dataset <ref type="bibr" target="#b20">[20]</ref>. They are both near-duplicate instance search datasets. The UKBench dataset consists of 2550 objects and 4 photos are taken for each of them (10200 images in total). The Holiday dataset is composed of 500 groups of objects or scenes with a handful of instances in each of them. The standard evaluation uses the N-S score (the average number of true-positive images in top-4 results) for UKBench, and the mAP (mean average precision) score for Holiday. To test the scalability of our model, we also mix the Holiday dataset with one million irrelevant images crawled from the Web.</p><p>For manually defined objects, we use LO = 5 layers, with fixed parameters (s1, s2, s3, s4, s5) = (1.0, 1.2, 1.5, 2.0, 2.5) and (r1, r2, r3, r4, r5) = (1, 2, 3, 4, 5), respectively. For automatic object detection, we use Selective Search <ref type="bibr" target="#b48">[48]</ref> and choose top-ranked objects according to the confidence scores. For object representation, we compute a pre-trained 19-layer VGG-Net <ref type="bibr" target="#b45">[45]</ref>, and extract the 4096-D responses of the second-to-last fully connected layers, without applying ReLU activation. Feature vectors are square-root normalized and then 2-normalized after PCA reduction.   </p><formula xml:id="formula_5">Objects LO = 1 LO = 2 LO = 3 LO = 4 LO = 5 LR = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model and Parameters</head><p>We discuss the parameters of the ONE algorithm, i.e., the object proposal set P, the PCA-reduced dimension D, and the number of segments M and codewords T for PQ. For quick evaluations, we evaluate the classification and retrieval accuracy on two relatively smaller datasets, namely the Flower-102 dataset and the Holiday dataset.</p><p>We first use accurate NN search to evaluate classification and retrieval accuracy with respect to the object proposals. Results are summarized in Table <ref type="table" target="#tab_0">1</ref> (the number of layers of manually defined object proposals) and Figure <ref type="figure" target="#fig_4">3</ref> (automatic detection vs. manual definition with random selection), respectively. One can observe that a larger number of object proposals often produce better accuracy. Meanwhile, manually defined and automatically detected objects produce comparable results, suggesting that it is the number LandUse-21 Indoor-67 SUN-397 <ref type="bibr" target="#b24">[24]</ref> 92 of object proposals that makes the major contribution to boosting accuracy. When the number of object proposals is sufficiently large (e.g., K 60), manually defined objects work even slightly better (86.24% vs. 86.16% on Flower-102 and 0.887 vs. 0.878 on Holiday). Therefore, we only use manually defined objects in later experiments.</p><p>For large-scale image search, we consider both PCA and PQ for acceleration (see Section 3.4). The impact of parameters, i.e., PCA dimension D, PQ segments M and codewords T , are summarized in Figure <ref type="figure" target="#fig_6">4</ref>, 5 and 6, respectively. We use K = 220 object proposals (LO = LR = 5) in these experiments. Empirically, a proper set of parameters achieve a good tradeoff between accuracy and computational overheads. For example, partitioning a vector into 128 segments produces nearly the same accuracy compared to 64 segments, but requires almost doubled time and memory consumption on the online querying stage. We choose parameters D = 512, M = 32 and T = 4096 as an accuracycomplexity tradeoff. For small-scale experiments, i.e., the number of training or candidate images is not greater than 10 4 , we use PCA but do not use PQ for higher classification and retrieval accuracy.</p><p>Moreover, we need to constraint the number of object proposals K to fit the limited GPU memory. Recall from Section 3.4 that PQ stores KN M log 2 T bits and DT floating numbers, i.e, 1  8 KN M log 2 T + 4DT bytes, which should be less than 6GB (the memory of a Titan GPU). When we are dealing with one million images (N ≈ 10 6 ) in largescale experiments, K shall be no larger than 120. We use LO = LR = 4 (K = 120) in practise. Of course the tradeoff between more objects and more rotations could be determined according to detailed conditions. For example, in the Oxford Buildings dataset <ref type="bibr" target="#b38">[38]</ref> (less image rotation), it might be better to use LO = 6 and LR = 2 (K = 106).      <ref type="table">3</ref>: Comparison of retrieval accuracy with the state-of-the-arts. Among the competitors, <ref type="bibr" target="#b70">[70]</ref> and <ref type="bibr" target="#b43">[43]</ref> also use deep conv-net features extracted on the Alex-Net <ref type="bibr" target="#b25">[25]</ref>). While <ref type="bibr" target="#b70">[70]</ref>, <ref type="bibr" target="#b12">[12]</ref> and <ref type="bibr" target="#b67">[67]</ref> adopt post-processing, ONE does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BoVW+ONE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to the State-of-the-Arts</head><p>We compare ONE with the state-of-the-art algorithms.</p><p>Image classification results are summarized in Table <ref type="table" target="#tab_1">2</ref>. One could observe that our algorithm achieves competitive accuracy in both scene recognition and fine-grained recognition tasks. For fine-grained recognition, we do not use any part detectors but simply extract regular object proposals, leading to inferior classification accuracy compared to those using complicated part detectors, such as <ref type="bibr" target="#b66">[66]</ref> (73.89%) and <ref type="bibr" target="#b5">[5]</ref> (85.4%) on the Bird-200 dataset. Our algorithm could also cooperate with specialized part detectors.</p><p>Since a training process provides much benefit for image classification, we also compare our model with conventional machine learning approaches. We adopt LibLINEAR <ref type="bibr" target="#b15">[15]</ref>, a scalable SVM implementation with a tradeoff parameter C = 10. In most cases, ONE outperforms SVM, defending the ability of a training-free algorithm for classification. The fusion of ONE and SVM results, obtained by adding the confidence scores directly and re-ranking, also shows superior performance over both models, suggesting that complementary information is captured by different approaches.</p><p>Next we report image retrieval accuracy in Table <ref type="table">3</ref>. One could observe that our algorithm, without post-processing, outperforms all the competitors. Similar to image classification, we also compare ONE with the conventional BoVW model. Our model is implemented with SIFT extraction, large codebook training and hard quantization <ref type="bibr" target="#b38">[38]</ref>, an inverted index structure and p-norm weighting <ref type="bibr" target="#b72">[72]</ref>. Although ONE outperforms BoVW significantly, we notice that BoVW works better to capture local clues that help retrieval. When we fuse the results generated by both models, they complement each other and produce even higher accuracy. To the best of our knowledge, the 0.899 mAP score on Holiday and the 3.887 N-S score on UKBench rank among the highest scores ever reported on these two datasets. When Holiday is mixed with one million distractors, we get a 0.758 mAP score with PCA and PQ approximation, which outperforms 0.724 reported in <ref type="bibr" target="#b70">[70]</ref> and 0.633 in <ref type="bibr" target="#b68">[68]</ref>.</p><p>The excellent classification and retrieval performance comes from a perfect cooperation of object detection and description, i.e., efficient image representation by conv-net features and powerful retrieval process by ONE. Either applying NN search on global image features (see Table <ref type="table" target="#tab_0">1</ref>) or replacing deep conv-net features with BoVW-based features would case dramatic accuracy drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Time and Memory Costs</head><p>This part provides an experimental study on the computational overheads of ONE. A theoretical analysis could be found in Section 3.4. Parameters, i.e., D = 512, M = 32 and C = 4096, are inherited from Section 4.2. Time and memory costs of different models are summarized in Table <ref type="table" target="#tab_3">4</ref>.</p><p>For image classification, we take SUN-397 <ref type="bibr" target="#b53">[53]</ref>, the largest dataset in our experiments, as the example. Following the same setting provided by the authors, the numbers of training and testing images in one split are both N = 397 × 50 ≈ 20K. K = 220 (LO = LR = 5) object proposals are extracted on each image. According to the analysis in Section 3.4, classifying a single image requires approximately 4.5 × 10 8 floating multiplications and 3.2 × 10 9 floating summations, which takes less than 0.1s on a Titan GPU, i.e., about 1800s (0.5h) on the whole testing set. The memory storage on GPU is about 200MB in this case. Compared with conventional SVM classification which requires 8h and 2560MB for one training and testing, our method is significantly faster in time and cheaper in memory.</p><p>For image retrieval, we evaluate our algorithm on the Holiday dataset <ref type="bibr" target="#b20">[20]</ref> with one million distractors. With N ≈ 1M and K = 120 (LO = LR = 4), we need approximately 2.4 × 10 8 floating multiplications and 4.5 × 10 11 floating summations for each query, which require about 1.2s on a Titan GPU, which is comparable with conventional systems performed on CPU. The storage of about 45G bits and 2M floating numbers fits well in the 6GB GPU memory. Both time and memory costs are comparable to the stateof-the-art approaches with deep conv-net features <ref type="bibr" target="#b70">[70]</ref>. It is worth noting that the actual computational costs in ONE are much more expensive than conventional algorithms. For example, ONE requires nearly 5 × 10 11 floating operations for searching one query among one million candidates, while a simple BoVW-based approach often needs no more than 10 9 [34] <ref type="bibr" target="#b73">[73]</ref>. GPU is the key to make ONE produce results in a reasonable time (e.g. around one second). Conventional algorithms often contain complicated modules (e.g., inverted index, spatial verification, etc.) with asynchronous memory access and/or a large number of serial operations, making them difficult to be transplanted to GPU for acceleration. As the rapid development of multi-GPU, our algorithm might attract more attentions in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>This paper proposes to unify image classification and retrieval algorithms into ONE (Online Nearest-neighbor Estimation). We demonstrate that, with the help of highquality regional features, both classification and retrieval tasks could be accomplished with a simple NBNN search <ref type="bibr" target="#b4">[4]</ref>. We take advantage of PCA and PQ approximation as well as GPU parallelization to alleviate heavy computational costs. Despite the simplicity, our algorithm achieves state-of-theart image classification and retrieval performance.</p><p>The success of our algorithm inspires future research in three aspects. First, the essence of image classification and retrieval are the same, both of them could be tackled by measuring image similarity. Second, extracting more objects often leads to higher classification/retrieval accuracy, enlightening that image description, even with deep convnet features, is far from Third, GPU is the future of high performance computing, therefore designing a GPUfriendly algorithm is necessary and beneficial.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An image retrieval example illustrating the intuition of ONE (best viewed in color PDF). On a query image, it is possible to find a number of semantic objects. Searching for nearest neighbors with one object might not capture the exact query intention, but fusing yields satisfying results. A yellow circle with the word TP indicates a true-positive image. Images are collected from the Holiday dataset [20].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The upper-left corner of the (a, b)-th box, 0 a, b &lt; r l , is located at a • Wn -Wn s l / (r l -1) , b • Hn -Hn s l / (r l -1) .An LO-layer model has KL O = L O l=1 r 2 l object proposals. Either a detected or manually defined object might be rotated. Here we consider 4 simplest rotations, i.e., θ n,k ∈ {0 • , 90 • , 180 • , 270 • }. As an increasing number of objects might result in heavier computational overheads, we might only rotate the top-KR detected regions (KR KD), or regions on the first LR (LR LO) layers. Such a strategy produces KD + 3KR or KL O + 3KL R object proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Classification/retrieval accuracy with respect to the way of defining objects and the number of proposals. We first construct K = 220 objects (LO = LR = 5 for manual definition and KD = KR = 55 for automatic detection), and randomly select a subset of them for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy w.r.t. PCA dimension D. PQ is not used here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :Figure</head><label>5</label><figDesc>Figure 5: Accuracy w.r.t. PQ segments M , with D = 1024 and T = 4096.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification and retrieval accuracy with respect to the complexity of manually defined objects.</figDesc><table><row><cell>67.18</cell><cell>73.41</cell><cell>77.63</cell><cell>82.18</cell><cell>85.71</cell></row><row><cell>LR = 1 67.40</cell><cell>73.69</cell><cell>77.95</cell><cell>82.42</cell><cell>86.11</cell></row><row><cell>LR = 2 -</cell><cell>73.78</cell><cell>78.01</cell><cell>82.47</cell><cell>86.14</cell></row><row><cell>LR = 3 -</cell><cell>-</cell><cell>78.03</cell><cell>82.51</cell><cell>86.18</cell></row><row><cell>LR = 4 -</cell><cell>-</cell><cell>-</cell><cell>82.53</cell><cell>86.22</cell></row><row><cell>LR = 5 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.24</cell></row><row><cell cols="5">(a) Classification Accuracy (%) on Flower-102</cell></row><row><cell cols="5">Objects LO = 1 LO = 2 LO = 3 LO = 4 LO = 5</cell></row><row><cell>LR = 0 0.751</cell><cell>0.772</cell><cell>0.796</cell><cell>0.826</cell><cell>0.847</cell></row><row><cell>LR = 1 0.816</cell><cell>0.832</cell><cell>0.837</cell><cell>0.854</cell><cell>0.871</cell></row><row><cell>LR = 2 -</cell><cell>0.838</cell><cell>0.848</cell><cell>0.858</cell><cell>0.874</cell></row><row><cell>LR = 3 -</cell><cell>-</cell><cell>0.861</cell><cell>0.868</cell><cell>0.880</cell></row><row><cell>LR = 4 -</cell><cell>-</cell><cell>-</cell><cell>0.882</cell><cell>0.883</cell></row><row><cell>LR = 5 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.887</cell></row><row><cell cols="5">(b) Retrieval Accuracy (mAP) on Holiday</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy on different datasets. In each subtable, the first and second parts contain algorithms without and with using deep conv-net features, respectively.</figDesc><table><row><cell></cell><cell>.8</cell><cell>63.4</cell><cell>46.1</cell></row><row><cell>[61]</cell><cell>-</cell><cell>63.48</cell><cell>45.91</cell></row><row><cell>[14]</cell><cell>-</cell><cell>-</cell><cell>40.94</cell></row><row><cell>[43]</cell><cell>-</cell><cell>69.0</cell><cell>-</cell></row><row><cell>SVM</cell><cell>94.52</cell><cell>68.46</cell><cell>53.00</cell></row><row><cell>ONE</cell><cell>93.98</cell><cell>69.61</cell><cell>54.47</cell></row><row><cell cols="2">ONE+SVM 94.71</cell><cell>70.13</cell><cell>54.87</cell></row><row><cell></cell><cell cols="3">(a) Scene Recognition Accuracy (%)</cell></row><row><cell></cell><cell>Pet-37</cell><cell cols="2">Flower-102 Bird-200</cell></row><row><cell>[2]</cell><cell>54.30</cell><cell>80.66</cell><cell>-</cell></row><row><cell>[52]</cell><cell>59.29</cell><cell>75.26</cell><cell>-</cell></row><row><cell>[32]</cell><cell>56.8</cell><cell>84.6</cell><cell>33.3</cell></row><row><cell>[14]</cell><cell>-</cell><cell>-</cell><cell>58.75</cell></row><row><cell>[43]</cell><cell>-</cell><cell>86.8</cell><cell>61.8</cell></row><row><cell>SVM</cell><cell>88.05</cell><cell>85.49</cell><cell>59.66</cell></row><row><cell>ONE</cell><cell>89.50</cell><cell>86.24</cell><cell>61.54</cell></row><row><cell cols="2">ONE+SVM 90.03</cell><cell>86.82</cell><cell>62.02</cell></row><row><cell></cell><cell cols="3">(b) Fine-Grained Recognition Accuracy (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of computational costs between ONE and BoVW. Results are reported on SUN-397 (about 100K images) for classification, and Holiday with 1M distractors for retrieval. For abbreviations: GMM -Gaussian Mixture Model, AKM -Approximate K-Means<ref type="bibr" target="#b38">[38]</ref>, FV -Fisher Vectors<ref type="bibr" target="#b44">[44]</ref>, VQ -Vector Quantization, IND -inverted index.</figDesc><table><row><cell></cell><cell>Classification</cell><cell>Classification</cell><cell>Retrieval</cell><cell>Retrieval</cell></row><row><cell></cell><cell>with ONE</cell><cell>with BoVW</cell><cell>with ONE</cell><cell>with BoVW</cell></row><row><cell cols="2">Descriptor Extraction Time (s, per image) 1.81 (CNN)</cell><cell>1.36 (SIFT)</cell><cell>1.75 (CNN)</cell><cell>0.83 (SIFT)</cell></row><row><cell>Codebook Training Time (h)</cell><cell>0.39 (PQ)</cell><cell>2.41 (GMM)</cell><cell>0.39 (PQ)</cell><cell>6.18 (AKM)</cell></row><row><cell>Codebook Training Memory (GB)</cell><cell>0.63</cell><cell>2.50</cell><cell>0.63</cell><cell>8.31</cell></row><row><cell>Feature Quantization Time (s, per image)</cell><cell>0.17 (PQ)</cell><cell>0.23 (FV)</cell><cell>0.17 (PQ)</cell><cell>0.10 (VQ)</cell></row><row><cell>Offline Training Time (h)</cell><cell>-</cell><cell>7.71 (SVM)</cell><cell>-</cell><cell>2.85 (IND)</cell></row><row><cell>Offline Training Memory (GB)</cell><cell>-</cell><cell>2.50 (SVM)</cell><cell>-</cell><cell>4.19 (IND)</cell></row><row><cell>Online Querying Time (s, per query)</cell><cell>0.08</cell><cell>&lt; 0.01</cell><cell>1.17</cell><cell>0.56</cell></row><row><cell>Online Querying Memory (GB)</cell><cell>0.21 (PQ)</cell><cell>0.05</cell><cell>5.65 (PQ)</cell><cell>4.19 (IND)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>This work was supported by the 973 Program of China (Grant Nos. 2013CB329403 and 2012CB316301), the NNSF of China (Grant Nos. 61332007, 61273023 and 61429201), and the Tsinghua University Initiative Scientific Research Program (Grant No. 20121088071). This work was also supported in part to Dr. Hong by State High-Tech Development Plan 2014AA015104, and the Program for New Century Excellent Talents in University under grant NCET-13-0764; and in part to Dr. Tian by ARO grant W911NF-12-1-0057, and Faculty Research Awards by NEC Labs of America.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring the Objectness of Image Windows</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient Object Detection and Segmentation for Fine-Grained Recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">SURF: Speeded Up Robust Features. ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Defense of Nearest-Neighbor Based Image Classification</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<title level="m">Bird Species Categorization Using Pose Normalized Deep Convolutional Nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BING: Binarized Normed Gradients for Objectness Estimation at 300fps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geometric Min-Hashing: Finding a (Thick) Needle in a Haystack</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visual Categorization with Bags of Keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual Reranking through Weakly Supervised Multi-Graph Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Imagenet: A Large-scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric Lp-norm Feature Pooling for Image Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An Algorithm for Finding Best Matches in Logarithmic Expected Time</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Finkel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>ACM ToMS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<title level="m">Similarity Search in High Dimensions via Hashing. VLDB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Product Quantization for Nearest Neighbor Search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aggregating Local Descriptors into a Compact Image Representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VisualRank: Applying PageRank to Large-Scale Image Search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dirichlet-based Histogram Feature Transform for Image Classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Handwritten Digit Recognition with a Back-Propagation Network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distinctive Image Features from Scale-Invariant Keypoints. IJCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<title level="m">Robust Wide-Baseline Stereo from Maximally Stable Extremal Regions. Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scale &amp; Affine Invariant Interest Point Detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VISIGRAPP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<title level="m">Generalized Max Pooling. CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Automated Flower Classification over a Large Number of Classes. Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scalable Recognition with a Vocabulary Tree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-Means</forename><surname>Cartesian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<title level="m">Cats and Dogs</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving the Fisher Kernel for Large-scale Image Classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Object Retrieval with Large Vocabularies and Fast Spatial Matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Lost in Quantization: Improving Particular Object Retrieval in Large Scale Image Databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring Context and Content Links in Social Media: A Latent Space Method</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two-Dimensional Multi-Label Active Learning with an Efficient Online Adaptation Model for Image Classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recognizing Indoor Scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CNN Features off-the-shelf: an Astounding Baseline for Recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image Classification with the Fisher Vector: Theory and Practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video Google: A Text Retrieval Approach to Object Matching in Videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Going Deeper with Convolutions. NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Selective Search for Object Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trinary-Projection Trees for Approximate Nearest Neighbor Search. TPAMI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<title level="m">Locality-Constrained Linear Coding for Image Classification</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Collaborative Linear Coding for Robust Image Classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Sun Database: Large-scale Scene Recognition from Abbey to Zoo. CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image Classification with Max-SIFT Descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial Pooling of Heterogeneous Features for Image Classification</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature Normalization for Part-based Image Classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hierarchical Part Matching for Fine-Grained Visual Categorization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Max-SIFT: Flipping Invariant Descriptors for Web Logo Search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Generalized Regular Spatial Pooling for Image Classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Fast and Accurate Near-Duplicate Scale Web Image Search via Graph Propagation and Search Process Tradeoff</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Orientational Pyramid Matching for Recognizing Indoor Scenes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fine-Grained Image Search</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>TMM</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linear Spatial Pyramid Matching Using Sparse Coding for Image Classification</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bag-of-Visual-Words and Spatial Extensions for Land-Use Classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICAGIS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Part-Based R-CNNs for Fine-Grained Category Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Query Specific Fusion for Image Retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Semantic-aware Co-indexing for Image Retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Composite Quantization for Approximate Nearest Neighbor Search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.0132</idno>
		<title level="m">Seeing the Big Picture: Deep Embedding with Contextual Evidences</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Packing and Padding: Coupled Multi-Index for Accurate Image Retrieval</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lp-Norm IDF for Scalable Image Retrieval</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Scalar Quantization for Large Scale Image Search</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
