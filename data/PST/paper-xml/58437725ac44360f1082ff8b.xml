<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Federated Optimization: Distributed Machine Learning for On-Device Intelligence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-10-11">October 11, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jakub</forename><surname>Konečný</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Mcmahan</surname></persName>
							<email>mcmahan@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
							<email>dramage@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
							<email>peter.richtarik@ed.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Federated Optimization: Distributed Machine Learning for On-Device Intelligence</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-10-11">October 11, 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1610.02527v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal.</p><p>A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimization, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network -as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution.</p><p>We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of federated optimization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Mobile phones and tablets are now the primary computing devices for many people. In many cases, these devices are rarely separated from their owners <ref type="bibr" target="#b18">[19]</ref>, and the combination of rich user interactions and powerful sensors means they have access to an unprecedented amount of data, much of it private in nature. Models learned on such data hold the promise of greatly improving usability by powering more intelligent applications, but the sensitive nature of the data means there are risks and responsibilities to storing it in a centralized location.</p><p>We advocate an alternative -federated learning -that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally computed updates via a central coordinating server. This is a direct application of the principle of focused collection or data minimization proposed by the 2012 White House report on the privacy of consumer data <ref type="bibr" target="#b97">[98]</ref>. Since these updates are specific to improving the current model, they can be purely ephemeralthere is no reason to store them on the server once they have been applied. Further, they will never contain more information than the raw training data (by the data processing inequality), and will generally contain much less. A principal advantage of this approach is the decoupling of model training from the need for direct access to the raw training data. Clearly, some trust of the server coordinating the training is still required, and depending on the details of the model and algorithm, the updates may still contain private information. However, for applications where the training objective can be specified on the basis of data available on each client, federated learning can significantly reduce privacy and security risks by limiting the attack surface to only the device, rather than the device and the cloud.</p><p>If additional privacy is needed, randomization techniques from differential privacy can be used. The centralized algorithm could be modified to produce a differentially private model <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b0">1]</ref>, which allows the model to be released while protecting the privacy of the individuals contributing updates to the training process. If protection from even a malicious (or compromised) coordinating server is needed, techniques from local differential privacy can be applied to privatize the individual updates <ref type="bibr" target="#b31">[32]</ref>. Details of this are beyond the scope of the current work, but it is a promising direction for future research.</p><p>A more complete discussion of applications of federated learning as well as privacy ramifications can be found in <ref type="bibr" target="#b61">[62]</ref>. Our focus in this work will be on federated optimization, the optimization problem that must be solved in order to make federated learning a practical alternative to current approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Problem Formulation</head><p>The optimization community has seen an explosion of interest in solving problems with finite-sum structure in recent years. In general, the objective is formulated as min</p><formula xml:id="formula_0">w∈R d f (w) where f (w) def = 1 n n i=1 f i (w).<label>(1)</label></formula><p>The main source of motivation are problems arising in machine learning. The problem structure <ref type="bibr" target="#b0">(1)</ref> covers linear or logistic regressions, support vector machines, but also more complicated models such as conditional random fields or neural networks. We suppose we have a set of input-output pairs {x i , y i } n i=1 , and a loss function, giving rise to the functions f i . Typically, x i ∈ R d and y i ∈ R or y i ∈ {−1, 1}. Simple examples include</p><formula xml:id="formula_1">• linear regression: f i (w) = 1 2 (x T i w − y i ) 2 , y i ∈ R</formula><p>• logistic regression: f i (w) = − log(1 + exp(−y i x T i w)), y i ∈ {−1, 1}</p><p>• support vector machines: f i (w) = max{0, 1 − y i x T i w}, y i ∈ {−1, 1}</p><p>More complicated non-convex problems arise in the context of neural networks, where rather than via the linear-in-the-features mapping x T i w, the network makes prediction through a nonconvex function of the feature vector x i . However, the resulting loss can still be written as f i (w), and gradients can be computed efficiently using backpropagation.</p><p>The amount of data that businesses, governments and academic projects collect is rapidly increasing. Consequently, solving problem <ref type="bibr" target="#b0">(1)</ref> arising in practice is often impossible on a single node, as merely storing the whole dataset on a single node becomes infeasible. This necessitates the use of a distributed computational framework, in which the training data describing the problem is stored in a distributed fashion across a number of interconnected nodes and the optimization problem is solved collectively by the cluster of nodes.</p><p>Loosely speaking, one can use any network of nodes to simulate a single powerful node, on which one can run any algorithm. The practical issue is that the time it takes to communicate between a processor and memory on the same node is normally many orders of magnitude smaller than the time needed for two nodes to communicate; similar conclusions hold for the energy required <ref type="bibr" target="#b88">[89]</ref>. Further, in order to take advantage of parallel computing power on each node, it is necessary to subdivide the problem into subproblems suitable for independent/parallel computation.</p><p>State-of-the-art optimization algorithms are typically inherently sequential. Moreover, they usually rely on performing a large number of very fast iterations. The problem stems from the fact that if one needs to perform a round of communication after each iteration, practical performance drops down dramatically, as the round of communication is much more time-consuming than a single iteration of the algorithm.</p><p>These considerations have lead to the development of novel algorithms specialized for distributed optimization (we defer thorough review until Section 2). For now, we note that most of the results in literature work in the setting where the data is evenly distributed, and further suppose that K n/K where K is the number of nodes. This is indeed often close to reality when data is stored in a large data center. Additionally, an important subfield of the field of distributed learning relies on the assumption that each machine has a representative sample of the data available locally. That is, it is assumed that each machine has an IID sample from the underlying distribution. However, this assumption is often too strong; in fact, even in the data center paradigm this is often not the case since the data on a single node can be close to each other on a temporal scale, or clustered by its geographical origin. Since the patterns in the data can change over time, a feature might be present frequently on one node, while not appear on another at all.</p><p>The federated optimization setting describes a novel optimization scenario where none of the above assumptions hold. We outline this setting in more detail in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The Setting of Federated Optimization</head><p>The main purpose of this paper is to bring to the attention of the machine learning and optimization communities a new and increasingly practically relevant setting for distributed optimization, where none of the typical assumptions are satisfied, and communication efficiency is of utmost importance. In particular, algorithms for federated optimization must handle training data with the following characteristics:</p><p>• Massively Distributed: Data points are stored across a large number of nodes K. In particular, the number of nodes can be much bigger than the average number of training examples stored on a given node (n/K).</p><p>• Non-IID: Data on each node may be drawn from a different distribution; that is, the data points available locally are far from being a representative sample of the overall distribution.</p><p>• Unbalanced: Different nodes may vary by orders of magnitude in the number of training examples they hold.</p><p>In this work, we are particularly concerned with sparse data, where some features occur on a small subset of nodes or data points only. Although this is not necessary characteristic of the setting of federated optimization, we will show that the sparsity structure can be used to develop an effective algorithm for federated optimization. Note that data arising in the largest machine learning problems being solved nowadays, ad click-through rate predictions, are extremely sparse.</p><p>We are particularly interested in the setting where training data lives on users' mobile devices (phones and tablets), and the data may be privacy sensitive. The data {x i , y i } is generated through device usage, e.g., via interaction with apps. Examples include predicting the next word a user will type (language modelling for smarter keyboard apps), predicting which photos a user is most likely to share, or predicting which notifications are most important.</p><p>To train such models using traditional distributed learning algorithms, one would collect the training examples in a centralized location (data center) where it could be shuffled and distributed evenly over proprietary compute nodes. In this paper we propose and study an alternative model: the training examples are not sent to a centralized location, potentially saving significant network bandwidth and providing additional privacy protection. In exchange, users allow some use of their devices' computing power, which shall be used to train the model.</p><p>In the communication model of this paper, in each round we send an update δ ∈ R d to a centralized server, where d is the dimension of the model being computed/improved. The update δ could be a gradient vector, for example. While it is certainly possible that in some applications the δ may encode some private information of the user, it is likely much less sensitive (and orders of magnitude smaller) than the original data itself. For example, consider the case where the raw training data is a large collection of video files on a mobile device. The size of the update δ will be independent of the size of this local training data corpus. We show that a global model can be trained using a small number of communication rounds, and so this also reduces the network bandwidth needed for training by orders of magnitude compared to copying the data to the datacenter.</p><p>Further, informally, we choose δ to be the minimum piece of information necessary to improve the global model; its utility for other uses is significantly reduced compared to the original data. Thus, it is natural to design a system that does not store these δ's longer than necessary to update the model, again increasing privacy and reducing liability on the part of the centralized model trainer. This setting, in which a single vector δ ∈ R d is communicated in each round, covers most existing first-order methods, including dual methods such as CoCoA+ <ref type="bibr" target="#b56">[57]</ref>.</p><p>Communication constraints arise naturally in the massively distributed setting, as network connectivity may be limited (e.g., we may wish to deffer all communication until the mobile device is charging and connected to a wi-fi network). Thus, in realistic scenarios we may be limited to only a single round of communication per day. This implies that, within reasonable bounds, we have access to essentially unlimited local computational power. Consequently, the practical objective is solely to minimize the number of communication rounds.</p><p>The main purpose of this work is initiate research into, and design a first practical implementation of federated optimization. Our results suggest that with suitable optimization algorithms, very little is lost by not having an IID sample of the data available, and that even in the presence of a large number of nodes, we can still achieve convergence in relatively few rounds of communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section we provide a detailed overview of the relevant literature. We particularly focus on algorithms that can be used to solve problem (1) in various contexts. First, in Sections 2.1 and 2.2 we look at algorithms designed to be run on a single computer. In Section 2.3 we follow with a discussion of the distributed setting, where no single node has direct access to all data describing f . We describe a paradigm for measuring the efficiency of distributed methods, followed by overview of existing methods and commentary on whether they were designed with communication efficiency in mind or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline Algorithms</head><p>In this section we shall describe several fundamental baseline algorithms which can be used to solve problems of the form (1).</p><p>Gradient Descent. A trivial benchmark for solving problems of structure ( <ref type="formula" target="#formula_0">1</ref>) is Gradient Descent (GD) in the case when functions f i are smooth (or Subgradient Descent for non-smooth functions) <ref type="bibr" target="#b68">[69]</ref>. The GD algorithm performs the iteration</p><formula xml:id="formula_2">w t+1 = w t − h t ∇f (w t ),</formula><p>where h t &gt; 0 is a stepsize parameter. As we mentioned earlier, the number of functions, or equivalently, the number of training data pairs, n, is typically very large. This makes GD impractical, as it needs to process the whole dataset in order to evaluate a single gradient and update the model.</p><p>Gradient descent can be substantially accelerated, in theory and practice, via the addition of a momentum term. Acceleration ideas for gradient methods in convex optimization can be traced back to the work of Polyak <ref type="bibr" target="#b72">[73]</ref> and Nesterov <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>. While accelerated GD methods have a substantially better convergence rate, in each iteration they still need to do at least one pass over all data. As a result, they are not practical for problems where n very large.</p><p>Stochastic Gradient Descent. At present a basic, albeit in practice extremely popular, alternative to GD is Stochastic Gradient Descent (SGD), dating back to the seminal work of Robbins and Monro <ref type="bibr" target="#b81">[82]</ref>. In the context of (1), SGD samples a random function (i.e., a random data-label pair) i t ∈ {1, 2, . . . , n} in iteration t, and performs the update</p><formula xml:id="formula_3">w t+1 = w t − h t ∇f it (w t ),</formula><p>where h t &gt; 0 is a stepsize parameter. Intuitively speaking, this method works because if i t is sampled uniformly at random from indices 1 to n, the update direction is an unbiased estimate of the gradient -E[∇f it (w)] = ∇f (w). However, noise introduced by sampling slows down the convergence, and a diminsihing sequence of stepsizes h k is necessary for convergence. For a theoretical analysis for convex functions we refer the reader to <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref> and <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b92">93]</ref> for SVM problems. In a recent review <ref type="bibr" target="#b11">[12]</ref>, the authors outline further research directions. For a more practically-focused discussion, see <ref type="bibr" target="#b10">[11]</ref>. In the context of neural networks, computation of stochastic gradients is referred to as backpropagation <ref type="bibr" target="#b48">[49]</ref>. Instead of specifying the functions f i and its gradients explicitly, backpropagation is a general way of computing the gradient. Performance of several competitive algorithms for training deep neural networks has been compared in <ref type="bibr" target="#b69">[70]</ref>.</p><p>One common trick that has been practically observed to provide superior performance, is to replace random sampling in each iteration by going through all the functions in a random order. This ordering is replaced by another random order after each such cycle <ref type="bibr" target="#b9">[10]</ref>. Theoretical understanding of this phenomenon had been a long standing open problem, understood recently in <ref type="bibr" target="#b39">[40]</ref>.</p><p>The core differences between GD and SGD can be summarized as follows. GD has a fast convergence rate, but each iteration in the context of ( <ref type="formula" target="#formula_0">1</ref>) is potentially very slow, as it needs to process the entire dataset in each iteration. On the other hand, SGD has slower convergence rate, but each iteration is fast, as the work needed is independent of number of data points n. For the problem structure of (1), SGD is usually better, as for practical purposes relatively low accuracy is required, which SGD can in extreme cases achieve after single pass through data, while GD would make just a single update. However, if a high accuracy was needed, GD or its faster variants would prevail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Novel Breed of Randomized Algorithms</head><p>Recent years have seen an explosion of new randomized methods which, in a first approximation, combine the benefits of cheap iterations of SGD with fast convergence of GD. Most of these methods can be said to belong to one of two classes -dual methods of the randomized coordinate descent variety, and primal methods of the stochastic gradient descent with variance reduction variety.</p><p>Randomized Coordinate Descent. Although the idea of coordinate descent has been around for several decades in various contexts (and for quadratic functions dates back even much further, to works on the Gauss-Seidel methods), it came to prominence in machine learning and optimization with the work of Nesterov <ref type="bibr" target="#b66">[67]</ref> which equipped the method with a randomization strategy. Nesterov's work on Randomized Coordinate Descent (RCD) popularized the method and demonstrated that randomization can be very useful for problems of structure <ref type="bibr" target="#b0">(1)</ref>.</p><p>The RCD algorithm in each iteration chooses a random coordinate j t ∈ {1, . . . , d} and performs the update</p><formula xml:id="formula_4">w t+1 = w t − h jt ∇ jt f (w t )e jt ,</formula><p>where h jt &gt; 0 is a stepsize parameter, ∇ j f (w) denotes the j th partial derivative of function f , and e j is the j th unit standard basis vector in R d . For the case of generalized linear models, when the data exhibits certain sparsity structure, it is possible to evaluate the partial derivative ∇ j f (w) efficiently, i.e., without need to process the entire dataset, leading to a practically efficient algorithm, see for instance <ref type="bibr" target="#b78">[79,</ref><ref type="bibr">Section 6]</ref>. Numerous follow-up works extended the concept to proximal setting <ref type="bibr" target="#b78">[79]</ref>, single processor parallelism <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b79">80]</ref> and develop efficiently implementable acceleration <ref type="bibr" target="#b50">[51]</ref>. All of these three properties were connected in a single algorithm in <ref type="bibr" target="#b34">[35]</ref>, to which we refer the reader for a review of the early developments in the area of RCD, particularly to overview in Table <ref type="table">1</ref> therein.</p><p>Stochastic Dual Coordinate Ascent. When an explicit strongly convex, but not necessarily smooth, regularizer is added to the average loss <ref type="bibr" target="#b0">(1)</ref>, it is possible to write down its (Fenchel) dual and the dual variables live in n-dimensional space. Applying RCD leads to an algorithm for solving (1) known under the name Stochastic Dual Coordinate Ascent <ref type="bibr" target="#b87">[88]</ref>. This method has gained broad popularity with practicioners, likely due to the fact that for a number of loss functions, the method comes without the need to tune any hyper-parameters. The work <ref type="bibr" target="#b87">[88]</ref> was first to show that by applying RCD <ref type="bibr" target="#b78">[79]</ref> to the dual problem, one also solves the primal problem <ref type="bibr" target="#b0">(1)</ref>. For a theoretical and computational comparison of applying RCD to the primal versus the dual problems, see <ref type="bibr" target="#b20">[21]</ref>.</p><p>A directly primal-dual randomized coordinate descent method called Quartz, was developed in <ref type="bibr" target="#b74">[75]</ref>. It has been recently shown in SDNA <ref type="bibr" target="#b73">[74]</ref> that incorporating curvature information contained in random low dimensional subspaces spanned by a few coordinates can sometimes lead to dramatic speedups. Recent works <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b19">20]</ref> interpret the SDCA method in primal-only setting, shedding light onto why this method works as a SGD method with a version of variance reduction property.</p><p>We now move the the second class of novel randomized algorithms which can be generally interpreted as variants of SGD, with an attempt to reduce variance inherent in the process of gradient estimation.</p><p>Stochastic Average Gradient. The first notable algorithm from this class is the Stochastic Average Gradient (SAG) <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b84">85]</ref>. The SAG algorithm stores an average of n gradients of functions f i evalueated at different points in the history of the algorithm. In each iteration, the algotithm, updates randomly chosen gradient out of this average, and makes a step in the direction of the average. This way, complexity of each iteration is independent of n, and the algorithm enjoys a fast convergence. The drawback of this algorithm is that it needs to store n gradients in memory because of the update operation. In the case of generalized linear models, this memory requirement can be reduced to the need of n scalars, as the gradient is a scalar multiple of the data point. This methods has been recently extended for use in Conditional Random Fields <ref type="bibr" target="#b83">[84]</ref>. Nevertheless, the memory requirement makes the algorithm infeasible for application even in relatively small neural networks.</p><p>A followup algorithm SAGA <ref type="bibr" target="#b25">[26]</ref> and its simplification <ref type="bibr" target="#b24">[25]</ref>, modifies the SAG algorithm to achieve unbiased estimate of the gradients. The memory requirement is still present, but the method significantly simplifies theoretical analysis, and yields a slightly stronger convergence guarantee.</p><p>Stochastic Variance Reduced Gradient. Another algorithm from the SGD class of methods is Stochastic Variance Reduced Gradient<ref type="foot" target="#foot_0">1</ref> (SVRG) <ref type="bibr" target="#b42">[43]</ref> and <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b43">44]</ref>. The SVRG algorithm runs in two nested loops. In the outer loop, it computes full gradient of the whole function, ∇f (w t ), the expensive operation one tries to avoid in general. In the inner loop, the update step is iteratively computed as</p><formula xml:id="formula_5">w = w − h[∇f i (w) − ∇f i (w t ) + ∇f (w t )].</formula><p>The core idea is that the stochastic gradients are used to estimate the change of the gradient between point w t and w, as opposed to estimating the gradient directly. We return to more detailed description of this algorithm in Section 3.2.</p><p>The SVRG has the advantage that it does not have the additional memory requirements of SAG/SAGA, but it needs to process the whole dataset every now and then. Indeed, comparing to SGD, which typically makes significant progress in the first pass through data, SVRG does not make any update whatsoever, as it needs to compute the full gradient. This and several other practical issues have been recently addressed in <ref type="bibr" target="#b40">[41]</ref>, making the algorithm competitive with SGD early on, and superior in later iterations. Although there is nothing that prevents one from applying SVRG and its variants in deep learning, we are not aware of any systematic assessment of its performance in this setting. Vanilla experiments in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b76">77]</ref> suggest that SVRG matches basic SGD, and even outperforms in the sense that variance of the iterates seems to be significantly smaller for SVRG. However, in order to draw any meaningful conclusions, one would need to perform extensive experiments and compare with state-of-the-art methods usually equipped with numerous heuristics.</p><p>There already exist attempts at combining SVRG type algorithms with randomized coordinate descent <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b96">97]</ref>. Although these works highlight some interesting theoretical properties, the algorithms do not seem to be practical at the moment; more work is needed in this area. The first attempt to unify algorithms such as SVRG and SAG/SAGA already appeared in the SAGA paper <ref type="bibr" target="#b25">[26]</ref>, where the authors interpret SAGA as a midpoint between SAG and SVRG. Recent work <ref type="bibr" target="#b75">[76]</ref> presents a general algorithm, which recovers SVRG, SAGA, SAG and GD as special cases, and obtains an asynchronous variant of these algorithms as a byproduct of the formulation. SVRG can be equipped with momentum (and negative momentum), leading to a new accelerated SVRG method known as Katyusha <ref type="bibr" target="#b2">[3]</ref>. SVRG can be further accelerated via a raw clustering mechanism <ref type="bibr" target="#b3">[4]</ref>.</p><p>Stochastic Quasi-Newton Methods. A third class of new algorithms are the Stochastic quasi-Newton methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>. These algorithms in general try to mimic the limited memory BFGS method (L-BFGS) <ref type="bibr" target="#b53">[54]</ref>, but model the local curvature information using inexact gradients -coming from the SGD procedure. A recent attempt at combining these methods with SVRG can be found in <ref type="bibr" target="#b62">[63]</ref>. In <ref type="bibr" target="#b37">[38]</ref>, the authors utilize recent progress in the area of stochastic matrix inversion <ref type="bibr" target="#b38">[39]</ref> revealing new connections with quasi-Newton methods, and devise a new stochastic limited memory BFGS method working in tandem with SVRG. The fact that the theoretical understanding of this branch of research is the least understood and having several details making the implementation more difficult compared to the methods above may limit its wider use. However, this approach could be most promising for deep learning once understood better.</p><p>One important aspect of machine learning is that the Empirical Risk Minimization problem (1) we are solving is just a proxy for the Expected Risk we are ultimately interested in. When one can find exact minimum of the empirical risk, everything reduces to balancing approximationestimation tradeoff that is the object of abundant literature -see for instance <ref type="bibr" target="#b95">[96]</ref>. An assessment of asymptotic performance of some optimization algorithms as learning algorithms in large-scale learning problems<ref type="foot" target="#foot_1">2</ref> has been introduced in <ref type="bibr" target="#b12">[13]</ref>. Recent extension in <ref type="bibr" target="#b40">[41]</ref> has shown that the variance reduced algorithms (SAG, SVRG, . . . ) can in certain setting be better learning algorithms than SGD, not just better optimization algorithms.</p><p>Further Remarks. A general method, referred to as Universal Catalyst <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b36">37]</ref>, effectively enables conversion of a number of the algorithms mentioned in the previous sections to their 'accelerated' variants. The resulting convergence guarantees nearly match lower bounds in a number of cases. However, the need to tune additional parameter makes the method rather impractical.</p><p>Recently, lower and upper bounds for complexity of stochastic methods on problems of the form (1) were recently obtained in <ref type="bibr" target="#b98">[99]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distributed Setting</head><p>In this section we review the literature concerning algorithms for solving (1) in the distributed setting. When we speak about distributed setting, we refer to the case when the data describing the functions f i are not stored on any single storage device. This can include setting where one's data just don't fit into a single RAM/computer/node, but two is enough. This also covers the case where data are distributed across several datacenters around the world, and across many nodes in those datacenters. The point is that in the system, there is no single processing unit that would have direct access to all the data. Thus, the distributed setting does not include single processor parallelism <ref type="foot" target="#foot_2">3</ref> . Compared with local computation on any single node, the cost of communication between nodes is much higher both in terms of speed and energy consumption <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b88">89]</ref>, introducing new computational challenges, not only for optimization procedures.</p><p>We first reveiew a theoretical decision rule for determining the practically best algorithm for a given problem in Section 2.3.1, followed by overview of distributed algorithms in Section 2.3.2, and communication efficient algorithms in Section 2.3.3. The following paradigm highlights why the class of communication efficient algorithms are not only preferable choice in the trivial sense. The communication efficient algorithms provide us with much more flexible tools for designing overall optimization procedure, which can make the algorithms inherently adaptive to differences in computing resources and architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">A Paradigm for Measuring Distributed Optimization Efficiency</head><p>This section reviews a paradigm for comparing efficency of distributed algorithms. Let us suppose we have many algorithms A readily available to solve the problem (1). The question is: "How do we decide which algorithm is the best for our purpose?" Initial version of this reasoning already appeared in <ref type="bibr" target="#b56">[57]</ref>, and applies also to <ref type="bibr" target="#b77">[78]</ref>.</p><p>First, consider the basic setting on a single machine. Let us define I A ( ) as the number of iterations algorithm A needs to converge to some fixed accuracy. Let T A be the time needed for a single iteration. Then, in practice, the best algorithm is one that minimizes the following quantity. <ref type="foot" target="#foot_3">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TIME = I</head><formula xml:id="formula_6">A ( ) × T A .<label>(2)</label></formula><p>The number of iterations I A ( ) is usually given by theoretical guarantees or observed from experience. The T A can be empirically observed, or one can have idea of how the time needed per iteration varies between different algorithms in question. The main point of this simplified setting is to highlight key issue with extending algorithms to the distributed setting.</p><p>The natural extension to distributed setting is the following. Let c be time needed for communication during a single iteration of the algorithm A. For sake of clarity, we suppose we consider only algorithms that need to communicate a single vector in R d per round of communication. Note that essentially all first-order algorithms fall into this category, so this is not a restrictive assumption, which effectively sets c to be a constant, given any particular distributed architecture one has at disposal.</p><formula xml:id="formula_7">TIME = I A ( ) × (c + T A )<label>(3)</label></formula><p>The communication cost c does not only consist of actual exchange of the data, but also many other things like setting up and closing a connection between nodes. Consequently, even if we need to communicate very small amount of information, c always remains above a nontrivial threshold.</p><p>Most, if not all, of the current state-of-the-art algorithms that are the best in setting of (2), are stochastic and rely on doing very large number (big I A ( )) of very fast (small T A ) iterations. As a result, even relatively small c can cause the practical performance of those algorithms drop down dramatically, because c T A . This has been indeed observed in practice, and motivated development of new methods, designed with this fact in mind from scratch, which we review in Section 2.3.2. Although this is a good development for academia -motivation to explore new setting, it is not necessarily a good news for the industry.</p><p>Many companies have spent significant resources to build excellent algorithms to tackle their problems of form <ref type="bibr" target="#b0">(1)</ref>, fine tuned to the specific patterns arising in their data and side applications required. When the data companies collect grows too large to be processed on a single machine, it is understandable that they would be reluctant to throw away their fine tuned algorithms. This issue was first time explicitly addressed in CoCoA <ref type="bibr" target="#b56">[57]</ref>, which is rather framework than a algorithm, which works as follows (more detailed description follows in Section 2.3.3).</p><p>The CoCoA framework formulates a general way to form a specific subproblem on each node, based on data available locally and a single shared vector that needs to be distributed to all nodes. Within a iteration of the framework, each node uses any optimization algorithm A, to reach a relative Θ accuracy on the local subproblem. Updates from all nodes are then aggregated to form an update to the global model.</p><p>The efficiency paradigm changes as follows:</p><formula xml:id="formula_8">TIME = I( , Θ) × (c + T A (Θ))<label>(4)</label></formula><p>The number of iterations I( , Θ) is independent of choice of the algorithm A used as a local solver, because there is theory predicting how many iterations of the CoCoA framework are needed to achieve accuracy, if we solve the local subproblems to relative Θ accuracy. Here, Θ = 0 would mean we require the subproblem to be solved to optimality, and Θ = 1 that we don't need any progress whatsoever. The general upper bound on number of iterations of the CoCoA framework is I( , Θ) = O(log(<ref type="foot" target="#foot_4">1</ref>/ )) 1−Θ <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b56">57]</ref> for strongly convex objectives. From the inverse dependence on 1 − Θ, we can see that there is a fundamental limit to the number of communication rounds needed. Hence, it will probably not be efficient to spend excessive resources to attain very high local accuracy (small Θ). Time per iteration T A (Θ) denotes the time algorithm A needs to reach the relative Θ accuracy on the local subproblem.</p><p>This efficiency paradigm is more powerful for a number of reasons.</p><p>one cluster to another, a completely different algorithm is optimal, which is a major change.</p><p>In the setting (4), this can be improved by simply changing Θ, which is typically implicitly determined by number of iterations algorithm A runs for.</p><p>In this work we propose a different way to formulate the local subproblems, which does not rely on duality as in the case of CoCoA. We also highlight that some algorithms seem to be particularly suitable to solve those local subproblems, effectively leading to novel algorithms for distributed optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Distributed Algorithms</head><p>As discussed below in Section 2.3.1, this setting creates unique challenges. Distributed optimization algorithms typically require a small number (1-4) of communication rounds per iteration. By communication round we typically understand a single MapReduce operation <ref type="bibr" target="#b23">[24]</ref>, implemented efficiently for iterative procedures <ref type="bibr" target="#b35">[36]</ref>, such as optimization algorithms. Spark <ref type="bibr" target="#b101">[102]</ref> has been established as a popular open source framework for implementing distributed iterative algorithms, and includes several of the algorithms mentioned in this section.</p><p>Optimization in distributed setting has been studied for decades, tracing back to at least works of Bertsekas and Tsitsiklis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b94">95]</ref>. Recent decade has seen an explosion of interest in this area, greatly motivated by rapid increase of data availability in machine learning applications.</p><p>Much of the recent effort was focused on creating new optimization algorithms, by building variants of popular algorithms suitable for running on a single processor (See Section 2.1). A relatively common feature of many of these efforts is a) The computation overhead in the case of synchronous algorithms, and b) The difficulty of analysing asynchronous algorithms without restrictive assumptions. By computation overhead we mean that if optimization program runs in a compute-communicate-update cycle, the update part cannot start until all nodes finish their computation. This causes some of the nodes be idle, while remaining nodes finish their part of computation, clearly an inefficient use of computational resources. This pattern often diminishes or completely reverts potential speed-ups from distributed computation. In the asynchronous setting in general, an update can be applied to a parameter vector, followed by computation done based on a now-outdated version of that parameter vector. Formally grasping this pattern, while keeping the setting realistic is often quite challenging. Consequently, this is very open area, and optimal choice of algorithm in any particular case is often heavily dependent on the problem size, details in its structure, computing architecture available, and above all, expertise of the practitioner. This general issue is best exhibited with numerous attempts at parallelizing the Stochastic Gradient Descent and its variants. As an example, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> provide theoretically linear speedup with number of nodes, but are difficult to implement efficiently, as the nodes need to synchronize frequently in order to compute reasonable gradient averages. As an alternative, no synchronization between workers is assumed in <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31]</ref>. Consequently, each worker reads w t from memory, parameter vector w at time point t, computes a stochastic gradient ∇f i (w t ) and applies it to already changed state of the parameter vector w t+τ . The above mentioned methods assume that the delay τ is bounded by a constant, which is not necessarily realistic assumption <ref type="foot" target="#foot_7">5</ref> . Some of the works also introduce assumptions on the sparsity structures or conditioning of the Hessian of f . Asymptotically optimal convergent rates were proven in <ref type="bibr" target="#b29">[30]</ref> with considerably milder assumptions. Improved analysis of asynchronous SGD was also presented in <ref type="bibr" target="#b21">[22]</ref>, simultaneously with a version that uses lower-precision arithmetic was introduced without sacrificing performance, which is a trend that might find use in other parts of machine learning in the following years.</p><p>The negative effect of asynchronous distributed implementations of SGD seem to be negligible, when applied to the task of training very large deep networks -which is the ultimate industrial application of today. The practical usefulness has been demonstrated for instance by Google's Downpour SGD <ref type="bibr" target="#b22">[23]</ref> and Microsoft's Project Adam <ref type="bibr" target="#b17">[18]</ref>.</p><p>The first distributed versions of Coordinate Descent algorithms were the Hydra and its accelerated variant, Hydra 2 , <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b33">34]</ref>, which has been demonstrated to be very efficient on large sparse problems implemented on a computing cluster. An extended version with description of implementation details is presented in <ref type="bibr" target="#b60">[61]</ref>. Effect of asynchrony has been explored and partially theoretically understood in the works of <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b54">55]</ref>. Another asynchronous, rather framework than an algorithm, for coordinate updates, applicable to wider class of objectives is presented in <ref type="bibr" target="#b71">[72]</ref>.</p><p>The data are assumed to be partitioned to nodes by features/coordinates in the above algorithms. This setting can be restrictive if one is not able to distribute the data beforehand, but instead the data are distributed "as is" -in which case the data are most commonly distributed by data points. This does not need to be an issue, if a dual version of coordinate descent is usedin which the distribution is done by data points <ref type="bibr" target="#b93">[94]</ref> followed by works on Communication Efficient Dual Cooridante Ascent, described in next section. The use of duality however requires usage of additional explicit strongly convex regularization term, hence can be used to solve smaller class of problems. Despite the apparent practical disadvantages, variants of distributed coordinate descent algorithms are among the most widely used methods in practice.</p><p>Moving to variance reduced methods, distributed versions of SAG/SAGA algorithms have not been proposed yet. However, several distributed versions of the SVRG algorithm already exist. A scheme for replicating data to simulate iid sampling in distributed environment was proposed in <ref type="bibr" target="#b49">[50]</ref>. Although the performance is well analysed, the setting requires significantly stronger control of data distribution which is rarely practicaly feasible. A relatively similar method to Algorithm 3 presented here has been proposed in <ref type="bibr" target="#b77">[78]</ref>, which was analysed, and in <ref type="bibr" target="#b58">[59]</ref>, a largely experimental work that can be also cast as communication efficient -described in detail in Section 2.3.3.</p><p>Another class of algorithms relevant for this work is Alternating Direction Method of Multipliers (ADMM) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>. These algorithms are in general applicable to much broader class of problems, and hasn't been observed to perform better than other algorithms presented in this section, in the machine learning setting of (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Communication-Efficient Algorithms</head><p>In this Section, we describe algorithms that can be cast as "communication efficient". The common theme of the algorithms presented here, is that in order to perform better in the sense of (3), one should design algorithms with high T A , in order to make the cost of communcation c negligible.</p><p>Before moving onto specific methods, it is worth the noting some of the core limits concerning the problem we are trying to solve in distributed setting. Fundamental limitations of stochastic versions of the problem (1) in terms of runtime, communication costs and number of samples used are studied in <ref type="bibr" target="#b89">[90]</ref>. Efficient algorithms and lower bounds for distributed statistical estimation are established in <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b102">103]</ref>.</p><p>However, these works do not fit into our framework, because they assume that each node has access to data generated IID from a single distribution. In the case of <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b102">103]</ref> also K n/K, that the number of nodes K is much smaller than the number of data point on each node is also assumed. As we stress in the Introduction, these assumptions are far from being satisfied in our setting. Intuitively, relaxing these assumptions should make the problem harder. However, it is not as straightforward to conclude this, as there are certainly particular non-iid data distributions that simplify the problem -for instance if data are distributed according to separability structure of the objective. Lower bounds on communication complexity of distributed convex optimization of (1) are presented in <ref type="bibr" target="#b4">[5]</ref>, concluding that for IID data distributions, existing algorithms already achieve optimal complexity in specific settings.</p><p>Probably first, rather extreme, work <ref type="bibr" target="#b106">[107]</ref> proposed to parallelize SGD in a single round of communication. Each node simply runs SGD on the data available locally, and their outputs are averaged to form a final result. This approach is however not very robust to differences in data distributions available locally, and it has been shown [91, Appendix A] that in general it cannot perform better than using output of a single machine, ignoring all the other data.</p><p>Shamir et al. proposed the DANE algorithm, Distributed Approximate Newton <ref type="bibr" target="#b90">[91]</ref>, to exactly solve a general subproblem available locally, before averaging their solutions. The method relies on similarity of Hessians of local objectives, representing their iterations as an average of inexact Newton steps. We describe the algorithm in greater detail in Section 3.4 as our proposed work builds on it. A quite similar approach was proposed in <ref type="bibr" target="#b58">[59]</ref>, with richer class class of subproblems that can be formulated locally, and solved approximately. An analysis of inexact version of DANE and its accelerated variant, AIDE, appeared recently in <ref type="bibr" target="#b77">[78]</ref>. Inexact DANE is closely related to the algorithms presented in this paper. We, however, continue in different direction shaped by the setting of federated optimization.</p><p>The DiSCO algorithm <ref type="bibr" target="#b104">[105]</ref> of Zhang and Xiao is based on inexact damped Newton method. The core idea is that the inexact Newton steps are computed by distributed preconditioned conjugate gradient, which can be very fast, if the data are distributed in an IID fashion, enabling a good preconditioner to be computed locally. The theoretical upper bound on number of rounds of communication improves upon DANE and other methods, and in certain settings matches the lower bound presented in <ref type="bibr" target="#b4">[5]</ref>. The DiSCO algorithm is related to <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b105">106]</ref>, a distributed truncated Newton method. Although it was reported to perform well in practice, the total number of conjugate gradient iterations may still be high to be considered a communication efficient algorithm.</p><p>Common to the above algorithms is the assumption that each node has access to data points sampled IID from the same distribution. This assumption is not required only in theory, but can cause the algorithms to converge significantly slower or even diverge (as reported for instance in <ref type="bibr" target="#b90">[91,</ref><ref type="bibr">Table 3]</ref>). Thus, these algorithms, at least in their default form, are not suitable for the setting of Federated Optimization presented here.</p><p>An algorithm that bypasses the need for IID data assumption is CoCoA, which provably converges under any distribution of the data, while the convergence rate does depend on properties of the data distribution. The first version of the algorithm was proposed as DisDCA in <ref type="bibr" target="#b100">[101]</ref>, without convergence guarantees. First analysis was introduced in <ref type="bibr" target="#b41">[42]</ref>, with further improvements in <ref type="bibr" target="#b57">[58]</ref>, and a more general version in <ref type="bibr" target="#b56">[57]</ref>. Recently, its variant for L1-regularized objectives was introduced in <ref type="bibr" target="#b91">[92]</ref>.</p><p>The CoCoA framework formulates general local subproblems based on the dual form of (1) (See for instance [57, Eq. ( <ref type="formula" target="#formula_6">2</ref>)]). Data points are distributed to nodes, along with corresponding dual variables. Arbitrary optimization algorithm is used to attain a relative Θ accuracy on the local subproblem -by changing only local dual variables. These updates have their corresponding updates to primal variable w, which are synchronously aggregated (could be averaging, adding up, or anything in between; depending on the local subproblem formulation).</p><p>From the description in this section it appears that the CoCoA framework is the only usable tool for the setting of Federated Optimization. However, the theoretical bound on number of rounds of communications for ill-conditioned problems scales with the number of nodes K. Indeed, as we will show in Section 4 on real data, CoCoA framework does converge very slowly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithms for Federated Optimization</head><p>In this section we introduce the first algorithm that was designed with the unique challenges of federated optimization in mind. Before proceeding with the explanation, we first revisit two important and at first sight unrelated algorithms. The connection between these algorithms helped to motivate our research. Namely, the algorithms are the Stochastic Variance Reduced Gradient (SVRG) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47]</ref>, a stochastic method with explicit variance reduction, and the Distributed Approximate Newton (DANE) <ref type="bibr" target="#b90">[91]</ref> for distributed optimization.</p><p>The descriptions are followed by their connection, giving rise to a new distributed optimization algorithm, at first sight almost identical to the SVRG algorithm, which we call Federated SVRG (FSVRG).</p><p>Although this algorithm seems to work well in practice in simple circumstances, its performance is still unsatisfactory in the general setting we specify in Section 3.3. We proceed by making the FSVRG algorithm adaptive to different local data sizes, general sparsity patterns and significant differences in patterns in data available locally, and those present in the entire data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Desirable Algorithmic Properties</head><p>It is a useful thought experiment to consider the properties one would hope to find in an algorithm for the non-IID, unbalanced, and massively-distributed setting we consider. In particular:</p><p>(A) If the algorithm is initialized to the optimal solution, it stays there. For convex problems, "converges" has the usual technical meaning of finding a solution sufficiently close to the global minimum, but these properties also make sense for non-convex problems where "converge" can be read as "finds a solution of sufficient quality". In these statements, O(1) round is ideally exactly one round of communication.</p><p>Property (A) is valuable in any optimization setting. Properties (B) and (C) are extreme cases of the federated optimization setting (non-IID, unbalanced, and sparse), whereas (D) is an extreme case of the classic distributed optimization setting (large amounts of IID data per machine). Thus, (D) is the least important property for algorithms in the federated optimization setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SVRG</head><p>The SVRG algorithm <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47]</ref> is a stochastic method designed to solve problem (1) on a single node. We present it as Algorithm 1 in a slightly simplified form. Compute and store ∇f (w t ) = Set w = w t 5:</p><p>for t = 1 to m do 6:</p><p>Pick i ∈ {1, 2, . . . , n}, uniformly at random 7:</p><formula xml:id="formula_9">w = w − h ∇f i (w) − ∇f i (w t ) + ∇f (w t ) Stochastic update 8:</formula><p>end for 9:</p><p>w t+1 = w 10: end for</p><p>The algorithm runs in two nested loops. In the outer loop, it computes gradient of the entire function f (Line 3). This constitutes for a full pass through data -in general expensive operation one tries to avoid unless necessary. This is followed by an inner loop, where m fast stochastic updates are performed. In practice, m is typically set to be a small multiple (1-5) of n. Although the theoretically optimal choice for m is a small multiple of a condition number <ref type="bibr" target="#b46">[47,</ref><ref type="bibr">Theorem 6]</ref>, this is often of the same order as n in practice.</p><p>The central idea of the algorithm is to avoid using the stochastic gradients to estimate the entire gradient ∇f (w) directly. Instead, in the stochastic update in Line 7, the algorithm evaluates two stochastic gradients, ∇f i (w) and ∇f i (w t ). These gradients are used to estimate the change of the gradient of the entire function between points w t and w, namely ∇f (w) − ∇f (w t ). Using this estimate together with ∇f (w t ) pre-computed in the outer loop, yields an unbiased estimate of ∇f (w).</p><p>Apart from being an unbiased estimate, it could be intuitively clear that if w and w t are close to each other, the variance of the estimate ∇f i (w) − ∇f i (w t ) should be small, resulting in estimate of ∇f (w) with small variance. As the inner iterate w goes further, variance grows, and the algorithm starts a new outer loop to compute new full gradient ∇f (w t+1 ) and reset the variance.</p><p>The performance is well understood in theory. For λ-strongly convex f and L-smooth functions f i , convergence results are in the form</p><formula xml:id="formula_10">E[f (w t ) − f (w * )] ≤ c t [f (w 0 ) − f (w * )],<label>(5)</label></formula><p>where w * is the optimal solution, and c = Θ 1 mh + Θ(h). <ref type="foot" target="#foot_9">7</ref>It is possible to show [47, Theorem 6] that for appropriate choice of parameters m and h, the convergence rate (5) translates to the need of (n + O(L/λ)) log(1/ ) evaluations of ∇f i for some i to achieve E[f (w) − f (w * )] &lt; .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distributed Problem Formulation</head><p>In this section, we introduce notation and specify the structure of the distributed version of the problem we consider (1), focusing on the case where the f i are convex. We assume the data {x i , y i } n i=1 , describing functions f i are stored across a large number of nodes. Let K be the number of nodes. Let P k for k ∈ {1, . . . , K} denote a partition of data point indices {1, . . . , n}, so P k is the set stored on node k, and define n k = |P k |. That is, we assume that P k ∩ P l = ∅ whenever k = l, and K k=1 n k = n. We then define local empirical loss as</p><formula xml:id="formula_11">F k (w) def = 1 n k i∈P k f i (w),<label>(6)</label></formula><p>which is the local objective based on the data stored on machine k. We can then rephrase the objective (1) as</p><formula xml:id="formula_12">f (w) = K k=1 n k n F k (w) = K k=1 n k n • 1 n k i∈P k f i (w).<label>(7)</label></formula><p>The way to interpret this structure is to see the empirical loss f (w) = 1 n n i=1 f i (w) as a convex combination of the local empirical losses F k (w), available locally to node k. Problem (1) then takes the simplified form min</p><formula xml:id="formula_13">w∈R d f (w) ≡ K k=1 n k n F k (w). (<label>8</label></formula><formula xml:id="formula_14">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DANE</head><p>In this section, we introduce a general reasoning providing stronger intuitive support for the DANE algorithm <ref type="bibr" target="#b90">[91]</ref>, which we describe in detail below. We will follow up on this reasoning in Appendix A and draw a connection between two existing methods that was not known in the literature.</p><p>If we wanted to design a distributed algorithm for solving the above problem <ref type="bibr" target="#b7">(8)</ref>, where node k contains the data describing function F k . The first, and as we shall see, a rather naive idea is to ask each node to minimize their local functions, and average the results (a variant of this idea appeared in <ref type="bibr" target="#b106">[107]</ref>):</p><formula xml:id="formula_15">w t+1 k = arg min w∈R d F k (w), w t+1 = K k=1 n k n w t+1 k .</formula><p>Clearly, it does not make sense to run this algorithm for more than one iteration as the output w will always be the same. This is simply because w t+1 k does not depend on t. In other words, this method effectively performs just a single round of communication. While the simplicity is appealing, the drawback of this method is that it can't work. Indeed, there is no reason to expect that in general the solution of (8) will be a weighted average of the local solutions, unless the local functions are all the same -in which case we do not need a distributed algorithm in the first place and can instead solve the much simpler problem min w∈R d F 1 (w). This intuitive reasoning can be also formally supported, see for instance <ref type="bibr" target="#b90">[91,</ref><ref type="bibr">Appendix A]</ref>.</p><p>One remedy to the above issue is to modify the local problems before each aggregation step. One of the simplest strategies would be to perturb the local function F k in iteration t by a quadratic term of the form: −(a t k ) T w + µ 2 w − w t 2 and to ask each node to solve the perturbed problem instead. With this change, the improved method then takes the form</p><formula xml:id="formula_16">w t+1 k = arg min w∈R d F k (w) − (a t k ) T w + µ 2 w − w t 2 , w t+1 = 1 K K k=1 w t+1 k .<label>(9)</label></formula><p>The idea behind iterations of this form is the following. We would like each node k ∈ [K] to use as much curvature information stored in F k as possible. By keeping the function F k in the subproblem in its entirety, we are keeping the curvature information nearly intact -the Hessian of the subproblem is ∇ 2 F k + µI, and we can even choose µ = 0.</p><p>As described, the method is not yet well defined, since we have not described how the vectors a t k would change from iteration to iteration, and how one should choose µ. In order to get some insight into how such a method might work, let us examine the optimality conditions. Asymptotically as t → ∞, we would like a t k to be such that the minimum of each subproblem is equal to w * ; the minimizer of <ref type="bibr" target="#b7">(8)</ref>. Hence, we would wish for w * to be the solution of</p><formula xml:id="formula_17">∇F k (w) − a t k + µ(w − w t ) = 0.</formula><p>Hence, in the limit, we would ideally like to choose a t k = ∇F k (w * )+µ(w * −w t ) ≈ ∇F k (w * ), since w * ≈ w t . Not knowing w * however, we cannot hope to be able to simply set a t k to this value. Hence, the second option is to come up with an update rule which would guarantee that a t k converges to ∇F k (w * ) as t → ∞. Notice at this point that it has been long known in the optimization community that the gradient of the objective at the optimal point is intimately related to the optimal solution of a dual problem. Here the situation is further complicated by the fact that we need to learn K such gradients. In the following, we show that DANE is in fact a particular instantiation of the scheme above. DANE. We present the Distributed Approximate Newton algorithm (DANE) <ref type="bibr" target="#b90">[91]</ref>, as Algorithm 2. The algorithm was originally analysed for solving the problem of structure <ref type="bibr" target="#b6">(7)</ref>, with n k being identical for each k -i.e., each computer has the same number of data points. Nothing prevents us from running it in our more general setting though.</p><p>As alluded to earlier, the main idea of DANE is to form a local subproblem, dependent only on local data, and gradient of the entire function -which can be computed in a single round of communication (Line 3). The subproblem is then solved exactly (Line 4), and updates from individual nodes are averaged to form a new iterate (Line 5). This approach allows any algorithm to be used to solve the local subproblem <ref type="bibr" target="#b9">(10)</ref>. As a result, it often achieves communication efficiency in the sense of requiring expensive local computation between rounds of communication, hopefully rendering the time needed for communication insignificant (see Section 2.3.1). Further, note that DANE belongs to the family of distributed method that operate via the quadratic perturbation trick (9) with a t k = ∇F k (w t ) − η∇f (w t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Distributed Approximate Newton (DANE)</head><p>1: Input: regularizer µ ≥ 0, parameter η (default: µ = 0, η = 1) 2: for s = 0, 1, 2, . . . do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Compute ∇f (w t ) = 1 n n i=1 ∇f i (w t ) and distribute to all machines</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>For each node k ∈ {1, . . . , K}, solve</p><formula xml:id="formula_18">w k = arg min w∈R d F k (w) − ∇F k (w t ) − η∇f (w t ) T w + µ 2 w − w t 2<label>(10) 5:</label></formula><p>Compute w t+1 = 1 K K k=1 w k 6: end for If we assumed that the method works, i.e., that w t → w * and hence ∇f (w t ) → ∇f (w * ) = 0, then a t k → ∇F k (w * ), which agrees with the earlier discussion.</p><p>In the default setting when µ = 0 and η = 1, DANE achieves desirable property (D) (immediate convergence when all local datasets are identical), since in this case ∇F k (w t ) − η∇f (w t ) = 0, and so we exactly minimize F k (w) = f (w) on each machine. For any choice of µ and η, DANE also achieves property (A), since in this case ∇f (w t ) = 0, and w t is a minimizer of F k (w) − ∇F k (w t ) • w as well as of the regularization term. Unfortunately, DANE does not achieve the more federated optimization-specific desirable properties (B) and (C).</p><p>The convergence analysis for DANE assumes that the functions are twice differentiable, and relies on the assumption that each node has access to IID samples from the same underlying distribution. This implies that that the Hessians of ∇ 2 F k (w) are similar to each other [91, <ref type="bibr">Lemma 1]</ref>. In case of linear regression, with λ = O(1/ √ n)-strongly convex functions, the number of DANE iterations needed to achieve -accuracy is O(K log(1/ )). However, for general L-smooth loss, the theory is significantly worse, and does not match its practical performance.</p><p>The practical performance also depends on the additional local regularization parameter µ. For small number of nodes K, the algorithm converges quickly with µ = 0. However, as reported [91, Figure <ref type="figure">3</ref>], it can diverge quickly with growing K. Bigger µ makes the algorithm more stable at the cost of slower convergence. Practical choice of µ remains an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">SVRG meets DANE</head><p>As we mentioned above, the DANE algorithm can perform poorly in certain settings, even without the challenging aspects of federated optimization. Another point that is seen as drawback of DANE is the need to find the exact minimum of (10) -this can be feasible for quadratics with relatively small dimension, but infeasible or extremely expensive to achieve for other problems. We adapt the idea from the CoCoA algorithm <ref type="bibr" target="#b56">[57]</ref>, in which an arbitrary optimization algorithm is used to obtain relative Θ accuracy on a locally defined subproblem. We replace the exact optimization with an approximate solution obtained by using any optimization algorithm.</p><p>Considering all the algorithms one could use to solve <ref type="bibr" target="#b9">(10)</ref>, the SVRG algorithm seems to be a particularly good candidate. Starting the local optimization of (10) from point w t , the algorithm automatically has access to the derivative at w t , which is identical for each node -∇f (w t ). Hence, the SVRG algorithm can skip the initial expensive operation, evaluation of the entire gradient (Line 3, Algorithm 1), and proceed only with the stochastic updates in the inner loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It turns out that this modified version of the DANE algorithm is equivalent to a distributed version of SVRG.</head><p>Proposition 1. Consider the following two algorithms.</p><p>1. Run the DANE algorithm (Algorithm 2) with η = 1 and µ = 0, and use SVRG (Algorithm 1) as a local solver for <ref type="bibr" target="#b9">(10)</ref>, running it for a single iteration, initialized at point w t .</p><p>2. Run a distributed variant of the SVRG algorithm, described in Algorithm 3.</p><p>The algorithms are equivalent in the following sense. If both start from the same point w t , they generate identical sequence of iterates {w t }.</p><p>Proof. We construct the proof by showing that single step of the SVRG algorithm applied to the problem <ref type="bibr" target="#b9">(10)</ref> on computer k is identical to the update on Line 8 in Algorithm 3.</p><p>The way to obtain a stochastic gradient of ( <ref type="formula" target="#formula_18">10</ref>) is to sample one of the functions composing</p><formula xml:id="formula_19">F k (w) = 1 n k i∈P k f i (w)</formula><p>, and add the linear term ∇F k (w t ) − ηf (w t ), which is known and does not need to be estimated. Upon sampling an index i ∈ P k , the update direction follows as</p><formula xml:id="formula_20">∇f i (w) − ∇F k (w t ) − f (w t ) − ∇f i (w t ) − ∇F k (w t ) − f (w t ) +∇f (w t ) = ∇f i (w)−∇f i (w t )+∇f (w t ),</formula><p>which is identical to the direction in Line 8 in Algorithm 3. The claim follows by chaining the identical updates to form identical iterate w t+1 . for k = 1 to K do in parallel over nodes k Distributed loop</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 naive Federated SVRG (FSVRG)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Initialize: w k = w t 6:</p><p>for t = 1 to m do Actual update loop 7:</p><p>Sample i ∈ P k uniformly at random 8:</p><formula xml:id="formula_21">w k = w k − h ∇f i (w k ) − ∇f i (w t ) + ∇f (w t ) 9:</formula><p>end for 10:</p><p>end for 11:</p><formula xml:id="formula_22">w t+1 = w t + 1 K K k=1 (w k − w t ) Aggregate 12:</formula><p>end for Remark 2. The algorithms considered in Proposition 1 are inherently stochastic. The statement of the proposition is valid under the assumption that in both cases, identical sequence of samples i ∈ P k would be generated by all nodes k ∈ {1, 2, . . . , K}. Remark 3. In the Proposition 1 we consider the DANE algorithm with particular values of η and µ. The Algorithm 3 and the Proposition can be easily gereralized, but we present only the default version for the sake of clarity.</p><p>Since the first version of this paper, this connection has been mentioned in <ref type="bibr" target="#b77">[78]</ref>, which analyses an inexact version of the DANE algorithm. We proceed by adapting the above algorithm to other challenges arising in the context of federated optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Federated SVRG</head><p>Empirically, the Algorithm 3 fits in the model of distributed optimization efficiency described in Section 2.3.1, since we can balance how many stochastic iterations should be performed locally against communication costs. However, several modifications are necessary to achieve good performance in the full federated optimization setting (Section 3.3). Very important aspect that needs to be addressed is that the number of data points available to a given node can differ greatly from the average number of data points available to any single node. Furthermore, this setting always comes with the data available locally being clustered around a specific pattern, and thus not being a representative sample of the overall distribution we are trying to learn. In the Experiments section we focus on the case of L2 regularized logistic regression, but the ideas carry over to other generalized linear prediction problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Notation</head><p>Note that in large scale generalized linear prediction problems, the data arising are almost always sparse, for example due to bag-of-words style feature representations. This means that only a small subset of d elements of vector x i have nonzero values. In this class of problems, the gradient ∇f i (w) is a multiple of the data vector x i . This creates additional complications, but also potential for exploitation of the problem structure and thus faster algorithms. Before continuing, let us summarize and denote a number of quantities needed to describe the algorithm.</p><p>• n -number of data points / training examples / functions. • P k -set of indices, corresponding to data points stored on device k.</p><p>• n k = |P k | -number of data points stored on device k.</p><p>• n j = {i ∈ {1, . . . , n} : x T i e j = 0} -the number of data points with nonzero j th coordinate • n j k = {i ∈ P k : x T i e j = 0} -the number of data points stored on node k with nonzero j th coordinate • φ j = n j /n -frequency of appearance of nonzero elements in j th coordinate • φ j k = n j k /n k -frequency of appearance of nonzero elements in j th coordinate on node k • s j k = φ j /φ j k -ratio of global and local appearance frequencies on node k in j th coordinate • S k = Diag(s j k ) -diagonal matrix, composed of s j k as j th diagonal element • ω j = {P k : n j k = 0} -Number of nodes that contain data point with nonzero j th coordinate • a j = K/ω j -aggregation parameter for coordinate j • A = Diag(a j ) -diagonal matrix composed of a j as j th diagonal element With these quantities defined, we can state our proposed algorithm as Algorithm 4. Our experiments show that this algorithm works very well in practice, but the motivation for the particular scaling of the updates may not be immediately clear. In the following section we provide the intuition that lead to the development of this algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Intuition Behind FSVRG Updates</head><p>The difference between the Algorithm 4 and Algorithm 3 is in the introduction of the following properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Local stepsize -h</head><formula xml:id="formula_23">k = h/n k . Algorithm 4 Federated SVRG (FSVRG) 1: parameters: h = stepsize, data partition {P k } K k=1 , diagonal matrices A, S k ∈ R d×d for k ∈ {1, . . . , K} 2: for s = 0, 1, 2, . . . do Overall iterations 3: Compute ∇f (w t ) = 1 n n i=1 ∇f i (w t ) 4:</formula><p>for k = 1 to K do in parallel over nodes k Distributed loop</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Initialize:</p><formula xml:id="formula_24">w k = w t and h k = h/n k 6:</formula><p>Let {i t } n k t=1 be random permutation of P k 7:</p><p>for t = 1, . . . , n k do Actual update loop 8:</p><formula xml:id="formula_25">w k = w k − h k S k ∇f it (w k ) − ∇f it (w t ) + ∇f (w t ) 9:</formula><p>end for 10:</p><p>end for 11: As a simplification, assume that at some point in time, we have for some w, w k = w for all k ∈ [K]. In other words, all the nodes have the same local iterate. Although this is not exactly the case in practice, thinking about the issue in this simplified setting will give us insight into what would be meaningful to do if it was true. Further, we can hope that the reality is not too far from the simplification and it will still work in practice. Indeed, all nodes do start from the same point, and adding the linear term ∇F k (w t ) − ∇f (w t ) to the local objective forces all nodes to move in the same direction, at least initially.</p><formula xml:id="formula_26">w t = w t + A K k=1 n k n (w k − w t ) Aggregate 12: end for</formula><p>Suppose the nodes are about to make a single step synchronously. Denote the update direction on node k as G k = ∇f i (w) − ∇f i (w t ) + ∇f (w t ), where i is sampled uniformly at random from P k .</p><p>If we had only one node, i.e., K = 1, it is clear that we would have E[G 1 ] = ∇f (w t ). If K is more than 1, the values of G k are in general biased estimates of ∇f (w t ). We would like to achieve the following:</p><formula xml:id="formula_27">E K k=1 α k G k = ∇f (w t )</formula><p>, for some choice of α k . This is motivated by the general desire to make stochastic first-order methods to make a gradient step in expectation.</p><p>We have</p><formula xml:id="formula_28">E K k=1 α k G k = K k=1 α k 1 n k i∈P k ∇f i (w) − ∇f i (w t ) + ∇f (w t ) .</formula><p>By setting α k = n k n , we get</p><formula xml:id="formula_29">E K k=1 α k G k = 1 n K k=1 i∈P k ∇f i (w) − ∇f i (w t ) + ∇f (w t ) = ∇f (w).</formula><p>This motivates the aggregation of updates from nodes proportional to n k , the number of data points available locally (Point 2).</p><p>Next, we realize that if the local data sizes, n k , are not identical, we likely don't want to do the same number of local iterations on each node k. Intuitively, doing one pass through data (or a fixed number of passes) makes sense. As a result, the aggregation motivated above does not make perfect sense anymore. Nevertheless, we can even it out, by setting the stepsize h k inversely proportional to n k , making sure each node makes progress of roughly the same magnitude overall. Hence, h k = h/n k (Point 1).</p><p>To motivate the Point 3, scaling of stochastic gradients by diagonal matrix S k , consider the following example. We have 1, 000, 000 data points, distributed across K = 1, 000 nodes. When we look at a particular feature of the data points, we observe it is non-zero only in 1, 000 of them. Moreover, all of them happen to be stored on a single node, that stores only these 1, 000 data points. Sampling a data point from this node and evaluating the corresponding gradient, will clearly yield an estimate of the gradient ∇f (w) with 1000-times larger magnitude. This would not necessarily be a problem if done only once. However, repeatedly sampling and overshooting the magnitude of the gradient will likely cause the iterative process to diverge quickly.</p><p>Hence, we scale the stochastic gradients by a diagonal matrix. This can be seen as an attempt to enforce the estimates of the gradient to be of the correct magnitude, conditioned on us, algorithm designers, being aware of the structure of distribution of the sparsity pattern.</p><p>Let us now highlight some properties of the modification in Point 4. Without any extra information, or in the case of fully dense data, averaging the local updates is the only way that actually makes sense -because each node outputs approximate solution of a proxy to the overall objective, and there is no induced separability structure in the outputs such as in CoCoA <ref type="bibr" target="#b56">[57]</ref>. However, we could do much more in the other extreme. If the sparsity structure is such that each data point only depends on one of disjoint groups of variables, and the data were distributed according to this structure, we would efficiently have several disjoint problems. Solving each of them locally, and adding up the results would solve the problem in single iteration -desired algorithm property (C).</p><p>What we propose is an interpolation between these two settings, on a per-variable basis. If a variable appears in data on each node, we are going to take average. However, the less nodes a particular variable appear on, the more we want to trust those few nodes in informing us about the meaningful update to this variable -or alternatively, take a longer step. Hence the per-variable scaling of aggregated updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Further Notes</head><p>Looking at the Proposition 1, we identify equivalence of two algorithms, take the second one and try modify it to make it suitable for the setting of federated optimization. A question naturally arise: Is it possible to achieve the same by modifying the first algorithm suitable for federated optimization -by only altering the local optimization objective?</p><p>We indeed tried to experiment with idea, but we don't report the details for two reasons. First, the requirement of exact solution of the local subproblem is often impractical. Relaxing it gradually moves us to the setting we presented in the previous sections. But more importantly, using this approach we have only managed to get results significantly inferior to those reported later in the Experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we present the first experimental results in the setting of federated optimization. In particular, we provide results on a dataset based on public Google+ posts<ref type="foot" target="#foot_10">8</ref> , clustered by user -simulating each user as a independent node. This preliminary experiment demonstrates why none of the existing algorithms are suitable for federated optimization, and the robustness of our proposed method to challenges arising there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Predicting Comments on Public Google+ Posts</head><p>The dataset presented here was generated based on public Google+ posts. We randomly picked 10, 000 authors that have at least 100 public posts in English, and try to predict whether a post will receive at least one comment (that is, a binary classification task).</p><p>We split the data chronologically on a per-author basis, taking the earlier 75% for training and the following 25% for testing. The total number of training examples is n = 2, 166, 693. We created a simple bag-of-words language model, based on the 20, 000 most frequent words in dictionary based on all Google+ data. This results in a problem with dimension d = 20, 002. The extra two features represent a bias term and variable for unknown word. We then use a logistic regression model to make a prediction based on these features.</p><p>We shape the distributed optimization problem as follows. Suppose that each user corresponds to one node, resulting in K = 10, 000. The average n k , number of data points on node k is thus roughly 216. However, the actual numbers n k range from 75 to 9, 000, showing the data is in fact substantially unbalanced.</p><p>It is natural to expect that different users can exhibit very different patterns in the data generated. This is indeed the case, and hence the distribution to nodes cannot be considered an IID sample from the overall distribution. Since we have a bag-of-words model, our data are very sparse -most posts contain only small fraction of all the words in the dictionary. This, together with the fact that the data are naturally clustered on a per-user basis, creates additional challenge that is not present in the traditional distributed setting.</p><p>Figure <ref type="figure" target="#fig_4">1</ref> shows the frequency of different features across nodes. Some features are present everywhere, such as the bias term, while most features are relatively rare. In particular, over 88% of features are present on fewer than 1, 000 nodes. However, this distribution does not necessarily resemble the overall appearance of the features in data examples. For instance, while an unknown word is present in data of almost every user, it is far from being contained in every data point.</p><p>Naive prediction properties. Before presenting the results, it is useful to look at some of the important basic prediction properties of the data. We use L2-regularized logistic regression, with regularization parameter λ = 1/n. We chose λ to be the best in terms of test error in the optimal solution.</p><p>• If one chooses to predict −1 (no comment), classification error is 33.16%.</p><p>• The optimal solution of the global logistic regression problem yields 26.27% test set error.</p><p>• Predicting the per-author majority from the training data yields 17.14% test error. That is, predict +1 or −1 for all the posts of an author, based on which label was more common in In summary, this data is representative for our motivating application in federated optimization. It is possible to improve upon naive baseline using a fixed global model. Further, the per-author majority result suggests it is possible to improve further by adapting the global model to each user individually. Model personalization is common practice in industrial applications, and the techniques used to do this are orthogonal to the challenges of federated optimization. Exploring its performance is a natural next step, but beyond the scope of this work.</p><p>While we do not provide experiments for per user personalized models, we remark that this could be a good descriptor of how far from IID the data is distributed. Indeed, if each node has access to an IID sample, any adaptation to local data is merely over-fitting. However, if we can significantly improve upon the global model by per user/node adaptation, this means that the data available locally exhibit patterns specific to the particular node.</p><p>The performance of the Algorithm 4 is presented below. The only parameter that remains to be chosen by user is the stepsize h. We tried a set of stepsizes, and retrospectively choose one that works best -a typical practice in machine learning.</p><p>In Figure <ref type="figure" target="#fig_3">2</ref>, we compare the following optimization algorithms<ref type="foot" target="#foot_11">9</ref> :</p><p>• The blue squares (OPT) represent the best possible offline value (the optimal value of the optimization task in the first plot, and the test error corresponding to the optimum in the second plot). • The teal diamonds (GD) correspond to a simple distributed gradient descent.</p><p>• The purple triangles (COCOA) are for the CoCoA+ algorithm <ref type="bibr" target="#b56">[57]</ref>.</p><p>• The green circles (FSVRG) give values for our proposed algorithm.  The first thing to notice is that CoCoA+ seems to be worse than trivial benchmark -distributed gradient descent. This behaviour can be predicted from theory, as the overall convergence rate directly depends on the best choice of aggregation parameter σ . For sparse problems, it is upperbounded by the maximum of the values reported in Figure <ref type="figure" target="#fig_4">1</ref>, which is K, and it is close to it also in practice. Althought it is expected that the algorithm could be modified to depend on average of these quantities (which could be orders of magnitude smaller), akin to coordinate descent algorithms <ref type="bibr" target="#b78">[79]</ref>, it has not been done yet. Note that other communication efficient algorithms fail to converge altogether.</p><p>The algorithm we propose, FSVRG, converges to optimal test classification accuracy in just 30 iterations. Recall that in the setting of federated optimization we introduced in Section 1.2, minimization of rounds of communication is the principal goal. However, concluding that the approach is stunningly superior to existing methods would not be completely fair nor correct. The conclusion is that the FSVRG is the first algorithm to tackle federated optimization, a problem that existing methods fail to generalize to. It is important to stress that none of the existing methods were designed with these particular challenges in mind, and we formulate the first benchmark.</p><p>Since the core reason other methods fail to converge is the non-IID data distribution, we test our method on the same problem, with data randomly reshuffled among the same number of nodes (FSVRGR; red stars). Since the difference in convergence is subtle, we can conclude that the techniques described in Section 3.6.2 serve its purpose and make the algorithm robust to challenges present in federated optimization.</p><p>This experiment demonstrates that learning from massively decentralized data, clustered on a per-user basis is indeed problem we can tackle in practice. Since the first version of this paper <ref type="bibr" target="#b44">[45]</ref>, additional experimental results were presented in <ref type="bibr" target="#b61">[62]</ref>. We refer the reader to this paper for experiments in more challenging setting of deep learning, and a further discussion on how such system would be implemented in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Challenges</head><p>We have introduced a new setting for distributed optimization, which we call federated optimization. This setting is motivated by the outlined vision, in which users do not send the data they generate to companies at all, but rather provide part of their computational power to be used to solve optimization problems. This comes with a unique set of challenges for distributed optimization. In particular, we argue that the massively distributed, non-IID, unbalanced, and sparse properties of federated optimization problems need to be addressed by the optimization community.</p><p>We explain why existing methods are not applicable or effective in this setting. Even the distributed algorithms that can be applied converge very slowly in the presence of large number of nodes on which the data are stored. We demonstrate that in practice, it is possible to design algorithms that work surprisingly efficiently in the challenging setting of federated optimization, which makes the vision conceptually feasible.</p><p>We realize that it is important to scale stochastic gradients on a per-coordinate basis, differently on each node to improve performance. To the best of our knowledge, this is the first time such per-node scaling has been used in distributed optimization. Additionally, we use per-coordinate aggregation of updates from each node, based on distribution of the sparsity patterns in the data.</p><p>Even though our results are encouraging, there is a lot of room for future work. One natural direction is to consider fully asynchronous versions of our algorithms, where the updates are applied as soon as they arrive. Another is developing a better theoretical understanding of our algorithm, as we believe that development of a strong understanding of the convergence properties will drive further research in this area.</p><p>Study of the federated optimization problem for non-convex objectives is another important avenue of research. In particular, neural networks are the most important example of a machine learning tool that yields non-convex functions f i , without any convenient general structure. Consequently, there are no useful results describing convergence guarantees of optimization algorithms. Despite the lack of theoretical understanding, neural networks are now state-of-the-art in many application areas, ranging from natural language understanding to visual object detection. Such applications arise naturally in federated optimization settings, and so extending our work to such problems is an important direction.</p><p>The non-IID data distribution assumed in federated optimization, and mobile applications in particular, suggest that one should consider the problem of training a personalized model together with that of learning a global model. That is, if there is enough data available on a given node, and we assume that data is drawn from the same distribution as future test examples for that node, it may be preferable to make predictions based on a personalized model that is biased toward good performance on the local data, rather than simply using the global model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Distributed Optimization via Quadratic Perturbations</head><p>This appendix follows from the discussion motivating DANE algorithm by a general algorithmic perturbation template (9) for λ-strongly convex objectives. We use this to propose a similar but new method, which unlike DANE converges under arbitrary data partitioning {P k } K k=1 , and we highlight its relation to the dual CoCoA algorithm for distributed optimization.</p><p>For simplicity and ease of drawing the above connections we assume that n k is identical for all k ∈ {1, 2, . . . , K} throughout the appendix. All the arguments can be simply extended, but would unnecessarily complicate the notation for current purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 New Method</head><p>We now present a new method (Algorithm 5), which also belongs to the family of quadratic perturbation methods <ref type="bibr" target="#b8">(9)</ref>. However, the perturbation vectors a t k are different from those of DANE. In particular, we set</p><formula xml:id="formula_30">a t k def = ∇F k (w t ) − (η∇F k (w t ) + g t k )</formula><p>, where η &gt; 0 is a parameter, and the vectors g t k are maintained by the method. As we show in Lemma 4, Algorithm 5 satisfies</p><formula xml:id="formula_31">K k=1 g t k = 0 for all iterations t. This implies that 1 K K k=1 a t k = (1 − η)∇f (w t ).</formula><p>That is, both DANE and the new method use a linear perturbation which, when averaged over the nodes, involves the gradient of the objective function f at the latest iterate w t . Therefore, the methods have one more property in common beyond both being of the form <ref type="bibr" target="#b8">(9)</ref>. However, as we shall see in the rest of this section, Algorithm 5 allows an insightful dual interpretation. Moreover, while DANE may not converge for arbitrary problems (even when restricted to ridge regression)-and is only known to converge under the assumption that the data stored on each node are in some precise way similar, Algorithm 5 converges for any ridge regression problem and any data partitioning.</p><p>Let us denote by X k the matrix obtained by stacking the data points x i as column vectors for all i ∈ P k . We have the following Lemma. Lemma 4. For all t ≥ 0 we have K k=1 g t k = 0. Proof. The statement holds for t = 0. Indeed,</p><formula xml:id="formula_32">K k=1 g t k = η K k=1 K n X k α 0 k − λw 0 = 0,</formula><p>where the last step follows from the definition of w 0 . Assume now that the statement hold for t.</p><formula xml:id="formula_33">Then K k=1 g t+1 k = K k=1 g t k + ηλ(w t+1 k − w t+1 ) = ηλ K k=1 (w t+1 k − w t+1 ).</formula><p>The first equation follows from the way g k is updated in the algorithm. The second equation follows from the inductive assumption, and the last equation follows from the definition of w t+1 in the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 5 Primal Method</head><formula xml:id="formula_34">1: Input: σ ∈ [1, K] 2: Choose: α 0 k ∈ R |P k | for k = 1, 2, . . . , K 3: Set: η = K σ , µ = λ(η − 1) 4: Set: w 0 = 1 λn K k=1 X k α 0 k 5: Set: g 0 k = η( K n X k α 0 k − λw 0 ) for k = 1,<label>2</label></formula><p>, . . . , K 6: for t = 0, 1, 2, . . . do 7:</p><p>for k = 1 to K do 8:</p><formula xml:id="formula_35">w t+1 k = arg min w∈R d F k (w) − ∇F k (w t ) − (η∇F k (w t ) + g t k ) T w + µ 2 w − w t 2 9:</formula><p>end for 10:</p><formula xml:id="formula_36">w t+1 = 1 K K k=1 w t+1 k 11:</formula><p>for k = 1 to K do 12:</p><formula xml:id="formula_37">g t+1 k = g t k + λη(w t+1 k − w t+1 ) 13:</formula><p>end for 14:</p><p>return w t 15: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 L2-Regularized Linear Predictors</head><p>In the rest of this section we consider the case of L2-regularized linear predictors. That is, we focus on problem (1) with f i of the form</p><formula xml:id="formula_38">f i (w) = φ i (x T i w) + λ 2 w 2 ,</formula><p>where λ &gt; 0 is a regularization parameter. This leads to L2 regularized empirical risk minimization (ERM) problem min</p><formula xml:id="formula_39">w∈R d f (w) def = 1 n n i=1 φ i (x T i w) + λ 2 w 2 . (<label>11</label></formula><formula xml:id="formula_40">)</formula><p>We assume that the loss functions φ i : R → R are convex and 1/γ-smooth for some γ &gt; 0; these are standard assumptions. As usual, we allow the loss function φ i to depend on the label y i . For instance, we may choose the quadratic loss: φ i (t) = 1 2 (t − y i ) 2 (for which γ = 1). Let X = [x 1 , . . . , x n ] ∈ R d×n . As described in Section 3.3, we assume that the data (x i , y i ) n i=1 is distributed among K nodes of a computer cluster as follows: node k = 1, 2, . . . , K contains pairs (x i , y i ) for i ∈ P k , where P 1 , . . . , P K forms a partition of the set</p><formula xml:id="formula_41">[n] = {1, 2, . . . , n}. Letting X = [X 1 , . . . , X K ], where X k ∈ R d×|P k | is a submatrix of A corresponding to columns i ∈ P k , and y k ∈ R |P k |</formula><p>is the subvector of y corresponding to entries i ∈ P k . Hence, node k contains the pair (X k , y k ). With this notation, we can write the problem in the form <ref type="bibr" target="#b7">(8)</ref>, where</p><formula xml:id="formula_42">F k (w) = K n i∈P k φ i (x T i w) + λ 2 w 2 .<label>(12)</label></formula><p>A.3 A Dual Method: Dual Block Proximal Gradient Ascent</p><p>The dual of ( <ref type="formula" target="#formula_39">11</ref>) is the problem</p><formula xml:id="formula_43">max α∈R n D(α) def = − 1 2λn 2 Xα 2 − 1 n n i=1 φ * i (−α i ) ,<label>(13)</label></formula><p>where φ * i is the convex conjugate of φ i . Since we assume that φ i is 1/γ smooth, it follows that φ * i is γ strongly convex. Therefore, D is a strongly concave function.</p><p>From dual solution to a primal solution. It is well known that if α * is the optimal solution of the dual problem <ref type="bibr" target="#b10">(11)</ref>, then w * def = 1 λn Xα * is the optimal solution of the primal problem. Therefore, for any dual algorithm producing a sequence of iterates α t , we can define a corresponding primal algorithm via the linear mapping</p><formula xml:id="formula_44">w t def = 1 λn Xα t .<label>(14)</label></formula><p>Clearly, if α t → α * , then w t → w * . We shall now design a method for maximizing the dual function D and then in Theorem 5 we claim that for quadratic loss functions, Algorithm 5 arises as an image, defined via ( <ref type="formula" target="#formula_44">14</ref>), of dual iterations of this dual ascent method.</p><p>Design of the dual gradient ascent method. Let ξ(α</p><formula xml:id="formula_45">) def = 1 2 Xα 2 . Since ξ is a convex quadratic, we have ξ(α + h) = ξ(α) + ∇ξ(α), h + 1 2 h T ∇ 2 ξ(α)h, ≤ ξ(α) + ∇ξ(α), h + σ 2 h 2 B ,</formula><p>where ∇ξ(α) = X T Xα and ∇ 2 ξ(α) = X T X. Further, we define the block-diagonal matrix B def = Diag(X T 1 X 1 , . . . , X T K X K ), and a norm associate with this matrix:</p><formula xml:id="formula_46">h 2 B def = K k=1 X k h k 2 .</formula><p>By σ we refer to a large enough constant for which X T X σB. In order to avoid unnecessary technicalities, we shall assume that the matrices X T k X k are positive definite, which implies that • B is a norm. It can be shown that 1 ≤ σ ≤ K. Clearly, ξ is σ-smooth with respect to the norm • B . In view of the above, for all h ∈ R n we can estimate D from below as follows:</p><formula xml:id="formula_47">D(α t + h) ≥ − 1 λn 2 ξ(α t ) + ∇ξ(α t ), h + σ 2 K k=1 X k h k 2 − 1 n n i=1 φ * i (−α t i − h i ) = − 1 λn 2 ξ(α t ) − K k=1   1 λn 2 ∇ k ξ(α t ), h k + σ 2λn 2 X k h k 2 + 1 n i∈P k φ * i (−α t i − h i )   ,</formula><p>where ∇ k ξ(α t ) corresponds to the subvector of ∇ξ(α t ) formed by entries i ∈ P k . We now let h t = (h t 1 , . . . , h t K ) be the maximizer of this lower bound. Since the lower bound is separable in the blocks {h t k } k , we can simply set h t k := arg min α t+1 = α t + h t 8: end for 9: return w t Having computed h t k for all k, we can set α t+1 k = α t k + h t k for all k, or equivalently, α t+1 = α t + h t . This is formalized as Algorithm 6. Algorithm 6 is a proximal gradient ascent method applied to the dual problem, with smoothness being measured using the block norm h B . It is known that gradient ascent converges at a linear rate for smooth and strongly convex (for minimization problems) objectives.</p><formula xml:id="formula_48">u∈R |P k |    D t k (u) def = 1 λn 2 ∇ k ξ(α t ), u + σ 2λn 2 X k u 2 + 1 n i∈P k φ * i (−α t i − u i )    . (<label>15</label></formula><p>One of the main insights of this section is the following equivalence result.</p><p>Theorem 5 (Equivalence of Algorithms 5 and 6 for Quadratic Loss). Consider the ridge regression problem. That is, set φ i (t) = 1 2 (t − y i ) 2 for all i. Assume α 0 1 , . . . , α 0 K is chosen in the same way in Algorithms 5 and 6. Then the dual iterates α t and the primal iterates w t produced by the two algorithms are related via (14) for all t ≥ 0.</p><p>Since the dual method converges linearly, in view of the above theorem, so does the primal method. Here we only remark that the popular algorithm CoCoA+ <ref type="bibr" target="#b56">[57]</ref> arises if Step 5 in Algorithm 6 is done inexactly. Hence, we show that duality provides a deep relationship between the CoCoA+ and DANE algorithms, which were previously considered completely different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Theorem 5</head><p>In this part we prove the theorem.</p><p>Primal and Dual Problems. Since φ i (t) = 1 2 (t − y i ) 2 , the primal problem ( <ref type="formula" target="#formula_39">11</ref>) is a ridge regression problem of the form min</p><formula xml:id="formula_49">w∈R d f (w) = 1 2n X T w − y 2 + λ 2 w 2 , (<label>16</label></formula><formula xml:id="formula_50">)</formula><p>where X ∈ R d×n and y ∈ R n . In view of ( <ref type="formula" target="#formula_43">13</ref>), the dual of ( <ref type="formula" target="#formula_49">16</ref>) is</p><formula xml:id="formula_51">min α∈R n D(α) = 1 2λn 2 Xα 2 + 1 2n α 2 − 1 n y T α. (<label>17</label></formula><formula xml:id="formula_52">)</formula><p>Primal Problem: Distributed Setup. The primal objective function is of the form <ref type="bibr" target="#b7">(8)</ref>, where in view of (12), we have F k (w) = K 2n X T k w − y k 2 + λ 2 w 2 . Therefore,</p><formula xml:id="formula_53">∇F k (w) = K n X k (X T k w − y k ) + λw<label>(18)</label></formula><p>and ∇f (w We know that X T X σDiag(X T 1 X 1 , . . . , X T K X K ). With this approximation, for all h ∈ R n we can estimate D from above by a node-separable quadratic function as follows:</p><formula xml:id="formula_54">) = 1 K k ∇F k (w) = 1 K k K n X k (X</formula><formula xml:id="formula_55">D(α t + h) ≤ D(α t ) + 1 λn 2 X T Xα t + 1 n (α t − y) T h + 1 2n h 2 + σ 2λn 2 K k=1 X k h k 2 = D(α t ) + 1 n 1 λn (Xα t ) T Xh + (α t − y) T h + 1 2 h 2 + σ 2λn K k=1 X k h k 2 = D(α t ) + 1 n K k=1 (w t ) T X k h k + (α t k − y k ) T h k + 1 2 h k 2 + σ 2λn X k h k 2 .</formula><p>Next, we shall define</p><formula xml:id="formula_56">h t k def = arg min h k ∈R |P k | σ 2λn X k h k 2 + 1 2 h k 2 − (y k − X T k w t − α t k ) T h k<label>(19)</label></formula><p>for k = 1, 2, . . . , K and then set</p><formula xml:id="formula_57">α t+1 = α t + h t . (<label>20</label></formula><formula xml:id="formula_58">)</formula><p>Primal Version of the Dual Method. Note that <ref type="bibr" target="#b18">(19)</ref> has the same form as <ref type="bibr" target="#b16">(17)</ref>, with X replaced by X k , λ replaced by λ/σ and y replaced by c k := y k − X T k w t − α t k . Hence, we know that</p><formula xml:id="formula_59">s t k def = 1 (λ/σ)n X k h t k (<label>21</label></formula><formula xml:id="formula_60">)</formula><p>is the optimal solution of the primal problem of ( <ref type="formula" target="#formula_61">22</ref>):</p><formula xml:id="formula_61">s t k = arg min s∈R d 1 2n X T k s − c k 2 + λ/σ 2 s 2 .<label>(22)</label></formula><p>Hence, the primal version of method ( <ref type="formula" target="#formula_57">20</ref>) is given by</p><formula xml:id="formula_62">w t+1 (14) = 1 λn Xα t+1 (20) = 1 λn X(α t + h t )<label>(14)</label></formula><formula xml:id="formula_63">= w t + 1 λn K k=1 X k h t k = 1 K K k=1 w t + K σ σ λn X k h t k (21) = 1 K K k=1 w t + K σ s t k .</formula><p>With the change of variables w := w t + K σ s (i.e., s = σ K (w − w t )), from <ref type="bibr" target="#b21">(22)</ref> we know that w </p><formula xml:id="formula_64">L k (w) = 1 2n X T k σ K (w − w t ) − c k 2 + λ/σ 2 σ K (w − w t ) 2 = 1 2n σ 2 K 2 (X T k w − y k ) − X T k w t − y k + K σ c k d k 2 + λσ 2 2K 3 w 2 − λσ 2 2K 3 w 2 + λ/σ 2 σ K (w − w t ) 2 = 1 2n σ 2 K 2 X T k w − y k 2 + d k 2 − 2(X T k w − y k ) T d k + λσ 2 2K 3 w 2 − λσ 2 2K 3 w 2 + λσ 2K 2 w − w t 2 = σ 2 K 3 K 2n X T k w − y k 2 + K 2n d k 2 − K n (X T k w − y k ) T d k + λσ 2 2K 3 w 2 − λσ 2 2K 3 w 2 + λσ 2K 2 w − w t 2 = σ 2 K 3 K 2n X T k w − y k 2 + λ 2 w 2 F k (w) + σ 2 K 3 K 2n d k 2 − K (X T k w − y k ) T d k − λσ 2 2K 3 w 2 + λσ 2K 2 w − w t 2 = σ 2 K 3 F k (w) − σ 2 K 2 n (X T k w − y k ) T d k + σ 2 2nK 2 d k 2 − λσ 2 2K 3 w 2 + λσ 2K 2 w − w t 2 = σ 2 K 3 F k (w) − σ 2 K 2 n (X k d k ) T w − λσ 2 2K 3 w 2 + λσ 2K 2 w − w t 2 + σ 2 2nK 2 d k 2 + σ 2 K 2 n y T k d k β 1 .</formula><p>Next, since w 2 = w − w t 2 − w t 2 + 2(w t ) T w, we can further write</p><formula xml:id="formula_65">L k (w) = σ 2 K 3 F k (w) − σ 2 K 2 n (X k d k ) T w − λσ 2 2K 3 ( w − w t 2 − w t 2 + 2(w t ) T w) + λσ 2K 2 w − w t 2 + β 1 = σ 2 K 3 F k (w) − σ 2 K 2 n (X k d k ) T w − λσ 2 K 3 (w t ) T w + λσ 2K 2 − λσ 2 2K 3 w − w t 2 + λσ 2 2K 3 w t 2 + β 1 β 2 = σ 2 K 3 F k (w) − K n X k d k + λw t T w + λ 2 K σ − 1 w − w t 2 + β 2 = σ 2 K 3 F k (w) − ∇F k (w t ) − K 2 σn X k (X T k w t − y k + α t k ) T w + µ 2 w − w t 2 + β 2 = σ 2 K 3      F k (w) −      ∇F k (w t ) − K σ K n X k (X T k w t − y k + α t k ) z t k      T w + µ 2 w − w t 2      + β 2 = σ 2 K 3 F k (w) − ∇F k (w t ) − (η∇F k (w t ) + g t k ) T w + µ 2 w − w t 2 + β 2 ,</formula><p>where the last step follows from the claim that ηz t k = η∇F k (w t ) + g t k . We now prove the claim. First, we have</p><formula xml:id="formula_66">ηz t k = η K n X k (X T k w t − y k + α t k ) = η K n X k (X T k w t − y k ) + η K n X k α t k = η K n X k (X T k w t − y k ) + λw t + η K n X k α t k − λw t<label>(18)</label></formula><p>= η∇F k (w t ) + η K n X k α t k − λw t .</p><p>Due to the definition of g 0 k in Step 5 of Algorithm 5 as g 0 k = η( K n X k α 0 k − λw 0 ), we observe that the claim holds for t = 0. If we show that</p><formula xml:id="formula_67">g t k = η K n X k α t k − λw t</formula><p>for all t ≥ 0, then we are done. This can be shown by induction. This finishes the proof of Theorem 5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(B) If all the data is on a single node, the algorithm should converge in O(1) rounds of communication. (C) If each feature occurs on a single node, so the problems are fully decomposable (each machine is essentially learning a disjoint block of parameters), then the algorithm should converge in O(1) rounds of communication 6 . (D) If each node contains an identical dataset, then the algorithm should converge in O(1) rounds of communication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 SVRG 1 :</head><label>11</label><figDesc>parameters: m = number of stochastic steps per epoch, h = stepsize 2: for s = 0, 1, 2, . . . do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 : 2 : 3 :</head><label>123</label><figDesc>parameters: m = # of stochastic steps per epoch, h = stepsize, data partition {P k } K k=1 for s = 0, 1, 2, . . . do Overall iterations Compute ∇f (w t ) = 1 n n i=1 ∇f i (w t ) 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 .</head><label>2</label><figDesc>Aggregation of updates proportional to partition sizes -n k n (w k − w t ) 3. Scaling stochastic gradients by diagonal matrix -S k 4. Per-coordinate scaling of aggregated updates -A(w k − w t ) Let us now explain what motivated us to get this particular implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Features vs. appearance on nodes. The x-axis is a feature index, and the y-axis represents the number of nodes where a given feature is present.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Rounds of communication vs. objective function (left) and test prediction error (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>) Algorithm 6 2 :</head><label>62</label><figDesc>Dual Method1: Input: σ ∈ [1, K] Choose: α 0 k ∈ R |P k | for k = 1, 2, . . . , K 3: for t = 0, 1, 2, . . . do 4:for k = 1 to K do 5:h t+1 k = arg min u∈R |P k | D t k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>− w t ) − c k 2 +</head><label>2</label><figDesc>us now rewrite the function in<ref type="bibr" target="#b22">(23)</ref> so as to connect it to Algorithm 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>•</head><label></label><figDesc>The red stars (FSVRGR) correspond to the same algorithm applied to the same problem with randomly reshuffled data. That is, we keep the unbalanced number of examples per node, but populate each node with randomly selected examples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Dual Method. Since D is a quadratic, we haveD(α t + h) = D(α t ) + ∇D(α t ) T h + 1 2 h T ∇ 2 D(α t )h,</figDesc><table><row><cell>with</cell><cell>∇D(α t ) =</cell><cell>1 λn 2 X T Xα t +</cell><cell>1 n</cell><cell>(α t − y),</cell><cell>∇ 2 D(α t ) =</cell><cell>1 λn 2 X T X +</cell><cell>1 n</cell><cell>I.</cell></row></table><note>T k w − y k ) + λw .</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The same algorithm was simultaneously introduced as Semi-Stochastic Gradient Descent (S2GD)<ref type="bibr" target="#b46">[47]</ref>. Since the former work gained more attention, we will for clarity use the name SVRG throughout this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">See<ref type="bibr" target="#b12">[13,</ref> Section 2.3]  for their definition of large scale learning problem.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">It should be noted that some of the works presented in this section were originally presented as parallel algorithms. We include them anyway as many of the general ideas in carry over to the distributed setting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Considering only algorithms that can be run on a given machine.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_4">. It allows practicioners to continue using their fine-tuned solvers, that can run only on single machine, instead of having to implement completely new algorithms from scratch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_5">. The actual performance in terms of number of rounds of communication is independent from the choice of optimization algorithm, making it much easier to optimize the overall performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_6">3. Since the constant c is architecture dependent, running optimal algorithm on one node network does not have to be optimal on another. In the setting (3), this could mean moving from</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_7">A bound on the delay τ can be deterministic or probabilistic. However, in practice, the delays are mostly about the number of nodes in the network, and there rare very long delays, when a variety of operating system-related events can temporarily postpone computation of a single node. To the best of our knowledge, no formal assumptions reflect this setting well. In fact, two recent works<ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b47">48]</ref> highlight subtle but important issue with labelling of iterates in the presence of asynchrony, rendering most of the existing analyses of asynchronous optimization algorithms incorrect.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_8">This is valid only for generalized linear models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_9">See<ref type="bibr" target="#b46">[47,</ref> Theorem 4]  and<ref type="bibr" target="#b42">[43,</ref> Theorem 1]  for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_10">The posts were public at the time the experiment was performed, but since a user may decide to delete the post or make it non-public, we cannot release (or even permanently store) any copies of the data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_11">We thank Mark Schmidt for his prettyPlot function, available on his website.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">H</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00133</idno>
		<title level="m">Deep learning with differential privacy</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed delayed stochastic optimization</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Zeyuan Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Katyusha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05953</idno>
		<title level="m">The first direct acceleration of stochastic gradient methods</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exploiting the structure: Stochastic gradient methods using raw clusters</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02151</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Communication complexity of distributed convex learning and optimization</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Arjevani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1756" to="1764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scaling up machine learning: Parallel and distributed approaches</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Bekkerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<title level="m">Distributed asynchronous computation of fixed points. Mathematical Programming</title>
				<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="107" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Parallel and distributed computation: numerical methods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dimitri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sgd-qn: Careful quasi-Newton stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Curiously fast convergence of some stochastic gradient descent algorithms</title>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the symposium on learning and data science</title>
				<meeting>the symposium on learning and data science</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent tricks</title>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="421" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04838</idno>
		<title level="m">Optimization methods for large-scale machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The tradeoffs of large scale learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends R in Machine Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parallel coordinate descent for l1-regularized loss minimization</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
				<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A stochastic quasi-Newton method for large-scale optimization</title>
		<author>
			<persName><forename type="first">Samantha</forename><forename type="middle">L</forename><surname>Richard H Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1008" to="1031" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Differentially private empirical risk minimization</title>
		<author>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Monteleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1069" to="1109" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Project adam: Building an efficient and scalable deep learning training system</title>
		<author>
			<persName><forename type="first">Trishul</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Kalyanaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Where (and when) do you use your smartphone</title>
		<ptr target="http://edition.cnn.com/2013/07/13/tech/smartphone-use-survey/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>CNN</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Primal method for ERM with flexible mini-batching schemes and nonconvex losses</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Csiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02227</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Coordinate descent face-off: primal or dual?</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Csiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08982</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Taming the wild: A unified analysis of hogwild-style algorithms</title>
		<author>
			<persName><forename type="first">Christopher M De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2656" to="2664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Quoc V Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A simple practical accelerated method for finite sums</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Defazio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02442</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimal distributed online prediction using mini-batches</title>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="202" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the global and linear convergence of the generalized alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="889" to="916" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dual averaging for distributed optimization: convergence analysis and network scaling. Automatic control</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="592" to="606" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Sorathan</forename><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Chaturapruek</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.00882</idno>
		<title level="m">Asynchronous stochastic convex optimization</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimation, optimization, and parallelism when data is sparse</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">H</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2832" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Privacy aware learning</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Computing Machinery</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The Algorithmic Foundations of Differential Privacy. Foundations and Trends in Theoretical Computer Science</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Now Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast distributed coordinate descent for nonstrongly convex losses</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Fercoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>MLSP)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accelerated, parallel, and proximal coordinate descent</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Fercoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1997" to="2023" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">MPI: A message passing interface standard</title>
		<author>
			<persName><forename type="first">Mpi</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName><surname>Forum</surname></persName>
		</author>
		<ptr target="http://www.mpi-forum.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sham M Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
				<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stochastic block BFGS: squeezing more curvature out of data</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mansel Gower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
				<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1869" to="1878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Randomized quasi-Newton updates are linearly convergent matrix inversion algorithms</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mansel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gower</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01768</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1869" to="1878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Why random reshuffling beats stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Mert</forename><surname>Gürbüzbalaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asu</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Parrilo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08560</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stop wasting my gradients: Practical SVRG</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Harikandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">Osama</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alim</forename><surname>Virani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sallinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2251" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Communication-efficient distributed dual coordinate ascent</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Terhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3068" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mini-batch semi-stochastic gradient descent in the proximal setting</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="242" to="255" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">H</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03575</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Federated optimization: Distributed optimization beyond the datacenter</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6293</idno>
		<title level="m">Semi-stochastic coordinate descent</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.1666</idno>
		<title level="m">Semi-stochastic gradient descent methods</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04809</idno>
		<title level="m">ASAGA: Asynchronous parallel saga</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName><forename type="first">Léon</forename><surname>Yann A Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07595</idno>
		<title level="m">Distributed stochastic variance reduced gradient methods</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Large-scale logistic regression and linear support vector machines using spark</title>
		<author>
			<persName><forename type="first">Chieh-Yen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Hao</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Pei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
	<note>Big Data (Big Data)</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A universal catalyst for first-order optimization</title>
		<author>
			<persName><forename type="first">Hongzhou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3366" to="3374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Nocedal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
	<note>Mathematical programming</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic coordinate descent: Parallelism and convergence properties</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="351" to="376" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An asynchronous parallel stochastic coordinate descent algorithm</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bittorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikrishna</forename><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="285" to="322" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Distributed optimization with arbitrary local solvers</title>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><surname>Takáč</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04039</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adding vs. averaging in distributed primal-dual optimization</title>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
				<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1973" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">An efficient distributed learning algorithm based on effective local functional approximations</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><surname>Sathiya Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.8418</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Perturbed iterate analysis for asynchronous stochastic optimization</title>
		<author>
			<persName><forename type="first">Horia</forename><surname>Mania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06970</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Distributed block coordinate descent for minimizing partially separable functions</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Numerical Analysis and Optimization</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="261" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Eider</forename><surname>Brendan H Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><surname>Aguera Y Arcas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05629</idno>
		<title level="m">Federated learning of deep networks using model averaging</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A linearly-convergent stochastic l-bfgs algorithm</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the 19th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="249" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Non-asymptotic analysis of stochastic approximation algorithms for machine learning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Moulines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm</title>
		<author>
			<persName><forename type="first">Deanna</forename><surname>Needell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Robust stochastic approximation approach to stochastic programming</title>
		<author>
			<persName><forename type="first">Arkadi</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatoli</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Efficiency of coordinate descent methods on huge-scale optimization problems</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="362" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate o(1/k 2 )</title>
		<author>
			<persName><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Mathematics Doklady</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="372" to="376" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Introductory Lectures on Convex Optimization. A Basic Course</title>
		<author>
			<persName><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">On optimization methods for deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahbik</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby</forename><surname>Prochnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
				<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Arock: an algorithmic framework for asynchronous parallel coordinate updates</title>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02396</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">SDNA: Stochastic dual newton ascent for empirical risk minimization</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Fercoq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
				<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1823" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Quartz: Randomized dual coordinate ascent with arbitrary sampling</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="865" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On variance reduction in stochastic gradient descent and its asynchronous variants</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Póczós</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2647" to="2655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Suvrit Sra, Barnabás Póczós, and Alex Smola. Stochastic variance reduction for nonconvex optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><surname>Hefny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06160</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Sashank J Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06879</idno>
		<title level="m">Barnabás Póczós, and Alex Smola. AIDE: Fast and communication efficient distributed optimization</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Parallel coordinate descent methods for big data optimization</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="433" to="484" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Distributed coordinate descent method for learning with big data</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">75</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">A stochastic approximation method. The Annals of Mathematical Statistics</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A stochastic gradient method with an exponential convergence rate for finite training sets</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2663" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Nonuniform stochastic average gradient method for training conditional random fields</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Babanezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="819" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Minimizing finite sums with the stochastic average gradient</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.2388</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01582</idno>
		<title level="m">SDCA without duality, regularization, and individual convexity</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Pegasos: Primal estimated subgradient solver for svm</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Stochastic dual coordinate ascent methods for regularized loss</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="567" to="599" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Exascale computing technology challenges</title>
		<author>
			<persName><forename type="first">John</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudip</forename><surname>Dosanjh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing for Computational Science-VECPAR 2010</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Distributed stochastic optimization and learning</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication, Control and Computing (Allerton), 2014 52nd Annual Allerton Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="850" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Communication-efficient distributed optimization using an approximate newton-type method</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
				<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1000" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">L1-regularized distributed optimization: A communication-efficient primal-dual framework</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04011</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Mini-batch primal and dual methods for SVMs</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avleen</forename><surname>Bijral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
				<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takáč</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08322</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Distributed mini-batch SDCA</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Problems in decentralized decision making and computation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Nikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsitsiklis</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">An overview of statistical learning theory</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="988" to="999" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Randomized block coordinate descent for online and stochastic optimization</title>
		<author>
			<persName><forename type="first">Huahua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.0107</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Consumer data privacy in a networked world: A framework for protecting privacy and promoting innovation in the global digital economy</title>
		<author>
			<persName><forename type="first">White</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Report</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">Blake</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08003</idno>
		<title level="m">Tight complexity bounds for optimizing composite objectives</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A proximal stochastic gradient method with progressive variance reduction</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2057" to="2075" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Trading computation for communication: Distributed stochastic dual coordinate ascent</title>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="629" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Spark: cluster computing with working sets</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Michael J Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd USENIX conference on Hot topics in cloud computing</title>
				<meeting>the 2nd USENIX conference on Hot topics in cloud computing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Information-theoretic lower bounds for distributed statistical estimation with communication constraints</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2328" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Communication-efficient algorithms for statistical optimization</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3321" to="3363" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">DiSCO: Distributed optimization for self-concordant empirical loss</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32th International Conference on Machine Learning</title>
				<meeting>The 32th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Distributed newton methods for regularized logistic regression</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="690" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Parallelized stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2595" to="2603" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
