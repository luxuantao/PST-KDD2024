<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphGLOW: Universal and Generalizable Structure Learning for Graph Neural Networks</title>
				<funder ref="#_xysTGwu">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_8FkCTt3">
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
				<funder ref="#_55CgywQ">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_SkYVbRp">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-20">20 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country>China Qitian Wu</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<settlement>Chenxiao Yang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">KDD &apos;23</orgName>
								<address>
									<addrLine>August 6-10</addrLine>
									<postCode>2023</postCode>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">2023. Graph-GLOW: Universal and Generalizable Structure Learning for Graph Neural Networks. In</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GraphGLOW: Universal and Generalizable Structure Learning for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-20">20 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3580305.3599373</idno>
					<idno type="arXiv">arXiv:2306.11264v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Structure Learning</term>
					<term>Out-of-Distribution Generalization</term>
					<term>Graph Neural Networks</term>
					<term>Network Representation Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph structure learning is a well-established problem that aims at optimizing graph structures adaptive to specific graph datasets to help message passing neural networks (i.e., GNNs) to yield effective and robust node embeddings. However, the common limitation of existing models lies in the underlying closed-world assumption: the testing graph is the same as the training graph. This premise requires independently training the structure learning model from scratch for each graph dataset, which leads to prohibitive computation costs and potential risks for serious over-fitting. To mitigate these issues, this paper explores a new direction that moves forward to learn a universal structure learning model that can generalize across graph datasets in an open world. We first introduce the mathematical definition of this novel problem setting, and describe the model formulation from a probabilistic data-generative aspect. Then we devise a general framework that coordinates a single graph-shared structure learner and multiple graph-specific GNNs to capture the generalizable patterns of optimal message-passing topology across datasets. The well-trained structure learner can directly produce adaptive structures for unseen target graphs without any fine-tuning. Across diverse datasets and various challenging cross-graph generalization protocols, our experiments show that even without training on target graphs, the proposed model i) significantly outperforms expressive GNNs trained on input (nonoptimized) topology, and ii) surprisingly performs on par with state-of-the-art models that independently optimize adaptive structures for specific target graphs, with notably orders-of-magnitude acceleration for training on the target graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Machine learning algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>, as a de facto model class based on the message passing principle, have shown promising efficacy for learning node representations for graph-structured data, with extensive applications to, e.g., physics simulation <ref type="bibr" target="#b28">[29]</ref>, traffic prediction <ref type="bibr" target="#b15">[16]</ref>, drug recommendation <ref type="bibr" target="#b47">[48]</ref>. However, due to the inevitable error-prone data collection <ref type="bibr" target="#b5">[6]</ref>, the input graph may contain spurious and unobserved edges that lead to sub-optimal results of GNNs and degrade the downstream performance.</p><p>Graph structure learning <ref type="bibr" target="#b54">[55]</ref> serves as a plausible remedy for such an issue via optimizing graph structures and GNN classifiers at the same time. To this end, recent endeavors explore different technical aspects, e.g., parameterizing each potential edge between any pair of nodes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> or estimating potential links through a parameterized network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>, etc. However, existing models limit their applicability within a closed-world hypothesis: the training and testing of structure learning models, which optimize the graph structures, are performed on the same graph. The issue, however, is that since structure learning is often heavy-weighted and requires sophisticated optimization, it can be prohibitively resourceconsuming to train structure learning models from scratch for each graph dataset. Moreover, due to limited labels in common graphbased predictive tasks, structure learning models are prone to overfitting given that they cannot utilize the common knowledge shared across different graph datasets.</p><p>To resolve the above dilemma, this paper attempts to explore a novel problem setting termed Open-World Graph Structure Learning. Specifically, we target learning a generalizable graph structure learning model which is trained with multiple source graphs and can be directly adapted for inference (without re-training or finetuning) on new unseen target graphs. We formulate the problem as a bi-level optimization target that jointly learns a single datasetshared structure learner and multiple dataset-specific GNNs tailored for particular graph datasets, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Under such a framework, the well-trained structure learner can leverage the common transferrable knowledge across datasets for enhancing generalization and more critically, be readily utilized to yield adaptive message-passing topology for arbitrarily given target graphs.</p><p>With the guidance of the aforementioned general goal, we propose GraphGLOW (short for A Graph Structure Learning Model for Open-World Generalization) that aims at learning the generalizable patterns of optimal message-passing topology across source graphs. Specifically, we first take a bottom-up perspective and formulate the generative process for observed data in a probabilistic manner. On top of this, we derive a tractable and feasible learning objective through the lens of variational inference. The structure learner is specified as a multi-head weighted similarity function so as to guarantee enough expressivity for accommodating diverse structural information, and we further harness an approximation scheme to reduce the quadratic complexity overhead of learning potential edges from arbitrary node pairs.</p><p>To reasonably and comprehensively evaluate the model, we devise experiments with a diverse set of protocols that can measure the generalization ability under different difficulty levels (according to the intensity of distribution shifts between source graphs and target graphs). Concretely, we consider: 1) In-domain generalization, in which we generalize from some citation (social) networks to other citation (social) networks. 2) Cross-domain networks generalization between citation and social networks. The results, which are consistent across various combinations of source and target graph datasets, demonstrate that when evaluated on the target graphs, our approach i) consistently outperforms directly training the GNN counterpart on original non-optimized graph structures of the target datasets and ii) performs on par with state-of-the-art structure learning methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref> trained on target graphs from scratch with up to 25? less training time consumed. Our code is available at https://github.com/WtaoZhao/GraphGLOW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY AND PROBLEM DEFINITION</head><p>Node-Level Predictive Tasks. Denote a graph with ? nodes as G = (A, X, Y) where A = {? ?? } ? ?? is an adjacency matrix (? ?? = 1 means the edge between node ? and ? exists and 0 otherwise), X = {x ? } ? ?? is a feature matrix with x ? a ?-dimensional node feature vector of node ?, and Y = {? ? } ? ?? with ? ? the label vector of node ? and ? class number. The node labels are partially observed as training data, based on which the node-level prediction aims to predict the unobserved labels for testing nodes in the graph using node features and graph structures. The latter is often achieved via a GNN model, denoted as ? ? , that yields predicted node labels ? = ? ? (A, X) and is optimized with the classification loss ? * = arg min ? = L ( ?, Y) using observed labels from training nodes.</p><p>Closed-World Graph Structure Learning (GLCW). The standard graph structure learning for node-level predictive tasks trains a graph structure learner ? ? to refine the given structure, i.e., ? = ? ? (A, X), over which the GNN classifier ? ? conducts message passing for producing node representations and predictions. The ? ? is expected to produce optimal graph structures that can give rise to satisfactory downstream classification performance of the GNN classifier. Formally speaking, the goal for training ? ? along with ? ? can be expressed as a nested optimization problem:</p><formula xml:id="formula_0">? * = arg min ? min ? L (? ? (? ? (A, X), X), Y) .<label>(1)</label></formula><p>The above formulation of graph structure learning under closedworld assumptions constrains the training and testing nodes in the same graph, which requires ? ? to be trained from scratch on each graph dataset. Since ? ? is often much more complicated (e.g., with orders-of-magnitude more trainable parameters) and difficult for optimization (due to the bi-level optimization (1)) than the GNN ? ? , the GLCW would lead to undesired inefficiency and vulnerability for serious over-fitting (due to limited labeled information).</p><p>Open-World Graph Structure Learning (GLOW). In this work, we turn to a new learning paradigm that generalizes graph structure learning to open-world assumptions, borrowing the concepts of domain generalization <ref type="bibr" target="#b26">[27]</ref> and out-of-distribution generalization <ref type="bibr" target="#b39">[40]</ref>, more broadly. Specifically, assume that we are given multiple source graphs, denoted as</p><formula xml:id="formula_1">{G ? ? } ? ?=1 = {(A ? ? , X ? ? , Y ? ? )} ? ?=1</formula><p>, and a target graph G ? = (A ? , X ? , Y ? ), whose distribution is often different from any source graph. The goal is to train a universal structure learner ? ? on source graphs which can be directly used for inference on the target graph without any re-training or finetuning. The trained structure learner is expected to produce desired graph structures that can bring up better downstream classification of a GNN classifier optimized for the target graph.</p><p>More specifically, we consider a one-to-many framework that coordinates a shared graph structure learner ? ? and multiple datasetspecific GNNs {? ? ? } ? ?=1 , where ? ? ? with independent parameterization ? ? is optimized for a given source graph G ? ? . With the aim of learning a universal ? ? that can generalize to new unseen target graphs, our training goal can be formulated as the following bi-level optimization problem:  where the inner optimization is a multi-task learning objective. Generally, (2) aims at finding an optimal ? ? that can jointly minimize the classification loss induced by ? GNN models, each trained for a particular source graph. After training, we can directly adapt ? ? * to the target graph for testing purpose, and only need to train a GNN ? ? on the target graph:</p><formula xml:id="formula_2">? * = arg min ? min ? 1 ,??? ,? ? ? ?? ?=1 L ? ? ? (? ? (A ? ? , X ? ? ), X ? ? ), Y ? ? ,<label>(2)</label></formula><formula xml:id="formula_3">? * = arg min ? L ? ? (? ? * (A ? , X ? ), X ? ), Y ? . (<label>3</label></formula><formula xml:id="formula_4">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED MODEL</head><p>To handle the above problem, we present an end-to-end learning framework GraphGLOW that guides the central graph structure learner to learn adaptive message-passing structures exploited by multiple GNNs. The overview of GraphGLOW is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The fundamental challenge of GLOW lies in how to model and capture the generalizable patterns among adaptive structures of different graphs. To this end, we first take a data-generative perspective that treats the inputs and inter-mediate results as random variables and investigate into their dependency, based on which we present the high-level model formulation in a probabilistic form (Sec. 3.1). Then we proceed to instantiate the model components (Sec. 3.2). Finally, we discuss differentiable training approaches for optimization (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Formulation</head><p>To commence, we characterize the data generation process by a latent variable model, based on which we derive the formulation of our method. We treat the latent graph ? (given by ? ? ) as a latent variable whose prior distribution is given by ? ( ?|A, X). The prior distribution reflects how one presumed on the latent structures before observed labels arrive. Then, the prediction is given by a predictive distribution ? (Y| ?, A, X). The learning objective aims at maximizing the log-likelihood of observed labels, which can be written as: log ? (Y|A, X) = log ? ? ? (Y|A, X, ?)? ( ?|A, X)? ?. To estimate latent graphs that could enhance message passing for downstream tasks, one plausible way is to sample from the posterior, i.e., ? ( ?|Y, A, X), conditioned on the labels from downstream tasks. Using the Bayes' rule, we have</p><formula xml:id="formula_5">? ( ?|Y, A, X) = ? (Y|A, X, ?)? ( ?|A, X) ? ? ? (Y|A, X, ?)? ( ?|A, X)? ? .<label>(4)</label></formula><p>However, the integration over ? in the denominator is intractable for computation due to the exponentially large space of ?.</p><p>To circumvent the difficulty, we can introduce a variational distribution ?( ?|A, X) over ? as an approximation to ? ( ?|Y, A, X). We can sample latent graphs from ?( ?|A, X), i.e., instantiate it as the structure learner ? ? , and once ?( ?|A, X) = ? ( ?|Y, A, X), we could have samples from the posterior that ideally generates the optimal graph structures for downstream prediction. By this principle, we can start with minimizing the Kullback-Leibler divergence between ? and ? and derive the learning objective as follows:</p><formula xml:id="formula_6">D ?? (?( ?|A, X)?? ( ?|Y, A, X)) = -E ??? ( ?|A,X) log ? (Y|A, X, ?)? ( ?|A, X) ?( ?|A, X)</formula><p>Evidence Lower Bound + log ? (Y|A, X).</p><p>(</p><formula xml:id="formula_7">)<label>5</label></formula><p>Based on this equation, we further have the inequality which bridges the relationship between the Evidence Lower Bound (ELBO) and observed data log-likelihood: log ? (Y|A, X) ? E ??? ( ?|A,X) log ? (Y|A, X, ?)? ( ?|A, X) ?( ?|A, X)</p><p>.</p><p>The equality holds if and only if D ?? (?( ?|A, X)?? ( ?|Y, A, X)) = 0. The above fact suggests that we can optimize the ELBO as a surrogate for log ? (Y|A, X) which involves the intractable integration. More importantly, when the ELBO is optimized w.r.t. ? distribution, the variational bound is lifted to the original log-likelihood and one has ?( ?|A, X) = ? ( ?|Y, A, X), i.e., the variational distribution equals to the true posterior, which is what we expect.</p><p>Pushing further and incorporating source graphs G ? (we omit the superscript for simplicity), we arrive at the following objective:</p><formula xml:id="formula_9">E G ? ?? ( G) E ??? ? ( ?|A=A ? ,X=X ? ) log ? ? ? (Y|A = A ? , X = X ? , ?) + log ? 0 ( ?|A = A ? , X = X ? ) -log ? ? ( ?|A = A ? , X = X ? ) .<label>(7)</label></formula><p>Here we instantiate ?( ?|A, X) as the shared structure learner ? ? , ? ( ?|A, X) as a (shared) non-parametric prior distribution ? 0 for latent structures, and ? (Y|A, X, ?) as the dataset-specific GNN model ? ? ? , to suit the framework for our formulated problem in Section 2. The formulation of (7) shares the spirits with Bayesian meta learning <ref type="bibr" target="#b8">[9]</ref>. We can treat the GNN training as a dataset-specific learning task and latent graph as a certain 'learning algorithm' or 'hyper-parameter', so <ref type="bibr" target="#b6">(7)</ref> essentially aims at learning a structure learner that can yield desirable 'learning algorithm' for each specific learning task on graphs. Furthermore, the three terms in <ref type="bibr" target="#b6">(7)</ref> have distinct effects: i) the predictive term log ? ? ? acts as a supervised classification loss; ii) the prior term log ? 0 serves for regularization on the generated structures; iii) the third term, which is essentially the entropy of ? ? , penalizes high confidence on certain structures.</p><p>To sum up, we can optimize <ref type="bibr" target="#b6">(7)</ref> with joint learning of the structure learner ? ? and GNN models {? ? ? } ? ?=1 on source graphs {G ? } ? ?=1 for training the structure learner. After that, we can generalize the well-trained ? ? * to estimate latent graph structures for a new target graph G ? = (A ? , X ? ) and only need to train the GNN model ? ? w.r.t. the predictive objective with fixed ? * :</p><formula xml:id="formula_10">E ??? ? * ( ?|A=A ? ,X=X ? ) log ? ? (Y|A = A ? , X = X ? , ?) . (8)</formula><p>We next discuss how to specify ? ? , ? ? ? and ? 0 with special focus on their expressiveness and efficiency in Section 3.2. Later, we present the details for loss computation and model training based on the formulation stated above in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Instantiations</head><p>3.2.1 Instantiation for ? ? ( ?|A, X). The variational distribution aims at learning the conditional distribution that generates suitable latent structures for message passing based on input observations. A natural means is to assume each edge of the latent graph as a Bernoulli random variable and the distribution ? is a product of ? ? ? independent Bernoulli random variables <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>The graph structure learner ? ? can be used for predicting the Bernoulli parameter matrix. To accommodate the information from node features and graph structure, we can use the node representation, denoted as z ? ? R ? , where ? is the embedding dimension, to compute the edge probability ? ?? for edge (?, ?) as</p><formula xml:id="formula_11">? ?? = ? 1 ? ? ?? ?=1 ? (w 1 ? ? z ? , w 2 ? ? z ? ) ,<label>(9)</label></formula><p>where ? (?, ?) is a similarity function for two vectors, ? denotes Hadamard product, ? is a function that converts the input into values within [0, 1] and w 1 ? , w 2 ? ? R ? are two weight vectors of the ?-th head. Common choices for ? (?, ?) include simple dot-product, cosine distance <ref type="bibr" target="#b27">[28]</ref>, RBF kernel <ref type="bibr" target="#b20">[21]</ref>, etc. Here we introduce ? heads and aggregate their results to enhance model's expressiveness for capturing the underlying influence between nodes from multifaceted causes, following the spirit of multi-head attention <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Besides, the weight vectors in ( <ref type="formula" target="#formula_11">9</ref>) could learn to element-wisely scale the input vectors, i.e., node representations, and adaptively attend to dominant features. Apart from these, two weight vectors w 1 ? , w 2 ? with independent parameterization could potentially have the same or distinct directions, which makes the model capable of connecting similar or dissimilar nodes and expressive enough to handle both homophilous and non-homophilous graphs.</p><p>To obtain discrete latent graph ? = { ??? } ? ?? , one can sample from ??? ? ????????? (? ?? ) to obtain each latent edge. However, such an approach induces the quadratic algorithmic complexity ? (? 2 ) for computing and storing an estimated structure that entails potential links between any node pair, which could be prohibitive for large graphs. To reduce space and time complexity, we adopt a pivot-based structure learning method, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Concretely, we randomly choose ? nodes in the graph as pivots, where ? is a hyperparameter much smaller than ? (e.g., ? ? 1 10 ? ). We then leverage pivot nodes as intermediates and convert the ? ? ? graph ?, which can be prohibitively large with dense edges, into a cascade of one ? ? ? node-pivot bipartite graph B1 and one ? ? ? pivot-node bipartite graph B2 = B? 1 , which can effectively control the computational cost with proper ?. In this way, we can compute a node-pivot similarity matrix ? = {? ?? } ? ?? based on <ref type="bibr" target="#b8">(9)</ref>, to parameterize the distribution of latent graph structures. This only requires ? (? ?) time and space complexity, and one can sample from each ????????? (? ?? ) to obtain B1 and B2 . In the meanwhile, the original ? ?? adjacency matrix could be retrieved by ? = B1 B2 , which suggests that one can execute message passing on B1 and B2 to approximate that on ? (see more details in Section 3.2.2). In terms of the acceleration of structure learning, other strategies like the all-pair message passing schemes with linear complexity explored by <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref> can also be utilized to achieve the purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Instantiation for ? ? ? (Y|A, X, ?). The predictive distribution, parameterized by the GNN network ? ? ? , aims at recursively propagating features along the latent graph to update node representations and producing the prediction result for each node. We then present the details of GNN's message passing on the latent graph in order for enough expressiveness, stability and efficiency.</p><p>To begin with, we review the message-passing rule in common GNN models, like GCN <ref type="bibr" target="#b17">[18]</ref>, that operates on the original graph A: to ? (? ?). We choose ? nodes as pivots and convert the ? ? ? matrix to the product of two ? ?? node-pivot matrices (where the message passing is executed with two steps, i.e., node-topivot and pivot-to-node.).</p><formula xml:id="formula_12">Z (?+1) = ? MP 1 (Z (? ) , A)W (? ) = ? D -1 2 AD -1 2 Z (? ) W (? ) ,<label>(10)</label></formula><p>where W (? ) ? R ? ?? is a weight matrix, ? is non-linear activation, and D denotes a diagonal degree matrix from input graph A and</p><formula xml:id="formula_13">Z (? ) = {z (? )</formula><p>? } ? ?? is a stack of node representations at the ?-th layer. With the estimated latent graph ? = B1 B2 , we perform message passing MP 2 (?) in a two-step fashion to update node representations: i) node-to-pivot passing:</p><formula xml:id="formula_14">C (?+ 1 2 ) = RowNorm(? ? )Z (? ) ,</formula><p>ii) pivot-to-node passing:</p><formula xml:id="formula_15">C (?+1) = RowNorm(?)C (?+ 1 2 ) ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_16">C (?+ 1 2</formula><p>) is an intermediate node representation and ? = {? ?? } ? ?? is the node-pivot similarity matrix calculated by <ref type="bibr" target="#b8">(9)</ref>. Such a two-step procedure can be efficiently conducted within ? (? ?) time and space complexity.</p><p>Despite that the feature propagation on the estimated latent structure could presumably yield better node representations, the original input graph structures also contain useful information, such as effective inductive bias <ref type="bibr" target="#b4">[5]</ref>. Therefore, we integrate two message-passing functions to compute layer-wise updating for node representations:</p><formula xml:id="formula_17">Z (?+1) = ? ?MP 1 (Z (? ) , A)W (? ) + (1 -?)MP 2 (Z (? ) , ?)W (? ) , (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where ? is a trading hyper-parameter that controls the concentration weight on input structures. Such design could also improve the training stability by reducing the impact from large variation of latent structures through training procedure.</p><p>With ? GNN layers, one can obtain the prediction ? by setting ? = Z (?) and W (?-1) ? R ? ?? where ? is the number of classes. Alg. 1 shows the feed-forward computation of message passing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.3</head><p>Instantiation for ? 0 ( ?|A, X). The prior distribution reflects how we presume on the latent graph structures without the information of observed labels. In other words, it characterizes how likely a given graph structure could provide enough potential for feature propagation by GNNs. The prior could be leveraged for regularization on the estimated latent graph ?. In this consideration, we choose the prior as an energy function that quantifies the smoothness of the graph:</p><formula xml:id="formula_19">? 0 ( ?|X, A) ? exp -? ?? ?,? ??? ?x ? -x ? ? 2 2 -? ? ?? 2 ? ,<label>(13)</label></formula><p>where ? ? ? ? is the Frobenius norm. The first term in (13) measures the smoothness of the latent graph <ref type="bibr" target="#b1">[2]</ref>, with the hypothesis that graphs with smoother feature has lower energy (i.e., higher probability). The second term helps avoiding too large node degrees <ref type="bibr" target="#b16">[17]</ref>. The hyperparameters ? and ? control the strength for regularization effects.</p><p>While we can retrieve the latent graph via ? = B1 B2 , the computation of (13) still requires ? (? 2 ) cost. To reduce the overhead, we apply the regularization on the ? ? ? pivot-pivot adjacency matrix ? = B2 B1 as a proxy regularization:</p><formula xml:id="formula_20">R ( ?) = log ? 0 ( ?|X, A) ? -? ?? ?,? ??? ?x ? ? -x ? ? ? 2 2 -? ? ?? 2 ? ,<label>(14)</label></formula><p>where x ? ? denotes the input feature of the ?-th pivot node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>For optimization with <ref type="bibr" target="#b6">(7)</ref>, we proceed to derive the loss functions and updating gradients for ? and ? ? based on the three terms</p><formula xml:id="formula_21">E ? ? [log ? ? ? ], E ? ? [log ? 0 ] and E ? ? [log ? ? ]. 3.3.1 Optimization for E ? ? [log ? ? ? ].</formula><p>The optimization difficulty stems from the expectation over ? ? , where the sampling process is non-differentiable and hinders back-propagation. Common strategies for approximating the sampling for discrete random variables include Gumbel-Softmax trick <ref type="bibr" target="#b13">[14]</ref> and REINFORCE trick <ref type="bibr" target="#b34">[35]</ref>. However, both strategies yield a sparse graph structure each time of sampling, which could lead to high variance for the prediction result log ? ? ? (Y|A, X, ?) produced by message passing over a sampled graph. To mitigate the issue, we alternatively adopt the Normalized Weighted Geometric Mean (NWGM) <ref type="bibr" target="#b42">[43]</ref> to move the outer expectation into the feature-level. Specifically, we have (see Appendix A for detailed derivations)</p><formula xml:id="formula_22">? ? E ? ? ( ?|A,X) [log ? ? ? (Y|A, X, ?)] ?? ? log ? ? ? (Y|A, X, ? = E ? ? ( ?|A,X) [ ?]).<label>(15)</label></formula><p>We denote the opposite of the above term as ? ? L ? (? ). The gradient w.r.t. ? ? can be similarly derived. The above form is a biased estimation for the original objective, yet it can reduce the variance from sampling and also improve training efficiency (without the need of message passing over multiple sampled graphs).( <ref type="formula" target="#formula_22">15</ref>) induces the supervised cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Optimization for</head><formula xml:id="formula_23">E ? ? [log ? 0 ].</formula><p>As for the second term in <ref type="bibr" target="#b6">(7)</ref>, we adopt the REINFORCE trick, i.e., policy gradient, to tackle the non-differentiability of sampling from ? ? . Specifically, for each feedforward computation, we sample from the Bernoulli distribution for each edge given by the estimated node-pivot similarity matrix, i.e., ????????? (? ?? ), and obtain the sampled latent bipartite graph B1 and subsequently have ? = B1 B2 = B1 B? 1 . The probability for the latent structure could be computed by</p><formula xml:id="formula_24">? ? ( ?) = ?,? B1,?? ? ?? + (1 -B1,?? ) ? (1 -? ?? ) . (<label>16</label></formula><formula xml:id="formula_25">)</formula><p>Denote ?? as the sampled result at the ?-th time, we can independently sample ? times and obtain { ?? } ? ?=1 and {? ? ( ?? )} ? ?=1 . Recall that the regularization reward from log ? 0 has been given by <ref type="bibr" target="#b13">(14)</ref>. The policy gradient <ref type="bibr" target="#b34">[35]</ref> yields the gradient of loss for ? as</p><formula xml:id="formula_26">? ? L ? (? ) = -? ? E ??? ( ?|X,A) [log ? 0 ( ?|X, A)] ? -? ? 1 ? ? ?? ?=1 log ? ? ( ?? ) R ( ?? ) -R ,<label>(17)</label></formula><p>where R acts as a baseline function by averaging the regularization rewards R ( ?? ) in one feed-forward computation, which helps to reduce the variance during policy gradient training <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Optimization with</head><formula xml:id="formula_27">E ? ? [log ? ? ].</formula><p>The last entropy term for ? ? could be directly computed by</p><formula xml:id="formula_28">L ? (? ) = E ??? ( ?|X,A) [log ?( ?|X, A)] ? 1 ? ? ? ?? ?=1 ? ?? ?=1 ? ?? log ? ?? + (1 -? ?? ) log(1 -? ?? ) ,<label>(18)</label></formula><p>where we again adopt the node-pivot similarity matrix as a proxy for the estimated latent graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Iterative Structure Learning for Acceleration.</head><p>A straightforward way is to consider once structure inference and once GNN's message passing for prediction in each feed-forward computation. To enable structure learning and GNN learning mutually reinforce each other <ref type="bibr" target="#b5">[6]</ref>, we consider multiple iterative updates of graph structures and node representations before once back-propagation. More specifically, in each epoch, we repeatedly update node representations Z ? (where the superscript ? denotes the ?-th iteration) and latent graph ?? until a given maximum budget is achieved. To accelerate the training, we aggregate the losses L ? in each iteration step for parameter updating. As different graphs have different feature space, we utilize the first layer of GNN as an encoder at the very beginning and then feed the encoded representations to structure learner. The training algorithm for structure learner ? ? on source graphs is described in Alg. 2 (in the appendix) where we train structure learner for multiple episodes and in each episode, we train ? ? on each source graph for several epochs. In testing, the well-trained ? ? is fixed and we train a GNN ? ? on the target graph with latent structures inferred by ? ? , as described in Alg. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORKS</head><p>Graph Neural Networks. Graph neural networks (GNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref> have achieved impressive performances in modeling graph-structured data. Nonetheless, there is increasing evidence suggesting GNNs' deficiency for graph structures that are inconsistent with the principle of message passing. One typical situation lies in non-homophilous graphs <ref type="bibr" target="#b52">[53]</ref>, where adjacent nodes tend to have dissimilar features/labels. Recent studies devise adaptive feature propagation/aggregation to tackle the heterophily <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. Another situation stems from graphs with noisy or spurious links, for which several works propose to purify the observed structures for more robust node representations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51]</ref>. Our work is related to these works by searching adaptive graph structures that is suitable for GNN's message passing. Yet, the key difference is that our method targets learning a new graph out of the scope of input one, while the above works focus on message passing within the input graph.</p><p>Graph Structure learning. To effectively address the limitations of GNNs' feature propagation within observed structures, many recent works attempt to jointly learn graph structures and the GNN model. For instance, <ref type="bibr" target="#b9">[10]</ref> models each edge as a Bernoulli random variable and optimizes graph structures along with the GCN. To exploit enough information from observed structure for structure learning, <ref type="bibr" target="#b20">[21]</ref> proposes a metric learning approach based on RBF kernel to compute edge probability with node representations, while <ref type="bibr" target="#b14">[15]</ref> adopts attention mechanism to achieve the similar goal. Furthermore, <ref type="bibr" target="#b5">[6]</ref> considers an iterative method that enables mutual reinforcement between learning graph structures and node embeddings. Also, <ref type="bibr" target="#b49">[50]</ref> presents a probabilistic framework that views the input graph as a random sample from a collection modeled by a parametric random graph model. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref> harnesses variational inference to estimate a posterior of graph structures and GNN parameters. While learning graph structures often requires ? (? 2 ) complexity, a recent work <ref type="bibr" target="#b40">[41]</ref> proposes an efficient Transformer that achieves latent structure learning in each layer with ? (? ) complexity. However, though these methods have shown promising results, they assume training nodes and testing nodes are from the same graph and consider only one graph. By contrast, we consider graph structure learning under the cross-graph setting and propose a general framework to learn a shared structure learner which can generalize to target graphs without any re-training.</p><p>Out-of-Distribution Generalization on Graphs. Due to the demand for handling testing data in the wild, improving the capability of the neural networks for performing satisfactorily on out-ofdistribution data has received increasing attention <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref>. Recent studies, e.g., <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54]</ref> explore effective treatments for tackling general distribution shifts on graphs, and there are also works focusing on particular categories of distribution shifts like size generalization <ref type="bibr" target="#b2">[3]</ref>, molecular scaffold generalization <ref type="bibr" target="#b46">[47]</ref>, feature/attribute shifts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>, topological shifts <ref type="bibr" target="#b45">[46]</ref>, etc. To the best of our knowledge, there is no prior works considering OOD generalization in the context of graph structure learning. In our case, the target graph, where the structure learner is expected to yield adaptive structures, can have disparate distributions than the source graphs. The distribution shifts could potentially stem from feature/label space, graph sizes or domains (e.g., from social networks to citation networks). As the first attempt along this path, our work can fill the research gap and enable the graph structure learning model to deal with new unseen graphs in an open world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We apply GraphGLOW to real-world datasets for node classification to test the efficacy of proposed structure learner for boosting performance of GNN learning on target graphs with distribution shifts from source graphs. We specify the backbone GNN network for GraphGLOW as a two-layer GCN <ref type="bibr" target="#b17">[18]</ref>. We focus on the following research questions: ? 1) How does GraphGLOW perform compared with directly training GNN models on input structure of target graphs? 66.8 ? 1.4 83.5 ? 0.6 73.9 ? 0.7 79.9 ? 0.5</p><p>? 2) How does GraphGLOW perform compared to state-of-the-art structure learning models that are directly trained on target datasets in terms of both accuracy and training time?</p><p>? 3) Are the proposed components of GraphGLOW effective and necessary for the achieved performance?</p><p>? 4) What is the impact of hyper-parameter on performance and what is the impact of attack on observed edges?</p><p>? 5) What is the property of inferred latent graphs and what generalizable pattern does the structure learner capture?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Protocols</head><p>Datasets. Our experiments are conducted on several public graph datasets. First we consider three commonly used citation networks Cora, CiteSeer and PubMed. We use the same splits as in <ref type="bibr" target="#b48">[49]</ref>. These three datasets have high homophily ratios (i.e., adjacent nodes tend to have similar labels) <ref type="bibr" target="#b23">[24]</ref>. Apart from this, we also consider four social networks from Facebook-100 <ref type="bibr" target="#b31">[32]</ref>, which have low homophily ratios. Readers may refer to Appendix B for more dataset information like splitting ratios.</p><p>Competitors. We mainly compare with GCN <ref type="bibr" target="#b17">[18]</ref>, the GNN counterpart trained on input structure, for testing the efficacy of produced latent graphs by GraphGLOW. As further investigation, we also compare with other advanced GNN models: GraphSAGE <ref type="bibr" target="#b12">[13]</ref>, GAT <ref type="bibr" target="#b33">[34]</ref>, APPNP <ref type="bibr" target="#b18">[19]</ref>, H 2 GCN <ref type="bibr" target="#b52">[53]</ref> and GPRGNN <ref type="bibr" target="#b6">[7]</ref>. Here APPNP, H 2 GCN and GPRGNN are all strong GNN models equipped with adaptive feature propagation and high-order aggregation. For these pure GNN models, the training and testing are considered on (the same) target graphs. Furthermore, we compete GraphGLOW with state-of-the-art graph structure learning models, IDS <ref type="bibr" target="#b9">[10]</ref>, IDGL <ref type="bibr" target="#b5">[6]</ref> and VGCN <ref type="bibr" target="#b7">[8]</ref>. Since these models are all designed for training on one dataset from scratch, we directly train them on the target graph and they in principle could yield better performance than GraphGLOW.</p><p>We also consider variants of GraphGLOW as baselines. We replace the similarity function ? with attention-based structure learner, denoted as GraphGLOW at , which follows the same training scheme as GraphGLOW. Besides, we consider some non-parametric similarity functions like dot-product, KNN and cosine distance (denoted as GraphGLOW dp , GraphGLOW knn and GraphGLOW cos , respectively). For these models, we only need to train the GNN network on target graphs with the non-parametric structure learners yielding latent structures. In addition, we introduce a variant GraphGLOW* that shares the same architecture as GraphGLOW and is directly trained on target graphs. Also, GraphGLOW* in principle could produce superior results than GraphGLOW. We report the test accuracy given by the model that produces the highest validation accuracy within 500 training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">In-domain Generalization</head><p>We first consider transferring within social networks or citation networks. The results are reported in Table <ref type="table" target="#tab_1">1</ref> where for each social network (resp. citation network) as the target, we use the other social networks (resp. citation networks) as the source datasets. GraphGLOW performs consistently better than GCN, i.e., the counterpart using observed graph for message passing, which proves that GraphGLOW can capture generalizable patterns for desirable message-passing structure for unseen datasets that can indeed boost the GCN backbone's performance on downstream tasks. In particular, the improvement over GCN is over 5% on Cornell5 and Reed98, two datasets with low homophily ratios (as shown in Table <ref type="table" target="#tab_7">3</ref>). The reason is that for non-homophilous graphs where the message passing may propagate inconsistent signals (as mentioned in Section 1), the GNN learning could better benefits from structure learning than homophilous graphs. Furthermore, compared to other strong GNN models, GraphGLOW still achieves slight improvement than the best competitors though the backbone GCN network is less expressive. One could expect further performance gain by GraphGLOW if we specify the GNN backbone as other advanced architectures.</p><p>In contrast with non-parametric structure learning models and GraphGLOW at , GraphGLOW outperforms them by a large margin throughout all cases, which verifies the superiority of our design of multi-head weighted similarity function that can accommodate multi-faceted diverse structural information. Compared with Graph-GLOW*, GraphGLOW performs on par with and even exceeds it on    <ref type="bibr" target="#b9">[10]</ref>, IDGL <ref type="bibr" target="#b5">[6]</ref> and VGCN <ref type="bibr" target="#b7">[8]</ref>). The radius of circle is proportional to standard deviation. The experiments are run on one Tesla V4 with 16 GPU memory. We adopt the same setting as Table <ref type="table" target="#tab_2">2</ref> and report the results on target datasets. For Cornell5 and PubMed, the competitor models suffer out-of-memory.</p><p>Cornell5 and Amherst41. The possible reasons are two-fold. First, there exist sufficient shared patterns among citation networks (resp. social networks), which paves the way for successful generalization of GraphGLOW. Second, GraphGLOW* could sometimes overfit specific datasets, since the amount of free parameters are regularly orders-of-magnitude more than the number of labeled nodes in the dataset. The results also imply that our transfer learning approach can help to mitigate over-fitting on one dataset. Moreover, Graph-GLOW can generalize structure learner to unseen graphs that is nearly three times larger than training graphs, i.e., Cornell5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cross-domain Generalization</head><p>We next consider a more difficult task, transferring between social networks and citation networks. The difficulty stems from two aspects: 1) social networks and citations graphs are from distinct categories thus have larger underlying data-generating distribution gaps; 2) they have varied homophily ratios, which indicates that the observed edges play different roles in original graphs. In Table <ref type="table" target="#tab_2">2</ref> we report the results. Despite the task difficulty, GraphGLOW manages to achieve superior results than GCN and also outperforms other non-parametric graph structure learning methods throughout all cases. This suggests GraphGLOW's ability for handling target graphs with distinct properties.</p><p>In Fig. <ref type="figure" target="#fig_4">4</ref> we further compare GraphGLOW with three state-ofthe-art graph structure learning models that are directly trained on target graphs. Here we follow the setting in Table <ref type="table" target="#tab_2">2</ref>. The results show that even trained on source graphs that are different from the target one, GraphGLOW still performs on par with the competitors that are trained and tested on (the same) target graphs. Notably, GraphGLOW significantly reduces training time. For instance, in John Hopkins55, GraphGLOW is 6x, 9x and 40x faster than IDGL, LDS and VGCN, respectively. This shows one clear advantage of GraphGLOW in terms of training efficiency and also verifies that our model indeed helps to reduce the significant cost of training time for structure learning on target graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>We conduct ablation studies to test the effectiveness of iterative learning scheme and regularization on graphs.</p><p>Effect of Iterative Learning. We replace the iterative learning process as a one-step prediction (i.e., once structure estimation and updating node representations in once feed-forward computation) and compare its test accuracy with GraphGLOW. The results are shown in Fig. <ref type="figure">5</ref>(a) where we follow the setting of Table   and Amherst41, respectively). Therefore, the iterative updates indeed help to learn better graph structures and node embeddings, contributing to higher accuracy for downstream prediction.</p><p>Effect of Regularization on structures. We remove the regularization on structures (i.e., setting ? = ? = 0) and compare with GraphGLOW. As shown in Fig. <ref type="figure">5</ref>(a), there is more or loss performance degradation. In fact, the regularization loss derived from the prior distribution for latent structures could help to provide some guidance for structure learning, especially when labeled information is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyper-parameter Sensitivity</head><p>In Fig. <ref type="figure" target="#fig_7">7</ref> (in the appendix), we study the variation of model's performance w.r.t. ? (the weight on input graphs) and ? (the number of pivots) on target datasets Cora and CiteSeer. Overall, the model is not sensitive to ?'s. For Cora, larger ? contributes to higher accuracy, while for CiteSeer, smaller ? yields better performance. The possible reason is that the initial graph of Cora is more suitable for message passing (due to higher homophily ratio). For the impact of pivot number, as shown in Fig. <ref type="figure" target="#fig_7">7</ref>(b), a moderate value of ? could provide decent downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Robustness Analysis</head><p>In addition, we find that GraphGLOW is more immune to edge deletion attack than GCN. We randomly remove 10-50% edges of target graphs respectively, and then apply GraphGLOW and GCN. We present the results in Johns Hopkins55 in Fig. <ref type="figure">5(b</ref>) and leave more results in Appendix D. When the drop ratio increases, the performance gap between two models becomes more significant. This is due to our structure learner's ability for learning new graph structures from node embeddings, making it less reliant on initial graph structures and more robust to attack on input edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Case Study</head><p>We further probe into why our approach is effective for node classification by dissecting the learnt graph structures. Specifically, we measure the homophily ratios of learnt structures and their variance of neighborhood distributions of nodes with same labels. As nodes receive messages from neighbors in message passing, the more similar the neighborhood patterns of nodes within one class are, the easier it is for GNNs to correctly classify them <ref type="bibr" target="#b25">[26]</ref>. We use homophily metric proposed in <ref type="bibr" target="#b23">[24]</ref> to measure homophily ratios. For calculation of variance of neighborhood distribution, we first calculate variance for each class, and then take weighted sum to get the final variance, where the weight is proportional to the number of nodes within corresponding class.</p><p>Homophily Ratio. We choose Amherst41, Johns Hopkins55 and Reed98 as target graphs, and record the homophily ratios of inferred latent structures every five epochs during training. As shown in Fig. <ref type="figure">6(a)</ref>. the homophily ratios of inferred latent graphs exhibit a clear increase as the training epochs become more and the final ratio is considerably larger than that of input graph. The results indicate that the trained structure learner incline to output more homophilous latent structures that are reckoned to be more suitable for message passing.</p><p>Neighborhood Distribution Variance. As shown in Fig. <ref type="figure">6</ref>(b), the variance of neighborhood distribution of nodes with the same label is significantly smaller in our learnt structure, making it easier to classify nodes through message passing. The results also imply that high homophily ratio and similar intra-class neighborhood patterns could be two of the underlying transferrable patterns of optimal message-passing structure, identified by GraphGLOW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper proposes Graph Structure Learning Under Cross-Graph Distribution Shift, a new problem that requires structure learner to transfer to new target graphs without re-training and handles distribution shift. We develop a transfer learning framework that guides the structure learner to discover shared knowledge across source datasets with respect to optimal message-passing structure for boosting downstream performance. We also carefully design the model components and training approach in terms of expressiveness, scalability and stability. We devise experiments with various difficulties and demonstrate the efficacy and robustness of our approach. Although our framework is pretty general, we believe their are other potential methods that can lead to equally competitive results, which we leave as future work.</p><p>Algorithm 1: Message Passing over Latent Graphs [Z, ?] = GNN(A, X, ?; ?)</p><p>Input: node features X, input adjacency A, latent graph node-pivot similarity matrix ? = {? ?? } ? ?? . Z (0) ? X; </p><formula xml:id="formula_29">for ? = 0, 1, ? ? ? , ? -1 do Z (?+1) ? D -1 2 AD -1 2 Z (? ) W (? ) ; C (?+ 1 2 ) = RowNorm(? ? )Z (? ) ; C (?+1) = RowNorm(?)C (?+ 1 2 ) ; Z (?+1) ? ? (?Z (?+1) + (1 -?)C (?+1) W (? ) ; end Output: node representations Z = Z (?-</formula><formula xml:id="formula_30">[Z ? , ?? ] = GNN(A, X, ? ? ; ? ? ); Compute ? ? L ? , ? ? L ? , ? ? L ? using (15),<label>(17)</label></formula><p>, <ref type="bibr" target="#b17">(18)</ref>, respectively ; </p><formula xml:id="formula_31">? (? ) ? ? ? ? L ? + ? ? L ? + ? ? L ? ; ? (? ) ? ? ? ? ? L ? ; end ? ? ? ? (0) ? + ? ?=2 ? (? ) ? /(? -1); ? ? ? ? (0) ? + ? ?=2 ?<label>(</label></formula><formula xml:id="formula_32">do ? ? 0, ? ? ? 0, Z (0) ? X ? , A ? A ? ; Z 0 = MLP(X; ?) or Z 0 = GCN(A, X; ?); while not converged do ? ? ? + 1; Compute ? ? = {? ?? } using (9) with Z ? -1 ; Sample ? times over ? ? to obtain { ?? ? } ? ?=1 ; Compute {? ? ( ?? ? )} ? ?=1 using (16); [Z, ?] = GNN(A, X, ? ? ; ?); Compute ? ? L ? using (15) ; ? (? ) ? ? ? ? ? L ? ; end ? ? ? ? (0) ? + ? ?=2 ? (? )</formula><p>? /(? -1); Use gradient ? ? to update ?; end Output: trained GNN network ? ? * for the target graph. rule. We then adopt Normalized Weighted Geometric Mean (NWGM) <ref type="bibr" target="#b42">[43]</ref>:</p><formula xml:id="formula_33">? ? log E ? ? ( ?|A,X) [? ? ? (Y ?,? = 1|A, X, ?)] =? ? log ?? ? exp(? 1 ( ?)) exp(? 1 ( ?)) + exp(? 2 ( ?)) ? ? ( ?|A, X) =? ? log ?? ? Softmax(? 1 ( ?))? ? ( ?|A, X) ?? ? log NWGM(Softmax(? 1 ( ?))) =? ? log ?? ? ? [exp(? 1 ( ?))] ? ? ( ?|A,X) ? exp(? 1 ( ?))] ? ? ( ?|A,X) + ? exp(? 2 ( ?))] ? ? ( ?|A,X) =? ? log ?? ? exp( ? ? 1 ( ?)? ? ( ?|A, X)) exp( ? ? 1 ( ?)? ? ( ?|A, X)) + exp( ? ? 2 ( ?)? ? ( ?|A, X)) =? ? log ?? ? exp(E ? ? ( ?|A,X) [? 1 ( ?)]) exp(E ? ? ( ?|A,X) [? 1 ( ?)]) + exp(E ? ? ( ?|A,X) [? 2 ( ?)]) =? ? log ? ? ? (Y ?,? = 1|A, X, ? = E ? ? ( ?|A,X) [ ?]),<label>(19)</label></formula><p>where ? 1 denotes the positive predicted score for class ? which is indeed associated with node ?, and ? 2 ( ?) = 0 in our case. We thus conclude the proof for <ref type="bibr" target="#b14">(15)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATASETS AND EXPERIMENTAL DETAILS</head><p>The statistical information on datasets is displayed in Table <ref type="table" target="#tab_7">3</ref>. For the splitting of Cora, CiteSeer and PubMed, we follow <ref type="bibr" target="#b48">[49]</ref> to randomly select 20 instances per class for training, 500/1000 instances for validation/testing in each dataset. In the remaining datasets, we employ random train/valid/test splits of 50%/25%/25%. The backbone GNN network is specified as a two-layer GCN model. We set the similarity function ? in (9) as cosine similarity and ? as a threshold-based truncation. Besides, since the dimensions of input node features are different across datasets, we adopt a transformation network that converts input features into a ?-dimensional node representations before the structure learning module as shown in Alg. 2 (Z 0 = MLP(X; ? ? ) or Z 0 = GCN(A, X; ? ? )). We can specify the transformation as a one-layer MLP or a one-layer GCN network (what we adopt). Most of the experiments were conducted on an NVIDIA GeForce RTX 2080 Ti with 11GB memory. For experiments involving two larger datasets, PubMed and Cornell5, we utilized an NVIDIA GeForce RTX 3090 with 24 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C HYPERPARAMETERS</head><p>We use grid search on validation set to tune the hyperparameters. The learning rate is searched in {0.001, 0.005, 0.01, 0.05}; Dropout is searched in {0, 0.2, 0.3, 0.5, 0.6}; Hidden channels is searched in {16, 32, 64, 96}. Other hyperparameters for specific models are stated below.</p><p>For GCN, GraphSAGE and H 2 GCN, we use 2 layers. For GAT, we search gat head number in {2, 4} and use 2 layers. For APPNP and GPR, we search ? in {0.1, 0.2, 0.5} and set ? to 10. We list the searching space of structure learning methods below.</p><p>? GraphGLOW and its variants: pivot number ? ? {800, 1000, 1200, 1400}, embedding size ? ? {16, 32, 64, 96}, ? ? [0.1, 0.9], ? ? {0, 0.1, 0.15, 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MORE EXPERIMENTAL RESULTS</head><p>We compare with using MLP and GCN, respectively, as the transformation network before structure learning module and report the results in Table <ref type="table" target="#tab_9">4</ref>. In summary, these two methods are of equal    competence, which suggests that GraphGLOW is not sensitive to the transformation network used for converting node features with various dimensions into embeddings with a shared dimension. This also implies that simple neural architectures, e.g. MLP and GCN, could provide enough capacity for extracting the information in input observation, which is leveraged by the shared graph structure learner to discover generalizable patterns in optimal messagepassing structure. We also provide more results of edge deletion experiments in Fig. <ref type="figure" target="#fig_8">8</ref>. We randomly remove 10-50% edges of target graphs respectively, and then apply GraphGLOW and GCN. The results demonstrate that GraphGLOW is more immune to edge deletion. This is due to our structure learner's ability for learning new structures, making it less reliant on initial graph structures and more robust to attack on input edges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of Open-World Graph Structure Learning. In a diverse set of source graphs, we train multiple dataset-specific GNNs and a shared structure learner. In the target graph, we directly utilize the learned structure learner and only need to train a new GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the proposed framework GraphGLOW targeting open-world graph structure learning. The middle part of the figure presents the training process for the structure learner together with multiple dataset-specific GNNs on source graphs. In (a)-(e) we illustrate the details of graph structure learner, backbone GNN, iterative training process, training procedure and transferring procedure. When the training is finished, the structure learner is fixed and we only need to train a dataset-specific GNN network on new target graph with latent structures inferred by the well-trained structure learner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration for scalable structure learning message passing, which reduces algorithmic complexity from ? (? 2 )to ? (? ?). We choose ? nodes as pivots and convert the ? ? ? matrix to the product of two ? ?? node-pivot matrices (where the message passing is executed with two steps, i.e., node-topivot and pivot-to-node.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of test accuracy and training time with SOTA structure learning models (LDS<ref type="bibr" target="#b9">[10]</ref>, IDGL<ref type="bibr" target="#b5">[6]</ref> and VGCN<ref type="bibr" target="#b7">[8]</ref>). The radius of circle is proportional to standard deviation. The experiments are run on one Tesla V4 with 16 GPU memory. We adopt the same setting as Table2and report the results on target datasets. For Cornell5 and PubMed, the competitor models suffer out-of-memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: (a) Ablation study for GraphGLOW. (b) Performance comparison of GraphGLOW and GCN w.r.t. randomly removing certain ratios of edges in Johns Hopkins55.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>2, 0.25, 0.3}, ? ? {0, 0.1, 0.15, 0.2, 0.25, 0.3}, threshold ? {4e-5, 8.5e-5}, ? ? {4, 6}, ? = 10, ? ? {1, 2, 3}. ? LDS: the sampling time ? = 16, the patience window size ? ?{10, 20}, the hidden size ? {8, 16, 32, 64}, the inner learning rate ? ? {1e-4, 1e-3, 1e-2, 1e-1}, and the number of updates used to compute the truncated hypergradient ? ? {5, 10, 15}. ? IDGL: ? = 0.01, hidden size ? {16, 64, 96,128}, ? ? {0.5, 0.6, 0.7, 0.8}, ? ? {0, 0.1, 0.2}, ? ? {0, 0.1, 0.2}, ? ? {0, 0.1}, ? ? {0.1, 0.2}, ? ? {6, 9, 12}. ? VGCN: ?1 ? {0.25, 0.5, 0.75, 0.99}, ?0 = 10 -5 , ? 0 ? {0.1, 0.5}, ? ? {0.1, 0.5}, ? ? {10 -4 , 10 -3 , 10 -2 , 1}. Sampling time is 3. Maximum number of training epochs is 5000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Hyper-parameter sensitivity analysis on the weight of input graphs ? and pivot number ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance comparison of GraphGLOW and GCN w.r.t. randomly removing certain ratios of edges in input graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) on target graphs for in-domain generalizations. For each social network (resp. citation network) as target dataset, we consider the other social networks (resp. citation networks) as source graphs. GraphGLOW* is an oracle model that shares the same architecture as our model GraphGLOW and is directly trained on target graphs.</figDesc><table><row><cell>Type</cell><cell>Method</cell><cell cols="2">Cornell5 Johns.55 Amherst41</cell><cell>Reed98</cell><cell>Cora</cell><cell>CiteSeer PubMed</cell></row><row><cell></cell><cell>GCN</cell><cell>68.6 ? 0.5 70.8 ? 1.0</cell><cell>65.8 ? 1.6</cell><cell cols="3">60.8 ? 1.6 81.6 ? 0.4 71.6 ? 0.3 78.8 ? 0.6</cell></row><row><cell></cell><cell>SAGE</cell><cell>68.7 ? 0.8 67.5 ? 0.9</cell><cell>66.3 ? 1.8</cell><cell cols="3">63.9 ? 1.9 81.4 ? 0.6 71.6 ? 0.5 78.6 ? 0.7</cell></row><row><cell></cell><cell>GAT</cell><cell>69.6 ? 1.2 69.4 ? 0.7</cell><cell>68.7 ? 2.1</cell><cell cols="3">64.5 ? 2.5 83.0 ? 0.7 72.1 ? 1.1 79.0 ? 0.4</cell></row><row><cell>Pure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>GPR</cell><cell>68.8 ? 0.7 69.6 ? 1.3</cell><cell>66.2 ? 1.5</cell><cell cols="3">62.7 ? 2.0 83.1 ? 0.7 72.4 ? 0.8 79.6 ? 0.5</cell></row><row><cell>GNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>APPNP</cell><cell>68.5 ? 0.8 69.1 ? 1.4</cell><cell>65.9 ? 1.3</cell><cell cols="3">62.3 ? 1.5 82.7 ? 0.5 71.9 ? 0.5 79.2 ? 0.3</cell></row><row><cell></cell><cell>H 2 GCN</cell><cell>71.4 ? 0.5 68.3 ? 1.0</cell><cell>66.5 ? 2.2</cell><cell cols="3">65.4 ? 1.3 82.5 ? 0.8 71.4 ? 0.7 79.4 ? 0.4</cell></row><row><cell></cell><cell>CPGNN</cell><cell>71.1 ? 0.5 68.7 ? 1.3</cell><cell>66.7 ? 0.8</cell><cell cols="3">63.6 ? 1.8 80.8 ? 0.4 71.6 ? 0.4 78.5 ? 0.7</cell></row><row><cell></cell><cell>GraphGLOW dp</cell><cell>71.5 ? 0.7 71.3 ? 1.2</cell><cell>68.5 ? 1.6</cell><cell cols="3">63.2 ? 1.2 83.1 ? 0.8 71.7 ? 1.0 77.3 ? 0.8</cell></row><row><cell>Graph</cell><cell cols="2">GraphGLOW knn 69.4 ? 0.8 71.0 ? 1.3</cell><cell>64.8 ? 1.2</cell><cell cols="3">63.6 ? 1.6 81.7 ? 0.8 71.5 ? 0.8 79.4 ? 0.6</cell></row><row><cell>Structure</cell><cell>GraphGLOW cos</cell><cell>69.9 ? 0.7 70.8 ? 1.4</cell><cell>65.2 ? 1.8</cell><cell cols="3">62.7 ? 1.3 82.0 ? 0.7 71.9 ? 0.9 78.7 ? 0.8</cell></row><row><cell>Learning</cell><cell>GraphGLOW at</cell><cell>69.3 ? 0.8 70.9 ? 1.3</cell><cell>65.0 ? 1.3</cell><cell cols="3">65.0 ? 1.7 81.9 ? 0.9 71.3 ? 0.7 78.8 ? 0.6</cell></row><row><cell></cell><cell>GraphGLOW</cell><cell>71.8 ? 0.9 71.5 ? 0.8</cell><cell>70.6 ? 1.4</cell><cell cols="3">66.8 ? 1.1 83.5 ? 0.6 73.6 ? 0.6 79.8 ? 0.8</cell></row><row><cell></cell><cell>GraphGLOW*</cell><cell>71.1 ? 0.3 72.2 ? 0.5</cell><cell>70.3 ? 0.9</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) on target graphs for cross-domain generalizations. For each social network (resp. citation network) as target dataset, we consider citation networks (resp. social networks) as source graphs.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Type</cell><cell></cell><cell>Method</cell><cell>Cornell5 Johns.55 Amherst41</cell><cell>Reed98</cell><cell>Cora</cell><cell>CiteSeer PubMed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GCN</cell><cell>68.6 ? 0.5 70.8 ? 1.0</cell><cell>65.8 ? 1.6</cell><cell>60.8 ? 1.6 81.6 ? 0.4 71.6 ? 0.3 78.8 ? 0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SAGE</cell><cell>68.7 ? 0.8 67.5 ? 0.9</cell><cell>66.3 ? 1.8</cell><cell>63.9 ? 1.9 81.4 ? 0.6 71.6 ? 0.5 78.6 ? 0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GAT</cell><cell>69.6 ? 1.2 69.4 ? 0.7</cell><cell>68.7 ? 2.1</cell><cell>64.5 ? 2.5 83.0 ? 0.7 72.1 ? 1.1 79.0 ? 0.4</cell></row><row><cell></cell><cell></cell><cell cols="2">Pure</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GPR</cell><cell>68.8 ? 0.7 69.6 ? 1.3</cell><cell>66.2 ? 1.5</cell><cell>62.7 ? 2.0 83.1 ? 0.7 72.4 ? 0.8 79.6 ? 0.5</cell></row><row><cell></cell><cell></cell><cell cols="2">GNN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>APPNP</cell><cell>68.5 ? 0.8 69.1 ? 1.4</cell><cell>65.9 ? 1.3</cell><cell>62.3 ? 1.5 82.7 ? 0.5 71.9 ? 0.5 79.2 ? 0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H 2 GCN</cell><cell>71.4 ? 0.5 68.3 ? 1.0</cell><cell>66.5 ? 2.2</cell><cell>65.4 ? 1.3 82.5 ? 0.8 71.4 ? 0.7 79.4 ? 0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CPGNN</cell><cell>71.1 ? 0.5 68.7 ? 1.3</cell><cell>66.7 ? 0.8</cell><cell>63.6 ? 1.8 80.8 ? 0.4 71.6 ? 0.4 78.5 ? 0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GraphGLOW dp</cell><cell>71.5 ? 0.7 71.3 ? 1.2</cell><cell>68.5 ? 1.6</cell><cell>63.2 ? 1.2 83.1 ? 0.8 71.7 ? 1.0 77.3 ? 0.8</cell></row><row><cell></cell><cell></cell><cell cols="3">Graph</cell><cell cols="2">GraphGLOW knn 69.4 ? 0.8 71.0 ? 1.3</cell><cell>64.8 ? 1.2</cell><cell>63.6 ? 1.6 81.7 ? 0.8 71.5 ? 0.8 79.4 ? 0.6</cell></row><row><cell></cell><cell></cell><cell cols="3">Structure</cell><cell>GraphGLOW cos</cell><cell>69.9 ? 0.7 70.8 ? 1.4</cell><cell>65.2 ? 1.8</cell><cell>62.7 ? 1.3 82.0 ? 0.7 71.9 ? 0.9 78.7 ? 0.8</cell></row><row><cell></cell><cell></cell><cell cols="3">Learning</cell><cell>GraphGLOW at</cell><cell>69.9 ? 1.0 70.4 ? 1.5</cell><cell>64.4 ? 1.2</cell><cell>65.0 ? 1.7 82.5 ? 0.9 71.8 ? 0.8 78.5 ? 0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GraphGLOW</cell><cell>72.0 ? 1.0 71.8 ? 0.7</cell><cell>69.8 ? 1.3</cell><cell>67.3 ? 1.2 83.2 ? 0.4 73.8 ? 0.9 79.6 ? 0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GraphGLOW*</cell><cell>71.1 ? 0.3 72.2 ? 0.5</cell><cell>70.3 ? 0.9</cell><cell>66.8 ? 1.4 83.5 ? 0.6 73.9 ? 0.7 79.9 ? 0.5</cell></row><row><cell>Accuracy (%)</cell><cell>80 82 84</cell><cell>Ours</cell><cell>IDGL LDS</cell><cell></cell><cell>VGCN</cell></row><row><cell></cell><cell>0</cell><cell cols="2">500</cell><cell>1000</cell><cell>1500</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Time (s)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. The non-iterative version exhibits a considerable drop in accuracy (as large as 5.4% and 8.8% when tested on target graphs Cornell5</figDesc><table><row><cell>Accuracy (%)</cell><cell>70 75 80 85</cell><cell>One-step No-reg Ours</cell><cell>Accuracy (%)</cell><cell>68 70 72</cell><cell cols="2">-1.12% -3.11% Ours</cell><cell cols="2">0.28% -4.80%</cell><cell>-0.56%-0.70% -0.98% -3.95% -5.37%-5.23%</cell></row><row><cell></cell><cell>65</cell><cell></cell><cell></cell><cell>66</cell><cell>GCN</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Cora</cell><cell>Pubmed Cornell5 Johns.55 Amherst41</cell><cell></cell><cell>0.0</cell><cell>0.1</cell><cell cols="2">0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1) , node-level prediction ? = Z(?) . ? , X ? , Y ? )} ? ?=1 , maximum training episode ?, maximum iteration ? , a shared graph structure learner ? ? , GNN networks {? ? ? } ? ?=1 for each source graph. Initialize ? and {? ? } ? ?=1 ; for episode: 1, 2, . . . , ? do for each source graph G ? do for ? = 1 : ? do ?</figDesc><table><row><cell>Algorithm 2: Training Graph Structure Learner</cell></row><row><cell>Input: observed source graphs</cell></row><row><cell>{G ? } ? ?=1 = {(A ? } ? ?=1 ;</cell></row><row><cell>Compute {? ? ( ?? ? )} ? ?=1 using (16);</cell></row></table><note><p><p>? ? 0, ? ? ? 0, ? ? 0; Z 0 = MLP(X; ? ? ) or Z 0 = GCN(A, X; ? ? );</p>while not converged do ? ? ? + 1; Compute ? ? = {? ?? } using (9) with Z ? -1 ; Sample ? times over ? ? to obtain { ??</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>First, when taking the gradient, we have? ? E ? ? [log ? ? ? (Y|A, X, ?)] ? ?? ? log E ? ? [? ? ? (Y|A, X, ?)] with basic applications of the chain Algorithm 3: Supervised Learning for target GNN Input: observed target graphs G = (A, X, Y), GNN network ? ? for the target graph, maximum iteration ? . Initialize ?; for ? = 1 : ?</figDesc><table><row><cell>? ) ? /(? -1);</cell></row><row><cell>Use gradient ? ? to update ? ;</cell></row><row><cell>Use gradient ? ? to update ? ? ;</cell></row><row><cell>end</cell></row><row><cell>end</cell></row><row><cell>end</cell></row><row><cell>Output: trained graph structure learner ? ?</cell></row></table><note><p><p>* .</p>A DERIVATIONS FOR NWGM</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Statistic information on experimental datasets. The column Homo. reports the homophily ratios, measured by the metric proposed in<ref type="bibr" target="#b23">[24]</ref>.</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell cols="3"># Node # Edge Homo.</cell></row><row><cell>Cora</cell><cell>citation</cell><cell>2,708</cell><cell>5,429</cell><cell>0.77</cell></row><row><cell>CiteSeer</cell><cell>citation</cell><cell>3,327</cell><cell>4,732</cell><cell>0.63</cell></row><row><cell>PubMed</cell><cell>citation</cell><cell>19,717</cell><cell>44,338</cell><cell>0.66</cell></row><row><cell>Amherst41</cell><cell>social</cell><cell>2,235</cell><cell>90,964</cell><cell>0.06</cell></row><row><cell>Cornell5</cell><cell>social</cell><cell>18,660</cell><cell>790,777</cell><cell>0.09</cell></row><row><cell>Johns Hopkins55</cell><cell>social</cell><cell>5,180</cell><cell>186,586</cell><cell>0.10</cell></row><row><cell>Reed98</cell><cell>social</cell><cell>962</cell><cell>18,812</cell><cell>0.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Comparison between one-layer GCN and MLP as transformation network before structure learning module.</figDesc><table><row><cell>Dataset</cell><cell cols="2">GraphGLOW-GCN GraphGLOW-MLP</cell></row><row><cell>Cora</cell><cell>83.2 ? 0.4</cell><cell>82.5 ? 0.5</cell></row><row><cell>CiteSeer</cell><cell>73.8 ? 0.9</cell><cell>73.3 ? 0.7</cell></row><row><cell>PubMed</cell><cell>79.6 ? 0.7</cell><cell>79.6 ? 0.7</cell></row><row><cell>Amherst41</cell><cell>68.1 ? 1.3</cell><cell>68.3 ? 1.5</cell></row><row><cell>Johns Hopkins55</cell><cell>71.8 ? 0.7</cell><cell>72.1 ? 0.9</cell></row><row><cell>Cornell5</cell><cell>70.5 ? 1.0</cell><cell>69.5 ? 1.0</cell></row><row><cell>Reed98</cell><cell>67.3 ? 1.2</cell><cell>65.9 ? 1.4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The work was supported in part by <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2020AAA0107600</rs>), <rs type="funder">NSFC</rs> (<rs type="grantNumber">62222607</rs>), <rs type="funder">Science and Technology Commission of Shanghai Municipality</rs> (<rs type="grantNumber">22511105100</rs>), and <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs> (<rs type="grantNumber">2021SHZDZX0102</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_55CgywQ">
					<idno type="grant-number">2020AAA0107600</idno>
				</org>
				<org type="funding" xml:id="_xysTGwu">
					<idno type="grant-number">62222607</idno>
				</org>
				<org type="funding" xml:id="_SkYVbRp">
					<idno type="grant-number">22511105100</idno>
				</org>
				<org type="funding" xml:id="_8FkCTt3">
					<idno type="grant-number">2021SHZDZX0102</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting the Silent Majority on Graphs: Knowledge Transferable Graph Neural Network</title>
		<author>
			<persName><forename type="first">Wendong</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
		<idno>arXiv preprint abs/1611.08097</idno>
		<title level="m">Geometric deep learning: going beyond Euclidean data</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational inference for graph convolutional networks in the absence of graph data and adversarial settings</title>
		<author>
			<persName><forename type="first">Pantelis</forename><surname>Elinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Tiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Probabilistic Model-Agnostic Meta-Learning</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhrajit</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02892</idno>
		<title level="m">Training Robust Graph Neural Networks with Topology Adaptive Edge Dropping</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pyramid Graph Neural Network: a Graph Sampling and Filtering Approach for Multi-scale Disentangled Representations</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaobing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with graph learning-convolutional networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doudou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph neural network for traffic forecasting: A survey</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayun</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="page">117921</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">How to Learn a Graph from Smooth Signals</title>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Kalofolias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In AISTATS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational Inference for Training Graph Neural Networks in Low-Data Regime through Joint Structure-Label Estimation</title>
		<author>
			<persName><forename type="first">Danning</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In AAAI</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Sean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14187</idno>
		<title level="m">Beyond Low-Pass Filters: Adaptive Feature Propagation on Graphs</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GraphDE: A Generative Framework for Debiased Learning and Out-of-Distribution Detection on Graphs</title>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New Benchmarks for Learning on Non-Homophilous Graphs</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Subgroup generalization and fairness of graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Is Homophily a Necessity for Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain Generalization via Invariant Feature Representation</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cosine similarity metric learning for face verification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. 2022. Causal attention for interpretable and generalizable graph classification</title>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Policy Gradient Methods for Reinforcement Learning with Function Approximation</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Social structure of facebook networks</title>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">L</forename><surname>Traud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><forename type="middle">A</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="page" from="4165" to="4180" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Energy-based Out-of-Distribution Detection for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards open-world feature extrapolation: An inductive graph learning approach</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards open-world recommendation: An inductive model-based collaborative filtering approach</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Node-Former: A Scalable Graph Structure Learning Transformer for Node Classification</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph Information Bottleneck</title>
		<author>
			<persName><forename type="first">Tailin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs</title>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Geometric Knowledge Distillation: Topology Compression for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning substructure invariance for out-of-distribution molecular representations</title>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MoleRec: Combinatorial Drug Recommendation with Substructure-Aware Molecular Representation Learning</title>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bayesian graph convolutional neural networks for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyasundar</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Ustebay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust graph representation learning via neural sparsification</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph neural networks with heterophily</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Shift-robust gnns: Overcoming the limitations of localized graph training data</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv preprint abs/2103.03036</idno>
		<title level="m">Deep Graph Structure Learning for Robust Representations: A Survey</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
