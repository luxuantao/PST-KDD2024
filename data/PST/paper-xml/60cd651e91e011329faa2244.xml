<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Augmentation for Graph Convolutional Network on Semi-Supervised Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-16">16 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhengzheng</forename><surname>Tang</surname></persName>
							<email>tangzhengzheng@cnic.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Network Information Center</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziyue</forename><surname>Qiao</surname></persName>
							<email>qiaoziyue@cnic.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Network Information Center</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuehai</forename><surname>Hong ‡3</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<email>wangyang@cnic.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Network Information Center</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fayaz</forename><forename type="middle">Ali</forename><surname>Dharejo</surname></persName>
							<email>fayazdharejo@cnic.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Network Information Center</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanchun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Du</surname></persName>
							<email>duyi@cnic.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data Augmentation for Graph Convolutional Network on Semi-Supervised Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-16">16 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.08848v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data Augmentation</term>
					<term>Graph Convolutional Network</term>
					<term>Semi-Supervised Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation aims to generate new and synthetic features from the original data, which can identify a better representation of data and improve the performance and generalizability of downstream tasks. However, data augmentation for graph-based models remains a challenging problem, as graph data is more complex than traditional data, which consists of two features with different properties: graph topology and node attributes. In this paper, we study the problem of graph data augmentation for Graph Convolutional Network (GCN) in the context of improving the node embeddings for semi-supervised node classification. Specifically, we conduct cosine similarity based cross operation on the original features to create new graph features, including new node attributes and new graph topologies, and we combine them as new pairwise inputs for specific GCNs. Then, we propose an attentional integrating model to weighted sum the hidden node embeddings encoded by these GCNs into the final node embeddings. We also conduct a disparity constraint on these hidden node embeddings when training to ensure that non-redundant information is captured from different features. Experimental results on five real-world datasets show that our method improves the classification accuracy with a clear margin(+2.5% -+84.2%) than the original GCN model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data augmentation can create several new feature spaces and increase the amount of training data without additional ground truth labels, which has been widely used to improve the performance and generalizability of downstream predictive models. Many works have proposed data augmentation technologies on different types of features, such as images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15]</ref>, texts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>, vectorized features <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6]</ref>, etc. However, how to effectively augment graph data remain a challenging problem, as graph data is more complex and has non-Euclidean structures. Graph Neural Network(GNN) is a family of graph representation learning approaches that encode node features into low-dimensional representation vectors by aggregating local neighbors' information, it has drawn increasing attention in recent years, due to the superior performance on graph data mining <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>For graph-based semi-supervised classification, the goal is to use the given graph data to predict the labels of unlabeled nodes. The given graph data usually consists of graph topology, node attributes(also called node features in some literature, we use node attributes to avoid the confusion with graph feature), as well as the labels of a subset node. Despite the labels, graph data can be specifically described as two graph features: an adjacency matrix of graph topology A ∈ R N ×N and a node attribute matrix X ∈ R N ×d , where N is the total number of nodes, and d is the dimension of node attribute. GNN models conduct on both of these two features simultaneously and fuse them into the final node embedding by stacking several aggregation layers. The whole model can be formulated as a multi-layer graph encoder Z = G(A, X), where Z ∈ R N ×h is the output node embedding matrix and h is the dimension of node embedding. In this work, we consider the most popular and representative GNN: Graph Convolutional Network(GCN), proposed by Kipf et al. <ref type="bibr" target="#b9">[10]</ref>, which is the state-of-the-art model for semi-supervised node classification. It uses an efficient layer-wise propagation rule based on a first-order approximation of spectral convolutions on graphs. The encoder function Z = G(A, X) of a L-layers' GCN can be specified as: Z = G(A, X) = σ( Â...σ( Âσ( ÂXW (0) )W (1) )...W (L) ) <ref type="bibr" target="#b0">(1)</ref> where L is the number of layers. W (i) is the weight matrix of the i-th layer of GCN, σ denotes an activation function. Â = D− 1 2 Ã D− 1  2 , Ã = A + I N , I N is the identity matrix and D is the diagonal degree matrix of Ã.</p><p>However, a fact is that as the pairwise input for the GCN model, both the original features A and X may not be positive correlated with the node labels, while GCN can not adequately learn the importance of these two features to extract the most correlated information, which dampens the performance of GCN on the classification task. Data augmentation can create new feature spaces and preserve the information in original graph data in multiple facets, some of which may contribute useful information to node classification. This leads to the question: besides the original graph features A and X, can we create new pairs of adjacency matrices and attribute matrices and adaptively choose some effective ones as new feature inputs for GCN models?</p><p>Many prior studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref> in data augmentation are to capture the interactions between features by taking addition, subtraction, or cross product of two original features, which are suitable for tensorial features. The major obstacle in graph data is that the original features, graph topology, and node attributes, are two types of data, one is usually encoded by position in Euclidean space, while the other is encoded by node connectivity in non-Euclidean space. It is difficult to take combination operations on these two features to create new features.</p><p>Some work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref> proposes different strategy of adding or removing edges to improve the robustness of GCN. However, these augmentation methods are limited to modifying just a part of the node featuring in the graph, which is unable to create a brand new feature space of the whole graphs for GCN.</p><p>In this paper, we first create multiple new graph topologies and node attributes from the given graph data and propose different combinations of them as inputs for specific GCN models. Then, the output node embeddings of different GCN models are assigned with different weights via an attention mechanism, to sum up to the final node embeddings. In the training, an independence measurement-based disparity constraint is integrated into the objective function to capture diverse information from different features. In this way, extensive information from the original graph is encoded into the final node embeddings to improve the semi-supervised node classification task. The main contributions of our work are summarized as follows:</p><p>1. We propose a graph data augmentation strategy to create new pairwise graph inputs for the GCN model by designing new node attributes and graph topologies from the original graph features. 2. We propose an attentional integrating model, which can learn the importance of different hidden node embeddings encoded from various pairwise graph inputs via specific GCNs, and integrate them into the final node embeddings. 3. We propose a Hilbert-Schmidt independence criterion-based disparity constraint to increase the independence between the node embeddings encoded from various pairwise graph inputs and capture more diverse information. 4. We conduct experiments to evaluate the performance of our proposed method on five datasets. Our improvement over original GCN is +2.5% -+84.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>In this section, we introduce the graph data augmentation strategies for GCN, then we investigate the availability of our augmented features by intuitive cases. Finally, we introduce the whole model including the attentional integrating model and the disparity constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Augmentation Strategy</head><p>Given the original features A and X of graph data, we aim to reconstruct the whole graph topology and node attributes. A naive and widely used way of data augmentation operation is cross operation, we first conduct cosine similaritybased cross operation on A and X to create two new features, which carry the information of global proximity of nodes with others in the views of local topology and node attributes. Specifically, for each row in A and X, we calculate the cosine similarities of it with all the other rows and concatenate these similarities as new features of its corresponding node. Finally, the new features matrices A C and X C of the graph can be formulated as:</p><formula xml:id="formula_0">A Cij = A i • A j A i A j , X Cij = X i • X j X i X j . (<label>2</label></formula><formula xml:id="formula_1">)</formula><p>where</p><formula xml:id="formula_2">A C ∈ R N ×N , X C ∈ R N ×N , A Cij</formula><p>and X Cij is the element in the i-th row and j-th column of A C and X C respectively, A i and X i is the i-th row of A and X respectively. We consider A C and X C as new node attribute matrices, as for each node, its corresponding row in A C preserves the information of global structural proximity with other nodes, and that in X C preserves the information of global proximity of attribute with other nodes. To some extent, these information can be regarded as different types of node attributes.</p><p>Further, we use the obtained A C and X C to construct k-nearest neighbor graphs A T ∈ {0, 1} N ×N , X T ∈ {0, 1} N ×N , that is, we set the largest k elements in each row as 1 and set other elements as 0. A T and X T are considered as new adjacency matrices, where each edge in A T represents the connecting nodes are similar in local topology and each edge in X T represents the connecting nodes are similar in node attribute.</p><p>Finally, we combine these attribute features and adjacency features to create 9 different inputs for GNN model, as shown in the Table <ref type="table">1</ref>:</p><p>Table <ref type="table">1</ref>. Different combinations of six graph features A, X, AC , XC , AT , XT as inputs for GNN model. Adj. means the adjacency matrices, Att. means the attribute matrices. Gi(•, •) represent the specific GNN encoder for the i-th combination of features. Adj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Att.</head><p>X AC XC A G1(A, X) G2(A, AC ) G3(A, XC ) AT G4(AT , X) G5(AT , AC ) G6(AT , XC ) XT G7(XT , X) G8(XT , AC ) G9(XT , XC )</p><p>Noted that the adjacency matrix is usually very sparse, making the cosine similarity matrix sparse, too. So before the process of data augmentation, we first use the update rule proposed in <ref type="bibr" target="#b2">[3]</ref> through the original adjacency matrix A to build new edges between neighbors within 2-hop links, and upgrade A as a denser high-order adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Availability Investigation</head><p>To further investigate the availability of the attribute features A C , X C and the adjacency features A T , X T , we use a simple yet intuitive case to show the distribution and topology of these augmented features and the original graph feature A and X. Specifically, we first generate a naive graph consisting of 90 nodes, and randomly assign 3 labels to these nodes. The edge between every two nodes with the same label is created with the probability of 0.03, and that between every two nodes with different labels is created with the probability of 0.01. Each node has a feature vector of 50 dimensions. We use the Gaussian distribution to generate the node features, the Gaussian distributions for the three classes of nodes have the same covariance matrix, but three different centers far away from each other. Then, we can obtain A and X of this graph and augment new features A C , X C , A T , and X T via the operations described above. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the first line shows the node distribution of the attribute features X, A C , and X C , we use t-SNE to project them into 2-dimensional spaces. In the second line, we draw edges between nodes via the adjacency features A, A T , and X T to show their different graph topologies, where the node positions are set to be the same as X. Attribute Features Analysis. The attribute features are X, A C , and X C . First, we can observe that when X is correlated with labels, X C can preserve the label correlation better, the nodes with the same labels are located in smaller groups and with different labels are farther away from others, we believe that is because X C preserve the global attribute similarity of nodes with others, and the global information can better improve the node distribution for classification. We can also observe that A C can preserve the label correlation inherited from A, but it presents a totally different node distribution with X as they contain different information. So when the graph topology is correlated with labels and the original attribute X is not, A C may further improve the accuracy of classification if it is chosen as node attributes.</p><p>Adjacency Features Analysis. The adjacency features are A, A T , and X T . we can observe that comparing with A, the topology structure in the augmented feature A T can preserve the label correlation better, the intra-class connections are denser than the inter-class connections, that may also because A T preserve the global structural similarity of nodes with others, and the global information can better improve the graph topology for classification. Also, X T provide another edge generation method that nodes with the higher similar attribute are more likely to connect each other. So when the node attributes are related with labels and graph topology is not, X T may further improve the accuracy of classification if it is chosen as the adjacency matrix.</p><p>To summarize, the augmented graph features A C , X C , A T , and X T broaden the availability of the original graph features X and A, which is important because the augmentation may improve the distribution of original features for classification by introducing the global information on the one hand, on the other hand, when the distribution of some features are not correlated with the node labels, these information can provide more input choices for GNN model than the original input pair (A, X), and some of them may contribute more than (A, X) for the final task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attentional Integration Model</head><p>After generating the new inputs for the GNN model, the next question is how do we select useful features. In the real-world, the graph data is complex, it is hard to know which of the augmented features and original features is correlated with the final task, and time-consuming to manually choose the related ones. So we proposed an attentional integration model, which can automatically assign high weights on features with high correlation for the final task.</p><p>Specifically, given the nine combinations of GNN inputs augmented above, we use the traditional GNN encoder, Graph Convolutional Network described in Section 2, to encode the i-th inputs into the node embedding matrices Z i :</p><formula xml:id="formula_3">Z i = G i (Adj i , Att i )<label>(3)</label></formula><p>where Z i ∈ R N ×h , h is the dimension of output node embedding, (Adj i , Att i ) is the i-th pairwise input specified in Table <ref type="table">1</ref>, G i (•, •) represent the GNN encoder for the i-th combination of input, Noted that these nine GNN encoders do not share parameters, this help to better extract the information of different features, but without increasing the time complexity and space complexity because the parameters just increase linearly. Now we obtain the nine output of node embedding matrices: {Z 1 , Z 2 , ..., Z 9 } from the nine GNN encoders. Considering they may have different correlations with the node labels, we use an attention mechanism on them to learn their corresponding importance weight and weighted sum them into the final node embedding matrix:</p><formula xml:id="formula_4">Z = α 1 • Z 1 + α 2 • Z 2 + ... + α 9 • Z 9<label>(4)</label></formula><p>where {α 1 , α 2 , ..., α 9 } ∈ R N ×1 indicate the attention weights of n nodes with embeddings {Z 1 , Z 2 , ..., Z 9 }, respectively. To calculate α i , We firstly transform the embeddings through a nonlinear transformation, and then use one shared attention parameter vector q ∈ R h ×1 to get the attention value ω i as follows:</p><formula xml:id="formula_5">ω i = q T • tanh(W i • (Z i ) T + b i ). (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where ω i ∈ R N ×1 , W i ∈ R h ×h is the weight matrix and b i ∈ R h ×1 is the bias vector for embedding matrix Z i . Then we can get the the attention values {ω 1 , ω 2 , ..., ω 9 } for embedding matrices {Z 1 , Z 2 , ..., Z 9 }, respectively. We then normalize the attention values {ω 1 , ω 2 , ..., ω 9 } for each node by softmax function to get the final importance weight:</p><formula xml:id="formula_7">α j i = sof tmax(ω j i ) = exp(ω j i ) 9 i=1 exp(ω j i )<label>(6)</label></formula><p>where α j i and ω j i represent the j-th element of α i and ω i , respectively. The larger α j i implies the the corresponding node embedding in Z i is more important for the j-th node and should contribute more to its final embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Objective Function</head><p>Disparity Constraint. Firstly, we use the Hilbert-Schmidt Independence Criterion(HSIC) <ref type="bibr" target="#b16">[17]</ref>, a widely used dependency measurement <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13]</ref>, as a penalty term in the objective function to ensure the nine output node embeddings {Z 1 , Z 2 , ..., Z 9 } encoded from nine inputs can capture non-redundant information. HSIC is simple and reliable to compute the independency between variables and the smaller the value is, the more independent they are. The HISC of any two embeddings Z i and Z j is defined as:</p><formula xml:id="formula_8">HSIC(Z i , Z j ) = (n − 1) −2 tr(K i HK j H),<label>(7)</label></formula><p>where K i , K j ∈ R N ×N are the Gram matrices with</p><formula xml:id="formula_9">K i uv = k i (Z i u , Z i v ), K j uv = k j (Z j u , Z j v ), K i uv is the element in u-th row and v-th column of K i , Z i u is the u-th row of Z i , and k i (•, •) is the kernel function. H = I − n −1 ee T</formula><p>, where e is an all-one column vector and I is an identity matrix. In our implementation, we use the inner product kernel function. Then we set the disparity constraint L d by minimizing the values of HISC among nine output nodes embeddings:</p><formula xml:id="formula_10">L d = i =j HISC(Z i , Z j ). (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>Optimization Objective. For semi-supervised multi-class classification, We feed the final node embeddings Z into a linear transformation and a sof tmax function. Denote classes set is C, and the probability of node i belonging to class c ∈ C is Ŷic , the prediction results on whole nodes Ŷ = [ Ŷic ] ∈ R N ×C can be calculated as:</p><formula xml:id="formula_12">Ŷ = sof tmax(W • Z + b),<label>(9)</label></formula><p>where sof tmax(x) = exp(x) C c=1 exp(xc) is actually a row-wise normalizer across all classes. Then the cross-entropy loss L for node classification over all labeled nodes is represented as:</p><formula xml:id="formula_13">L l = − l∈Y L C c=l Y lc ln Ŷlc . (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>Where Y L is the set of node indices that have labels, for each l ∈ L the real one-hot encoded label is Y l .</p><p>Finally, combining the node classification task and the disparity constraints, we have the following overall objective function:</p><formula xml:id="formula_15">L = L l + λL d . (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where λ is parameters of the disparity constraint terms. We use a mini-batch Adam optimizer to minimize L and optimize the parameters in the whole model. Noted that we use HISC to calculate the pairwise independence, it would take C 2 9 times of calculation of HISC among Z 1 to Z 9 in each training step, which we think is unnecessary. We use a sampling strategy to reduce the computation that randomly selecting t pairs of the output embeddings and summing their HISC as the disparity constraints loss in each training step. Through multiple iterations, all combinations of embeddings should be sampled and all embeddings should be trained to be independent of each other. 3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Setting</head><p>To adequately examine the effectiveness of our proposed data augmentation method, we evaluate the performance of our framework on five real-world benchmark datasets: Citeseer <ref type="bibr" target="#b9">[10]</ref> is research paper citation network, UAI2010 <ref type="bibr" target="#b22">[23]</ref> is a dataset for community detection, ACM <ref type="bibr" target="#b23">[24]</ref> is research paper coauthor network extracted from ACM dataset, BlogCatalog <ref type="bibr" target="#b13">[14]</ref> is a social network with bloggers relationships extracted from the BlogCatalog website, Flickr <ref type="bibr" target="#b13">[14]</ref>is a social network with users interaction from an image and video hosting website. Basic statistics of these datasets are summarized in Table <ref type="table" target="#tab_0">2</ref>.</p><p>We compared our method with some GCN and node classification related baselines: GCN <ref type="bibr" target="#b9">[10]</ref> is a classical semi-supervised graph convolutional network model, which obtains node representation through multi-layer neighbor aggregation. Chebyshev <ref type="bibr" target="#b3">[4]</ref> learns rich feature information by superimposing multiple Chebyshev filters with GCN. GAT <ref type="bibr" target="#b21">[22]</ref> is a graph neural network model that aggregates node features through multiple attention heads with different semantics. DEMO-Net <ref type="bibr" target="#b26">[27]</ref> proposes a generic graph neural network model which formulates the feature aggregation into a multi-task learning problem according to nodes' degree values. MixHop <ref type="bibr" target="#b0">[1]</ref> utilizes multiple powers of the adjacency matrix to learn the general mixing of neighborhood information, including averaging and delta operators in the feature space. We also compare our method with some related graph data augmentation based methods for semi-supervised node classification. GAug <ref type="bibr" target="#b30">[31]</ref> is to leverage information inherent in the graph to predict which non-existent edges should likely exist, and which existent edges should likely be removed in the original graph to produce modified graphs to improve the model performance. MCGL <ref type="bibr" target="#b4">[5]</ref> assigns pseudo-labels to some nodes in each convolutional layer, and improves the performance of the model by expanding the training set.</p><p>The weights of parameters are initialized like the original GCN <ref type="bibr" target="#b9">[10]</ref> and input vectors are row-normalized accordingly <ref type="bibr" target="#b7">[8]</ref>. For our model, we train nine 2-layer GCNs with the same hidden layer dimension(h 1 ) and the same output dimension (h 2 ) simultaneously, where h 1 of the UAI2010, BlogCatalog, and Flickr is 256 and the out dimension h 2 is 128. The h 1 and h 2 of ACM and Citeseer are 512 and 256 respectively. we use 5e − 4 learning rate with Adam optimizer, the dropout rate is 0.5, weight decay is 1e − 4. In addition, the hyper-parameter k for constructing k-nearest neighbor graphs is 4, t for sampling embeddings pairs is 8. For the baselines, we set the dimension of node embeddings in five datasets same as the setting of out method, and the other hyper-parameter setting are based on default values or the values specified in their own papers. We choose the number of labeled nodes per class as 20/40/60 respectively for training, and 500 nodes are used for validation and 1000 nodes for testing. All methods are repeatedly run 5 times, the average results are reported to make sure the results can reflect the performances of methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semi-Supervised Classification</head><p>The semi-supervised node classification results are reported in Table <ref type="table" target="#tab_1">3</ref>. We report the Accuracy (ACC) and macro F1-score (F1) of the classification results. From the results, we can observe that (1) our proposed method achieves the best performance on all datasets with all label rates, showing the superiority of our method in improving the semi-supervised node classification. (2) Our method consistently outperform the original GCN on all five datasets, the improvement of ACC over Citeseer, UAI2010, ACM, BlogCatalog, Flickr is {3.0%-6.1%,41.3%-44.9%,2.5%-4.6%,20.4%-25.1%,71.5%-84.2%}, respectively. indicating that the augmented graph features contain more useful information than original graph features and help to node classification. <ref type="bibr" target="#b2">(3)</ref> We noticed that two graph augmentation methods GAug and MCGL perform well on some datasets, but also fail in some datasets, while our method consistently performs well on all datasets, showing that our whole framework is robust on different types of graphs.</p><p>We further report the visualization of learned node embeddings of the Citeseer, UAI2010, and ACM datasets in Figure <ref type="figure" target="#fig_1">2</ref>. We use t-SNE to project the final node embeddings of our method and original GCN into 2-dimensional spaces and color nodes differently according to their labels. We can observe that the boundaries between different classes in our method are sharper than the original GCN, and nodes in the same class are more concentrated, especially in the Citeseer dataset, which proves our method can learn better node representations to improves the node classification performance of original GCN.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attentional Integration Model Analysis</head><p>We design nine combinations of features as inputs of GCN models and learn nine specific node embeddings for each node, then each embedding is associated with the corresponding attention values by our proposed attentional integrating model. Thus, we conduct attention distribution analysis on the ACM, UAI2010, and Citeseer datasets in Figure <ref type="figure">3</ref>, we report the Box-plots of the learned attention value distributions of all nodes respectively for nine GCN models {G 1 , ..., G 9 }. We can observe that the average of attention values for nine input combinations are evidently different, some of the combinations may have larger attention values than others, For example in ACM, the attention values of G 1 , G 5 , and G 9 are larger than others, which implies that the corresponding augmented inputs of (A, X), (A T , A C ), and (X T , X C ) contain more valuable information than other inputs for the classification task. Also, we can observe that between different datasets, the same combination input may be quite different in attention values, which proves that our proposed attentional integrating model is able to adaptively find and assign larger attention value for the important information on different datasets.</p><p>In Figure <ref type="figure">4</ref>, we further analyze the changing trends of attention values for different input combinations in the increasing of training epochs. We report the results of ACM, UAI2010, and Citeseer datasets as examples, we can observe that the average attention values of different combinations gradually increase or decrease when training, and finally converge to a relatively stable value. This phenomenon proves that the proposed attentional integrating model has a great fitting capability to learn attention values on different datasets. We also demonstrate the distribution of the output node embeddings of nine combination inputs when the model has converged. Figure <ref type="figure" target="#fig_3">5</ref> shows the embedding distributions of the ACM dataset projected by t-SNE. It can be observed that the node embeddings Z 1 , Z 5 , and Z 9 encoded from G 1 (A, X), G 5 (A T , A C ), and G 9 (X T , X C ) is obviously well classified into three classes, so the learned attention of them in Figure <ref type="figure">3</ref> is larger than others. It proves that our designed graph features can also capture useful information for node classification and the attentional integration model can adaptively integrate different information from multiple input features to improve the final classification results. Also, the distributions of nine node embeddings are significantly different from each other, showing the effectiveness of our designed disparity constraint in keeping the dependency of different embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter Sensitivity</head><p>The parameter k introduced in Section 2.1 is used to adjust the sparsity of our augmented features A T and X T . In Figure <ref type="figure" target="#fig_4">6</ref>, we evaluate how the k impacts the performance of our method on ACM, UAI2010, and Citeseer datasets with the number of training nodes as 20/40/60, respectively. We report the ACC of our method with various numbers of k ranging from 2 to 9 and other parameters remaining the same. From the figures, we observe that when k was small, the accuracy performance of our model is relatively limited, demonstrating that a smaller size of k led to the augmented adjacency features sparser and information loss. When k is increased to 4 or 5, our model can gain the highest accuracy results. However, when k is too large, the performance decreases slightly, which may probably because denser augmented adjacency features may introduce more noisy edges. In summary, properly setting the size of k can help to generate robust features to improve the performance of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Works</head><p>Graph data augmentation has drawn increasing attention in graph learning recently, it can create new graph data to improve the generalization of graph models, especially the GNN models. Existing graph augmentations mainly focus on augmenting graph structures by modifying local graph structure <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref>. <ref type="bibr" target="#b31">[32]</ref> introduce data augmentation on graphs and present two heuristic algorithms: random mapping and motif-similarity mapping, to generate more weakly labeled data for small-scale benchmark datasets via heuristic modification of graph structures. <ref type="bibr" target="#b10">[11]</ref> propose a simple but effective solution, FLAG, which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. <ref type="bibr" target="#b24">[25]</ref> construct a feature graph and propose an adaptive multi-channel graph convolutional networks to improve the node embeddings. <ref type="bibr" target="#b30">[31]</ref> shows that neural edge predictors can effectively encode class-homophilic structure to promote intra-class edges and demote inter-class edges in given graph structures, and their leverages these insights to improve performance in GNN-based node classification via edge prediction. <ref type="bibr" target="#b25">[26]</ref> present the Node-Parallel Augmentation scheme, that creates a 'parallel universe' for each node to conduct data augmentation. <ref type="bibr" target="#b18">[19]</ref> proposed GINN that uses supervised and unsupervised data to construct a similarity map between points in the dataset, and rebuild them to expand the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we study to improve the performance of GCN on semi-supervised classification via graph data augmentation. We create new attribute and adjacency features base on original graph features and pairwise combine them as inputs for specific GCNs, then use attention mechanism and disparity constraint to integrate diverse information from the GCNs' outputs to the final node embeddings. From the experiments, our proposed method can better extract the rich information of graphs and improve the qualities of node representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Visualization of attribute features: X, AC , and XC , and adjacency features : A, AT , and XT .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of the learned final node embeddings on ACM, UAI2010, and Citeseer datasets. (L/C=20)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Analysis of attention distribution. (L/C=20)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of hidden node embeddings on ACM datasets. (L/C=20)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Analysis of parameter k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>The statistics of the datasets</figDesc><table><row><cell>Dataset</cell><cell cols="3">Nodes Edges Classes Attribute</cell></row><row><cell>Citeseer</cell><cell>3327 4732</cell><cell>6</cell><cell>3703</cell></row><row><cell>UAI2010</cell><cell>3067 28311</cell><cell>19</cell><cell>4973</cell></row><row><cell>ACM</cell><cell>3025 13128</cell><cell>3</cell><cell>1870</cell></row><row><cell cols="2">BlogCatalog 5196 171743</cell><cell>6</cell><cell>8189</cell></row><row><cell>Flickr</cell><cell>7575 239738</cell><cell>9</cell><cell>12047</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Results of semi-supervised node classification(%). (Bold: best. L/C is the number of labeled nodes per class. The results of some baselines are taken from<ref type="bibr" target="#b24">[25]</ref>.)</figDesc><table><row><cell></cell><cell>Datasets</cell><cell>Citeseer</cell><cell>UAI2010</cell><cell>ACM</cell><cell>BlogCatalog</cell><cell>Flickr</cell></row><row><cell cols="7">L/C Method ACC F1 ACC F1 ACC F1 ACC F1 ACC F1</cell></row><row><cell></cell><cell>GCN</cell><cell cols="5">70.30 67.50 49.88 32.86 87.80 87.82 69.84 68.73 41.42 39.95</cell></row><row><cell></cell><cell cols="6">Chebyshev 69.80 65.92 50.02 33.65 75.24 74.86 38.08 33.39 23.26 21.27</cell></row><row><cell></cell><cell>GAT</cell><cell cols="5">72.50 68.14 56.92 39.61 87.36 87.44 64.08 63.38 38.52 37.00</cell></row><row><cell>20</cell><cell cols="6">DEMO-Net 69.50 67.84 23.45 16.82 84.48 84.16 54.19 52.79 34.89 33.53 MixHop 71.40 66.96 61.56 49.19 81.08 81.40 65.46 64.89 39.56 40.13</cell></row><row><cell></cell><cell>GAug</cell><cell cols="5">73.30 70.12 52.96 49.82 90.82 89.44 77.60 75.43 68.20 67.55</cell></row><row><cell></cell><cell>MCGL</cell><cell cols="5">66.88 63.26 42.56 24.78 90.95 91.01 54.22 50.15 15.67 15.54</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">74.60 70.20 72.20 60.87 91.90 91.81 84.10 84.60 76.30 76.27</cell></row><row><cell></cell><cell>GCN</cell><cell cols="5">73.10 69.70 51.80 33.80 89.06 89.00 71.28 70.71 45.48 43.27</cell></row><row><cell></cell><cell cols="6">Chebyshev 71.64 68.31 58.18 38.80 81.64 81.26 56.28 53.86 35.10 33.53</cell></row><row><cell></cell><cell>GAT</cell><cell cols="5">73.04 69.58 63.74 45.08 88.60 88.55 67.40 66.39 38.44 39.94</cell></row><row><cell>40</cell><cell cols="6">DEMO-Net 70.44 66.97 30.29 26.36 85.70 84.83 63.47 63.09 46.57 45.23 MixHop 71.48 67.40 65.05 53.86 82.34 81.13 71.66 70.84 55.19 56.25</cell></row><row><cell></cell><cell>GAug</cell><cell cols="5">74.60 71.32 55.26 53.36 91.24 91.01 79.46 77.79 73.24 72.28</cell></row><row><cell></cell><cell>MCGL</cell><cell cols="5">69.48 65.98 41.93 25.72 91.10 91.13 54.74 51.24 17.82 17.06</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">75.50 71.58 75.10 69.70 92.10 91.94 89.20 89.06 80.10 79.36</cell></row><row><cell></cell><cell>GCN</cell><cell cols="5">74.48 71.24 54.40 34.12 90.54 90.49 72.66 71.80 47.96 46.58</cell></row><row><cell></cell><cell cols="6">Chebyshev 73.26 70.31 59.82 40.60 85.43 85.26 70.06 68.37 41.70 40.17</cell></row><row><cell></cell><cell>GAT</cell><cell cols="5">74.76 71.60 68.44 48.97 90.40 90.39 69.95 69.08 38.96 37.35</cell></row><row><cell>60</cell><cell cols="6">DEMO-Net 71.86 68.22 34.11 29.05 86.55 84.05 76.81 76.73 57.30 56.49 MixHop 72.16 69.31 67.66 56.31 83.09 82.24 77.44 76.38 64.96 65.73</cell></row><row><cell></cell><cell>GAug</cell><cell cols="5">75.48 72.22 55.92 54.08 92.06 91.81 81.81 79.84 75.68 74.24</cell></row><row><cell></cell><cell>MCGL</cell><cell cols="5">74.02 70.69 44.30 22.46 92.03 92.04 55.24 49.41 22.36 21.28</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">76.70 72.88 76.90 69.79 92.80 92.75 89.70 89.53 82.29 82.85</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>This work is supported in part by the Natural Science Foundation of China under Grant No. 92046017, the Natural Science Foundation of China under Grant No. 61836013, Beijing Natural Science Foundation(4212030).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast network embedding enhancement via high order proximity approximation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Data augmentation view on graph convolutional network and the proposal of monte carlo graph learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Data augmentation using synthetic data for time series classification with deep residual networks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Muller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02455</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Synthetic data augmentation using gan for improved liver lesion classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frid-Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="289" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
				<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Flag: Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distilling gru with data augmentation for unconstrained handwritten text recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on Frontiers in Handwriting Recognition (ICFHR)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="56" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The hsic bottleneck: Deep learning without back-propagation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5085" to="5092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-embedding attributed networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="393" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Forward noise adjustment scheme for data augmentation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Moreno-Barea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strazzera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jerez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Urda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium Series on Computational Intelligence (SSCI)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="728" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supervised feature selection via dependence estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
				<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="823" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel automated approach for software effort estimation based on data augmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Minku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
				<meeting>the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="468" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient data augmentation using graph imputation neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Spinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scarpiniti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progresses in Artificial Intelligence and Neural Systems</title>
		<imprint>
			<biblScope unit="page">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved mixed-example data augmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Dinneen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1262" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified weakly supervised framework for community detection and semantic matching</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="218" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Am-gcn: Adaptive multi-channel graph convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nodeaug: Semisupervised node classification with data augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery &amp; Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Net: Degree-specific graph neural networks for node and graph classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="406" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Underwater image classification using deep convolutional neural networks and data augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Signal Processing, Communications and Computing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical data augmentation and the application in text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fish-mml: fisher-hsic multiview metric learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3054" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06830</idno>
		<title level="m">Data augmentation for graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data augmentation for graph classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2341" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
