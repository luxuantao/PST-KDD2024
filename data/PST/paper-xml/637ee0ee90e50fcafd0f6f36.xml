<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-driven Feature Tracking for Event Cameras</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-11-23">23 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nico</forename><surname>Messikommer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mathias Gehrig Davide Scaramuzza Robotics and Perception Group</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carter</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mathias Gehrig Davide Scaramuzza Robotics and Perception Group</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Data-driven Feature Tracking for Event Cameras</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-23">23 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.12826v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Because of their high temporal resolution, increased resilience to motion blur, and very sparse output, event cameras have been shown to be ideal for low-latency and lowbandwidth feature tracking, even in challenging scenarios. Existing feature tracking methods for event cameras are either handcrafted or derived from first principles but require extensive parameter tuning, are sensitive to noise, and do not generalize to different scenarios due to unmodeled effects. To tackle these deficiencies, we introduce the first data-driven feature tracker for event cameras, which leverages low-latency events to track features detected in a grayscale frame. We achieve robust performance via a novel frame attention module, which shares information across feature tracks. By directly transferring zero-shot from synthetic to real data, our data-driven tracker outperforms existing approaches in relative feature age by up to 120 % while also achieving the lowest latency. This performance gap is further increased to 130 % by adapting our tracker to real data with a novel self-supervision strategy.</p><p>Multimedia Material A video showing qualitative results is available at https://youtu.be/aZBapP5Gdv8</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite many successful implementations in the real world, existing feature trackers are still primarily constrained by the hardware performance of standard cameras. To begin with, standard cameras suffer from a bandwidthlatency trade-off, which noticeably limits their performance under rapid movements: at low frame rates, they have minimal bandwidth but at the expense of an increased latency; furthermore, low frame rates lead to large appearance changes between consecutive frames, significantly increasing the difficulty of tracking features. At high frame rates, the latency is reduced at the expense of an increased bandwidth overhead and power consumption for downstream systems. Another problem with standard cameras is mo-* equal contribution. Figure <ref type="figure">1</ref>. Our method leverages the high-temporal resolution of events to provide stable feature tracks in high-speed motion in which standard frames suffer from motion blur. To achieve this, we propose a novel frame attention module that combines the information across feature tracks. tion blur, which is prominent in high-speed low-lit scenarios, see Fig. <ref type="figure">1</ref>. These issues are becoming more prominent with the current commodification of AR/VR devices.</p><p>Event cameras have been shown to be an ideal complement to standard cameras to address the bandwidth-latency trade-off <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b14">17]</ref>. Event cameras are bio-inspired vision sensors that asynchronously trigger information whenever the brightness change at an individual pixel exceeds a predefined threshold. Due to this unique working principle, event cameras output sparse event streams with a temporal resolution in the order of microseconds and feature a highdynamic range and low power consumption. Since events are primarily triggered in correspondence of edges, event cameras present minimal bandwidth. This makes them ideal for overcoming the shortcomings of standard cameras.</p><p>Existing feature trackers for event cameras have shown unprecedented results with respect to latency and tracking robustness in high-speed and high-dynamic range scenarios <ref type="bibr">[4,</ref><ref type="bibr" target="#b14">17]</ref>. Nonetheless, until now, event-based trackers have been developed based on classical model assumptions, which typically result in poor tracking performance in the presence of noise. They either rely on iterative optimization of motion parameters <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b43">46]</ref> or employ a simple classification for possible translations of a feature [4], thus, do not generalize to different scenarios due to unmodeled effects. Moreover, they usually feature complex model parameters, requiring extensive manual hand-tuning to adapt to different event cameras and new scenes.</p><p>To tackle these deficiencies, we propose the first datadriven feature tracker for event cameras. Using a neural network, our method tracks features by localizing a template patch from a grayscale image in subsequent event patches. The network architecture features a correlation volume for the assignment and employs recurrent layers for long-term consistency. To increase the tracking performance, we introduce a novel frame attention module, which shares information across feature tracks in one image. We first train on a synthetic optical flow dataset and then finetune it with our novel self-supervision scheme based on 3D point triangulation using camera poses.</p><p>Our tracker outperforms state-of-the-art baselines by up to 5.5% and 130.2% on the Event Camera Dataset benchmark <ref type="bibr" target="#b27">[30]</ref> and the recently published EDS dataset <ref type="bibr" target="#b18">[21]</ref>, respectively. This performance is achieved without requiring extensive manual hand-tuning of parameters. Moreover, without optimizing the code for deployment, our method achieves faster inference than existing methods. Finally, we show how the combination of our method with the wellestablished frame-based tracker KLT <ref type="bibr" target="#b25">[28]</ref> leverages the best of both worlds for high-speed scenarios. This combination of standard and event cameras paves the path for the concept of sparingly triggering frames based on the tracking quality, which is a critical tool for future applications where runtime and power consumption are essential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Frame-Based Feature Tracking While no prior works have leveraged deep learning to track features from events, data-driven methods were recently proposed for feature tracking using standard frames. Among them is PIP <ref type="bibr" target="#b17">[20]</ref>, which estimates the trajectories of queried feature locations for an entire image sequence and thus can even track features through occlusions by leveraging the trajectory before and after. Instead of processing the whole sequence, DPVO <ref type="bibr" target="#b34">[37]</ref> takes a sequence of images and simultaneously estimates scene depth and camera pose on-the-fly. It does so by randomly sampling patches from feature maps from frames and adding them to a bipartite frame graph, which is iteratively optimized by correlating feature descriptors from patches observed at different camera poses. A related re-search field to feature tracking is optical flow estimation, i.e., dense pixel correspondence estimation between two frames. There exist many optical flow methods <ref type="bibr" target="#b10">[13]</ref>, with correlation-based networks <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b33">36]</ref> being the state-of-theart. However, despite recent advancements, frame-based feature trackers still suffer from the hardware limitation of standard cameras. To tackle this disadvantage, we propose a self-supervised tracker that unlocks the robustness characteristics of event cameras for feature tracking and, by doing so, outperforms state-of-the-art tracking methods.</p><p>Pose Supervision Leveraging camera poses was previously explored for training feature detection and matching networks. Wang et al. <ref type="bibr" target="#b38">[41]</ref> used pose data to supervise a network for pixel-wise correspondence estimation where the epipolar constraint between two frames is used to penalize incorrect predictions. More recently, a correspondence refinement network called Patch2Pix <ref type="bibr" target="#b41">[44]</ref> extends the epipolar constraint supervision by using the Sampson distance instead of the Euclidean Distance. Instead of only considering two camera poses, our self-supervision strategy computes a 3D point using DLT <ref type="bibr" target="#b0">[1]</ref> for each predicted track in multiple frames, which makes our supervision signal more robust to errors. Moreover, we supervise our network by computing a 2D distance between the reprojected and predicted points without the ambiguity of a distance to an epipolar line.</p><p>Event-Based Feature Tracking In recent years, multiple works have explored event-based feature tracking to increase robustness in challenging conditions. Early works <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b28">31,</ref><ref type="bibr" target="#b43">46]</ref> tracked features as point-sets of events and used ICP <ref type="bibr" target="#b2">[5]</ref> to estimate the motion between timesteps, which can also be combined with frame-based trackers to improve performance <ref type="bibr" target="#b9">[12]</ref>. Instead of point sets, EKLT <ref type="bibr" target="#b14">[17]</ref> estimates the parametric transform between a template and a target patch of brightness increment images alongside the feature's velocity. Other event-based trackers align events along B?zier curves <ref type="bibr" target="#b31">[34]</ref> or B-splines <ref type="bibr" target="#b7">[10]</ref> in space and time to obtain feature trajectories.</p><p>To exploit the inherent asynchronicity of event streams, event-by-event trackers have also been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">11]</ref>. One of them is HASTE [4], which reduces the space of possible transformations to a fixed number of rotations and translations. In HASTE, every new event leads to confidence updates for the hypotheses and a state transition if the confidence threshold is exceeded. Another work called eCDT <ref type="bibr" target="#b19">[22]</ref> first represents features as event clusters and then incorporates incoming events into existing ones, resulting in updated centroids and, consequently, updated feature locations. In a similar direction to feature tracking, several event-based feature detectors <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b26">29]</ref> were proposed, of which some are performing feature tracking based on proximity of detections in the image <ref type="bibr">[3,</ref><ref type="bibr" target="#b6">9]</ref>. Apart from event-based feature tracking and detection, multiple works tackle the problem of object tracking using event cam-eras <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b11">14,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b40">43]</ref>.</p><p>The task of optical flow estimation using event cameras gained popularity as well. Zhu et al. <ref type="bibr" target="#b43">[46]</ref> estimates the optical flow of features from events using ICP and an objective function based on expectation maximization to solve for the parameters of an affine transform. More recently, an adaptive block matching algorithm <ref type="bibr" target="#b24">[27]</ref> was proposed to estimate optical flow. Finally, recent data-driven methods for event-based optical flow estimation <ref type="bibr" target="#b15">[18,</ref><ref type="bibr" target="#b42">45]</ref> leverage advances in deep optical-flow estimation. Inspired by these advances, our tracking network leverages a correlation layer to update a feature's location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Feature tracking algorithms aim to track a given point in a reference frame in subsequent timesteps. They usually do this by extracting appearance information around the feature location in the reference frame, which is then matched and localized in subsequent ones. Following this pipeline, we extract an image patch P 0 in a grayscale frame for the given feature location at timestep t 0 and track the feature using the asynchronous event stream. The event stream E j = {e i } nj i=1 between timesteps t j-1 and t j consists of events e i , each encoding the pixel coordinate x i , timestamp with microsecond-level resolution ? i and polarity p i ? {-1, 1} of the brightness change. We refer to <ref type="bibr" target="#b12">[15]</ref> for more information about the working principles of event cameras.</p><p>Given the reference patch P 0 , our network predicts the relative feature displacement ? fj during t j-1 and t j using the corresponding event stream E j in the local neighborhood of the feature location at the previous timestep t j-1 . The events inside the local window are converted to a dense event representation P j , specifically a maximal timestamp version of SBT <ref type="bibr" target="#b37">[40]</ref> where each pixel is assigned the timestamp of the most recent event. Once our network has localized the reference patch P 0 inside the current event patch P j , the feature track is updated, and a new event patch P j+1 is extracted at the newly predicted feature location while keeping the reference patch P 0 . This procedure can then be iteratively repeated while accumulating the relative displacements to construct one continuous feature track. The overview of our method and our novel frame attention module are visualized in Fig. <ref type="figure" target="#fig_0">2</ref> In Sec. 3.1, we explain how the feature network processes each feature track independently. The resulting network output is given as input to our frame attention module, which combines information from all feature tracks in one image, see Sec. 3.2. Finally, we introduce our supervision scheme for data with ground truth in Sec. 3.3.1 and our selfsupervision strategy based on camera poses in Sec. 3.3.2. For the specific architectural details of each network, we refer to the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Network</head><p>To localize the template patch P 0 inside the current event patch P j , the feature network first encodes both patches using separate encoders based on Feature Pyramid Networks <ref type="bibr" target="#b23">[26]</ref>. The resulting outputs are per-pixel feature maps for both patches that contain contextual information while keeping the spatial information. To explicitly compute the similarity measure between each pixel in the event patch and the template patch, we construct a correlation map C j based on the bottleneck feature vector R 0 of the template patch encoder and the feature map of the event patch, as visualized in Fig. <ref type="figure" target="#fig_0">2</ref>. Together with the correlation map C j , both feature maps are then given as input to a second feature encoder in order to refine the correlation map. This feature encoder consists of standard convolutions, and one ConvL-STM block <ref type="bibr" target="#b32">[35]</ref> with a temporal cell state F j . The temporal information is crucial to predicting consistent feature tracks over time. Moreover, it enables the integration of the motion information provided by the events. The output of the feature network is a single feature vector with spatial dimension 1?1. Up to now, each feature has been processed independently from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Frame Attention Module</head><p>To share information between features in the same image, we introduce a novel frame attention module, which is visualized in Fig. <ref type="figure" target="#fig_0">2</ref>. Since points on a rigid body exhibit correlated motion in the image plane, there is a substantial benefit in sharing information between features across the image. To achieve this, our frame attention module takes the feature vectors of all patches at the current timestep t j as input and computes the final displacement for each patch based on a self-attention weighted fusion of all feature vectors. Specifically, we maintain a state S for each feature across time in order to leverage the displacement prediction of the previous timesteps in the attention fusion. The temporal information should facilitate the informationsharing of features with similar motion in the past. This way, it is possible to maintain vulnerable feature tracks in challenging situations by adaptively conditioning them on similar feature tracks. Each input feature vector is individually first fused with the current state S j-1 using two linear layers with Leaky ReLU activations. All of the resulting fused features in an image are then used as key, query, and value pairs for a multi-head attention layer <ref type="bibr" target="#b36">[39]</ref>, which performs self-attention over each feature in an image. To facilitate the training, we introduce a skip connection around the multi-head attention for each feature, which is adaptively weighted during the training by a Layerscale layer <ref type="bibr" target="#b35">[38]</ref>. The resulting feature vectors are then used in a simple gating layer to compute the updated state S j based on the previous state S j-1 . Finally, the updated state S j is then processed by one linear layer to predict the final displacement ? fj . fuses the processed feature vectors for all tracks in an image using self-attention and a temporal state S, which is used to compute the final displacement ? fj .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Supervision</head><p>In general, the supervision of trackers, extractors, or even flow networks is still an open research field since datasets containing pixel-wise correspondences as ground truth are rare. To make matters worse, there exist even fewer eventbased datasets containing accurate pixel correspondences. To overcome this limitation, we train our network in the first step on synthetic data from the Multiflow dataset <ref type="bibr" target="#b16">[19]</ref>, which contains frames, synthetically generated events, and ground truth pixel flow. However, since the noise is not modeled, synthetic events differ significantly from events recorded by a real event camera. Thus, in the second step, we fine-tune our network using our novel pose supervision loss to close the gap between synthetic and real events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Synthetic Supervision</head><p>Synthetic data has the benefit that it provides ground truth feature tracks. Thus, a loss based on the L1 distance can be directly applied for each prediction step j between the predicted and ground truth relative displacement, see Fig. <ref type="figure">3</ref>. It is possible that the predicted feature tracks diverge beyond the template patch such that the next feature location is not in the current search. Thus, if the difference between predicted and ground truth displacement ||? fj -?f j || 1 exceeds the patch radius r, we do not add the L1 distances to the final loss to avoid introducing noise in supervision. Our truncated loss L rp is formulated as follows.</p><formula xml:id="formula_0">err j = ||? fj -?f j || 1 ||? fj -?f j || 1 &lt; r 0 else<label>(1)</label></formula><formula xml:id="formula_1">L rp = j 1 (errj =0) err j j 1 (errj =0)<label>(2)</label></formula><p>To reduce the gap between synthetic and real data, we apply on-the-fly augmentation during training, which significantly increases the motion distribution. To teach the network geometrically robust representations, affine transformations W are applied to the current event patch P j to obtain an augmented Patch P aug j at each prediction step, as formulated in Eq. (3). The augmentation parameters for rotation, translation, and scale ? = (? r , ? t , ? s ) are randomly sampled from a uniform distribution at each prediction step during training. Our tracker then predicts a relative displacement ? fj-1 given the augmented patch P aug j and original template patch P 0 . The loss is then computed between the predicted displacement ? fj-1 and the augmented ground truth ?f aug j-1 , which is obtained by applying the same affine transformation W .</p><formula xml:id="formula_2">P aug j = W (P j , ?) (3) ? f aug j-1 = T (P 0 , P aug j ) (4) ? fj-1 = W -1 (? f aug j-1 , ?)<label>(5)</label></formula><p>The corrected displacement ? fj-1 is then accumulated in order to extract the next event patch P j+1 . Our augmentation strategy introduces dynamic trajectories and changes in patch appearance during training that improve performance on real data.</p><p>Figure <ref type="figure">3</ref>. The L1 distance j between the predicted ? fj and the ground truth displacement ?f j is used as a truncated loss, which is set to zero if the ground truth feature is outside of the current event patch Pj, as shown for timestep t j+k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Pose Supervision</head><p>To adapt the network to real events, we introduce a novel pose supervision loss solely based on ground truth poses of a calibrated camera. The ground truth poses can easily be obtained for sparse timesteps t j using structure-frommotion algorithms, e.g., COLMAP <ref type="bibr" target="#b30">[33]</ref>, or by an external motion capture system. Since our supervision strategy relies on the triangulation of 3D points based on poses, it can only be applied in static scenes.</p><p>In the first step of the fine-tuning, our network predicts multiple feature tracks for one sequence. For each predicted track i, we compute the corresponding 3D point X i using the direct linear transform <ref type="bibr" target="#b0">[1]</ref>. Specifically, for each feature location x j , we can write the projection equation assuming a pinhole camera model using the camera pose, represented as a rotation matrix R tj and a translation vector T tj , at timestep t j , and the calibration matrix K, see Eq. ( <ref type="formula">6</ref>). The resulting projection matrix can be expressed as matrix M j consisting of column vectors m k T j with k ? {1, 2, 3}.</p><formula xml:id="formula_3">x j = K[R t j |T t j ]X j = M j X j = ? ? ? m 1 T j m 2 T j m 3 T j ? ? ? X i (6)</formula><p>Using the direct linear transform, we can reformulate the projection equations as the homogenous linear system in Eq. <ref type="bibr" target="#b4">(7)</ref>. By using SVD, we obtain the 3D point X j , which minimizes the least square error of Eq. <ref type="bibr" target="#b4">(7)</ref>.</p><formula xml:id="formula_4">? ? ? u j m 3 T j -m 2 T j m 1 T j -v j m 3 T j ... ? ? ? = AX i = 0 (7)</formula><p>Once the 3D position of X i is computed, we can find the reprojected pixel point xj for each timestep t j using perspective projection Eq. ( <ref type="formula">6</ref>). The final pose supervision loss is then constructed based on the predicted feature xj and the reprojected feature xj for each available camera pose at timestep t j , as visualized in Fig. <ref type="figure" target="#fig_1">4</ref>. As in the supervised setting of Eq. ( <ref type="formula" target="#formula_1">2</ref>), we use a truncated loss which excludes the loss contribution if the reprojected feature is outside of the event patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets We compare our proposed data-driven tracker on the commonly used Event Camera dataset <ref type="bibr" target="#b27">[30]</ref> (EC), which includes APS frames (24 Hz) and events with a resolution of 240?180, recorded using a DAVIS240C camera <ref type="bibr" target="#b3">[6]</ref>. Additionally, the dataset provides ground truth camera poses at a rate of 200 Hz from an external motion capture system. Moreover, to evaluate the tracking performance with a newer sensor setup, we test our method on the newly published Event-aided Direct Sparse Odometry dataset <ref type="bibr" target="#b18">[21]</ref> (EDS). Compared to EC, the EDS dataset contains higher resolution frames and events (640?480 pixels) captured with a beam splitter setup. Similar to the EC dataset, it includes ground truth poses at a rate of 150 Hz from an external motion capture system. Most scenes in both datasets are static since the primary purpose of EDS and EC is the evaluation of camera pose estimation. For the specific finetuning and testing sequence selection, we refer to the supplementary.</p><p>Evaluation To evaluate the different feature trackers, we first extract features for each sequence with a Harris Corner detector. Based on the initial feature set, each tested tracker predicts the feature displacements according to its specific update rate. Unfortunately, no ground truth feature tracks are available for EDS and EC. To evaluate the event-based feature trackers without ground truth, previous works used tracks predicted by the frame-based KLT tracker as ground truth. Instead, to increase the accuracy of KLT tracks, we use an evaluation scheme based on our proposed pose supervision method. Specifically, the ground truth tracks are obtained by triangulating KLT tracks using ground truth poses and reprojecting them afterward to each of the selected target frames. The triangulation of KLT tracks has the benefit that minor tracking errors of KLT are filtered out, leading to geometrically consistent ground truth tracks. To verify the proposed evaluation, we conducted an experiment in simulation in which ground truth feature tracks are available. In this simulated setup, we computed the Pearson correlation between the KLT reprojected error and the ground truth feature tracks, which was 0.716. This indicates a significant correlation between our proposed evaluation technique and ground truth feature tracks verifying the effectiveness of our evaluation technique.</p><p>Since each tested tracker has its update rate, we linearly interpolated all feature tracks to the ground truth pose timesteps in order to compute the evaluation metric. Furthermore, to effectively test the event-based tracking abilities of the methods, we do not update the feature templates during evaluation. In addition, we deactivate any terminal criterion and report the time until the feature exceeds a certain distance to the ground truth, known as the feature age. Instead of choosing one error threshold as done in previous work [4], we evaluate the resulting tracks for multiple error thresholds in a range from 1 to 31 pixels with a step size of 1 pixel. Thus, we do not report the endpoint error since we test each trajectory with different error thresholds, which effectively incorporates the distance error into the feature age. As a first performance metric, we compute the tracked feature age normalized by the ground truth track duration in order to account for different trajectory lengths. However, since some feature tracks are lost immediately in the beginning, we report the feature age of stable tracks, i.e., we discard feature tracks lost during the early phase of the sequence for the feature age computations. The second error metric accounts for the lost tracks by taking the ratio of stable tracks and ground truth tracks. This ratio is then multiplied by the feature age, which gives us the expected feature age as the second performance metric. This metric combines the quality and the number of feature tracks tracked by a method. For more information about the two performance metrics, we refer to the supplementary Training Schedule As mentioned in Sec. 3, we first train our models supervised on the Multiflow <ref type="bibr" target="#b16">[19]</ref> dataset on 30000 feature tracks in a continual learning fashion with a learning rate of 1?10 -4 to gradually adapt the network recurrence to longer trajectory lengths. Starting initially from 4 unroll steps, we progressively increase the number of unroll steps to 16 and then 24 after 80000 and 120000 training steps, respectively. After training on Multiflow, we finetune our model using our novel supervision method for 700 optimization steps with a reduced learning rate of 1 ? 10 -6 on specific training sequences of both datasets, which are not used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Results</head><p>Baselines We compare our method against the current state-of-the-art method EKLT <ref type="bibr" target="#b14">[17]</ref>, which extracts a template patch from a grayscale image for each feature and tracks the feature with events, similar to our tracker. As another tracker relying on grayscale template patches, we also run the ICP <ref type="bibr" target="#b21">[24]</ref> tracker used for event-based visual odometry. In addition, we evaluate against the pure eventbased trackers HASTE [4] and EM-ICP <ref type="bibr" target="#b43">[46]</ref>. For EKLT, HASTE, and EM-ICP, we adopted the publicly available code to run the experiments. The implementation of ICP was taken from a related work <ref type="bibr" target="#b9">[12]</ref>. The hyper-parameters of all methods were tuned for the specific datasets, which required multiple hours to achieve optimal performance.</p><p>EC Results On the commonly used event-based tracking benchmark, EC, our proposed data-driven method outperforms the other baselines in terms of non-zero feature age and expected feature age, see Tab. 1. The second best approach is EKLT, which tracked the features for a duration similar to our proposed method as represented by the non-zero feature age metric in Tab. 1. However, our method was able to track more features from the initial feature set as reported by the expected feature age. The higher ratio of successfully tracked features and the longer feature age makes our method better suited for downstream tasks such as pose estimation. The top row of Fig. <ref type="figure" target="#fig_2">5</ref> shows that our method produces a higher number of smooth feature tracks compared to the closest baselines EKLT and HASTE. As expected, a performance gap exists between pure event-based methods (HASTE, EM-ICP) and methods using grayscale images as templates (Ours, EKLT). This confirms the benefit of leveraging grayscale images to extract template patches, which are subsequently tracked by events.</p><p>EDS Results Similar to the performance on the EC dataset, our proposed method outperforms all of the existing trackers on the EDS dataset with an even larger margin in terms of both non-zero feature age and expected feature age as reported in Tab. 1. The significant performance boost confirms the capability of our data-driven methods to deal with high-resolution data in various 3D scenes with different lighting conditions and noise patterns. Since a beam splitter setup was used to record the data for the EDS dataset, there are misalignment artifacts between events and images, as well as low-light noise in the events due to the reduction of the incoming light. Nevertheless, our learned method is able to deal with these different noise sources and still predict smooth feature tracks for a large number of features, as shown in the middle and bottom row of Fig. <ref type="figure" target="#fig_2">5</ref>. For more qualitative examples, we refer to the supplementary. Finally, in addition to the performance gain, our method does not require hours of manual fine-tuning for transferring the tracker from small resolution to high resolution event Table <ref type="table">1</ref>. The performance of the evaluated trackers on the EDS and EC dataset are reported in terms of "Feature Age (FA)" of the stable tracks and the "Expected FA", which is the multiplication of the feature age by the ratio of the number of stable tracks over the number of initial features.  cameras with different contrast threshold settings. Runtime Comparison To employ a feature tracker in real-world applications, it is crucial to provide feature displacement updates with low latency. Therefore, we report the runtime of the different evaluated methods in terms of the real time factor, i.e., compute time divided by the time of the received data, versus tracking performance in Fig. <ref type="figure">6</ref>. It should be noted that most of the evaluated trackers were not implemented for run time efficiency and thus are coded in different programming languages, which makes a fair comparison hard. Moreover, we tuned all the methods with a focus on the tracking performance, which explains the high runtime of EKLT since we significantly increased the number of optimization iterations. Nevertheless, the runtime comparison of the different methods still provides a rough picture of the inference speed of each method. In the case of HASTE, we additionally report the runtime for an ideal HASTE implementation, named HASTE* in Fig. <ref type="figure">6</ref>. The ideal HASTE* assumes perfect parallelization of the current code framework of HASTE, which tracks each feature sequentially. Even without optimizing the code for deploy-Figure <ref type="figure">6</ref>. The two plots show the tracking performance in terms of expected feature age in relation to the real-time factor, which is the ratio of compute time over track time. Thus, the top left corner represents the goal. Additionally to the existing implementation of HASTE, we also report the ideal HASTE*, which assumes perfect parallelization for processing all feature tracks. ment, our method achieves close to real-time performance on EC and is the fastest method on EDS while having a significantly higher tracking performance. The fast inference of our method can be explained by the batch-wise processing and the highly parallelized framework for deep learning architectures. This shows the potential of our method for real-world applications constrained by latency requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EDS EC</head><note type="other">Method</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Combination of Events and Frames</head><p>In a step to combine the contextual information of grayscale images and the high-latency information from events, we extended our event-based tracker using the popular KLT tracker for frames. Specifically, we use our event tracker to track features during the blind time between two frames and use the displacement prediction of our tracker as an initial guess for the KLT tracker once a new frame arrives. This has the benefit of effectively mitigating the negative effects of large baselines between two frames caused by high-speed motion. Additionally, the combination with our event tracker provides feature positions for the time in be- tween two frames, significantly increasing the frequency of feature position updates. On the other side, the KLT tracker can correct the feature position once reliable frame information is available. As used for the ground truth creation based on the camera poses, we use a KLT tracker with three hierarchical scales to cope with larger motion. We compare the combination of our method and the KLT tracker (ours+KLT) against the pure KLT tracker for different pixel motions between frames, as reported in Fig. <ref type="figure" target="#fig_3">7</ref> The different pixel motions are achieved by skipping frames in a sequence of the EC dataset, which corresponds to increasing the pixel motion between two frames. As can be seen in Fig. <ref type="figure" target="#fig_3">7</ref>, the combination of ours and KLT performs comparably to a pure KLT tracker for small pixel displacement between frames. However, with increasing pixel motion, the initial guess provided by our method helps the KLT tracker to track features over a longer time duration than a KLT tracker alone. In addition, our event-based tracker can provide robust feature tracks during periods of highspeed motion in which the frames suffer from motion blur. This can be qualitatively observed in Fig. <ref type="figure">1</ref>, which shows smooth features tracks predicted by our event-based tracker on a motion blurred frame due to high-speed motion. This high-rotational motion sequence was recorded by us with a beam splitter setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations</head><p>To test the specific contribution of each introduced network block, we perform several ablation experiments based on the reference model, which represents our model without the frame attention module, see Tab. 2. As verified by the performance drop (w/o augmentation), the augmentations during the training on synthetic data significantly boost the zero-shot transfer from synthetic to real-world data. Furthermore, the recurrence in the feature encoder leads to longer feature age (w/recurrence), which is also achieved on a smaller scale by introducing the correlation map (w/o correlation). While there is no improvement on the EC dataset, our proposed frame attention module significantly improves the performance on the challenging sequences of EDS. This performance increase confirms the benefit of sharing information between similar feature tracks for challenging scenarios. By adapting our network based on the frame attention module (Ref+Frame Attention) to real data using our self-supervision scheme, we achieve the highest tracking performance. Finally, the frame attention module relies on state variables (w/o state) to fully exploit the potential of sharing information across features in a frame. For more ablations regarding the input representation and specific augmentation parameters, we refer to the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Future Work &amp; Limitations</head><p>Since the EC and EDS datasets were recorded to benchmark pose estimation algorithms, they only contain static scenes. Thus, we did not evaluate how our method, and especially our frame attention module performs in scenes with dynamic objects. Nevertheless, we believe that our frame attention module can be useful for other trackers using event or standard cameras. Finally, our method relies on the quality of the feature detection in grayscale images, which can suffer in challenging scenarios. However, our self-supervision strategy opens up the possibility of finetuning also feature detectors for event cameras to increase the robustness of feature detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented the first data-driven feature tracker for event cameras, which leverages low-latency events to track features detected in a grayscale frame. With our novel frame attention module, which fuses information across feature tracks, our tracker outperforms state-of-art methods on two datasets while being faster in terms of inference time. Furthermore, our proposed method does not require intensive manual parameter tuning and can be adapted to new event cameras with our self-supervision strategy. Finally, we can combine our event-based tracker with a KLT tracker to predict stable tracks in challenging scenarios. This combination of standard and event cameras paves the path for the concept of sparingly triggering frames based on the tracking quality, which is a critical tool for future applications where runtime and power consumption are essential. Supplementary: Data-driven Feature Tracking for Event Cameras</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Dataset Split</head><p>We use five sequences from the Event Camera dataset <ref type="bibr" target="#b27">[30]</ref> (EC) and four sequences from the Event-aided Direct Sparse Odometry dataset <ref type="bibr" target="#b18">[21]</ref> (EDS) as test sequences. For fine-tuning, our pose supervision strategy is performed on five sequences from the EC and one sequence from the EDS dataset since EDS does not contain many sequences with ground truth pose in well-lit conditions. The overview of the test and fine-tuning sequences is shown in Tab. 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Network Architecture Details</head><p>Tab. 4 shows the architectural details of our proposed network, which consists of a feature network and our proposed frame attention module. In the first step, two patch encoders inside the feature network process the event and the grayscale patches. After the correlation and the concatenation of the feature maps from both patch networks, a joint encoder refines the correlation map and introduces temporal information sharing through a ConvLSTM layer. Finally, the frame attention module processes each feature in one frame using shared linear layers and one global multi-head attention over all features in a frame. We refer to Fig. <ref type="figure" target="#fig_0">2</ref> in the main paper for the network overview.</p><p>Table <ref type="table">4</ref>. Network architecture. Each convolution layer is followed by LeakyReLU and BatchNorm layers whereas the linear layers are followed by LeakyReLu layers. For the upsampling layers (Up), we use bilinear interpolation. The three numbers after each convolution layer indicate the two kernel dimensions and the output channel dimension. In the case of the linear layer, the single number stands for the output channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Spatial Size</head><p>Feature Network (2?Patch Encoders + Joint Encoder) </p><formula xml:id="formula_5">2? Conv2D 1?1?32 31?31 2? Conv2D 5?5?64 23?23 2? Conv2D 5?5?128 15?15 2? Conv2D 3?3?256 5?5 2? Conv2D 1?1?384 1?1 2? Conv2D 1?1?384 1?1 Up + Conv2D 1?1?384 5?5 Conv2D 3?3?384 5?5 Up + Conv2D 1?1?384 15?15 Conv2D 3?3?384 15?15 Up + Conv2D 1?1?384 23?23 Conv2D 3?3?384 23?23 Up + Conv2D 1?1?384 31?31 Conv2D 3?3?384 31?31 2? Conv2D 3?3?384 31?31 Correlation Layer 31?31 2? Conv2D 3?3?128 31?31 2? Conv2D 3?3?64 15?15 2? Conv2D 3?3?128 7?7 ConvLSTM 3?3?128 7?7 2? Conv2D 3?3?256 3?3 Conv2D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Quantitative Results &amp; Tracking Metrics</head><p>We report for each test sequence from the EC and EDS dataset the expected feature age in Tab. 5, the feature age in Tab. 6, the inlier ratio in Tab. 7 and the normalized tracking error in Tab. 8. For the normalized tracking error, we terminate the track if the distance to the ground truth exceeds 5 pixels, as done in <ref type="bibr">[4]</ref>. However, it is not obvious how to compute this metric if the tracking error is higher than 5 pixels directly after the initialization, as it occurred for the baseline methods in Tab. 8. Furthermore, this metric does not consider the duration of the predicted tracks, e.g., one feature can be tracked for a short time duration with a small tracking error, which would lead to a small normalized tracking error. In contrast, a feature tracked for a long time horizon but with a higher distance to the ground truth will be assigned a higher tracking error. This example shows that the normalized tracking error on its own is not necessarily a good metric to evaluate stable and long feature tracks. Thus, we decided to report the expected feature age as a metric since it considers the tracking duration and the number of tracked features. Moreover, the expected feature age is computed over a range of termination thresholds with respect to the ground truth, which effectively eliminates this hyperparameter for the metric computation. Specifically, the expected feature age represents the multiplication of the normalized feature age with the fraction of successfully predicted tracks over the number of given feature locations, defined as inlier ratio. A feature is defined to be tracked successfully if the predicted feature location at the second timestep after initialization is in the termination threshold to the ground truth location. The normalized feature age is computed for the successfully tracked features based on the division of the time duration until the predicted feature exceeds the termination threshold to the ground truth location by the duration of the ground truth tracks. Because of the range of termination thresholds and the consideration of the number of successfully tracked features, the expected feature age represents an expressive and objective metric for reporting the tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Input Event Representation</head><p>To provide the events in a patch as input to our network, we first convert them to a dense event representation. Specifically, we use a maximal timestamp version of SBT <ref type="bibr" target="#b37">[40]</ref>, named SBT-Max, which consists of five temporal bins for positive and negative polarity leading to 10 channels. In each temporal bin, we assign to each pixel coordinate the relative timestamp of the most recent event during the time interval of the temporal bin. For the EC and EDS dataset, we convert events inside a 10 ms and 5 ms window, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Additional Ablation Experiments</head><p>In addition to the ablation experiments reported in Tab. 2 in the main paper, we ablated the event input representation as well as the augmentation parameters used during training. Due to time reasons, we performed the following ablation experiments by training the reference model, which does not include the frame attention module, for 70000 steps instead of 140000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1.">Input Representations</head><p>The input event representation to an event-based network is an important consideration. Ideally, we aim to preserve as much of the spatiotemporal information as possible while minimizing the computational overhead of representation generations. We train the reference network with different representations: voxel grids <ref type="bibr" target="#b44">[47]</ref>, Stacking Based on Time (SBT) <ref type="bibr" target="#b37">[40]</ref>, a non-normalized version of SBT (SBTNo Norm) and a maximal timestamp version of SBT we call SBT-Max where each pixel is assigned the timestamp of the most recent event. The results are shown in Tab. 9. While many event-based networks have demonstrated promising results with voxel grids, their interpolation-based construction is computationally expensive. In contrast, SBT is a simpler, synchronous event representation that is more efficient. Each pixel simply accumulates or "stacks" incoming events. We find that SBT achieves competitive Expected FA compared to voxel grids on nearly all sequences. However, the performance of SBT degrades significantly without normalizing based on the number of events in the frame. In contrast to normalizing by the number of events, SBT-Max is normalized using the duration of the time window. In practice, the statistic-free normalization procedure of SBT-Max means that events outside the neighborhoods of tracked features can be ignored. Because of this deployment advantage and the competitive performance despite its more simplistic normalization, we select SBT-Max as event representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2.">Augmentation Parameters</head><p>To validate the utility of our augmentation strategy, we train the reference network with different augmentation parameters. In Tab. 10, we present the experimental results for using rotations (R) of up to ?30 ? , scaling (S) of up to ?10%, and translations (T) of up to ?5px. The default training settings use rotations of up to ?15 ? , scaling of up to ?10%, and translations of up to ?3px. Without augmentation, we observe significant degradation on both datasets. The benefit of additional translation augmentation is inconclusive, given the degradation on EC and improvement on EDS. Lastly, with increased rotation augmentation, we observe that the performance improves on average for both datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. As shown in (a), our event tracker takes as input a reference patch P0 in a grayscale image I0 and an event patch Pj constructed from an event stream Ej at timestep tj and predicts the relative feature displacement ? fj . Each feature is individually processed by a feature network, which uses a ConvLSTM layer with state F to process a correlation map Cj based on a template feature vector R0 and the pixel-wise feature maps of the event patch. To share information across different feature tracks, our novel frame attention module (b) fuses the processed feature vectors for all tracks in an image using self-attention and a temporal state S, which is used to compute the final displacement ? fj .</figDesc><graphic url="image-2.png" coords="4,50.11,72.00,494.99,166.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure4. To adapt our tracker to real event data, our selfsupervised loss computes a triangulated point based on the predicted track, and the camera poses. The 3D point is then reprojected to each camera plane, and the L1-distance j between reprojected and predicted point is used as a supervision signal.</figDesc><graphic url="image-4.png" coords="5,315.61,72.00,222.74,131.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Qualitative tracking predictions (blue) and ground truth tracks (green) for the EC dataset (top) and EDS dataset (middle / bottom). Our method predicts more accurate tracks for a higher number of initial features.</figDesc><graphic url="image-5.png" coords="7,51.91,223.65,232.64,157.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The tracking performance of KLT and our tracker combined with KLT (Ours+KLT) in relation to pixel motion per frame. In combination, our event-based tracker can successfully help KLT predict larger displacement while KLT can refine the predictions of our tracker.</figDesc><graphic url="image-7.png" coords="8,-247.61,72.00,500.01,157.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>This work was supported by the Swiss National Science Foundation through the National Centre of Competence in Research (NCCR) Robotics (grant number 51NF40 185543), and the European Research Council (ERC) under grant agreement No. 864042 (AGILE-FLIGHT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablations experiments on the EDS and EC dataset.</figDesc><table><row><cell>Expected FA ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Test and fine-tuning sequences for the EC and EDS dataset.</figDesc><table><row><cell></cell><cell cols="2">Dataset Sequence Name</cell><cell>Frames</cell></row><row><cell></cell><cell></cell><cell>Shapes Translation</cell><cell>8-88</cell></row><row><cell></cell><cell></cell><cell>Shapes Rotation</cell><cell>165-245</cell></row><row><cell></cell><cell>EC</cell><cell>Shapes 6DOF</cell><cell>485-454</cell></row><row><cell>Test</cell><cell></cell><cell>Boxes Translation Boxes Rotation</cell><cell>330-410 198-278</cell></row><row><cell></cell><cell></cell><cell>Peanuts Light</cell><cell>160-386</cell></row><row><cell></cell><cell>EDS</cell><cell>Rocket Earth Light Ziggy In The Arena</cell><cell>338-438 1350-1650</cell></row><row><cell></cell><cell></cell><cell>Peanuts Running</cell><cell>2360-2460</cell></row><row><cell></cell><cell></cell><cell>boxes hdr</cell><cell>all</cell></row><row><cell>Fine-Tuning</cell><cell>EC</cell><cell>calibration poster 6dof poster rotation poster translation</cell><cell>all all all all</cell></row><row><cell></cell><cell>EDS</cell><cell>all characters</cell><cell>all</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The performance of our proposed and the baseline trackers on the EDS and EC dataset in terms of Expected Feature Age.</figDesc><table><row><cell>Expected FA ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The performance of our proposed and the baseline trackers on the EDS and EC dataset in terms of Feature Age FA.</figDesc><table><row><cell>Feature Age (FA) ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The performance of our proposed and the baseline trackers on the EDS and EC dataset in terms of Inlier Ratio.</figDesc><table><row><cell>Inlier Ratio ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>The performance of our proposed and the baseline trackers on the EDS and EC dataset in terms of Track Normalized Error.</figDesc><table><row><cell>Track Normalized Error ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>The performance of the reference model when trained with different input event representations.</figDesc><table><row><cell>Expected FA ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>The performance of the reference model when trained with different augmentation parameters. Ignacio Alzugaray and Margarita Chli. Asynchronous corner detection and tracking for event cameras in real time. IEEE Robot. Autom. Lett., 3(4):3177-3184, Oct. 2018. 2 [4] Ignacio Alzugaray and Margarita Chli. HASTE: multi-</figDesc><table><row><cell>Expected FA ?</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Direct linear transformation from comparator co-ordinates into object space coordinates in close-range photogrammetry. Photogrammetric engineering &amp; remote sensing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Yousset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hauck</forename><surname>Abdel-Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Michael Karara</surname></persName>
		</author>
		<author>
			<persName><surname>Hauck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ACE: An efficient Hypothesis Asynchronous Speeded-up Tracking of Events</title>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Alzugaray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margarita</forename><surname>Chli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020. 2, 6, 7, 10, 11</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A method for registration of 3d shapes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<idno>1992. 2</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="39" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A 240x180 130dB 3?s latency global shutter spatiotemporal vision sensor</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Brandli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2333" to="2341" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Asynchronous tracking-by-detection on adaptive time surfaces for event-based object tracking</title>
		<author>
			<persName><forename type="first">Haosheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiangqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="473" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting stable keypoints from events through image gradient prediction</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Chiberre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1387" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Long-lived accurate keypoints in event streams</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Chiberre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<idno>abs/2209.10385, 2022. 2</idno>
		<imprint/>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Eventbased feature tracking in continuous time with sliding window optimization</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Klenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<idno>ArXiv, abs/2107.04536, 2021. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dardelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sio-Hoi</forename><surname>Ieng</surname></persName>
		</author>
		<title level="m">An Event-by-Event Feature Detection and Tracking Invariant to Motion Direction and Velocity</title>
		<imprint>
			<date type="published" when="2002">11 2021. 2</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Standard and event cameras fusion for feature tracking</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computing Machinery</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FlowNet: Learning Optical Flow with Convolutional Networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-temporalresolution object detection and tracking using images and events</title>
		<author>
			<persName><forename type="first">Zaid</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shair</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><forename type="middle">A</forename><surname>Rawashdeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Event-based vision: A survey</title>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eventbased, 6-DOF camera tracking from photometric depth maps</title>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">E A</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2402" to="2412" />
			<date type="published" when="2001">Oct. 2018. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">EKLT: Asynchronous Photometric Feature Tracking Using Events and Frames</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020">2020. 1, 2, 6, 7, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">E-RAFT: Dense Optical Flow from Event Cameras</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Millhaeusler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno>IEEE. 3</idno>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dense Continuous-Time Optical Flow from Events and Frames</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manasi</forename><surname>Muglikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories</title>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Event-aided Direct Sparse odometry</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Hidalgo-Carri?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">eCDT: Event Clustering for Simultaneous Feature Detection and Tracking</title>
		<author>
			<persName><forename type="first">Sumin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyungtae</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Myung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</title>
		<author>
			<persName><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="7" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Low-latency visual odometry using eventbased feature tracks</title>
		<author>
			<persName><forename type="first">Beat</forename><surname>Kueng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS)</title>
		<imprint>
			<date type="published" when="2016">2016. 2, 6, 7, 11</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust event-based object tracking combining correlation filter and cnn representation</title>
		<author>
			<persName><forename type="first">Hongmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">82</biblScope>
			<date type="published" when="2003">10 2019. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EDFLOW: Event Driven Optical Flow Camera With Keypoint Detection and Adaptive Block Matching</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5776" to="5789" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf. Artificial Intell. (IJCAI)</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speed invariant time surface for learning to detect corner points with eventbased cameras</title>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Manderscheid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Migliore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and SLAM</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Asynchronous event-based visual shape tracking for stable haptic feedback in microrobotics</title>
		<author>
			<persName><forename type="first">Zhenjiang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Bolopion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo?l</forename><surname>Agnus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">St?phane</forename><surname>R?gnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Garrick Orchard, and Cheng Xiang. Long-term object tracking with a moving event camera</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><forename type="middle">Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Mach. Vis. Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structure-from-Motion Revisited</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sch?nberger</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust feature tracking in dvs event stream using bezier mapping</title>
		<author>
			<persName><forename type="first">Hochang</forename><surname>Seok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2002">March 2020. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RAFT: Recurrent All-Pairs Field Transforms for Optical Flow</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<editor>
			<persName><surname>Editors</surname></persName>
		</editor>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science and Business Media Deutschland GmbH</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep Patch Visual Odometry</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lahav</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.04726</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003">October 2021. 3</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Eventbased high dynamic range image and very high frame rate video generation using conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yo-Sung</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning Feature Descriptors Using Camera Pose Supervision</title>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="757" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spiking transformers for event-based single object tracking</title>
		<author>
			<persName><forename type="first">Jiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object tracking by jointly exploiting frame and event domain</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vi-sion (ICCV)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021-10">oct 2021</date>
			<biblScope unit="page" from="13023" to="13032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Patch2Pix: Epipolar-Guided Pixel-Level Correspondences</title>
		<author>
			<persName><forename type="first">Qunjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems<address><addrLine>Pittsburgh, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">6 2018. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Event-based feature tracking with probabilistic data association</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Atanasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<imprint>
			<date type="published" when="2017">2017. 2, 3, 6, 7, 11</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised Event-Based Learning of Optical Flow, Depth, and Egomotion</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
