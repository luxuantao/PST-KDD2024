<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-Evolving Meta-Learning for Low-Resource Medical Dialogue Generation</title>
				<funder ref="#_yJ7YSWq">
					<orgName type="full">Zhijiang Lab&apos;s Open Fund</orgName>
				</funder>
				<funder ref="#_qCm3Fqj">
					<orgName type="full">Nature Science Foundation of Shenzhen</orgName>
				</funder>
				<funder ref="#_Mpt3wJH">
					<orgName type="full">Applied Basic Research (Regional Joint Fund-Key)</orgName>
				</funder>
				<funder ref="#_AZQm3mb #_dz3sCVJ">
					<orgName type="full">National Natural Science Foundation of China</orgName>
					<orgName type="abbreviated">NSFC</orgName>
				</funder>
				<funder>
					<orgName type="full">CSIG Young Fellow Support Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Lin</surname></persName>
							<email>shuailin97@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
							<email>pzhou@salesforce.com</email>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">DarkMatter AI Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruihui</forename><surname>Zhao</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Tencent Jarvis Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">DarkMatter AI Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-Evolving Meta-Learning for Low-Resource Medical Dialogue Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human doctors with well-structured medical knowledge can diagnose a disease merely via a few conversations with patients about symptoms. In contrast, existing knowledgegrounded dialogue systems often require a large number of dialogue instances to learn as they fail to capture the correlations between different diseases and neglect the diagnostic experience shared among them. To address this issue, we propose a more natural and practical paradigm, i.e., low-resource medical dialogue generation, which can transfer the diagnostic experience from source diseases to target ones with a handful of data for adaptation. It is capitalized on a commonsense knowledge graph to characterize the prior disease-symptom relations. Besides, we develop a Graph-Evolving Meta-Learning (GEML) framework that learns to evolve the commonsense graph for reasoning disease-symptom correlations in a new disease, which effectively alleviates the needs of a large number of dialogues. More importantly, by dynamically evolving disease-symptom graphs, GEML also well addresses the realworld challenges that the disease-symptom correlations of each disease may vary or evolve along with more diagnostic cases. Extensive experiment results on the CMDD dataset and our newly-collected Chunyu dataset testify the superiority of our approach over state-of-the-art approaches. Besides, our GEML can generate an enriched dialogue-sensitive knowledge graph in an online manner, which could benefit other tasks grounded on knowledge graph.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical dialogue system (MDS) aims to converse with patients to inquire additional symptoms beyond their selfreports and make a diagnosis automatically, which has gained increasing attention <ref type="bibr" target="#b16">(Lin et al. 2019;</ref><ref type="bibr" target="#b33">Wei et al. 2018;</ref><ref type="bibr" target="#b35">Xu et al. 2019)</ref>. It has a significant potential to simplify the diagnostic process and relieve the cost of collecting information from patients <ref type="bibr" target="#b12">(Kao, Tang, and Chang 2018)</ref>. Moreover, preliminary diagnosis reports generated by MDS may assist doctors to make a diagnosis more efficiently. Because of these considerable benefits, many researchers devote substantial efforts to address critical sub-problems in MDS, such as natural language understanding <ref type="bibr" target="#b28">(Shi et al. 2020;</ref><ref type="bibr" target="#b16">Lin et al. 2019)</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source diseases Target diseases</head><p>Figure <ref type="figure">1</ref>: Statistics of 15 diseases in our newly collected Chunyu dataset from the real world. One can observe the notable data-imbalance phenomenon of different diseases. Thus it is highly desirable to study how to transfer the diagnostic experience among diseases. dialogue policy learning <ref type="bibr" target="#b33">(Wei et al. 2018)</ref>, dialogue management <ref type="bibr" target="#b35">(Xu et al. 2019)</ref>, and make promising progress to build a satisfactory MDS.</p><p>Medical dialogue generation (MDG), which generates responses in natural language to request additional symptoms or make a diagnosis, is critical in MDS but rarely studied. Conventional generative dialogue models often employ neural sequence modeling <ref type="bibr" target="#b30">(Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b31">Vaswani et al. 2017)</ref> and cannot be applied to the medical dialogue scenario directly in absence of medical knowledge. Recently, large-scale pre-training language models <ref type="bibr" target="#b3">(Devlin et al. 2019;</ref><ref type="bibr" target="#b25">Radford et al. 2019;</ref><ref type="bibr" target="#b29">Song et al. 2019</ref>) over unsupervised corpora have achieved significant success. However, fine-tuning such large language models in the medical domain requires sufficient task-specific data <ref type="bibr" target="#b1">(Bansal, Jha, and McCallum 2019;</ref><ref type="bibr" target="#b5">Dou, Yu, and Anastasopoulos 2019</ref>) so as to learn the correlations between diseases and symptoms. Unfortunately, as depicted in Fig. <ref type="figure">1</ref>, there are a large portion of diseases that only have a few instances in practice, which means that newly-coming diseases in the realistic diagnosis scenario are often under low-resource conditions. Therefore, it is highly desirable to transfer the diagnostic experience from high-resource diseases to others of data scarcity. Besides, existing knowledge-grounded approaches <ref type="bibr" target="#b17">(Liu et al. 2018;</ref><ref type="bibr" target="#b15">Lian et al. 2019</ref>) may fail to perform such transfer well, as they only learn one unified model for all diseases and ignore the specificity and relationships of different diseases. Finally, in practice, the disease-symptom relations of each disease may vary or evolve along with more cases, which is also not considered in prior works.</p><p>Contributions. To address the above challenges, we first propose an end-to-end dialogue system for the low-resource medical dialogue generation. This model integrates three components seamlessly, a hierarchical context encoder, a meta-knowledge graph reasoning (MGR) network and a graph-guided response generator. Among them, the context encoder encodes the conversation into hierarchical representations. For MGR, it mainly contains a parameterized meta-knowledge graph, which is initialized by a prior commonsense graph and characterizes the correlations among diseases and symptoms. When fed into the context information, MGR can adaptively evolve its meta-knowledge graph to reason the disease-symptom correlations and then predict related symptoms of the patient in the next response to further determine the disease. Finally, the response generator generates a response for symptoms request under the guidance of the meta-knowledge graph.</p><p>The second contribution is that we further develop a novel Graph-Evolving Meta-Learning (GEML) framework to transfer the diagnostic experience in the low-resource scenario. Firstly, GEML trains the above medical dialogue model under the meta-learning framework. It regards generating responses to a handful of dialogues as a task and learns a meta-initialization for the above dialogue model that can fast adapt to each task of the new disease with limited dialogues. In this way, the learnt model initialization contains sufficient meta-knowledge<ref type="foot" target="#foot_0">1</ref> from all source diseases and can serve as a good model initialization to quickly transfer meta-knowledge to a new disease. More importantly, GEML also learns a good parameterized meta-knowledge graph in the MGR module to characterize the disease-symptom relationships from source diseases. Concretely, under the meta learning framework, for each disease, GEML enriches the meta-knowledge graph via constructing a global-symptom graph from the online dialogue examples. In this way, the learnt meta-knowledge graph can bridge the gap between the commonsense medical graph and the real diagnostic dialogues and thus can be fast evolved for the new target disease. Thanks to graph evolving, the dialogue model can request patients for underlying symptoms more efficiently and thus improve the diagnostic accuracy. Besides, GEML can also well address the real-world challenge that the disease-symptom correlations could vary along with more cases, since the meta-knowledge graph is trainable based on collected dialogue examples.</p><p>Finally, we construct a large medical dialogue dataset, called Chunyu<ref type="foot" target="#foot_1">2</ref> . It covers 15 kinds of diseases and 12,842 dialogue examples totally, and is much larger than the existing CMDD medical dialogue dataset <ref type="bibr" target="#b16">(Lin et al. 2019)</ref>. The more challenging benchmark can better comprehensively evaluate the performance of medical dialogue systems. Extensive experimental results on both datasets demonstrate the superiority of our method over the state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Medical Dialogue System (MDS). Recent research on MDS mostly focus on the natural language understanding (NLU) or dialogue management (DM) with the line of pipeline-based dialogue system. Various NLU problems have been studied to improve the MDS performance, e.g., entity inference <ref type="bibr">(Du et al. 2019b;</ref><ref type="bibr" target="#b16">Lin et al. 2019;</ref><ref type="bibr" target="#b18">Liu et al. 2020)</ref>, symptom extraction <ref type="bibr">(Du et al. 2019a</ref>) and slot-filling <ref type="bibr" target="#b28">(Shi et al. 2020)</ref>. For medical dialogue management, most works <ref type="bibr" target="#b4">(Dhingra et al. 2017;</ref><ref type="bibr" target="#b14">Li et al. 2017</ref>) focus on reinforcement learning (RL) based task-oriented dialogue system. <ref type="bibr" target="#b33">Wei et al. (2018)</ref> proposed to learn dialogue policy with RL to facilitate automatic diagnosis. <ref type="bibr" target="#b35">Xu et al. (2019)</ref> incorporated the knowledge inference into dialogue management via RL. However, no attention has been paid to the medical dialogue generation, which is a critical recipe in MDS. Differing from existing approaches, we investigate to build an end-to-end graph-guided medical dialogue generation model directly. Knowledge-grounded Dialog Generation. Recently, dialogue generation grounded on extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be derived from or open-domain knowledge graphs <ref type="bibr" target="#b38">(Zhou et al. 2018;</ref><ref type="bibr" target="#b36">Zhang et al. 2020;</ref><ref type="bibr" target="#b21">Moon et al. 2019)</ref> or retrieved from unstructured documents <ref type="bibr" target="#b15">(Lian et al. 2019;</ref><ref type="bibr" target="#b37">Zhao et al. 2019;</ref><ref type="bibr" target="#b13">Kim, Ahn, and Kim 2020)</ref>. Different from them, our MDG model is built on the dedicated medical-domain knowledge graph and further require evolving it to satisfy the need for the real-world diagnosis. Meta-Learning. By meta-training a model initialization from training tasks with the ability of fast adaptation to new tasks, meta-learning <ref type="bibr" target="#b8">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b39">Zhou et al. 2019</ref><ref type="bibr" target="#b40">Zhou et al. , 2020) )</ref> has achieved promising results in many NLP areas, such as machine translation <ref type="bibr" target="#b10">(Gu et al. 2018)</ref>, task-oriented dialogues <ref type="bibr" target="#b24">(Qian and Yu 2019;</ref><ref type="bibr" target="#b20">Mi et al. 2019), and</ref><ref type="bibr">text classification (2019;</ref><ref type="bibr">2019)</ref>. But there is the few effort to devote meta-learning into MDS, which requires grounding on the external medical knowledge and reasoning for disease-symptom correlations. In this work, we employ the Reptile <ref type="bibr" target="#b22">(Nichol, Achiam, and Schulman 2018)</ref>, one firstorder model-agnostic meta learning approach, because of its efficiency and effectiveness, and enhance it with the metaknowledge graph reasoning and evolving.</p><p>3 Task Definition: Low-Resource MDG Grounded on the external medical knowledge graph A, the medical dialogue generation models take the dialogue context U = {u 1 , . . . , u t-1 } as input and aim to (1) generate the next response R = u t and (2) predict the disease or symptom entity E = e t appearing in the next response as:</p><formula xml:id="formula_0">f ? (R, E|U, A; ?) = p (u t , e t |u 1:t , A; ?) ,<label>(1)</label></formula><p>Given the abundant dialogue examples of K different source diseases S k , the task of low-resource MDG require to obtain a good model initialization during meta-training process:</p><formula xml:id="formula_1">? meta : (U, A) ? S k ? (R source , E).</formula><p>(2)</p><p>For the adaptation to the new target disease T , we fine-tune the model ? meta with minimal dialogue examples (e.g., 1% ? 10% of the source disease) and require the induced model ? target to perform well in the target disease:</p><formula xml:id="formula_2">? target : (U, A) ? T ? (R target , E).</formula><p>(3)</p><p>4 End-to-End Medical Dialogue Model</p><p>In this section, we elaborate our end-to-end dialogue model whose framework is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The proposed approach integrates three components seamlessly, including hierarchical context encoder, meta-knowledge graph reasoning (MGR) and graph-guided response generator. Concretely, the context encoder first encodes the conversation history into hierarchical context representations. Then MGR incorporates the obtained representations into the knowledge graph reasoning process for the comprehension of the disease-symptom correlations. Finally, the graph-guided decoder generates informative responses via a well-designed copy mechanism over graph entity nodes. We will introduce them in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hierarchical Context Encoder</head><p>We first utilize a hierarchical context encoder <ref type="bibr" target="#b27">(Serban et al. 2016)</ref> to encode the dialogue history and obtain the hierarchical hidden representations of the context. Formally, given a dialogue context U = (u 1 , . . . , u l ), the hierarchical context encoder first exploits a long short term memory (LSTM) network to encode each utterance into a hidden representation:</p><formula xml:id="formula_3">h u i = LSTM ?u (e i 1 , . . . , e i j , . . . , e i li ),<label>(4)</label></formula><p>where e i j is the embedding of the j-th token in the i-th utterance. Then these hidden representations {h u i , i = 1, ? ? ? , l} of utterances are fed into another LSTM to obtain the representation of the entire dialogue history as</p><formula xml:id="formula_4">h dial = LSTM ? d (h u 1 , . . . , h u j , . . . , h u l ).<label>(5)</label></formula><p>After obtaining utterance-level and dialogue-level representations, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, we use h u i to initialize the utterance node features of the knowledge graph in Sec. 4.2, and then adopt h dial as the initial state of the decoder LSTM in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Meta-Knowledge Graph Reasoning</head><p>Based on the obtained utterance representation h u i , we need to learn the disease-symptom correlations and further inquiry the patient the existence of related symptoms to verify. To this end, we devise a meta-knowledge graph reasoning (MGR) network to learn and reason the above correlations. In practice, one often has a prior commonsense disease-symptom graph which roughly contains such correlations, e.g., cold being indicated by the symptom cough. Our MGR aims to (1) reason the correlations over diseases and symptoms through conversations with patients, (2) predict the possible symptoms in the next inquiry/response to the patient, (3) evolve this commonsense disease-symptom graph into a meta-knowledge graph with the graph evolving meta-learning (GEML) framework. Here we focus on introducing the first two points and present our GEML in Sec. 5.</p><p>In practice, the commonsense disease-symptom graph can be derived from the Chinese Symptom Library in OpenKG<ref type="foot" target="#foot_2">3</ref> . The library contains a huge amount of triples, e.g., (Diarrhea, related symptom, Gastroenteritis). Formally, we denote the commonsense graph as G = (V e , A, X ), where V e = {v e 1 , . . . , v e m } is the set of entity nodes, A is the corresponding adjacency matrix and X ? R |V e |?F is the node feature matrix (F is the number of features in each node). In the graph G, each entity node v e i ? V e denotes a symptom or disease. The feature vector of each entity node, i.e., each row of the feature matrix X , is trainable. Besides, we have a utterance node set denoted as V u = {v u 1 , . . . , v u l }, where the input feature of each utterance node v u i is initialized by the representation h u i obtained in Eqn. (4). To incorporate the context information into the knowledge graph reasoning, we connect each utterance node with all entity nodes it included. Now we introduce the graph reasoning process over diseases and symptoms. To boost the information propagation among entity nodes, we build a meta-knowledge graph where each entity node indicates a disease or symptom. Inspired by the graph attention network <ref type="bibr" target="#b32">(Veli?kovi? et al. 2018)</ref>, we devise the meta-knowledge graph reasoning (MGR) network that consists of two graph reasoning layers. In the first layer, entity nodes that occur in the dialogue history are activated by aggregating the information from their corresponding utterance nodes. Then in the second layer, these activated entity nodes diffuse the information to their neighborhood nodes for the correlation reasoning. Next we present the single graph reasoning layer that used to construct the MGR (through stacking this layer). Let N i be the neighbor set of the node i according to the adjacency matrix X . With the input feature h e j of some neighborhood nodes j ? N i , the graph reasoning layer updates the representation of node i as:</p><formula xml:id="formula_5">h e i = ? j?Ni ? ij W 0 h e j ? ij = softmax j (e ij ) = exp(e ij )/ k?Ni exp(e ik )<label>(6)</label></formula><p>where W 0 ? R F ?F is a weight matrix and e ij is the attention coefficient that indicates the importance of entity node j to node i. Following <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref>, the attention coefficient e ij is computed as</p><formula xml:id="formula_6">e ij = Sigmoid(a T W 1 [h e i ||h e j ]),<label>(7)</label></formula><p>where a ? R H?1 is a trainable vector, W 1 ? R H?2F is a weight matrix and || indicates the concatenation. Note we inject the graph structure (i.e. the adjacency matrix A) into the graph reasoning layer as we only compute the e ij for neighborhood nodes j of i. In Sec. 5.2, we will elaborate how to evolve the meta-knowledge graph structure in a metalearning paradigm. By stacking two graph reasoning layers, each entity node could grasp enough information from other related nodes. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, we then feed the final entity node representations{h e i , i = 1, ? ? ? , m} into the response generator to infer possible entities in the next-turn response. To this end, we introduce the entity prediction task beyond response generation. Concretely, we feed the final  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph-guided Response Generator</head><p>To incorporate the knowledge graph into the generation, we devise a graph-guided response generator with a copy mechanism adapted from <ref type="bibr" target="#b26">(See, Liu, and Manning 2017)</ref>. The main modification is that we apply the copy mechanism over the graph nodes distribution instead of the input source. More concretely, under the guidance of the entity node representations {h e i , i = 1, ? ? ?, m}, the decoder generates each word at the time step t via sampling from the vocabulary or copying directly from the graph entity nodes set E as:</p><formula xml:id="formula_7">P (t) out = gt ? P (t) V + (1 -gt) ? P (t) E ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_8">P (t)</formula><p>V is the normal vocabulary distribution from the decoder LSTM; P (t) E is the attention distribution over graph entity nodes. The soft switch g t ? [0, 1] to choose between sampling or copying is calculated given the decoder input x t and the decoder state s t as:</p><formula xml:id="formula_9">g t = ?(W 2 ? [x t ; s t ; h a t ]) with h a t = i ? e i ? h e i ,<label>(9</label></formula><p>) where W 2 is a trainable matrix and ? is the Sigmoid function. The aggregation vector h a t is computed through the weighted sum over the node representations h e i , and ? e i is the attention weight calculated as <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref>.</p><p>With the above graph-guided copy mechanism, the response generator can achieve the more accurate symptom request and disease diagnosis result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Graph-Evolving Meta-Learning</head><p>In this section, we present a Graph-Evolving Meta-Learning (GEML) framework which helps the above end-to-end medical dialogue model to handle the low-resource setting. This setting is more practical and challenging since many diseases in the real world are rare and costly to annotate as mentioned in Sec. 1. To address this challenge, GEML uses metaknowledge transfer and meta-knowledge graph evolving to transfer the diagnostic experience across different diseases. We'll introduce them in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Meta-Knowledge Transfer</head><p>The methodology of meta-knowledge transfer is to meta-train an end-to-end medical dialogue model f ?meta parameterized by ? meta with a fast adaptation capacity to new diseases with only limited data. To this end, we follow the meta-learning framework and use existing dialogue data of k source diseases to create a task set</p><formula xml:id="formula_10">T = {{T 1 i } N1 i=1 , {T 2 i } N2 i=1 . . . , {T k i } N k i=1 }</formula><p>, where each task T k i represents generating responses to a handful of dialogues in the k-th disease. Each task T k i ? T has only a few dialogue samples, which can be further split into the training (support) set D Ti tr and the validation (query) set D Ti va . Then in the meta-training stage, given a model initialization ? meta , we require that ? meta can fast adapt to any task T i ? T through one gradient update:</p><formula xml:id="formula_11">? i = ? meta -?? ? L D T i tr (f ?meta ) ,<label>(10)</label></formula><p>where L D T i tr is the training loss function of task T i and ? denotes a learning rate. To measure the quality of the adapted parameter ? i , MAML <ref type="bibr" target="#b8">(Finn, Abbeel, and Levine 2017)</ref>, which is an optimization-based meta-learning approach, requires ? i to have small validation loss on the validation set D Ti va . In this way, it can compute the gradient of validation loss and update the initialization ? meta as</p><formula xml:id="formula_12">? meta = ? meta -??L D T i va ? meta -?? ? L D T i tr (f ?meta ) ,<label>(11)</label></formula><p>where ? is step size. To alleviate the computational cost for the second-order gradient, i.e. Hessian matrix, in Eqn. ( <ref type="formula" target="#formula_12">11</ref>), Reptile <ref type="bibr" target="#b22">(Nichol, Achiam, and Schulman 2018)</ref> approximates the second derivatives of the validation loss as</p><formula xml:id="formula_13">? meta ? ? meta + ? 1 |{Ti}| Ti?p(T ) (? i -? meta ). (12)</formula><p>In this work, we use Reptile to update the initialization ? meta because of its effectiveness and efficiency. After obtaining the initialization ? meta , given a new target disease with only a few training data D tr , we can adapt the model f ?meta with initialization ? meta to the disease quickly via a few gradient steps to obtain the disease-adapted parameters. This fast adaptation ability comes from that, in the meta-training phase, we have already simulated the fast learning to a new disease via few steps of gradient descent on few validation data.</p><p>Note this meta-knowledge transfer only considers the fast adaptation in terms of model parameters and ignores the flaw of the sparsity in the commonsense graph. To address this problem, we devise a graph evolving approach to evolve the commonsense graph such that it can be tailored to the current disease and integrated with the dialogue instances better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Meta-Knowledge Graph Evolving</head><p>Since the commonsense graph is sparse and does not cover enough symptom entities, there is a gap between this prior graph and the real dialogue examples. For instance, "dysbacteriosis" may appear in the consultation of a patient while it doesn't exist in the commonsense graph since it is comparatively rare. To address the challenge, we propose to evolve the commonsense graph capitalized on the dialogue instances and learn the induced meta-knowledge graph during the meta-training and adaptation phases. Inspired by <ref type="bibr" target="#b16">Lin et al. (2019)</ref> that shows the related symptom entities have a certain probability of co-occurrence in the same dialogue, we construct a global-symptom graph G * = (V * , A * , X * ), where V * = {v 1 , . . . , v n } is the set of nodes, A * is the corresponding adjacency matrix and X * ? R |V * |?N is the node feature matrix. Concretely, the proposed approach first collects all observed dialogue examples in an online manner. Then if two entities co-occur in a dialogue example, there is an edge between both nodes in A * . The meta-knowledge graph is initialized with the adjacency matrix A of the prior commonsense graph and updated as:</p><formula xml:id="formula_14">A meta = A ? A * , (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where ? denotes the element-scale logic operator OR. A * is dynamically evolved along with more dialogue cases, which leads to the enrichment of the meta-knowledge graph synchronously, i.e., adding more nodes and edges.</p><p>The above approach for graph structure evolving can infer the existence of disease-symptom correlations while ignoring its intensity. To characterize such relations more delicately, GEML further learns the weight values of the meta-knowledge graph A meta with Eqn. ( <ref type="formula" target="#formula_5">6</ref>) during the metatraining and adaptation phases (while not during the testing). Finally, GEML utilizes the cross-entropy loss of the entity prediction task (in Sec. 4.2) to guide the learning of A meta efficiently, and we denote it as L e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">End-to-End Training Loss</head><p>In this section, we'll introduce the loss function for each task, i.e., L D T i tr in Eqn. (10) in detail. The generation loss function is the negative log-likelihood of generating the response R = {r 1 , . . . , r m } given the input dialogue context U = {u 1 , . . . , u n } as:</p><formula xml:id="formula_16">L g = -1 |R| |R| i=1 logp(r i |U ; ? meta ).<label>(14)</label></formula><p>The final training objective couples L g with L e and is of :</p><formula xml:id="formula_17">L = L g + ?L e ,<label>(15)</label></formula><p>where the constant ? balances the loss L g and the entity prediction loss L e in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Here we conduct extensive experiments on the CMDD dataset <ref type="bibr" target="#b16">(Lin et al. 2019</ref>) and the newly-collected Chunyu dataset to demonstrate the benefits of GEML.</p><p>Datasets. The CMDD dataset <ref type="bibr" target="#b16">(Lin et al. 2019</ref>) has 2,067 conversations totally ranging 4 pediatric diseases with approximately equal counts, while neglects the data-imbalance problem among diseases. To pose the challenge, we collect a much larger medical dialogue dataset, namely Chunyu, which contains 15 diseases with comparatively distinct data ratios. As depicted in Fig. <ref type="figure">1</ref>, the counts of each disease in Chunyu are significantly various and thus we can treat four low-resource diseases as target ones. The data statistics of two datasets are depicted in ) and human evaluation (on a 5-point scale). Top: For the CMDD dataset, target diseases from 1 to 4 refer to "bronchitis", "functional dyspepsia", "infantile diarrhea" and "upper respiratory infection" respectively. Bottom: For the Chunyu dataset, target diseases from 1 to 4 refer to "liver cirrhosis", "ileus", "pneumonia", and "pancreatitis".</p><p>each instance. The instances with very few turns or entities and with private information have been all discarded.</p><p>Experimental Settings. To apply the meta-learning, we consider generating responses to a handful of dialogues in one disease as a task. For Chunyu, as shown in Fig. <ref type="figure">1</ref>, high-resource diseases with more than 500 training instances are treated as source diseases and the remaining four lowresource ones as target diseases, whose size of adaptation data is ranging from 80 ? 200. For CMDD, we adopt the standard leave-one-out setup, i.e., using four diseases for meta-training and the one target disease left for adaptation (with the data size of 150 dialogues).</p><p>All experiments are based on the AllenNLP toolkit <ref type="bibr" target="#b9">(Gardner et al. 2017)</ref>. We implement encoders and decoders with a single-layer LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber 1997)</ref>, and use pkuseg <ref type="bibr" target="#b19">(Luo et al. 2019</ref>) toolkit to segment Chinese words. We set both dimensions of the hidden state and word embedding to 300 for LSTM. Adam optimization is adopted with the initial learning rate of 0.005 and the mini-batch size of 16. The maximum training epochs are set to 100 with the patience epoch of 10 for early-stopping. The best hyperparameter ? to balance the generation loss and the entity loss is 8. All baselines share the same configuration settings. Baselines. We first compare our base dialogue model MGR with two knowledge-grounded dialogue systems, NKD <ref type="bibr" target="#b17">(Liu et al. 2018)</ref> and POKS <ref type="bibr" target="#b15">(Lian et al. 2019)</ref>. NKD uses a neural knowledge diffusion module to introduce relative entities into dialogue generation. PostKS employs both prior and posterior distributions over knowledge to select the appropriate knowledge in response generation. Then we introduce several baselines induced from our GEML framework.</p><p>? Pre-train Only. We first pre-train each dialogue base model f ? with source diseases data in a multi-task learning paradigm, and then test it directly on target diseases. We test the above three base models and denote them as PT-NKD <ref type="bibr" target="#b17">(Liu et al. 2018)</ref>, PT-POKS <ref type="bibr" target="#b15">(Lian et al. 2019</ref>) and PT-MGR in Sec. 4. This is a zero-shot learning scenario. ? Fine-tuning. We pre-train f ? on source diseases with the same multi-task learning paradigm and then fine-tune these pre-trained models on each target disease, which are denoted as FT-NKD, FT-POKS and FT-MGR. ? Meta Learning. We first meta-train three base dialogue models over source diseases with the effective metalearning method, Reptile <ref type="bibr" target="#b22">(Nichol, Achiam, and Schulman 2018)</ref>, and then adapt the derived meta-learners to each target disease via fine-tuning. The resulting models are denoted as Meta-NKD, Meta-POKS and Meta-MGR. ? GEML-MGR. We also employ our GEML framework on the proposed MGR model and denote it as GEML-MGR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Results</head><p>Automatic Evaluation. We adopt two automatic metrics for performance comparisons as shown in Table <ref type="table" target="#tab_2">2</ref>. To evaluate the generation quality, we utilize the average of the sentencelevel BLEU-1, 2, 3 and 4 (Chen and Cherry 2014) and denote it as BLEU. To evaluate the success rate in the entity prediction task, we adopt Entity-F1, namely the F1 score between  predicted entities in generated response and the ground-truth entities. For the CMDD dataset, comparing to two other base models (NKD and POKS), our MGR always achieves the best performance in terms of both automatic and human evaluation, indicating the superiority of our end-to-end medical dialog model. The Fine-tuning method exceeds the Pretrain-Only in most cases and Meta-Learning methods often outperform multi-task learning in terms of BLEU slightly yet Entity-F1 significantly. This means that the Reptile algorithm can boost the capability of knowledge reasoning and transfer over diseases. By integrating our GEML method into the MGR, we can observe significant improvement for our GEML-MGR against all baselines, especially on Entity-F1, which demonstrates the stronger knowledge reasoning ability of our model in the medical diagnosis scenario. For the Chunyu dataset, we can observe similar results, including the superiority of the proposed GEML-MGR approach. Besides, we can see that the BLEU scores of all methods in CMDD dataset are much higher than those in the Chunyu, which demonstrates the challenge of the low-resource setting. Human Evaluation. We invited five well-educated graduate students majoring in medicine to score 100 generated replies for each method. For each dataset, the evaluators are requested to grade each case in terms of "knowledge rationality" and "generation quality" independently ranging from 1 (strongly bad) to 5 (strongly good). The right part of Table <ref type="table" target="#tab_2">2</ref> shows that our GEML-MGR achieves the statistically significant higher scores than Meta-MGR (t-test, p &lt;0.01) and FT-MGR (t-test, p &lt;0.005) in terms of the two aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Discussions</head><p>Ablation studies. To verify the effects of the main components of our GEML and the base dialogue model, we conducted ablation studies on the two datasets. Table <ref type="table" target="#tab_4">3</ref> shows that all these factors benefit our approach. Additionally, when we drop the graph reasoning module or the graph-guided copy mechanism, there were the remarkable performance degradation on both datasets, which indicates the significance of integrating these components completely.</p><p>Case Study of Graph Evolving. Fig. <ref type="figure">3</ref> shows the visualization of the evolved meta-knowledge graph and cases of generated responses. One can observe an significant gap between the commonsense graph and conversations in the Chunyu dataset, as the graph cannot cover all entities in the dialogue, e.g. borborygmus and exhaust. Through graph evolving, the learnt meta-knowledge graph is enriched with new entities and edges that can be derived from the conversation. For instance, the meta-knowledge graph absorbed new entities borborygmus and exhaust and enhance the edge weights among the disease node ileus and its neighbor nodes. Additionally, for generated responses, our GEML-MGR produces the rational and fluent diagnosis response with the least dialogue turns over other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we propose an end-to-end low-resource medical dialogue generation model which meta-learns a model initialization from source diseases with the ability of fast adaptation to new diseases. Moreover, we develop a Graph-Evolving Meta-Learning (GEML) framework that learns to fast evolve a meta-knowledge graph for adapting to new diseases and reasoning the disease-symptom correlations. Accordingly, our dialogue generation model enjoys the fast learning ability and can well handle low-resource medical dialogue tasks. Experiment results testify the advantages of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>graph P: Hello, doctor. I have a stomachache recently. What could be wrong with me? D: Do you feel heartburn? P: No. P: Yes. I also feel loss of appetite, and often hear borborygmus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework overview. Upper: The overview of the proposed GEML-MGR for the low-resource medical dialogue generation. The GEML-MGR first goes through the meta-training phase to learn and evolve a meta-knowledge graph and then adapts to new target diseases. Lower: The architecture of the end-to-end medical dialogue model, which integrates three components seamlessly: hierarchical context encoder, meta-knowledge graph reasoning and graph-guided response generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>In this way, updating the adjacency matrix A meta can reason the existence of edges among entity nodes. The structure of Statistics of the CMDD dataset<ref type="bibr" target="#b16">(Lin et al. 2019)</ref> and our Chunyu dataset.</figDesc><table><row><cell cols="2">Dataset CMDD</cell><cell>Chunyu</cell></row><row><cell># Disease types</cell><cell>4</cell><cell>15</cell></row><row><cell># Dialogues</cell><cell>2067</cell><cell>12,842</cell></row><row><cell># Utterances per dialogue</cell><cell>42.09</cell><cell>24.7</cell></row><row><cell># Entities per dialogue</cell><cell>7.5</cell><cell>12.9</cell></row><row><cell># Words per utterance</cell><cell>10.0</cell><cell>10.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The raw data of Chunyu is obtained from the Gastroenterology department of the Chinese online health community Chunyu 4 . It contains 15 gastrointestinal diseases and 62 symptoms in total. We use hand-crafted rules provided by doctors to label entities for</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Automatic Metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Human Evaluation</cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Target Disease 1</cell><cell cols="2">Target Disease 2</cell><cell cols="2">Target Disease 3</cell><cell cols="2">Target Disease 4</cell><cell cols="2">Average</cell><cell>Knowledge</cell><cell>Generation</cell></row><row><cell></cell><cell></cell><cell>BLEU</cell><cell>Enti.-F1</cell><cell>BLEU</cell><cell>Enti.-F1</cell><cell>BLEU</cell><cell>Enti.-F1</cell><cell>BLEU</cell><cell>Enti.-F1</cell><cell>BLEU</cell><cell>Enti.-F1</cell><cell>Rationality</cell><cell>Quality</cell></row><row><cell>CMDD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the two datasets in terms of automatic metrics (?10 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>:</head><label></label><figDesc>Hello, doctor. I feel nauseous recently. At the same time, my stomach is dddsuffering from indigestion. Do I have any disease? ???????????????????????????????? : Do you have a feeling of loss of appetite? ???????????? : I didn't feel it, but I always felt a stomachache when I finished eating. The visualization of the evolved meta-knowledge graph and examples of generated responses on Chunyu. Our graph evolving can enrich the commonsense graph and generates the response containing the correct entity (i.e., ileus).</figDesc><table><row><cell></cell><cell cols="3">Dialogue History</cell><cell></cell><cell></cell><cell></cell><cell>Ground-truth Response</cell></row><row><cell></cell><cell>? 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>: I think you are likely to have an ileus. ??????????? ? 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generated Responses</cell></row><row><cell></cell><cell cols="6">? 1 ? 2 ? 2 : Do you feel heartburn recently? ???????????? ?????????????????????</cell><cell>Meta-NKD Meta-POKS</cell><cell>: Do you feel bloating? ?????????? : I suspect you may have esophagitis. ????????????? ? 3 ? 3</cell></row><row><cell></cell><cell cols="5">: Yes, and I feel the barborygmus, always want to exhaust. ???????????????????? ? 3</cell><cell></cell><cell>GEML-MGR (Ours)</cell><cell>: You should have an ileus. ?????????? ? 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Commonsense Graph</cell><cell></cell><cell>Meta-Knowledge Graph</cell></row><row><cell></cell><cell></cell><cell cols="2">Vomit</cell><cell>Heartburn</cell><cell></cell><cell></cell><cell>Exhaust</cell><cell>Heartburn</cell></row><row><cell></cell><cell></cell><cell>Stomachache</cell><cell>Ileus</cell><cell></cell><cell cols="2">Esophagitis</cell><cell>Evolving Graph</cell><cell>Stomachache Vomit</cell><cell>Ileus</cell><cell>Esophagitis</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Nausea</cell><cell cols="2">Loss of appetite</cell><cell>Loss of appetite</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Nausea</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Bloating</cell><cell></cell><cell></cell><cell></cell><cell>Borborygmus</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Indigestion</cell><cell>Enteritis</cell><cell></cell><cell>Bloating</cell><cell>Indigestion</cell><cell>Enteritis</cell></row><row><cell>Figure 3: Graph</cell><cell>Copy</cell><cell>Meta-</cell><cell>Graph</cell><cell cols="2">CMDD</cell><cell cols="2">Chunyu</cell></row><row><cell cols="4">Reasoning Mecha. Transfer Evolving</cell><cell>BLEU</cell><cell>Enti.-F1</cell><cell>BLEU</cell><cell>Enti.-F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.73</cell><cell>39.37</cell><cell>23.07</cell><cell>38.67</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>29.73</cell><cell>32.89</cell><cell>19.53</cell><cell>28.18</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>32.59</cell><cell>33.78</cell><cell>21.95</cell><cell>30.81</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>37.27</cell><cell>34.26</cell><cell>20.98</cell><cell>29.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>38.1</cell><cell>35.87</cell><cell>21.79</cell><cell>33.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of ablation studies on two datasets (?10 2 ).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We name such knowledge as "meta-knowledge" since it is obtained through meta-training from different source diseases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Code and dataset are released at https://github.com/halins/GEML-MDG.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>OpenKG is a Chinese open knowledge graph project. The library is available at http://openkg.cn/dataset/symptom-in-chinese.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.chunyuyisheng.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported in part by <rs type="funder">National Natural Science Foundation of China (NSFC)</rs> under Grant No.<rs type="grantNumber">U19A2073</rs> and No.<rs type="grantNumber">61976233</rs>, <rs type="person">Guangdong Province Basic</rs> and <rs type="funder">Applied Basic Research (Regional Joint Fund-Key)</rs> Grant No.<rs type="grantNumber">2019B1515120039</rs>, <rs type="funder">Nature Science Foundation of Shenzhen</rs> Under Grant No. <rs type="grantNumber">2019191361</rs>, <rs type="funder">Zhijiang Lab's Open Fund</rs> (No. <rs type="grantNumber">2020AA3AB14</rs>) and <rs type="funder">CSIG Young Fellow Support Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AZQm3mb">
					<idno type="grant-number">U19A2073</idno>
				</org>
				<org type="funding" xml:id="_dz3sCVJ">
					<idno type="grant-number">61976233</idno>
				</org>
				<org type="funding" xml:id="_Mpt3wJH">
					<idno type="grant-number">2019B1515120039</idno>
				</org>
				<org type="funding" xml:id="_qCm3Fqj">
					<idno type="grant-number">2019191361</idno>
				</org>
				<org type="funding" xml:id="_yJ7YSWq">
					<idno type="grant-number">2020AA3AB14</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03863</idno>
		<title level="m">Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A systematic comparison of smoothing techniques for sentence-level bleu</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="362" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks</title>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1192" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting Symptoms and their Status from Clinical Conversations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="915" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to Infer Entities, Properties and their Relations from Clinical Conversations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4978" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">A Deep Semantic Natural Language Processing Platform</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meta-Learning for Low-Resource Neural Machine Translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3622" to="3631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-Aware Symptom Checking for Disease Diagnosis Using Hierarchical Reinforcement Learning</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2305" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07510</idno>
		<title level="m">Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">End-to-End Task-Completion Neural Dialogue Systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">IJCNLP</biblScope>
			<biblScope unit="page" from="733" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to select knowledge for response generation in dialog systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5032" to="5041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge Diffusion for Neural Dialogue Generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1489" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07497</idno>
		<title level="m">MedDG: A Large-scale Medical Consultation Dataset for Building Medical Dialogue System</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PKUSEG: A Toolkit for Multi-Domain Chinese Word Segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.11455" />
	</analytic>
	<monogr>
		<title level="m">CoRR abs/1906.11455</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3151" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Opendialkg: Explainable conversational reasoning with attentionbased walks over knowledge graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Obamuyide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5873" to="5879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain Adaptive Dialog Generation via Meta Learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2639" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding Medical Conversations with Scattered Keyword Attention and Weak Supervision from Responses</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8838" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MASS: Masked Sequence to Sequence Pre-training for Language Generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
			<pubPlace>In NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Graph Attention Networks. ICLR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Task-oriented dialogue system for automatic diagnosis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="201" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4354" to="4364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end knowledge-routed relational dialogue system for automatic diagnosis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7346" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grounded conversation generation as guided traverses in commonsense knowledge graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2031" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Low-Resource Knowledge-Grounded Dialogue Generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Commonsense knowledge aware conversation generation with graph attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4623" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient Meta Learning via Minibatch Proximal Update</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<title level="m">Task Similarity Aware Meta Learning: Theory-inspired Improvement on MAML</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
