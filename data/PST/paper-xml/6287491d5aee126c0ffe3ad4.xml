<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haw-Shiuan</forename><surname>Chang</surname></persName>
							<email>hschang@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CICS</orgName>
								<orgName type="institution" key="instit2">University of Massachusetts</orgName>
								<address>
									<addrLine>140 Governors Dr</addrLine>
									<postCode>01003</postCode>
									<settlement>Amherst, Amherst</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<email>mccallum@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CICS</orgName>
								<orgName type="institution" key="instit2">University of Massachusetts</orgName>
								<address>
									<addrLine>140 Governors Dr</addrLine>
									<postCode>01003</postCode>
									<settlement>Amherst, Amherst</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.</p><p>"The greater the ambiguity, the greater the pleasure." -Milan Kundera</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, researchers have found that transformerbased language models (LMs), such as GPT-2, can predict the next/masked word distribution better as their sizes grow <ref type="bibr" target="#b20">(Radford et al., 2019;</ref><ref type="bibr" target="#b8">Brown et al., 2020;</ref><ref type="bibr" target="#b8">Kaplan et al., 2020)</ref>. Compared to greedily outputting the most probable next word, sampling the next word from the predicted distribution allows a LM to generate more diverse and high-quality text sequences <ref type="bibr" target="#b6">(Holtzman et al., 2020)</ref>. By autoregressively sampling the next word according to its predicted probability, large LMs can be used to assist creative writing <ref type="bibr">(Akoury et al., 2020)</ref>, reduce the cost of building datasets <ref type="bibr" target="#b33">(West et al., 2021;</ref><ref type="bibr" target="#b12">Liu et al., 2022)</ref>, generate codes <ref type="bibr">(Li et al., 2022)</ref>, solve math problems <ref type="bibr">(Cobbe et al., 2021)</ref>, etc. As a result, one natural question arises: Do modern language modeling architectures still have restrictions in their ability to represent the appropriate distribution over next words or masked words?</p><p>In this paper, we discover that, when predicting the next word probabilities given an ambiguous context, GPT-2 is often incapable of assigning the highest probabilities to the appropriate nonsynonym candidates. For example, given the input prompt "After debating whether to bow to the woman or the king first, the jester decided on the [MASK]", we would expect the distribution over the <ref type="bibr">[MASK]</ref> fillers to put high probabilities on woman or king or their synonyms. However, GPT-2 might incorrectly assign the second-highest probability to "queen" as in Figure <ref type="figure">1</ref>.</p><p>In the final softmax layer of GPT-2, the log probabilities of the woman and king are computed based on the dot product between a single hidden state embedding and the global word embeddings of the woman and king, respectively. To have the highest but similar dot products for the two options, the transformer encoder in GPT-2 wants to output the hidden state that is close to the average of the woman embedding and the king embedding. However, the words queen, king, woman, and man tend to form a parallelogram in the embedding space <ref type="bibr" target="#b15">(Mikolov et al., 2013;</ref><ref type="bibr">Ethayarajh et al., 2019;</ref><ref type="bibr" target="#b32">Wang et al., 2019)</ref> <ref type="foot" target="#foot_0">1</ref> , which means the man and queen also have a similar average. Therefore, GPT-2 is forced to also output man or queen when it wants to output woman or king.</p><p>The problem not only happens to GPT-2 or the words whose embeddings form a parallelogram shape. Even though the hidden state embeddings of LMs are contextualized, the embedding of each Output Word Embedding Space</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-2 Encoder</head><p>After debating whether to bow to the woman or the king first, the jester decided on the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-2 + Multi-embedding Encoder</head><p>After debating … king first, the jester decided on the Figure <ref type="figure">1</ref>: Comparison between the softmax layers using a single embedding and multiple embeddings when the next word should be either woman or king. In GPT-2 and multi-embedding GPT-2, the hidden states of the context are visualized by the single facet and multiple facets , respectively. The word embeddings are visualized using</p><formula xml:id="formula_0">• • • • • • • • • • •.</formula><p>GPT-2 cannot output woman and king as the top two words because queen and man are close to the midpoint of woman and king. The improvement in this type of ambiguous context will be quantified in Section 5.</p><p>word in the softmax layer is global and static during the inference time. Globally dissimilar words could all become the suitable next word in a context while other interfering words might be between them, which makes the ideal next word embedding distribution have multiple modes and cannot be modeled by the single embedding representation.</p><p>In this work, we propose theorems showing that given any LM using the output softmax layer, when there are more than N word embeddings in a N − 1 dimensional subspace/hyperplane (e.g., four embeddings in a two-dimensional plane), we can always find a set of possible next words (e.g., woman and king) such that there are some other interfering words between them (e.g., man or queen). That is, the multimodal next word distribution must exist if a few word embeddings are linearly dependent.</p><p>Recently, mixture of softmax (MoS) <ref type="bibr" target="#b35">(Yang et al., 2018)</ref> regains attention as one of the few effective architecture modifications for transformer LM <ref type="bibr" target="#b18">(Narang et al., 2021;</ref><ref type="bibr">Anonymous, 2021)</ref>. In the meanwhile, <ref type="bibr" target="#b19">Parthiban et al. (2021)</ref> show that the softmax bottleneck <ref type="bibr" target="#b35">(Yang et al., 2018)</ref> theory is not sufficient to explain the improvement of MoS. As a remedy, our theorems not only provide geometrical intuitions of why and when the multiple embedding representation such as MoS would do better but also suggest that the softmax bottleneck might not be completely solved even if we adopt a very large hidden state size. For example, no matter how large the hidden state size is, as long as queen king = womanman in the embedding space, the LMs cannot output a pair of words in the longer diagonal of the parallelogram as the top two output words.</p><p>After better understanding why mixture of softmax (MoS) works well, we propose two enhancements over MoS. The first enhancement considers the hidden states of multiple positions and multiple transformer layers when determining the probability in each softmax; the second enhancement uses different contextualized embeddings to compute the probabilities of different subsets of words in the vocabulary.</p><p>The resulting method, multi-facet softmax (MFS), significantly outperforms the MoS and the softmax layer in GPT-2 on the perplexity for predicting the next word, especially in ambiguous context and non-English text in OpenWebText <ref type="bibr" target="#b20">(Radford et al., 2019)</ref>. Finally, we also show that MFS could improve the performance of GPT-2 on Pro-toQA <ref type="bibr">(Boratko et al., 2020)</ref>, a commonsense question answering dataset where each question has multiple acceptable answers.</p><p>We summarize our theoretical, methodological, and empirical contributions as follows.</p><p>• Theory: We show the softmax layer using a single embedding is sometimes not able to output an appropriate rank of probabilities on a set of words with linearly dependent embeddings.</p><p>• Method: Addressing two weaknesses in MoS <ref type="bibr" target="#b35">(Yang et al., 2018)</ref>, we propose multi-facet softmax (MFS), a new alternative to the output softmax layer. MFS can replace the softmax in pre-trained LMs to better handle ambiguous contexts without re-training the LMs from scratch.</p><p>• Analysis: Our comprehensive empirical analyses discover and explain several phenomena, such as a) why using multiple embeddings is usually better than the single embedding with the nonlinearity, b) why the improvement is larger in ambiguous contexts, less common languages, or GPT-2 compared to BERT, and c) why a LM often confuses with similar words.</p><p>2 Theoretical Limitations of the Single Embedding in the Softmax Layer</p><p>In this section, we first review the softmax layer of GPT-2 formally and explain why queenking = womanman still tends to hold in contextualized LMs. Next, we present our theoretical analyses, which generalize the woman and king example by showing that the candidate words in a low dimensional subspace would induce the impossibility of ranking some candidates on top of other candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>The LMs typically use a softmax layer to predict P S (x|c t ), the probability of the next word x given the context at the tth position c t :</p><formula xml:id="formula_1">P S (x|c t ) = exp(h T ct w x ) x exp(h T ct w x ) ,<label>(1)</label></formula><p>where h ct is the tth hidden state in the context c, and w x is the output word embedding for the word x (i.e., the linear weights that project the hidden state to the logit of the word x).<ref type="foot" target="#foot_2">2</ref>  <ref type="bibr" target="#b35">Yang et al. (2018)</ref> point out that the log probability distribution over all the words in the vocabulary</p><formula xml:id="formula_2">V is log (P S (x|c t )) | x∈V = h T ct w x − log x exp(h T ct w x ) | x∈V .</formula><p>The distribution is a linear projection from the hidden state h ct with dimension D, so the degree of freedom in the distribution is only D (i.e., there cannot be more than D linearly independent log distributions). We call this restriction softmax bottleneck theory.</p><p>During training, the ideal output word embedding w x should be close to the hidden states of the contexts h ct that co-occur with the word x while far away from the other hidden states. This objective is similar to the objective function of Word2Vec <ref type="bibr" target="#b15">(Mikolov et al., 2013)</ref> except that the context embeddings are contextualized <ref type="bibr" target="#b9">(Kong et al., 2020;</ref><ref type="bibr" target="#b10">Li et al., 2020)</ref>.</p><p>If a context c t has a higher chance to co-occur with queen compared to king, the context also has a higher chance to co-occur with woman compared to man to a similar degree. This is the main reason that makes queenking = womanman in the Word2Vec space <ref type="bibr">(Ethayarajh et al., 2019)</ref>. Therefore, the same linear relations tend to hold in the output word embedding space of GPT-2 as well <ref type="bibr" target="#b32">(Wang et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structural Weakness Theorems from</head><p>Linear Dependency</p><p>In addition to words satisfying the analogy relations, the following theorems imply that any linear dependency among the words causes the difficulties of LM in ranking the words in an arbitrary order according to their logits (i.e., dot products between the hidden state and the word embedding). For example, woman + king = queen + man makes a LM unable to assign the highest positive logits to woman and king and output them as the top two words in Figure <ref type="figure">1</ref>.</p><p>Theorem 1. If the nonzero output embeddings of N words in a set W are linearly dependent and on one side of a hyperplane through the origin, the single embedding representation cannot produce positive logits for a subset of the words in W that are higher than all the logits of the other words in W .</p><p>Here, we provide an intuitive justification: if N embeddings are in a subspace whose dimension is smaller than N − 1 (e.g., three points in a onedimensional line), the N embeddings are going to be linearly dependent and some set of words cannot have the top dot products due to the limited degree of freedom in the subspace. In Appendix D, we formally prove the theorem by identifying the sets of words that cannot be ranked top by the single embedding representation.</p><p>In practice, linear dependency holds approximately instead of exactly. For example, woman = queen + manking + ε. In this practical condition, the following theorem states that the logits of the After debating whether to bow to the woman or the king first, the jester decided on the interfering words (i.e., man and queen) cannot be much smaller than the logits of the candidate words (i.e., woman and king).</p><formula xml:id="formula_3">qct hct M qct layer M-2 ⊕i,mhct-i M-m L f (.) GELU(L h (.)) L π (.) layer M L π (.) L f (.)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>……</head><p>Theorem 2. Let the output word embeddings in the set</p><formula xml:id="formula_4">W = {w i = 0|i = 1...N } satisfy w 1 = a 2 w 2 + ... + a N w N + ε</formula><p>, where the constant a 2 , ..., a N are neither all zero nor all negative and ||ε|| &lt; . Then, there must be a nontrivial partition P = {G, S} of W such that there is no hidden state ||h|| ≤ r and a threshold τ ≥ r that make min wg∈G h T w g ≥ (1 + δ)τ and max ws∈S h T w s &lt; τ , where δ =</p><formula xml:id="formula_5">2 1+ i=2...N |a i | .</formula><p>In the king-woman example, (1+δ) = (1+ 2 4 ) = 1.5. Assuming ||ε|| &lt; = 0.01 and ||h|| ≤ r = 20, we can get h T ε ≤ 0.01 × 20 = 0.2. Then, we cannot find a hidden state h such that h T w king ≥ 1.5 × 0.01 × 20 = 0.3 and h T w woman ≥ 0.3 but h T w queen &lt; 0.2 and h T w man &lt; 0.2 because h T w king +h T w woman = h T w queen +h T w man + h T ε. The formal proof of Theorem 2 can be found in Appendix D and Appendix B.1 estimates in several language models. Even though, theoretically speaking, outputting woman and king as the top two words is possible due to the appearance of ε, LMs may not successfully learn to output the optimal h and the optimal hidden state for these four words could lead to the wrong probabilities of the other words. Consequently, LMs sometimes still rank queen or man higher than woman or king in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-facet Softmax</head><p>Using multiple embeddings is a natural solution for modeling a multimodal distribution <ref type="bibr">(Bishop, 1994)</ref>. For instance, we can use three embeddings to capture the high probability on the woman and king but low probability on the man and queen in Figure <ref type="figure">1</ref>.</p><p>Inspired by our geometric analysis on the limitation of the single embedding, we improve the state-of-the-art multiple embedding solution, mixture of softmax (MoS) <ref type="bibr" target="#b35">(Yang et al., 2018)</ref> by two enhancements: multiple input hidden states and multiple partitions on the vocabulary. <ref type="bibr">et al. (2018)</ref> propose mixture of softmax (MoS) to allow a LSTM-based <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref> LM to produce more linearly independent log probability distributions of the output words given different contexts. As in Figure <ref type="figure" target="#fig_1">2</ref> (c), the MoS first uses multiple linear layers L f k to project a hidden state h ct into multiple facet embeddings f ct,k = L f k (h ct ). <ref type="foot" target="#foot_3">3</ref> The multiple facets f ct,k and softmaxes would lead to multiple probability distributions, and output probability is the weighted average of the distributions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mixture of Softmax</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yang</head><formula xml:id="formula_6">P M oS (x|c t ) = K k=1 π ct,k exp(f T ct,k w x ) x exp(f T ct,k w x )</formula><p>.</p><p>(2)</p><p>The prior weights</p><formula xml:id="formula_7">π ct,k = exp(L π k (hc t )) k exp(L π k (hc t ))</formula><p>, where L π k is another linear projection for dynamically generating the weights and the projection goes through a softmax to ensure K k=1 π ct,k = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiple Input Hidden States</head><p>To model the multimodal distribution, the facets (i.e., the embeddings for different softmaxes) should be able to move freely. For example, in Figure <ref type="figure">1</ref>, we have three facets but only have two modes, so the two embeddings are very close to the word king. However, when we want to output three dissimilar top words such as the king, woman, and knight, one of the facets should be moved to be near to the embedding of the knight. Therefore, we want our solution to satisfy two properties: a) the linear transformation matrix in L f k should have a full rank to avoid limiting the degree of freedom in each facet, and b) the relative location of the facets should be context-dependent. MoS cannot satisfy both properties. If the first one is satisfied, the input hidden state is uniquely determined by a facet (e.g., h ct = (L f 1 ) −1 (f ct,1 )). Then, there exists a global transformation between two facets (e.g.,</p><formula xml:id="formula_8">f ct,2 = L f 2 (L f 1 ) −1 (f ct,1</formula><p>) ), which violates the second property. That is, assuming LM can move every facet freely (i.e., the facet's degree of freedom is the same as the dimension of the hidden state), LM cannot make the first two facets close to woman and king in one context but make the two facets close to woman and knight in another context. In other words, since the facet embeddings are the projection of a single hidden state, the total degree of freedom in all facet embeddings cannot exceed the dimension of the hidden state.</p><p>Our solution to this issue is using more input hidden states to construct the facets. As the orange box in Figure <ref type="figure" target="#fig_1">2</ref>, we first concatenate a W × H block of input hidden states into</p><formula xml:id="formula_9">⊕ i=0...W −1,m=0...H−1 h M −m c t−i ,</formula><p>where M − m is the transformer layer index and t − i is the index of the ith to the last word in the context. The W × H is fixed as 3×3 in this paper. We make its dimension the same as the original hidden state h M ct using a linear layer L h plus a GELU activation function <ref type="bibr" target="#b4">(Hendrycks and Gimpel, 2016)</ref>. Then, we concatenate it with the original hidden state to form a new input hidden state</p><formula xml:id="formula_10">q ct = h M ct ⊕ GELU L h (⊕ i,m h M −m c t−i ) . (3)</formula><p>The new input hidden state is passed through the linear transformation L f k to compute the facets f ct,k = L f k (q ct ) and our prior weights</p><formula xml:id="formula_11">π ct,k = exp(L π k (qc t )) k exp(L π k (qc t ))</formula><p>. Since the dimension of q ct is larger than the dimension of f ct,k , the inverse function (L f k ) −1 no longer exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiple Partitions</head><p>The next word distribution could have many modes. However, using many softmaxes significantly increases our computational burden because we need to compute the dot product between each facet and all the word embeddings in our vocabulary.</p><p>Inspired by our analysis, we propose to split all the words in the vocabulary into multiple partitions<ref type="foot" target="#foot_4">4</ref> and use different facets for different partitions. For example, if we can put any word from {queen, man, woman, king} into one partition and the rest of the words into another partition, we no longer have queenking = womanman in either of the partitions. In this method, each word only belongs to one partition, so we only need to compute one dot product for each word. Thus, the extra computational cost only comes from the extra linear projections for preparing the facets.</p><p>In many contexts c t , the distribution of the next word has only a single mode and the global similarity between words may be useful. Using the multiple partitions alone might lose the similarity information between words in different partitions. Therefore, we propose to only replace the first softmax layer in MoS with the multiple partition method to learn the global similarity of words in different partitions using the other softmaxes. The architecture is illustrated in Figure <ref type="figure" target="#fig_1">2 (d)</ref>. Formally, we compute the probability using</p><formula xml:id="formula_12">P M P (x|c t ) = π ct,1 exp((f jx ct,1 ) T w x )</formula><p>x exp((f</p><formula xml:id="formula_13">j x ct,1 ) T w x ) + K k=2 π ct,k exp(f T ct,k w x ) x exp(f T ct,k w x ) ,<label>(4)</label></formula><p>where j x is the partition index that the word x belongs to and f jx ct,1 is the facet for the j x th partition.</p><p>Multi-facet softmax (MFS) is equipped with multiple input hidden states and multiple partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Language Modeling Experiments</head><p>We evaluate different LM architectures by comparing their capability of predicting the next word in Wikipedia 2021 and a subset of OpenWeb-Text <ref type="bibr" target="#b20">(Radford et al., 2019)</ref>. In addition to perplexity, we also compare their mean reciprocal ranks (MRR) in Appendix C.1. The size of the training, validation, and testing set are 96%, 2%, and 2% of the whole corpus, respectively. After loading the pre-trained GPT-2 models, we train the GPT-2 Small for 1 epoch and GPT-2 Medium for 0.4 epochs. We also test our methods on BERT in Appendix B.2. Please see Appendix G for more details of our experiment setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>We set different numbers of softmaxes, input hidden states, and partitions in our MFS framework to construct our baselines. The configuration of different baselines could be seen in Table <ref type="table" target="#tab_2">1</ref>. Softmax (GPT-2): Using a single softmax, input hidden state, and partition as in Figure <ref type="figure" target="#fig_1">2</ref> (a) and Equation 1. The baseline is the same as the original GPT-2 except that we add one more linear layer that converts the hidden state h M ct to the facet embedding f ct,1 as in other methods.</p><p>SigSoftmax <ref type="bibr" target="#b7">(Kanai et al., 2018)</ref>: The same as Softmax except when predicting the next word, <ref type="bibr" target="#b7">Kanai et al. (2018)</ref> add some non-linearity into the softmax layer by multiplying the exponent and sigmoid of the logits.</p><p>Softmax + Multi-input: Letting Softmax access multiple input hidden states as in Figure <ref type="figure" target="#fig_1">2</ref> (b) and Equation 3. The method is similar to <ref type="bibr" target="#b30">Tenney et al. (2019);</ref><ref type="bibr">Fan et al. (2020), and</ref><ref type="bibr" target="#b29">Tay et al. (2021)</ref>.</p><p>MoS <ref type="bibr" target="#b35">(Yang et al., 2018)</ref>: MoS (3) is the mixture of softmax with 3 facets/softmaxes, whose probability comes from Equation 2. We also run the MoS with 4 softmaxes in GPT-2 Small and call the model MoS (4).</p><p>DOC <ref type="bibr" target="#b28">(Takase et al., 2018)</ref>: Similar to our enhancement using multiple input hidden states, direct output connection (DOC) makes each of their facets coming from a different input hidden state.</p><p>Other configurations include Softmax + Multipartition, which adds four partitions into the softmax, MFS w/o Multi-partition, which uses only one partition in MFS and could also be viewed as MoS + Multi-input, and MFS w/o Multi-input, which uses only one input hidden state to generate all facets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table" target="#tab_2">1</ref> shows that applying MFS to GPT-2 Small achieves more than 15% of the perplexity improvement between GPT-2 Small and GPT-2 Medium, while only increasing 5% of their size differences. Except for Softmax + Multi-partition, adding multiple input hidden states or partitions in different configurations significantly boost the performances. In Appendix B.3, we further show that the improvement of MFS over Softmax could even become 3-5 times larger in the top 5-10% of the most ambiguous contexts compared to the rest of the contexts, which suggests that some improvements indeed come from successfully modeling multimodal distribution.</p><p>MFS usually doubles the perplexity improvements between MoS (3) and Softmax but the running time of MFS remains similar to MoS (3) because MFS only needs a few more linear layers, which is more efficient than adding one more softmax as in MoS (4). DOC is worse than MoS (3). This may be due to a starvation problem: the facet from the last hidden state h M ct has the prior probability close to 1 and receives most of the gradients. Finally, compared with Softmax, the mixed results in SigSoftmax suggest that adding non-linearity into the softmax layer without modeling the multimodal distribution might not always improve the models <ref type="bibr" target="#b19">(Parthiban et al., 2021)</ref>.</p><p>OpenWebText is mostly composed of English text, but some non-English text in the corpus allows us to compare the capability of different models in a multi-lingual setting. Table <ref type="table" target="#tab_3">2</ref> shows that multiple embeddings improve the perplexity of the non-English text more than the perplexity of the English text. We hypothesize that the distribution of the next non-English word is more likely to be multi-mode because GPT-2 learns the global token embeddings mostly in the English contexts, which could make the embeddings of similar tokens in non-English contexts far away.</p><p>In Table <ref type="table" target="#tab_4">3</ref>, we present three contexts from the validation set of different datasets and compare the top three predictions of MFS and Softmax on GPT-2 Small. In OpenWebText and Wikipedia 2021, we can see that Softmax misses the correct answer in its top three predictions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation on Ambiguous Templates</head><p>We synthesize a dataset using templates <ref type="bibr" target="#b23">(Ribeiro et al., 2020)</ref> to verify whether the softmax layer in the original GPT-2 really has difficulty in learning to output the bimodal distribution in Figure <ref type="figure">1</ref> and whether the multiple embedding methods could overcome the problem. First, we collect the four words with semantic analogy relations in Google analogy dataset <ref type="bibr" target="#b15">(Mikolov et al., 2013)</ref>. Next, we insert two out of the four words into our manually written templates to form the contexts and the templates we used could be found in Appendix G.3. For example, given the context "I went to Paris and Germany before, and I love one of the places more, which is", the GPT-2 learns to predict either Paris or Germany.</p><p>The two words can be either the diagonal words (e.g., king and woman) or the edge word (e.g., king and queen) in the parallelogram. Finally, we create a dataset with 122k training contexts, 250k validation contexts, and 122k testing contexts, where the word pairs in the testing set are unseen in the training set to see whether the model could learn to output the bimodal distribution in a general way. 5 5 The setting is realistic because any related words could become the next word in some ambiguous contexts and all</p><p>We load the models pre-trained on OpenWeb-Text and continue fine-tuning the models on the last word of each sentence for 10 epochs. We report the testing performances of the best model selected by the validation loss. Since the sets of the word pairs in the training and testing set are disjoint, updating the output word embedding would make GPT-2 solve the task by memorizing/overfitting the training set quickly and lead to much worse testing performances. Thus, we freeze the output word embedding during the training.</p><p>We visualize the predictions of the Paris-Germany example in the last column of Table <ref type="table" target="#tab_4">3</ref>. We can see two of the softmaxes are close to Paris and the remaining one is close to German, while Softmax overestimates the probability of Paris and ranks France higher than the German. The result verifies that the correct probability distribution of the words in some ambiguous context is hard to learn using Softmax.</p><p>Quantitatively, Table <ref type="table" target="#tab_5">4</ref> indicates that when the possible next words are the diagonal words, the Softmax model performs much worse compared to other multiple embedding alternatives. In the edge word dataset, the multiple embedding solutions are still better but have a much smaller gap. MFS w/o Multi-partition slightly improves MoS. We hypothesize the reason is that multiple input hidden states could help the facets to be moved more freely. Finally, multiple partitions seem to cause slight overfitting in this bimodal distribution prediction task. 6 Answering Ambiguous Questions</p><p>ProtoQA <ref type="bibr">(Boratko et al., 2020</ref>) is a questionanswering dataset built for evaluating the commonsense reasoning ability of language models. Each question in ProtoQA is ambiguous and leads to a distribution of possible answers. For instance, the answer to "Name something that people usually do before they leave for work?" is "Shower 0.43, Breakfast 0.30, ...". The paper discovers that by reformulating the question-answering task as a context (e.g., "One thing people usually do before they leave for work is ..."), GPT-2 could generate the possible answers by sampling the next words from its word prediction distribution.</p><p>The dataset gives us a chance to directly compare the quality of the distributions generated by different LMs in Table <ref type="table">5</ref>. After pretraining GPT-2 Medium on the OpenWebText, we fine-tune them using the training data in ProtoQA for 2 epochs. We repeat the fine-tuning 5 times and compare their average perplexity in our validation set. Next, we generate 150 sentences starting from each context and compare the generated answers with the ground truth distribution. For each fine-tuned model, we repeat the generation evaluation 3 times and report the average accuracy of the resulting 15 trials.</p><p>We can see that the multiple softmaxes, input hidden states, and partitions usually improve the quality of prediction distribution, and the proposed MFS, which combines all modifications, achieves the best performances.</p><p>ciently, but the approaches gain less improvement compared to MoS.</p><p>A limitation of the aforementioned previous work is that they do not tell us which kinds of sentences would be affected by the bottleneck more and whether the order of the top few next words would be affected, which are the main research questions of our work. Contrary to the previous belief that a large hidden state dimension would eliminate the softmax bottleneck, our theorems suggest that some words in a low dimensional subspace could still make the single embedding in the softmax layer become a bottleneck of arbitrarily ranking the output words. Furthermore, our geometric analyses provide an intuitive explanation about why breaking the bottleneck using multiple embeddings leads to better performances compared to only adding the non-linearity.</p><p>Demeter et al. ( <ref type="formula">2020</ref>) also analyze the structural weakness of the softmax layer from a geometric perspective. They discover that the words with high prior frequencies could stop the LMs from assigning the high probabilities to rare words, which can be viewed as a special case of our theory (See Appendix E). For instance, our work shows that the softmax layer could still prevent the LMs from outputting some top words even if all the possible next words have the same prior frequency.</p><p>Our theory is deeply connected to the mathematical work that counts the number of possible rankings of points in an embedding space <ref type="bibr">(Cover, 1967;</ref><ref type="bibr" target="#b3">Good and Tideman, 1977)</ref>. Compared to the studies, our work focuses more on analyzing the multimodal distribution in the word embedding space and its implication to language models. An alternative to model the multimodal distribution is to use multiple embeddings to represent each output word (Athiwaratkun and Wilson, 2017; <ref type="bibr" target="#b14">Miao et al., 2019)</ref>. Compared to MoS or our approach that use multiple embeddings to represent each hidden state of the context, their method requires many extra parameters to store different senses of each output word. Another type of related model <ref type="bibr" target="#b24">(Shazeer et al., 2017;</ref><ref type="bibr" target="#b18">Fedus et al., 2021)</ref> dynamically routes the signals to different experts (i.e., feed-forward networks) and Zhang et al. ( <ref type="formula">2022</ref>); <ref type="bibr" target="#b17">Mittal et al. (2022)</ref> use multiple embeddings in the attention layers. The methods are similar to MoS and our approach, but they add the multiple embeddings inside each layer of the transformer encoder while the proposed MFS is an alternative to the output softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>When the ideal distribution in the output word embedding space has multiple modes, GPT-2 cannot learn to correctly rank the words in all the modes as the top next words. This shows that the single embedding in the softmax layer, which is used nearly universally by current LMs, constitutes a performance upper bound of predicting the next/masked word. To address the systematic failure caused by these structural weaknesses, we propose multifacet softmax (MFS). In our experiments, we confirm that the MFS significantly outperforms the standard softmax layer and alleviates the softmax bottleneck in the transformer-based LMs such as GPT-2 better than mixture of softmax (MoS).</p><p>This work studies a general limitation of LMs and proposes solutions. The proposed theory can help us to understand that some types of hallucinations, mistakes, or biases of LMs could come from softmax bottleneck and their incapability of modeling the correct distribution. For example, there are 60% of male characters and 40% of female characters in our training corpus. The language generation model might be forced to assign more than 60% probability to male characters as being much more likely to output king than woman in Figure <ref type="figure">1</ref>.</p><p>Recently, <ref type="bibr" target="#b18">Narang et al. (2021)</ref>; Anonymous (2021) show that MoS is one of the few architecture modifications of transformer-based LM that can provide consistent improvements in downstream applications. Our work provides a fundamental reason why the multiple embedding representation is better, which could inspire more future studies that propose a better multiple-embedding architecture to improve LMs (e.g., multi-lingual BERT) or downstream applications. As examples, we list several possible future directions in Appendix H.</p><p>Finally, a better LM could lead to both positive and negative societal impacts, but they are not the focus of this paper. Generally speaking, this paper deepens our understanding of the weaknesses of modern LMs and we believe the knowledge can help us to design a better LM that increases the positive impacts and reduces the negative impacts in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix Overview</head><p>To demonstrate the wide applicability of our approaches, we conduct more experiments such as applying MFS to BERT in Appendix B. We also show more results and conduct more analyses in Appendix C to further support our conclusions.</p><p>Next, we provide technical details including the proof of our theorems in Appendix D, show that the structure weakness studied by Demeter et al. ( <ref type="formula">2020</ref>) is a special case of our theory in Appendix E, the method details in Appendix F, and the experiment details in Appendix G. Finally, in Appendix H, we list several directions that could be further studied in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experiments</head><p>We conduct the following five extra experiments to measure the linear dependency among word embeddings in LMs, extend our multi-facet approaches to BERT, confirm the source of the improvement comes from modeling multimodal distribution, and extend our synthetic experiments to include the output candidate words that have various types of relations and to include the template that favors the single embedding representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Linear Dependency among Words</head><p>Theorem 2 shows that when N words are linearly dependent after moving one of the embeddings with a short distance , the output softmax layer of a LM cannot output a large logit margin between two subsets of the N words. We want to measure in the pretrained word embedding and compare the from different sets of words or from different LMs.</p><p>Given a set of N words, we form a matrix by their word embeddings and estimate the value by the minimal eigenvalue of the matrix. We first want to verify that the four analogical words used in Section 5 indeed have a smaller compared to a randomly selected four words. Thus, we define the min eigenvalue ratio as S R , where R is the average of minimal eigenvalues from 1,000 sampled N word sets and S is the average of minimal eigenvalues from sets of words (e.g., analogical words from the Google analogy dataset). We analyze the ratio instead of because the average word embedding magnitudes in different LMs would affect the absolute value of .</p><p>In addition to analogical words, we also test sets of N similar words, which are composed by the nearest N − 1 words of every query word in the vocabulary, and test the N similar stop words by finding the nearest N − 1 words of every query word in a stop word list. <ref type="foot" target="#foot_7">6</ref>We plot the min eigenvalue ratio versus N in Figure <ref type="figure" target="#fig_2">3</ref> and compare the curves from three GPT LMs and two T5 LMs <ref type="bibr" target="#b21">(Raffel et al., 2020)</ref>. All the ratios are below 0 and decrease as N increases, which shows the analogical words and similar words indeed have significantly smaller especially for a large N . The low minimal eigenvalues and our theory support the recent empirical finds that LMs tend to be confused by the similar words <ref type="bibr" target="#b37">(Zagoury et al., 2021)</ref>. This figure also provides a potential explanation why the candidates often include stop words when multiple embeddings outperform the single embedding in Table <ref type="table" target="#tab_8">3 and Table 7</ref>.</p><p>Surprisingly, we find that a larger LM does not necessarily yield a larger ratio (i.e., embeddings of related words do not become more linearly independent as dimension or the size of the LM increases). All the LMs have very similar ratios of similar stop words. Compared to GPT-small, although GPT-J-6B (Wang and Komatsuzaki, 2021) has a significantly higher ratio for analogical words, its ratio for similar words is significantly lower. Besides, T5-11B has significantly lower ratios compared to T5-small. We need further investigation to understand the reason for this empirical finding and whether a larger LM suffers less from the limitation caused by the single embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Language Modeling using BERT</head><p>To demonstrate that our proposed method could improve the LMs other than GPT-2, we apply multifacet softmax, MFS, to BERT. We test the model on Wikipedia 2021 and the validation size is 0.25% of the whole corpus. After loading pretrained model, we train bert_base_cased for 100k batches and bert_large_cased for 30k batches.</p><p>The results are presented in Table <ref type="table" target="#tab_7">6</ref>. First, MoS outperforms Softmax on BERT. The results support the finding of <ref type="bibr" target="#b18">Narang et al. (2021)</ref> that the softmax bottleneck not only exists in the next word prediction tasks but also in the masked word prediction tasks. Similar to GPT-2, MFS at least doubles Softmax + Multi-partition (S1I1P4) 5.8520 5.8656 MoS <ref type="bibr" target="#b35">(Yang et al., 2018)</ref>   The smaller improvement supports the conclusion of our geometric analyses that the multi-mode ambiguity intensifies the softmax bottleneck. We only observe the one-directional context before the next target word in GPT-2, but we can observe the bi-directional context surrounding the masked target word in BERT. Thus, compared to next word prediction, the multi-mode ambiguity of the masked word prediction occurs less frequently when the masking probability is small (e.g., 15% in BERT). Since the masked word distribution only has a single mode most of the time but we sometimes still want the distribution to have multiple modes, multiple input hidden states can improve the performance by helping the facets to move more freely. On the other hand, multiple partitions are less useful because the distribution rarely has more than three modes.</p><formula xml:id="formula_14">f avg ct fct,1 fct,2 fct,3 MFS Softmax 1 MFS Softmax 2 MFS Softmax 3 MFS Avg f 1 ct,1 f 2 ct,1 f 3 ct,1 f 4 ct,1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Analysis of Improvement on Multimodal Distribution</head><p>To confirm that the perplexity improvements actually come from modeling the multimodal distribution, we define a metric to measure how multimode a distribution is, and then we can compare the perplexity improvement from multimodal distributions and the improvement from the distributions that are close to a single-mode distribution.</p><p>For the method with multiple embeddings, we first compute the weighted average of all the facets f avg ct = K k=1 π ct,k f ct,k , where we lower the influence of kth facet embedding f ct,k with lower prior weight π ct,k and f ct,1 = 1 J J j=1 f j ct,1 if J partitions are used. Figure <ref type="figure" target="#fig_3">4</ref> illustrates f avg ct and f ct,k using the example in the second column of Table <ref type="table" target="#tab_8">7</ref>.</p><p>We visualize the new average facet using the words that are closest to the f avg ct in the MFS Avg row of Table <ref type="table" target="#tab_8">7</ref>. We can see that the predictions of MFS Avg is different from MFS but similar to Softmax. This means there are indeed some other words between the actual next word and the other possibilities, which makes the predictions of MFS multi-mode.</p><p>Next, to quantify the difference between MFS and MFS Avg, we define multi-mode ratio , where P M could be either P M oS from equation 2 or P M P from equation 4. {y 1 , ..., y T } is the set of words with embeddings closest to f avg ct and {x 1 , ..., x T } is the set  of words with highest P M (x b |c t ). Using the Wikipedia context in Table <ref type="table" target="#tab_8">7</ref> as an example, the word project is retrieved by MFS but not by MFS Avg, so its multi-mode ratio for T = 2 is P M F S (hom|ct)+P M F S (dual|ct) P M F S (project|ct)+P M F S (hom|ct) = 0.049+0.046 0.096+0.049 ≈ 0.66. Figure <ref type="figure" target="#fig_3">4</ref> illustrates the relation between the MFS Softmax k and MFS Avg.</p><p>When the ratio is closer to 1, the context is less ambiguous and the prediction is closer to a singlemode distribution. We set T = 20 and call the prediction with multi-mode ratio smaller than 0.9 multimodal distribution and in Table <ref type="table" target="#tab_9">8</ref>,<ref type="foot" target="#foot_8">7</ref> we compare the loss (i.e., log of the perplexity) improvements in the multimodal distributions and the improvements in the nearly single-mode distributions.</p><p>Table <ref type="table" target="#tab_9">8</ref> shows that all the multiple embedding approaches have larger loss improvements when outputting multimodal distributions. The table shows the results based on GPT-2 Small and the same analysis using GPT-2 Medium also show the same trend. As we use multiple input hidden states and partitions, the differences would be enlarged. Especially when we compare MFS and MFS w/o Multi-partition, the loss improvements of highly ambiguous context is 7 or 8 times larger than the other loss improvements, which means a large portion of the overall improvement lies on a small percentage of ambiguous contexts. For the multimodal distribution in Wikipedia, the loss improvement between MFS and Softmax could reach 0.10, which is close to the improvement between GPT-2 Small and Medium (0.16). Thus, we expect that if the corpus has more ambiguous contexts, MFS could achieve larger overall loss improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Template-based Analysis on Similar or Dissimilar Nouns</head><p>To know whether the single embedding also has trouble modeling the distribution over nouns without the analogy relation, we let the different models learn to assign similarly high probabilities to two related nouns in our templates. One example in our synthesized dataset is "I love the banana and the lemon, and my favorite is the [MASK]". The nouns come from a hypernymy detection benchmark <ref type="bibr" target="#b25">(Shwartz et al., 2017)</ref> containing 25,498 noun pairs. The relations between nouns in the benchmark include synonym, antonym, attribute, meronym, hypernym, coordination, event, or random. We further split the noun pairs into two datasets based on their cosine similarity in the output word embedding space of our Softmax baseline. The pairs with the cosine similarity higher than the medium of all cosine similarities are put into the similar word set and the other pairs are put into the dissimilar word set.</p><p>The results are presented in Table <ref type="table" target="#tab_10">9</ref>. In terms of the training, validation, and testing perplexity, multi-embedding approaches consistently outperform the single-embedding baselines, though the margins are smaller than those from the analogous words. Moreover, the improvement gap is larger when the nouns are dissimilar. We hypothesize that as the word embeddings of nouns become further away from each other, the next word distribution is more likely to be multi-mode and thus could be better captured by multiple embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Adversarial Template Analysis</head><p>To test whether the proposed methods still can effectively utilize the information from the global word embeddings, we design an adversarial template to create the contexts that can only be completed by averaging the global word embeddings. For example, "Miami is not in Wisconsin but is in [MASK]=Florida".</p><p>In this task, the validation perplexity of Softmax, MoS, MFS w/o Multi-partition, and MFS are 2.50, 2.59, 2.54, and 2.88, respectively. Since multiple embeddings are not required, it is not surprising that Softmax performs the best. Nevertheless, the differences are smaller than the differences in Table <ref type="table" target="#tab_5">4</ref>. We believe that the similar losses are because multiple embeddings are a generalization of the single embedding, so GPT-2 could learn to generate the same embedding for all facets to mimic the behavior of single embedding if required.</p><p>The significantly worse performance of MFS here is caused by the multiple partition technique. This result supports our motivation of combining multiple partitions with multiple softmaxes and shows that multiple partitions handle ambiguous contexts better (as shown in Table <ref type="table" target="#tab_9">8</ref>) by sacrificing some global word embedding structures. Nevertheless, a corpus usually has more ambiguous contexts than the adversarial context tested here, so using multiple embeddings and multiple partitions performs better in Wikipedia and OpenWebText overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Results</head><p>We provide more numbers and analyses of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Ranking Metric in Language Modeling Experiments</head><p>We would like to verify that our perplexity improvements come from not only the slight probability differences of each candidate but also the better ranks of the candidates. Thus, in Table <ref type="table" target="#tab_11">10</ref>, we evaluate different models using mean reciprocal rank (MRR). Similar to the perplexity, the MRR improvement from Softmax to MFS is around 15% of the MRR improvement from GPT-2 Small to GPT-2 Medium, which is similar to the percentage of perplexity improvement. This suggests that MFS could lead to not only a better probability prediction but also a better candidate rank prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Perplexity Curves in Language Modeling Experiments</head><p>In Table <ref type="table" target="#tab_2">1</ref>, we only show the testing perplexity at the end of our training. In Figure <ref type="figure">6</ref>, we plot the validation perplexity decay curves during the training on OpenWebText. We can see that the performance ranking of each model is stable during the training, while the improvement of each enhancement may vary. For example, in GPT-2 Medium, the improvement of MFS over MFS w/o Multi-partition is more obvious in epoch 0.25 compared to epoch 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Perplexity Curves in Template Analysis</head><p>In Table <ref type="table" target="#tab_5">4</ref>, we only show the lowest validation perplexity after each of the ten epochs. In Figure <ref type="figure" target="#fig_5">5</ref>, we plot the training and validation perplexity decay curves.</p><p>The curves tell us that the multi-embedding models perform better in both training and validation perplexity. As we train the single-embedding models longer, the validation perplexity increases quickly, which implies that using a single embedding to model multimodal distribution could cause severe overfitting when we predict the next word given an ambiguous context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Stability in Language Modeling Experiments</head><p>In our case, training our model requires a huge amount of GPU resources for us, so it is not very 36.5 ± 0.7 39.7 ± 0.5 43.5 ± 0.4 52.2 ± 0.6 20.9 ± 0.4 37.7 ± 0.6 46.7 ± 0.6 MoS <ref type="bibr" target="#b35">(Yang et al., 2018)</ref> (3) 36.6 ± 0.8 40.2 ± 0.6 43.2 ± 0.6 52.1 ± 0.4 21.3 ± 0.6 38.4 ± 0.5 45.9 ± 0.6 MFS w/o Multi-partition 37.7 ± 0.7 42.0 ± 0.6 44.6 ± 0.5 52.6 ± 0.3 22.9 ± 0.4 39.5 ± 0.5 47.4 ± 0.4 MFS 36.9 ± 0.7 41.6 ± 0.7 44.4 ± 0.6 52.3 ± 0.6 23.1 ± 0.5 39.7 ± 0.6 46.9 ± 0.6</p><p>Table <ref type="table" target="#tab_2">11</ref>: ProtoQA performances on the crowdsourced development sets. The matching between prediction and ground truth is done by WordNet. All the numbers are percentages. Max answers top k implies only evaluating the top k answers from different LMs. Max incorrect top k indicates only evaluating the top answers that contain k errors. The highest average performances are highlighted and the standard errors are reported as the confidence interval.</p><p>feasible to train multiple times using multiple random seeds. We indeed try to use different random seeds for a few models and we confirm that the validation loss difference is at least ten times smaller than the improvement of different models.</p><p>To verify that our testing dataset is large enough to provide stable perplexity, we randomly split the testing dataset into 10 subsets and compute the standard error of the average testing perplexity of the 10 subsets. We find that the standard error is less than 0.02 perplexity in all models and datasets in Table <ref type="table" target="#tab_2">1</ref>. The standard error is much smaller than most of the improvements, which means our testing dataset is large enough to make the reported perplexity stable. The consistent improvements during the whole training process in Figure <ref type="figure" target="#fig_5">5</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 ProtoQA Results using WordNet</head><p>In Table <ref type="table">5</ref>, we report the metrics using exact matching. In Table <ref type="table" target="#tab_2">11</ref>, we report the metrics that match the prediction with the ground truth using <ref type="bibr">Word-Net (Miller, 1992)</ref> and find the scores show a similar trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Perplexity Improvement versus Model</head><p>Size <ref type="bibr" target="#b8">Kaplan et al. (2020)</ref> empirically demonstrate that increasing the model size would decrease the loss and their relation follows a scaling law. That is, we can plot the log of model size (i.e., parameter number) versus its loss as in Figure <ref type="figure" target="#fig_6">7</ref>, and if a new LM model could result in lines that are closer to the origin than the baselines, the new model is better in terms of the loss than only increasing the model size of the baselines.</p><p>From Figure <ref type="figure" target="#fig_6">7</ref>, we can see that the approaches using multiple embedding are better than the Softmax baseline using single embedding. Although the lines formed by MFS w/o Multi-partition and MFS are not always closer to the origin than MoS, our perplexity improvement from adding multiple input hidden states or multiple partitions cannot be solely explained by their extra parameters for several reasons:</p><p>• Compared to MoS, the line formed by MFS w/o Multi-partition becomes slightly closer to the origin when the model size is close to GPT-2 Medium.</p><p>• The improvement of MFS w/o Multipartitions (S3I9P1) is larger than the improvement of Softmax + Multi-input (S1I9P1) plus the improvement of MoS (S3I1P1) in BERT and GPT-2. For example, in BERT base, the perplexity improvement of Softmax + Multi-input, MoS (3), and MFS w/o Multi-partitions are 0.018, 0.016, and 0.047, respectively.</p><p>• Our multi-mode analyses in Appendix B.3 indicate that our enhancements, especially using multiple partitions, capture the multimodal distribution better. We expect that the overall perplexity improvement would be larger if the corpus contains more ambiguous contexts. We also conduct a preliminary experiment to confirm the claim. We add more ambiguous contexts into Wikipedia 2016 by mapping all the uppercased words into the [U N K] token. That is, we add another mode corresponding to the [U N K] token in many context positions. Then, we train and test the uncased BERT in this synthesized dataset. We found that the improvement of MFS w/o Multi-partition in this case can do significantly better than simply increasing the model size.</p><p>• Our enhancements only require some extra linear layers, which are usually more efficient than increasing the model size (e.g., by adding another transformer layer).</p><p>• Unlike increasing the model size, keep increasing the number of input hidden states or the number of partitions would lead to a smaller improvement. This suggests that MFS cannot keep storing more and more knowledge into its extra linear layers as in the architecture using a larger hidden state size or a deeper transformer encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 More Visualization</head><p>In Table <ref type="table" target="#tab_4">3</ref>, we compare the prediction of MFS and Softmax on GPT-2 Small. In the first two columns of Table <ref type="table" target="#tab_8">7</ref>, we present the examples from the models built on GPT-2 Medium in OpenWebText and Wikipedia 2021. We can see a similar pattern. The embedding of the correct answer is different from the embeddings of other possibilities, so Softmax assigns lower probabilities to the correct answer, while MFS does much better. This suggests that a larger model such as GPT-2 Medium suffers from the softmax bottleneck in a similar way.</p><p>In the last column of Table 7, we visualize an example in another synthetic experiment described in Appendix B.4. We can see that although there may not be any words between the appropriate candidates, the prediction of Softmax may still be biased toward one option much more than the other, while the prediction of MFS is much closer to the equally likely bimodal distribution we created in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Theorems</head><p>To prove Theorem 1, we first introduce a lemma. Assuming in the word embedding of GPT-2, woman + king = queen + man, we want to show that GPT-2 cannot output woman and king as the top two words in this lemma. This means we cannot find a hidden state h and a threshold τ &gt; 0 such that h T woman≥ τ and h T king≥ τ but h T queen&lt; τ and h T man&lt; τ . This example could be generalized into the following Lemma and Theorems. We can generalize the example as follows:</p><p>Lemma 1. Let the output word embeddings in the set W = {w l j = 0|j = 1...L} ∪ {w r j = 0|j = 1...R} satisfy −a l 1 w l 1 − ... − a l L w l L = a r 1 w r 1 + ... + a r R w r R , where their coefficient −a l 1 , ..., −a l L , a r 1 , ..., a r R are all positive constants and −a l 1 − ... − a l L ≥ a r 1 + ... + a r R . Then, there is no hidden state h and a threshold τ &gt; 0 that make min Proof. To prove by contradiction, we assume there is a h such that ∀w l j ∈ G, h T w l j ≥ τ and ∀w r j ∈ S, h T w r j &lt; τ . Thus, we can get</p><formula xml:id="formula_15">−a l 1 h T w l 1 − ... − a l L h T w l L ≥ −a l 1 τ − ... − a l L τ ≥ (a r 1 + ... + a r R )τ &gt; a r 1 h T w r 1 + ... + a r R h T w r R , which contradicts to −a l 1 w l 1 − ... − a l L w l L = a r 1 w r 1 + ... + a r R w r R .</formula><p>We can rephrase the condition and the conclusion to have our Theorem 1.</p><p>Theorem 1. If the nonzero output embeddings of N words in a set W are linearly dependent and on one side of a hyperplane through the origin, the single embedding representation cannot produce positive logits to a subset of the words in W that are higher than all the logits of the other words in W . 8</p><p>Proof. The set W = {w i = 0|i = 1...N } contain the embeddings of the N words. Based on the premise, we can write 0 = a 1 w 1 + ... + a N w N and min w i ∈W h T 0 w i &gt; 0, where h 0 is a normal vector of the hyperplane. At least one of the a i is negative. Otherwise, we will get the contradiction</p><formula xml:id="formula_16">0 = h T 0 0 = a 1 h T 0 w 1 + ... + a N h T 0 w N ≥ (a 1 + ... + a N ) min w i ∈W h T 0 w i &gt; 0.</formula><p>Similarly, at least one of a i is positive. We can move all the terms in 0 = a 1 w 1 +...+a N w N with negative a i to the left as −a l 1 w l</p><formula xml:id="formula_17">1 −...−a l L w l L = a r 1 w r 1 +...+a r R w r R . If −a l 1 − ... − a l L ≥ a r 1 + ... + a r R , we choose G = {w l j |j = 1...L}. Otherwise, we choose G = {w r j |j = 1...R}</formula><p>If we can have a hidden state such that the positive logits of words in G are always larger than the logits of the other words in W (let's call the complementary set S), there must exist τ &gt; 0 that can make min wg∈G h T w g ≥ τ and max ws∈S h T w s &lt; τ , which violates our Lemma 1.</p><p>Next, we would like to generalize our Theorem 1 by using a more practical condition where the word embeddings are almost linearly dependent. Notice that the theorem needs to assume the magnitude of the hidden state is limited. Otherwise, the margin could be arbitrarily magnified. In practice, the magnitude is not arbitrarily large in GPT-2 and BERT because a too large magnitude of hidden state could magnify the gradients too much to have a stable training process.</p><p>Theorem 2. Let the output word embeddings in the set W = {w i = 0|i = 1...N } satisfy w 1 = a 2 w 2 + ... + a N w N + ε, where the constant a 2 , ..., a N are neither all zero nor all neg-8 Notice that Theorem 1 does not cover the situations where the target top words have negative logits (i.e., some logits of the words in G are negative). In the single softmax model, we believe the situations rarely happen in the LMs empirically.</p><p>If some logits of the target top words are still positive, the words that are somehow similar to those words are very likely to also be positive, which would be ranked higher than the target top words with the negative logits.</p><p>If the logits of all the target top words are negative in some contexts, the logits of all the words would be negative. Then, the word embeddings with smaller magnitudes tend to have the logits closer to 0, so having the larger logits than the other negative logits. This means the prior probability of the words would be inversed when the hidden states sometimes produce all negative logits.</p><p>If a LM always uses negative logits to compute probability in all the contexts, Lemma 1 and Theorem 1 still hold if we set τ &lt; 0 and switch the choices of G and S.</p><p>ative and ||ε|| &lt; . Then, there must be a nontrivial partition P = {G, S} of W such that there is no hidden state ||h|| ≤ r and a threshold τ ≥ r that makes min wg∈G h T w g ≥ (1+δ)τ and max ws∈S h T w s &lt; τ , where δ = 2 1+ i=2...N |a i | .</p><p>Proof. We can first move all the terms with negative a i to the left as w 1 − a l 1 w l 1 − ... − a l L w l L = a r 1 w r 1 + ... + a r R w r R + ε. We perform proof by contradiction, so we assume the logits of the words in G can always be larger than (1 + δ)τ and the logits of the words in S can always be smaller than τ .</p><p>Case 1:</p><formula xml:id="formula_18">1 − a l 1 − ... − a l L ≥ a r 1 + ... + a r R , so 1 − a l 1 − ... − a l L ≥ 1+ i=2...N |a i | 2</formula><p>. We choose G = {w 1 , w l 1 , ..., w l L } and S = {w r 1 , ..., w r R }. Thus, we can get h T ε ≤ ||h||||ε|| ≤ r ≤ τ and</p><formula xml:id="formula_19">h T w 1 − a l 1 h T w l 1 − ... − a l L h T w l L (5) ≥(1 − a l 1 − ... − a l L )(1 + δ)τ (6) =(1 − a l 1 − ... − a l L )(1 + 2 1 + i=2...N |a i | )τ (7) ≥(1 − a l 1 − ... − a l L )(1 + 1 1 − a l 1 − ... − a l L )τ<label>(8)</label></formula><p>=(1 − a l 1 − ... − a l L + 1)τ (9) ≥(a r 1 + ... + a r R + 1)τ (10)</p><formula xml:id="formula_20">&gt;a r 1 h T w r 1 + ... + a r R h T w r R + h T ε,<label>(11)</label></formula><p>which contradict with The theory in Demeter et al. ( <ref type="formula">2020</ref>) is as follows: "Let C be the convex hull of the embeddings {x i } of a vocabulary V . If an embedding x i for word w i ∈ V is interior to C, then the maximum probability P (w i ) assigned to w i using a dot-product softmax is bounded by the probability assigned to at least one word w i whose embedding is on the convex hull"</p><formula xml:id="formula_21">w 1 −a l 1 w l 1 −...−a l L w l L = a r 1 w r 1 + ... + a r R w r R + ε. Case 2: 1 − a l 1 − ... − a l L &lt; a r 1 + ... + a r R . We choose G = {w r 1 , ..., w r R } and S = {w 1 , w l 1 , ..., w l L }. Therefore, a r 1 h T w r 1 + ... + a r R h T w r R (12) ≥(a r 1 + ... + a r R )(1 + 2 1 + i=2...N |a i | )τ (13) &gt;(a r 1 + ... + a r R )(1 + 1 a r 1 + ... + a r R )τ (14) =(a r 1 + ... + a r R + 1)τ (15) &gt;(1 − a l 1 − ... − a l L + 1)τ (16) &gt;h T w 1 − a l 1 h T w l 1 − ... − a l L h T w l L − h T ε.<label>(17</label></formula><p>The theory is a special case of our Lemma 1 if we only consider the hidden states that would lead to the positive logit of the interior word w i . To see that, we first find a constant a l i &gt; 1 such that a l i x i intersects with one supporting hyperplane of the convex hull. This intersection point could be expressed by j a r j x r j , where the word embeddings x r j are vertexes of C and j a r j = 1. As a result, we satisfy the condition of our Lemma 1: a l i x i = j a r j x r j and a l i &gt; j a r j . Please see an illustration in Figure <ref type="figure" target="#fig_8">8</ref> for an example. Then, Lemma 1 suggests that the logit h T x i cannot be larger than the logits of all the word embeddings h T x r j . This means at least one of the h T x r j on the convex hull would lead to a larger prediction probability, which is also the conclusion of the theory in <ref type="bibr">Demeter et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Method Details</head><p>When replacing the softmax layer in the pretrained LMs, we found that the initialization of the extra linear layers should make the initial prediction of LMs close to the prediction using a softmax layer, which is the architecture used in the pretraining. Otherwise, the performance would drop significantly. The initialization is especially important for BERT. To achieve the goal, we initialize the weights of the linear layers such that different facets are almost identical at the beginning and let the LMs gradually learn to output diverse facets during the training. Specifically, we can write the linear layer on the new hidden state L f k (q ct ) as</p><formula xml:id="formula_22">f ct,k = L f k (q ct ) = L I k h M ct + L B k GELU L h (⊕ i,m h M −m c t−i ) + b. (<label>18</label></formula><formula xml:id="formula_23">)</formula><p>We initialize L I k as an identity matrix, b ← 0, and</p><formula xml:id="formula_24">L B k ← U(− , )</formula><p>, where U is the uniform distribution and = 0.00005 if k = K. Otherwise, = 0. Consequently, all the facets f ct,k are initially close to the last hidden state of the original GPT-2 h M ct . Our baselines (e.g., Softmax, MoS, and DOC) also adopt the same way to initialize their weights.</p><p>We implement our models based on huggingface 9 <ref type="bibr" target="#b34">(Wolf et al., 2020)</ref>. Please see our codes for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Architecture Differences in BERT</head><p>The architecture of MFS for BERT is mostly the same as the one for GPT-2 and the differences are described in this subsection.</p><p>In GPT-2 the block of input hidden state is rightaligned with the last word to prevent seeing the ground truth. On the other hand, the block in BERT is centered at the masked word.</p><p>The softmax layer of BERT is slightly different from that of GPT-2. For example, BERT adds a bias term after the dot product between the hidden state and the output word embedding. We keep the bias term in our experiments. Besides, the pretrained BERT has a language modeling head including a linear layer, a GELU (Gaussian Error Linear Unit) layer <ref type="bibr" target="#b4">(Hendrycks and Gimpel, 2016)</ref>, and a layernorm layer <ref type="bibr">(Ba et al., 2016)</ref>, so instead of adding an extra linear layer as in GPT-2, we just use different language modeling heads to create different facets in BERT. All the heads are initialized using the weights in the pretrained BERT except that the linear layer is initialized as in Equation <ref type="formula" target="#formula_22">18</ref>when the multiple input hidden states are used and 9 https://huggingface.co/ the corresponding linear weights L B k ← U(− , ), where = 0.05 if k = K. Otherwise, = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Experimental Details</head><p>In this section, we describe some details of our experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Baselines</head><p>The MoS <ref type="bibr" target="#b35">(Yang et al., 2018)</ref> and DOC <ref type="bibr" target="#b28">(Takase et al., 2018)</ref> are originally designed for RNNbased LM. To improve their methods on pretrained Transformer-based LM and make their results more comparable to MFS, we change some of their implementation details.</p><p>MoS originally has a tanh layer before the softmax layers. However, we found that adding tanh hurts the performances of all methods we tested, especially the Softmax and MoS baselines. For example, after adding tanh and training GPT-2 Small for 0.4 epoch on Wikipedia, the validation perplexity degradation of Softmax is from 25.70 to 26.15, the degradation of MoS is from 25.42 to 25.83, and the degeneration of MFS is from 25.06 to 25.12. We suspect this is because GPT-2 is pretrained without the tanh layer and the tanh limits the magnitude of facets ||f ct,k ||, which could be viewed as the inverse of the temperature in the softmax layer. Therefore, we remove the tanh layer in all of our experiments. From the theoretical perspective, adding tanh does not invalidate our motivation because adding tanh does not change the total degree of freedom in all facet embeddings and the dimension of the hidden state.</p><p>In DOC, we use the hidden states of the last three transformer layers to compute the three facets and we set λ β = 0. Each facet is only determined by one layer of hidden state, so the first two facets cannot access the last hidden state. We found that the model quickly learns to only use the last facet because only the last hidden state is trained to perform the LM task in the pretrained models. This prevents the first two facets from getting any gradients and causes a starvation problem.</p><p>We tried an aggressive dropout trick to solve the starvation problem in DOC. If one of the softmaxes does not assign the highest probability to any of the correct next words in a batch, we consider that the corresponding facet starves, so we drop the other facets with some probability to ensure this starved facet receives some gradients and gradually gets back on track. However, our preliminary experi-ment suggests that the dropout trick cannot improve the perplexity of DOC. The dropout probability is either too low to solve the starvation problem or too high to preserve the knowledge learned from pretraining. Thus, we do not adopt this trick in our final experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Language Modeling</head><p>We download Wikipedia using http: //medialab.di.unipi.it/wiki/ Wikipedia_Extractor and OpenWebText using https://github.com/jcpeterson/ openwebtext. For Wikipedia, we preprocess the text using https://github.com/ attardi/wikiextractor. For OpenWeb-Text, we download the pre-filtered URLs in 2017 and 2018 and scrape the text on April 2021. When splitting the corpus into training, validation, and testing sets, we do not shuffle the data. Instead, we use the text near the end of the corpus as the validation and test set to reduce information leakage. To ensure every model is trained on the same data and accelerate the training in our machines, we split the training data into 20 consecutive partitions and load only one partition at a time during training. When training GPT-2 Medium, we only use the first 8 partitions to let the training be finished within a week. For BERT, we perform the sentence segmentation using SpaCy 10 and input one sentence into BERT at a time.</p><p>We set our hyperparameters (e.g., facet number K = 3 and W × H = 3 × 3 when using multiple input hidden states) based on the validation performance in Wikipedia 2016, the resulting model size, and the memory constraint in GPUs. To explore the limitation of the softmax layer, we untie the input word embeddings and output word embeddings in all of our experiments. The untying allows the LMs to arrange the output word embeddings more freely and allows us to observe if the resulting output word embeddings still cause multi-mode distribution. This is also the main reason the model size of our GPT-2 baseline is larger than the size of pretrained GPT-2 <ref type="bibr" target="#b20">(Radford et al., 2019)</ref>. We use AdamW <ref type="bibr" target="#b13">(Loshchilov and Hutter, 2019)</ref> optimizer and set the learning rate as 1e-5 and do not use the warm-up because the training starts from the pretrained models. The sequence length (i.e., bptt) is set as 200 for GPT-2 and 256 for BERT. The batch sizes are set as 4 for GPT-2 Small, 16 10 https://spacy.io/ for GPT-2 Large, 120 for BERT base, and 128 for BERT large.</p><p>The analyses in Table <ref type="table" target="#tab_3">2</ref> and Table <ref type="table" target="#tab_9">8</ref> use the first 4000 sequences in the validation dataset and all the methods are based on GPT-2 Small. We use PYCLD2<ref type="foot" target="#foot_9">11</ref> to distinguish between English and non-English text.</p><p>We use NVIDIA GeForce RTX 2080 for training GPT-2 Small and BERT base, GeForce RTX 8000 for training GPT-2 Medium, Tesla M40 for training BERT large. Since we start from the pretrained LM, we can finish training each LM within 2 weeks using 1 GPU for GPT-2 Small, BERT base, and GPT-2 Medium, and using 4 GPUs for training BERT large.</p><p>When testing the inference time in Table <ref type="table" target="#tab_2">1</ref>, we average the time of running NVIDIA TITAN X on 10,000 batches, where each batch contains 4 sequences whose length are 200.</p><p>When visualizing the prediction in Table <ref type="table" target="#tab_4">3</ref>, we exclude the non-ASCII symbol prediction from the top word list of all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Ambiguous Templates Analysis</head><p>Among the semantic relations in Google analogy dataset, we choose three different relations between locations: capital-common-countries, capital-world, city-in-state, and one relation between people: family. We exclude the currency category because their instance often does not form a parallelogram in the word embedding space <ref type="bibr">(Ethayarajh et al., 2019)</ref>. The templates we use are listed in Table <ref type="table" target="#tab_13">12</ref>. For the family category, our templates assume the words are not pronouns, so we exclude the set of four words that include he or she.</p><p>For each of the four words in an analogy instance (e.g., queen : king = woman : man), we would create 32 training or testing sequences<ref type="foot" target="#foot_10">12</ref> based on the diagonal words such as king or woman. Similarly, we would create 64 sequences in the edge datasets. Some words contain multiple word pieces and we average the losses of all word pieces during training and testing.</p><p>We split the synthesized sequences based on their word pair overlapping. First, we randomly sample half of the word pairs (e.g., king and queen) in each category as our training pairs. If both of the word pairs in an analogy instance are training  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 ProtoQA Evaluation</head><p>In our experiments, we use the scraped development set as our validation set and the crowdsourced development set as our test set. We do not test our methods on the test set of ProtoQA because the result of every submission would show up in their leaderboard and we do not want to overwhelm the leaderboard with our 15 trials. Due to our limited GPU resources, we compare the methods built on GPT-2 Medium rather than GPT-2 Large. To maximize the perplexity of the GPT-2 Medium model using Softmax on the scraped development set, we fine-tune our models using learning rate 3e-5 and warmup step 500.</p><p>The original paper <ref type="bibr">(Boratko et al., 2020)</ref> does not consider the frequency of the answer during the fine-tuning (i.e., the most possible answer and the least possible answer of each question appear in the training data with the same chance). In terms of the performance of the scraped development set, we find that weighting each answer based on the square root of its frequency is better than weighting each answer uniformly as in the original paper or weighting each answer based on its frequency, so we use the square root weighting to finetuning all our models.</p><p>During testing time, each model generates the answers using Nucleus Sampling <ref type="bibr" target="#b6">(Holtzman et al., 2020)</ref> with p = 0.9 and temperature = 1. Then, we collect all the words before the first period as an answer and drop the generated sentences without a period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Future Work</head><p>Capturing the next word distribution well given an ambiguous context could be important in some downstream applications. A next step could be investigating whether multiple facets lead to a better language generation model for the applications.</p><p>For example, we would like to know whether breaking the softmax bottleneck could reduce the hallucination of LMs (e.g., outputting queen when the reasonable next words should be king or woman) and increase the coherence of the generated text.</p><p>We also want to more systematically investigate whether modeling multi-mode distribution could help LMs to reduce the undesired bias and to better distinguish similar words <ref type="bibr" target="#b37">(Zagoury et al., 2021)</ref> as in Appendix B.4. <ref type="bibr" target="#b18">Narang et al. (2021)</ref>; Anonymous (2021) find that MoS can significantly improve the BERT-like LMs on natural language understanding (NLU) tasks when the LMs are trained from scratch. Although we find that the perplexity improvement of multi-embedding BERT is not as large as multiembedding GPT-2, pretraining using multiple embeddings does not decrease the inference speed of the BERT encoder on NLU tasks. This motivates the future studies that test if MFS also provides a larger improvement than MoS in NLU tasks.</p><p>Table <ref type="table" target="#tab_3">2</ref> suggests that multiple embeddings improve more in a non-English context. We wonder whether multiple embeddings are more beneficial to the LMs that are trained on a non-English dominating corpus. <ref type="bibr" target="#b18">Chung et al. (2021)</ref> discover that using a larger output embedding dimension improves the multilingual BERT. An interesting research question is whether the improvement comes from alleviating the softmax bottleneck and whether MFS could also lead to similar improvements in multilingual benchmarks.</p><p>The hidden state size of <ref type="bibr">GPT-3 175B (Brown et al., 2020)</ref> is huge <ref type="bibr">(12,</ref><ref type="bibr">288</ref>). An interesting question is whether some sets of output word embeddings in GPT-3 are still in a low-dimensional subspace and whether the softmax bottleneck is still a prominent problem on the road of pursuing general intelligence when such a large hidden state dimension is used. We also would like to know if models using multiple facets could reach a similar performance by a smaller hidden state size.</p><p>Recently, <ref type="bibr" target="#b1">Gao et al. (2019a)</ref>; <ref type="bibr" target="#b22">Rajaee and Pilehvar (2021);</ref><ref type="bibr">Cai et al. (2021)</ref>; <ref type="bibr" target="#b27">Su et al. (2022)</ref> point out the structure in the contextual embedding space prevents it from having an isotropic property. Our study and <ref type="bibr">Demeter et al. (2020)</ref> show that the structure in the word embedding space only models the global similarity between words and prevents the LM from outputting arbitrary context-dependent word distributions. We would like to know if we can discover a new LM architecture with a better contextual/word embedding space that could better model context-dependent word similarities and balance it with the global word similarities. In addition, our finding might be one of the reasons that we can improve the language generation quality by encouraging word embedding to be more isotropic <ref type="bibr" target="#b27">(Su et al., 2022)</ref>. <ref type="bibr" target="#b2">Gao et al. (2019b)</ref> show that a mixture of kernel functions outperforms MoS. Mixtape <ref type="bibr" target="#b36">(Yang et al., 2019)</ref> is another efficient solution to the softmax bottleneck, whose hidden state for each word is the weighted average of the facets where the weights are dynamically predicted. If only using one softmax (i.e., K = 1), our multiple partition method could be viewed as a special case of Mixtape that uses a global and binarized weight to prevent complications of predicting the weights of each word. Our results indicate that multiple partitions need to be combined with multiple softmax layers in order to gain consistent performance improvement. A potential future direction is to compare MFS with a mixture of kernel functions and Mixtape on the transformer-based LMs or combine MFS with a mixture of kernel functions and Mixtape to gain further improvements.</p><p>The results in <ref type="bibr" target="#b9">Kong et al. (2020)</ref> suggest that predicting n-grams could be better than predicting individual words in BERT in some applications. The total number of possible n-grams is several orders of magnitude higher than the number of individual tokens in the vocabulary. In addition, the linear dependency among n-grams might be common. For example, the embedding of the brown color + a dog may be similar to the embedding of the brown dog. The problem would be more serious as the length of the prediction sequence (n) increases, so predicting the next sentence using a single embedding might suffer from the softmax bottleneck even more. Therefore, our solutions to softmax bottleneck may lead to a better phrase representation or sentence representation in this type of self-supervised pretraining.</p><p>Finally, language modeling is only an example of extreme classification. The nearly ubiquitous usage of single embedding representation in the classification, self-supervised models (e.g., contrastive learning models), or recommendation problems provides many research opportunities. We believe that our theoretical results could guide researchers to identify the potential applications where the softmax bottleneck is serious and multi-embedding representation is accordingly helpful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>…</head><label></label><figDesc>king … queen … man … woman …</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between different architectures. The #S , #I , and #P are the number of softmaxes, input hidden states, and partitions, respectively. The green boxes refer to embeddings/vectors. The vocab means the embeddings of all words in the vocabulary. ⊕ refers to concatenation. L h , L f , and L π are linear projection layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Minimal eigenvalue ratios to indicate the linear dependency among different groups of N word embeddings BERT base after training on 100k batches Softmax (S1I1P1) SigSoftmax (S1I1P1) 5.8699 5.8749 Softmax + Multi-input (S1I9P1) Softmax + Multi-partition (S1I1P4) 5.8520 5.8656 MoS (Yang et al., 2018) (4) (S4I1P1) MoS (Yang et al., 2018) (3) (S3I1P1) DOC (Takase et al., 2018) (S3I3P1) 5.8523 5.8535 5.8547 MFS w/o Multi-partition (S3I9P1) MFS w/o Multi-input (S3I1P4) MFS (S3I9P4) 5.8231 5.8536 5.8231</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the MFS predictions given the Wikipedia context in the second column of Table7. The green circles mean the facet embeddings from MFS. The orange circle is the average of the facet embeddings (MFS Avg). The blue circles are the word embeddings that are close to the facet embeddings and MFS Avg. The word project is highlighted because it is the next word in our ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>as T b=1 P M (y b |ct) T b=1 P M (x b |ct)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: The perplexity curves for the language modeling tasks using the validation set of OpenWebText.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The log of model size versus the log of perplexity in the text set of OpenWebText. The group of points on the left comes from the models based on GPT-2 Small. The group of points on the right comes from the models based on GPT-2 Medium. The models are trained for 0.4 epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>wg∈G h T w g ≥ τ and max ws∈S h T w s &lt; τ , where G = {w l j |j = 1...L} and S = {w r j |j = 1...R}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: An example for explaining the connection between our Theorem 1 and the theorem from Demeter et al. (2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Dataset ↓ Templates Anology Between the $ARG1 and the $ARG2, I decided to first talk to the [MASK] (PersonThe $ARG1 and the $ARG2 are my favorites, and I especially love the [MASK] orThe $ARG1 and the $ARG2 happily live together. One day, bad luck happens to the [MASK] Person)The $ARG1 and the $ARG2 stay at my house, and I need to take care of the [MASK] Anology I went to $ARG1 and $ARG2 before, and I love one of the places more, which is [MASK] (Location $ARG1 and $ARG2 are my favorites, and I especially love[MASK]   or My uncle used to live in $ARG1 and $ARG2 but now, he is selling his house in [MASK] Location) The traveler plans to visit $ARG1 and $ARG2, and the traveler first arrives in [MASK] Similarity I love the $ARG1 and the $ARG2, and my favorite is the[MASK]   (Noun Yesterday, a man encountered the $ARG1 and the $ARG2. Today, he again saw the [MASK] or There are the $ARG1 and the $ARG2 in front of a woman, and she decides to pursue the [MASK] Noun)If you can choose the $ARG1 or the $ARG2, would you choose the[MASK]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Mixture of Softmax (Yang et al., 2018)</head><label></label><figDesc></figDesc><table><row><cell>dot product (a) Softmax fct,1 Softmax vocab</cell><cell>(b) Softmax + Multi-input fct,1 Softmax vocab</cell><cell cols="2">(d) Multi-facet Softmax (Ours) f 1 ct,1 f 3 ct,1 Softmax Softmax fct,3 Weighted Sum Softmax (c) fct,2 πct fct,1 fct,2 fct,3 πct Softmax Softmax Weighted Sum Softmax vocab vocab vocab vocab vocab vocab</cell></row><row><cell></cell><cell></cell><cell></cell><cell>f 2 ct,1</cell><cell>f 4 ct,1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Input Hidden States (#I)</cell></row><row><cell></cell><cell></cell><cell>GPT-2 encoder</cell><cell>Sec. 3.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Perplexity comparison between MFS (Ours) and baselines. #S, #I, #P are the number of softmaxes (i.e., K), input hidden states, and partitions, respectively. The top four baselines use a single softmax. OWT and Wiki are the test set perplexity of OpenWebText and Wikipedia 2021, respectively. The standard errors of all models are smaller than 0.02 perplexity. We also compare the number of parameters and the inference time on one batch.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Configuration</cell><cell></cell><cell>GPT-2 Small</cell><cell>GPT-2 Medium</cell></row><row><cell>Models ↓</cell><cell></cell><cell cols="3">#S #I #P</cell><cell>Size</cell><cell>Time OWT Wiki</cell><cell>Size</cell><cell>Time OWT Wiki</cell></row><row><cell>Softmax (GPT-2)</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="2">163.6M 84ms 18.72 24.06 407.3M 212ms 15.89 20.34</cell></row><row><cell cols="3">SigSoftmax (Kanai et al., 2018) 1</cell><cell>1</cell><cell>1</cell><cell cols="2">163.6M 91ms 18.63 24.06 407.3M 221ms 16.07 20.65</cell></row><row><cell>Softmax + Multi-input</cell><cell></cell><cell>1</cell><cell>9</cell><cell>1</cell><cell cols="2">169.5M 87ms 18.50 23.89 417.8M 219ms 15.76 20.29</cell></row><row><cell cols="2">Softmax + Multi-partition</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell cols="2">165.4M 88ms 18.77 24.08 410.5M 218ms 15.89 20.30</cell></row><row><cell cols="2">MoS (Yang et al., 2018) (4)</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell cols="2">165.4M 152ms 18.61 23.77 410.5M 299ms 15.75 20.08</cell></row><row><cell cols="2">MoS (Yang et al., 2018) (3)</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell cols="2">164.8M 130ms 18.63 23.81 409.4M 270ms 15.79 20.11</cell></row><row><cell cols="2">DOC (Takase et al., 2018)</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell cols="2">164.8M 130ms 18.69 24.02 409.4M 270ms 15.88 20.34</cell></row><row><cell cols="2">MFS w/o Multi-partition</cell><cell>3</cell><cell>9</cell><cell>1</cell><cell cols="2">171.9M 133ms 18.37 23.56 422.0M 276ms 15.65 20.06</cell></row><row><cell>MFS w/o Multi-input</cell><cell></cell><cell>3</cell><cell>1</cell><cell>4</cell><cell cols="2">166.6M 134ms 18.60 23.72 412.6M 275ms 15.71 20.08</cell></row><row><cell>MFS (Ours)</cell><cell></cell><cell>3</cell><cell>9</cell><cell>4</cell><cell cols="2">175.4M 138ms 18.29 23.45 428.3M 283ms 15.64 20.02</cell></row><row><cell></cell><cell cols="2">Non-English</cell><cell></cell><cell cols="2">English</cell></row><row><cell>Ratio in Corpus →</cell><cell></cell><cell>14%</cell><cell></cell><cell cols="2">86%</cell></row><row><cell>Softmax</cell><cell cols="5">13.50 (0.0%) 19.23 (0.0%)</cell></row><row><cell cols="6">MoS (Yang et al., 2018) (3) 13.19 (2.3%) 19.16 (0.4%)</cell></row><row><cell>MFS w/o Multi-partition</cell><cell cols="5">12.98 (3.8%) 18.91 (1.7%)</cell></row><row><cell>MFS (Ours)</cell><cell cols="5">12.83 (5.0%) 18.83 (2.1%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Perplexity of the GPT-2 Small in OpenWeb-Text. The percentages of the perplexity reduction compared to Softmax are presented in the parentheses.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The Elastic Endpoint Security and Elastic SIEM solutions mentioned in this post are now referred to as Elastic ... law and chance working together cannot generate CSI, either. Moreover, he claims that CSI I went to Paris and Germany before, and I love one of the places more, which is Germany Prediction visualization using a context in each dataset. We show the top three words with the highest prediction probabilities of each method. In the last three rows, we visualize the outputs of the softmax grey boxes in Figure2 (d), which model different modes of the next word distribution. The prediction target is boldfaced in the context and the predictions. ## indicates there is no space before the word.</figDesc><table><row><cell>Corpus →</cell><cell></cell><cell></cell><cell>OpenWebText</cell><cell></cell><cell></cell><cell cols="2">Wikipedia 2021</cell><cell></cell><cell cols="3">Analogy in Templates (Section 5)</cell></row><row><cell cols="2">Input Context ... Softmax (GPT-2)</cell><cell cols="3">the 0.087, E 0.043, End 0.039</cell><cell></cell><cell cols="3">the 0.174, this 0.054, if 0.038</cell><cell cols="3">Paris 0.893, France 0.045, Germany 0.033</cell></row><row><cell>MFS (Ours)</cell><cell cols="4">Elastic 0.220, the 0.089, EC 0.033</cell><cell cols="4">CSI 0.186, the 0.140, there 0.033</cell><cell cols="3">Paris 0.544, Germany 0.389, France 0.064</cell></row><row><cell>MFS Softmax 1</cell><cell cols="4">end 0.051, the 0.043, security 0.023</cell><cell></cell><cell cols="3">the 0.191, law 0.127, if 0.053</cell><cell cols="3">Paris 0.979, France 0.013, Germany 0.007</cell></row><row><cell>MFS Softmax 2</cell><cell cols="4">Elastic 0.652, EC 0.080, ES 0.046</cell><cell cols="4">the 0.191, there 0.049, this 0.047</cell><cell cols="3">Paris 1.000 Berlin 0.000 ##Paris 0.000</cell></row><row><cell>MFS Softmax 3</cell><cell></cell><cell cols="3">the 0.193, E 0.040, a 0.014</cell><cell cols="4">CSI 0.677, law 0.029, laws 0.019</cell><cell cols="3">Germany 0.852, France 0.139, China 0.004</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Diagonal (e.g., king or woman)</cell><cell></cell><cell></cell><cell cols="3">Edge (e.g., king or queen)</cell></row><row><cell cols="3">Analogy Relation Types → Models ↓</cell><cell cols="5">capital-capital-city-in-family valid common world state</cell><cell cols="4">capital-capital-city-in-family valid common world state</cell></row><row><cell cols="3">Softmax (GPT-2)</cell><cell>2.30</cell><cell>3.30</cell><cell>2.00</cell><cell>2.25</cell><cell>2.95</cell><cell>2.11</cell><cell>2.42</cell><cell>1.91</cell><cell>2.26</cell><cell>2.38</cell></row><row><cell cols="4">MoS (Yang et al., 2018) (3) 1.75</cell><cell>2.18</cell><cell>1.60</cell><cell>1.85</cell><cell>2.82</cell><cell>1.87</cell><cell>2.26</cell><cell>1.70</cell><cell>2.04</cell><cell>2.27</cell></row><row><cell cols="3">MFS w/o Multi-partition</cell><cell>1.72</cell><cell>2.13</cell><cell>1.59</cell><cell>1.82</cell><cell>2.52</cell><cell>1.84</cell><cell>2.23</cell><cell>1.72</cell><cell>1.96</cell><cell>2.16</cell></row><row><cell cols="2">MFS (Ours)</cell><cell></cell><cell>1.74</cell><cell>2.15</cell><cell>1.59</cell><cell>1.82</cell><cell>2.63</cell><cell>1.92</cell><cell>2.28</cell><cell>1.78</cell><cell>2.00</cell><cell>2.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Perplexity comparison of different GPT-2 Small models on the words with different types of analogy relations. The validation set (valid) includes all four types of relations.</figDesc><table><row><cell></cell><cell>Perplexity on Scraped</cell><cell></cell><cell cols="2">Max Answers</cell><cell></cell><cell></cell><cell>Max Incorrect</cell><cell></cell></row><row><cell>Models ↓</cell><cell>Development Set</cell><cell>Top 1</cell><cell>Top 3</cell><cell>Top 5</cell><cell>Top 10</cell><cell>Top 1</cell><cell>Top 3</cell><cell>Top 5</cell></row><row><cell>Softmax (GPT-2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1.5432 ± 0.0003 34.1 ± 0.8 35.2 ± 0.5 37.8 ± 0.4 45.0 ± 0.5 18.3 ± 0.4 30.7 ± 0.5 38.5 ± 0.6 MoS<ref type="bibr" target="#b35">(Yang et al., 2018)</ref> (3) 1.5407 ± 0.0004 33.9 ± 0.8 36.0 ± 0.6 37.7 ± 0.6 44.9 ± 0.4 18.3 ± 0.4 31.7 ± 0.6 38.2 ± 0.6 MFS w/o Multi-partition 1.5411 ± 0.0003 34.3 ± 0.7 36.7 ± 0.7 38.1 ± 0.5 45.2 ± 0.4 19.4 ± 0.4 32.0 ± 0.5 38.6 ± 0.3 MFS (Ours) 1.5402 ± 0.0005 34.1 ± 0.6 36.7 ± 0.5 38.6 ± 0.4 45.4 ± 0.5 19.7 ± 0.4 32.1 ± 0.4 39.7 ± 0.4 Table 5: ProtoQA performances. All the numbers except perplexity are the percentages of the predictions that match the ground truth exactly on the crowdsourced development set. Max answers top k implies only evaluating the top k answers. Max incorrect top k indicates only evaluating the top answers that contain k errors. The best average performances are highlighted and the standard errors are reported as the confidence interval.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Zhong Zhang, Nian Shao, ChongmingGao, Rui Miao,  Qinli Yang, and Junming Shao. 2022. Mixhead: Breaking the low-rank bottleneck in multi-head attention language models. Knowledge-Based Systems, page 108075.</figDesc><table><row><cell></cell><cell>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie</cell></row><row><cell></cell><cell>Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind</cell></row><row><cell></cell><cell>Neelakantan, Pranav Shyam, Girish Sastry, Amanda</cell></row><row><cell></cell><cell>Askell, Sandhini Agarwal, Ariel Herbert-Voss,</cell></row><row><cell></cell><cell>Gretchen Krueger, Tom Henighan, Rewon Child,</cell></row><row><cell></cell><cell>Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,</cell></row><row><cell></cell><cell>Clemens Winter, Christopher Hesse, Mark Chen,</cell></row><row><cell></cell><cell>Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin</cell></row><row><cell></cell><cell>Chess, Jack Clark, Christopher Berner, Sam Mc-</cell></row><row><cell></cell><cell>Candlish, Alec Radford, Ilya Sutskever, and Dario</cell></row><row><cell></cell><cell>Amodei. 2020. Language models are few-shot learn-</cell></row><row><cell></cell><cell>ers. In Advances in Neural Information Processing</cell></row><row><cell></cell><cell>Systems 33: Annual Conference on Neural Informa-</cell></row><row><cell></cell><cell>tion Processing Systems 2020, NeurIPS 2020, De-</cell></row><row><cell></cell><cell>cember 6-12, 2020, virtual.</cell></row><row><cell></cell><cell>Xingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth</cell></row><row><cell></cell><cell>Church. 2021. Isotropy in the contextual embed-</cell></row><row><cell></cell><cell>ding space: Clusters and manifolds. In 9th Inter-</cell></row><row><cell></cell><cell>national Conference on Learning Representations,</cell></row><row><cell></cell><cell>ICLR 2021, Virtual Event, Austria, May 3-7, 2021.</cell></row><row><cell></cell><cell>OpenReview.net.</cell></row><row><cell></cell><cell>Hyung Won Chung, Thibault Févry, Henry Tsai,</cell></row><row><cell></cell><cell>Melvin Johnson, and Sebastian Ruder. 2021. Re-</cell></row><row><cell></cell><cell>thinking embedding coupling in pre-trained lan-</cell></row><row><cell></cell><cell>guage models. In ICLR.</cell></row><row><cell></cell><cell>Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-</cell></row><row><cell></cell><cell>ian, Jacob Hilton, Reiichiro Nakano, Christopher</cell></row><row><cell>Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STO-RIUM: A Dataset and Evaluation Platform for</cell><cell>Hesse, and John Schulman. 2021. Training veri-fiers to solve math word problems. arXiv preprint arXiv:2110.14168.</cell></row><row><cell>Machine-in-the-Loop Story Generation. In Proceed-ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6470-6484, Online. Association for Computational</cell><cell>Thomas M Cover. 1967. The number of linearly in-ducible orderings of points in d-space. SIAM Jour-nal on Applied Mathematics, 15(2):434-439.</cell></row><row><cell>Linguistics.</cell><cell>David Demeter, Gregory Kimmel, and Doug Downey.</cell></row><row><cell>Anonymous. 2021. Scaling laws vs model architec-tures: How does inductive bias influence scaling? an extensive empirical study on language tasks. In ACL ARR Blind Submission.</cell><cell>2020. Stolen probability: A structural weakness of neural language models. In Proceedings of the 58th Annual Meeting of the Association for Compu-tational Linguistics, pages 2191-2197, Online. As-sociation for Computational Linguistics.</cell></row><row><cell>Ben Athiwaratkun and Andrew Wilson. 2017. Mul-timodal word distributions. In Proceedings of the 55th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers),</cell><cell>Kawin Ethayarajh, David Duvenaud, and Graeme Hirst. 2019. Towards understanding linear word analo-gies. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,</cell></row><row><cell>pages 1645-1656, Vancouver, Canada. Association for Computational Linguistics.</cell><cell>pages 3253-3262, Florence, Italy. Association for Computational Linguistics.</cell></row><row><cell>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-</cell><cell>Angela Fan, Thibaut Lavril, Edouard Grave, Armand</cell></row><row><cell>ton. 2016. Layer normalization. arXiv preprint</cell><cell>Joulin, and Sainbayar Sukhbaatar. 2020. Address-</cell></row><row><cell>arXiv:1607.06450.</cell><cell>ing some limitations of transformers with feedback</cell></row><row><cell>Christopher M Bishop. 1994. Mixture density net-</cell><cell>memory. arXiv preprint arXiv:2002.09402.</cell></row><row><cell>works.</cell><cell>William Fedus, Barret Zoph, and Noam Shazeer. 2021.</cell></row><row><cell>Michael Boratko, Xiang Li, Tim O'Gorman, Rajarshi</cell><cell>Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv</cell></row><row><cell></cell><cell>preprint arXiv:2101.03961.</cell></row></table><note>Das, Dan Le, and Andrew McCallum. 2020. Pro-toQA: A question answering dataset for prototypical common-sense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1122-1136, Online. Association for Computational Linguistics.Octavian Ganea, Sylvain Gelly, Gary Bécigneul, and Aliaksei Severyn. 2019. Breaking the softmax bottleneck via learnable monotonic pointwise nonlinearities. In Proceedings of the 36th International</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Perplexity of models building on BERT in Wikipedia 2021.</figDesc><table><row><cell></cell><cell>(4) (S4I1P1)</cell><cell cols="2">MoS (Yang et al., 2018) (3) (S3I1P1)</cell><cell>DOC (Takase et al., 2018) (S3I3P1)</cell></row><row><cell></cell><cell>5.8523</cell><cell></cell><cell>5.8535</cell><cell>5.8547</cell></row><row><cell cols="2">MFS w/o Multi-partition (S3I9P1)</cell><cell cols="2">MFS w/o Multi-input (S3I1P4)</cell><cell>MFS (S3I9P4)</cell></row><row><cell></cell><cell>5.8231</cell><cell></cell><cell>5.8536</cell><cell>5.8231</cell></row><row><cell cols="2">BERT large after training on 30k batches</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Softmax (S1I1P1)</cell><cell></cell><cell>SigSoftmax (S1I1P1)</cell></row><row><cell></cell><cell>4.8355</cell><cell></cell><cell>4.8354</cell></row><row><cell cols="2">Softmax + Multi-input (S1I9P1)</cell><cell cols="2">Softmax + Multi-partition (S1I1P4)</cell></row><row><cell></cell><cell>4.8305</cell><cell></cell><cell>4.8363</cell></row><row><cell cols="2">MoS (Yang et al., 2018) (4) (S4I1P1)</cell><cell cols="2">MoS (Yang et al., 2018) (3) (S3I1P1)</cell><cell>DOC (Takase et al., 2018) (S3I3P1)</cell></row><row><cell></cell><cell>4.8268</cell><cell></cell><cell>4.8291</cell><cell>4.8231</cell></row><row><cell cols="2">MFS w/o Multi-partition (S3I9P1)</cell><cell></cell><cell>MFS w/o Multi-input (S3I1P4)</cell><cell>MFS (S3I9P4)</cell></row><row><cell></cell><cell>4.8111</cell><cell></cell><cell>4.8287</cell><cell>4.8109</cell></row><row><cell>Corpus →</cell><cell>OpenWebText</cell><cell></cell><cell>Wikipedia 2021</cell><cell>Similar Nouns in Templates</cell></row><row><cell></cell><cell cols="2">... "Part of the Clinton inevitability</cell><cell>... The projective line over the dual</cell><cell>There are the militia and the enemy in front of</cell></row><row><cell></cell><cell cols="2">strategy was to lock down the usual</cell><cell>numbers was described by Josef</cell><cell>a woman, and she decides to pursue the</cell></row><row><cell>Input Context</cell><cell cols="2">Dan Nexon, a Georgetown professor suspects in left-liberal policy," said</cell><cell>nonzero nilpotent "n" satisfying. The Grünwald in 1906. This ring includes a</cell><cell>militia</cell></row><row><cell></cell><cell cols="2">who served as one of those informal</cell><cell>plane of dual numbers has a project</cell></row><row><cell></cell><cell>Sanders advisers. Nex</cell><cell></cell><cell></cell></row><row><cell>Softmax (GPT-2)</cell><cell cols="2">He 0.014, But 0.011, The 0.007</cell><cell>finite 0.062, hom 0.059, project 0.034</cell><cell>enemy 0.860, militia 0.111, Militia 0.005</cell></row><row><cell>MFS (Ours)</cell><cell cols="2">Nex 0.013, He 0.012, But 0.011</cell><cell>project 0.096, hom 0.049, dual 0.046</cell><cell>enemy 0.535, militia 0.433, enemies 0.029</cell></row><row><cell>MFS Avg</cell><cell>", He, But, The, In, And, (, It</cell><cell></cell><cell>hom, dual, finite, non, ", complex, unit</cell><cell>militia, enemy, Militia, enemies, militias</cell></row><row><cell>MFS Softmax 1</cell><cell cols="2">But 0.005, He 0.004, The 0.002</cell><cell>project 0.201, dual 0.075, finite 0.030</cell><cell>enemy 0.772, militia 0.189, Militia 0.017</cell></row><row><cell>MFS Softmax 2</cell><cell>Nex 0.260, " 0.028, He 0.023</cell><cell></cell><cell>hom 0.093, unit 0.040, non 0.037</cell><cell>militia 0.938, Militia 0.062, militias 0.000</cell></row><row><cell>MFS Softmax 3</cell><cell cols="2">He 0.025, But 0.022, The 0.014</cell><cell>finite 0.065, map 0.041, plane 0.030</cell><cell>enemy 1.000, enemies 0.000, foe 0.003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Prediction visualization using a context in each dataset. Each row visualizes a model as in Table3. The models are built on GPT-2 Medium in OpenWebText and Wikipedia and on GPT-2 Small in the synthesized dataset. MFS Avg shows the words that are closest to the average facet embedding in MFS. See the details in Appendix B.3. We underline the words that appear in the top predictions of both MFS and MFS Avg.</figDesc><table><row><cell>Corpus →</cell><cell></cell><cell></cell><cell>OpenWebText</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Wikipedia 2021</cell><cell></cell></row><row><cell>Improvement Model</cell><cell cols="10">S3I9P4 S3I9P4 S3I9P4 S3I9P1 S3I1P1 S3I9P4 S3I9P4 S3I9P4 S3I9P1 S3I1P1</cell></row><row><cell>Reference Model</cell><cell cols="10">S3I9P1 S3I1P1 S1I1P1 S1I9P1 S1I1P1 S3I9P1 S3I1P1 S1I1P1 S1I9P1 S1I1P1</cell></row><row><cell>Multi-mode Percentage (%)</cell><cell>10.03</cell><cell>10.03</cell><cell>10.03</cell><cell>4.81</cell><cell>3.24</cell><cell>5.85</cell><cell>5.85</cell><cell>5.85</cell><cell>2.66</cell><cell>3.05</cell></row><row><cell cols="11">Multi-mode Loss Improvement 0.0248 0.0474 0.0649 0.0203 0.0110 0.0282 0.0644 0.1000 0.0472 0.0295</cell></row><row><cell>Other Loss Improvement</cell><cell cols="10">0.0035 0.0158 0.0211 0.0086 0.0064 0.0033 0.0128 0.0219 0.0136 0.0100</cell></row><row><cell>Improvement Ratio</cell><cell>7.01</cell><cell>3.00</cell><cell>3.08</cell><cell>2.34</cell><cell>1.71</cell><cell>8.63</cell><cell>5.04</cell><cell>4.57</cell><cell>3.47</cell><cell>2.94</cell></row></table><note>the improvement of MoS. The most improvement over MoS comes from using multiple input hidden states while adding multiple partitions yield a small or no improvement. Finally, the improvement between MFS and Softmax is around 4.5%, which is much smaller than 15% in GPT-2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The loss improvement comparison between the Improvement Models and Reference Models. The models are named using their number of softmaxes, input hidden states, and partitions. Thus, S3I9P4 is MFS, S3I9P1 is MFS w/o Multi-partition, S1I9P1 is Softmax + Multi-input, S3I1P1 is MoS (3), and S1I1P1 is Softmax. Multimode Percentage is the percentage of the contexts where the Improvement Models output multimodal distribution. Multi-mode Loss Improvement refers to the average improvement when Improvement Models outputs multimodal distribution and Other Loss Improvement refers to the improvement of the contexts where the facets of Improvement Models are close to each other. Improvement Ratio divides Multi-mode Loss Improvement by Other Loss Improvement.</figDesc><table><row><cell>non</cell><cell>unit</cell></row><row><cell>dual</cell><cell></cell></row><row><cell>hom</cell><cell></cell></row><row><cell>project</cell><cell></cell></row><row><cell>finite</cell><cell></cell></row><row><cell>plane</cell><cell></cell></row><row><cell>map</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Perplexity comparison of different models on the similar words or dissimilar words. The models are based on GPT-2 Small and trained in OpenWebText.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Dissimilar Words</cell><cell></cell><cell></cell><cell>Similar Words</cell></row><row><cell>Models ↓</cell><cell cols="7">Testing Validation Training Testing Validation Training</cell></row><row><cell>Softmax</cell><cell></cell><cell>1.97</cell><cell>1.98</cell><cell>1.95</cell><cell>2.16</cell><cell>2.16</cell><cell>2.17</cell></row><row><cell>MoS (3)</cell><cell></cell><cell>1.81</cell><cell>1.80</cell><cell>1.69</cell><cell>2.05</cell><cell>2.05</cell><cell>1.87</cell></row><row><cell>MFS w/o Multi-partition</cell><cell></cell><cell>1.78</cell><cell>1.79</cell><cell>1.70</cell><cell>2.04</cell><cell>2.06</cell><cell>1.88</cell></row><row><cell>MFS</cell><cell></cell><cell>1.79</cell><cell>1.79</cell><cell>1.69</cell><cell>2.02</cell><cell>2.05</cell><cell>1.89</cell></row><row><cell>GPT-2 Small after 1 epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Softmax (S1I1P1)</cell><cell></cell><cell></cell><cell cols="2">SigSoftmax (S1I1P1)</cell><cell></cell><cell></cell></row><row><cell>0.5494</cell><cell></cell><cell></cell><cell>0.5489</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Softmax + Multi-input (S1I9P1)</cell><cell></cell><cell cols="3">Softmax + Multi-partition (S1I1P4)</cell><cell></cell><cell></cell></row><row><cell>0.5508</cell><cell></cell><cell></cell><cell>0.5492</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoS (Yang et al., 2018) (4) (S4I1P1)</cell><cell></cell><cell cols="3">MoS (Yang et al., 2018) (3) (S3I1P1)</cell><cell cols="3">DOC (Takase et al., 2018) (S3I3P1)</cell></row><row><cell>0.5501</cell><cell></cell><cell></cell><cell>0.5499</cell><cell></cell><cell></cell><cell>0.5494</cell></row><row><cell>MFS w/o Multi-partition (S3I9P1)</cell><cell></cell><cell></cell><cell cols="2">MFS w/o Multi-input (S3I1P4)</cell><cell></cell><cell cols="2">MFS (S3I9P4)</cell></row><row><cell>0.5515</cell><cell></cell><cell></cell><cell>0.5502</cell><cell></cell><cell></cell><cell>0.5519</cell></row><row><cell>GPT-2 Medium after 0.4 epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Softmax (S1I1P1)</cell><cell></cell><cell></cell><cell cols="2">SigSoftmax (S1I1P1)</cell><cell></cell><cell></cell></row><row><cell>0.5665</cell><cell></cell><cell></cell><cell>0.5650</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Softmax + Multi-input (S1I9P1)</cell><cell></cell><cell cols="3">Softmax + Multi-partition (S1I1P4)</cell><cell></cell><cell></cell></row><row><cell>0.5677</cell><cell></cell><cell></cell><cell>0.5665</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MoS (Yang et al., 2018) (4) (S4I1P1)</cell><cell cols="3">MoS (Yang et al., 2018) (3) (S3I1P1)</cell><cell></cell><cell cols="2">DOC (Takase et al., 2018) (S3I3P1)</cell></row><row><cell>0.5674</cell><cell></cell><cell></cell><cell>0.5672</cell><cell></cell><cell></cell><cell>0.5665</cell></row><row><cell>MFS w/o Multi-partition (S3I9P1)</cell><cell></cell><cell></cell><cell cols="2">MFS w/o Multi-input (S3I1P4)</cell><cell></cell><cell cols="2">MFS (S3I9P4)</cell></row><row><cell>0.5685</cell><cell></cell><cell></cell><cell>0.5677</cell><cell></cell><cell></cell><cell>0.5685</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>MRR (mean reciprocal rank) of different models in OpenWebText. Larger is better.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>further sup-</figDesc><table><row><cell>2.95 3.00</cell><cell cols="2">Softmax MoS (3) MFS -Multi-partition MFS</cell></row><row><cell>2.90</cell><cell></cell><cell></cell></row><row><cell>loss</cell><cell></cell><cell></cell></row><row><cell>2.85</cell><cell></cell><cell></cell></row><row><cell>2.80</cell><cell></cell><cell></cell></row><row><cell>2.75</cell><cell></cell><cell></cell></row><row><cell>19.0</cell><cell>19.2 log(number of parameters) 19.4 19.6</cell><cell>19.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>The templates used in the analysis. The first four templates are for the analogy relations from the capital-common-countries, capital-world, and city-in-state categories. The next four templates are for the analogy relations from the family category. The final four templates are for similar or dissimilar nouns.pairs, the instance is put into our training set. If only one of the word pairs is a training pair, the instance would belong to our validation set. The rest of the instances form our testing set. During the fine-tuning, we evaluate a model using the validation set after each epoch and select the model based on its best validation perplexity.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Section</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">.1 provides more background knowledge about the parallelogram shape and the softmax bottleneck.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">Notice that some LMs such as BERT add a bias term for each word before the softmax layer. For simplicity, our theoretical analyses focus on the LMs without the bias term such as GPT-2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">We remove the tanh layer in the original MoS to improve its performance on GPT-2. See Appendix G.1 for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">In this work, we simply put the J × n + jth word into jth partition (e.g., when the number of partitions J = 4, the first partition includes the words with indexes 0, 4, 8, ... ). This simple global partitioning method reduces the chance of putting all the interfering words and candidates in the same partition, while minimizing the extra computational cost in our PyTorch implementation because PyTorch supports strided index slicing without copying the variable.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5">the words are related in a certain way<ref type="bibr" target="#b26">(Sigman and Cecchi, 2002)</ref>. We cannot expect the training corpora to contain the ambiguous contexts with so many possible next words.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><ref type="bibr" target="#b35">Yang et al. (2018)</ref> propose the concept of softmax bottleneck, which points out that the dot product in the softmax layer restricts the representation power of outputting arbitrary conditional probabilities. It also proposes MoS to break the softmax bottleneck in an RNN-based LM.<ref type="bibr" target="#b7">Kanai et al. (2018)</ref> andGanea et al. (2019)  add nonlinearities into the softmax layer to break the bottleneck more effi-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7">We find that some rare words or special characters might have nearly identical word embeddings due to the lack of training instances, so we exclude half of rarer word pieces in the vocabulary and exclude the word pieces whose first character is not a space. The rarity of a word piece is determined by the l2 norm of its word embedding.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8">We also tried T=5 or 10 and the trends are similar. If we set the threshold smaller than 0.9, the improvement ratios (e.g., MFS over MoS) would increase but the multi-mode percentages would decrease.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9">https://github.com/aboSamoor/pycld2)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10">2 (diagonal words) × 4 (templates) × 2 (word orders in the template) × 2 (possible next words)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgement</head><p>We thank Michael Boratko, Jay Yoon Lee, Sabrina J. Mielke, Steve Cheng-Xian Li, and the anonymous reviewers for their constructive feedback. This work was supported in part by the Center for Data Science and the Center for Intelligent Information Retrieval, in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction, in part by the IBM Research AI through the AI Horizons Network, in part using high performance computing equipment obtained under a grant from the Collaborative R&amp;D Fund managed by the Massachusetts Technology Collaborative, in part by the National Science Foundation (NSF) grant numbers DMR-1534431, IIS-1763618,  and IIS-1955567, and  in part by the Office of Naval Research (ONR) via Contract No. N660011924032 under Subaward No. 123875727 from the University of Southern California. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
				<meeting>Machine Learning Research<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06">2019, 9-15 June 2019</date>
			<biblScope unit="page" from="2073" to="2082" />
		</imprint>
	</monogr>
	<note>Conference on Machine Learning, ICML</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation degeneration problem in training natural language generation models</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019a. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring kernel functions in the softmax layer for contextual word classification</title>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Herold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Spoken Language Translation</title>
				<meeting>the 16th International Conference on Spoken Language Translation<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stirling numbers and a geometric, structure from voting theory</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><surname>Tideman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series A</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sigsoftmax: Reanalysis of the softmax bottleneck</title>
		<author>
			<persName><forename type="first">Sekitoshi</forename><surname>Kanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhiro</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Yamanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuichi</forename><surname>Adachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="284" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.733</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><forename type="middle">Dal</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Choy ; Autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Cherepanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mankowitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07814</idno>
	</analytic>
	<monogr>
		<title level="m">Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with alphacode</title>
		<title level="s">Cyprien de Masson</title>
		<meeting><address><addrLine>Esme Sutherland Robson</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Wanli: Worker and ai collaboration for natural language inference dataset creation</title>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05955</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kernelized bayesian softmax for text generation</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="12487" to="12497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
				<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05">2013. December 5-8, 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop</title>
				<meeting><address><addrLine>Harriman, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-02-23">1992. February 23-26, 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compositional attention: Disentangling search and retrieval</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharath</forename><surname>Chandra Raparthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karishma</forename><surname>Malkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11972</idno>
		<title level="m">Do transformer modifications transfer across implementations and applications? arXiv preprint</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the softmax bottleneck of recurrent language models</title>
		<author>
			<persName><forename type="first">Dwarak</forename><surname>Govind Parthiban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13640" to="13647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A cluster-based approach for improving isotropy in contextual embedding space</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rajaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.73</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<title level="s">Short Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond accuracy: Behavioral testing of NLP models with CheckList</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.442</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4902" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hypernyms under siege: Linguistically-motivated artillery for hypernymy detection</title>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Schlechtweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="65" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Global organization of the wordnet lexicon</title>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Sigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><forename type="middle">A</forename><surname>Cecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1742" to="1747" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06417</idno>
		<title level="m">A contrastive framework for neural text generation</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Direct output connection for a high-rank language model</title>
		<author>
			<persName><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1489</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4599" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01075</idno>
		<title level="m">Omninet: Omnidirectional representations from transformers</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What do you learn from context? probing for sentence structure in contextualized word representations</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Najoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gpt-j-6b: A 6 billion parameter autoregressive language model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How can bert help lexical semantics tasks?</title>
		<author>
			<persName><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02929</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07178</idno>
		<title level="m">Symbolic knowledge distillation: from general language models to commonsense models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno>ICLR 2018</idno>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. Open-Review</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mixtape: Breaking the softmax bottleneck efficiently</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="15922" to="15930" />
		</imprint>
	</monogr>
	<note>Vancouver</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What&apos;s the best place for an ai conference, vancouver or _: Why completing comparative questions is difficult</title>
		<author>
			<persName><forename type="first">Avishai</forename><surname>Zagoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Einat</forename><surname>Minkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14292" to="14300" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
