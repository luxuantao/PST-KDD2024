<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward a computer vision-based wayfinding aid for blind persons to access unfamiliar indoor environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-05-24">24 May 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yingli</forename><surname>Tian</surname></persName>
							<email>ytian@ccny.cuny.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chucai</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aries</forename><surname>Arditi</surname></persName>
							<email>arditi@visibilitymetrics.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Y. Tian (B) Electrical Engineering Department</orgName>
								<orgName type="department" key="dep2">The City College, and Graduate Center</orgName>
								<orgName type="institution">City University of New York</orgName>
								<address>
									<postCode>10031</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">The City College</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">City University of New York</orgName>
								<address>
									<postCode>10031</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">The Graduate Center</orgName>
								<orgName type="institution">City University of New York</orgName>
								<address>
									<postCode>10036</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Visibility Metrics LLC</orgName>
								<address>
									<postCode>10514</postCode>
									<settlement>Chappaqua</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward a computer vision-based wayfinding aid for blind persons to access unfamiliar indoor environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-05-24">24 May 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">85B0225FC6A3A13CB105B9E01F6C844F</idno>
					<idno type="DOI">10.1007/s00138-012-0431-7</idno>
					<note type="submission">Received: 23 May 2011 / Revised: 10 March 2012 / Accepted: 23 April 2012 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Indoor wayfinding</term>
					<term>Computer vision</term>
					<term>Object detection</term>
					<term>Text extraction</term>
					<term>Optical character recognition (OCR)</term>
					<term>Blind/visually impaired persons</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Independent travel is a well-known challenge for blind and visually impaired persons. In this paper, we propose a proof-of-concept computer vision-based wayfinding aid for blind people to independently access unfamiliar indoor environments. In order to find different rooms (e.g. an office, a laboratory, or a bathroom) and other building amenities (e.g. an exit or an elevator), we incorporate object detection with text recognition. First, we develop a robust and efficient algorithm to detect doors, elevators, and cabinets based on their general geometric shape, by combining edges and corners. The algorithm is general enough to handle large intraclass variations of objects with different appearances among different indoor environments, as well as small inter-class differences between different objects such as doors and doorlike cabinets. Next, to distinguish intra-class objects (e.g. an office door from a bathroom door), we extract and recognize text information associated with the detected objects. For text recognition, we first extract text regions from signs with multiple colors and possibly complex backgrounds, and</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Robust and efficient indoor object detection can help people with severe vision impairment to independently access unfamiliar indoor environments and avoid dangers <ref type="bibr" target="#b2">[3]</ref>. While GPS-guided electronic wayfinding aids show much promise in outdoor environments, there are few indoor orientation and navigation aids <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>. Computer vision technology in principle has the potential to assist blind individuals to independently access, understand, and explore such environments. Yet it remains a challenge for the following four reasons. First, there are large intra-class variations of appearance and design of objects in different architectural environments. Second, there are relatively small interclass variations of different object models. Third, relative to richly textured and colored objects in natural scene or outdoor environments, most indoor objects are man made and have little texture. Feature descriptors which work well for outdoor environments may not effectively describe indoor objects. Finally, object with large view variations and often only parts of object (within the field of view) are captured when a blind user moves. An effective indoor wayfinding aid should handle object occlusion and view variations.</p><p>In our approach, we exploit the fact that context information (including signage) plays an important role in navigation and wayfinding for sighted persons. Signage is particularly important for discriminating between similar objects in indoor environments such as elevators, bathrooms, exits, and office doors. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the basic shapes of a bathroom, an exit, a laboratory, and an elevator are very similar. It is very difficult to distinguish them without using the associated context information.</p><p>To improve the ability of people who are blind or have significant visual impairments to independently access, understand, and explore unfamiliar indoor environments, we propose a proof-of-concept navigation and wayfinding prototype system by using a single camera to detect and recognize doors and elevators. The system can further detect text signage associated with the detected object. Our object detection method is based on general geometric shape by analyzing configuration of edges and corners. The proposed algorithm is robust, efficient, and generic enough to handle large intraclass variations of object appearances across different environments, as well as small inter-class variations of different objects such as doors and door-like shape cabinets. In some cases, text information from the signage associated with the detected objects is extracted in order to distinguish more subtle differences, such as distinguishing an office door from a bathroom door. Note that due to the Americans with Disabilities Act and other legislation, most doors in public accommodations are now required to be labeled with appropriate signage that includes Braille. Braille is difficult for blind users to locate haptically, however, and is of limited value in wayfinding. See <ref type="bibr" target="#b0">[1]</ref> for a discussion of these issues. Our text extraction method is robust to indoor signage with multiple colors and complex backgrounds. The extracted text is then recognized by using off-the-shelf optical character recognition (OCR) software. The object type, relative position, and text information are presented as speech for blind travelers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Computer vision-based technologies for blind persons</head><p>There have been many previous efforts to apply computer vision-based technologies to aid blind persons <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. A survey about recent research on wearable obstacle avoidance electronic travel aids for people with visual impairments can be found in paper <ref type="bibr" target="#b9">[10]</ref>. The vOICe vision technology offers the experience of live camera views through proprietary image-to-sound renderings <ref type="bibr" target="#b39">[40]</ref>. The Smith-Kettlewell Eye Research Institute developed a series of camera phone-based technology tools and methods for understanding, assessment, and rehabilitation of blindness and visual impairment, including text detection and loca-tion <ref type="bibr" target="#b32">[33]</ref>, crosswalk identification and location <ref type="bibr" target="#b17">[18]</ref>, and a wayfinding system based on machine-readable codes applied to the environment <ref type="bibr" target="#b24">[25]</ref>. Chen and Yuille <ref type="bibr" target="#b6">[7]</ref> developed an AdaBoost learning-based algorithm to detect and read text in natural scenes. Zandifar et al. <ref type="bibr" target="#b43">[44]</ref> used one head-mounted camera together with existing OCR techniques to detect and recognize text in outdoor environments and then convert the text to speech. Everingham et al. conceived of a wearable mobility aid for people with low vision for outdoor scene classification in a Markov random field model framework based on color information <ref type="bibr" target="#b13">[14]</ref>. Although many efforts of finding ways to apply vision technology helping blind people understand their surroundings have been made, no approach as yet has been widely or successfully adopted. Recently, our efforts for developing robust algorithms of indoor object detection in unfamiliar environments demonstrated promising results for indoor wayfinding and navigation applications <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Indoor object detection</head><p>Object detection and recognition is a fundamental component of wayfinding. The human visual system is very powerful, selective, robust, and fast <ref type="bibr" target="#b20">[21]</ref> and can discriminate on the order of tens of thousands of different object categories <ref type="bibr" target="#b3">[4]</ref> In stark contrast to the exquisite capabilities of human vision, it is extremely difficult to build robust and selective computer vision algorithms for general object detection and recognition. In this paper, we simplify the problem by focusing on object detection for indoor wayfinding and navigation for the following reasons: (1) lighting and luminance are relatively stable in indoor environments; <ref type="bibr" target="#b1">(2)</ref> recognition and localization of a small and well-defined set of objects such as doors, elevators, bookshelf/cabinet, and signage provide a smaller subset of information likely to be particularly useful for blind travelers; (3) indoor environments are structured by artificial objects with relatively standard size, shape, and location; and (4) indoor environments are relatively safe for blind users who participate in our research.</p><p>Among indoor objects, doors are important landmarks for wayfinding and navigation and play a significant role in providing transition points between separated spaces as well as entrance and exit information. Therefore, reliable and efficient door detection is a key component of an effective indoor wayfinding aid. Most existing door detection approaches for robot navigation employ laser range finders, sonar, or stereo vision to obtain distance data to refine the detection results from cameras <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref>. However, the problems of portability, high-power, and high cost in these systems with complex and multiple sensors, limit their feasibility in wayfinding systems for visually impaired people. To reduce the cost and complexity of the device and enhance the portability, we use a single camera in our system. A few algorithms using  <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Chen and Birchfield <ref type="bibr" target="#b7">[8]</ref> trained an Ada-Boost classifier to detect doors by combining the features of pairs of vertical lines, concavity, gap between the door and floor, color, texture, kick plate, and vanishing point. However, in practice, some of these features (e.g. a perceptible gap below a door and the floor and a kick plate) are not always present in different instances of a door. In <ref type="bibr" target="#b26">[27]</ref>, an algorithm is described that detects the doors of one building, where all the doors are of similar color, by using color and shape features. It would fail, however, if the colors of the doors varied. Munoz-Salinas et al. <ref type="bibr" target="#b25">[26]</ref> developed a doorframe model-based algorithm by using Hough Transform to extract frame segments. They subsequently use fuzzy logic to analyze relationships between the segments. Their algorithm, however, cannot discriminate doors from other large rectangular objects, such as bookshelves, cabinets, and cupboards.</p><p>To overcome the above limitations, we have developed an image-based door detection algorithm to detect doors which may have variable appearance, by adopting a very general geometric door model that utilizes the general and stable features of doors-edges and corners. Furthermore, our proposed algorithm is able to differentiate doors from other objects with door-like shape and size by analyzing geometric cues that are apparent when the door is inset within a wall, such as is especially common with elevator doors (see Fig. <ref type="figure" target="#fig_4">4a</ref>, b for illustration). Our detection results (see below) demonstrate that our door detection algorithm is general and robust to different environments with a wide variety of color, texture, occlusions, illumination, scales, and viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Context information extraction and recognition</head><p>Context information obviously plays an important role in human object identification and detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>. Paletta and Greindl <ref type="bibr" target="#b30">[31]</ref>, for example, extracted context from simple features to determine regions of interest in a multi-stage object detection process. Luo et al. <ref type="bibr" target="#b23">[24]</ref> developed a spatial context-aware object detection system to improve the accuracy of natural object detection. Divvala et al. <ref type="bibr" target="#b11">[12]</ref> investigated the different types of contextual information on a standard, highly regarded test set (the PASCAL VOC 2008) by incorporating contextual information into a post-process which re-scores detection hypotheses based on their coincidence with the various context cues.</p><p>Context information includes many kinds of visual features. In our method, we focus on finding and extracting a particularly useful kind of feature that is particularly amenable to computer vision analysis -text from signage. In recent years, there have been many efforts to identify and extract text regions from images. Dubey <ref type="bibr" target="#b12">[13]</ref> used statistics of the frequency of vertical stroke occurrences, while Dinh et al. <ref type="bibr" target="#b10">[11]</ref> focused on the consistency of stroke width. Neither of these algorithms performs well when the number of continuous text characters is small. Dense intensity variation based on the edge map is another potentially useful texture-like feature of text characters. Shivakumara et al. <ref type="bibr" target="#b33">[34]</ref> worked on different characteristics of edge detectors. Wan et al. <ref type="bibr" target="#b40">[41]</ref> performed edge classification by corner information. Liu et al. <ref type="bibr" target="#b21">[22]</ref> described edge features by defining six characteristic values involved in four dominant directions of edge detection. Different sliding windows are defined to transverse the whole image to extract text information. In addition to dense intensity variations, Liu et al. <ref type="bibr" target="#b22">[23]</ref> brought in texture features of text characters. Wong et al. <ref type="bibr" target="#b41">[42]</ref> applied maximum gradient difference (MGD) to detect potential line segments that constitute the text regions. The ridge points with local extreme intensity variation were employed in <ref type="bibr" target="#b38">[39]</ref> to describe text strokes at a high resolution and text orientations at a low resolution. However, the calculation of both MGD and ridge points was sensitive to background noise.</p><p>We propose a new and robust algorithm to extract text from signage. Both structural and topological features of text characters in the same text string are employed to refine the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">General indoor object detection algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Door detection</head><p>To robustly detect doors in different environments, we use edges and corners to model the geometric shape of a door frame, which contains two horizontal lines and two vertical lines between four corners. We extract edges using the Canny edge detector <ref type="bibr" target="#b5">[6]</ref> and corners through the corner detector described in <ref type="bibr" target="#b15">[16]</ref>. We further propose a matching method that combines edges and corners rather than directly detecting door frames. This matching process can avoid some pitfalls of existing line detection algorithms, such as missing start and end points, sensitivity to parameters, and unwanted merging or splitting of lines. In addition, we use perspective geometry to estimate the relative position of the detected door for the blind user. Furthermore, in combination with information about neighboring lateral areas, the door detection algorithm is able to indirectly obtain the depth information of the doorframe, which can be used to differentiate doors from other indoor objects with door-like shape and size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Geometric door model</head><p>Our geometric model of a door consists of four corners and four lines. Figure <ref type="figure" target="#fig_1">2a</ref> depicts the ideal image view with-out occlusion or perspective. Figure <ref type="figure" target="#fig_1">2b</ref> shows the deformed geometric shape caused by perspective. In both cases, the four door corners are all visible. Often, however, due to perspective and occlusion, only a part of a door is captured by a wearable camera, especially for visually impaired users who cannot aim the camera to "frame" the door. As shown in Fig. <ref type="figure" target="#fig_1">2c,</ref><ref type="figure">d</ref>, each occluded vertical line can form a corner with the horizontal axis of an image. In our algorithm, we assume the following about the image: (1) At least two door corners are visible. (2) Both vertical lines of a door frame are visible.</p><p>(3) Vertical lines of a door frame are nearly perpendicular to the horizontal axis of an image. (4) A door has at least a certain width and length (to be defined in the next section). These assumptions are easy to achieve in practice. The geometric door model is robust to variations in color, texture, occlusion and door status without any other requirements. For instance, the color of a door can be similar or different from that of an adjacent wall (provided the door frame is contrasting in color). The surface of a door can be highly textured or not. The status of a door may be closed, open, or partially open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Door-corner candidates</head><p>Edges and corners are insensitive to variations in color, viewpoints, scales, and illumination. Our model employs a combination of edges and corners to represent a door, which generalizes to doors in different environments. We first apply pre-processing (i.e. down-sampling, Gaussian smoothing) to eliminate noise for reducing corners detected from highly textured surrounding walls. Then the Canny edge detection is performed on the down-sampled and smoothed image to generate a binary edge map. Contours are extracted from the edge map by connecting the gaps between close endpoints of edges. Next, corners are extracted from contours using the method of He and Yung <ref type="bibr" target="#b15">[16]</ref>. In the geometric door model, each line terminated by the image border corresponds to an open contour. Therefore, four corners of the geometric door model (see Fig. <ref type="figure" target="#fig_1">2</ref>) can always be extracted even when only the upper (or lower) part of the door is captured.</p><p>Door-corner candidates can be grouped from the detected corners in the image based on the relationship of four corners of the doorframe in the geometric door model. As shown in Fig. <ref type="figure" target="#fig_1">2a</ref>, for any combination of four corners, the top left corner is denoted C 1 , other three corners are named as C 2 , C 3 , and C 4 in counterclockwise direction. The coordinate of corner C i is (x i , y i ). L 12 is the line connecting C 1 and C 2 , similarly for L 23 , L 34 , and L 41 . The ratio Siz i j between the length of L i j and the length of the diagonal DI of an image, and the direction of L i j corresponding to the horizontal axis of an image Dir i j can be used to obtain the door-corner candidacy from detected corners.</p><p>The combination of four corners will be selected as a doorcorner candidate if the following requirements are satisfied:</p><p>(1) A door in an image has a certain height and width. So, Siz 12 and Siz 34 should be within a range:  <ref type="bibr" target="#b4">(5)</ref> The ratio between height and width of a doorframe should be within a range:</p><formula xml:id="formula_0">Height T hreshL &lt; Si z 12 ,</formula><formula xml:id="formula_1">H W T hreshL &lt; (Si z 12 + Si z 34 ) / (Si z 23 + Si z 41 ) &lt; H W T hresh H</formula><p>Based on the camera configuration and door size, we set HeightThreshL = 0.5 and HeightThreshH = 0.9; WidthThresh L = 0.1 and WidthThreshH = 0.8; DirectionThreshL = 35 and DirectionThreshH = 80; ParallelThresh = 6; RatioThreshL = 2.0 and RatioThreshH = 3.0 in our system. The four-corner groups that satisfy the above rules are marked as door-corner candidates, which are subsequently tested by the following matching procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Door detection by matching edges and door-corner candidates</head><p>Each door-corner candidate represents an abstract geometric frame. To determine whether a door-corner candidate represents a real doorframe, we check if there are matching edges of the door frame between corners of each door-corner candidate based on our geometric door model. As shown in Fig. <ref type="figure" target="#fig_1">2a</ref>, C i and C j are two corners of a door-corner candidate. We first create a matching mask (i.e. the gray area) by expanding the line connecting C i and C j with a N × N window. The "fill-ratio" F R i j of corner C i and C j is defined as the ratio of the overlap between the detected edge pixels falling in the matching mask and the line connecting the two corners.</p><p>Combining the edge map and door-corner candidates, we can obtain a "fill-ratio" vector</p><formula xml:id="formula_2">[F R 12 , F R 23 , F R 34 , F R 41 ]</formula><p>for each door-corner candidate. If each element of a "fillratio" vector is larger than FRThreshL and the average value of four elements is larger than FRThreshH, then this doorcorner candidate corresponds to a real door in the image. If there is more than one door-corner candidate with matching edges of the same doorframe, they will be merged as one detected door.</p><p>In practice, if FRThreshL or FRThreshH is too large, the false-positive rate will be low, but the true-positive rate will decrease; if FRThreshL or FRThreshH is too small, the truepositive rate will be high, but the false-positive rate will increase. In order to reduce the false-positive rate and increase the detection rate simultaneously, we initialize FRThreshL and FRThreshH by two relatively low values. Then, the detection result is checked: if only one door-corner candidate is matched, it is the detection result; if more than one door-corner candidate is matched, then FRThreshL and FRThreshH with relatively high values are used to re-match the matched door-corner candidates. In our experiments, the increased thresholds are effective in eliminating spurious detection results. In our experiments, parameter values were </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Determining relative position of a detected door</head><p>For an indoor wayfinding device to assist blind or visually impaired persons, we not only need to recognize the presence of a door but also the door's position relative to the user. Figure <ref type="figure" target="#fig_3">11a</ref> depicts the scenario that a door is located on the left side with respect to the camera. Figure <ref type="figure" target="#fig_3">11b</ref> demonstrates a door located in front of the observer and Fig. <ref type="figure" target="#fig_3">11c</ref> demonstrates a door located on the right side. The angle between L 41 and the horizontal axis (see Fig. <ref type="figure" target="#fig_1">2a</ref>) can be used to determine the relative position of a door.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inset and protrude object detection</head><p>Depth information plays an important role in indoor object recognition, as well as in robotics, scene understanding, and 3-D reconstruction. In most cases, doors are recessed or inset into a wall, especially the doors of elevators. Other large objects with door-like shape and size, such as bookshelves and cabinets, protrude in relief from a wall. In order to distinguish inset (e.g. doors and elevators) from protruding (e.g. bookshelves, cabinets) objects, we propose a novel and straightforward algorithm by utilizing information from faces formed by an inset or protrusion to obtain the depth information with respect to the wall. Due to the different geometric characteristics and different relative positions, the different positions of lateral faces with respect to the detected object indicate door-like inset objects (assumed to be doors) or protruding objects (assumed to be other furnishings such as bookshelves, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. In Fig. <ref type="figure" target="#fig_2">3a</ref>, since L 12 &lt;L 34 , the elevator door is determined to be located on the right side with respect to the user. Furthermore, the elevator door, an inset object, presents its lateral (C 1 -C 2 -C 5 -C 6 ) on the left side of the doorframe (C 1 -C 2 -C 3 -C 4 ). In Fig. <ref type="figure" target="#fig_2">3c</ref>, since L 12 &lt;L 34 , the bookshelf is identified as located on the right side and as a protruding object, its lateral (C 4 -C 3 -C 5 -C 6 ) being on the right side of the frame (C 1 -C 2 -C 3 -C 4 ). Similar relations can be found in Fig. <ref type="figure" target="#fig_2">3b,</ref><ref type="figure">d</ref>. Therefore, combining the position of a frame and the position of a lateral, we can determine the inset or relief of a frame-like object, as shown in Table <ref type="table">1</ref>. Note that the position of a frame is relative to the user; the position of a lateral is relative to the frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Color decomposition</head><p>To extract text information from indoor signage with complex backgrounds, we have designed a robust algorithm to extract the coarse regions of interest (ROI) that are most likely to contain text information. We observe that each text string generally has a uniform color in most indoor environments. Thus, color decomposition is performed to obtain image regions with identical colors to different color layers. Each color layer contains only one foreground color with a clean background. To obtain fewer layers, we apply color quantization to reduce the number of colors in the original image, but keep the essential information for text extraction and segmentation. Inspired by Nikolaou et al. <ref type="bibr" target="#b28">[29]</ref>, first, we perform edge detection to create the edge map. Second, to avoid the drastic color variations around edge pixels, only non-edge pixels are sampled for the next step. Third, to group pixels with similar colors together, seeds are randomly selected from the sampled pixels to cluster the pixels with similar colors to the corresponding seed pixels. Fourth, considering the mean color value of each color cluster as an updated seed, a mean shift-based clustering algorithm <ref type="bibr" target="#b18">[19]</ref>, a nonparametric clustering technique which does not require prior knowledge of the number of clusters, is applied iteratively to group clusters with similar color together to produce larger clusters, each of which corresponds to a color layer. Each color layer is a binary image with one foreground color and white background color. Figure <ref type="figure" target="#fig_4">4</ref> shows an example of an exit door image with three color layers after color decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Text region extraction based on density analysis of binary transitions</head><p>After color decomposition, we obtain the color layer images that contain regions with similar colors. Now, we extract text regions based on the structure of text characters. Text characters that are composed of strokes and major arcs are well aligned along the direction of whatever text string they belong to. For each text string, well-structured characters result in a high frequency of regular intensity variations. Since each color layer is a binary image, the regular intensity variations between the text color and the background color are represented as binary transitions in the highly textured regions.</p><p>Generally, text strings on indoor signage have layouts with a dominant horizontal direction with respect to the image 123 frame. We apply a 1 × N sliding window to calculate the binary transitions around the center pixel of the window P.</p><p>If there are no less than two binary transitions occurring inside the sliding window, the corresponding central pixel will be assigned BS(P)=1, otherwise BS(P)=0, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>. Thus, a map of binary transitions for each color layer is created to accurately represent the regional intensity variation. In the binary transitions map, the clusters constituted by value 1 correspond to higher frequencies of binary transitions, which mean that they are most likely regions containing text information.</p><p>To extract text regions with high frequencies of binary transitions, an accumulation calculation is used to locate the rows and columns that belong to the regions in the BS map by Eqs. ( <ref type="formula">1</ref>) and <ref type="bibr" target="#b1">(2)</ref>. Let H (i) and V ( j) indicate the horizontal projection at row i and vertical projection at column j in the BS map, respectively. If H (i) is greater than a threshold, row i is part of a text region. Accumulation is then performed vertically in the regions constituted by extracted rows; if V ( j) is greater than a threshold, the column j located in extracted rows are also part of the text region.</p><formula xml:id="formula_3">H (i) = j B S [P (i, j)] , Row i ∈ TextRegionifH (i) &gt; T H (1) V ( j) = H (i)&gt;T H B S [P (i, j)] , Col j ∈ TextRegionifV ( j) &gt; T V (2)</formula><p>Therefore, the regions that are very likely to contain text information are extracted as ROIs. Note that in this stage, the extracted ROIs still contain some highly textured non-text regions. These non-text regions will further be filtered out based on structure of text characters (see details in Sect. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Text region extension</head><p>In the extracted ROIs, text characters that are larger than their neighbors (as shown in Fig. <ref type="figure" target="#fig_6">6</ref>) might be truncated, because the top parts have fewer binary transitions than lower text characters. In order to acquire regions with fully formed text characters, we check boundaries of each component in an extended text region. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, if the complete boundaries could be covered by a restricted extension of the region's size, the stretched region will be deemed as a new ROI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text localization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Primary localization of text characters</head><p>To localize text characters and filter out non-text regions from the coarse extracted ROIs, we use a text structure model to identify letters and numbers. This model is based on the fact that each of these characters is shaped by closed edge boundary, which contains no more than two holes (e.g. "A" contains one hole and "8" contains two holes).</p><p>As in the algorithm proposed by Kasar et al. <ref type="bibr" target="#b18">[19]</ref>, the edge map of the text regions is first computed. The edge bounding box is then obtained by performing connected component labeling. Several rules are defined to filter out obvious non- text bounding boxes, including the size, aspect ratio, and the number of nested bounding boxes. Figure <ref type="figure" target="#fig_7">7</ref> shows two examples of text character localization. The green box in Fig. <ref type="figure" target="#fig_7">7d</ref> is a non-text region which will be filtered out by aspect ratio of the bounding box, and red boxes represent the bounding box of text characters for further topological analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Text localization refinement by topological analysis</head><p>To further filter out non-text bounding boxes, topological features from text strings are taken into account. We assume that a text string contains at least three three characters, as shown in Fig. <ref type="figure" target="#fig_8">8</ref>. Based on the observation that the distances and areas between neighboring characters are highly consistent in text strings, we first measure the distance variation among three neighboring bounding boxes in accordance with their central points, as shown in Fig. <ref type="figure" target="#fig_8">8a</ref>. In addition to filtering out the non-text bounding boxes with inconsistent inter-letter or inter-word distances, we can also categorize independent text strings. Moreover, the area variation is calculated among the three neighboring bounding boxes by the foreground area ratio of each pair of bounding boxes, as shown in Fig. <ref type="figure" target="#fig_8">8b</ref>. If one of them is much larger or smaller than the two neighbors that have similar foreground areas, it will be filtered out. The topological analysis results in purified text strings on cleaner backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Text recognition</head><p>To recognize the extracted text information, we employ offthe-shelf OCR software to translate the text images into character codes. Currently available commercial OCR is designed for scanned documents with large regions of text characters </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">System and interface design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Indoor wayfinding system design</head><p>The computer vision-based indoor wayfinding aid for blind persons integrates a camera, a microphone, a portable computer, and a speaker connected by Bluetooth for audio description of objects identified. A mini-camera mounted on sunglasses is used to capture video of the environment for computer vision-based object class recognition. The presence of environmental objects (class, location, etc.) is described to the blind user by verbal display with minimal distraction to the user's hearing sense. The user can control the system by speech input via microphone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interface design:</head><p>In order to interact with the blind user, the aid provides function selection and system control through speech commands input from a microphone. As shown in Fig. <ref type="figure" target="#fig_9">9</ref>, the basic interface design includes "Basic Functions" and 'High Priority Commands", which will be refined based on users' evaluation and feedback. Basic functions: The blind user will speak out the destination or target object for which he/she is looking as the input of the indoor wayfinding aid (e.g. "find exit door"). The object detection results will be presented to the blind user as speech outputs including "Target found", "Target not found", "Stop function", "Start a new function", etc. For the "Target found" function, the next level of functions includes "Target location", to announce the location and orientation of the target, "Repeat", to repeat the detected result, and "Save result", to save the object image with the associated context information in the computer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High priority system configuration:</head><p>The system configuration can be set by a number of high priority speech commands such as "System Restart", "Turn off system", "Abort Current Task", speaker volume and speed control commands (e.g. "Louder", "Quieter", "Slower", "Faster"), and "Help". The commands with high priority can be used at any time. The user may verbally request "Help", and the indoor wayfinding system will respond with which options are available within the current function. To protect privacy and minimize masking environmental sounds, bone conduction earphones or small wireless Bluetooth speakers can be used. The system will check battery level and send out an audio warning when the battery level is low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Audio output</head><p>For the audio display, we use operating system speech facilities, which are standard in modern portable computer systems (and smartphones). Currently, we use the Microsoft Speech software development kit (SDK), which conveniently supports imported script files. Many configuration options are available including speech rate, and volume and voice gender according to user preference. More studies are needed (and planned) on how best to describe complex indoor environments in compact yet effective verbal terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">System prototype implementation</head><p>As shown in Fig. <ref type="figure" target="#fig_12">15</ref>, we have developed a prototype system that implements portions of the design described in this paper. Our research on the interface study is ongoing. The hardware of the prototype system includes a Logitech web camera with auto focus which is attached on a pair of sunglasses to capture images. The camera is connected to an HP mini laptop by a USB connection. The HP mini laptop processes the computation. In order to avoid serious blocking or distracting the hearing of blind people, a sense that they rely upon heavily, we choose a wireless BlueTooth earpiece for presenting detection results as speech outputs to the blind travelers. For speech input and output, we employ Microsoft Speech SDK in our system. For text recognition, we evaluate two OCR engines, Tesseract and Nuance OmniPage. Om-niPage demonstrates better performance in most cases, but it is a commercial software without open source codes and expensive. Tesseract is an open-source OCR engine that can be more conveniently integrated into systems. We integrate Tesseract in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental results and discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Database for indoor object detection</head><p>We constructed a database containing 221 images collected from a wide variety of environments by static cameras to test the performance of the proposed door detection and text recognition algorithms. The database includes both door and non-door images. Door images include doors and elevators with different colors and texture, and doors captured from different viewpoints, illumination conditions, and occlusions, as well as open and glass doors. Non-door images include door-like objects, such as bookshelves and cabinets.</p><p>To evaluate the proposed methods, we first categorized the database into three groups: Simple (57 images), Medium (113 images), and Complex (51 images), based on the complexity of backgrounds, intensity of deformation, and occlusion, as well as changes of illumination and scale. To test the accuracy of door position detection, we further regroup the door images in the database based on the views as: Left, Frontal, and Right. There are a total of 209 door images with 36 of Left, 141 of Frontal, and 32 of Right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Door detection</head><p>We evaluated the proposed algorithm with and without performing the function of differentiating doors from door-like protruding objects. For images with resolution of 320 × 240, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Door position detection</head><p>To determine the relative position of a detected door, we classify the relative position as Left, Front, and Right (see examples in Fig. <ref type="figure" target="#fig_3">11</ref>). The relative position detection rate for Left, Front, and Right are 97.2, 95.0, and 93.8 %, respectively.</p><p>So, the method proposed in this paper appears to be highly reliable for inferring the relative position of a detected door with respect to a user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Door signage recognition</head><p>Text region extraction is very important for recognizing indoor signage and is of course necessary before OCR can be performed for text recognition. Some example results are demonstrated in Fig. <ref type="figure" target="#fig_3">12</ref>. The first row of Fig. <ref type="figure" target="#fig_3">12</ref> shows the detected door and signage regions. The second row displays the binarized signage. The last row of Fig. <ref type="figure" target="#fig_3">12</ref> displays that recognized text from OCR as readable codes on the extracted and binarized text regions. Of course, if we had input the unprocessed images of the indoor environment directly into the OCR engine, only messy codes, if any, would have been produced.</p><p>We quantitatively evaluated the proposed text localization method against the Robust Reading Dataset <ref type="bibr" target="#b44">[45]</ref> from International Conference on Document Analysis and Recognition (ICDAR) 2003. In our testing, we selected 420 images which are compatible with the assumption that a text string contains at least three characters with relatively uniform color. To evaluate the performance, we calculate the precision, which is the ratio of area of the successfully extracted text regions to area of the whole detected regions. Our method achieves the state of the art with an average precision of 71 %. Some examples of text localization results are demonstrated in Fig. <ref type="figure" target="#fig_3">13</ref>. The detected regions of text strings are marked by the blue mask. In our application, the proposed algorithm of As shown in Table <ref type="table" target="#tab_1">2</ref>, the true-positive rate decreases and the false-positive rate increases from "Simple" to "Complex" images, since the complexity of background increases a lot. The false-positive rate of "Medium" is a little larger than that of "Complex." We believe this is due to disparity in the number of images in each category. Failure of detecting door corners appears to be the main reason for missing doors (see examples in Fig. <ref type="figure" target="#fig_4">14a,</ref><ref type="figure">b</ref>.)</p><p>The protrusion detection appears to be effective in lowering the false-positive rate and maintaining the true-positive rates of "Simple" and "Medium". For "Complex" images, both the rates of true positive and false positive are decreased.</p><p>In images with highly complex backgrounds, some background corners adjacent to a door are detected as spurious lateral faces indicating an inset or protrusion. Such doors may be detected as protrusion objects and incorrectly eliminated. However, considering the safety of blind users, the lower false-positive rate is more desirable.</p><p>Our text extraction and localization algorithms remain subject to interference from background interference from features of color or size similar to text characters, because these pixel-based algorithms cannot filter out the information that satisfies the predefined features. Consequently, some text characters shown against complex backgrounds will inevitably lead to recognition failures or false results. For signage with fewer than three text characters in a text string, our system will fail. For example, the floor number "2" in Fig. <ref type="figure" target="#fig_4">14b</ref> and the bathroom signage in Fig. <ref type="figure" target="#fig_4">14c</ref> are missed.</p><p>In pilot interface studies, we have conducted a survey and collected a testing data set by ten blind subjects using a wearable camera on sunglasses (see Fig. <ref type="figure" target="#fig_12">15</ref>). We obtain the following observations: (1) motion blur and very large occlusions happen when subjects have sudden head movements, which our method currently cannot handle; and (2) with a cane or a guiding dog, most of the blind users can find doors without any problem. We will focus our future research on improving our text localization and recognition, signage detection and recognition, and user interface study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and future work</head><p>We have described some algorithms and techniques that would be important in making a prototype of computer vision-based indoor wayfinding aid to help blind persons access unfamiliar environments. Our algorithms include detection of doors and wall protrusions, and recognition text signage to differentiate doors with different functions (e.g. office from bathroom). Our novel and robust door detection algorithm is able to detect doors and discriminate other objects with rectangular shape, such as bookshelves and cabinets. Since the algorithm is based only on the general features (e.g. edges and corners) and geometric relationships, it is able to detect objects in different environments with varied colors, texture, occlusions, illumination, and viewpoints. The text signage recognition is incorporated with the detected door and further represented to blind persons in audio dis- Our future work will focus on handling large occlusions by using more discriminative features like door knobs and other hardware, detecting and recognizing more types of indoor objects and icons on signage, in addition to text for indoor wayfinding aid, to assist blind people travel independently. We will also study the significant human interface issues including auditory output and spatial updating of object location, orientation, and distance. With real-time updates, users will be able to better use spatial memory to comprehend the surrounding environment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Typical indoor objects (top row) and their associated contextual information (bottom row). a A bathroom, b an exit, c a laboratory, and d an elevator</figDesc><graphic coords="3,184.24,56.87,360.04,175.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Geometric door model: a ideal 'canonical' view, b with perspective effects. c-d with occlusion and perspective effects</figDesc><graphic coords="4,186.25,56.99,357.64,268.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Inset and protruding models. a Inset object: door frame located on user's right side and lateral face located on the doorframe's left side. b Protruding object: frame located on the user's left side and lateral face located on the frame's right side. c Protruding object: frame located on the user's right side and lateral face located on the frame's right side. d Protruding object: frame located on the left side; lateral face located on the left side</figDesc><graphic coords="6,184.24,56.51,359.08,268.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 1</head><label>1</label><figDesc>Protrusion and inset determination combining the position of a frame and the position of a lateral face</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Three color layers are extracted from an exit door. a Original image. b-d Different color layers extracted by color decomposition</figDesc><graphic coords="7,184.24,56.09,360.04,321.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 binary transition occurs within the sliding window in blue, while binary transitions are detected within the one in orange (color figure online)</figDesc><graphic coords="8,206.26,56.39,337.48,142.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Region extension by recovering the truncated boundary denoted in red (color figure online)</figDesc><graphic coords="8,308.68,219.56,232.84,184.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 a</head><label>7</label><figDesc>Fig. 7 a, b Original images. c, d Bounding boxes at the extracted regions. The green boxes are the non-text regions which will be filtered out, leaving only red boxes for topological analysis (color figure online)</figDesc><graphic coords="9,54.07,56.24,232.36,183.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 a</head><label>8</label><figDesc>Fig. 8 a |E F| and |M N| demonstrate inconsistency compared to the neighboring distances, so two text strings are separated and the arrow is filtered out. b Area of M demonstrates inconsistency, because it is much larger than its neighbors</figDesc><graphic coords="9,308.68,56.33,232.84,200.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9</head><label>9</label><figDesc>Fig.9interface design for the proposed computer vision-based indoor wayfinding system by using speech commands. The high priority commands can be used at any time to overwrite the basic functions</figDesc><graphic coords="10,53.56,56.93,232.84,143.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 Fig. 12</head><label>1012</label><figDesc>Fig. 10 Examples of successfully detected doors in different environments. The first row shows "Simple" examples with clear background, but includes illumination variations and occlusions. The second row</figDesc><graphic coords="11,84.64,450.95,425.32,222.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 Fig. 14</head><label>1314</label><figDesc>Fig. 13 Examples text string detection on the Robust Reading Dataset. The detected regions of text strings are marked in blue (color figure online)</figDesc><graphic coords="13,184.24,56.81,360.04,203.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref> Example of a prototype system includes a Logitech web camera with auto focus on sunglasses</figDesc><graphic coords="13,308.68,439.28,232.36,106.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,184.24,171.95,360.04,221.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Door detection results for groups of "Simple", "Medium", and "Complex"</figDesc><table><row><cell>Data category</cell><cell>True positive</cell><cell>False positive</cell><cell>Protrusion</cell></row><row><cell></cell><cell>rate (%)</cell><cell>rate (%)</cell><cell>detection (%)</cell></row><row><cell>Simple</cell><cell>98.2</cell><cell>0</cell><cell>Yes</cell></row><row><cell></cell><cell>98.2</cell><cell>1.8</cell><cell>No</cell></row><row><cell>Medium</cell><cell>90.5</cell><cell>3.5</cell><cell>Yes</cell></row><row><cell></cell><cell>91.4</cell><cell>6.2</cell><cell>No</cell></row><row><cell>Complex</cell><cell>80.0</cell><cell>2.0</cell><cell>Yes</cell></row><row><cell></cell><cell>87.8</cell><cell>5.9</cell><cell>No</cell></row><row><cell>Total</cell><cell>89.5</cell><cell>2.3</cell><cell>Yes</cell></row><row><cell></cell><cell>92.3</cell><cell>5.0</cell><cell>No</cell></row><row><cell cols="4">the proposed algorithm achieved 92.3 % true-positive rate</cell></row><row><cell cols="4">with a false-positive rate of 5.0 % without protruding object</cell></row><row><cell cols="4">detection. With protruding object detection, the algorithm</cell></row><row><cell cols="4">achieves 89.5 % true-positive rate with a false-positive rate</cell></row><row><cell cols="4">of 2.3 %. Table 2 displays the details of detection results for</cell></row><row><cell cols="4">each category with protrusion detection and without protru-</cell></row><row><cell cols="4">sion detection. Some examples of door detection are illus-</cell></row><row><cell cols="2">trated in Fig. 10.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was partially supported by NIH grant 1R21EY020990, NSF grants IIS-0957016, EFRI-1137172, and ARO grant W911NF-09-1-0565. The authors would like to thank the anonymous reviewers for their constructive comments and insightful suggestions that improved the quality of this manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Biographies</head><note type="other">YingLi</note></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Signage and wayfinding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arditi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brabyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Lighthouse Handbook on Visual Impairment and Vision Rehabilitation</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Silverstone</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Rosenthal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Faye</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting and modeling doors with mobile robots</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on robotics and automation</title>
		<meeting>the IEEE international conference on robotics and automation</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind Man is Found Dead in Elevator Shaft</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>City Room</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition-by-components: a theory of human image understanding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Blind Sight: A camera for visually impaired people</title>
		<ptr target="http://accessability.blogspot.com/2008/10/blind-sight-camera-for-visually.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analy. Mach. Intell. PAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual detection of lintel-occluded doors from a single image</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society workshop on visual localization for mobile platforms</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Door detection via signage context-based hierarchical compositional model. 2nd workshop on use of context in video processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>UCVP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wearable obstacle avoidance electronic travel aids for blind: a survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Bourbakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE IEEE Trans. Syst. Man Cybern. Part C Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient method for text detection in video based on stroke width similarity</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision (ACCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Edge based text detection for multi-purpose application</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Signal Process</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wearable mobility aid for low vision using scene classification in a Markov random field model framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum. Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blind navigation and the role of technology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Giudice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Legge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The engineering handbook of smart technology for aging, disability, and independence</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Helal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Mokhtari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Abdulrazak</surname></persName>
		</editor>
		<meeting><address><addrLine>Hoboken</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Corner detector based on global and local curvature properties</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time door detection based on adaboost learning algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hensler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bittel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on research and education in robotics</title>
		<imprint>
			<publisher>Eurobot</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Crosswatch: a camera phone system for orienting visually impaired pedestrians at traffic intersections. 11th international conference on computers helping people with special needs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>ICCHP &apos;08</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Font and background color independent text binarization. Second international workshop on camera-based document analysis and recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kasar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A method for recognition and localization of generic objects for indoor navigation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ARPA image understanding workshop</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Biological object recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<ptr target="http://www.scholarpedia.org/article/Biological_object_recognition" />
	</analytic>
	<monogr>
		<title level="j">Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2667</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Text detection in images based on unsupervised classification of edge-based features. International conference on document analysis and recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text segmentation based on stroke filter</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on multimedia</title>
		<meeting>international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural object detection in outdoor scenes based on probabilistic spatial context models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on multimedia and expo</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Search strategies of visually impaired persons using a camera phone wayfinding system. 11th international conference on computers helping people with special needs (ICCHP &apos;08</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanchenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Door-detection using computer vision and fuzzy logic</title>
		<author>
			<persName><forename type="first">R</forename><surname>Munoz-Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garcia-Silvente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th WSEAS international conference on mathematical methods and computational techniques in electrical engineering</title>
		<meeting>the 6th WSEAS international conference on mathematical methods and computational techniques in electrical engineering</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual door detection integrating appearance and shape cues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Murillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sagues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. Auton. Syst</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m">Electronic travel aids: new directions for research. Working group on mobility aids for the visually impaired and blind</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>National Academy Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page">107</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Research Council</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Color reduction for complex document images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nikolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papamarkos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Imaging Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cognit. Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Context based object detection from video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Paletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Greindl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on computer vision systems</title>
		<meeting>international conference on computer vision systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Piecewise planar modeling for step detection using stereo vision</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weiland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Grouping using factor graphs: an approach for finding text with a camera phone. Workshop on graph-based representations in pattern recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coughlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An efficient edge based technique for text detection in video frames</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The eighth IAPR workshop on document analysis systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Realtime door detection in cluttered environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stoeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mauff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th IEEE international symposium on intelligent control</title>
		<meeting>the 15th IEEE international symposium on intelligent control</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving computer vision-based indoor wayfinding for blind persons with context information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arditi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th international conference on computers helping people with special needs (ICCHP)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Computer vision-based door detection for accessibility of unfamiliar environments to blind persons</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arditi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th international conference on computers helping people with special needs (ICCHP)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contextual priming for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="191" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A novel approach for text detection in images using structural features. The 3rd international conference on advances in pattern recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boucher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Seeing with sound-the vOICe</title>
		<ptr target="http://www.seeingwithsound.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Text localization in spam image using edge features</title>
		<author>
			<persName><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on communications, circuits and system</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A new robust algorithm for video text extraction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Robust door detection in unfamiliar environments by combining edge and corner features. 3rd workshop on computer vision applications for the visually impaired (CVAVI</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A video based interface to textual information for the visually impaired</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zandifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chahine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE 4th international conference on multimodal interfaces</title>
		<meeting>IEEE 4th international conference on multimodal interfaces</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<ptr target="http://algoval.essex.ac.uk/icdar/Datasets.html" />
		<title level="m">Robust Reading Dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
