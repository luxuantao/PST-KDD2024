<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MICRONETS: NEURAL NETWORK ARCHITECTURES FOR DEPLOYING TINYML APPLICATIONS ON COMMODITY MICROCONTROLLERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-21">21 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Colby</forename><surname>Banbury</surname></persName>
							<email>&lt;cbanbury@g.harvard.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Arm ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuteng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arm ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Igor</forename><surname>Fedorov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arm ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ramon</forename><forename type="middle">Matas</forename><surname>Navarro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arm ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Urmish</forename><surname>Thakkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arm ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dibakar</forename><surname>Gope</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arm ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arm ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arm ML Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MICRONETS: NEURAL NETWORK ARCHITECTURES FOR DEPLOYING TINYML APPLICATIONS ON COMMODITY MICROCONTROLLERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-21">21 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.11267v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Executing machine learning workloads locally on resource constrained microcontrollers (MCUs) promises to drastically expand the application space of IoT. However, so-called TinyML presents severe technical challenges, as deep neural network inference demands a large compute and memory budget. To address this challenge, neural architecture search (NAS) promises to help design accurate ML models that meet the tight MCU memory, latency and energy constraints. A key component of NAS algorithms is their latency/energy model, i.e., the mapping from a given neural network architecture to its inference latency/energy on an MCU. In this paper, we observe an intriguing property of NAS search spaces for MCU model design: on average, model latency varies linearly with model operation (op) count under a uniform prior over models in the search space. Exploiting this insight, we employ differentiable NAS (DNAS) to search for models with low memory usage and low op count, where op count is treated as a viable proxy to latency. Experimental results validate our methodology, yielding our MicroNet models, which we deploy on MCUs using Tensorflow Lite Micro, a standard open-source NN inference runtime widely used in the TinyML community. MicroNets demonstrate state-of-the-art results for all three TinyMLperf industry-standard benchmark tasks: visual wake words, audio keyword spotting, and anomaly detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine learning (ML) methods play an increasingly central role in a myriad of internet-of-things (IoT) applications. Using ML, we can interpret the wealth of sensor data that IoT devices generate. Prototypical uses in IoT include tasks such as monitoring environmental conditions such as temperature and atmosphere (e.g., carbon monoxide levels), monitoring mechanical vibrations from machinery to predict failure, or visual tasks such as detecting people or animals. User interfaces based on speech recognition and synthesis are also very common, as many IoT devices have limited user input features and small displays. In mobile applications, ML inference is often off-loaded to the cloud, where compute resources are more abundant. However, offloading introduces overheads in terms of latency, energy and privacy. It also requires access to communications, such as WiFi or cellular access. For the proliferating class of IoT devices, offloading can be prohibitively expensive, in terms of both the radio chips which increase the bill of materials, as well as the network access costs.</p><p>TinyML is an alternative paradigm, where we execute ML tasks locally on IoT devices. This allows for real time analysis and interpretation of data at the point of collection, which translates to huge advantages in terms of cost and privacy. Microcontroller units (MCUs) are the ideal hardware platform for TinyML, as they are typically small (?1cm 3 ), cheap (?$1) and low-power (?1mW) compared to mobile and cloud platforms (Table <ref type="table" target="#tab_0">1</ref>). MCUs typically integrate a CPU, digital and analog peripherals, on-chip embedded flash (eFlash) memory for program storage and Static Random-Access Memory (SRAM) for intermediate data. However, deploying deep neural networks on MCUs is extremely challenging; the most severe limitation being the small and flat memory system (Figure <ref type="figure" target="#fig_0">1</ref>) within which the model weights and activations must be stored. Therefore, to achieve the promise of TinyML, we must aggressively optimize models to best exploit the limited resources provided by an MCU hardware and software stack. Mounting interest in TinyML has led to some maturity in both software stacks and benchmarks. The open-source TensorFlow Lite for Microcontrollers (TFLM) inference runtime allows for straightforward and portable deployment of NN workloads. TFLM uses an interpreter to execute an NN graph, which means the same model graph can be deployed across different hardware platforms. When compared to code generation based methods (uTensor), TFLM provides portability across MCU vendors, at the cost of a fairly minimal memory overhead. Recently, the ML performance (MLPerf) benchmarking organization has outlined a suite of benchmarks for TinyML called TinyMLPerf <ref type="bibr" target="#b2">(Banbury et al., 2020)</ref>, which consists of three TinyML tasks of visual wake words (VWW), audio keyword spotting (KWS), and anomaly detection (AD). Standardizing TinyML research results around a common open-source runtime and benchmark suite makes comparing reasearch results easier and fairer, hopefully driving research progress.</p><p>Previous work on TinyML has largely considered model design without consideration for the real deployment scenario (e.g. SpArSe <ref type="bibr" target="#b15">(Fedorov et al., 2019)</ref>), or has used closed-source software stacks which make deployment and comparison impossible (e.g. MCUNet <ref type="bibr" target="#b32">(Lin et al., 2020)</ref>). In this paper, we describe MicroNets, a family of models which can be deployed with publicly available TFLM, for the three TinyMLperf tasks of VWW, KWS and AD. In contrast to previous TinyML work that uses black-box optimizations, such as Bayesian optimization <ref type="bibr" target="#b15">(Fedorov et al., 2019)</ref>, and evolutionary search <ref type="bibr" target="#b32">(Lin et al., 2020)</ref>, MicroNets are optimized for MCU inference performance using differentiable neural architecture search (DNAS).</p><p>The contributions of this work are summarized below.</p><p>? Using an extensive characterization of NN inference performance on three representative MCUs, we demonstrate that the number of operations is a viable proxy for inference latency and energy.</p><p>? We show that differentiable neural architecture search (DNAS) with appropriate constraints can be used to automatically construct models that fit the MCU resources, while maximizing performance and accuracy.</p><p>? We provide state of the art models for all three TinyML tasks, deployable on standard MCUs using TFLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Since its inception, deep learning has been synonymous with expensive, power-hungry GPUs <ref type="bibr" target="#b27">(Krizhevsky et al., 2012)</ref>. However, the current interest in deploying deep learning models on MCUs is reflected in a small number of papers that have begun to explore this promising space. In this section, we briefly survey the literature related to TinyML, divided between hardware, software and machine learning.</p><p>Hardware The current interest in ML has led to a growing demand for arithmetic compute performance in MCU platforms, which was previously driven by digital signal processing workloads. Single-instruction multiple-data (SIMD) extensions <ref type="bibr">(Hennessy &amp; Patterson, 2011)</ref> are one of the most effective approaches to achieving this in the CPU context, but increase silicon area and power consumption. The Arm Helium extensions (Helium) address this using a lightweight SIMD implementation targeted to MCUs. Beyond CPUs, various accelerators <ref type="bibr" target="#b54">(Whatmough et al., 2019)</ref> and co-processors such as digital signal processors (DSPs) (?) and micro neural processing units (microN-PUs) (Ethos-U55) typically offer greater performance and energy efficiency, at the cost of a more complex and less portable programming model. Finally, subthreshold circuit operation is another notable technology to increase energy efficiency, that has been demonstrated in commercial MCUs <ref type="bibr">(Ambiq)</ref>. In this work we specifically target commodity MCUs (  Compared to code generation based methods, TFLM is more portable but has some overheads. We use TFLM due to its portability, ease of deployment and open-source nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Learning</head><p>The challenges with implementing CNNs on MCUs were discussed in Bonsai <ref type="bibr" target="#b28">(Kumar et al., 2017)</ref>, namely that the feature maps of typical NNs require prohibitively large SRAM buffers. As a more storage efficient alternative to CNNs, pruned decision trees were proposed to suit the smallest MCUs with as little as 2KB of SRAM. <ref type="bibr" target="#b20">Gupta et al. (2017)</ref> propose a variant of k-nearest neighbors tailored for MCUs. <ref type="bibr" target="#b21">Gural &amp; Murmann (2019)</ref> propose a novel convolution kernel, reducing activation memory and enabling inference on low-end MCUs. SpArSe <ref type="bibr" target="#b15">(Fedorov et al., 2019)</ref> demonstrated that by optimizing the model architecture, CNNs in fact can be deployed on MCUs with SRAM down to 2KB. This was achieved using NAS, which has emerged as a vibrant area of research, whereby ML algorithms construct application specific NNs to meet very specific constraints <ref type="bibr" target="#b12">(Elsken et al., 2019)</ref>. SpArSe employs a Bayesian optimization framework that jointly selects model architecture and optimizations such as pruning. Similarly, MCUNet <ref type="bibr" target="#b32">(Lin et al., 2020)</ref> uses evolutionary search to design NNs for larger MCUs (2MB eFlash / 512KB SRAM) and larger datasets including visual wakewords (VWW) <ref type="bibr" target="#b9">(Chowdhery et al., 2019)</ref> and keyword spotting (KWS) <ref type="bibr" target="#b53">(Warden, 2018)</ref>). Reinforcement learning (RL) has also been used to choose quantization options in order to help fit an ImageNet model onto a larger MCU (2MB eFlash) <ref type="bibr">(Rusci et al., 2020b)</ref>. As well as images, audio tasks are an important driver for TinyML. TinyLSTMs <ref type="bibr" target="#b16">(Fedorov et al., 2020)</ref> shows that LSTMs for speech enhancement in smart hearing aids are similarly amenable to deployment on MCUs, after targeted optimization.</p><p>In this paper, we use differentiable NAS (DNAS) <ref type="bibr" target="#b33">(Liu et al., 2019)</ref> to design specialized MCU models to target the three TinyMLperf tasks. Unlike black-box optimization methods that have previously been applied to TinyML problems, like Bayesian optimization <ref type="bibr" target="#b15">(Fedorov et al., 2019)</ref> and evolutionary search <ref type="bibr" target="#b32">(Lin et al., 2020)</ref>, DNAS uses gradient descent and lends itself to straightforward implementation in modern auto-differentiation software like Tensorflow with acceleration on GPUs. Our work provides experimental evidence that DNAS is capable of satisfying MCU-specific model constraints, including eFlash, SRAM, and latency. In contrast to <ref type="bibr" target="#b32">(Lin et al., 2020)</ref>, our work uses a standard deployment framework (TFLM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HARDWARE CHARACTERIZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview</head><p>In this section we characterize the performance of NN inference workloads on MCUs. The MCUs we use (Table <ref type="table" target="#tab_0">1</ref>) are fairly self-contained, consisting of an Arm Cortex-M processor, SRAM for working memory, embedded flash for non-volatile program storage, and a variety of digital and analog peripherals. Unlike their mobile, desktop and datacenter counterparts, MCUs have a rather flat memory system, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Mobile and cloud computer systems universally employ a large off-chip main memory (usually DRAM). However, MCUs are typically equipped with only on-chip memory, which is relatively small to keep the die size reasonable. Figure <ref type="figure">2</ref> gives an example memory map showing how a KWS model is mapped onto the STM32F746ZG devices by TFLM. Activation buffers are allocated in the SRAM, while the model weights and biases and graph definition are allocated in the eFlash memory.</p><p>Alternatively, weights can be stored in SRAM, but we found experimentally that this results in only about a 1% speedup in end-to-end latency, while significantly reducing the space available for activations, which cannot be stored in eFlash.</p><p>In terms of throughput, this flat memory system coupled with the lower clock frequencies and simple (cheap) microarchitectures used in MCUs results in a predominately compute-bound system. The Cortex-M7 can dual issue load and ALU instructions, which the Cortex-M4 cannot. This gives higher IPC, which, combined with a 20% higher clock rate, makes the STM32F646ZG and the STM32F767ZI approximately twice as fast as the STM32F446RE.</p><p>Note that the runtime overhead for the TFLM interpreter is fairly minimal, requiring just 4KB of SRAM and 37 KB of eFlash. The 34KB SRAM block labeled as persistent buffers in Figure <ref type="figure">2</ref> scales with the size of the model and contains buffered quantization parameters and the c structs that hold pointers to the intermediate tensors and to the operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer Latency</head><p>In this section, we examine the hardware performance of typical NN layers. To do this, we generate a large number of layer types 1 and sizes and characterize them on the hardware.</p><p>Figure <ref type="figure" target="#fig_1">3</ref> shows the measured latency of each layer in TFLM and CMSIS-NN kernels, as a function of the number of operations 2 (ops). We observe that different layer types and sizes result in some spread in throughput, which was previously observed by <ref type="bibr">Lai et al. (2018b)</ref>. 2D convolutions and fully connected layers exhibit lower latency per op than depth-wise convolutions. This is likely due to depthwise convolutions having less operations relative to their IM2COL overhead. We also note some variability in ops/s between 2D convolution layers. This is primarily caused by the sensitivity of the CMSIS-NN kernel to input and output channel sizes. The CMSIS-NN CONV 2D kernel is substantially faster when the number of input and output channels are divisible by four. As an example, we observe that increasing the input/output channels of a convolution 1 Excluding RNN layers, not currently supported in TFLM.</p><p>2 A single multiply-accumulate is defined as two operations. layer from 138/138 to 140/140 decreases the latency from 37.5ms to 21.5ms (57% speedup).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Latency</head><p>Next, we characterize whole end-to-end models. To do this, we setup a parameterized supernet backbone that we randomly sample. This allows us to automatically generate a large number of random models with different layer types and dimensions, which we then characterize on the hardware in terms of latency and, in the next subsection, energy. Interestingly, the measured latency for the end-to-end models is linear with op count (0.95 &lt; r 2 &lt; 0.99). This is perhaps surprising given the variation seen with the layer-wise latency measurements (Figure <ref type="figure" target="#fig_1">3</ref>). Also, we observe that models sampled from the two different backbones results in a different slope. The explanation for this is that although single layers exhibit variation in latency as a function of ops, in a whole model this is averaged across many layers. Since a given search space will typically be dominated by a particular layer type, in terms of ops, the result is that we see a linear latency for models sampled from the same backbone.</p><p>The KWS backbone has ?40% higher throughput (Mops/S) than the CIFAR10 backbone, which is due to the mix in layer types and sizes. Finally, STM32F746ZG is around twice as fast as STM32F446RE (Section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Energy</head><p>Energy consumption is obviously a critical metric for TinyML. Following the same random model sampling methodology used to characterize latency, we measured the current consumption of 400 models from the CIFAR10 backbone. We use the Qoitech Otii Arc (Otii) to power the MCU boards and measure the current draw with the inference workload looping. Figure <ref type="figure" target="#fig_4">5</ref> shows the average power consumption versus the op count of each model on two MCUs. Clearly, there is little variance in power consumption between models (?/? = 0.00731), i.e. power is essentially independent of model size or architecture. Additionally, Figure <ref type="figure" target="#fig_4">5</ref> shows the energy consumption versus the op count of each model. We observe that executing the same model on a smaller MCU reduces the total energy consumption despite an increase in latency. This decreased energy consumption motivates the design of models that can fit within the tighter constraints of smaller devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary</head><p>Below is a summary of the findings of this section:</p><p>? Although ops is not a good predictor for the latency of a single layer, it is a viable proxy for the latency of an entire model sampled from a given backbone.</p><p>? For a given MCU, power is largely independent of model size and design. Therefore, energy per inference is a function of the size of the MCU, which determines power, and the number of ops, which dictates latency.</p><p>Therefore, when designing a model from within a backbone for a given task, ops is a viable proxy for both latency and energy, as measured on the target hardware and software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TINYMLPERF BENCHMARK TASKS</head><p>This section describes the TinyMLPerf benchmark tasks: Visual Wake Words (VWW), Keyword Spotting (KWS), and Anomaly Detection (AD). These were selected by a committee from industry and academia, to represent common TinyML application domains <ref type="bibr" target="#b2">(Banbury et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Wake Words</head><p>The VWW dataset used in TinyMLperf is a visual classification task, where each image is labels as 1 when a person occupies at least 0.5% of the frame and 0 when no person is present <ref type="bibr" target="#b9">(Chowdhery et al., 2019)</ref>. The dataset contains 82,783 train and 40,504 test images, which we resize to a common resolution of 224 ? 224. We use the standard ImageNet data prepropecessing pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Audio Keyword Spotting</head><p>Audio KWS (a.k.a. wake words), finds application in a plethora of use cases in commercial IoT products (e.g. Google Assistant, Amazon Alexa, etc.). Recent research has explored various model architectures suitable for resource constrained devices <ref type="bibr" target="#b6">(Chen et al., 2019;</ref><ref type="bibr" target="#b55">Wong et al., 2020;</ref><ref type="bibr" target="#b29">Kusupati et al., 2018;</ref><ref type="bibr" target="#b51">Thakker et al., 2019)</ref>. Among these, CNNs achieve good accuracy <ref type="bibr" target="#b8">(Choi et al., 2019;</ref><ref type="bibr" target="#b1">Anderson et al., 2020;</ref><ref type="bibr">Zhang et al., 2017c;</ref><ref type="bibr" target="#b18">Gope et al., 2019)</ref> and have the advantage of being deployable on commodity hardware using existing software stacks. The KWS dataset in TinyMLperf is Google Speech Commands (V2) <ref type="bibr" target="#b53">(Warden, 2018)</ref>. A model trained on this dataset is required to classify an 1-second long incoming audio clip from a vocabulary of 35 words into one of the 12 classes-10 keyword classes along with "silence" (i.e. no word spoken), and an "unknown" class, which is the remaining 25 keywords from the dataset. The raw time-domain speech signal is converted to 2-D MFCC (Mel-frequency cepstral coefficients). 40 MFCC features are then obtained from a speech frame of length 40ms with a stride of 20ms, yielding an input dimension of 49 ? 10 ? 1 features for 1 second of audio. Training samples are augmented by applying background noise and random timing jitter to provide robustness against noise and alignment errors. We follow the input data processing procedure described in <ref type="bibr">(Zhang et al., 2017b;</ref><ref type="bibr" target="#b38">Mo et al., 2020)</ref> for training the baselines and other DNAS variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Anomaly Detection</head><p>Anomaly detection is a task that classifies temporal signals as "normal" or "abnormal". Anomaly detection finds numerous application, including industrial factories, where it is deployed on smart sensors to monitor equipment and detect problems. The dataset used for anomaly detection in TinyMLperf is MIMII(Slide Rail) <ref type="bibr" target="#b41">(Purohit et al., 2019)</ref>. It is a dataset of industrial machine sounds operating under normal or anomalous conditions recorded in real factory environments. The original dataset contains different machine types, but we focus on the Slide Rail task as selected in TinyMLPerf benchmarks.</p><p>Anomaly detection is an unsupervised learning problem.</p><p>The model only sees "normal" samples at training time and is expected to make predictions on a mix of normal and abnormal cases at test time. Many unsupervised learning methods can be applied, however, inspired by state-of-theart solutions <ref type="bibr" target="#b17">(Giri et al., 2020)</ref>, we reformulate the problem as a self-supervised <ref type="bibr" target="#b22">(Hendrycks et al., 2019)</ref> learning problem, so that it can be handled in a similar way as the other two tasks. The essential idea is to leverage machine ID metadata provided in this dataset. The training dataset contains 4 different machine IDs, each corresponding to a different slide machine for which the audio is recorded. We train a classifier in a supervised way to identify the machine ID given the audio as input. The classifier needs to learn useful information about the normal operating sound of these machines to tell them apart, which can then be used to detect anomaly. At testing time, we use the softmax score for the test sample machine ID as an index of how confident the classifier is about the test sample falling into the normal operating regime data on which it has been trained. Therefore its negative can be used as an anomaly score (higher meaning more likely to be abnormal). The area under the curve (AUC) metric from the receiver operating characteristic (ROC) is calculated using this anomaly score.</p><p>Data preprocessing is done in a similar way as for KWS: the audio signal is transformed into log-Mel spectrograms, which are then input to a CNN classifier. An audio clip of length 10s is split into overlapping frames of length 64ms with a stride (hop length) of 32ms between frames. 64 MFCC features are extracted for each frame. The preprocessed dataset is available on Kaggle (Kaggle AD). We then stack 64 frames together to get 64 by 64 images and the next image has an overlap of 44 frames. We found that CNNs can tolerate even lower resolution spectrograms so the image is further down-sampled to 32?32 using bilinear interpolation. This is the input to our CNN classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MICRONET MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Optimizations</head><p>We use DNAS to discover models which are highly accurate, while also satisfying SRAM, eFlash, and latency constraints.</p><p>In the following, we briefly review DNAS and how it can be applied to ML model design for MCUs. For further information, we refer the reader to <ref type="bibr" target="#b33">(Liu et al., 2019;</ref><ref type="bibr" target="#b4">Cai et al., 2019;</ref><ref type="bibr" target="#b11">Dong &amp; Yang, 2019;</ref><ref type="bibr" target="#b52">Wan et al., 2020)</ref>. The search begins with the definition of a supernet consisting of decision nodes. The output of a decision node expresses a choice between K options</p><formula xml:id="formula_0">y = K k=1 z k f k (x, ? k ), K k=1 z k = 1 (1)</formula><p>where x is the input tensor, f k () is the operation executed by choice k and parameterized by ? k , K is the total number of options for the decision node, and z k ? {0, 1} represents the selection of one of K options. The goal of the search is to select z = z 1 ? ? ? z K for all of the decision nodes in the supernet. In the present work, we restrict our search to the width for each layer in the supernet. In this case, each option f k () represents on operation with a different number of channels <ref type="bibr" target="#b52">(Wan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Optimizing for MCU Memory</head><p>Without any model constraints, DNAS may produce models which violate one or more MCU hardware limits. Given the model size and the size of the intermediate activations produced by modern NNs, eFlash and SRAM play an important role in model design. We incorporate appropriate regularization terms in our DNAS experiments such that the selected models both fit in eFlash memory and produce activations which can fit in available SRAM. For model size considerations, we express the size of a particular selection from the supernet using</p><formula xml:id="formula_1">K k=1 z k |? k | (2)</formula><p>where |? k | denotes the cardinality of ? k . Summing the size of each node, we obtain the size of the supernet as a function of decision parameters z for each decision node, which we use to regularize the DNAS such that the selected architecture meets the MCU eFlash constraint.</p><p>In order to ensure that the selected architecture satisfies SRAM constraints, we adopt the working memory model from <ref type="bibr" target="#b15">Fedorov et al. (2019)</ref>, which states that the working memory required for a particular node with inputs</p><formula xml:id="formula_2">{x 1 , ? ? ? , x N } and outputs {y 1 , ? ? ? , y M } is given by N n=1 |x n | + M m=1 |y m | .<label>(3)</label></formula><p>For tensors which are outputs of decision nodes, we replace |x n | by (2). The total model working memory is then defined as the maximum over the working memory of every network node, which we include in the DNAS objective function such that the discovered architecture meets the MCU SRAM constraint. We define the constraint as the available SRAM minus the expected TFLM overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Optimizing for Latency</head><p>In addition to making sure that discovered models are deployable, we also incorporated a latency constraint into our DNAS experiments. Due to the (almost) linear relationship between latency and number of operations for ML inference on MCUs, we treat the operation count as a strong proxy for latency during optimization. As with memory, we begin by defining the operation count of each decision node as a function of the decision vector z:</p><formula xml:id="formula_3">K k=1 z k c k (4)</formula><p>where c k is the number of ops required to execute option k.</p><p>Note that the number of operations for a particular option typically depends on the input and output tensor sizes, which are a function of decision parameters z <ref type="bibr" target="#b52">(Wan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Sub-Byte Quantization</head><p>The predominate datatype for NN inference on microcontrollers is 8-bit integer. The use of smaller 4-bit datatypes <ref type="bibr" target="#b13">(Esser et al., 2020;</ref><ref type="bibr" target="#b40">Park &amp; Yoo, 2020;</ref><ref type="bibr" target="#b3">Banner et al., 2019)</ref> for weights (and/or activations) allows for more parameters (and/or feature maps), potentially realizing higher accuracy in the same memory footprint. However, current MCUs do not natively support sub-byte datatypes, so they must be emulated using 8-bit instructions. We investigated the benefit of 4-bit quantization on the KWS task.</p><p>Currently, the CMSIS-NN <ref type="bibr">(Lai et al., 2018a)</ref>, does not provide convolution operators for 4-bit values. Therefore, we developed optimized kernels for 4-bit datatypes, and incorporated them into CMSIS-NN for use in our experiments. This allows our DNAS to expand the search space to fit models with more weights and/or activations, potentially achieving higher accuracy in the same memory footprint. The unpacking and packing routines required to emulate hardware support for 4-bit using native 8or 16-bit operations add negligible latency overhead. These optimized kernels can efficiently support sub-byte quantization on either weights or activations or both. Prior work on mixed-precision inference (CMix-NN <ref type="bibr" target="#b5">(Capotondi et al., 2020)</ref>) does not support operations on signed sub-byte weight and activation values, nor non-modulo-4 feature-map channel numbers, and therefore is not compatible with current CMSIS-NN software and TFLM runtime stack. We anticipate that future MCUs may provide native hardware support for 4-bit datatypes, further increasing the value of this research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DNAS Backbones and Training Recipes</head><p>DNAS requires a backbone supernet to be defined as the starting point for the search. The design of the backbone is an important step which requires human experience of network operators and connectivity patterns that work well for a given task. If the backbone is too large, the supernet will not fit in GPU memory. On the other hand, if the backbone is too small, it may not provide a rich enough search space within which to find models that satisfy the constraints. In this section, we describe the backbones used for the TinMLperf tasks and the training methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Visual Wake Words (VWW)</head><p>We use a MobilenetV2 <ref type="bibr" target="#b46">(Sandler et al., 2018)</ref> backbone, consisting of a series of inverted bottleneck (IBN) blocks.</p><p>Each IBN block includes the sequence: 1 ? 1 conv, 3 ? 3 depthwise conv, 1 ? 1 conv. We restrict our search space to the width of the first and last convolutions in each IBN, as well the convolutions preceding and following the sequence of IBN blocks. For each convolution, we choose between 10% and 100% of the width of the corresponding layer in MobilenetV2, in increments of 10%. In order to ensure that the input itself does not violate the SRAM constraint, we resize the input images to 50 ? 50 ? 1 and 160 ? 160 ? 1 for the small (STMF446RE) and medium (STM32F746ZG) sized MCUs, respectively. Note that we convert the RGB images to grayscale, such that the input only has 1 channel, in order to trade off color resolution for spatial resolution <ref type="bibr" target="#b15">(Fedorov et al., 2019;</ref><ref type="bibr" target="#b9">Chowdhery et al., 2019)</ref>.</p><p>We run DNAS for 200 epochs, batch size 768, decaying the learning rate from 0.36 to 0.0008 with a cosine schedule. We use quantization aware training <ref type="bibr" target="#b26">(Krishnamoorthi, 2018)</ref> to emulate 8-bit quantization of both weights and activations during training. Discovered architectures are finetuned for 200 epochs with the same learning rate schedule, weight decay of 0.00004, and knowledge distillation using MobilenetV2 as the teacher, knowledge distillation coefficient 0.5, and temperature 4 <ref type="bibr" target="#b24">(Hinton et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Keyword Spotting (KWS)</head><p>After experimenting with different architectures on the KWS task, we settled on an enlarged DS-CNN(L) <ref type="bibr">(Zhang et al., 2017b)</ref> model as the backbone for DNAS. The backbone is built by adding four more depthwise-separable blocks of output channels 276 to the largest variant of DS-CNN. A skip connection branch (average pooling if the parallel convolutional block downsamples the input) is also added in parallel to each depthwise-separable block in the backbone to create shortcuts for choosing the number of layers. We use DNAS to choose the number of channels and the number of layers in this backbone network, while trying to satisfy the hardware constraints. The number of channels are restricted to multiples of 4 for good performance on hardware. For the small and medium models, the constrains were set to achieve 10FPS and 5FPS on the medium (STM32F746ZG) board while also suiting the smallest (STMF446RE) board. It is thus a combination of latency and working memory constraints. For the large model, we target latency of less than one second, in order to achieve real-time throughput.</p><p>DNAS is run for 100 epochs, with a batch size of 512, decaying the learning rate from 0.01 to 0.00001 with a cosine schedule. A weight decay coefficient of 0.001 is used. Additionally, we quantize weights, activations and input to 8-bit using fake quantization nodes to simulate deployment. The ranges of quantizers are learnt with gradient descent. We observe that model accuracy at the end of DNAS is often very good and no further fine-tuning is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Anomaly Detection (AD)</head><p>For AD, the model operates on spectrograms of audio signals in a similar way as for KWS, so it makes sense for the two tasks to share similar backbone networks. Hence, the backbone network we used for AD was DSCNN-L, with parallel skip connections (or average pooling if downsampling) to skip layers. The strides of the last two depthwiseseparable blocks are increased to 2 to downsample the input patch down to 4?4 before applying the final pooling. DNAS searches for channel numbers and the total number of layers to meet the hardware deployment constraints. An anomaly detection system is expected to run in real-time for continuous monitoring, and should therefore take less time than the increment between two successive spectrogram images (considering overlapping). In our setting, this latency cutoff can be calculated as 32 ? 20ms = 640ms. This latency constraint together with the SRAM limits for each board are used as constraints in our DNAS runs.</p><p>We use the same set of DNAS hyperparameters as for KWS, except we only train for 50 epochs, as convergence is faster on this task. We also apply a mixup <ref type="bibr">(Zhang et al., 2017a)</ref> augmentation coefficient of 0.3 to avoid overfitting. We have experimented with the spectral warping augmentations as suggested in <ref type="bibr" target="#b17">Giri et al. (2020)</ref> but did not observe benefits in our setting. It is likely that since our models are relatively parameter-efficient and we use quantization aware training, less data augmentations are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methodology</head><p>To deploy our models, we convert them to TFlite format and then execute them on each MCU using the TFLM runtime. The eFlash occupancy is determined using the Mbed compiler (Mbed OS) and the SRAM consumption is obtained using the TFLM recording memory APIs. We measure latency on the MCU using the Mbed Timer API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Visual Wake Words (VWW)</head><p>Figure <ref type="figure">8</ref> compares our DNAS results (MicroNets) to three state of the art results, including ProxylessNAS <ref type="bibr" target="#b4">(Cai et al., 2019)</ref>, MSNet <ref type="bibr" target="#b7">(Cheng et al., 2019)</ref>, and the TFLM example model <ref type="bibr" target="#b9">(Chowdhery et al., 2019)</ref>. The largest network in our search space is MobileNetV2, which achieves 88.75% accuracy. The MicroNets models are visualized in Figure <ref type="figure" target="#fig_5">6</ref>. We found that the model produced by targeting the medium MCU (88.03%) nearly matched the accuracy of MobileNetV2, obviating the need to search for a large-MCU specific model. MicroNets are pareto-optimal for the small and medium sized MCUs. For the small MCU, our MicroNet is 3.1% more accurate than the TFLM reference, the only network considered which can be deployed on the small MCU with TFLM, while being 21ms faster. For the medium MCU, our MicroNet model was the only model considered that could be deployed on that MCU. tics. These limitations underline the motivation for DNAS optimized models to target a specific MCU size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The limitations of previous</head><p>MicroNet models are not tight against the SRAM and flash constraints for a number of reasons. TFLM has to schedule every graph and perform memory management, which leads to some variability in the resulting model size. Therefore, we estimate the maximum model size and activation footprint possible for a given hardware platform by performing some experiments with TFLM. However, the final binary size is still somewhat dependent on the graph itself, so this prevents us from tightly meeting the constraints. In a real application context there will also be application logic and potentially even a real-time operating system (RTOS) (Mbed OS), which will take additional eFlash memory resources that must be budgeted into the model constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Keyword Spotting (KWS)</head><p>The results on KWS are shown in Figure <ref type="figure">7</ref> where we compare our results (MicroNet) to DS-CNNs <ref type="bibr">(Zhang et al., 2017b)</ref> and models built by stacking MobileNetV2 <ref type="bibr" target="#b46">(Sandler et al., 2018)</ref> bottleneck blocks. Few results are currently available for version 2 of the Google speech commands dataset, therefore we train baseline models for comparison.</p><p>We trained all the models with exactly the same training recipe and quantized them to 8-bit weights and activations (including input) before measuring their accuracy. MicroNet models are the Pareto optimal for latency, SRAM usage and model size. MicroNet small and medium models also come very close to the latency constraints set for them, achieving 9.2FPS and 5.4FPS on the medium sized MCU while having accuracy of 93.2% and 94.2% and being deployable on the smallest MCU. Detailed description of MicroNet KWS model architectures can be found in the Appendix.</p><p>We can further leverage sub-byte quantization to make bigger but more accurate models deployable on smaller MCUs. Table <ref type="table">2</ref> demonstrates the accuracy, latency, and SRAM memory trade-off of a 4-bit MicroNet discovered by DNAS, targeting the small MCU (STM32F446RE) on the KWS task. The 4-bit KWS MicroNet outperforms the 8-bit mediumsized model by 0.3%, because it is able to have more weights and activations on the same small MCU board. Table <ref type="table">2</ref> reports the latency measured on the different models on the medium MCU (STM32F746ZG). The increase in latency of the 4-bit model is primarily attributed to increase in ops due to larger feature maps. When compared to the sub-byte kernels of CMix-NN <ref type="bibr" target="#b5">(Capotondi et al., 2020)</ref>, our 4-bit kernels can substantially hide the latency overhead due to software-emulation of 4-bit operations, by fully exploiting the available instruction-level-parallelism (ILP) bandwidth on Cortex-M microcontrollers. Furthermore, we believe the accuracy of the 4-bit KWS MicroNet can be further improved by selectively quantizing lightweight depthwise layers to 8-bits, while quantizing remaining memory-and latency-heavy pointwise and standard convolutional layers to 4-bits <ref type="bibr">(Rusci et al., 2020a;</ref><ref type="bibr" target="#b19">Gope et al., 2020)</ref>. Table <ref type="table">2</ref>. KWS results for 4-bit quantized MicroNet models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Anomaly Detection (AD)</head><p>Results for AD are given in Table <ref type="table">3</ref>. To aid comparison, we use the "Uptime" metric defined as model latency divided by stride time between two successive inputs. For real-time AD, this ratio is the duty cycle of the MCU workload. For example, our models expect a stride of 640ms while some other models expect 32ms and need to be run more often. This metric is also relates directly to power consumption.</p><p>The models obtained from DNAS are called MicroNet-AD.</p><p>The three different sized models each targets a different sized MCU. Our solutions are compared with a baseline fully-connected auto-encoder (FC-AE) for anomaly detection <ref type="bibr" target="#b41">(Purohit et al., 2019)</ref>, which has a 640 dimensional input, followed by 4 fully-connected hidden layers of 128 neurons each, a bottleneck layer of 8 neurons, 4 fully-connected hidden layers of 128 neurons again and the output. This baseline model achieves 84.76% AUC and runs fast. However, once we try to scale it up for better anomaly detection performance, the size of the model quickly exceeds the flash limit of all MCUs used in this work. The wide FC-AE model which scales up all the hidden neuron number from 128 to 512 in the baseline, achieves only 87.1% AUC but its size exceeds 2MB in 8-bit, making it undeployable on our MCUs. An alternative to fully-connected AE that is more parameter efficient is convolutional AE, which we have also included in the comparison. Convolutional AEs require the transposed convolution operator, not supported in TFLM.</p><p>We also compare with the MobileNetV2-0.5AD model trained in a similar self-supervised way as ours, which is a component of the winning solution at DCASE2020 challenge (DCASE) presented in <ref type="bibr" target="#b17">Giri et al. (2020)</ref>. Since the authors submitted ensembles of multiple classifiers, we take the average of AUCs reported where the MobileNetV2-0.5AD is a component as an estimate of its accuracy. This model can only be deployed on the largest MCU because of it relatively large size (close to 1MB). The model is light in number of operations but its uptime requirement is worse than our solutions since it expects a time stride of 256ms.</p><p>Our large MicroNet model is equally performing in terms of AUC, requires less than half the Flash size and consumes less compute resources in terms of uptime requirement. The smallest MicroNet-AD model can be deployed on the small MCU performing real-time AD with &gt; 95% AUC performance. As we have shown previously, the small MCU only draws about 1/3 the power of the medium, which is attrac- Table <ref type="table">3</ref>. AD results. The AUC for our models are with 8-bit weights/activations, while Conv-AE <ref type="bibr" target="#b43">(Ribeiro et al., 2020)</ref> and MBNETV2-0.5AD <ref type="bibr" target="#b17">(Giri et al., 2020)</ref> are reported for unquantized FP32. S, M and L denote small, medium and large MCU targets.</p><p>tive since most of these tiny IoT devices run on batteries or need to be energy self-sufficient. MicroNet-AD model architectures can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with State-of-the-Art</head><p>A key previous TinyML work is SpArSe <ref type="bibr" target="#b15">(Fedorov et al., 2019)</ref>. This work is focused on even smaller MCUs, with memory down to 2KBs. However, it also generally targets smaller datasets with smaller input dimensions than the industry-standard tinyMLperf tasks that we use in our work. In a parallel line of work, MCUNet <ref type="bibr" target="#b32">(Lin et al., 2020)</ref> demonstrated SOTA MCU models using a framework that jointly designs the model architecture and the lightweight code-generation inference engine. Their latency and SRAM measurement relies on a closed-source software stack that is not available to us, so it is difficult to make comparison with their results. However, our models are pareto optimal compared to MCUNet on the KWS task even though we use a readily available, open source software stack.</p><p>In support of this paper, we will also open source our models for all three TinyMLperf tasks. We hope these models will be especially useful for MCU vendors and researchers, as a set of standard models for benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>TinyML promises to enable a broad array of IoT applications, but is technically challenging. This is primarily due to the memory demands of deep neural network inference, which are at conflict with the limitations of MCUs. We start by analyzing measured MCU inference performance. Measurements demonstrate that for models sampled from a given network search space, the inference latency of the model is, in fact, linear with the total operation count. Since MCU power is largely independent of workload, operation count is also a strong proxy for energy per inference. Therefore, we use operation count as a proxy for both latency and energy, and setup a differentiable NAS search to design a family of models called MicroNets. MicroNet models optimized for multiple MCUs demonstrate state-of-the-art performance on all three tinyMLperf tasks: visual wake words, audio keyword spotting and anomaly detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RESULTS TABLE</head><p>In Table <ref type="table" target="#tab_7">4</ref> we provide a table of our results and baselines for easy comparison with future work. We report the flash consumption of the model flatbuffer and the SRAM consumption of the model whole model. We also report latency on the STM32F446RE (S), STM32F746ZG (M) and SRM32F767ZI (L) as well as energy consumption on the STM32F446RE (S) and STM32F746ZG (M). All of models are deployed using the TFLM inference framework. The eFlash consumption is determined by the MBED compiler and the SRAM consumption is obtained using the TFLM recording micro interpreter. We measure latency on the MCU in microseconds using the MBED Timer API. Finally we use the Qoitech Otii Arc (Otii) to measure energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B POWER TRACE</head><p>We plot the current vs. time for a small model and a medium model on the STM32f446RE and STMF746ZG in Figure <ref type="figure" target="#fig_6">9</ref>. We also report the average power consumption over 1 second to illustrate the impact of the deep sleep power consumption on the overall energy consumption of a tinyML application with a duty cycle of one frame per second. We show that the current consumption varies little between models but the smaller model consume significantly less energy due to its reduced latency. Figure <ref type="figure" target="#fig_6">9</ref> also demonstrates that the smaller mcu consumes less power on average despite being active for longer. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Illustration of memory hierarchies for (a) a mobile SoC which has a deep memory hierarchy with many levels of on-chip cache and a large off-chip DRAM main memory, and (b) an MCU with a flat on-chip memory system with no off-chip main memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Measured latency of a range of different individual layer types and sizes on the STM32F767ZI using TFLM. Different layers can exhibit a spread in latencies for the same ops count, due to variations in, for example, data reuse and IM2COL overheads.</figDesc><graphic url="image-1.png" coords="4,55.44,67.06,234.00,175.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Measured latency of whole models randonly sampled from two backbones, on STM32F446RE and STM32F746ZG. Models sampled from a given search space exhibit latency linear with ops, despite the variation seen with individual layers.</figDesc><graphic url="image-2.png" coords="4,307.44,67.06,234.00,175.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4 shows measured model latency on the STMF446RE and the STMF746ZG. Measurements are shown for random models sampled from backbones tailored to two different tasks viz. image classification and audio classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Measured energy and power of models randomly sampled from an image classification CNN backbone. MCUs have simple microarchitectures and memory systems and hency power is fairly constant. Therefore, energy is largely determined by latency, which is in turn a linear function of model ops.</figDesc><graphic url="image-3.png" coords="5,55.44,67.06,234.00,175.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. VWW architectures discovered by DNAS targeting (a) small (STM32F446RE), and (b) medium (STM32F446RE) MCUs. The two numbers following IBN denote the number of expansion and compression filters. Tensor dimensions are provided in black text.</figDesc><graphic url="image-5.png" coords="8,55.44,133.13,486.00,52.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Current consumption of a small and medium model on the STM32f446RE and STMF746ZG. We report the average power consumption over one second.</figDesc><graphic url="image-6.png" coords="14,307.44,388.28,234.01,178.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Illustrative comparison of hardware for CloudML, Mo-bileML and TinyML, including the MCUs targeted in this work.</figDesc><table><row><cell>Platform</cell><cell>Architecture</cell><cell>Memory</cell><cell>Storage</cell><cell>Power</cell><cell>Price</cell></row><row><cell>CloudML</cell><cell>GPU</cell><cell>HBM</cell><cell>SSD/Disk</cell><cell></cell><cell></cell></row><row><cell>Nvidia V100</cell><cell>Nvidia Volta</cell><cell>16GB</cell><cell>TB?PB</cell><cell>250W</cell><cell>$9K</cell></row><row><cell>MobileML</cell><cell>CPU</cell><cell>DRAM</cell><cell>Flash</cell><cell></cell><cell></cell></row><row><cell>iPhone 11</cell><cell>Arm A-Class</cell><cell>4GB</cell><cell>64GB</cell><cell>?8W</cell><cell>$750</cell></row><row><cell>TinyML</cell><cell>MCU</cell><cell>SRAM</cell><cell>eFlash</cell><cell></cell><cell></cell></row><row><cell>ST F446RE</cell><cell>Arm M4</cell><cell>128KB</cell><cell>0.5MB</cell><cell>0.1W</cell><cell>$3</cell></row><row><cell>ST F746ZG</cell><cell>Arm M7</cell><cell>320KB</cell><cell>1MB</cell><cell>0.3W</cell><cell>$5</cell></row><row><cell>ST F767ZI</cell><cell>Arm M7</cell><cell>512KB</cell><cell>2MB</cell><cell>0.3W</cell><cell>$8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 )</head><label>1</label><figDesc>, and expect results to improve with new hardware generations.</figDesc><table><row><cell>Software A critical element for TinyML is the inference</cell></row><row><cell>software stack. MCU vendors typically provide low-level</cell></row><row><cell>libraries with specific primitives for basic NN operators</cell></row><row><cell>like convolution, such as the CMSIS-NN (Lai et al., 2018a)</cell></row><row><cell>kernels which are optimized for Arm Cortex-M devices. Al-</cell></row><row><cell>ternatively, MicroTVM automatically generates low level</cell></row><row><cell>kernels. These kernels need stitching together to implement</cell></row><row><cell>a neural network inference graph. Popular ML frameworks</cell></row><row><cell>like TensorFlow and PyTorch are unsuitable for inference on</cell></row><row><cell>MCUs, as the memory requirements are too large. A num-</cell></row><row><cell>ber of ML inference runtimes have emerged to fill this need</cell></row><row><cell>on MCUs. There are two fundamental approaches: code</cell></row></table><note><p><p>generation and interpreter. The code generation approach takes a model definition and automatically generates C code directly. In general, this approach typically gives the best results, but the generated code is not portable between different platforms. Examples include uTensor, tinyEngine</p>(Lin   </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>TinyML models is clear in: they do not fully exploit the precious memory resources. For instance, the ProxylessNAS model easily fits the flash memory on the smallest MCU (STM32F446RE), but requires the largest MCU (STM32F767ZI) to fit the activations in SRAM. Therefore, ProxylessNAS will only run on the large MCU. MSNet shows similar characteris-KWS results. The DNAS search targets the smallest MCU for both small and medium models. MicroNet medium model is more accurate than DS-CNN(L) and runs 2.7? faster. The largest MobileNetV2 variant model does not fit and is omitted.</figDesc><table><row><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy</cell><cell>90 92 94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MobileNet V2 MicroNet s</cell><cell>DSCNN</cell></row><row><cell></cell><cell>88</cell><cell>100</cell><cell>200</cell><cell></cell><cell>300 Lat ency (m s) 400</cell><cell></cell><cell>500</cell><cell>600</cell></row><row><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy</cell><cell>90 92 94</cell><cell>STMF446RE</cell><cell></cell><cell></cell><cell>STMF746ZG</cell><cell></cell><cell>STMF767ZI</cell></row><row><cell></cell><cell>88</cell><cell>100</cell><cell cols="2">200</cell><cell>300 SRAM (KB)</cell><cell>400</cell><cell>500</cell></row><row><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy</cell><cell>90 92 94</cell><cell></cell><cell>STMF446RE</cell><cell></cell><cell>STMF746ZG</cell><cell></cell><cell></cell><cell>STMF767ZI</cell></row><row><cell></cell><cell>88</cell><cell>250</cell><cell>500</cell><cell>750</cell><cell>1000 Flash (KB) 1250</cell><cell>1500</cell><cell>1750</cell><cell>2000</cell></row><row><cell cols="2">Figure 7.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>VWW results. We target DNAS to find models which can be deployed on the small and medium MCUs. Our search yields MicroNets which are pareto optimal.</figDesc><table><row><cell>Test Accuracy</cell><cell>77 80 83 86 89 92</cell><cell></cell><cell></cell><cell></cell><cell>MicroNet s TFLM Proxylessnas</cell><cell>MSNet MobileNet V2</cell></row><row><cell></cell><cell>74</cell><cell>0</cell><cell>2000</cell><cell>4000 Lat ency (m s)</cell><cell>6000</cell><cell>8000</cell></row><row><cell>Test Accuracy</cell><cell>77 80 83 86 89 92</cell><cell>STMF446RE</cell><cell></cell><cell>STMF746ZG</cell><cell></cell><cell>STMF767ZI</cell></row><row><cell></cell><cell>74</cell><cell>100</cell><cell>200</cell><cell>300 SRAM (KB)</cell><cell>400</cell><cell>500</cell></row><row><cell>Test Accuracy</cell><cell>77 80 83 86 89 92</cell><cell>STMF446RE</cell><cell>STMF746ZG</cell><cell></cell><cell>STMF767ZI</cell></row><row><cell></cell><cell>74</cell><cell>500</cell><cell>1000</cell><cell>1500 Flash (KB)</cell><cell>2000</cell><cell>2500</cell></row><row><cell cols="3">Figure 8.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Results Table. (*) Estimated (-) Unable to measure do to SRAM or eFlash constraints</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Accuracy</cell><cell>Flash(KB)</cell><cell>SRAM(bytes)</cell><cell>S latency (S)</cell><cell>M latency (S)</cell><cell>L latency (S)</cell><cell>S Energy(mJ)</cell><cell>M Energy(mJ)</cell></row><row><cell>GSC</cell><cell>MicroNet-KWS-L</cell><cell>95.3</cell><cell>612</cell><cell>208840</cell><cell>-</cell><cell>0.610128</cell><cell>0.595811</cell><cell>-</cell><cell>274.32</cell></row><row><cell>GSC</cell><cell>MicroNet-KWS-M</cell><cell>94.2</cell><cell>163</cell><cell>103360</cell><cell>0.425768</cell><cell>0.18671</cell><cell>0.181068</cell><cell>70.56</cell><cell>83.16</cell></row><row><cell>GSC</cell><cell>MicroNet-KWS-S</cell><cell>93.2</cell><cell>102</cell><cell>53208</cell><cell>0.249885</cell><cell>0.108839</cell><cell>0.107573</cell><cell>40.68</cell><cell>48.6</cell></row><row><cell>VWW</cell><cell>MicroNet-VWW-M</cell><cell>87.3</cell><cell>855</cell><cell>284692</cell><cell>-</cell><cell>1.165829</cell><cell>1.126236</cell><cell>-</cell><cell>507.6</cell></row><row><cell>VWW</cell><cell>MicroNet-VWW-S</cell><cell>79.6</cell><cell>217</cell><cell>70060</cell><cell>0.187745</cell><cell>0.084759</cell><cell>0.083967</cell><cell>29.196</cell><cell>38.16</cell></row><row><cell>Anomaly</cell><cell>MicroNet-AD-L</cell><cell>AUC: 97.28</cell><cell>442</cell><cell>383776</cell><cell>-</cell><cell>-</cell><cell>0.613999</cell><cell>-</cell><cell>-</cell></row><row><cell>Anomaly</cell><cell>MicroNet-AD-M</cell><cell>AUC: 96.05</cell><cell>453</cell><cell>274528</cell><cell>-</cell><cell>0.607672</cell><cell>0.566909</cell><cell>-</cell><cell>269.64</cell></row><row><cell>Anomaly</cell><cell>MicroNet-AD-S</cell><cell>AUC: 95.35</cell><cell>247</cell><cell>114292</cell><cell>0.457176</cell><cell></cell><cell>0.19404</cell><cell>74.16</cell><cell>91.8</cell></row><row><cell>GSC</cell><cell>DSCNN-L</cell><cell>93.9</cell><cell>490</cell><cell>201392</cell><cell>-</cell><cell>0.515165</cell><cell>0.497092</cell><cell>-</cell><cell>229.32</cell></row><row><cell>GSC</cell><cell>DSCNN-M</cell><cell>93.5</cell><cell>181</cell><cell>123348</cell><cell>-</cell><cell>0.219424</cell><cell>0.211568</cell><cell>-</cell><cell>98.64</cell></row><row><cell>GSC</cell><cell>DSCNN-S</cell><cell>92.1</cell><cell>49</cell><cell>47188</cell><cell>0.130769</cell><cell>0.058416</cell><cell>0.057685</cell><cell>21.132</cell><cell>25.956</cell></row><row><cell>GSC</cell><cell>MBNETV2-L</cell><cell>91.2</cell><cell>988</cell><cell>530000*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GSC</cell><cell>MBNETV2-M</cell><cell>90.4</cell><cell>233</cell><cell>265976</cell><cell>-</cell><cell>0.330264</cell><cell>0.31697</cell><cell>-</cell><cell>147.6</cell></row><row><cell>GSC</cell><cell>MBNETV2-S</cell><cell>89.2</cell><cell>87</cell><cell>134200</cell><cell>-</cell><cell>0.119629</cell><cell>0.115458</cell><cell>54</cell><cell>15.264</cell></row><row><cell>VWW</cell><cell>ProxylessNas (Cai et al., 2019)</cell><cell>94.6</cell><cell>309</cell><cell>349772</cell><cell>-</cell><cell>7.72*</cell><cell>7.543249</cell><cell>-</cell><cell>-</cell></row><row><cell>VWW</cell><cell>MSNet(Cheng et al., 2019)</cell><cell>95.13</cell><cell>264</cell><cell>413020</cell><cell>-</cell><cell>8.69*</cell><cell>8.498659</cell><cell>-</cell><cell>-</cell></row><row><cell>VWW</cell><cell>TFLM Person Detection (TFLM)</cell><cell>76</cell><cell>294</cell><cell>82276</cell><cell>0.254136</cell><cell>0.107987</cell><cell>0.107831</cell><cell>39.96</cell><cell>49.32</cell></row><row><cell>Anomaly</cell><cell>AD-baseline</cell><cell>AUC:84.76</cell><cell>270</cell><cell>4740</cell><cell>0.007115</cell><cell>0.003326</cell><cell>0.002947</cell><cell>1.1736</cell><cell>1.26</cell></row><row><cell>Anomaly</cell><cell>MBNetV2-0.5 (Giri et al., 2020)</cell><cell>AUC: 97.62?</cell><cell>965</cell><cell>206832</cell><cell>-</cell><cell>-</cell><cell>0.252809</cell><cell>-</cell><cell>-</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ambiq Micro Subthreshold Power Optimized Technology (SPOT)</title>
		<author>
			<persName><surname>Ambiq</surname></persName>
		</author>
		<ptr target="https://ambiq.com/technology/spot/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Performance-oriented neural architecture search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lokhmotov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04821</idno>
		<title level="m">Benchmarking tinyml systems: Challenges and direction</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Posttraining 4-bit quantization of convolution networks for rapid-deployment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">NeurIPS 2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1812.00332.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cmix-nn: Mixed low-precision cnn library for memoryconstrained edge devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fariselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems II: Express Briefs</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="871" to="875" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Small-footprint keyword spotting with graph convolutional network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structural wired neural architecture search for internet of things</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Teague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Msnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.03814" />
		<title level="m">Temporal convolution for real-time keyword spotting on mobile devices. CoRR, abs/1904.03814</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rhodes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05721</idno>
		<title level="m">Visual wake words dataset</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dcase</surname></persName>
		</author>
		<author>
			<persName><surname>Dcase</surname></persName>
		</author>
		<ptr target="http://dcase.community/challenge2020/task-unsupervised-detection-of-anomalous-sounds" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Network pruning via transformable architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://microsoft.github.io/ELL/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="759" to="770" />
		</imprint>
	</monogr>
	<note>ELL. Embedded learning library</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v20/18-598.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learned step size quantization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgO66VKDS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="https://developer.arm.com/ip-products/processors/machine-learning/ethos-u55" />
		<title level="m">Ethos-U55. Arm ethos-u55</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SpArSe: Sparse architecture search for cnns on resourceconstrained microcontrollers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4977" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mandell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><surname>Tinylstms</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11138</idno>
		<title level="m">Efficient Neural Speech Enhancement for Hearing Aids</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised anomalous sound detection using self-supervised classification and group masked autoencoder for density estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Tenneti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
		<respStmt>
			<orgName>DCASE2020 Challenge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ternary hybrid neural-tree networks for highly constrained iot applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ternary mobilenets via per-layer hybrid filter banks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Protonn: Compressed and accurate knn for resource-scarce devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G S</forename><surname>Suggala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simhadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<idno>PMLR 70</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02">February 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Memory-optimal direct convolutions for maximizing classification accuracy in embedded applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Murmann</surname></persName>
		</author>
		<ptr target="https://developer.arm.com/architectures/instruction-sets/simd-isas/helium" />
	</analytic>
	<monogr>
		<title level="m">of Proceedings of Machine Learning Research</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
	<note>PMLR. Helium. Arm Helium Technology</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15663" to="15674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Computer Architecture, Fifth Edition: A Quantitative Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>5th edition, 2011. ISBN 012383872X</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/daisukelab/dc" />
		<title level="m">Preprocessed Anomaly Detection Dataset hosted on Kaggle</title>
		<imprint>
			<date>2020task2prep</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference: A whitepaper</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Resource-efficient machine learning in 2 kb ram for the internet of things</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1935" to="1944" />
		</imprint>
	</monogr>
	<note>ICML&apos;17</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<ptr target="all_papers/KusupatiSBKJV18.pdf.slides/fastgrnn.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9031" to="9042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">CMSIS-NN: efficient neural network kernels for arm cortex-m cpus</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>CoRR, abs/1801.06601</idno>
		<ptr target="http://arxiv.org/abs/1801.06601" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Not all ops are created equal! CoRR</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>abs/1801.04326</idno>
		<ptr target="http://arxiv.org/abs/1801.04326" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Mcunet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10319</idno>
		<title level="m">Tiny deep learning on iot devices</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1eYHoC5FX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="https://developer.arm.com/ip-products/processors/cortex-m/cortex-m4" />
		<title level="m">Arm cortex-m4</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><surname>M7</surname></persName>
		</author>
		<ptr target="https://developer.arm.com/ip-products/processors/cortex-m/cortex-m7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">S</forename><surname>Mbed</surname></persName>
		</author>
		<ptr target="https://os.mbed.com/mbed-os/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Microtvm</surname></persName>
		</author>
		<ptr target="https://tvm.apache.org/docs/api/python/micro.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00165</idno>
		<title level="m">Neural architecture search for keyword spotting</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><surname>Otii</surname></persName>
		</author>
		<ptr target="https://www.qoitech.com/otii/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">PROFIT: A novel training method for sub-4-bit mobilenet models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<idno>CoRR, abs/2008.04693</idno>
		<ptr target="https://arxiv.org/abs/2008" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ichige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nikaido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suefusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><surname>Mimii</surname></persName>
		</author>
		<author>
			<persName><surname>Dataset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09347</idno>
		<title level="m">Sound dataset for malfunctioning industrial machine investigation and inspection</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep dense and convolutional autoencoders for unsupervised anomaly detection in machine condition sounds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pilastri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10417</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Leveraging automated mixed-low-precision quantization for tiny edge microcontrollers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fariselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<idno>CoRR, abs/2008.05124</idno>
		<ptr target="https://arxiv.org/abs/2008.05124" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Leveraging automated mixed-low-precision quantization for tiny edge microcontrollers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fariselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<orgName type="collaboration">ST F446RE</orgName>
		</author>
		<ptr target="https://www.st.com/en/microcontrollers-microprocessors/stm32f446re.html" />
		<title level="m">STM32F446RE Microcontroller</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><surname>St F746zg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STM</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">stm32-32-bit-arm-cortex-mcus/ stm32-high-performance-mcus/ stm32f7-series/stm32f7x6/stm32f746zg. html. ST F767ZI</title>
		<author>
			<persName><surname>Microcontroller</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
	</analytic>
	<monogr>
		<title level="m">STM32F767ZI Microcontroller</title>
		<imprint>
			<biblScope unit="page">767</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Tensorflow lite for microcontrollers</title>
		<author>
			<persName><surname>Tflm</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/lite/microcontrollers" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Compressing rnns for iot devices by 15-38x using kronecker products</title>
		<author>
			<persName><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<idno>CoRR, abs/1906.02876</idno>
		<ptr target="https://utensor.github.io/website/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient Hardware for Mobile Computer Vision via Transfer Learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Venkataramanaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><surname>Fixynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference</title>
		<meeting>the 2nd SysML Conference<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Attention condensers for deep speech recognition neural networks on edge devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Famouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Surana</surname></persName>
		</author>
		<author>
			<persName><surname>Tinyspeech</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Hello edge: Keyword spotting on microcontrollers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>CoRR, abs/1711.07128</idno>
		<ptr target="http://arxiv.org/abs/1711.07128" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07128</idno>
		<title level="m">Keyword spotting on microcontrollers</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
