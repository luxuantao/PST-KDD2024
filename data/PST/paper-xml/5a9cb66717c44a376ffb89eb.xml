<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
							<email>&lt;aathalye@mit.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technol</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Nicholas Carlini</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimizationbased attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining noncertified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In response to the susceptibility of neural networks to adversarial examples <ref type="bibr" target="#b25">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b2">Biggio et al., 2013)</ref>, there has been significant interest recently in constructing defenses to increase the robustness of neural networks. While progress has been made in understanding and defending against adversarial examples in the white-box setting, where the adversary has full access to the network, a complete solution has not yet been found.</p><p>As benchmarking against iterative optimization-based attacks (e.g., <ref type="bibr" target="#b12">Kurakin et al. (2016a)</ref>; <ref type="bibr" target="#b15">Madry et al. (2018)</ref>; <ref type="bibr" target="#b6">Carlini &amp; Wagner (2017c)</ref>) has become standard practice in evaluating defenses, new defenses have arisen that appear to be robust against these powerful optimization-based attacks.</p><p>We identify one common reason why many defenses provide apparent robustness against iterative optimization attacks: obfuscated gradients, a term we define as a special case of gradient masking <ref type="bibr" target="#b17">(Papernot et al., 2017)</ref>. Without a good gradient, where following the gradient does not successfully optimize the loss, iterative optimization-based methods cannot succeed. We identify three types of obfuscated gradients: shattered gradients are nonexistent or incorrect gradients caused either intentionally through non-differentiable operations or unintentionally through numerical instability; stochastic gradients depend on test-time randomness; and vanishing/exploding gradients in very deep computation result in an unusable gradient.</p><p>We propose new techniques to overcome obfuscated gradients caused by these three phenomena. We address gradient shattering with a new attack technique we call Backward Pass Differentiable Approximation, where we approximate derivatives by computing the forward pass normally and computing the backward pass using a differentiable approximation of the function. We compute gradients of randomized defenses by applying Expectation Over Transformation <ref type="bibr" target="#b0">(Athalye et al., 2017)</ref>. We solve vanishing/exploding gradients through reparameterization and optimize over a space where gradients do not explode/vanish.</p><p>To investigate the prevalence of obfuscated gradients and understand the applicability of these attack techniques, we use as a case study the ICLR 2018 non-certified defenses that claim white-box robustness. We find that obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on this phenomenon. Applying the new attack techniques we develop, we overcome obfuscated gradients and circumvent 6 of them completely, and 1 partially, under the original threat model of each paper. Along with this, we offer an analysis of the evaluations performed in the papers.</p><p>Additionally, we hope to provide researchers with a common baseline of knowledge, description of attack techniques, and common evaluation pitfalls, so that future defenses can avoid falling vulnerable to these same attack approaches.</p><p>To promote reproducible research, we release our reimplementation of each of these defenses, along with implementations of our attacks for each. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notation</head><p>We consider a neural network f (•) used for classification where f (x) i represents the probability that image x corresponds to label i. We classify images, represented as x ∈ [0, 1] w•h•c for a c-channel image of width w and height h. We use f j (•) to refer to layer j of the neural network, and f 1..j (•) the composition of layers 1 through j. We denote the classification of the network as c(x) = arg max i f (x) i , and c * (x) denotes the true label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adversarial Examples</head><p>Given an image x and classifier f (•), an adversarial example <ref type="bibr" target="#b25">(Szegedy et al., 2013)</ref> x satisfies two properties: D(x, x ) is small for some distance metric D, and c(x ) = c * (x). That is, for images, x and x appear visually similar but x is classified incorrectly.</p><p>In this paper, we use the ∞ and 2 distortion metrics to measure similarity. Two images which have a small distortion under either of these metrics will appear visually identical. We report ∞ distance in the normalized [0, 1] space, so that a distortion of 0.031 corresponds to 8/256, and 2 distance as the total root-mean-square distortion normalized by the total number of pixels (as is done in prior work).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Datasets &amp; Models</head><p>We evaluate these defenses on the same datasets on which they claim robustness.</p><p>If a defense argues security on MNIST and any other dataset, we only evaluate the defense on the larger dataset. On MNIST and CIFAR-10, we evaluate defenses over the entire test set and generate untargeted adversarial examples. On ImageNet, we evaluate over 1000 randomly selected images in the test set, construct targeted adversarial examples with randomly selected target classes, and report attack success rate in addition to model accuracy. Generating targeted adversarial examples is a strictly harder problem that we believe is a more meaningful metric for evaluating attacks. <ref type="foot" target="#foot_0">2</ref> Conversely, for a defender, the harder task is to argue robustness to untargeted attacks.</p><p>We use standard models for each dataset. For MNIST we use a standard 5-layer convolutional neural network which reaches 99.3% accuracy. On CIFAR-10 we train a wide ResNet <ref type="bibr" target="#b29">(Zagoruyko &amp; Komodakis, 2016;</ref><ref type="bibr" target="#b10">He et al., 2016)</ref> to 95% accuracy. For ImageNet we use the InceptionV3 <ref type="bibr" target="#b26">(Szegedy et al., 2016)</ref> network which reaches 78.0% top-1 and 93.9% top-5 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Threat Models</head><p>Prior work considers adversarial examples in white-box and black-box threat models. In this paper, we consider defenses designed for the white-box setting, where the adversary has full access to the neural network classifier (architecture and weights) and defense, but not test-time randomness (only the distribution). We evaluate each defense under the threat model under which it claims to be secure (e.g., bounded ∞ distortion of = 0.031). It often easy to find imperceptibly perturbed adversarial examples by violating the threat model, but by doing so under the original threat model, we show that the original evaluations were inadequate and the resultant claims of security were incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Attack Methods</head><p>We construct adversarial examples with iterative optimization-based methods.</p><p>For a given instance x, these attacks attempt to search for a δ such that c(x + δ) = c * (x) either minimizing δ , or maximizing the classification loss on f (x + δ). To generate ∞ bounded adversarial examples we use Projected Gradient Descent (PGD) confined to a specified ∞ ball; for 2 , we use the Lagrangian relaxation of <ref type="bibr" target="#b6">Carlini &amp; Wagner (2017c)</ref>. We use between 100 and 10,000 iterations of gradient descent, as needed to obtain convergance. The specific choice of optimizer is far less important than choosing to use iterative optimization-based methods <ref type="bibr" target="#b15">(Madry et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Obfuscated Gradients</head><p>A defense is said to cause gradient masking if it "does not have useful gradients" for generating adversarial examples <ref type="bibr" target="#b17">(Papernot et al., 2017)</ref>; gradient masking is known to be an incomplete defense to adversarial examples <ref type="bibr" target="#b17">(Papernot et al., 2017;</ref><ref type="bibr" target="#b27">Tramèr et al., 2018)</ref>. Despite this, we observe that 7 of the ICLR 2018 defenses rely on this effect.</p><p>To contrast from previous defenses which cause gradient masking by learning to break gradient descent (e.g., by learning to make the gradients point the wrong direction <ref type="bibr" target="#b27">(Tramèr et al., 2018</ref>)), we refer to the case where defenses are designed in such a way that the constructed defense necessarily causes gradient masking as obfuscated gradients. We discover three ways in which defenses obfuscate gradients (we use this word because in these cases, it is the defense creator who has obfuscated the gradient information); we briefly define and discuss each of them.</p><p>Shattered Gradients are caused when a defense is nondifferentiable, introduces numeric instability, or otherwise causes a gradient to be nonexistent or incorrect. Defenses that cause gradient shattering can do so unintentionally, by using differentiable operations but where following the gradient does not maximize classification loss globally.</p><p>Stochastic Gradients are caused by randomized defenses, where either the network itself is randomized or the input is randomly transformed before being fed to the classifier, causing the gradients to become randomized. This causes methods using a single sample of the randomness to incorrectly estimate the true gradient.</p><p>Exploding &amp; Vanishing Gradients are often caused by defenses that consist of multiple iterations of neural network evaluation, feeding the output of one computation as the input of the next. This type of computation, when unrolled, can be viewed as an extremely deep neural network evaluation, which can cause vanishing/exploding gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Identifying Obfuscated &amp; Masked Gradients</head><p>Some defenses intentionally break gradient descent and cause obfuscated gradients. However, others defenses unintentionally break gradient descent, but the cause of gradient descent being broken is a direct result of the design of the neural network. We discuss below characteristic behaviors of defenses which cause this to occur. These behaviors may not perfectly characterize all cases of masked gradients.</p><p>One-step attacks perform better than iterative attacks. Iterative optimization-based attacks applied in a white-box setting are strictly stronger than single-step attacks and should give strictly superior performance. If single-step methods give performance superior to iterative methods, it is likely that the iterative attack is becoming stuck in its optimization search at a local minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Black-box attacks are better than white-box attacks.</head><p>The black-box threat model is a strict subset of the whitebox threat model, so attacks in the white-box setting should perform better; if a defense is obfuscating gradients, then black-box attacks (which do not use the gradient) often perform better than white-box attacks <ref type="bibr" target="#b17">(Papernot et al., 2017)</ref>.</p><p>Unbounded attacks do not reach 100% success. With unbounded distortion, any classifier should have 0% robustness to attack. If an attack does not reach 100% success with sufficiently large distortion bound, this indicates the attack is not performing optimally against the defense, and the attack should be improved.</p><p>Random sampling finds adversarial examples. Bruteforce random search (e.g., randomly sampling 10 5 or more points) within some -ball should not find adversarial examples when gradient-based attacks do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Increasing distortion bound does not increase success.</head><p>A larger distortion bound should monotonically increase attack success rate; significantly increasing distortion bound should result in significantly higher attack success rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Attack Techniques</head><p>Generating adversarial examples through optimizationbased methods requires useful gradients obtained through backpropagation <ref type="bibr" target="#b19">(Rumelhart et al., 1986)</ref>. Many defenses therefore either intentionally or unintentionally cause gradient descent to fail because of obfuscated gradients caused by gradient shattering, stochastic gradients, or vanishing/exploding gradients. We discuss a number of techniques that we develop to overcome obfuscated gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Backward Pass Differentiable Approximation</head><p>Shattered gradients, caused either unintentionally, e.g. by numerical instability, or intentionally, e.g. by using nondifferentiable operations, result in nonexistent or incorrect gradients. To attack defenses where gradients are not readily available, we introduce a technique we call Backward Pass Differentiable Approximation (BPDA)<ref type="foot" target="#foot_1">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">A SPECIAL CASE: THE STRAIGHT-THROUGH ESTIMATOR</head><p>As a special case, we first discuss what amounts to the straight-through estimator <ref type="bibr" target="#b1">(Bengio et al., 2013)</ref> applied to constructing adversarial examples.</p><p>Many non-differentiable defenses can be expressed as follows: given a pre-trained classifier f (•), construct a preprocessor g(•) and let the secured classifier f (x) = f (g(x))</p><p>where the preprocessor g(•) satisfies g(x) ≈ x (e.g., such a g(•) may perform image denoising to remove the adversarial perturbation, as in <ref type="bibr" target="#b9">Guo et al. (2018)</ref>). If g(•) is smooth and differentiable, then computing gradients through the combined network f is often sufficient to circumvent the defense <ref type="bibr" target="#b5">(Carlini &amp; Wagner, 2017b)</ref>. However, recent work has constructed functions g(•) which are neither smooth nor differentiable, and therefore can not be backpropagated through to generate adversarial examples with a white-box attack that requires gradient signal.</p><p>Because g is constructed with the property that g(x) ≈ x, we can approximate its derivative as the derivative of the identity function: ∇ x g(x) ≈ ∇ x x = 1. Therefore, we can approximate the derivative of f (g(x)) at the point x as:</p><formula xml:id="formula_0">∇ x f (g(x))| x=x ≈ ∇ x f (x)| x=g(x)</formula><p>This allows us to compute gradients and therefore mount a white-box attack. Conceptually, this attack is simple. We perform forward propagation through the neural network as usual, but on the backward pass, we replace g(•) with the identity function. In practice, the implementation can be expressed in an even simpler way: we approximate ∇ x f (g(x)) by evaluating ∇ x f (x) at the point g(x). This gives us an approximation of the true gradient, and while not perfect, is sufficiently useful that when averaged over many iterations of gradient descent still generates an adversarial example.</p><p>The math behind the validity of this approach is similar to the special case.</p><p>4.1.2. GENERALIZED ATTACK: BPDA While the above attack is effective for a simple class of networks expressible as f (g(x)) when g(x) ≈ x, it is not fully general. We now generalize the above approach into our full attack, which we call Backward Pass Differentiable Approximation (BPDA).</p><p>Let f (•) = f 1...j (•) be a neural network, and let f i (•) be a non-differentiable (or not usefully-differentiable) layer. To approximate ∇ x f (x), we first find a differentiable approximation g(x) such that g(x) ≈ f i (x). Then, we can approximate ∇ x f (x) by performing the forward pass through f (•) (and in particular, computing a forward pass through f i (x)), but on the backward pass, replacing f i (x) with g(x). Note that we perform this replacement only on the backward pass.</p><p>As long as the two functions are similar, we find that the slightly inaccurate gradients still prove useful in constructing an adversarial example. Applying BPDA often requires more iterations of gradient descent than without because each individual gradient descent step is not exactly correct.</p><p>We have found applying BPDA is often necessary: replacing f i (•) with g(•) on both the forward and backward pass is either completely ineffective (e.g. with <ref type="bibr" target="#b24">Song et al. (2018)</ref>) or many times less effective (e.g. with <ref type="bibr" target="#b3">Buckman et al. (2018)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attacking Randomized Classifiers</head><p>Stochastic gradients arise when using randomized transformations to the input before feeding it to the classifier or when using a stochastic classifier. When using optimizationbased attacks on defenses that employ these techniques, it is necessary to estimate the gradient of the stochastic function.</p><p>Expectation over Transformation. For defenses that employ randomized transformations to the input, we apply Expectation over Transformation (EOT) <ref type="bibr" target="#b0">(Athalye et al., 2017)</ref> to correctly compute the gradient over the expected transformation to the input.</p><p>When attacking a classifier f (•) that first randomly transforms its input according to a function t(•) sampled from a distribution of transformations T , EOT optimizes the expectation over the transformation E t∼T f (t(x)). The optimization problem can be solved by gradient descent, noting that ∇E t∼T f (t(x)) = E t∼T ∇f (t(x)), differentiating through the classifier and transformation, and approximating the expectation with samples at each gradient descent step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Reparameterization</head><p>We solve vanishing/exploding gradients by reparameterization. Assume we are given a classifier f (g(x)) where g(•) performs some optimization loop to transform the input x to a new input x. Often times, this optimization loop means that differentiating through g(•), while possible, yields exploding or vanishing gradients.</p><p>To resolve this, we make a change-of-variable x = h(z) for some function h(•) such that g(h(z)) = h(z) for all z, but h(•) is differentiable. For example, if g(•) projects samples to some manifold in a specific manner, we might construct h(z) to return points exclusively on the manifold. This allows us to compute gradients through f (h(z)) and thereby circumvent the defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Case Study: ICLR 2018 Defenses</head><p>As a case study for evaluating the prevalence of obfuscated gradients, we study the ICLR 2018 non-certified defenses that argue robustness in a white-box threat model. Each of these defenses argues a high robustness to adaptive, whitebox attacks. We find that seven of these nine defenses rely on this phenomenon, and we demonstrate that our techniques can completely circumvent six of those (and partially circumvent one) that rely on obfuscated gradients. We omit two defenses with provable security claims <ref type="bibr" target="#b18">(Raghunathan et al., 2018;</ref><ref type="bibr" target="#b23">Sinha et al., 2018)</ref> and one that only argues black-box security <ref type="bibr" target="#b27">(Tramèr et al., 2018)</ref>. We include one paper, <ref type="bibr" target="#b14">Ma et al. (2018)</ref>, that was not proposed as a defense per se, but suggests a method to detect adversarial examples.</p><p>There is an asymmetry in attacking defenses versus constructing robust defenses: to show a defense can be bypassed, it is only necessary to demonstrate one way to do so; in contrast, a defender must show no attack can succeed. We study the adversarial training approach of <ref type="bibr" target="#b15">Madry et al. (2018)</ref> which for a given -ball solves</p><formula xml:id="formula_1">θ * = arg min θ E (x,y)∈X max δ∈[− , ] N (x + δ; y; F θ ) .</formula><p>To approximately solve this formulation, the authors solve the inner maximization problem by generating adversarial examples using projected gradient descent.</p><p>Discussion. We believe this approach does not cause obfuscated gradients: our experiments with optimizationbased attacks do succeed with some probability (but do not invalidate the claims in the paper). Further, the authors' evaluation of this defense performs all of the tests for characteristic behaviors of obfuscated gradients that we list. However, we note that (1) adversarial retraining has been shown to be difficult at ImageNet scale <ref type="bibr" target="#b13">(Kurakin et al., 2016b)</ref> Given an image x, for each pixel color x i,j,c , the l-level thermometer encoding τ (x i,j,c ) is a l-dimensional vector where τ (x i,j,c ) k = 1 if if x i,j,c &gt; k/l, and 0 otherwise (e.g., for a 10-level thermometer encoding, τ (0.66) = 1111110000).</p><p>Due to the discrete nature of thermometer encoded values, it is not possible to directly perform gradient descent on a thermometer encoded neural network. The authors therefore construct Logit-Space Projected Gradient Ascent (LS-PGA) as an attack over the discrete thermometer encoded inputs. Using this attack, the authors perform the adversarial training of <ref type="bibr" target="#b15">Madry et al. (2018)</ref> on thermometer encoded networks.</p><p>On CIFAR-10, just performing thermometer encoding was found to give 50% accuracy within = 0.031 under ∞ distortion. By performing adversarial training with 7 steps of LS-PGA, robustness increased to 80%.</p><p>Discussion. While the intention behind this defense is to break the local linearity of neural networks, we find that this defense in fact causes gradient shattering. This can be observed through their black-box attack evaluation: adversarial examples generated on a standard adversarially trained model transfer to a thermometer encoded model reducing the accuracy to 67%, well below the 80% robustness to the white-box iterative attack.</p><p>Evaluation. We use the BPDA approach from §4.1.2, where we let f (x) = τ (x). Observe that if we define τ (x i,j,c ) k = min(max(x i,j,c − k/l, 0), 1)</p><formula xml:id="formula_2">then τ (x i,j,c ) k = floor(τ (x i,j,c ) k )</formula><p>so we can let g(x) = τ (x) and replace the backwards pass with the function g(•).</p><p>LS-PGA only reduces model accuracy to 50% on a thermometer-encoded model trained without adversarial training (bounded by = 0.031). In contrast, we achieve 1% model accuracy with the lower = 0.015 (and 0% with = 0.031). This shows no measurable improvement from standard models, trained without thermometer encoding.</p><p>When we attack a thermometer-encoded adversarially trained model<ref type="foot" target="#foot_2">4</ref> , we are able to reproduce the 80% accuracy at = 0.031 claim against LS-PGA. However, our attack reduces model accuracy to 30%. This is significantly weaker than the original <ref type="bibr" target="#b15">Madry et al. (2018)</ref> model that does not use thermometer encoding. Because this model is trained against the (comparatively weak) LS-PGA attack, it is unable to adapt to the stronger attack we present above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">INPUT TRANSFORMATIONS</head><p>Defense Details. <ref type="bibr" target="#b9">Guo et al. (2018)</ref> propose five input transformations to counter adversarial examples.</p><p>As a baseline, the authors evaluate image cropping and rescaling, bit-depth reduction, and JPEG compression. Then the authors suggest two new transformations: (a) randomly drop pixels and restore them by performing total variance minimization; and (b) image quilting: reconstruct images by replacing small patches with patches from "clean" images, using minimum graph cuts in overlapping boundary regions to remove edge artifacts.</p><p>The authors explore different combinations of input transformations along with different underlying ImageNet classifiers, including adversarially trained models. They find that input transformations provide protection even with a vanilla classifier.</p><p>Discussion. The authors find that a ResNet-50 classifier provides a varying degree of accuracy for each of the five proposed input transformations under the strongest attack with a normalized 2 dissimilarity of 0.01, with the strongest defenses achieving over 60% top-1 accuracy. We reproduce these results when evaluating an InceptionV3 classifier.</p><p>The authors do not succeed in white-box attacks, crediting lack of access to test-time randomness as "particularly crucial in developing strong defenses" (Guo et al., 2018). 5   Evaluation. It is possible to bypass each defense independently (and ensembles of defenses usually are not much stronger than the strongest sub-component <ref type="bibr" target="#b11">(He et al., 2017)</ref>). We circumvent image cropping and rescaling with a direct application of EOT. To circumvent bit-depth reduction and JPEG compression, we use BPDA and approximate the backward pass with the identity function. To circumvent total variance minimization and image quilting, which are both non-differentiable and randomized, we apply EOT and use BPDA to approximate the gradient through the transformation. With our attack, we achieve 100% targeted attack success rate and accuracy drops to 0% for the strongest defense under the smallest perturbation budget considered in <ref type="bibr" target="#b9">Guo et al. (2018)</ref>, a root-mean-square perturbation of 0.05 (and a "normalized" 2 perturbation as defined in <ref type="bibr" target="#b9">Guo et al. (2018)</ref> of 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">LOCAL INTRINSIC DIMENSIONALITY (LID)</head><p>LID is a general-purpose metric that measures the distance from an input to its neighbors. <ref type="bibr" target="#b14">Ma et al. (2018)</ref> propose using LID to characterize properties of adversarial examples. The authors emphasize that this classifier is not intended as a defense against adversarial examples<ref type="foot" target="#foot_4">6</ref> , however the authors argue that it is a robust method for detecting adversarial examples that is not easy to evade by attempting their own adaptive attack and showing it fails.</p><p>Analysis Overview. Instead of actively attacking the detection method, we find that LID is not able to detect high confidence adversarial examples <ref type="bibr" target="#b4">(Carlini &amp; Wagner, 2017a)</ref>, even in the unrealistic threat model where the adversary is entirely oblivious to the defense and generates adversarial examples on the original classifier. A full discussion of this attack is given in Supplement Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Stochastic Gradients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">STOCHASTIC ACTIVATION PRUNING (SAP)</head><p>Defense Details. SAP <ref type="bibr" target="#b7">(Dhillon et al., 2018)</ref> introduces randomness into the evaluation of a neural network to defend against adversarial examples. SAP randomly drops some neurons of each layer f i to 0 with probability proportional to their absolute value. That is, SAP essentially applies dropout at each layer where instead of dropping with uniform probability, nodes are dropped with a weighted distribution. Values which are retained are scaled up (as is done in dropout) to retain accuracy. Applying SAP decreases clean classification accuracy slightly, with a higher drop probability decreasing accuracy, but increasing robustness. We study various levels of drop probability and find they lead to similar robustness numbers.</p><p>Discussion. The authors only evaluate SAP by taking a single step in the gradient direction <ref type="bibr" target="#b7">(Dhillon et al., 2018)</ref>. While taking a single step in the direction of the gradient can be effective on non-randomized neural networks, when randomization is used, computing the gradient with respect to one sample of the randomness is ineffective.</p><p>Evaluation. To resolve this difficulty, we estimate the gradients by computing the expectation over instantiations of randomness. At each iteration of gradient descent, instead of taking a step in the direction of ∇ x f (x) we move in the direction of k i=1 ∇ x f (x) where each invocation is randomized with SAP. We have found that choosing k = 10 provides useful gradients. We additionally had to resolve a numerical instability when computing gradients: this defense caused computing a backward pass to cause exploding gradients due to division by numbers very close to 0.</p><p>With these approaches, we are able to reduce SAP model accuracy to 9% at = .015, and 0% at = 0.031. If we consider an attack successful only when an example is classified incorrectly 10 times out of 10 (and consider it correctly classified if it is ever classified as the correct label), model accuracy is below 10% with = 0.031.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">MITIGATING THROUGH RANDOMIZATION</head><p>Defense Details. <ref type="bibr" target="#b28">Xie et al. (2018)</ref> propose to defend against adversarial examples by adding a randomization layer before the input to the classifier. For a classifier that takes a 299 × 299 input, the defense first randomly rescales the image to a r × r image, with r ∈ [299, 331), and then randomly zero-pads the image so that the result is 331×331. The output is then fed to the classifier.</p><p>Discussion. The authors consider three attack scenarios: vanilla attack (an attack on the original classifier), singlepattern attack (an attack assuming some fixed randomization pattern), and ensemble-pattern attack (an attack over a small ensemble of fixed randomization patterns). The authors strongest attack reduces InceptionV3 model accuracy to 32.8% top-1 accuracy (over images that were originally classified correctly).</p><p>The authors dismiss a stronger attack over larger choices of randomness, stating that it would be "computationally impossible" (emphasis ours) and that such an attack "may not even converge" <ref type="bibr" target="#b28">(Xie et al., 2018)</ref>.</p><p>Evaluation. We find the authors' ensemble attack overfits to the ensemble with fixed randomization. We bypass this defense by applying EOT, optimizing over the (in this case, discrete) distribution of transformations.</p><p>Using this attack, even if we consider the attack successful only when an example is classified incorrectly 10 times out of 10, we achieve 100% targeted attack success rate and reduce the accuracy of the classifier from 32.8% to 0.0% with a maximum ∞ perturbation of = 0.031.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Vanishing &amp; Exploding Gradients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">PIXELDEFEND</head><p>Defense Details. <ref type="bibr" target="#b24">Song et al. (2018)</ref> propose using a PixelCNN generative model to project a potential adversarial example back onto the data manifold before feeding it into a classifier. The authors argue that adversarial examples mainly lie in the low-probability region of the data distribution. PixelDefend "purifies" adversarially perturbed images prior to classification by using a greedy decoding procedure to approximate finding the highest probability example within an -ball of the input image.</p><p>Discussion. The authors evaluate PixelDefend on CIFAR-10 over various classifiers and perturbation budgets. With a maximum ∞ perturbation of = 0.031, PixelDefend claims 46% accuracy (with a vanilla ResNet classifier). The authors dismiss the possibility of end-to-end attacks on PixelDefend due to the difficulty of differentiating through an unrolled version of PixelDefend due to vanishing gradients and computation cost.</p><p>Evaluation. We sidestep the problem of computing gradients through an unrolled version of PixelDefend by approximating gradients with BPDA, and we successfully mount an end-to-end attack using this technique<ref type="foot" target="#foot_5">7</ref> . With this attack, we can reduce the accuracy of a naturally trained classifier which achieves 95% accuracy to 9% with a maximum ∞ perturbation of = 0.031. We find that combining adversarial training <ref type="bibr" target="#b15">(Madry et al., 2018)</ref> with PixelDefend provides no additional robustness over just using the adversarially trained classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">DEFENSE-GAN</head><p>Defense-GAN <ref type="bibr" target="#b21">(Samangouei et al., 2018)</ref> uses a Generative Adversarial Network <ref type="bibr" target="#b8">(Goodfellow et al., 2014a)</ref> to project samples onto the manifold of the generator before classifying them. That is, the intuition behind this defense is nearly identical to PixelDefend, but using a GAN instead of a PixelCNN. We therefore summarize results here and present the full details in Supplement Section 2.</p><p>Analysis Overview. Defense-GAN is not argued secure on CIFAR-10, so we use MNIST. We find that adversarial examples exist on the manifold defined by the generator. That is, we show that we are able to construct an adversarial example x = G(z) so that x ≈ x but c(x) = c(x ). As such, a perfect projector would not modify this example x because it exists on the manifold described by the generator. However, while this attack would defeat a perfect projector mapping x to its nearest point on G(z), the imperfect gradient descent based approach taken by Defense-GAN does not perfectly preserve points on the manifold. We therefore construct a second attack using BPDA to evade Defense-GAN, although at only a 45% success rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Having demonstrated attacks on these seven defenses, we now take a step back and discuss the method of evaluating a defense against adversarial examples.</p><p>The papers we study use a variety of approaches in evaluating robustness of the proposed defenses. We list what we believe to be the most important points to keep in mind while building and evaluating defenses. Much of what we describe below has been discussed in prior work <ref type="bibr" target="#b4">(Carlini &amp; Wagner, 2017a;</ref><ref type="bibr" target="#b15">Madry et al., 2018)</ref>; we repeat these points here and offer our own perspective for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Define a (realistic) threat model</head><p>A threat model specifies the conditions under which a defense argues security: a precise threat model allows for an exact understanding of the setting under which the defense is meant to work. Prior work has used words including whitebox, grey-box, black-box, and no-box to describe slightly different threat models, often overloading the same word.</p><p>Instead of attempting to, yet again, redefine the vocabulary, we enumerate the various aspects of a defense that might be revealed to the adversary or held secret to the defender: model architecture and model weights; training algorithm and training data; test time randomness (either the values chosen or the distribution); and, if the model weights are held secret, whether query access is allowed (and if so, the type of output, e.g. logits or only the top label).</p><p>While there are some aspects of a defense that might be held secret, threat models should not contain unrealistic constraints. We believe any compelling threat model should at the very least grant knowledge of the model architecture, training algorithm, and allow query access.</p><p>It is not meaningful to restrict the computational power of an adversary artificially (e.g., to fewer than several thousand attack iterations). If two defenses are equally robust but generating adversarial examples on one takes one second and another takes ten seconds, the robustness has not increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Make specific, testable claims</head><p>Specific, testable claims in a clear threat model precisely convey the claimed robustness of a defense. For example, a complete claim might be: "We achieve 90% accuracy when bounded by ∞ distortion with = 0.031, when the attacker has full white-box access."</p><p>In this paper, we study all papers under the threat model the authors define. However, if a paper is evaluated under a different threat model, explicitly stating so makes it clear that the original paper's claims are not being violated.</p><p>A defense being specified completely, with all hyperparameters given, is a prerequisite for claims to be testable. Releasing source code and a pre-trained model along with the paper describing a specific threat model and robustness claims is perhaps the most useful method of making testable claims. At the time of writing this paper, four of the defenses we study made complete source code available <ref type="bibr" target="#b15">(Madry et al., 2018;</ref><ref type="bibr" target="#b14">Ma et al., 2018;</ref><ref type="bibr" target="#b9">Guo et al., 2018;</ref><ref type="bibr" target="#b28">Xie et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Evaluate against adaptive attacks</head><p>A strong defense is robust not only against existing attacks, but also against future attacks within the specified threat model. A necessary component of any defense proposal is therefore an attempt at an adaptive attack.</p><p>An adaptive attack is one that is constructed after a defense has been completely specified, where the adversary takes advantage of knowledge of the defense and is only restricted by the threat model. One useful attack approach is to perform many attacks and report the mean over the best attack per image. That is, for a set of attacks a ∈ A instead of reporting the value min If a defense is modified after an evaluation, an adaptive attack is one that considers knowledge of the new defense. In this way, concluding an evaluation with a final adaptive attack can be seen as analogous to evaluating a model on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Constructing defenses to adversarial examples requires defending against not only existing attacks but also future attacks that may be developed. In this paper, we identify obfuscated gradients, a phenomenon exhibited by certain defenses that makes standard gradient-based methods fail to generate adversarial examples. We develop three attack techniques to bypass three different types of obfuscated gradients. To evaluate the applicability of our techniques, we use the ICLR 2018 defenses as a case study, circumventing seven of nine accepted defenses.</p><p>More generally, we hope that future work will be able to avoid relying on obfuscated gradients (and other methods that only prevent gradient descent-based attacks) for perceived robustness, and use our evaluation approach to detect when this occurs. Defending against adversarial examples is an important area of research and we believe performing a careful, thorough evaluation is a critical step that can not be overlooked when designing defenses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>summarizes our results. Of the 9 accepted papers,</cell></row><row><cell>7 rely on obfuscated gradients. Two of these defenses</cell></row><row><cell>argue robustness on ImageNet, a much harder task than</cell></row><row><cell>CIFAR-10; and one argues robustness on MNIST, a much</cell></row><row><cell>easier task than CIFAR-10. As such, comparing defenses</cell></row><row><cell>across datasets is difficult.</cell></row><row><cell>5.1. Non-obfuscated Gradients</cell></row><row><cell>5.1.1. ADVERSARIAL TRAINING</cell></row><row><cell>Defense Details. Originally proposed by Goodfellow</cell></row><row><cell>et al. (2014b), adversarial training solves a min-max game</cell></row><row><cell>through a conceptually simple process: train on adversarial</cell></row><row><cell>examples until the model learns to classify them correctly.</cell></row></table><note>Given training data X and loss function (•), standard training chooses network weights θ as θ * = arg min θ E (x,y)∈X (x; y; F θ ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Summary of Results: Seven of nine defense techniques accepted at ICLR 2018 cause obfuscated gradients and are vulnerable to our attacks. Defenses denoted with * propose combining adversarial training; we report here the defense alone, see §5 for full numbers. The fundamental principle behind the defense denoted with * * has 0% accuracy; in practice, imperfections cause the theoretically optimal attack to fail, see §5.4.2 for details.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Discussion. Again, as above, we are unable to reduce the claims made by the authors. However, these claims are weaker than other defenses (because the authors correctly performed a strong optimization-based attack<ref type="bibr" target="#b6">(Carlini &amp; Wagner, 2017c)</ref>): 16% accuracy with = .015, compared to over 70% at the same perturbation budget with adversarial training as in<ref type="bibr" target="#b15">Madry et al. (2018)</ref>.</figDesc><table><row><cell>5.2. Gradient Shattering</cell></row><row><cell>5.2.1. THERMOMETER ENCODING</cell></row><row><cell>Defense Details. In contrast to prior work (Szegedy et al.,</cell></row><row><cell>2013) which viewed adversarial examples as "blind spots"</cell></row><row><cell>in neural networks, Goodfellow et al. (2014b) argue that the</cell></row><row><cell>reason adversarial examples exist is that neural networks be-</cell></row><row><cell>have in a largely linear manner. The purpose of thermometer</cell></row><row><cell>encoding is to break this linearity.</cell></row><row><cell>,</cell></row><row><cell>and (2) training exclusively on ∞ adversarial examples</cell></row><row><cell>provides only limited robustness to adversarial examples</cell></row><row><cell>under other distortion metrics (Sharma &amp; Chen, 2017).</cell></row><row><cell>5.1.2. CASCADE ADVERSARIAL TRAINING</cell></row><row><cell>Cascade adversarial machine learning (Na et al., 2018) is</cell></row><row><cell>closely related to the above defense. The main difference</cell></row><row><cell>is that instead of using iterative methods to generate ad-</cell></row><row><cell>versarial examples at each mini-batch, the authors train a</cell></row><row><cell>first model, generate adversarial examples (with iterative</cell></row></table><note>methods) on that model, add these to the training set, and then train a second model on the augmented dataset only single-step methods for efficiency. Additionally, the authors construct a "unified embedding" and enforce that the clean and adversarial logits are close under some metric.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Misclassification is a less meaningful metric on ImageNet, where a misclassification of closely related classes (e.g., a German shepherd classified as a Doberman) may not be meaningful.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">The BPDA approach can be used on an arbitrary network, even if it is already differentiable, to obtain a more useful gradient.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">That is, a thermometer encoded model that is trained using the approach of<ref type="bibr" target="#b15">(Madry et al., 2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">This defense may be stronger in a threat model where the adversary does not have complete information about the exact quilting process used (personal communication with authors).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">Personal communication with authors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">In place of a PixelCNN, due to the availability of a pre-trained model, we use a PixelCNN++<ref type="bibr" target="#b20">(Salimans et al., 2017)</ref> and discretize the mixture of logistics to produce a 256-way softmax.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to Aleksander Madry, Andrew Ilyas, and Aditi Raghunathan for helpful comments on an early draft of this paper. We thank Bo Li, Xingjun Ma, Laurens van der Maaten, Aurko Roy, Yang Song, and Cihang Xie for useful discussion and insights on their defenses. This work was partially supported by the National Science Foundation through award CNS-1514457, Qualcomm, and the Hewlett Foundation through the Center for Long-Term Cybersecurity.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07397</idno>
		<title level="m">Synthesizing robust adversarial examples</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">S18S</biblScope>
		</imprint>
	</monogr>
	<note>accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017a</date>
			<publisher>AISec</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Magnet and &quot;efficient defenses against adversarial attacks&quot; are not robust to adversarial examples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08478</idno>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security &amp; Privacy</title>
				<imprint>
			<date type="published" when="2017">2017c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1uR4GZRZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<editor>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014a. 2014b</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Explaining and harnessing adversarial examples</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SyJ7ClWCb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04701</idno>
		<title level="m">Adversarial example defenses: Ensembles of weak defenses are not strong</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1gJ1L2aW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted as oral presentation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJzIBfZAb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cascade adversarial machine learning regularized with a unified embedding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyRVBzap-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<idno type="DOI">10.1145/3052973.3053009</idno>
		<ptr target="http://doi.acm.org/10.1145/3052973.3053009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, ASIA CCS &apos;17</title>
				<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security, ASIA CCS &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bys4ob-Rb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A pixelcnn implementation with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Pixelcnn++</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Defensegan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BkJ3ibb0-.ac-ceptedasposter" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Attacking the madry defense model with L 1 -based adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10733</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Certifiable distributional robustness with principled adversarial training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hk6kPgZA-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><surname>Pixeldefend</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJUYGxbCW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkZvSe-RZ.ac-ceptedasposter" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sk9yuql0Z" />
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
