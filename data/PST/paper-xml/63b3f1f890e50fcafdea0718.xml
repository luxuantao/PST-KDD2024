<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation of the Intel Thread Director technology on an Alder Lake processor</title>
				<funder>
					<orgName type="full">EU (FEDER)</orgName>
				</funder>
				<funder ref="#_huGkmX4">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_xTvFHud">
					<orgName type="full">Spanish MINECO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Juan</forename><surname>Carlos Saez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facultad de Inform?tica</orgName>
								<orgName type="institution">Universidad Complutense de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Prieto-Matias</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facultad de Inform?tica</orgName>
								<orgName type="institution">Universidad Complutense de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation of the Intel Thread Director technology on an Alder Lake processor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3546591.3547532</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Asymmetric multicore processors</term>
					<term>Hybrid processors</term>
					<term>Scheduling</term>
					<term>Operating Systems</term>
					<term>Linux kernel</term>
					<term>Alder Lake</term>
					<term>Thread Director</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Asymmetric multicore processors (AMPs) combine high-performance big cores with more energy-efficient small cores, all exposing a shared instruction-set architecture but different features, such as clock frequency or microarchitecture. In the last decade, most commercial AMP products have mainly targetted the embedded and mobile domains. Today, major hardware players are releasing new AMP-based products that aim to move beyond the mobile niche, towards the desktop/server segments. The Apple M1 SoC or the recent Intel Alder Lake processor family are clear examples of these new AMP systems. Despite their energy-efficiency benefits, AMPs pose significant challenges to the operating system scheduler.</p><p>In this paper, we assess the effectiveness of the Thread Director (TD) technology, a set of hardware facillities -first introduced in Alder Lake processors-that provide the OS with hints on the performance and energy efficiency that a thread delivers when running on the various core types. The main focus of our analysis is to evaluate how effectively the OS can drive scheduling decisions with TD's performance hints. To this end, we incorporated support in Linux to conveniently access TD facillites from the OS kernel.</p><p>Motivated by various TD's limitations identified with our analysis, we opted to build hardware-counter based prediction models (generated via machine-learning methods) to better aid the OS in making throughput-oriented and fairness-aware scheduling decisions. The effectiveness of both TD and the hardware-counter based models for performance prediction is evaluated both via offline monitoring, and also online, by utilizing our implementation of various asymmetry-aware schedulers in the Linux kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Software and its engineering ? Scheduling; ? Computer systems organization ? Multicore architectures; Heterogeneous (hybrid) systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Asymmetric multicore processors (AMPs), which integrate highperformance big cores and power-efficient small cores, are capable to deliver higher performance per watt than symmetric multicores for diverse workloads <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b33">35]</ref>. The general-purpose nature of the various cores, coupled with their shared ISA (instruction set architecture) allows the execution of unmodified (legacy) programs, making AMPs an attractive heterogeneous architecture <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b19">21]</ref>.</p><p>In the last decade, the main commercial niche of AMPs was in the embedded and mobile domains. The clear role of big and small cores in this context coupled with the surgent need for extended battery life led to the widespread utilization of AMP processors, such as the ARM big.LITTLE <ref type="bibr" target="#b1">[3]</ref>. Specifically, in mobile workloads, big cores are predominantly used for running foreground and latencysensitive tasks, while background tasks could be mapped to powerefficient small cores. More recently, major hardware players are releasing new products that bring AMP processors to the desktop segment; this is the case of the Apple M1 SoC <ref type="bibr">[2]</ref> or the Intel Alder Lake processor family <ref type="bibr" target="#b0">[1]</ref>. Despite the potential of AMPs, transparently delivering their benefits to unmodified applications poses big challenges to the system software, and in particular to the OS scheduler, which has to effectively distribute big and small-core cycles among the various threads in the workload <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b26">28]</ref>.</p><p>Previous research has demonstrated that to optimize key system metrics, such as throughput, fairness or energy efficiency, the scheduler must factor in the performance benefit that each thread in the workload derives when it runs on a big core, relative to a small one <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b34">36]</ref>. Henceforth we will refer to this relative benefit as the thread's Speedup Factor (SF). Obtaining an accurate prediction for threads' SFs on-line is generally a challenging task <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b25">27]</ref>, and the fact that the SF may vary over time for a thread across different program phases requires the underlying prediction method to be efficient enough for its practical utilization from the OS <ref type="bibr" target="#b9">[11]</ref>.</p><p>In this paper we evaluate the effectiveness of Thread Director (TD), a set of hardware facillities introduced in Intel Alder Lake processors to aid the OS in making thread scheduling decisions on AMPs <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16]</ref>. Among other things, TD enables the OS to seamlessly obtain an estimate for the SF of any thread online. Windows 11 is the first operating system that leverages TD features from the process scheduler. Unfortunately, the associated kernel-level (b) Fair-sharing big cores Figure <ref type="figure">1</ref>: Distribution of per-application slowdown across 10 runs of the same workload under (a) the default Linux scheduler, and (b) an asymmetry-aware round-robin scheduler, which we implemented in Linux.</p><p>implementation remains proprietary, making our study in this OS impractical. In this work, we conduct our analysis in the Linux kernel. We should highlight that Linux default scheduler -the Completely Fair Scheduler (CFS)-does not currently leverage TD and is still largely asymmetry unaware <ref type="bibr" target="#b9">[11]</ref>. The latest scheduling-related changes to improve performance on Alder Lake processors (introduced in Linux v5. <ref type="bibr" target="#b14">16</ref>) make the load balancer populate idle big cores first, maximizing its utilization. However, despite these changes, CFS still provides highly variable completion times of an application across different runs of the same workload. To illustrate this fact, we measured the completion time of 16 single-threaded SPEC CPU applications, when running together on a 16-core Intel Alder Lake processor. Fig. <ref type="figure">1a</ref> plots the distribution of slowdowns (i.e., performance degradation vs. isolated big-core execution ) for each application observed across 10 runs of the multi-program workload. As it is evident, an application's completion time may increase by up to 2.3x relative to the fastest run, leading to highly variable system throughput and uneven degree of fairness across executions. This high variability -also present in the latest stable version of Linux (v5.17)-makes CFS misleading when considered as a baseline for experimental analyses on AMPs <ref type="bibr" target="#b9">[11]</ref>. The behavior of CFS stands in contrast with the more consistent completion times provided by an asymmetry-aware scheduler that equally-shares big-core cycles among applications, as the results of Fig. <ref type="figure">1b</ref> reveal.</p><p>The major contributions of this paper are as follows:</p><p>? We implement the necessary support in Linux to access TDprovided information from the kernel and user space. ? We conduct an offline analysis to assess the degree of accuracy of SF estimations provided by TD over time for a diverse set of compute-intensive programs. Motivated by various TD's limitations, and for comparison purposes, we also opted to build performance-counter based prediction models generated via machine learning. ? We created kernel-level implementations for several existing asymmetry-aware scheduling algorithms <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b25">27]</ref> that leverage threads' SFs to optimize different system metrics.</p><p>? To analyze the impact of the different methods for SF estimation, we carried out an experimental study using a wide range of multi-program workloads running on an Intel Alder Lake processor under various asymmetry-aware schedulers.</p><p>The remainder of the paper is organized as follows. Section 2 discusses the closest related works. Section 3 describes Intel Thread Director, and presents our analysis on the accuracy of SF estimation. Section 4 outlines the implementation of the various asymmetryaware algorithms considered. Section 5 covers the experimental evaluation, and Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our main goal is to analyze how accurate SF predictions provided by Thread Director are, and how effectively the OS can drive scheduling decisions with these predictions. Existing methods to determine threads' SFs at run time can be grouped in three categories: direct measurement, utilization of performance-counter based prediction models, and reliance on specific hardware support for SF estimation.</p><p>Direct measurement, also referred to as IPC sampling <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b30">32]</ref>, comes down to measuring the number of instructions per cycle (IPC) of the thread on both core types with performance monitoring counters (PMCs), and then approximating the SF with the IPC ratio, while factoring in the cores' frequencies. Despite the simplicity of this approach, it usually incurs higher overhead than the other techniques, due to the additional thread migrations across core types required for gathering IPC values <ref type="bibr" target="#b25">[27]</ref>. More importantly, this technique is known to provide inaccurate SF predictions due to (1) the utilization of IPC values from potentially different program phases to build an SF estimate <ref type="bibr" target="#b28">[30]</ref>, and (2) the fact that the IPC may suffer frequent oscillations, even within the same program phase, due to shared-resource contention effects <ref type="bibr" target="#b9">[11]</ref>.</p><p>The second approach consists in gathering different runtime metrics (IPC, cache miss rate, etc.) with PMCs as the thread runs on the current core type, and then feeding a prediction model with these metrics to obtain SF estimates <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b28">30]</ref>. This technique removes the need for thread migrations to read PMCs, but requires building two platform-specific models: one for prediction of the SF from the big core, and another for SF prediction from the small core. Different machine-learning methods have been explored to aid in the generation of SF estimation models <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b25">27]</ref>. In this work, we opted to utilize the methodology proposed in our earlier work <ref type="bibr" target="#b25">[27]</ref>, which -to the best of our knowledge-is the only one whose associated models were evaluated and implemented in a kernel-level scheduler for AMPs, like the ones we used.</p><p>Lastly, hardware-based mechanisms specifically designed for SF prediction have been also proposed <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b29">31]</ref>. Intel Thread Director constitutes a clear example of this type of hardware support, and it is the first one of its kind that has been adopted in comercial AMP processors. Unlike Windows 11's, the Linux kernel does not currently feature support for TD. A recent kernel patch from Intel <ref type="bibr" target="#b21">[23]</ref> that provides access to the hardware feedback interface (HFI) <ref type="bibr" target="#b13">[15]</ref> -a simpler version of TD-will be included in the next stable version of Linux (v5.18). However the purpose of this patch is to expose HFI-provided information only to user space. Our goal, however, is to leverage this information directly from asymmetry-aware schedulers, so as to perform thread-to-core mappings transparently by the OS kernel, without any kind of user intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PREDICTING THE SPEEDUP FACTOR</head><p>In this section we begin by describing how Intel Thread Director works, and by conducting an experimental analysis on its SF prediction accuracy. Lastly, we outline the mechanism we used to build SF prediction models via machine learning, and assess the accuracy of the models obtained for our Alder Lake platform.</p><p>A) Intel Thread Director (TD). TD is a set of hardware facillities enabling to guide the OS in making thread scheduling decisions on Intel hybrid multicores <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16]</ref>. The TD hardware interface -only directly accessible from the OS kernel-consists of a memoryresident table maintained by the hardware, and a set of per-core and socket-wise registers. Henceforth, we will refer to the memoryresident table as the TD table. As a thread runs on a given logical processor (LP) the hardware provides the OS with feedback on the thread's execution characteristics. To obtain feedback, the OS must periodically read the IA32_THREAD_FEEDBACK_CHAR register, which reports the class ID (index) of the thread running on the current LP. Scheduling-relevant performance and energy-efficiency values associated with the thread's execution can be retrieved by accessing entries of the TD table associated with that class ID.</p><p>In the 16-core Alder Lake processor used in our experiments (Intel Core i9-12900K), TD supports 4 class IDs (0-3), and provides two feedback values (aka capabilities) for each class and LP: performance (??) and energy efficiency (? ? ). ?? and ? ? values range between 0 and 255. As described in detail in <ref type="bibr" target="#b13">[15]</ref> the TD table consists of a global header -made up of a timestamp and several flags that signal recent table changes-, and a set of feedback entries for various LPs that contain the actual feedback values for each class ID. In our platform, the TD table features ten 8-byte feedback entries (1 byte for each capability and class). While each (big) P-core features a separate entry -first eight entries-, a shared feedback entry exists for each group consisting of 4 (small) E-cores -last two entries.</p><p>This work focuses on analyzing the effects of throughput and fairness-oriented scheduling decisions made by catering to the SF of the various threads. With Thread Director, an SF estimate can be obtained by dividing the values of the ?? capabilities for the thread's current class ID (as reported by the hardware) stored in P-core and E-core entries of the TD table. So for example, let us consider that, while a thread runs on a P-core, its current class ID is 1. On our system, the ?? values for this class ID on a P-core and on E-core are 77 and 39 respectively, so the estimated SF is 1.97.</p><p>We should highlight that reading the IA32_THREAD_FEEDBACK _CHAR register does not always provide a usable class ID, as the hardware may not always have enough telemetry information to determine it <ref type="bibr" target="#b13">[15]</ref>. In particular, the lower byte of the register, which reports the class ID, is only considered valid when bit 63 of the same register (valid bit) is 1. When the valid bit is 0, the retrieved class ID should be discarded, and then the OS must use the last valid class ID obtained for its scheduling decisions <ref type="bibr" target="#b13">[15]</ref>. A major limitation of the hardware TD implementation in our platform, is that while reading the IA32_THREAD_FEEDBACK_CHAR register reports a valid class ID most of the time for the current thread on P-cores, an invalid class is always obtained on E-cores. This is the case for all the programs we used (including all SPEC CPU2006 and CPU2017 benchmarks). Being unable to obtain a direct TD estimate for the thread's SF from a E-core poses a challenge for the implementation of TD-based asymmetry-aware schedulers, as discussed in Sec 4.</p><p>To evaluate the accuracy of the SF prediction provided by Thread Director, we ran all the benchmarks in the SPEC CPU2006 and CPU2017 suites on both core types for 300B instructions, and compared the actual SF values observed over time with the prediction obtained with TD (P-cores only). To determine the actual SF over time we monitored the IPC with PMCs during the execution of each benchmark on a E-core and a P-core in isolation, and gathered the IPC every 500M retired instructions. The SF for a certain instruction window is calculated with the observed IPC on each core type for that specific window, and by factoring in each core's frequency. The TD-prediction used for the comparison is the average of the SF estimates gathered with during every tick (4ms in our platform) that falls within the current instruction window. Reading TD information at this rate provides negligible overhead <ref type="bibr" target="#b5">[7]</ref>.</p><p>Fig. <ref type="figure">2</ref> shows the actual and TD-predicted SF values over time for 20 representative benchmarks, which cover a wide spectrum of profiles regarding prediction accuracy and microarchitectural behavior. TD's implementation in our platform provides a fixed SF estimate for each class: 1.67 for Class 0, 1.97 for Class 1, 2.62 for Class 2, and 1.31 for Class 3. However, 99.9% of the TD class readings in our experiments resulted in Class 0 or 1, so the predicted SF values reported in the figures are mostly 1.67 or 1.97. In fact, Class 3 was never reported for any program, and Class 2 was assigned only to a handful of samples of the milc program. This range of SF estimates allows TD to get close to the actual average SF for a few benchmarks (see Figs. <ref type="figure">2(a)-(f)</ref>), but clearly overpredicts the SF of many other programs (see Figs. <ref type="figure">2(g)-(i)</ref>). Notably, for programs like deepsjeng, bzip2, sjeng and exchange -which exhibit a CPUintensive execution profile but incur a high number of mispredicted branches per 1K instructions-TD tends to overestimate the SF. The results also reveal that TD may also underpredict the SF for some programs (Figs. <ref type="figure">2(m)-(p)</ref>), and even obtain predictions that greatly differ from the actual values (Figs. 2(q)-(t)). All in all, TD provides a mean absolute error of 0.38 in the SF prediction across all SPEC CPU benchmarks, and a very low correlation coefficient (&lt;0.1) is observed when comparing predictions and actual values.</p><p>B) PMC-based SF models for Intel Alder Lake. The inaccuracies observed in TD-based SF predictions for some programs, motivated us to build PMC-based prediction models by leveraging machine learning techniques. To this end, we used the Phase-SF methodology <ref type="bibr" target="#b25">[27]</ref>, which was proven succesful in building prediction models for the Intel QuickIA prototype system. Phase-SF consists of the following steps. First, a representative set of singlethreaded applications (?), and a diverse collection of performance metrics (PM) for comprehensive microarchitectural characterization of programs on both core types must be selected. Second, all applications in ? must be run on big and small cores in isolation so as to enable the gathering of all performance metrics in PM over time with PMCs. Notably, PMC values must be collected and reset every time a fixed-size instruction window completes, so as to later match PMC samples for the same program's instruction window gathered on different core types. Third, after merging the per-program PMC data obtained on both core types, and calculating the value of the various performance metrics and the SF for each instruction window, these merged execution profiles are broken down into a set of coarse-grained SF phases using an offline method. Fourth, a summary file is generated for each program based on the previous offline analysis; this file consists of a set of tuples -one for each SF phase-, each including the average of the SF and that of the values for each metric in PM for the PMC samples that belong to the same program phase. Finally, the data of the summary files for all programs is used as input to the additive-regression engine provided by the WEKA machine-learning tool <ref type="bibr" target="#b11">[13]</ref>. This results in the generation of two prediction models, enabling SF estimation from the big and from the small core, respectively.   Table <ref type="table">1</ref> enumerates the set of performance metrics that the final estimation models for the big and the small core depend upon, which constitute only a subset of all the metrics we gathered offline. Note that the machine-learning method we used automatically assigns low additive-regression coefficients to less relevant performance metrics in the models, so irrelevant metrics could be automatically discarded from the models <ref type="bibr" target="#b25">[27]</ref>. Notably, most of the metrics identified as relevant by the machine-learning engine for SF prediction on the big core are based on the TMA (Top-Down Microarchitecture Analysis) event type, which were recently introduced by Intel to aid in the fine-grained identification of application performance bottlenecks <ref type="bibr" target="#b32">[34]</ref>. As a new feature of Intel Alder Lake processors, these TMA metrics can be monitored altogether on P-cores by using a single PMC <ref type="bibr" target="#b13">[15]</ref>. In any case, the amount of required PMC events for both estimation models do not exceed the number of physical PMCs available, which greatly simplifies the implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance metrics</head><p>Small core Big core Instructions retired per cycle (IPC) ? L3 cache accesses per 1K retired instr. ? Mispredicted branches per 1K retired instr. ? Fraction of pipeline (TMA) slots where no micro-ops are being issued due to lack of back-end resources. ? Table <ref type="table">1</ref>: Metrics used for SF prediction on an Intel Core i9-12900K processor.</p><p>Fig. <ref type="figure" target="#fig_1">3</ref> shows PMC-based SF predictions on the big core for the same benchmarks shown in Fig. <ref type="figure">2</ref>. The corresponding predictions on the small core, which show slightly better accuracy, were omitted due to space constraints. Although the PMC-based models still lead to important SF mispredictions for some programs (see Figs. <ref type="figure" target="#fig_1">3(i),</ref><ref type="figure">(j</ref>) or (q)), they enable us to get closer to the average SF of the various applications. The correlation coefficients (and mean absolute error) for the estimation on the big and the small core across all 54 SPEC benchmarks are 0.8 (0.19) and 0.84 (0.13), respectively. Note that in generating the models, we used performance data from 150 SF phases corresponding to 15 SPEC programs with a wide range of behaviors regarding branch prediction, memory intensity and cache reuse. In our experimental evaluation of Sec. 5 we use additional applications different from those used to build these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ASYMMETRY-AWARE SCHEDULERS</head><p>For our experimental analysis we considered three previouslyproposed scheduling algorithms: an Asymmetry-Aware Round Robin policy (AARR), Throughput-Optimized scheduling (TO), and the ACFS algorithm, which strives to optimize fairness.</p><p>AARR <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b30">32]</ref> equally shares big-core cycles among threads. To this end, it keeps track of the amount of clock ticks each thread has consumed on a big core, and triggers thread migrations every so often to ensure an even distribution of big core cycles among threads. Specifically, two threads running on different core types are swapped when RR detects that the difference between the cycles consumed on a big core by both threads exceeds a certain threshold.</p><p>TO <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b30">32]</ref> optimizes throughput by using big cores to run those threads that currently exhibit the highest SF values.</p><p>ACFS <ref type="bibr" target="#b25">[27]</ref> was proposed to improve the degree of fairness provided by AARR, while delivering better system throughput. Its rationale is to even out the progress made by the various threads by granting a greater fraction of big-core cycles to threads whose performance is more likely to be highly degraded when mapped to a small core. To this end, ACFS assigns each thread an amp_vruntime counter, which tracks its progress on the AMP throughout the execution. This counter is incremented every tick consumed by the thread on a big or a small core, and the increment applied reflects both the current core's performance, and the thread's current SF <ref type="bibr" target="#b25">[27]</ref>. Because threads mapped on small cores tend to make slower progress than big-core threads, ACFS evens out the progress by swaping threads running on opposite core types when the difference between the associated amp_vruntime counters is greater than a certain threshold. We should highlight that in our experiments we did not consider other recently proposed fairness-aware schedulers <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b27">29]</ref>, as they rely on specific assumptions on the cache hierarchy that do not hold on Alder Lake processors.</p><p>To develop the various schedulers in the Linux kernel v5.16, we used the PMCSched framework <ref type="bibr" target="#b3">[5]</ref>, implemented on top of the PM-CTrack open-source monitoring tool <ref type="bibr" target="#b24">[26]</ref>. This novel scheduling framework makes it possible to implement asymmetry-aware scheduling algorithms in a kernel module that can be loaded in vanilla (unpatched) kernels with standard tracing support enabled <ref type="bibr" target="#b3">[5]</ref>. To evaluate how sensitive the TO and ACFS algorithms are to the choice of the underlying method of SF prediction, we added support in our framework for three SF-estimation methods, referred to as Thread Director (TD) based, Model based, and Big Model based.</p><p>The TD-based method relies on the threads' SF estimates provided by Intel Thread Director, which -in the Alder Lake processor we used-are available directly on big cores only (as stated in Sec. 3A). We observed that triggering periodic migrations from small to big cores so as to obtain up-to-date SF values for threads assigned to small cores, leads to the same program-phase related mispredictions issues of IPC sampling (see Sec. 2). To address these issues -and inspired by previous works <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b31">33]</ref>-our TD-based method uses a per-thread history table to aid in SF prediction from small cores. A thread's history table -empty when the thread enters the system-stores SF values corresponding to past execution phases obtained with TD on a big core. As in <ref type="bibr" target="#b9">[11]</ref>, we represent each phase in the table by means of a pair of control metrics gathered on-line: the number of L1 cache accesses per 1K instructions, and the fraction of branch instructions retired. Good properties of these metrics are that they do not vary significantly across core types for a specific phase, and remain stable even if shared-resource contention exists <ref type="bibr" target="#b9">[11]</ref>. The scheduler accesses the history table as soon as new values of the control metrics are obtained with PMCs. If the thread is currently running on a big core, the information for the current phase is simply updated with the latest average SF obtained with TD by using the procedure described in Sec. 3A. When the thread runs on a small core, the table is accessed to retrieve an SF prediction. If no information is found for the current phase (i.e., the pair of control metrics is not close enough to any of those registered in the table) the estimated SF used by the scheduler is the average across SF samples in the history table. Notably, when the phase miss rate is too high the scheduler forces a migration of the thread onto a big core, thus making it possible to refresh the history table with recent SF estimates. To prevent threads requiring frequent history-table refresh operations from monopolizing big cores, their migration rate is throttled using the mechanism proposed in <ref type="bibr" target="#b9">[11]</ref>.</p><p>The Model-based method leverages the PMC-based SF models presented in Sec. 3B, which were built using machine learning. When the Model-based method is enabled, the OS continuously gathers the PMC metrics that the prediction model of the current core type depends upon (Table <ref type="table">1</ref>), and obtains the SF prediction by using the metric values as input to the model's inference function. Lastly, the Big-Model based method was exclusively created to conduct a fairer comparison with the TD-based method under similar restrictions: SF estimates are only available directly on the big core, and SF predictions on the small core are obtained indirectly based on a history table. Note that the history table here stores SF estimates provided by the PMC-based big-core model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>For the evaluation we used a 16-core platform featuring an Intel Core i9-12900K processor and 32GB DDR4 SDRAM. The processor integrates 8 "Golden Cove" big (P) cores, and 8 "Gracemont" small (E) cores, all sharing a 30MiB L3 LLC. Each P-core has a private 1.25MiB L2 cache. E-cores are grouped into two 4-core clusters; cores in each cluster share a 2MiB L2 cache (L1 is private to each core). To assess the effectiveness of Thread Director in aiding the OS to improve throughput and fairness, we experimented with the AARR, TO and ACFS scheduling algorithms, presented in Sec. 4.</p><p>For our experiments we ramdomly built 22 diverse workloads using 47 SPEC CPU programs. The total thread count in each computeintensive workload was set to match the total number of cores in the AMP, as done in <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b34">36]</ref>. The composition of the various program mixes is depicted in Fig. <ref type="figure">4</ref>. The first 10 mixes (W1-W10) consist of programs that cover a wide range of average SF values. The remaining workloads include mostly medium-SF and high-SF programs (W11-W16), or predominantly low-SF and medium-SF applications (W17-W22). In running the workloads, we follow a similar methodology to that of previous works <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b28">30]</ref>. All applications in the mix are started simultaneously, and when one of them completes, the program is restarted repeatedly until the slowest application in the set completes three times. We use the geometric mean of the completion times for each program to calculate the degree of fairness and throughput, by using the Unfairness <ref type="bibr" target="#b6">[8]</ref><ref type="bibr" target="#b7">[9]</ref><ref type="bibr" target="#b8">[10]</ref><ref type="bibr" target="#b31">33]</ref> and Aggregate Speedup <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b25">27]</ref> metrics, respectively. Fig. <ref type="figure">5</ref> shows the degree of throughput (the higher the better) and unfairness (the lower the better) delivered by the different scheduling algorithms normalized to AARR. The suffix in parentheses in the names of the different variants of TO and ACFS denotes the underlying method used to determine SFs at run time: "TD" -Thread Director (TD) based-, "B" -Big-Model based method-, and "M" for the Model-based method. All in all, the inaccuracies in the TD-based SF prediction do not enable TO and ACFS to accomplish their goals. Specifically, TO(TD) degrades throughput vs. AARR by 1% on average, and provides only a 9.3% maximum throughput improvement (W16). Similarly, ACFS(TD) increases AARR's average unfairness figures by 5%. By contrast, when TO uses the PMC-based estimation models on both core types (M variant), substantial throughput improvements are obtained over the baseline (by up to 29.2%, and by 20.6% on average). These estimation models are also effective for ACFS(M), which achieves up to a 15.8% unfairness reduction (W6), while reaping non-negligible throughput improvements over AARR (10.4% on average). These results come from the higher SFprediction accuracy provided by the Model-based method, which allows schedulers to better identify actual high-SF programs, and, in turn, to run them on big cores for longer time periods than the other threads. In addition, the figures of TO(M) and ACFS(M) also underscore that minimizing unfairness and maximizing throughput are often conflicting optimization objectives on AMPs <ref type="bibr" target="#b25">[27]</ref>.</p><p>Lastly, we observe that the "B" method is in general more effective than TD, especially when it comes to assisting the TO scheduler; in fact, TO(B) improves throughput by 7,43% on average vs. TO(TD). This observation suggests that the utilization of the history table to retrieve past-SF predictions is effective as long as big-core estimates are minimally accurate, such as the ones we provided by our PMC-based model. However, the modest fairness improvements of ACFS in its different variants also indicate that enforcing fairness requires SF prediction models with a higher degree of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we have performed an experimental analysis to determine the accuracy of big-to-small speedup (or SF) predictions provided by Thread Director (TD), and also assessed how effectively the OS utilizes these predictions to make scheduling decisions on an Intel Alder Lake processor. For comparison purposes, we also built SF-prediction models for this processor based on performance monitoring counters (PMCs). To this end, we use a methodology that exploits machine-learning methods <ref type="bibr" target="#b25">[27]</ref>. To carry out our evaluation we implemented the necessary support for TD in the Linux kernel, to allow different kernel-level scheduling algorithms <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b25">27]</ref> to access TD-provided SF estimates. Our experiments reveal that the PMC-based models provide better SF-estimation accuracy than TD, and are especially well suited to aid the OS in optimizing throughput. As for future work, we plan on exploiting the hardware cachepartitioning support present in the L2 cache shared by E-cores in Intel Alder Lake to conduct scheduling optimizations. Another interesting research avenue would be the experimentation with future TD-enabled processors that allow the direct and independent gathering of SF predictions from any core type.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Observed SF values vs SF predictions over time provided by the big-core PMC-based estimation model for various SPEC CPU programs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>Fraction of pipeline (TMA) slots wasted due to lack of core front-end resources ? ? Fraction of pipeline (TMA) slots wasted due to incorrect speculation ? Fraction of pipeline (TMA) slots wasted due to branch mispredictions ? Fraction of pipeline (TMA) slots wasted due to misses in the instruction cache ? Ratio of the number of cache lines brought into the L2 and into the L3 (cache reuse indicator proposed in [14])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Multiprogram workloads used for our experiments. A blank cell indicates that the associated program is not included in the workload. Applications whose average SF is lower than 1.7 are considered low-SF programs in our platform, and those with an SF value greater than 2.05 are classified as high-SF. The remaining programs are labeled as medium-SF.</figDesc><graphic url="image-1.png" coords="6,66.04,96.93,474.48,106.52" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported by the <rs type="funder">EU (FEDER)</rs>, the <rs type="funder">Spanish MINECO</rs> and CM, under grants <rs type="grantNumber">RTI2018-093684-B-I00</rs> and <rs type="grantNumber">S2018/TCS-4423</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xTvFHud">
					<idno type="grant-number">RTI2018-093684-B-I00</idno>
				</org>
				<org type="funding" xml:id="_huGkmX4">
					<idno type="grant-number">S2018/TCS-4423</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Alder Lake Extends Battery Life</title>
		<author>
			<persName><forename type="first">Jani</forename><surname>Aakash</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-09">2021. Sept. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="http://www.arm.com/files/downloads/Benefits_of_the_big.LITTLE_architecture.pdf" />
		<title level="m">Benefits of the big.LITTLE Architecture</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2015" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Appropriate allocation of workloads on performance asymmetric multicore architectures via deep learning algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gomatheeshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Selvakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocessors and Microsystems</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102996</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rapid development of OS support with PMCSched for scheduling on asymmetric multicore systems</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Bilbao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Saez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prieto-Matias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Euro-Par 22: Parallel Processing Workshops</title>
		<meeting>Euro-Par 22: Parallel Processing Workshops</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Case for NUMA-Aware Contention Management on Multicore Systems</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Blagodurov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX ATC &apos;11</title>
		<meeting>USENIX ATC &apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Intel Architecture Day 2021: Alde Lake, Golden Cove and Gracemont Detailed</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Cutress</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Frumusanu</surname></persName>
		</author>
		<ptr target="https://www.anandtech.com/show/16881/a-deep-dive-into-intels-alder-lake-microarchitectures" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Application-to-core mapping policies to reduce memory system interference in multi-core systems</title>
		<author>
			<persName><forename type="first">Reetuparna</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fairness via source throttling: a configurable and highperformance fairness substrate for multi-core memory systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In 15th Int&apos;l Conf. Architectural Support Programming Lang. and Oper. Syst</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perf &amp; Fair: a Progress-Aware Scheduler to Enhance Performance and Fairness in SMT Multicores</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feliu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. PP</title>
		<imprint>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contention-Aware Fair Scheduling for Asymmetric Single-ISA Multicore Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="1703" to="1719" />
			<date type="published" when="2018-12">2018. Dec 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HeteroMates: Providing high dynamic power range on client devices using heterogeneous core groups</title>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 International Green Computing Conference (IGCC)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The WEKA data mining software: an update</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contention-Aware Scheduling Policies for Fairness and Throughput</title>
		<author>
			<persName><forename type="first">Alexandros-Herodotos</forename><surname>Haritatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Co-Scheduling of HPC Applications</title>
		<imprint>
			<publisher>COSH@HiPEAC</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="22" to="45" />
		</imprint>
	</monogr>
	<note>Advances in Parallel Computing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intel? 64 and IA-32 Architectures Software Developer&apos;s Manual Volume</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html" />
	</analytic>
	<monogr>
		<title level="m">System Programming Guide</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Optimizing software for x86 Hybrid Archiecture</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10">2021. Oct. 2021</date>
		</imprint>
	</monogr>
	<note>Intel White Paper</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bias Scheduling in Heterogeneous Multi-core Architectures</title>
		<author>
			<persName><forename type="first">David</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurosys 10</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single-ISA Heterogeneous Multi-Core Architectures for Multithreaded Workload Performance</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In 31st Ann. Int&apos;l Symp. Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="64" to="75" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>ISCA 04</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring Machine Learning for Thread Characterization on Heterogeneous Multiprocessors</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Petrucci</surname></persName>
		</author>
		<author>
			<persName><surname>Moss?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="123" />
			<date type="published" when="2017-09">2017. sep 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Operating system support for overlapping-ISA heterogeneous multi-core architectures</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Intl. Symp. High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Survey of Techniques for Architecting and Managing Asymmetric Multicore Processors</title>
		<author>
			<persName><forename type="first">Sparsh</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2016-02">2016. Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Machine Learning Approach for Performance Prediction and Scheduling on Heterogeneous CPUs</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nemirovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th International Symposium on Computer Architecture and High Performance Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Thermal: Introduce the Hardware Feedback Interface for thermal and performance management</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Neri</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/880587/" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Octopus-Man: QoS-driven task management for heterogeneous multicores in warehouse-scale computers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Int&apos;l. Symp. High-Performance Comp</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="246" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Power-performance modeling on asymmetric multicores</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Pricopi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2013 Int&apos;l Conf. Compilers Architectures and Synthesis for Embed. Syst. (CASES 13)</title>
		<meeting>2013 Int&apos;l Conf. Compilers Architectures and Synthesis for Embed. Syst. (CASES 13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PMCTrack: Delivering Performance Monitoring Counter Support to the OS Scheduler</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saez</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="60" to="85" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards completely fair scheduling on asymmetric single-ISA multicore processors</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saez</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="115" to="131" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enabling Performance Portability of Data-Parallel OpenMP Applications on Asymmetric Multicore Processors</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Carlos Saez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Prieto-Matias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th International Conference on Parallel Processing -ICPP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online Energy-Efficient Fair Scheduling for Heterogeneous Multi-Cores Considering Shared Resource Contention</title>
		<author>
			<persName><forename type="first">Bagher</forename><surname>Salami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Noori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Naghibzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Supercomput</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="7729" to="7748" />
			<date type="published" when="2022-04">2022. apr 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HASS: a Scheduler for Heterogeneous Multicore Systems</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Shelepov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Syst. Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scheduling heterogeneous multi-cores through Performance Impact Estimation (PIE)</title>
		<author>
			<persName><forename type="first">K</forename><surname>Van Craeynest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In 39th Ann. Int&apos;l Symp. Computer Arch. (ISCA</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="213" to="224" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fairness-aware scheduling on single-ISA heterogeneous multi-cores</title>
		<author>
			<persName><forename type="first">K</forename><surname>Van Craeynest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd Int&apos;l Conf. Parallel Arch. Compilation Techniques (PACT 13</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Providing Fairness on Shared-memory Multiprocessors via Process Scheduling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int&apos;l Conf. Measurement and Modeling Comp. Syst. (SIGMETRICS 12)</title>
		<meeting>ACM Int&apos;l Conf. Measurement and Modeling Comp. Syst. (SIGMETRICS 12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="295" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Metric-Guided Method for Discovering Impactful Features and Architectural Insights for Skylake-Based Processors</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Yasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019-12">2019. dec 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">COLAB: A Collaborative Multi-Factor Scheduler for Asymmetric Multicore Processors</title>
		<author>
			<persName><forename type="first">Teng</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th ACM/IEEE International Symposium on Code Generation and Optimization</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="268" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-architecture Prediction Based Scheduling for Energy Efficient Execution on single-ISA Heterogeneous Chip-multiprocessors</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocess. Microsyst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="271" to="285" />
			<date type="published" when="2015-06">2015. June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
