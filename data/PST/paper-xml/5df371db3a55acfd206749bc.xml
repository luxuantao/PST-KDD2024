<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Voice Puppetry: Audio-driven Facial Reenactment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-11">11 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Voice Puppetry: Audio-driven Facial Reenactment</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-11">11 Dec 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1912.05566v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis. Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input. This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space. Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames. Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio-and text-based puppetry examples. Our method is not only more general than existing works since we are generic to the input person, but we also show superior visual and lip sync quality compared to photo-realistic audio-and video-driven reenactment techniques.</p><p>-We highly recommend to watch the supplemental videohttps://justusthies.github.io</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the recent years, speech-based interaction with computers made significant progress. Digital voice assistants are now ubiquitous due to their integration into many commodity devices such as smartphone, tvs, cars, etc.; even companies use more and more machine learning techniques to drive service bots that interact with their customers. These virtual agents aim for a user-friendly man-machine interface while keeping maintenance costs low. However, a significant challenge is to appeal to humans by delivering information through a medium that is most comfortable to them. While speech-based interaction is already very successful, such as shown in virtual assistants like Siri, Alexa, Google, etc., the visual counterpart is largely missing. This comes to no surprise given that a user would also like to associate the visuals of a face with the generated audio, similar to the ideas behind video conferencing. In fact, the level of engagement for audio-visual interactions is higher than for purely audio ones <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>The aim of this work is to provide the missing visual channel by introducing Neural Voice Puppetry, a photo-realistic facial animation method that can be used in the scenario of a visual digital assistant. To this end, we build on the recent advances in text-to-speech synthesis literature <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15]</ref>, which is able to provide a synthetic audio stream from a text that can be generated by a digital agent. As visual basis, we leverage a short target video of a real person. The key component of our method is to estimate lip motions that fit the Given an audio sequence we use the DeepSpeech RNN to predict a window of character logits that are fed into a small network. This generalized network predicts coefficients that drive a person-specific expression blendshape basis which lies in the subspace of a generic expression 3D face model. We render the target face model with the new expressions using a neural rendering network.</p><p>input audio and to render the appearance of the target person in a convincing way. This mapping from audio to visual output is trained using the ground truth information that we can gather from a target video (aligned real audio and image data). We designed Neural Voice Puppetry to be an easy to use audio-to-video translation tool which does not require vast amount of video footage of a single target video or any manual user input. In our experiments, the target videos are comparably short (2-3 min), thus, allowing us to work on a large amount of video footage that can be downloaded from the Internet. To enable this easy applicability to new videos, we generalize specific parts of our pipeline. Specifically, we compute a latent expression space that is generalized among multiple persons (in our experiments 116). This also ensures the capability of being able to handle different audio inputs. Besides the generation of a visual appearance of a digital agent, our method can also be used as audio-based facial reenactment. Facial reenactment is the process of reanimating a target video in a photo-realistic manner with the expressions of a source actor. In the recent years, facial reenactment has witnessed a growing interested from the research community <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>. This enables a variety of applications, ranging from consumer-level teleconferencing through photo-realistic virtual avatars <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22]</ref> to movie production applications such as video dubbing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>. Recently, several authors started to exploit the audio signal for facial reenactment <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref>. This has the potential of avoiding failures of visual-based approaches, when the visual signal is not reliable, e.g., due to occluded face, noise, distorted views and so on. Many of these approaches, however, lack videorealism <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>. An exception is the work of Suwajanakorn et al. <ref type="bibr" target="#b23">[24]</ref>, where they have shown photo-realistic videos of President Obama that can be synthesized just from the audio signal. This approach, however, requires very large quantities of data for training (17 hours of President Obama weekly speeches) and, thus, limits its application and generalization to other identities.</p><p>To summarize, we propose a generalized audio-driven facial animation approach that • can be trained on 'in-the-wild' portrait videos (2-3 min per target video).</p><p>• includes a representation of person-specific talking styles (i.e., we preserve the talking style of the target video).</p><p>• can be driven by synthetic voices generated from textto-speech approaches, thus, enabling the transfer from text to facial animations without the need of video-text annotations.</p><p>• is able to render photo-realistic video content of a target actor that is in sync with the speech using a novel neural rendering technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural Voice Puppetry is a facial reenactment approach based only on audio input. In the literature, there are many video-based facial reenactment systems that enable dubbing and other general facial expression manipulation. Our focus in this related work section lies on the audio-based methods. These methods can be organized in facial animation and facial reenactment. Facial animation concentrates on the prediction of expressions that can be applied to a predefined avatar. In contrast, audio-driven facial reenactment aims to generate photo-realistic videos of an existing person including all idiosyncrasies from an audio signal. In the following, we will discuss these and related fields.</p><p>Video-Driven Facial Reenactment Recently, several works haven been proposed for video-driven facial reenactment which are also covered by a state-of-the-art report of Zollhöfer et al. <ref type="bibr" target="#b35">[36]</ref>. A source and target face are first reconstructed using a parametric face model. The target face is reenacted by replacing its expression parameters with that of the source face. Thies et al. <ref type="bibr" target="#b27">[28]</ref> uses a static skin texture and a data-driven approach to synthesize the mouth interior. In Deep Video Portraits <ref type="bibr" target="#b18">[19]</ref> a generative adversarial network is used to produce photo-realistic skin texture that can handle skin deformations with synthetic renderings as input. Recently, Thies et al. <ref type="bibr" target="#b26">[27]</ref> proposed neural textures, a highdimensional feature maps learned during scene capture and accessed throuh a UV look up. A defereed neural renderer refines the reconstruction. Results show that neural textures can generate high quality facial reenactments. For instance, it produces higher fidelity mouth interiors with less artifacts. Kim et al. <ref type="bibr" target="#b17">[18]</ref> analyzed the notion of style for facial expressions and showed its importance for dubbing. They define the style as a person-specific temporal global signature that can be related to the speaker's facial geometry and personality, e.g., how a person speaks, smiles. An audio-visual reenactment technique with a focus on dubbing has been proposed by Garrido et al. <ref type="bibr" target="#b10">[11]</ref>. The dubbing language track is used to force lip closure by detecting bilabial consonants /m/, /p/, and /b/. The synthesized face is rendered using the estimated target lighting and skin reflectance.</p><p>Audio-Driven Facial Animation Audio-driven facial animation is the field of generating animations for predefined 3D facial avatars from audio inputs. These methods do not focus on photo-realistic results, but on the prediction of facial motions. There is a variety of proposed techniques in the literature, we will focus on the most relevant publications.</p><p>Karras et al. <ref type="bibr" target="#b16">[17]</ref> drives a 3D facial animation using an LSTM that maps input waveforms to the 3D vertex coordinates of a face mesh, also considering the emotional state of the person. In contrast to our method it needs high quality 3D reconstructions for supervised training and does not render photo realistic output. Taylor et al. <ref type="bibr" target="#b25">[26]</ref> presented a technique to animate different avatars by any input speaker. To handle different input speakers, the audio signal is first converted into a phoneme transcript using an off the shelf speech recognition. A deep neural network then maps the phonemes into the parameters of a reference face model. The network is trained on data collected for only one person speaking for 8 hours. They show animations of different synthetic avatars using deformation retargeting. VOCA <ref type="bibr" target="#b5">[6]</ref> is an end-to-end deep neural network for speechto-animation translation trained on multiple subjects. Similar to our approach, a low-dimensional audio embedding based on features of the DeepSpeech network <ref type="bibr" target="#b12">[13]</ref> is used. From this embedding, VOCA regresses 3D vertices on a FLAME face model <ref type="bibr" target="#b20">[21]</ref> conditioned on a subject label. In contrast to our method, it requires high quality 4D scans recorded in a studio setup. Our approach works on 'in the wild' videos, with a focus on temporally coherent predictions and photorealistic renderings. Tzirakis et al. <ref type="bibr" target="#b30">[31]</ref> presented Deep Canonical Attentional Warping (DCAW) which is trained to map the audio signal to expression blendshape parameters. Trained on the Lip Reading Words (LRW) dataset <ref type="bibr" target="#b3">[4]</ref>, the DCAW network learns to warp the words of an input video to the words of the LRW dataset. Result show the ability of the approach to generalize to different speakers.</p><p>Audio-Driven Facial Reenactment The most relevant literature is in the field of audio-driven facial reenactment that has the goal to generate photo-realistic videos that are in sync with the input audio stream. A number of techniques are available for audio-driven facial reenactment <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref>. Suwajanakorn et al. <ref type="bibr" target="#b23">[24]</ref> uses an audio stream from President Barack Obama to synthesize a high quality video of him speaking. A Recurrent Neural Network is trained on many hours of his speech to learn the mouth shape from the audio. The mouth is then composited with proper 3D matching to reanimate an original video in photo-realistic manner. Because of the huge amount of used training data (17h), it is not applicable to other target actors. In contrast, our approach only needs a 2-3 min long video of a target sequence. Chung et al. <ref type="bibr" target="#b2">[3]</ref> present a technique that animates the mouth of a still, normalized image to follow an audio speech. First, the image and audio is projected into a latent space through a deep encoder. A decoder then utilizes the joint embedding of the face and audio to synthesize the talking head. The technique is trained on tens of hours of data in an unsupervised manner. In contrast to our method, it is a pure 2D image based method. Another 2D image-based method has been presented by Vougioukas et al. <ref type="bibr" target="#b32">[33]</ref>. They us a temporal GAN to produce a video of a talking face given a still image and an audio signal as input. The generator feeds the still image and the audio to an encoder-decoder architecture with a RNN to better capture temporal relations. It uses discriminators that work on per-frame and on a sequence level to improve temporal coherence. As conditioning, it also takes the audio signal as input to enforce the synthesized mouth to be in sync with the audio. In <ref type="bibr" target="#b33">[34]</ref> a dedicated mouth-audio syn discriminator is used to improve the results.</p><p>Text-Based Video Editing Fried et al. <ref type="bibr" target="#b9">[10]</ref> presented a technique for text-based editing of videos. Their approach allows overwriting existing video segments with new texts in a seamless manner. A face model <ref type="bibr" target="#b11">[12]</ref> is registered to the examined video and a viseme search finds video segments with similar mouth movements to the editing text. The corresponding face parameters of the matching video segment are blended with the original sequence parameters based on a heuristic, followed by a deep renderer to synthesize photorealistic results. The method is person specific and requires a one hour long training sequence of the target actor and, thus, is not applicable to short videos from the Internet. The viseme search is slow (∼ 5min for three words) and does not allow for interactive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Neural Voice Puppetry consists of two main parts (see Fig. <ref type="figure" target="#fig_0">2</ref>), a generalized network that predicts a latent expression vector, thus, spanning an 'audio-expression' space. This audio-expression space is shared among all persons and allows for reenactment, i.e., transferring the predicted motions from one person to another. The audio expressions are interpreted as blendshape coefficients of a 3D face model rig. This face model rig is person-specific and is optimized in the second part of our pipeline. This specialized stage captures the idiosyncrasies of a target person including the facial motion and appearance. It is trained on a short video sequence of 2 − 3 minutes (in comparison to hours that are required by state-of-the-art methods). The facial motions are represented as delta-blendshapes which we constrain to be in the subspace of a generic face template <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>. A neural texture in conjunction with a deferred neural renderer is used to store the appearance of the face of an individual person. In the following, we first focus on the required data to train the generalized and the specialized components. An advantage of our method is that we do not need a studio setup to retrieve the data, but we can use short video clips downloaded from the internet. Based on this data, we train a generalized temporal-coherent audio-expression estimation network that is used to drive person-specific video avatars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data</head><p>Learning-based approaches heavily rely on the data they are trained on. In contrast to previous model-based methods, Neural Voice Puppetry is based on 'in-the-wild' videos that can be download from the internet. Especially, we do only require RGB videos without the need of complex capturing setups and specific lighting. The videos have to be synced with the audio stream, such that we can extract ground truth pairs of audio features and image content. In our experiments the videos have a resolution of 512 × 512 with 25f ps.</p><p>Training Corpus for the Audio2ExpressionNet Fig. <ref type="figure" target="#fig_1">3</ref> shows an overview of our video training corpus that is used for the training of the small network that predicts the 'audio expressions' from the input audio features (see Sec. 5.1). The dataset consists of 116 videos with an average length of 1.7min (in total 302750 frames). We selected the training corpus, such that the persons are in a neutral mood (commentators of the German public TV).</p><p>Target Sequences For a new target sequence, we extract the person-specific talking style in the sequence. I.e., we compute a mapping from the generalized audio expression space to the actual facial movements of the target actor (see Sec. 5.3). The target sequences have a length of 2 − 3min and, thus, are easy to obtain from the Internet. The video data is also used to train a person-specific rendering network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preprocessing</head><p>We preprocess the input data to extract face tracking information as well as audio features. The preprocessing is done automatically, no manual interaction is required.</p><p>3D Face Tracking: Our method is using a statistical face model and delta-blendshapes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref> to represent a 3D latent space for modelling facial animation. The 3D face model space reduces the face space to only a few hundred parameters (100 for shape, 100 for albedo and 76 for expressions) and stays fixed in this work. Using the dense face tracking method of Thies et al. <ref type="bibr" target="#b27">[28]</ref>, we estimate the model parameters for every frame of a sequence. Note that the shape and albedo parameters are shared between all frames of one sequence. During tracking, we extract the per-frame expression parameters that are used to train the audio to expression network. To train the neural renderer, we also store the rasterized texture coordinates of the reconstructed face mesh.</p><p>Audio-feature Extraction: The video contains a synced audio stream. We use the recurrent feature extractor of the pre-trained speech-to-text model DeepSpeech <ref type="bibr" target="#b12">[13]</ref> (v0.1.0). Similar to Voca <ref type="bibr" target="#b5">[6]</ref>, we extract a window of character logits per video frame. Each window consists of 16 time intervals à 20ms, resulting in an audio feature of 16 × 29. The Deep-Speech model is generalized among thousands of different voices, trained on Mozilla's CommonVoice dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Method</head><p>To enable photo-realistic facial reenactment based on audio signals, we employ a 3D face model as intermediate representation of facial motion. A key component of our pipeline is the audio-based expression estimation. Since every person has its own talking style and, thus, different expressions, we establish person-specific expression spaces that can be computed for every target sequence. To ensure generalization among multiple persons, we created a latent audio expression space that is shared by all persons. From this audio expression space, one can map to the person specific expression space, enabling reenactment. Given the estimated expression and the extracted audio features, we apply a novel deferred neural rendering technique that generates the final output image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Audio2ExpressionNet</head><p>Our method is designed to generate temporally smooth predictions of facial motions. To this end, we employ a deep neural network with two stages. First, we predict perframe facial expression predictions. These expressions are potentially noisy, thus, we use an expression aware temporal filtering network. Given the noisy per-frame predictions as input the neural network predicts filter weights to compute smooth audio-expressions for a single frame. The per-frame as well as the filtering network can be trained jointly and outputs audio expression coefficients. This audio-expression space is shared among multiple persons and is interpreted as blendshape coefficients. Per person we compute a blendshape basis which is in the subspace of our generic face model <ref type="bibr" target="#b27">[28]</ref>. The networks are trained with a loss that works on a vertex level of this face model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-frame Audio-Expression Estimation Network</head><p>Since our goal is a generalized audio-based expression estimation, we rely on generalized audio features. We use the RNN-part of the speech to text approach DeepSpeech <ref type="bibr" target="#b12">[13]</ref> to extract these features. These features represent the logits of the DeepSpeech alphabet for 20ms audio signal. For each video frame, we extract a time window of 16 features around the frame that consist of 29 logits (length of the DeepSpeech alphabet is 29). This, 16 × 29 tensor is input to our per-frame estimation network. To map from this feature space to the unfiltered audio-expression space, we apply 4 convolutional layer and 3 fully connected layer. Specifically, we apply 2D convolutions with kernel dimensions (3, 1) and stride (2, 1), thus, filtering in the time dimension. The convolutional layers have a bias and are followed by a leaky ReLU (slope 0.02). The feature dimensions are reduced successively from (16 × 29),(8 × 32),(4 × 32),(2 × 64) to (1 × 64). This reduced feature is input to the fully connected layers that have a bias and are also followed by a leaky ReLU (0.02), except the last layer. The fully connected layers map the 64 features from the convolutional network to 128, then to 64 and, finally, to the audio expression space of dimension 32, where a TanH activation is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporally Stable Audio-Expression Estimation</head><p>To generate temporally stable audio-expression predictions, we jointly learn a filtering network that gets T per-frame estimates as input (see Fig. <ref type="figure" target="#fig_2">4</ref>). Specifically, we estimate the audio-expressions for frame t using a linear combination of the per-frame predictions of the timesteps t−T /2 to t+T /2. The weights for the linear combination are computed using a neural network that gets the audio-expressions as input (which results in an expression-aware filtering). The filter weight prediction network consists of five 1D convolutions followed by a linear layer with softmax activation (see sup- plemental material for detailed description). This content aware temporal filtering is also inspired by the self-attention mechanism <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person-specific Expressions</head><p>To retrieve the 3D model from this audio-expression space, we learn a person-specific audio expression blendshape basis which we constrain by the generic blendshape basis of our statistical face model. I.e., the audio-expression blendshapes of a person are a linear combination of the generic blendshapes. This linear relation, results in a linear mapping from the audio expression space which is output of the generalized network to the generic blendshape basis. This linear mapping is person specific, resulting in N matrices with dimension 76 × 32 during training (N being the number of training sequences and 76 being the number of generic blendshapes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss:</head><p>The network and the mapping matrices are learned end-to-end using the visually tracked training corpus. We employ a vertex-based loss function, with a higher weight (10x) on the mouth region of the face model. Specifically, we compute a vertex-to-vertex distance from the audio-based predicted and the visually tracked face model in terms of a root mean squared (RMS) distance:</p><formula xml:id="formula_0">L expr = RM S(v t − v * t ) + λ • L temp with v t ,</formula><p>the vertices based on the filtered expression estimation of frame t and v * t being the visual tracked face vertices. In addition to the absolute loss between predictions and the visual tracked face geometry, we also use a temporal loss that considers the vertex displacements of consecutive frames:</p><formula xml:id="formula_1">L temp = RM S((v t − v t−1 ) − (v * t − v * t−1 )) + RM S((v t+1 − v t ) − (v * t+1 − v * t )) + RM S((v t+1 − v t−1 ) − (v * t+1 − v * t−1</formula><p>)) These forward, backward and central differences are weighted with λ (in our experiments λ = 20). The losses are measured in millimeters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Neural Face Rendering</head><p>Based on the recent advances in neural rendering, we employ a deferred neural rendering technique that is based on neural textures to store the appearance of a face <ref type="bibr" target="#b26">[27]</ref>. Our rendering pipeline synthesizes the lower face in the target video based on the audio-driven expression estimations. Specifically, we use two networks. One network that focuses on the face interior, and another network that embeds this rendering into the original image. The estimated 3D face model is rendered using the rigid pose observed from the original target image. We render a neural texture with a resolution of 256 × 256 × 16. The network for the face interior translates these rendered feature descriptors to RGB colors. The network is using a similar structure as a classical U-Net with 5 layers. But instead of using strided convolutions that result in a downsampling in each layer, we are using dilated convolutions with increasing dilation factor and a stride of one. Instead of transposed convolutions we are using standard convolutions. All convolutions have kernel size 3 × 3. Note, dilated instead of strided convolutions do not increase the number of learnable parameters, but it increases the memory load during training and testing. Dilated convolutions help to reduce visual artifacts and result in smoother results (also temporally). The second network that blends the face interior with the 'background image' has the same structure. To remove potential movements of the chin in the background image, we erode the background image around the rendered face. Thus, the task of this second network is to inpaint the region between the face and the background.</p><p>Loss: We use a per-frame loss function that is based on an 1 loss to measure absolute errors and a VGG style loss <ref type="bibr" target="#b15">[16]</ref>.</p><p>L rendering = 1 (I, I * ) + 1 ( Î, I * ) + V GG(I, I * ) with I being the final synthetic image, I * the ground truth image and Î the intermediate result of the first network that focuses on the face interior (loss is masked to this region).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Training</head><p>Our training procedure has two stages -the generalization and the specialization phase. In the first phase, we optimize for the shared network parameters that enable a generalization among different source actors. Specifically, we train the audio-based expression estimation among all sequences from our dataset (see Sec. 4) in a supervised fashion. Given the face tracking information acquired in an automatic data preprocessing step, we know the 3D face model of a specific person for every frame. In the training process, we reproduce these 3D reconstructions based on the audio input by optimizing the network parameters and the person-specific mapping from the audio expression space to the 3D space.</p><p>In the second phase, the rendering network for a specific target sequence is trained. Given the ground truth images and the visual tracking information, we train the deferred neural renderer end-to-end including the neural texture.</p><p>Our pipeline is implemented in PyTorch. For both stages we are using the Adam <ref type="bibr" target="#b19">[20]</ref> optimizer with default settings (β 1 = 0.9, β 2 = 0.999, = 1 • e −8 ) and a learning rate of 0.0001. The Audio2ExpressionNet is trained for 50 epochs (resulting in a training time of ∼ 28 hours on a Nvidia 1080Ti) with a learning rate decay for the last 30 epochs, a batch size of 16 and Xavier initialization. The rendering networks are also trained for 50 epochs for each target person individually with a batch size of 1 (∼ 30 hours training time, ∼ 5 hours in case of strided convolutions).</p><p>New target video Since the audio-based expression estimation network is generalized among multiple persons, we can apply it to unseen actors. The person specific mapping between the predicted audio expression space coefficients and the expression space of the new person can be obtained by solving a linear system of equations. Specifically, we extract the audio-expression for all training images and compute the linear mapping to the expressions that are visually estimated. In addition to this step, the person-specific rendering network for the new target video is trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Inference</head><p>At test time, we only require a source audio sequence. Based on the target actor selection, we use the corresponding person-specific mapping. The mapping from the audio features to the person specific expression space takes less than 2ms on an Nvidia 1080Ti. Generation of the 3D model and the rasterization using these predictions takes another 2ms. The deferred neural rendering takes ∼ 5ms which results in a real-time capable pipeline.</p><p>Text-to-Video Our pipeline is trained on real video sequences, where the audio is in sync with the visual content. Thus, we learned a mapping directly from audio to video that ensures synchronicity. Instead of going directly from text to video, where such a natural training corpus is not available, we synthesize voice from the text and feed this into our pipeline. For our experiments we used samples from the DNN-based text-to-speech demo of IBM Watson<ref type="foot" target="#foot_0">1</ref> . Which gives us state-of-the-art synthetic audio streams that are comparable to the synthetic voices of virtual assistants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>Neural Voice Puppetry has several important use cases, i.e., audio-driven video avatars, video dubbing and textdriven video synthesis of a talking head, see supplemental video. In the following sections, we discuss these results including comparisons to state-of-the-art approaches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Audio-driven model-based video avatars:</head><p>First we discuss model-based video avatars that can be controlled by audio input. In Fig. <ref type="figure" target="#fig_3">5</ref> we show a representative image from a comparison to Taylor et al. <ref type="bibr" target="#b25">[26]</ref>, Karras et al. <ref type="bibr" target="#b16">[17]</ref> and Suwajanakorn et al. <ref type="bibr" target="#b23">[24]</ref>. All three methods were published at SIGGRAPH 2017, where this sequence has been shown as a direct comparison, thus, all results are generated by the original implementation of the authors. Only the method of Suwajanakorn et al. is able to produce photo-realistic output. The method is fitted to the scenario where a large video dataset of the target person is available and, thus, limited in its applicability. They demonstrate it on sequences of Obama, using 14 hours of training data and 3 hours for validation. In contrast, our method works on short 2 − 3 min target video clips. In our supplemental video, we show multiple comparisons to Voca <ref type="bibr" target="#b5">[6]</ref>. Fig. <ref type="figure" target="#fig_4">6</ref> shows an image of a legacy Winston Churchill sequence. In contrast to Voca, our aim is to generate photo-realistic output videos that are in sync with the audio. Voca focuses on the 3D geometry requiring a 4D training corpus, while our approach uses a 3D proxy only as an intermediate step and works on videos from the Internet. Our 3D proxy is based on a generic face model and, thus, has not the details as a person-specific modelled mesh. Nevertheless, using our neural rendering approach, we are able to generate photo-realistic results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Audio-driven 2D-based image avatars:</head><p>'You said that?' <ref type="bibr" target="#b2">[3]</ref> is a GAN-based method that works without an explicit 3D model. It is operating in a normalized space of facial imagery (cropped, frontal faces) and needs a  single image of the target person. In contrast, our method employs a 3D model to ensure 3D consistent movements in the output video. Instead of a normalized image, we generate an output that is embedded in a real video (see Fig. <ref type="figure" target="#fig_5">7</ref>). Similar to 'You said that?', Vougioukas et al. <ref type="bibr" target="#b33">[34]</ref> generate talking head animations from a still image, including movements of eyebrows and eye blinks. Fig. <ref type="figure" target="#fig_5">7</ref> also shows a comparison to this method on a sequence of President Trump that is driven by the audio of an impersonator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Video dubbing:</head><p>State-of-the-art video dubbing is based on video-driven facial reenactment <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18]</ref>. In contrast, our method is only relying on the voice of the dubber. The 'Deferred Neural Rendering' <ref type="bibr" target="#b26">[27]</ref> is a generic neural rendering approach, but the authors also show the usage in the scenario of facial reenactment. It builds upon the Face2Face <ref type="bibr" target="#b27">[28]</ref> pipeline and directly transfers the deformations from the source to the target actor. Thus, tracking errors that occur in the source video (e.g., due to occlusions or fast motions) are transferred to the target video. In a dubbing scenario, the goal is to keep the talking style of the target actor which is not the case for <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>. To compensate the influence of the source actor talking style, Kim et al. <ref type="bibr" target="#b17">[18]</ref> proposed a method to map from the source style to the target actor style. Our approach directly operates in the target actor expression space, thus, no mapping is needed (we also do not capture the source actor style). This enables us to also work on strong expressions, as shown in Fig. <ref type="figure" target="#fig_6">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Text-driven video synthesis</head><p>Fried et al. presented 'Text-based Editing of Talkinghead Video' <ref type="bibr" target="#b9">[10]</ref> which provides a video editing tool that is based on the transcript of the video. The method reassembles captured expression snippets from the target video, requiring blending heuristics. To achieve their results they rely on more than one hour of training data. We show a direct comparison to this method in the supplemental video. Note, our method only uses the synthetic audio sequence as input, while the method of Fried et al. uses both the transcript and the audio. In the comparison our method generates the entire video, while the text-based editing method only synthesizes the frames of the new three words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Ablation studies</head><p>We use self-reenactment to evaluate our pipeline (Fig. <ref type="figure" target="#fig_7">9</ref>), since it gives us access to a ground truth video sequence where we can also retrieve visual face tracking. As a distance measurement, we use an 2 distance in color space (colors in [0,1]). Using this measure, we evaluate the rendering network (assuming good visual face tracking) and the entire pipeline. Specifically, we compare the results using visual tracked mouth movements to the results using audio-based predictions (see video). The mean color difference of the re-rendering on a test sequence of 645 frame is 0.003 for the visual and 0.005 for the audio-based expressions.</p><p>In the supplemental video we also show a side-by-side comparison of our rendering network using dilated convolutions and our network with strided convolutions (and a kernel size of 4 to reduce block artifacts in the upsampling). Both networks are trained with the same number of epochs (50). As can be seen, dilated convolutions lead to visually more pleasing results (smoother in spatial and temporal domain).</p><p>Our results are covering different target persons which demonstrates the wide applicability of our method and that we are able to map the generalized audio-expression space to different person-specific talking styles and appearances. As can be seen in the supplemental video, the expression estimation network that is trained on multiple target sequences (302750 frames) results in more coherent predictions than the network solely trained on a sequence of Obama (3145 frames). The usage of more target videos increases the training corpus size and the variety of input voices and, thus, leads to more robustness.</p><p>Our training corpus is based on German news speakers. Nevertheless, most of our results are in English and show a good transferability. In the video we also show a comparison of the transfer from different source languages to different target videos that are originally also in different languages. User study We further quantify the output quality of our approach in a user study. To this end, we show sequences of the competing methods, as well as results of our method. The 56 attendees with a computer science background judged upon synchronicity and visual quality ('very bad', 'bad', 'neither bad nor good', 'good', 'very good'). The study consists of 24 videos which are presented to the user in randomized order. See supplemental material for the collection of videos we used and the statistics. In Fig. <ref type="figure" target="#fig_8">10</ref>, we show the percentage of attendees that rated the specific approach good or very good. As can be seen our approach gives the best visual quality and also state-of-the-art quality for audio-visual sync for photo-realistic methods based on audio input similar to the video-based approach of Thies et al. <ref type="bibr" target="#b26">[27]</ref>. The method of Vougioukas <ref type="bibr" target="#b33">[34]</ref> achieves higher audio-visual sync but lacks visual quality and is not able to synthesize natural videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations</head><p>As can be seen in the supplemental video, our approach works robustly on different audio sources and target videos. But it still has limitations. Especially, in the scenario of multiple voices in the audio stream our method fails. Recent work is solving this 'cocktail party' issue using visual clues <ref type="bibr" target="#b6">[7]</ref>. As all other reenactment approaches, the target videos have to be occlusion free to allow good visual tracking. Another limitation is the fixed talking style. We assume that the target actor has a constant talking style during a target sequence. In follow-up work we plan to estimate the talking style from the audio signal to control the expressiveness of the facial motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we presented a novel audio-driven facial reenactment approach that is generalized among different audio sources. This allows us not only to synthesize videos of a talking head from an audio sequence from another person, but also to generate a photo-realistic video based on a synthesized voice. I.e., text-driven video synthesis can be achieved that is in sync with artificial voice. We hope that our work is a stepping stone in the direction to audio-visual assistants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgments</head><p>We gratefully acknowledge the support by the AI Foundation, Google, Sony, a TUM-IAS Rudolf Mößbauer Fellowship, the ERC Starting Grant Scan2CAD (804724), the ERC Consolidator Grant 4DRepLy (770784), and a Google Faculty Award.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architectures</head><p>Audio2ExpressionNet: A core component of Neural Voice Puppetry is the estimation of facial expressions based on audio. To retrieve temporal coherent estimations, we employed a process with two stages. In the first stage, we estimate per frame expressions based on DeepSpeech features. The network is depicted in Fig. <ref type="figure" target="#fig_9">11</ref>. The output of this network is an audio-expression vector of length 32. This audio-expression is temporally noisy and is filtered using an expression aware filtering network which can be trained in conjunction with the per frame expression estimation network. The temporal filtering mechanism is also depicted in the main paper. The underlying network that predicts the filter weights gets as input T = 8 per-frame predicted audio expressions. We apply 5 1D-convolutional filters with kernel size 3 that reduce the feature space successively from 32 × 8 over 16 × 8, 8 × 8, 4 × 8, 2 × 8 to 1 × 8. Each of these convolutions has a bias and is followed by a leaky ReLU activation (negative slope of 0.02). The output of the convolutional network is input to a fully connected layer with bias that maps the 1 × 8 input to the 8 filter weights that are normalized using a softmax function.</p><p>To train the network we apply a vertex-based loss as described in the main paper. The vertices that refer to the mouth region are weighted with a 10× higher loss. We use the mask that is depicted in Fig. <ref type="figure" target="#fig_10">12</ref>. For generalization we used a dataset composed of commentators from the German public TV (e.g., https://www.tagesschau.de/ multimedia/video/video-587039.html). In total the dataset contained 116 videos.</p><p>Rendering network: In Fig. <ref type="figure" target="#fig_11">13</ref>, we show an overview of our neural rendering approach. Based on the expression  predictions, that drive a person-specific 3D face model, we render a neural texture to the image space of the target video. A first network is used to convert the neural descriptors sampled from the neural texture to RGB color values. A second network embeds this image into the target video frame. We erode the target image around the synthetic image, to remove motions of the target actor like chin movements. Using this eroded target image as background and the output of the first network, the second network outputs the final image. Both networks have the same structure, only the input dimensions are different. The first network gets an image with 16 feature channels as input (dimension of the neural descriptors that are sampled from a neural texture with dimensions 256 × 256 × 16), while the second network composites the background and the output of the first network, resulting in an 6 channel input. The networks are implemented in the Pix2Pix framework <ref type="bibr" target="#b13">[14]</ref>. Instead of a classical U-Net with strided convolutions, we build on dilated convolutions. Specifically, we replace the strided convolutions in a U-Net of depth 5. Instead of transposed convolutions we use a standard convolution, since we do not downsample the image and always keep the same image dimensions. Note that we also keep the skip connections of the classical U-Net. The number of features per layer is 32 in our experiments, resulting in networks with ∼ 2.35mio parameters (which is low in comparison to the used network in Deferred Neural Rendering <ref type="bibr" target="#b26">[27]</ref> with ∼ 16mio parameters). We employ the structure that is depicted in Fig. <ref type="figure" target="#fig_13">15</ref>. Each convolution layer  has a kernel size of 3 × 3 and is followed by a leaky ReLU with negative slop of 0.2. All layers have stride 1 which means that all layers intermediate feature maps have the same spatial size as the input (512 × 512). The first convolutional layer maps to a feature space of dimension 32 and has a dilation of 1. With increasing layer depth the feature space dimension as well as the dilation increases by a factor of 2. After layer depth 5, we use standard convolutions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. User Study</head><p>In this section, we present the statistics of our user study. Fig. <ref type="figure" target="#fig_14">16</ref> shows a collection of videos that we used for the user study. The clips are from the official videos of the corresponding methods and are similar to the clips that we show in our supplemental video. Fig. <ref type="figure" target="#fig_14">16</ref> shows the average answers of our questions, including the variance.</p><p>In the user study we asked the following questions:</p><p>• How would you rate the audio-visual alignment (lip sync) in this video?</p><p>• How would you rate the visual quality of the video?</p><p>With the answer possibilities "very good","good","Neither good nor bad","bad", "very bad".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ethical Considerations</head><p>In conjunction with person specific audio generators like Jia et al. <ref type="bibr" target="#b14">[15]</ref>, a pipeline can be established that creates videorealistic (temporal voice-and photo-realistic) content of a person. This is perfect for creative people in movie and content production, to edit and create new videos. On the other hand, it can be misused. To this end, the field of digital media forensics is getting more attention. Recent publications <ref type="bibr" target="#b22">[23]</ref> show that humans have a hard time in detecting fakes, especially, in the case of compressed video content. Learned detectors are showing promising results, but are lacking generalizeability to other manipulation methods that are not in the training corpus. Few-shot learning methods like ForensicTransfer <ref type="bibr" target="#b4">[5]</ref> try to solve this issue. As part of our responsibility, we are happy to share generated videos of our method with the forensics community. Nevertheless, our approach enables several practical use-cases, ranging from movie-dubbing to text-driven photo-realistic video avatars. We hope that our work is a stepping stone in the direction of audio-based reenactment and is inspiring more follow-up projects in this field. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pipeline of Neural Voice Puppetry.Given an audio sequence we use the DeepSpeech RNN to predict a window of character logits that are fed into a small network. This generalized network predicts coefficients that drive a person-specific expression blendshape basis which lies in the subspace of a generic expression 3D face model. We render the target face model with the new expressions using a neural rendering network.</figDesc><graphic url="image-2.png" coords="2,50.11,72.00,495.00,120.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Video training corpus downloaded from the Internet.</figDesc><graphic url="image-3.png" coords="4,308.86,27.18,236.25,112.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: To get smooth audio-expressions, we employ a contentaware filtering in the time dimension.</figDesc><graphic url="image-4.png" coords="5,360.77,72.00,129.94,131.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison to state-of-the-art audio-driven model-based video avatars. Our approach is applicable to multiple targets.</figDesc><graphic url="image-5.png" coords="7,50.11,72.00,236.25,113.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Qualitative comparison of our method to Voca [6]. It is a representative image for a talking sequence of Winston Churchill.</figDesc><graphic url="image-6.png" coords="7,72.49,225.29,188.99,69.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison to the 2D-based methods 'You said that?' [3] and 'Realistic Speech-Driven Facial Animation with GANs' [34].</figDesc><graphic url="image-7.png" coords="7,308.86,72.00,236.24,76.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visual dubbing fails to map strong expressions from the source to plausible expressions of the target actor.</figDesc><graphic url="image-8.png" coords="7,308.86,186.62,236.24,78.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Self-reenactment: Evaluation of our rendering network and the audio-prediction network.</figDesc><graphic url="image-10.png" coords="8,308.86,72.00,236.25,75.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: User study: percentage of attendees (in total 56) that rated the visual audio-visual quality good or very good.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Based on the audio features extracted by the RNN of DeepSpeech, we predict an audio-expression vector.</figDesc><graphic url="image-11.png" coords="9,50.11,585.86,236.23,102.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: During the training of the Audio2ExpressionNet, we give higher weight to the per-vertex loss in the region of the mouth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Our neural rendering approach consists of a deferred neural renderer and an inpainting network that blends the modified face interior into the target image.</figDesc><graphic url="image-13.png" coords="9,308.86,534.81,236.25,142.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Our user study contained 24 videos from different state-of-the-art methods, including 3 original videos. Here we show some frames of the videos.</figDesc><graphic url="image-14.png" coords="10,50.11,72.00,494.99,106.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: We use a modified U-Net architecture that uses dilated convolutions instead of strided convolutions. Transposed convolutions are replaced by std. convolutions.</figDesc><graphic url="image-15.png" coords="10,73.74,338.29,189.00,253.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Statistics of our user study including the mean and the variance with respect to the specific methods and question about visual quality and audio-visual sync (lip sync).</figDesc><graphic url="image-16.png" coords="11,111.99,72.00,371.24,139.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,50.11,205.38,494.97,129.15" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://text-to-speech-demo.ng.bluemix.net/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH)</title>
				<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video rewrite: Driving visual speech with audio</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;97</title>
				<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Forensictransfer: Weakly-supervised domain adaptation for forgery detection</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rössler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anurag Ranjan, and Michael Black. Capture, learning, and synthesis of 3D speaking styles</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cudeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Laidlaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: A speakerindependent audio-visual model for speech separation</title>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinatan</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trainable videorealistic speech animation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ezzat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. on Graph</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="388" to="398" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Video-Mediated Communication</title>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">E</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>L. Erlbaum Associates Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text-based editing of talking-head video</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH)</title>
				<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VDub -modifying face video of actors for plausible visual alignment to a dubbed audio track</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Sarmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum (Proceedings of EUROGRAPHICS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reconstruction of personalized 3D face rigs from monocular video</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH)</title>
				<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep-Speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>arxiv</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transfer learning from speaker verification to multispeaker text-to-speech synthesis</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH)</title>
				<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural style-preserving visual dubbing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH-Asia)</title>
				<meeting>SIGGRAPH-Asia)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIG-GRAPH)</title>
				<meeting>SIG-GRAPH)</meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a model of facial shape and expression from 4D scans</title>
		<author>
			<persName><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
	<note>Two first authors contributed equally</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep appearance models for face rendering</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH)</title>
				<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Faceforen-sics++: Learning to detect manipulated facial images</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rössler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Synthesizing obama: Learning lip sync from audio</title>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH)</title>
				<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2007">July 2017. 2, 3, 7</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Seeing is believing but is hearing? comparing audio and video communication for young children</title>
		<author>
			<persName><forename type="first">Joanne</forename><surname>Tarasuik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordy</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roslyn</forename><surname>Galligan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deep learning approach for generalized speech animation</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taehwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Krahe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasio</forename><surname>Garcia Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">July 2017. 3, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deferred neural rendering: Image synthesis using neural textures</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH)</title>
				<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2009">2019. 3, 6, 7, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face2Face: Real-time Face Capture and Reenactment of RGB Videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Justus Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2016. 2, 3, 4, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FaceVR: Real-time gaze-aware facial reenactment in virtual reality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graph</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">HeadOn: Real-time reenactment of human portrait videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH)</title>
				<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Synthesising 3D facial motion from &quot;in-the-wild&quot; speech</title>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lattas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michail</forename><surname>Tarasiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno>CoRR, abs/1904.07002</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">Aron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<editor>Arxiv</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stavros Petridis, and Maja Pantic. End-to-end speech-driven facial animation with temporal gans</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Realistic speech-driven facial animation with gans. International Journal of Computer Vision (IJCV)</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">Oct 2019. 3, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-attention generative adversarial networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<title level="m">State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications. Computer Graphics Forum (Eurographics State of the Art Reports)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
