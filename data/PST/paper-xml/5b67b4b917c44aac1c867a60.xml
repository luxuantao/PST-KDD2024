<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">ing Laboratory</orgName>
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Signal Processing and Acoustics</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">266A1455658A8D1BDDA4E9E55B4E7B47</idno>
					<idno type="DOI">10.1109/JSTSP.2018.2885636</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSTSP.2018.2885636, IEEE Journal of Selected Topics in Signal Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSTSP.2018.2885636, IEEE Journal of Selected Topics in Signal Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSTSP.2018.2885636, IEEE Journal of Selected Topics in Signal Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sound event detection</term>
					<term>direction of arrival estimation</term>
					<term>convolutional recurrent neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a convolutional recurrent neural network for joint sound event localization and detection (SELD) of multiple overlapping sound events in threedimensional (3D) space. The proposed network takes a sequence of consecutive spectrogram time-frames as input and maps it to two outputs in parallel. As the first output, the sound event detection (SED) is performed as a multi-label classification task on each time-frame producing temporal activity for all the sound event classes. As the second output, localization is performed by estimating the 3D Cartesian coordinates of the direction-ofarrival (DOA) for each sound event class using multi-output regression. The proposed method is able to associate multiple DOAs with respective sound event labels and further track this association with respect to time. The proposed method uses separately the phase and magnitude component of the spectrogram calculated on each audio channel as the feature, thereby avoiding any method-and array-specific feature extraction. The method is evaluated on five Ambisonic and two circular array format datasets with different overlapping sound events in anechoic, reverberant and real-life scenarios. The proposed method is compared with two SED, three DOA estimation, and one SELD baselines. The results show that the proposed method is generic and applicable to any array structures, robust to unseen DOA values, reverberation, and low SNR scenarios. The proposed method achieved a consistently higher recall of the estimated number of DOAs across datasets in comparison to the best baseline. Additionally, this recall was observed to be significantly better than the best baseline method for a higher number of overlapping sound events.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S OUND event localization and detection (SELD) is the combined task of identifying the temporal activities of each sound event, estimating their respective spatial location trajectories when active, and further associating textual labels with the sound events. Such a method can for example automatically describe social and human activities and assist the hearing impaired to visualize sounds. Robots can employ this for navigation and natural interaction with surroundings <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Smart cities, smart homes, and industries could use it for audio surveillance <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Smart meeting rooms can recognize speech among other events and use this information to beamform and enhance the speech for teleconferencing or for robust automatic speech recognition <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Naturalists could use it for bio-diversity monitoring <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Further, in virtual reality (VR) applications with 360°audio SELD can be used to assist the user in visualizing sound events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sound event detection</head><p>The SELD task can be broadly divided into two sub-tasks, sound event detection (SED) and sound source localization. SED aims at detecting temporally the onsets and offsets of sound events and further associating textual labels to the detected events. The sound events in real-life most often overlap with other sound events in time and the task of recognizing all the overlapping sound events is referred as polyphonic SED. The SED task in literature has most often been approached using different supervised classification methods that predict the framewise activity of each sound event class. Some of the classifiers include Gaussian mixture model (GMM)hidden Markov model (HMM) <ref type="bibr" target="#b26">[27]</ref>, fully connected (FC) neural networks <ref type="bibr" target="#b27">[28]</ref>, recurrent neural networks (RNN) <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>, and convolutional neural networks (CNN) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. More recently state-of-the-art results were obtained by stacking CNN, RNN and FC layers consecutively, referred jointly as the convolutional recurrent neural network (CRNN) <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>.</p><p>Lately, in order to improve recognition of overlapping sound events, several multichannel SED methods have been proposed <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref> and these were among the top performing methods in the real-life SED task of DCASE 2016 <ref type="foot" target="#foot_0">1</ref> and 2017<ref type="foot" target="#foot_1">2</ref> evaluation challenges. More recently, we studied the SED performance on identical sound scenes captured using single, binaural and first-order Ambisonics (FOA) microphones <ref type="bibr" target="#b34">[35]</ref>, where the order denotes the spatial resolution of the format and the first order corresponds to four channels. The results showed that the recognition of overlapping sound events improved with increase in spatial sampling, and the best performance was obtained with FOA.  <ref type="bibr" target="#b19">[20]</ref> Spectral power azi (Full) for each class Multiple CNN Circular Yiwere et al. <ref type="bibr" target="#b20">[21]</ref> ILD, cross-correlation azi and dist 1 FC Binaural × Ferguson et al. <ref type="bibr" target="#b21">[22]</ref> GCC, cepstrogram azi and dist (regression) 1 CNN Linear × Vesperini et al. <ref type="bibr" target="#b22">[23]</ref> GCC x and y (regression) 1 FC Distributed × Sun et al. <ref type="bibr" target="#b23">[24]</ref> GCC azi and ele 1 PNN Cartesian × Adavanne et al. <ref type="bibr" target="#b24">[25]</ref> Phase and magnitude spectrum azi and ele (Full) Multiple CRNN Generic ×</p><p>Roden et al. <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sound source localization</head><p>Sound source localization is the task of determining the direction or position of a sound source with respect to the microphone. In this paper, we only deal with the estimation of the sound event direction, generally referred as directionof-arrival (DOA) estimation. The DOA methods in literature can be broadly categorized into parametric-and deep neural network (DNN)-based approaches. Some popular parametric methods are based on time-difference-of-arrival (TDOA) <ref type="bibr" target="#b43">[44]</ref>, the steered-response-power (SRP) <ref type="bibr" target="#b44">[45]</ref>, multiple signal classification (MUSIC) <ref type="bibr" target="#b45">[46]</ref>, and the estimation of signal parameters via rotational invariance technique (ESPRIT) <ref type="bibr" target="#b46">[47]</ref>. These methods vary in terms of algorithmic complexity, constraints in array geometry, and model assumptions on the acoustic scenarios. Subspace methods like MUSIC can be applied with different array types and can produce high-resolution DOA estimates of multiple sources. On the other hand, subspace methods require a good estimate of the number of active sources that may be hard to obtain, and they have been found sensitive to reverberant and low signal-to-noise (SNR) scenarios <ref type="bibr" target="#b47">[48]</ref>.</p><p>Recently, DNN-based methods were employed to overcome some of the drawbacks of parametric methods, while being robust towards reverberation and low SNR scenarios. Additionally, implementing the localization task in the DNN framework allows seamless integration into broader DNN tasks such as SELD <ref type="bibr" target="#b19">[20]</ref>, robots can use it for sound source based navigation and natural interaction in multi-speaker scenarios <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. A summary of the most recent DNN-based DOA estimation methods is presented in Table <ref type="table" target="#tab_0">I</ref>. All these methods estimate DOAs for static point sources and were shown to perform equally or better than the parametric methods in reverberant scenarios. Further, methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> proposed to simultaneously detect DOAs of overlapping sound events by estimating the number of active sources from the data itself. Most methods used a classification approach, thereby estimating the source presence likelihood at a fixed set of angles, while <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> used a regression approach and let the DNN produce continuous output.</p><p>All of the past works were evaluated on different array geometries, making a direct performance comparison difficult. Most of the methods estimated full azimuth ('Full' in Table <ref type="table" target="#tab_0">I</ref>) using microphones mounted on a robot, circular and distributed arrays, while the rest of the methods used linear arrays thereby estimating only the azimuth angles in a range of 180°. Although few of the existing methods estimated the azimuth and elevation jointly <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, most of them estimated only the azimuth angle <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. In particular, we studied the joint estimation of azimuth and elevation angles in <ref type="bibr" target="#b24">[25]</ref>, this was enabled by the use of Ambisonic signals (FOA) obtained using a spherical array. Ambisonics are also known as spherical harmonic (SH) signals in the array processing literature, and they can be obtained from various array configurations such as circular or planar (for 2D capture) and spherical or volumetric (for 3D capture) using an appropriate linear transform of the recordings <ref type="bibr" target="#b48">[49]</ref>. The same ambisonic channels have the same spatial characteristics independent of the recording setup, and hence, studies on such hardware-independent formats make the evaluation and results more easily comparable in the future.</p><p>Most of the previously proposed DNN-based DOA estimation methods that relied on a single array or distributed arrays of omnidirectional microphones, captured source location information mostly in phase-or time-delay differences between the microphones. However, compact microphone arrays with full azimuth and elevation coverage, such as spherical microphone arrays, rely strongly on the directionality of the sensors to capture spatial information, this reflects mainly in the magnitude differences between channels. Motivated by this fact we proposed to use both the magnitude and phase component of the spectrogram as input features in <ref type="bibr" target="#b24">[25]</ref>. Thus making the DOA estimation method <ref type="bibr" target="#b24">[25]</ref> generic to array configuration by avoiding method-specific feature extractions like inter-aural level difference (ILD), the inter-aural time difference (ITD), generalized cross-correlation (GCC) or eigenvectors of spatial covariance matrix used in previous methods (Table <ref type="table" target="#tab_0">I</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Joint localization and detection</head><p>In the presence of multiple overlapping sound events, the DOA estimation task becomes the classical tracking problem of associating correctly the multiple DOA estimates to respective sources, without necessarily identifying the source <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>. The problem is further extended for the polyphonic SELD task if the SED and DOA estimation are done separately, resulting in the data association problem between the recognized sound events and the estimated DOAs <ref type="bibr" target="#b12">[13]</ref>. One solution to the data association problem is to jointly predict the SED and DOA. In this regard, to the best of the authors' knowledge, <ref type="bibr" target="#b19">[20]</ref> is the only DNN-based method which performs SELD. Other works combining SED and parametric DOA estimation include <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. Lopatka et al. <ref type="bibr" target="#b52">[53]</ref> used a 3D sound intensity acoustic vector sensor, with MPEG-7 spectral and temporal features along with a support vector machine classifier to estimate DOA along azimuth for five classes of non-overlapping sound events. Butko et al. <ref type="bibr" target="#b12">[13]</ref> used distributed microphone arrays to recognize 14 different sound events with an overlap of two at a time, using a GMM-HMM classifier, and localized them inside a meeting room using the SRP method. Chakraborty et al. <ref type="bibr" target="#b51">[52]</ref> replaced SRP-based localization in <ref type="bibr" target="#b12">[13]</ref> with a sound-modelbased localization, thereby fixing the data association problem faced in <ref type="bibr" target="#b12">[13]</ref>. In contrast, Hirvonen <ref type="bibr" target="#b19">[20]</ref>, extracted the framewise spectral power from each microphone of a circular array and used a CNN classifier to map it to eight angles in full azimuth for each sound event class in the dataset. In this output format, the resolution of azimuth is limited to the trained directions and the performance of unseen DOA values is unknown. For larger datasets with a higher number of sound events and increased resolution along azimuth and elevation directions, this approach results in a large number of output nodes. Training such a DNN with a large number of output nodes where the number of positive class labels per frame is one or two with respect to a high number of negative class labels poses challenges of an imbalanced dataset. Additionally, training such a large number of classes requires a huge dataset with enough examples for each class. On the other hand, this output format allows the network to simultaneously recognize more than one instance of the same sound event in a given time frame, at different locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contributions of this paper</head><p>In general, the number of existing SELD methods is limited <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>, with only one published DNN-based approach <ref type="bibr" target="#b19">[20]</ref>. On the other hand, there are several DNNbased methods in the literature for the SELD sub-tasks of SED and DOA estimation. Yet, there is no comprehensive work published that studies the various choices affecting the performance of these DNN-based SED, DOA and SELD methods, compare them with multiple competitive baselines, and evaluate them over a wide range of acoustic conditions. Besides, with respect to the SELD task, the existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53</ref>] localize up to one or maximum two overlapping sound events and do not scale to a higher number of overlapping sources. Further, the only DNN-based SELD method <ref type="bibr" target="#b19">[20]</ref> localizes sound events exclusively at a predefined grid of directions and requires a large number of output classes for a higher number of sound event labels and increased spatial resolution. Additionally, all the above SELD approaches use method-specific features and hence not independent of input array structure.</p><p>In contrast to existing SELD methods, this paper presents novelty in two broad areas: the proposed SELD method, and the exhaustive evaluation studies presented. The novelty of the proposed SELD method is as follows. It is the first method that addresses the problem of localizing and recognizing more than two overlapping sound events simultaneously and tracking their activity with respect to time. The proposed method is able to localize sources at any azimuth and elevation angles while being robust to unseen spatial locations, reverberation, and ambiance. Further, the method itself is generic enough to learn to perform SELD from any input array structure. Specifically, as our method, we propose to use the polyphonic SED output <ref type="bibr" target="#b38">[39]</ref> as a confidence measure for choosing the DOAs estimated in a regression manner. By this approach, we not only extend the state-of-the-art polyphonic SED performance <ref type="bibr" target="#b38">[39]</ref> for polyphonic SELD but also tackle the dataassociation problem faced due to the polyphony in SELD tasks <ref type="bibr" target="#b12">[13]</ref>. As the second broad area of novelty, we present the performance of the proposed method with respect to various design choices made such as the DNN architecture, input feature and DOA output format. Additionally, we also present the comprehensive results of the proposed method with respect to six baselines (two SED, three DOA estimation, and one SELD baseline) evaluated on seven datasets with different acoustic conditions (anechoic and reverberant scenarios with simulated and real-life impulse responses), array configurations (Ambisonic and circular array) and the number of overlapping sound events.</p><p>In order to facilitate reproducibility of research, the proposed method and all the datasets used have been made publicly available <ref type="foot" target="#foot_2">3</ref> . Additionally, the real-life impulse responses used to simulate datasets have also been published to enable users to experiment with custom sound events.</p><p>The rest of the paper is organized as follows. In Section II, we describe the proposed SELD method and the training procedure. In Section III, we describe the datasets, the baseline methods, the metrics and the experiments carried out for evaluating the proposed method. The experimental results on the evaluation datasets are presented, compared with baselines and discussed in Section IV. Finally, we summarize the conclusions of the work in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHOD</head><p>The block diagram of the proposed method for SELD is presented in Figure <ref type="figure" target="#fig_0">1a</ref>. The input to the method is the multichannel audio. The phase and magnitude spectrograms are extracted from each audio channel and are used as separate features. The proposed method takes a sequence of features in consecutive spectrogram frames as input and predicts all the sound event classes active for each of the input frames along with their respective spatial location, producing the temporal activity and DOA trajectory for each sound event class. In particular, a CRNN is used to map the feature sequence to the two outputs in parallel. At the first output, SED is performed as a multi-label classification task, allowing the network to simultaneously estimate the presence of multiple sound events for each frame. At the second output, DOA estimates in the continuous 3D space are obtained as a multi-output regression task, where each sound event class is associated with three regressors that estimate the 3D Cartesian coordinates x, y and z of the DOA on a unit sphere around the microphone. The SED output of the network is in the continuous range of [0 1] for each sound event in the dataset, and this value is thresholded to obtain a binary decision for the respective sound event activity as shown in Figure <ref type="figure" target="#fig_0">1b</ref>. Finally, the respective DOA estimates for these active sound event classes provide their spatial locations. The detailed description of the feature extraction and the proposed method is explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature extraction</head><p>The spectrogram is extracted from each of the C channels of the multichannel audio using an M -point discrete Fourier transform (DFT) on Hamming window of length M and 50% overlap. The phase and magnitude of the spectrogram are then extracted and used as separate features. Only the M/2 positive frequencies without the zeroth bin are used. The output of the feature extraction block in Figure <ref type="figure" target="#fig_0">1a</ref> is a feature sequence of T frames, with an overall dimension of T × M/2 × 2C, where the 2C dimension consists of C magnitude and C phase components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural network architecture</head><p>The output of the feature extraction block is fed to the neural network as shown in Figure <ref type="figure" target="#fig_0">1a</ref>. In the proposed architecture the local shift-invariant features in the spectrogram are learned using multiple layers of 2D CNN. Each CNN layer has P filters of 3 × 3 × 2C (as in <ref type="bibr" target="#b24">[25]</ref>) dimensional receptive fields acting along the time-frequency-channel axis with a rectified linear unit (ReLU) activation. The use of filter kernels spanning all the channels allows the CNN to learn relevant inter-channel features required for localization, whereas the time and frequency dimensions of the kernel allows learning relevant intra-channel features suitable for both the DOA and SED tasks. After each layer of CNN, the output activations are normalized using batch normalization <ref type="bibr" target="#b53">[54]</ref>, and the dimensionality is reduced using max-pooling (M P i ) along the frequency axis, thereby keeping the sequence length T unchanged. The output after the final CNN layer with P filters is of dimension T × 2 × P , where the reduced frequency dimension of 2 is a result of max-pooling across CNN layers (see Section IV-1).</p><p>The output activation from CNN is further reshaped to a T frame sequence of length 2P feature vectors and fed to bidirectional RNN layers which are used to learn the temporal context information from the CNN output activations. Specifically, Q nodes of gated recurrent units (GRU) are used in each layer with tanh activations. This is followed by two branches of FC layers in parallel, one each for SED and DOA estimation. The FC layers share weights across time steps. The first FC layer in both the branches contains R nodes each with linear activation. The last FC layer in the SED branch consists of N nodes with sigmoid activation, each corresponding to one of the N sound event classes to be detected. The use of sigmoid activation enables multiple classes to be active simultaneously. The last FC layer in the DOA branch consists of 3N nodes with tanh activation, where each of the N sound event classes is represented by 3 nodes corresponding to the sound event location in x, y, and z, respectively. For a DOA estimate on a unit sphere centered at the origin, the range of location along each axes is [-1, 1], thus we use the tanh activation for these regressors to keep the output of the network in a similar range. We refer to the above architecture as SELDnet. The SED output of the SELDnet is in the continuous range of [0, 1] for each class, while the DOA output is in the continuous range of [-1, 1] for each axes of the sound class location. A sound event is said to be active, and its respective DOA estimate is chosen if the SED output exceeds the threshold of 0.5 as shown in Figure <ref type="figure" target="#fig_0">1b</ref>. The network hyperparameters are optimized based on cross-validation as explained in Section III-D1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training procedure</head><p>In each frame, the target values for each of the active sound events in the SED branch output are one while the inactive events are zero. Similarly, for the DOA branch, the reference DOA x, y, and z values are used as targets for the active sound events and x = 0, y = 0, and z = 0 is used for inactive events. A binary cross-entropy loss is used between the SED predictions of SELDnet and reference sound class activities, while a mean square error (MSE) loss is used for the DOA estimates of the SELDnet and the reference DOA. By using the MSE loss for DOA estimation in 3D Cartesian coordinates we truly represent the distance between two points in space. The distance between two points (x 1 , y 1 , z 1 ) and (x 2 , y 2 , z 2 ) in 3D space is given by</p><formula xml:id="formula_0">√ SE, where SE = (x 1 -x 2 ) 2 +(y 1 - y 2 ) 2 + (z 1 -z 2 ) 2 ,</formula><p>while the MSE between the same points is given by SE/3. Thus the MSE loss is simply a scaled version of the distance in 3D space, and reducing the MSE loss implies the reduction in the distance between the two points.</p><p>Theoretically, the advantage of using Cartesian coordinates instead of azimuth and elevation for regression can be observed when predicting DOA in full azimuth and/or full elevation. The angles are discontinuous at the wraparound boundary (for example the -180°, 180°boundary for azimuth), while the Cartesian coordinates are continuous. This continuity allows the network to learn better. Further experiments on this are discussed in Section III-D.</p><p>We train the SELDnet with a weighted combination of MSE and binary cross-entropy loss for 1000 epochs using Adam optimizer with default parameters as used in the original paper <ref type="bibr" target="#b54">[55]</ref>. Early stopping is used to control the network from over-fitting to training split. The training is stopped if the SELD score (Section III-C) on the test split does not improve for 100 epochs. The network was implemented using Keras library <ref type="bibr" target="#b55">[56]</ref> with TensorFlow <ref type="bibr" target="#b56">[57]</ref> backend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The proposed SELDnet is evaluated on seven datasets that are summarized in Table <ref type="table" target="#tab_0">II</ref>. Four of the datasets are synthesized with artificial impulse responses (IR), that consists of anechoic and reverberant scenarios virtually recorded both with a circular array and in the Ambisonics format. Three of the datasets are synthesized with real-life impulse responses, recorded with a spherical array and encoded into the Ambisonics format. All the datasets consist of stationary point sources each associated with a spatial coordinate. The synthesis procedure in all the datasets consists of mixing isolated sound event instances at different spatial locations, since this allows producing the reference event locations and times of activity for evaluation and training of the methods.</p><p>1) TUT Sound Events 2018 -Ambisonic, Anechoic and Synthetic Impulse Response (ANSYN) dataset: This dataset consists of spatially located sound events in an anechoic environment synthesized using artificial IRs. It comprises three subsets: no temporally overlapping sources (O1), maximum two temporally overlapping sources (O2) and maximum three temporally overlapping sources (O3). Each of the subsets consists of three cross-validation splits with 240 training and 60 testing FOA format recordings of length 30 s sampled at 44100 Hz. The dataset is generated using the 11 isolated sound event classes from the DCASE 2016 task 2 dataset <ref type="bibr" target="#b57">[58]</ref> such as speech, coughing, door slam, page-turning, phone ringing and keyboard. Each of these sound classes has 20 examples, of which 16 are randomly chosen for the training set and the rest four for the testing set, amounting to 176 examples from 11 classes for training, and 44 for testing. During synthesis of a recording, a random collection of examples are chosen from the respective set and are randomly placed in a spatial grid of 10°resolution along azimuth and elevation, such that two overlapping sound events are separated by 10°, and the elevation is in the range of [-60°, 60°). In order to have a variability of amplitude, the sound events are randomly placed at a distance ranging from 1 to 10 m with 0.5 m resolution from the microphone. More details regarding the synthesis can be found in <ref type="bibr" target="#b24">[25]</ref>.</p><p>2) TUT Sound Events 2018 -Ambisonic, Reverberant and Synthetic Impulse Response (RESYN) dataset: This dataset is synthesized with the same details as the ambisonic ANSYN dataset, with the only difference being that the sound events are spatially placed within a room using the image source method <ref type="bibr" target="#b58">[59]</ref>. Specifically, the microphone is placed at the center of the room, and the sound events are randomly placed around the microphone, with their distance ranging from 1 m from the microphone to the respective end of the room at 0.5 m resolution. The three cross-validation splits of each of the three subsets O1, O2 and O3 are generated for a moderately reverberant room of size 10 × 8 × 4 m (Room 1), with reverberation times 1.0, 0.8, 0.7, 0.6, 0.5, and 0.4 s per each octave band, and 125 Hz-4 kHz band center frequencies. Additionally, to study the performance in mismatched reverberant scenarios, testing splits are generated for two different sized rooms: room 2 that is 80% the volume (8 × 8 × 4 m) and reverberation-time per band of room 1, and room 3 that is 125% the volume (10 × 10 × 4 m) and reverberation-time per band of room 1. In order to remove any ambiguity while comparing the performance difference of room 1 with room 2 and 3, we keep the sound events and their respective spatial locations in room 2 and 3 identical to the testing split of room 1. But the individual sound events whose distance from the microphone exceeded the room size were reassigned a new distance within the room. Further details on the reverberant synthesis can be read in <ref type="bibr" target="#b24">[25]</ref>.</p><p>3) TUT Sound Events 2018 -Ambisonic, Reverberant and Real-life Impulse Response (REAL) dataset: In order to study the performance of SELDnet in a real-life scenario, we generated a dataset by collecting impulse responses from a real environment using the Eigenmike<ref type="foot" target="#foot_4">4</ref> spherical microphone array. For the IR acquisition, we used a continuous measurement signal as in <ref type="bibr" target="#b59">[60]</ref>. The measurement was done by slowly moving a Genelec G Two loudspeaker <ref type="foot" target="#foot_5">5</ref> continuously playing a maximum length sequence around the array in circular trajectory in one elevation at a time, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. The playback volume was set to be 30 dB greater than the ambient sound level. The recording was done in a corridor inside the university with classrooms around it.</p><p>The moving-source IRs were obtained by a freely available tool from CHiME challenge <ref type="bibr" target="#b60">[61]</ref> which estimates the timevarying responses in STFT domain by forming a least-squares regression between the known measurement signal and the far-field recording independently at each frequency. The IR for any azimuth within one trajectory can be analyzed by assuming block-wise stationarity of acoustic channel. The average angular speed of the loudspeaker in the measurements was 6°/s and we used a block size of 860 ms (81 STFT frames with analysis frame size of 1024 with 50 % overlap and sample rate F s = 48 kHz) for estimation of IR of length 170 ms (16 STFT frames).</p><p>The IRs were collected at elevations -40°to 40°with 10°i ncrements at 1 m from the Eigenmike and at elevations -20°t o 20°with 10°increments at 2 m. For the dataset creation, we analyzed the DOA of each time frame using MUSIC and extracted IRs for azimuthal angles at 10°resolution (36 IRs for each elevation). The IR estimation tool <ref type="bibr" target="#b60">[61]</ref> was applied independently on all 32 channels of the Eigenmike.</p><p>In order to synthesize the sound scene from the estimated IRs, we used isolated real-life sound events from the urban-sound8k dataset <ref type="bibr" target="#b61">[62]</ref>. This dataset consists of 10 sound event classes such as: air conditioner, car horn, children playing, dog barking, drilling, engine idling, gunshot, jackhammer, siren and street music. Among these, we did not include children playing and air conditioner classes since these can also occur in our ambiance recording which we use as background recording in dataset REALBIGAMB (Section III-A5). From the sound examples in urbansound8k, we only used the ones marked as foreground in order to have clean isolated sound events. Similarly to the other datasets used in this paper, we used the splits 1, 8 and 9 provided in the urbansound8k as the three cross-validation splits. These splits were chosen as they had a good number of examples for all the chosen sound event classes after selecting only the foreground examples. The final selected examples varied in length from 100 ms to 4 s and amount to 15671.5 seconds from 4542 examples.</p><p>During the sound scene synthesis, we randomly chose a sound event example and associated it with a random distance among the collected ones, azimuth and elevation angle. The sound event example was then convolved with the respective IR for the given distance, azimuth and elevation to spatially position it. Finally, after positioning all the sound events in a recording we converted this multichannel audio to FOA format. The transform of the microphone signals to FOA was performed using the tools published in <ref type="bibr" target="#b62">[63]</ref>. In total, we generated 300 such 30 s recordings in a similar fashion as ANSYN and RESYN with 240 of them earmarked for training and 60 for testing. Similar to the ANSYN recordings we also generated three subsets O1, O2 and O3 with a different number of overlapping sound events.</p><p>4) TUT Sound Events 2018 -Ambisonic, Reverberant and Real-life Impulse Response big (REALBIG) dataset: In order to study the performance of SELDnet with respect to the size of the dataset, we generated for each of three ambisonic REAL subsets a 750 recordings REALBIG subset of 30 s length, with 600 for training and 150 for testing.</p><p>5) TUT Sound Events 2018 -Ambisonic, Reverberant, Reallife Impulse Response and Ambiance big (REALBIGAMB) dataset: Additionally, to simulate a real sound-scene we recorded 30 min of ambient sound to use as background noise in the same location as the IR recordings without changing the setup. We mixed randomly chosen segments of the recorded ambiance at three different SNRs: 0, 10 and 20 dB for each  <ref type="bibr" target="#b19">[20]</ref> (Section III-B3), we synthesized the ANSYN recordings for a circular array of radius 5 cm with eight omnidirectional microphones at 0, 45, 90, 135, 180, 225, 270, 315 o , and the array plane parallel to the ground, and refer to it as CANSYN. It is an exact replica of the ANSYN dataset in terms of the synthesized sound events except for the microphone array setup, and hence the number of channels. Similar to ANSYN, the CANSYN dataset has three subsets with a different number of overlapping sound events each with three cross-validation splits.</p><p>7) TUT Sound Events 2018 -Circular array, Reverberant and Synthetic Impulse Response (CRESYN) dataset: Similar to the CANSYN dataset, we synthesize the circular array version of ambisonic RESYN room 1 dataset, referred as CRESYN. During synthesis, the circular microphone array is placed in the center of the room, and the array plane parallel to the floor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline methods</head><p>The SELDnet is compared with six different as summarized in Table <ref type="table" target="#tab_0">III</ref>: two SED baselines (single-and multichannel), three DOA baselines (parametric and DNNbased), and a SELD baseline.</p><p>1) SED baseline: The SED capabilities of the proposed SELDnet is compared with the existing state-of-the-art multichannel SED method <ref type="bibr" target="#b38">[39]</ref>, referred here as MSEDnet. MSEDnet is easily scalable to any number of input audio channels and won <ref type="bibr" target="#b37">[38]</ref> the recently concluded real-life SED task in DCASE 2017 <ref type="bibr" target="#b63">[64]</ref>. In particular, it won the top two positions among 34 submissions, first using single-channel mode (referred as SEDnet) and a close second using multichannel mode. The SED performance of SELDnet is compared with both the single-and the multichannel modes of MSEDnet.</p><p>In the original MSEDnet implementation <ref type="bibr" target="#b38">[39]</ref> the input is a sequence of log mel-band energy (40-bands) frames, that are mapped to an equal-length sequence of sound event activities. The SED metrics (Section III-C) for MSEDnet did not change much on using phase and magnitude components of the STFT spectrogram instead of log mel-band energies. Hence, in order to have a one-to-one comparison with SELDnet, we use the phase and magnitude components of the STFT spectrogram for MSEDnet in this paper. We train the MSEDnet for 500 epochs and use early stopping when SED score (Section III-C) stops improving for 100 epochs.</p><p>2) DOA baseline: The DOA estimation performance of the SELDnet is evaluated with respect to three baselines. As a parametric baseline, we use MUSIC <ref type="bibr" target="#b45">[46]</ref> and as DNN-based baselines, we use the recently proposed DOAnet <ref type="bibr" target="#b24">[25]</ref> that estimates DOAs in 3D and <ref type="bibr" target="#b17">[18]</ref> that estimates only the DOA azimuth angle referred as AZInet.</p><p>i) MUSIC: is a versatile high-resolution subspace method that can detect multiple narrowband source DOAs and can Single channel All MSEDnet <ref type="bibr" target="#b38">[39]</ref> Multichannel DOA MUSIC * Azi and ele All except CANSYN and CRESYN DOAnet <ref type="bibr" target="#b24">[25]</ref> Azi and ele AZInet <ref type="bibr" target="#b17">[18]</ref> Azi CANSYN and CRESYN SELD HIRnet <ref type="bibr" target="#b19">[20]</ref> Azi SELDnet-azi Azi All SELDnet Azi and ele * Parametric, all other methods are DNN based be applied to generic array setups. It is based on a subspace decomposition of the spatial covariance matrix of the multichannel spectrogram. For a broadband estimation of DOAs, we combine narrowband spatial covariance matrices over three frames and frequency bins from 50 to 8000 Hz. The steering vector information required to produce the MUSIC pseudospectrum from which the DOAs are extracted is adapted to the recording system under use, meaning uniform circular array steering vectors for CANSYN and CRESYN datasets, and real SH vectors for all the other ambisonic datasets.</p><p>MUSIC requires a good estimate of the number of active sound sources in order to estimate their DOAs. In this paper, we use MUSIC with the number of active sources taken from the reference of the dataset. Hence, the DOA estimation results of MUSIC can be considered as the best possible for the given dataset and serve as a benchmark for DOA estimation with and without the knowledge of the number of active sources. For a detailed description on MUSIC and other subspace methods, the reader is referred to <ref type="bibr" target="#b64">[65]</ref>, while for application of MUSIC to SH signals similar to this work, please refer to <ref type="bibr" target="#b65">[66]</ref>.</p><p>ii) DOAnet: Among the recently proposed DNN-based DOA estimation methods listed in Table <ref type="table" target="#tab_0">I</ref>, the only method that attempts DOA estimation of multiple overlapping sources in 3D space is the DOAnet <ref type="bibr" target="#b24">[25]</ref>. Hence, DOAnet serves as a suitable baseline to compare against the DOA estimation performance of the proposed SELDnet. DOAnet is based on a similar CRNN architecture, the input to which is a sequence of multichannel phase and magnitude spectrum frames. It considers DOA estimation as a multi-label classification task by directional sampling with a resolution of 10°along azimuth and elevation and estimating the likelihood of a sound source being active in each of these points.</p><p>iii) AZInet: Among the DOA-only estimation methods listed in Table <ref type="table" target="#tab_0">I</ref>, apart from the DOAnet <ref type="bibr" target="#b24">[25]</ref>, methods <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b3">[4]</ref> are the only ones which attempt simultaneous DOA estimation of overlapping sources. Since <ref type="bibr" target="#b3">[4]</ref> is evaluated on a dataset collected using microphones mounted on a humanoid robot, it is difficult to replicate the setup. Hence in this paper, we use the AZInet evaluated on a linear array in <ref type="bibr" target="#b17">[18]</ref> as the baseline. The AZInet is a CNN-based method that uses the phase component of the spectrogram of each channel as input, and maps it to azimuth angles in the range 0°to 180°at 5°r esolution as a multi-label classification task. AZInet uses only the phase spectrogram since the dataset evaluated on employs omnidirectional microphones, which for compact arrays and sources in the far-field, preserve directional information in inter-channel phase differences. Thus, although the evaluation in <ref type="bibr" target="#b17">[18]</ref> was carried out on a linear array, the method is generic to any omnidirectional array under these conditions. Further, in order to have a direct comparison, we extend the output of AZInet to full-azimuth with 10°resolution and reduce the output of SELDnet to generate only the azimuth, i.e., we only estimate x and y coordinates of the DOA (SELDnetazi). To enable this full-azimuth estimation we use the circular array with omnidirectional microphones datasets CANSYN and CRESYN.</p><p>3) SELD baseline (HIRnet): The joint SED and DOA estimation performance of SELDnet is compared with the method proposed by Hirvonen <ref type="bibr" target="#b19">[20]</ref>, hereafter referred to as HIRnet. The HIRnet was proposed for a circular array of omnidirectional microphones, hence we compare its performance only on the CANSYN and CRESYN datasets. HIRnet is a CNN-based network that uses the log-spectral power of each channel as the input feature and maps it to eight angles in full azimuth for each of the two classes (speech and music) as a multi-label classification task. More details about HIRnet can be found in <ref type="bibr" target="#b19">[20]</ref>. In order to have a direct comparison with SELDnet-azi, we extend HIRnet to estimate DOAs at a 10°r esolution for each of the sound event classes in our testing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation metrics</head><p>The proposed SELDnet is evaluated using individual metrics for SED and DOA estimation. For SED, we use the standard polyphonic SED metrics, error rate (ER) and F-score calculated in segments of one second with no overlap as proposed in <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>. The segment-wise results are obtained from the frame-level predictions of the classifier by considering the sound events to be active in the entire segment if it is active in any of the frames within the segment. Similarly, we obtain labels for one-second segments of reference from its framewise annotation, and calculate the segment-wise ER and Fscores. Mathematically, the F-score is calculated as follows:</p><formula xml:id="formula_1">F = 2 • K k=1 T P (k) 2 • K k=1 T P (k) + K k=1 F P (k) + K k=1 F N (k) ,<label>(1)</label></formula><p>where the number of true positives T P (k) is the total number of sound event classes that were active in both reference and predictions for the kth one-second segment. The number of false positives in a segment F P (k) is the number of sound event classes that were active in the prediction but were inactive in the reference. Similarly, F N (k) is the number of false negatives, i.e. the number of sound event classes inactive in the predictions but active in the reference.</p><p>The ER metric is calculated as</p><formula xml:id="formula_2">ER = K k=1 S(k) + K k=1 D(k) + K k=1 I(k) K k=1 N (k) ,<label>(2)</label></formula><p>where, for each one-second segment k, N (k) is the total number of active sound event classes in the reference. Substitution S(k) is the number of times an event was detected but given the wrong level, this is obtained by merging the false negatives and false positives without individually correlating which false positive substitutes which false negative. The remaining false positives and false negatives, if any, are counted as insertions I(k) and deletions D(k) respectively. These statistics are mathematically defined as follows:</p><formula xml:id="formula_3">S(k) = min(F N (k), F P (k)),<label>(3)</label></formula><formula xml:id="formula_4">D(k) = max(0, F N (k) -F P (k)),<label>(4)</label></formula><formula xml:id="formula_5">I(k) = max(0, F P (k) -F N (k)).</formula><p>(</p><formula xml:id="formula_6">)<label>5</label></formula><p>An SED method is jointly evaluated using the F-score and ER metric, and an ideal method will have an F-score of one (reported as percentages in Table ) and ER of zero. More details regarding the F-score and ER metric can be read in <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>.</p><p>The predicted DOA estimates (x E , y E , z E ) are evaluated with respect to the reference (x G , y G , z G ) used to synthesize the dataset, utilizing the central angle σ ∈ [0, 180]. The σ is the angle formed by (x E , y E , z E ) and (x G , y G , z G ) at the origin in degrees, and is given by</p><formula xml:id="formula_7">σ = 2 • arcsin ∆x 2 + ∆y 2 + ∆z 2 2 • 180 π ,<label>(6)</label></formula><p>where, ∆x = x G -x E , ∆y = y G -y E , and ∆z = z G -z E .</p><p>The DOA error for the entire dataset is then calculated as</p><formula xml:id="formula_8">DOA error = 1 D • D d=1 σ((x d G , y d G , z d G ), (x d E , y d E , z d E )) (7)</formula><p>where D is the total number of DOA estimates across the entire dataset, and σ((</p><formula xml:id="formula_9">x d G , y d G , z d G ), (x d E , y d E , z d E )</formula><p>) is the angle between d-th estimated and reference DOAs.</p><p>Additionally, in order to account for time frames where the number of estimated and reference DOAs are unequal, we report the frame recall, calculated as T P/(T P + F N ) in percentage, where true positives T P is the total number of time frames in which the number of DOAs predicted is equal to reference, and false negatives F N is the total number of frames where the predicted and reference DOA are unequal.</p><p>The DOA estimation method is jointly evaluated using the DOA error and the frame recall, and an ideal method has a frame recall of one (reported as percentages in Table ) and DOA error of zero.</p><p>During the training of SELDnet, we perform early stopping based on the combined SELD score calculated as</p><formula xml:id="formula_10">SELD score = (SED score + DOA score)/2,<label>(8)</label></formula><p>where</p><formula xml:id="formula_11">SED score = (ER + (1 -F ))/2,<label>(9)</label></formula><p>DOA score = DOA error/180 + (1 -f rame recall) /2, <ref type="bibr" target="#b9">(10)</ref> and an ideal SELD method will have an SELD score of zero. In the proposed method, the localization performance is dependent on the detection performance. This relation is represented by the frame recall metric of DOA estimation. As a consequence, the SELD score which is comprised of frame recall metric in addition to the SED metrics can be seen to weigh the SED performance more than DOA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments</head><p>The SELDnet is evaluated in different dimensions to understand its potential and drawbacks. The experiments carried out with different datasets in this regard are explained below.</p><p>1) SELDnet architecture and model parameter tuning: A wide variety of architectures with different combinations of CNN, RNN and FC layers are explored on the ANSYN O2 subset with frame length M = 1024 (23.2 ms). Additionally, for each architecture, we tune the model parameters such as the number of CNN, RNN, and FC layers (0 to 4) and nodes (in the set of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>). The input sequence length is tuned in the set of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>, the DOA and SED branch output loss weights in the set of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr">500]</ref>, the regularization (dropout in the set of [0, 0.1, 0.2, 0.3, 0.4, 0.5], L1 and L2 in the set of [0, 10 -1 ,10 -2 ,10 -3 ,10 -4 ,10 -5 ,10 -6 ,10 -7 ]) and the CNN max-pooling in the set of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref> for each layer. The best set of parameters are the ones which give the lowest SELD score on the three cross-validation splits of the dataset. After finding the best network architecture and configuration, we tune the input audio feature parameter M by varying it in the set of <ref type="bibr">[512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048]</ref>. Simultaneously the sequence length is also changed with respect to M such that the input audio length is kept constant (1.49 s obtained from the first round of tuning). We perform fine-tuning of model parameters for different M and sequence length values, this time only the number of CNN, RNN and FC nodes are tuned in a small range (neighboring nodes in the set of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>) to identify the optimum parameters. Similar fine-tuning is repeated for other datasets.</p><p>2) Selecting SELDnet output format: The output format for polyphonic SED in the literature has become standardized to estimating the temporal activity of each sound class using frame-wise binary numbers <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. On the other hand, the output formats for DOA estimation are still being experimented with as seen in Table <ref type="table" target="#tab_0">I</ref>. Among the DOA estimation methods using regression mode, there are two possible output formats, predicting azimuth and elevation, and predicting x, y, z coordinates of the DOA on the unit sphere. In order to identify the best output format among these two, we evaluate the SELDnet for both. During this evaluation, only the output weight parameter of the model is fine-tuned in the set of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr">500]</ref>. Additionally, for a regression-based model, the default output i.e. the DOA target when the event is not active should be chosen carefully. In this study, we chose the default DOA output to be 180°in azimuth and 60°in elevation (the datasets do not contain sound events for these DOA values), and x = 0, y = 0 and z = 0 for default Cartesian outputs. The chosen default Cartesian coordinates are equidistant from all the possible DOA values. On the other hand, there are no such equidistant azimuth and elevation values. Hence we chose the default values (180°, 60°) to be in a similar range as the true DOA values.</p><p>3) Continuous DOA estimation and performance on unseen DOA values: Theoretically, the advantage of using a regression-based DOA estimator over a classification-based one is that the network is not limited to a set of DOA angles, but it can operate as a high-resolution continuous DOA estimator. To study this, we train the SELDnet on ANSYN subsets whose sound events are placed on an angular grid of 10°resolution along azimuth and elevation, and test the model on a dataset where the angular grid is shifted by 5°along azimuth and elevation while keeping the temporal location unchanged. This shift makes the DOA values of the testing split unseen, and correctly predicting the DOAs will prove that the regression model can estimate the DOAs in a continuous space. Additionally, it also proves the robustness of the SELDnet to predict unseen DOA values.</p><p>4) Performance on mismatched reverberant dataset: Parametric DOA estimation methods are known to be sensitive to reverberation <ref type="bibr" target="#b47">[48]</ref>. In this regard, we first evaluate the performance of SELDnet on the simulated (RESYN), and real-life (REAL, REALBIG, and REALBIGAMB) reverberant datasets and further compare the results with the parametric baseline MUSIC.</p><p>DNN based methods are known to fail when the training and testing splits come from different domains. For example, the performance of a DNN trained on anechoic dataset would be poor on a reverberant testing dataset. This performance can only be improved by training the DNN on a similar reverberant dataset as the testing dataset. On the other hand, it is impractical to train such a DNN for every existing room-dimension, its surface material distribution, and the reverberation times associated with it. In this regard, it would be ideal if the proposed method is robust to a moderate mismatch in reverberant conditions so that a single model can be used for a range of comparable room configurations. Motivated by this, we study the sensitivity of SELDnet on moderately mismatched reverberant data. Specifically, we train the SELDnet with RESYN room 1 dataset and test it on RESYN room 2 and 3 datasets that are mismatched in terms of volume and reverberation times as described in Section III-A2.</p><p>5) Performance on the size of the dataset: We study the performance of SELDnet on two datasets, REAL, and REAL-BIG that are similar in content, but different in size.</p><p>6) Performance with ambiance at different SNR: The performance of SELDnet with respect to different SNRs (0, 10 and 20 dB) of the sound event is studied on the REAL-BIGAMB dataset.</p><p>7) Generic to array structure: SELDnet is a generic method that learns to localize and recognize sound events from any array structure. This additionally implies that the SELDnet will continue to work in the desired manner if the configuration of the array such as individual microphone response, microphone spacing and the number of microphones remains the same between the training and testing set. If the array configuration changes between the training and testing set, then the SELDnet will have to be retrained for the new array configuration.</p><p>In order to prove that the SELDnet is applicable to any array configuration and not just dependent on the Ambisonics format, SELDnet is evaluated on a circular array. In comparison to the Ambisonic format, the chosen circular array has a different number of microphones, each placed on a single plane, and with an omnidirectional response. Further, we compare the SELDnet performance with dataset compatible 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3. <ref type="bibr" target="#b4">5</ref>    baselines such as SEDnet, MSEDnet, HIRnet, and AZInet. Since the HIRnet and AZInet baselines methods are proposed for estimating azimuth only, we compare the results with the SELDnet-azi version. Additionally, we also report the performance of using SELDnet with DOA estimation in x, y, z axis on CANSYN and CRESYN datasets.</p><p>In general, for all our experiments the only difference between the training and testing splits is the mutually exclusive set of sound examples. Apart from experiment III-D3 the training and testing splits contains the same set of spatial locations i.e., azimuth and elevation angles at 10°resolution amounting to 468 spatial locations (= 36 azimuth angles * 13 elevation angles). But the distance of the sound event at each of this 468 spatial locations is an added variable. For example, in the anechoic case, a sound event can be placed anywhere between 1-10 m at 0.5 m resolution. This variable amounts to 8892 spatial locations (= 468 * 19 distance positions) that are being coarsely grouped to 468 locations. This complexity is stretched further in experiment III-D3 where the testing split sound event examples and their spatial locations both are different from the training split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) SELDnet architecture and model parameter tuning:</head><p>The SELD scores obtained with hyper-parameter tuning of different CNN, RNN, and CRNN configurations as explained in Section III-D1 are visualized with respect to the number of model parameters in Figure <ref type="figure">3</ref>. CNN in this figure refers to a SELDnet architecture which had no RNN layers but just CNN and FC layers. Similarly, RNN refers to SELDnet without CNN layers, while CRNN refers to SELDnet with CNN, RNN and FC layers. This experiment was carried out on ANSYN O2 dataset. The CRNN architecture was seen to perform the best followed by the RNN architecture.</p><p>The optimum model parameters across the ANSYN subsets after hyper-parameter tuning the CRNN architecture was found to have three layers of CNN with 64 nodes each (P in Figure <ref type="figure" target="#fig_0">1a</ref>), followed by two layers of GRU with 128 nodes each (Q in Figure <ref type="figure" target="#fig_0">1a</ref>), and one FC layer with 128 nodes (R in Figure <ref type="figure" target="#fig_0">1a</ref>). The max-pooling over frequency after each of the three CNN layers (M P i in Figure <ref type="figure" target="#fig_0">1a</ref>) was <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2)</ref>. This configuration had about 513,000 parameters.</p><p>Further, the SELDnet was seen to perform best with no regularization (dropout, or L1 or L2 regularization of weights). A frame length of M = 512 and sequence length of 256 frames was seen to give the best results across ANSYN subsets (Figure <ref type="figure">4</ref>). Furthermore, on tuning the sequence length with frame length fixed (M = 512), the best scores were obtained using 512 frames (2.97 s). Sequences longer than this could not be studied due to hardware restrictions. For the output weights, DOA output weighted 50 times more than SED output was seen to give the best results across subsets (Figure <ref type="figure" target="#fig_2">5</ref>).</p><p>On fine-tuning the SELDnet parameters obtained with AN-SYN dataset for RESYN subsets, the only parameter that helped improve the performance was using a sequence length of 256 instead of 512, leaving the total number of network parameters unchanged at 513,000. Similar configuration gave the best results for CANSYN and CRESYN datasets.</p><p>Model parameters identical to ANSYN dataset were observed to perform the best on the REAL subsets. The same parameters were also used for the study of REALBIG and REALBIGAMB subsets.</p><p>2) Selecting SELDnet output format: In the output data formats study, it was observed that using the Cartesian x, y, z format in place of azimuth/elevation angle was truly helping the network learn better across datasets as seen in Figure <ref type="figure" target="#fig_3">6</ref>. This suggests that the discontinuity at the angle wrap-around boundary actually reduces the performance of DOA estimation and hence the SELD score. 3) Continuous DOA estimation and performance on unseen DOA values: The input and outputs of SELDnet trained on ANSYN O1 and O2 subsets for a respective 1000 frame test sequence are visualized in Figure <ref type="figure">7</ref>. The Figure represents each sound event class and its associated DOA outputs with a unique color. In the case of ANSYN O1, we see that the network predictions of SED and DOA are almost perfect. In the case of unseen DOA values (× markers), the network predictions continue to be accurate. This shows that the regression mode output format helps the network learn continuous DOA values, and further that the network is robust to unseen DOA values. In case of ANSYN O2, the SED predictions are accurate, while the DOA estimates, in general, are seen to vary around the respective mean reference value. In this work, the DOA and SED labels for a single sound event instance are considered to be constant for the entire duration even though the instance has inherent magnitude variations and silences within. From Figure <ref type="figure">7b</ref> it seems that these variations and silences are leading to fluctuating DOA estimates, while the SED predictions are unaffected. In general, we see that the proposed method successfully recognizes, localizes in time and space, and tracks multiple overlapping sound events simultaneously.</p><p>Table IV presents the evaluation metric scores for the SELDnet and the baseline methods with ANSYN and RESYN datasets. In the SED metrics for the ANSYN datasets, the SELDnet performed better than the best baseline MSEDnet for O1 subset while MSEDnet performed slightly better for O2 and O3 subsets. With regard to DOA metrics, the SELDnet is significantly better than the baseline DOAnet in terms of frame recall. This improvement in frame recall is a direct result of using SED output as a confidence measure for estimating DOA, thereby extending state-of-the-art SED performance to SELD. Although the frame recall of DOAnet is poor, its DOA error for O1 and O2 subsets is observed to be lower than SELDnet. The DOA error of the parametric baseline MUSIC with the knowledge of the number of sources is seen to be the best among the evaluated methods for O2 and O3 subsets.  <ref type="figure">7</ref>. SELDnet input and outputs visualized for ANSYN O1 and O2 datasets. The horizontal-axis of all sub-plots for a given dataset represents the same time frames, the vertical-axis for spectrogram sub-plot represents the frequency bins, vertical-axis for SED reference and prediction sub-plots represents the unique sound event class identifier, and for the DOA reference and prediction sub-plots, it represents the distance from the origin along the respective axes. The bold lines visualize both the reference labels and predictions of DOA and SED for ANSYN O1 and O2 datasets, while the × markers in Figure <ref type="figure">7a</ref> visualize the results for testing split with unseen DOA values (shifted by 5°along azimuth and elevation).  higher frame recall, the DOAnet has better DOA error, the MUSIC performs poorly, and the SED metrics of SELDnet are comparable to MSEDnet. These results prove that the SELDnet is robust to reverberation in comparison to baseline methods and performs seamlessly on moderately mismatched room configurations. Figure <ref type="figure" target="#fig_5">8</ref> visualizes the confusion matrices for the estimated number of sound event classes per frame by SELDnet. For example in Figure <ref type="figure" target="#fig_5">8c</ref> the SELDnet correctly estimated the number of sources to be two in 76% (true positive percentage) of the frames which had two sources in the reference. In context, the frame recall value used as a metric to evaluate DOA estimation represents this confusion matrix in one number. From the confusion matrices, we observe that the percentage of true positives drops with higher number of sources, and this drop is even more significant in the reverberant scenario. But, in comparison to the frame recall metric of the baseline DOAnet in Table <ref type="table" target="#tab_7">IV</ref>, the SELDnet frame recall is significantly better for higher number of overlapping sound events, especially in the reverberant conditions.</p><p>5) Performance on the size of the dataset: The overall performance of SELDnet on REAL dataset (Table <ref type="table" target="#tab_8">V</ref>) reduced in comparison to ANSYN and RESYN datasets. The baseline MSEDnet is seen to perform better than SELDnet in terms of SED metrics. Similar performance drop on real-life datasets has also been reported on SED datasets in other studies <ref type="bibr" target="#b36">[37]</ref>. With regard to DOA metrics, the frame recall of SELDnet continues to be significantly better than DOAnet, while the DOA error of DOAnet is lower than SELDnet. The performance of MUSIC is seen to be poor in comparison to both DOAnet and SELDnet. With the larger REALBIG dataset the SELDnet performance was seen to improve. A similar study was done with larger ANSYN and RESYN datasets, where the results were comparable with that of smaller datasets. This shows that the datasets with real-life IR are more complicated than synthetic IR datasets, and having larger real-life datasets helps the network learn better.</p><p>6) Performance with ambiance at different SNR: In presence of ambiance, SELDnet was seen to be robust for 10 and 20 dB SNR REALBIGAMB datasets (Table <ref type="table" target="#tab_8">V</ref>). In comparison to the SED metrics of REALBIG dataset with no ambiance, the SELDnet performance on O1 subsets of 10 dB and 20 dB ambiance is comparable, while a small drop in performance was observed with the respective O2 and O3 subsets. Whereas, the performance was seen to drop considerably for the 0 dB SNR dataset. With respect to DOA error, the SELDnet performed better than MUSIC but poorer than DOAnet across datasets, on the other hand, SELDnet gave significantly higher frame recall than DOAnet. From the insight of SELDnet performance on REAL dataset (Section IV-5), the more complex the acoustic scene the larger the dataset size required to learn better. Considering that the SELDnet is jointly estimating the DOA along with SED in a challenging acoustic scene with ambiance the SELDnet performance can potentially improve with larger datasets. 7) Generic to array structure: The results on circular array datasets are presented in Table <ref type="table" target="#tab_9">VI</ref>. With respect to SED metrics, the SELDnet-azi performance is seen to be better than the best baseline MSEDnet for all subsets of CRESYN dataset, while MSEDnet is seen to perform better for O2 and O3 subsets of CANSYN dataset. Similarly, in the case of DOA metrics, the SELDnet-azi has better frame recall than the best baseline method AZInet across datasets (except for CANSYN O1). Whereas, AZInet has lower DOA error than SELDnetazi. Between SELDnet and SELDnet-azi, even though the frame recall is in the same order the DOA error of SELDnetazi are lower than SELDnet. This shows that estimating DOA in 3D (x, y, z) is challenging using a circular array. Overall, the SELDnet is shown to perform consistently across different array structures (Ambisonic and circular array), with good results in comparison to baselines.</p><p>The usage of SED output as a confidence measure for estimating the number of DOAs in the proposed SELDnet is shown to improve the frame recall significantly and consistently across the evaluated datasets. On the other hand, the DOA error obtained with SELDnet is consistently higher than the classification based baseline DOA estimation methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. We believe that this might be a result of the regressionbased DOA estimation approach in SELDnet not having completely learned the full mapping between input feature and the continuous DOA space. The investigation of which is planned for future work. In general, a classification only or a classification-regression based SELD approach can be chosen based on the required frame recall, DOA error, resolution of DOA labels, training split size, and robustness to unseen DOA values and reverberation.</p><p>V. CONCLUSION In this paper, we proposed a convolutional recurrent neural network (SELDnet) to simultaneously recognize, localize and track sound events with respect to time. The localization is done by estimating the direction of arrival (DOA) on a unit sphere around the microphone using 3D Cartesian coordinates. We tie each sound event output class in the SELDnet to three regressors to estimate the respective Cartesian coordinates. We show that using regression helps estimating DOA in a continuous space, and also estimating unseen DOA values accurately. On the other hand, estimating a single DOA for each sound event class does not allow recognizing multiple instances of the same class overlapping. We plan to tackle this problem in our future work.</p><p>The usage of SED output as a confidence measure to estimate DOA was seen to extend the state-of-the-art SED performance to SELD resulting in a higher recall of DOAs. With respect to the estimated DOA error, although the classification based baseline methods had poor recall they had lower DOA error in comparison to the proposed regression based DOA estimation. The proposed SELDnet uses phase and magnitude spectrogram as the input feature. The usage of such non-method-specific feature makes the method generic and easily extendable to different array structures. We prove this by evaluating on datasets of Ambisonic and circular array format. The proposed SELDnet is shown to be robust to reverberation, low SNR scenarios and unseen rooms with comparable roomsizes. Finally, the overall performance on dataset synthesized using real-life impulse response (IR) was seen to drop in comparison to artificial IR dataset, suggesting the need for real-life training datasets and more powerful classifiers in future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. a) The proposed SELDnet and b) the frame-wise output for frame t in Figure a). A sound event is said to be localized and detected when the confidence of the SED output exceeds the threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Recording real-life impulse responses for sound scene generation. A person walks around the Eigenmike 4 holding a Genelec loudspeaker 5 playing a maximum length sequence at different elevation angles and distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. SELD score for ANSYN datasets with respect to different weights for DOA output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. SELD score for ANSYN datasets with respect to DOA output formats.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig.7. SELDnet input and outputs visualized for ANSYN O1 and O2 datasets. The horizontal-axis of all sub-plots for a given dataset represents the same time frames, the vertical-axis for spectrogram sub-plot represents the frequency bins, vertical-axis for SED reference and prediction sub-plots represents the unique sound event class identifier, and for the DOA reference and prediction sub-plots, it represents the distance from the origin along the respective axes. The bold lines visualize both the reference labels and predictions of DOA and SED for ANSYN O1 and O2 datasets, while the × markers in Figure7avisualize the results for testing split with unseen DOA values (shifted by 5°along azimuth and elevation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Confusion matrix for the number of sound event classes estimated to be active per frame by the SELDnet for ANSYN and RESYN datasets. The horizontal axis represents the SELDnet estimate, and the vertical axis represents the reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF DNN BASED DOA ESTIMATION METHODS IN THE LITERATURE. THE AZIMUTH AND ELEVATION ANGLES ARE DENOTED AS 'AZI' AND 'ELE', DISTANCE AS 'DIST', 'X' AND 'Y' REPRESENT THE DISTANCE ALONG THE RESPECTIVE CARTESIAN AXIS. 'FULL' REPRESENTS THE ESTIMATION IN THE COMPLETE RANGE OF THE RESPECTIVE FORMAT, AND 'REGRESSION' REPRESENTS THE CLASSIFIER ESTIMATION TYPE.</figDesc><table><row><cell>Approach</cell><cell>Input feature</cell><cell>Output format</cell><cell>Sources</cell><cell>DNN</cell><cell>Array</cell><cell>SELD</cell></row><row><cell>Chakrabarty et al. [17, 18]</cell><cell>Phase spectrum</cell><cell>azi</cell><cell>1, multiple</cell><cell>CNN</cell><cell>Linear</cell><cell>×</cell></row><row><cell>Yalta et al. [3]</cell><cell>Spectral power</cell><cell>azi (Full)</cell><cell>1</cell><cell>CNN Resnet</cell><cell>Robot</cell><cell>×</cell></row><row><cell>Xiao et al. [19]</cell><cell>GCC</cell><cell>azi (Full)</cell><cell>1</cell><cell>FC</cell><cell>Circular</cell><cell>×</cell></row><row><cell>Takeda et al. [1, 2]</cell><cell>Eigen vectors of spatial covariance matrix</cell><cell>azi (Full)</cell><cell>1, 2</cell><cell>FC</cell><cell>Robot</cell><cell>×</cell></row><row><cell>He et al. [4]</cell><cell>GCC</cell><cell>azi (Full)</cell><cell>Multiple</cell><cell>CNN</cell><cell>Robot</cell><cell>×</cell></row><row><cell>Hirvonen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSTSP.2018.2885636, IEEE Journal of Selected Topics in Signal Processing of the three ambisonic REALBIG subsets and refer to it as REALBIGAMB subsets. The ambiance used for the testing set was kept separate from the training set. 6) TUT Sound Events 2018 -Circular array, Anechoic and Synthetic Impulse Response (CANSYN) dataset: To study the performance of SELDnet on generic array configurations, similarly to the SELD baseline method</figDesc><table /><note><p>1932-4553 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSTSP.2018.2885636, IEEE Journal of Selected Topics in Signal Processing IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. X, X 20XX</figDesc><table /><note><p>1932-4553 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV SED</head><label>IV</label><figDesc>AND DOA ESTIMATION METRICS FOR ANSYN AND RESYN DATASETS. THE RESULTS FOR THE RESYN ROOM 2 AND 3 TESTING SPLITS WERE OBTAINED FROM CLASSIFIERS TRAINED ON RESYN ROOM 1 TRAINING SET. BEST SCORES FOR SUBSETS IN BOLD.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ANSYN</cell><cell></cell><cell cols="3">RESYN Room 1</cell><cell cols="3">RESYN Room 2</cell><cell cols="3">RESYN Room 3</cell></row><row><cell></cell><cell>Overlap</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>SED metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SELDnet</cell><cell>ER</cell><cell>0.04</cell><cell>0.16</cell><cell>0.19</cell><cell>0.10</cell><cell>0.29</cell><cell>0.32</cell><cell>0.11</cell><cell>0.33</cell><cell>0.35</cell><cell cols="2">0.13 0.32</cell><cell>0.34</cell></row><row><cell></cell><cell>F</cell><cell>97.7</cell><cell>89.0</cell><cell>85.6</cell><cell>92.5</cell><cell>79.6</cell><cell>76.5</cell><cell>91.6</cell><cell>79.5</cell><cell>75.8</cell><cell cols="2">89.8 79.1</cell><cell>75.5</cell></row><row><cell>MSEDnet [39]</cell><cell>ER</cell><cell>0.10</cell><cell>0.13</cell><cell>0.17</cell><cell>0.17</cell><cell>0.28</cell><cell>0.29</cell><cell>0.19</cell><cell>0.30</cell><cell>0.26</cell><cell cols="2">0.18 0.29</cell><cell>0.30</cell></row><row><cell></cell><cell>F</cell><cell>94.4</cell><cell>90.1</cell><cell>87.2</cell><cell>89.1</cell><cell>79.1</cell><cell>75.6</cell><cell>88.3</cell><cell>78.2</cell><cell>74.2</cell><cell cols="2">86.5 80.5</cell><cell>76.1</cell></row><row><cell>SEDnet [39]</cell><cell>ER</cell><cell>0.14</cell><cell>0.16</cell><cell>0.18</cell><cell>0.18</cell><cell>0.28</cell><cell>0.30</cell><cell>0.19</cell><cell>0.32</cell><cell>0.28</cell><cell>0.21</cell><cell>0.32</cell><cell>0.33</cell></row><row><cell></cell><cell>F</cell><cell>91.9</cell><cell>89.1</cell><cell>86.7</cell><cell>88.2</cell><cell>76.9</cell><cell>74.1</cell><cell>87.6</cell><cell>76.4</cell><cell>73.2</cell><cell>85.1</cell><cell>78.2</cell><cell>75.6</cell></row><row><cell>DOA metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SELDnet</cell><cell>DOA error</cell><cell>3.4</cell><cell>13.8</cell><cell>17.3</cell><cell>9.2</cell><cell>20.2</cell><cell>26.0</cell><cell>11.5</cell><cell>26.0</cell><cell>33.1</cell><cell>12.1</cell><cell cols="2">25.4 31.9</cell></row><row><cell></cell><cell>Frame recall</cell><cell>99.4</cell><cell>85.6</cell><cell>70.2</cell><cell>95.8</cell><cell>74.9</cell><cell>56.4</cell><cell>96.2</cell><cell>78.9</cell><cell>61.2</cell><cell>95.9</cell><cell>78.2</cell><cell>60.7</cell></row><row><cell>DOAnet [25]</cell><cell>DOA error</cell><cell>0.6</cell><cell>8.0</cell><cell>18.3</cell><cell>6.3</cell><cell>11.5</cell><cell>38.4</cell><cell>3.4</cell><cell>6.9</cell><cell>-</cell><cell>4.6</cell><cell cols="2">10.9 -</cell></row><row><cell></cell><cell>Frame recall</cell><cell>95.4</cell><cell>42.7</cell><cell>1.8</cell><cell>59.3</cell><cell>15.8</cell><cell>1.2</cell><cell>46.2</cell><cell>14.3</cell><cell>-</cell><cell>49.7</cell><cell>14.1</cell><cell>-</cell></row><row><cell>MUSIC</cell><cell>DOA error</cell><cell>4.1</cell><cell>7.2</cell><cell>15.8</cell><cell>40.2</cell><cell>47.1</cell><cell>50.5</cell><cell>45.7</cell><cell>58.1</cell><cell>74.0</cell><cell>48.3</cell><cell>60.6</cell><cell>75.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V SED</head><label>V</label><figDesc>AND DOA ESTIMATION METRICS FOR REAL, REALBIG AND REALBIGAMB DATASETS. BEST SCORES FOR SUBSETS IN BOLD. From TableIVresults on RESYN room 1 subsets, we see that the performance of parametric method MUSIC is poor in comparison to SELDnet in reverberant conditions. The SELDnet is seen to perform significantly better than the baseline DOAnet in terms of frame recall, although the DOAnet has lower DOA error for O1 and O2 subsets. The SED metrics of SELDnet are comparable if not better than the best baseline performance of MSEDnet. Further, on training the SELDnet on room 1 dataset and testing on moderately mismatched reverberant room 2 and 3 datasets the SED and DOA metric trends remain similar to the results of room 1 testing split. That is, the SELDnet has</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>REAL</cell><cell></cell><cell></cell><cell>REALBIG</cell><cell></cell><cell cols="3">REALBIGAMB 20dB</cell><cell cols="3">REALBIGAMB 10dB</cell><cell cols="3">REALBIGAMB 0dB</cell></row><row><cell></cell><cell>Overlap</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>SED metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SELDnet</cell><cell>ER</cell><cell>0.40</cell><cell>0.49</cell><cell>0.53</cell><cell>0.37</cell><cell>0.42</cell><cell>0.50</cell><cell>0.34</cell><cell>0.46</cell><cell>0.52</cell><cell cols="2">0.37 0.49</cell><cell>0.52</cell><cell>0.46</cell><cell>0.58</cell><cell>0.59</cell></row><row><cell></cell><cell>F</cell><cell>60.3</cell><cell>53.1</cell><cell>51.1</cell><cell>65.4</cell><cell>61.5</cell><cell>56.5</cell><cell>65.6</cell><cell>58.5</cell><cell>55.0</cell><cell cols="2">66.3 55.4</cell><cell>53.3</cell><cell>57.9</cell><cell>48.6</cell><cell>49.0</cell></row><row><cell>MSEDnet [39]</cell><cell>ER</cell><cell>0.35</cell><cell>0.38</cell><cell>0.41</cell><cell>0.34</cell><cell>0.39</cell><cell>0.38</cell><cell>0.35</cell><cell>0.40</cell><cell>0.41</cell><cell cols="2">0.38 0.43</cell><cell>0.42</cell><cell>0.48</cell><cell>0.56</cell><cell>0.54</cell></row><row><cell></cell><cell>F</cell><cell>66.2</cell><cell>61.6</cell><cell>59.5</cell><cell>67.3</cell><cell>61.8</cell><cell>61.9</cell><cell>66.0</cell><cell>61.6</cell><cell>60.1</cell><cell cols="2">63.2 58.7</cell><cell>59.3</cell><cell>54.5</cell><cell>49.3</cell><cell>51.3</cell></row><row><cell>SEDnet [39]</cell><cell>ER</cell><cell>0.38</cell><cell>0.42</cell><cell>0.43</cell><cell>0.38</cell><cell>0.43</cell><cell>0.44</cell><cell>0.39</cell><cell>0.42</cell><cell>0.43</cell><cell>0.41</cell><cell>0.44</cell><cell>0.46</cell><cell>0.51</cell><cell>0.61</cell><cell>0.57</cell></row><row><cell></cell><cell>F</cell><cell>64.6</cell><cell>61.5</cell><cell>57.2</cell><cell>68.0</cell><cell>62.4</cell><cell>62.4</cell><cell>65.7</cell><cell>60.1</cell><cell>59.2</cell><cell>62.7</cell><cell>56.3</cell><cell>56.9</cell><cell>52.6</cell><cell>46.0</cell><cell>50.4</cell></row><row><cell>DOA metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SELDnet</cell><cell>DOA error</cell><cell>26.6</cell><cell>33.7</cell><cell>36.1</cell><cell>23.1</cell><cell>31.3</cell><cell>34.9</cell><cell>25.4</cell><cell>32.5</cell><cell>36.1</cell><cell>27.2</cell><cell>32.5</cell><cell>36.1</cell><cell>30.7</cell><cell>33.7</cell><cell>36.7</cell></row><row><cell></cell><cell>Frame recall</cell><cell>64.9</cell><cell>41.5</cell><cell>24.6</cell><cell>68.0</cell><cell>45.2</cell><cell>28.3</cell><cell>69.1</cell><cell>42.8</cell><cell>25.8</cell><cell>66.9</cell><cell>40.0</cell><cell>27.3</cell><cell>62.5</cell><cell>35.2</cell><cell>23.4</cell></row><row><cell>DOAnet [25]</cell><cell>DOA error</cell><cell>6.3</cell><cell>20.1</cell><cell>25.8</cell><cell>7.5</cell><cell>17.8</cell><cell>22.9</cell><cell>6.3</cell><cell>18.9</cell><cell>25.78</cell><cell>8.0</cell><cell>20.1</cell><cell>24.1</cell><cell>14.3</cell><cell>24.1</cell><cell>27.5</cell></row><row><cell></cell><cell>Frame recall</cell><cell>46.5</cell><cell>11.5</cell><cell>2.9</cell><cell>44.1</cell><cell>12.5</cell><cell>3.1</cell><cell>34.7</cell><cell>11.6</cell><cell>3.2</cell><cell>42.1</cell><cell>13.5</cell><cell>3.3</cell><cell>30.1</cell><cell>10.5</cell><cell>2.8</cell></row><row><cell>MUSIC</cell><cell>DOA error</cell><cell>36.3</cell><cell>49.5</cell><cell>54.3</cell><cell>35.8</cell><cell>49.6</cell><cell>53.8</cell><cell>54.5</cell><cell>56.1</cell><cell>61.3</cell><cell>51.6</cell><cell>54.5</cell><cell>62.6</cell><cell>41.9</cell><cell>47.5</cell><cell>62.3</cell></row><row><cell cols="7">4) Performance on mismatched reverberant dataset:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI SED</head><label>VI</label><figDesc>AND DOA ESTIMATION METRICS FOR CANSYN AND CRESYN DATASETS. BEST SCORES FOR SUBSETS IN BOLD.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CANSYN</cell><cell></cell><cell></cell><cell>CRESYN</cell><cell></cell></row><row><cell></cell><cell>Overlap</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>SED metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SELDnet</cell><cell>ER</cell><cell cols="2">0.11 0.18</cell><cell>0.19</cell><cell>0.13</cell><cell cols="2">0.22 0.30</cell></row><row><cell></cell><cell>F score</cell><cell>93.0</cell><cell cols="2">86.6 85.3</cell><cell>90.4</cell><cell cols="2">82.2 78.0</cell></row><row><cell>SELDnet-azi</cell><cell>ER</cell><cell cols="2">0.08 0.19</cell><cell>0.24</cell><cell>0.06</cell><cell cols="2">0.18 0.20</cell></row><row><cell></cell><cell>F score</cell><cell cols="2">94.7 87.5</cell><cell>83.8</cell><cell>96.3</cell><cell cols="2">87.9 85.6</cell></row><row><cell>MSEDnet [39]</cell><cell>ER</cell><cell cols="2">0.09 0.18</cell><cell>0.16</cell><cell>0.12</cell><cell cols="2">0.22 0.26</cell></row><row><cell></cell><cell>F score</cell><cell cols="2">94.6 89.0</cell><cell>86.7</cell><cell>92.7</cell><cell cols="2">83.7 80.7</cell></row><row><cell>SEDnet [39]</cell><cell>ER</cell><cell>0.15</cell><cell cols="2">0.21 0.20</cell><cell>0.18</cell><cell cols="2">0.26 0.25</cell></row><row><cell></cell><cell>F score</cell><cell>91.4</cell><cell cols="2">87.3 84.7</cell><cell>90.5</cell><cell cols="2">84.3 82.8</cell></row><row><cell>HIRnet [20]</cell><cell>ER</cell><cell>0.41</cell><cell cols="2">0.45 0.62</cell><cell>0.43</cell><cell cols="2">0.46 0.50</cell></row><row><cell></cell><cell>F score</cell><cell>60.0</cell><cell cols="2">54.9 58.8</cell><cell>59.3</cell><cell cols="2">60.2 58.6</cell></row><row><cell>DOA metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SELDnet</cell><cell>DOA error</cell><cell>29.5</cell><cell cols="2">31.3 34.3</cell><cell>28.4</cell><cell cols="2">33.7 41.0</cell></row><row><cell></cell><cell>Frame recall</cell><cell>97.9</cell><cell cols="2">78.8 67.0</cell><cell>96.4</cell><cell cols="2">75.7 60.7</cell></row><row><cell>SELDnet-azi</cell><cell>DOA error</cell><cell>7.5</cell><cell cols="2">14.4 19.6</cell><cell>5.2</cell><cell cols="2">13.2 18.4</cell></row><row><cell></cell><cell>Frame recall</cell><cell cols="2">98.0 82.1</cell><cell>66.2</cell><cell>98.5</cell><cell cols="2">82.3 70.6</cell></row><row><cell>HIRnet [20]</cell><cell>DOA error</cell><cell>5.2</cell><cell cols="2">16.3 33.0</cell><cell>7.4</cell><cell cols="2">18.6 43.3</cell></row><row><cell></cell><cell>Frame recall</cell><cell>60.2</cell><cell cols="2">35.9 18.4</cell><cell>56.9</cell><cell cols="2">20.5 10.7</cell></row><row><cell>AZInet [18]</cell><cell>DOA error</cell><cell>1.2</cell><cell>4.0</cell><cell>7.4</cell><cell>2.3</cell><cell>6.9</cell><cell>9.7</cell></row><row><cell></cell><cell>Frame recall</cell><cell cols="2">99.4 80.5</cell><cell>60.5</cell><cell>97.3</cell><cell cols="2">65.2 44.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.cs.tut.fi/sgn/arg/dcase2016/task-results-sound-event-detectionin-real-life-audio#system-characteristics</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>2 http://www.cs.tut.fi/sgn/arg/dcase2017/challenge/task-sound-eventdetection-in-real-life-audio-results#system-characteristics</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/sharathadavanne/seld-net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. X, X 20XX</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://mhacoustics.com/products</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>https://www.genelec.com/home-speakers/g-series-active-speakers</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The research leading to these results has received funding from the European Research Council under the European Unions H2020 Framework Program through ERC Grant Agreement 637422 EVERYSOUND. The authors also wish to acknowledge CSC-IT Center for Science, Finland</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sound source localization based on deep neural networks with directional activate function exploiting phase information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Komatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative multiple sound source localization based on deep neural networks using independent location model</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sound source localization using deep learning models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics and Mechatronics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for multiple speaker detection and localization</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Audio surveillance: A systematic review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Crocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sound based localization and identification in industrial environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hancke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Industrial Electronics Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection and localization of impulsive sound events for environmental noise assessment</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Wessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Der Eerden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Audio surveillance of roads: A system for detecting anomalous sounds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saggese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Strisciuglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Smart room: participant and speaker localization and identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hernanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-I. Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Voice source localization for automatic camera pointing system in videoconferencing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distant speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wölfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdonough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for distant speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-source acoustic event detection and localization: Online implementation in a smart-room</title>
		<author>
			<persName><forename type="first">T</forename><surname>Butko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Pla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Segura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nadeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Environmental sound recognition with time-frequency audio features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimating animal population density using passive acoustics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological reviews of the Cambridge Philosophical Society</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using automated recorders and occupancy models to monitor common forest birds across a large geographic region</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Callas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Wildlife Management</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Broadband DOA estimation using convolutional neural networks trained with noise signals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<imprint>
			<publisher>WASPAA</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-speaker localization using convolutional neural network trained with noise</title>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A learning-based approach to direction of arrival estimation in noisy and reverberant environments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification of spatial audio location and content using convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hirvonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio Engineering Society Convention 138</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distance estimation and localization of sound sources in reverberant conditions using deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yiwere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Engineering Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">22</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sound source localization in a multipath environment using convolutional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural network based algorithm for speaker localization in a multi-room environment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Vesperini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vecchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Principi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Squartini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indoor sound source localization with probabilisitic neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahardja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On sound source localization of speech signals using deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Roden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerlach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weinzierl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deutsche Jahrestagung für Akustik (DAGA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Acoustic event detection in real-life recordings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Polyphonic sound event detection using multi-label deep neural networks</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for polyphonic sound event detection in real life recordings</title>
		<author>
			<persName><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sound event detection in multichannel audio using spatial and harmonic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pertila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Detection and Classification of Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Duration-controlled LSTM for polyphonic sound event detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Virtual adversarial training and data augmentation for acoustic event detection with gated recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zöhrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust sound event recognition using convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Robust audio event recognition with 1-max pooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hertel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multichannel sound event detection using 3D convolutional neural networks for learning interchannel features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rare sound event detection using 1D convolutional recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Detection and Classification of Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks for polyphonic sound event detection</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A report on sound event detection with different binaural features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Detection and Classification of Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sound event detection using spatial features and convolutional recurrent neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pertilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Audio event detection using multiple-input convolutional neural network</title>
		<author>
			<persName><forename type="first">I.-Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Detection and Classification of Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sound event detection in multichannel audio LSTM network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Detection and Classification of Acoustic Scenes and Events (DCASE)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bidirectional gru for sound event detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">and Classification of Acoustic Scenes and Events (DCASE)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Acoustic event detection: SVMbased system and evaluation setup in CLEAR&apos;07</title>
		<author>
			<persName><forename type="first">A</forename><surname>Temko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nadeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-I</forename><surname>Biel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Technologies for Perception of Humans</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Real-time passive source localization: a practical linear-correction least-squares approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Elko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mersereati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A high-accuracy, low-latency technique for talker localization in reverberant environments using microphone arrays</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brandstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple emitter location and signal parameter estimation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Antennas and Propagation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ESPRIT-estimation of signal parameters via rotational invariance techniques</title>
		<author>
			<persName><forename type="first">R</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust localization in reverberant rooms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Dibiase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brandstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microphone Arrays</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Modal array signal processing: principles and applications of acoustic wavefield decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Teutsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">348</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust localization and tracking of simultaneous moving sound sources using beamforming and particle filtering</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rouat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="216" to="228" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multiple speaker tracking with the Factorial Von Mises-Fisher filter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Traa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sound-model-based acoustic source localization using distributed microphone arrays</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nadeu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Detection, classification and localization of acoustic events in the presence of background noise for acoustic surveillance of hazardous situations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lopatka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kotus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Czyzewsk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications Journal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">17</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Keras v2.0.8</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015-05">2015. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015-05">2015. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Sound event detection in synthetic audio</title>
		<author>
			<persName><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lafay</surname></persName>
		</author>
		<ptr target="https://archive.org/details/dcase2016task2traindev" />
		<imprint>
			<date type="published" when="2016-05">2016. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Image method for efficiently simulating small-room acoustics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Berkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">3D-continuous-azimuth acquisition of head-related impulse responses using multi-channel adaptive filtering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Enzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The third CHiME speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM-MM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Microphone array processing for parametric spatial audio techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Aalto University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">DCASE 2017 challenge setup: tasks, datasets and baseline system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diment</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Detection and Classification of Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exact and large sample maximum likelihood techniques for parameter estimation and detection in array processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Radar Array Processing</title>
		<title level="s">Springer Series in Information Sciences</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Acoustic analysis by spherical microphone array processing of room impulse responses</title>
		<author>
			<persName><forename type="first">D</forename><surname>Khaykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rafaely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Metrics for polyphonic sound event detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Datasets and evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Analysis of Sound Scenes and Events</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Virtanen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Plumbley</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ellis</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ch. 6. Sharath Adavanne received his M.Sc. degree in</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
