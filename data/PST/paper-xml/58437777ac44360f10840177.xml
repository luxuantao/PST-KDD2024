<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Social Etiquette: Human Trajectory Understanding In Crowded Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
							<email>arobicqu@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVGL</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
							<email>amirabs@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVGL</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
							<email>alahi@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVGL</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVGL</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Social Etiquette: Human Trajectory Understanding In Crowded Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B95FAE4325B8F1DD31B1DB436BB95D2</idno>
					<idno type="DOI">10.1007/978-3-319-46484-8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Trajectory forecasting</term>
					<term>Multi-target tracking</term>
					<term>Social Forces</term>
					<term>UAV</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans navigate crowded spaces such as a university campus by following common sense rules based on social etiquette. In this paper, we argue that in order to enable the design of new target tracking or trajectory forecasting methods that can take full advantage of these rules, we need to have access to better data in the first place. To that end, we contribute a new large-scale dataset that collects videos of various types of targets (not just pedestrians, but also bikers, skateboarders, cars, buses, golf carts) that navigate in a real world outdoor environment such as a university campus. Moreover, we introduce a new characterization that describes the "social sensitivity" at which two targets interact. We use this characterization to define "navigation styles" and improve both forecasting models and state-of-the-art multi-target tracking-whereby the learnt forecasting models help the data association step.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When pedestrians or bicyclists navigate their way through crowded spaces such as a university campus, a shopping mall or the sidewalks of a busy street, they follow common sense conventions based on social etiquette. For instance, they would yield the right-of-way at an intersection as a bike approaches very quickly from the side, avoid walking on flowers, and respect personal distance. By constantly observing the environment and navigating through it, humans have learnt the way other humans typically interact with the physical space as well as with the targets that populate such spaces e.g., humans, bikes, skaters, electric carts, cars, toddlers, etc. They use these learned principles to operate in very complex scenes with extraordinary proficiency.</p><p>Researchers have demonstrated that it is indeed possible to model the interaction between humans and their surroundings to improve or solve numerous computer vision tasks: for instance, to make pedestrian tracking more robust and accurate <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, to enable the understanding of activities performed by groups of individuals <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, to enable accurate prediction of target trajectories in future instants <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Most of the time, however, these approaches operate under restrictive assumptions whereby the type and number of interactions are limited or the testing environment is often contrived or artificial. Fig. <ref type="figure">1</ref>. We aim to understand human social navigation in a multi-class setting where pedestrians, bicyclists, skateboarders and carts (to name a few) share the same space. To that end, we have collected a new dataset with a quadcopter flying over more than 100 different crowded campus scenes.</p><p>In this paper, we argue that in order to learn and use models that allow mimicking, for instance, the remarkable human capability to navigate in complex and crowded scenes, the research community needs to have access to better data in the first place. To that end, we contribute a new large scale dataset that collects videos of various types of targets (not just pedestrians, but also bikes, skateboarders, cars, buses, golf carts) that navigate in a real world outdoor environment such as a university campus. Our dataset comprises of more than 100 different top-view scenes for a total of 20,000 targets engaged in various types of interactions. Target trajectories along with their target IDs are annotated which makes this an ideal testbed for learning and evaluating models for multitarget tracking, activity understanding and trajectory prediction at scale (see Figs. <ref type="figure">1</ref> and<ref type="figure" target="#fig_0">2</ref>).</p><p>Among all the problems discussed above, in this paper we are interested in evaluating techniques related to two classes of problems: (i) target trajectory forecasting -whereby the ability to comply to social etiquettes and common sense behavior is critical, (ii) Multi-Target Tracking (MTT) -whereby the learnt forecasting model is used to enhance tracking results. In particular, we believe that our new dataset creates the opportunity to generalize state-of-the-art methods for understanding human trajectory, and evaluate them on a more effective playground. For instance, two leading families of methods for target trajectory forecasting (Social Forces <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> and Gaussian Processes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>) have shown promising results on existing datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>; however, they have never been tested at scale and in real-world scenarios where multiple classes of targets are present (i.e., not just pedestrian but also cars, bikes, etc.) as part of a complex ecosystem of interacting targets.</p><p>In addition to evaluating state-of-the-art forecasting and tracking methods, in this paper we also introduce a novel characterization that describes the "social sensitivity" at which two targets interact. It captures both the preferred distance a target wants to preserve with respect to its surrounding as well as when (s)he decides to avoid other targets. Low values for the social sensitivity feature means that a target motion is not affected by other targets that are potentially interacting with it. High values for the social sensitivity feature means that the target navigation is highly dependent on the position of other targets. This characterization allows to define the "navigation style" targets follow in interacting with their surrounding. We obtain different classes of navigation styles by clustering trajectory samples in the social sensitivity space (see Fig. <ref type="figure">3</ref> for examples). This allows to increase the flexibility in characterizing various modalities of interactions -for instance, some pedestrians may look more aggressive while walking because they are in rush whereas others might show a milder behavior because they are just enjoying their walk. Navigation style classes are used to select the appropriate forecasting model to best predict targets' trajectories as well as improve multi-target tracking. We believe that the ability to model social sensitivity is a key step toward learning common sense conventions based on social etiquette for enhancing forecasting and tracking tasks.</p><p>We present an extensive experimental evaluation that compares various stateof-the-art methods on the newly proposed dataset, and demonstrates that our social sensitivity feature and the use of navigation style enable better prediction and tracking results than previous methods that assume that all the targets belong to the same class (i.e., follow the same navigation style).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>A large variety of methods has been proposed in the literature to describe, model and predict human behaviors in a crowded space. Here we summarize the most relevant methods for human trajectory forecasting and multi-target tracking.</p><p>Human trajectory forecasting. An exhaustive study of crowd analysis is introduced by Treuille et al. <ref type="bibr" target="#b18">[19]</ref>. Antonini et al. use the Discrete Choice Model to synthesize human trajectories in crowded scenes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Other methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref> use Gaussian Processes to forecast human trajectories. They avoid the problems associated with discretization and their generated motion paths are smooth. Unfortunately, they often assume that the location of the destination is known. More recently, a set of methods use Inverse Reinforcement Learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> whereby a reward (or cost) function is learnt that best explains the final decisions <ref type="bibr" target="#b24">[25]</ref>. While these techniques have shown to work extremely well in several applications <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>, they assume that all feature values are known and static during each demonstrated planning cycle. They have been used to mainly model human and static space interaction as opposed to the dynamic content.</p><p>The most popular method for multi-target trajectory forecasting remains the Social forces (SF) model by D. Hellbing and P. Molnar <ref type="bibr" target="#b14">[15]</ref>. Targets react to energy potentials caused by the interactions with other targets and static obstacles through forces (repulsion or attraction). The SF model has been extensively used in robotics <ref type="bibr" target="#b27">[28]</ref>, and in the context of target tracking <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. All these previous work use a single set of parameters to model multiple targets. We argue and show in the remainder of this paper that a single set of parameters is too limited to model all the navigation styles in complex crowded scenes when multiple classes of targets are present (pedestrians, bikers, skateboarders,...).</p><p>Multi-Target Tracking. Over the past decade, Multi-Target Tracking (MTT) algorithms have made great progress in solving the data association problem as a graph theoretic problem <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. Several methods have incorporated the Social Forces (SF) model to improve the motion prior <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Recently, Xiang et al. <ref type="bibr" target="#b38">[39]</ref> demonstrate the power of a strong appearance model over all these previous work. They reached state-of-the-art performance over the publicly available MTT challenge <ref type="bibr" target="#b39">[40]</ref>. In this work, we use their method and demonstrates the impact of our "social sensitivity" feature in crowded multi-class complex scenes. In the next sections, we first present our collected dataset. Then, we introduce our social sensitivity feature. In Sect. 5, we share details behind our forecasting and tracking model. Finally, we conclude with a detailed evaluation of our forecasting task, and its impact on the Multi-Target Tracking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Campus Dataset</head><p>We aim to learn the remarkable human capability to navigate in complex and crowded scenes. Existing datasets mainly capture the behavior of humans in spaces occupied by a single class of target, e.g., pedestrian-only scenes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. However, in practice, pedestrians share the spaces with other classes of targets such as bicyclists, or skateboarders to name a few. For instance, on university campuses, a large variety of these targets interacts at peak hours. We want to study social navigation in these complex and crowded scenes occupied by several classes of targets. Datasets such as <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> do contain multiple classes of objects but are either limited in the number of scenes (just one for <ref type="bibr" target="#b40">[41]</ref>), or in the number of classes of moving targets (just pedestrians in <ref type="bibr" target="#b41">[42]</ref>).</p><p>To the best of our knowledge, we have collected the first large-scale dataset that has images and videos of various classes of targets that are moving and interacting in a real-world university campus. The dataset comprises more than 19K targets consisting of 11.2K pedestrians, 6.4K bicyclists, 1.3K cars, 0.3K skateboarders, 0.2K golf carts, and 0.1K buses. Although only videos of campus scenes are collected, the data is general enough to capture all type of interactions:</p><p>-target-target interactions, e.g., a bicyclist avoiding a pedestrian, -target-space interactions, e.g., a skateboarder turning around a roundabout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target-target interactions</head><p>We say that two targets interact when their collision energy (described by Eq. 1) is non-zero, e.g., a pedestrian avoiding a skateboarder. These interactions involve multiple physical classes of targets (pedestrians, bicyclists, or skateboarders to name a few), resulting into 185 K annotated target-target interactions. We intentionally collected data at peak hours (between class breaks in our case) to observe high density crowds. For instance, during a period of 20 s, we observe in average from 20 to 60 targets in a scene (of approximately 900 m 2 ).</p><p>Target-space interactions. We say that a target interacts with the space when its trajectory deviates from a linear one in the absence of other targets in its surrounding, e.g., a skateboarder turning around a roundabout. To further analyze these interactions, we also labeled the scene semantics of more than 100 static scenes with the following labels: road, roundabout, sidewalk, grass, building, and bike rack (see Fig. <ref type="figure" target="#fig_0">2</ref>). We have approximately 40k "target-space" interactions.</p><p>In our model, the whole target space interaction is implicitly considered in the Social Force model. We only take dynamic obstacles into account. However, in most common scenes, people will also try to avoid static obstacles. Similar to <ref type="bibr" target="#b17">[18]</ref> we model such obstacles as agents with zero velocity.</p><p>Tables 1 presents more details on our collected dataset. The scenes are grouped into 6 areas based on their physical proximity on campus. Each scene is captured with a 4k camera mounted on a quadcopter platform (a 3DR solo) hovering above various intersections on a University campus at an altitude of approximately eighty meters. The videos have a resolution of 1400 × 1904 and have been processed (i.e. undistorted and stabilized). Targets are annotated with their class label and their trajectory in time and space is identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling Social Sensitivity</head><p>We claim that modeling human trajectory with a single navigation style is not suitable for capturing the variety of social behaviors that targets exhibit when interacting in complex scenes. We believe that conditioning such models on navigation style (i.e., the way targets avoid each other) is a better idea and propose a characterization (feature) which we call social sensitivity. Given this characterization, we hence assign a navigation style to each target to better forecast its trajectory and improve tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Sensitivity feature.</head><p>Inspired by the Social Forces model (SF) <ref type="bibr" target="#b0">[1]</ref>, we model targets' interactions with an energy potential E ss . A high potential means that the target is highly sensitive to others. We define E ss as follows:</p><p>At each time step t, the target i is defined by a state variable s</p><formula xml:id="formula_0">(t) i = {p (t) i , v (t) i }, where p (t)</formula><p>i is the position, and v (t) i the velocity. The energy potential encoding the social sensitivity is computed as follows:</p><formula xml:id="formula_1">E ss (v (t) i ; s i , s -i |σ d , σ w , β) = j =i w(s i , s j ) exp - d 2 (v, s i , s j ) 2σ 2 d ,<label>(1)</label></formula><p>with w(s i , s j ) defined as:</p><formula xml:id="formula_2">w(s i , s j ) = exp - |Δp ij | 2σ ω . 1 2 1 - Δp ij |Δp ij | v i |v i | , β ,<label>(2)</label></formula><p>and</p><formula xml:id="formula_3">d 2 (v, s i , s j ) = Δp ij - Δp ij (v -v j ) |v -v j | 2 (v -v j ) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>The energy E ss is modeled as a product of Gaussians where the variances σ w,d represent the distances at which other targets will influence each other. For instance, if two targets i, j are close to each other (Δp ij is small), E ss will be large when σ w,d are small.</p><p>We define the parameter Θ ss = {σ d , σ w , β} as the social sensitivity feature and interpret its dimension as follows:</p><p>-σ d is the preferred distance a target maintains to avoid collision, -σ w is the distance at which a target reacts to prevent a collision (distance at which (s)he starts deviating from its linear trajectory), -and β controls the peakiness of the weighting function.</p><p>In other words, the parameters {σ d , σ w , β} aim at describing how targets avoid each others -i.e., their social sensitivity. We now present how we infer the parameters Θ ss at training and testing time.</p><p>Training . At training time, since we observe all targets' velocities, V train , we could learn a unique set of parameters, i.e., a single value for social sensitivity, that minimizes the energy potential as follows (similarly to what previous methods do <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>):</p><formula xml:id="formula_5">{σ d , σ w , β} = argmin {σ d ,σw,β} T -1 i=1 E ss (v train i , s i , s -i |σ d , σ w , β) , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where T is the number of targets in the training data. This minimization is operated with an interior-point method and is set with the following constraint on σ d : σ d &gt; 0.1 (it specifies that every target can't have a "vital space" smaller than 10cm). As mentioned previously, however, we claim that learning a unique set of parameters is not suitable when one needs to deal with complex multi-class target scenarios whereby targets can have different social sensitivity. To validate this claim, we plot in Fig. <ref type="figure">3</ref> each target into a social sensitivity space where the x-axis is the σ d values and the y-axis is the σ w ones. These data are computed using training images from our dataset (see Sect. 6 for more details). We did not plot the third parameter β since it does not change much across targets. Even if our approach can handle an arbitrary number of classes, we cluster the points into four clusters for illustration purposes. Each cluster corresponds to what we define as a "navigation style". A navigation style describes the sensitivity of a target to its surrounding. We illustrate on the sides of Fig. <ref type="figure">3</ref> how targets follow different strategies in avoiding each other as different navigation styles are used. Thanks to the above analysis of the social sensitivity space, at training, we solve Eq. 4 for each target to get its social sensitivity feature. We then cluster the points with K-mean clustering to have N number of clusters. Each cluster represents a navigation style. In Sect. 6, we study the impact of the number of clusters used by our method on the forecasting accuracy in Table <ref type="table" target="#tab_3">4</ref>.</p><p>Testing . At test time, we observe the targets until time t, and want to assign a navigation style.</p><p>In the presence of other targets, we solve Eq. 5 for each specific target i at time t:</p><formula xml:id="formula_7">{σ d (i), σ w (i), β(i)} = argmin {σ d (i),σw(i),β(i)} (E ss (v t i , s i , s -i |σ d (i), σ w (i), β(i))) . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>We obtain the social sensitivity feature Θ ss (i) = {σ d (i), σ w (i), β(i)} for each target i. Given the clusters found at training, we assign each Θ ss (i) to its corresponding cluster, i.e., navigation style.</p><p>In the absence of interactions, a target takes either a "neutral" navigation style (when entering a scene) or inherit the last inferred class from the previous Fig. <ref type="figure">3</ref>. of the social sensitivity space where we have illustrated how targets avoid each other with four navigation styles (from a top view). Each point in the middle plot is a target. The x-axis is the preferred distance σ d a target keeps with its surrounding targets, and y-axis is the distance σw at which a target reacts to prevent a collision. Each color code represents a cluster (a navigation style). Even if our approach can handle an arbitrary number of classes, we only use 4 clusters for illustration purposes. In this plot, the green cluster represents targets with a mild behavior, willing to avoid other targets as much as possible and considering them from afar, whereas the red cluster describes targets with a more aggressive behavior and with a very small safety distance, considering others at the last moment. We illustrate on the sides of the plot examples of how targets follow different strategies in avoiding each other as different navigation styles are used. (Color figure online) interaction. The "neutral" navigation style is the most popular one (in green in Fig. <ref type="figure">3</ref>). In Fig. <ref type="figure">4</ref>, we show that when the target is surrounded by other targets, its class changes with respect to its social sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Forecasting and Tracking with Social Sensitivity</head><p>Our new collected dataset creates the opportunity to study methods for trajectory forecasting and multi-target tracking, and evaluate them on a large-scale broad setting, i.e. a space occupied by several classes of targets. Thanks to our proposed social sensitivity feature, we have more flexibility in modeling target interactions to forecast future trajectories. In the remaining of this section, we present the details behind our forecasting model driven by social sensitivity. Then, in Sect. 5.2, we show how to use our forcasting model on multi-target tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Forecasting Multiple Classes of Targets</head><p>Problem formulation. Given the observed trajectories of several targets at time t, we aim to forecast their future positions over the next N time frames (where N is in seconds).</p><p>We adapt the Social Forces model <ref type="bibr" target="#b0">[1]</ref> from single class to multiple classes. Each target makes a decision on its velocity v (t+1) i</p><p>. The energy function, E Θ , Fig. <ref type="figure">4</ref>. Illustration of the class assignment for each target. The same color represents the same navigation style (cluster) described in Fig. <ref type="figure">3</ref>. Note that for a given target its class changes across time regardless of its physical class (i.e., whether it is a pedestrian, bike, etc.). When the target is surrounded by other targets, its class changes with respect to its social sensitivity. In this scene, first we can observe a cyclist (shown as label 1 in the images) belonging to a black cluster, i.e., being aggressive in his moves, then belonging to some milder clusters (purple and green). We also can see the evolution of a group of pedestrians (shown as labels 2,3) in the images), initially "mild" (green at T = 1), who become red at time T = 3 at which they decide to overtake another group and accelerate. (Color figure online) associated to every single target is defined as:</p><formula xml:id="formula_9">E Θ (v t+1 ; s i , s -i ) = λ 0 (c)E damp (v t+1 ; s i ) + λ 1 (c)E speed (v t+1 ; s i ) +λ 2 (c)E dir (v t+1 ; s i ) + λ 3 (c)E att (v t+1 ; s i ) + λ 4 (c)E group (v t+1 ; s i , s Ai ) +E ss (v t+1 ; s i , s -i |σ d (v t ), σ w (v t ), β)<label>(6)</label></formula><p>where</p><formula xml:id="formula_10">Θ = {λ 0 (c), λ 1 (c), λ 2 (c), λ 3 (c), λ 4 (c), σ d (v t ), σ w (v t )</formula><p>, β} and c is the navigation class. More details on the definition of each of the energy terms can be found in <ref type="bibr" target="#b0">[1]</ref>.</p><p>In our work, we propose to compute σ d , and σ w directly from the observed velocity v t using Eq. 5. Both distances σ d , and σ w will then be used to identify the navigation class c. For each class c, the parameter Θ can be learned from training data by minimizing the energy in Eq. 6. Time Complexity. At test time, we only need to infer 3 parameters instead of few dozen at training time. Once these 3 parameters are inferred, we use the result from our k-means clustering to get the remaining parameters. Consequently, the computation cost went from 1 min (to infer all parameters) to 0.1 sec (to infer three parameters) (per frame and agent with a matlab implementation).</p><p>There is an additional computational complexity of O(nkdi) for k-means which comes at negligible computational cost (less than 1 ms), where n is the number of d-dimensional vectors (in this application 2), k the number of clusters (number of behavioral classes) and i the number of iterations needed until convergence which is not more than 10 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-target Tracking</head><p>Problem formulation. Given the detected targets at each time frame (using for instance a target detector <ref type="bibr" target="#b42">[43]</ref>, or a background subtraction method <ref type="bibr" target="#b43">[44]</ref>), we want to link the detection results across time to form trajectories, commonly referred to as tracking-by-detection.</p><p>As mentioned in Sect. 2, we modify the Multi-target Tracking (MTT) algorithm from Xiang et al. <ref type="bibr" target="#b38">[39]</ref> to utilize our multi-class forecasting model based on social sensitivity. They formulate the MTT problem as a Markov Decision Process (MDP), which seeks to model the trajectory of targets according to a set of valid states (e.g., s tracked , s lost ) and transitions. They construct an approach to data association by computing a feature vector φ t i that describes the appearance of the targets in each of these possible states. They furthermore use a linear motion prior to reason on the navigation of targets, to thus determine a heuristic as to where a target should generally lie in future frames.</p><p>In order to evaluate the effectiveness of social sensitivity, we replace their linear motion prior with our multi-class forecasting method. More specifically, we modify φ t i , the feature vector for target i at time t as follows: Given the coordinates x t i , y t i of the target, we first apply our social force model to obtain a prediction x t+1 i , y t+1 i of the target at the next timestep. Then, given a list of candidate detections D t+1 for data association, we compute a normalized Euclidean distances {d 1 , d 2 , . . .} between each detection and the predicted coordinates, and append e -dj to φ t i , where d j is the distance to detection j. In Sect. 6, we show the gain in performance from applying this method to our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We run two sets of experiments: First, we study the performance of our method on trajectory forecasting problem. Then, we demonstrate the effectiveness of our proposed social sensitivity feature on state-of-the-art multi-target tracking -whereby the learnt forecasting models help the data association step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Forecasting Accuracy</head><p>Datasets and metrics. We evaluate our multi-class forecasting framework on our new collected dataset as well as previous existing pedestrian-only ones <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>. Our dataset has two orders of magnitude more targets than the combined pedestrian-only datasets. We evaluate the performance of forecasting methods with the following measures: average prediction error over (i) the full estimated trajectory, (ii) the final estimated point, and (iii) the average displacement during collision avoidance's. Similar to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>, we observe trajectories for 2.4 s and predict for 4.8 s. We sub-sample a trajectory every 0.4 s. We also focus our evaluation when non-linear behaviors occur in the trajectories to not be affected by statistically long linear behaviors.</p><p>Quantitative and qualitative results. We evaluate our proposed multi-class forecasting framework against the following baselines: (i) single class forecasting methods such as SF <ref type="bibr" target="#b0">[1]</ref> and IGP <ref type="bibr" target="#b44">[45]</ref>, (ii) physical class based forecasting (SFpc), i.e., using the ground truth physical class, and (iii) our proposed method inferring navigation style of the targets referred to as SF-mc. We present our quantitative results in Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref>:</p><p>On pedestrian-only dataset. (Table <ref type="table" target="#tab_1">2</ref>), our SF-mc performs the same as the single class Social Forces model in ETH dataset, and outperforms other methods in UCY datasets. This result can be justified by the fact that the UCY dataset is considerably more crowded, with more collisions, and therefore presenting different types of behaviors. Non-linear behaviors such as people stopping and talking to each other, walking faster, or turning around each others are more common in UCY than in ETH. Our forecasting model is able to infer these navigation patterns hence better predict the trajectories of pedestrians. We also report the performance of the IGP model on these pedestrian-only datasets for completeness. While IGP performs better on the less crowded dataset, it does not do well on the crowded ones. Notice that IGP uses the destination and time of arrival as additional inputs (which our method don't use).</p><p>On our multi-class dataset. (Table <ref type="table" target="#tab_2">3</ref>), we can see that our approach is more accurate on every scenes when a large amount of different classes are present. Our highest gain in performance is visible on the last three scenes, rich in classes and collisions (see Table <ref type="table" target="#tab_0">1</ref>). In Hobbiton and Edoras scenes, our algorithm, trained on a multi-class dataset, matches the single class Social Forces. This happens because the social sensitivity feature stays the same across targets. In a scene with less number of classes, this could become a drawback, but yet our algorithm can perform with the same accuracy. In Sect. 5.1, we present our method to forecast multiple classes of targets where we use the learned navigation styles as classes. One can argue that instead of using the navigation styles, we could use target's class (e.g. pedestrian, bicyclist, etc.). Table <ref type="table" target="#tab_2">3</ref> compares the performance of using navigation style against targets' class (e.g. one parameter per pedestrian, bicyclist, and so on...), referred to as SF-Physical. We use the ground truth class label to associate each target to their corresponding physical class -this gives an upper bound accuracy. Interestingly, both multi-class strategies perform almost the same although our method does not require ground truth physical class labels as it automatically assign the navigation style class to each target as described in Sect. 5.1.</p><p>We study the impact of the number of navigation styles (clusters) used by our method on the forecasting accuracy in Table <ref type="table" target="#tab_3">4</ref>. The optimal performance is obtained with 7 navigation styles which coincidentally, is very similar to the number of target's class (6 in our dataset). All experiments results in Table <ref type="table" target="#tab_2">3</ref> are given considering 7 clusters.</p><p>Once a target is associated to one of the navigation styles, the corresponding parameter θ from Eq. 6 is used to predict the trajectory of the target. We can visualize the impact of the navigation style on the prediction. In Fig. <ref type="figure">5</ref>, we show the predicted trajectories when several navigation styles are used to perform the forecasting. This shows the need to assign targets into specific classes.</p><p>Finally, in Fig. <ref type="figure" target="#fig_1">6</ref>, we show more examples of our predicted trajectories and compare them with previous works. Our proposed multi-class framework outperforms previous methods in crowded scenes. However, in the absence of interactions, all methods perform the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Multi-target Tracking Evaluation</head><p>Dataset and metrics. We evaluate the impact of our social sensitivity feature on multi-target tracking using our newly collected dataset which contain images Fig. <ref type="figure">5</ref>. We show the predicted trajectory of a given target (red circle) in which four different navigation styles are used to perform the prediction. The corresponding predicted trajectories are overlaid on one other and shown with different color codes (the same as those used for depicting the clusters in Fig. <ref type="figure">3</ref>). The ground truth is represented in blue. Predicted trajectories are shown for 6 subsequent frames indicated by T = 1, ..., 6 respectively. Interestingly, when the target is far away from other targets (no interactions are taking place) the predicted trajectories are very similar to each other (they almost overlap and show a linear trajectory). However, when the red target gets closers to other targets (e.g. the ones indicated in yellow), the predicted trajectories start showing different behaviors depending the navigation style: a conservative navigation style activates trajectories' prediction that keep large distances to the yellow targets in order to avoid them (green trajectory) whereas an aggressive navigation style activates trajectories' prediction that are not too distant from the yellow targets (red trajectory). Notice that our approach is capable to automatically associate the target to one of the 4 clusters based on the characteristics in the social sensitivity space that have been observed until present. In this example, our approach selects the red trajectory which is the closest to the ground truth's predicted trajectory (in blue). (Color figure online) from crossing roads, sidewalks, and many other types of scene semantics with roughly 30 people observed per frame. We use the same evaluation metric as the MTT challenge <ref type="bibr" target="#b39">[40]</ref>, such as the multi object tracking accuracy (MOTA), or mostly tracked (MT) objects. In details the multiple object tracking accuracy (MOTA) takes into account false positives, missed targets and identity switches, multiple object tracking precision (MOTP) is simply the average distance between true and estimated targets. The other metrics such as mostly tracked (MT) and mostly lost (ML) counts the number of mostly tracked trajectories (more than 80 % of the frames) and mostly lost (was not able to track more than 20 % of the frames). The full list of metrics can be found in <ref type="bibr" target="#b39">[40]</ref>.</p><p>Quantitative results. We evaluate our proposed MTT algorithm against the following baselines: (i) Xiang's MDP algorithm <ref type="bibr" target="#b38">[39]</ref> with a linear motion prior, (ii) <ref type="bibr" target="#b38">[39]</ref> with single class forecasting model <ref type="bibr" target="#b0">[1]</ref>, (iii) <ref type="bibr" target="#b38">[39]</ref> with our proposed multi-class forecasting model based on social sensitivity. We show that using our proposed MTT with social sensitivity feature outperforms previous work. Our quantitative results are shown in Table <ref type="table" target="#tab_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented our efforts to study human navigation at a new scale. We have contributed the first large-scale dataset of aerial videos from multiple classes of targets interacting in complex outdoor spaces. We have presented our work on predicting the trajectories of several classes of targets without explicitly solving the target classification task. We further demonstrate the impact of our forecasting model on multi-target tracking. Future work will study other forecasting methods such as Long Short-Term Memory (LSTM) to jointly solve the prediction task. Finally, by sharing our dataset, we hope that researchers will push the limits of existing methods in modeling human interactions, learning scene specific human motion, or detecting and tracking tiny targets from UAV data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Some examples of the scenes captured in our dataset. We have annotated all the targets (with bounding boxes) as well as the static scene semantics. The color codes associated to target bounding boxes represents different track IDs.</figDesc><graphic coords="4,75.81,279.65,272.80,89.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. of the predicted trajectories by our SF-mc method (in red) across time. Predicted trajectories are shown for 4 subsequent frames indicated by T = 1, ..., 4 respectively. We compare them with previous work<ref type="bibr" target="#b0">[1]</ref>. The ground truth is represented in blue. Our proposed multi-class framework outperforms previous methods when targets start interacting with other target (t = 2, 3, 4). However, in the absence of interactions (t = 1), all methods perform the same. (Color figure online)</figDesc><graphic coords="14,99.30,54.11,225.64,148.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Our campus dataset characteristics. We group the scenes and refer to them using fictional places from the "Lord of the Rings". Bi = bicyclist, Ped = pedestrian,</figDesc><table><row><cell cols="2">Skate = skateboarders</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">Frames Targets Interactions Bi</cell><cell>Ped</cell><cell cols="4">Skate Carts Car Bus</cell></row><row><cell cols="2">Isengard 134079 2044</cell><cell>6472</cell><cell>1004</cell><cell cols="2">926 57</cell><cell>19</cell><cell cols="2">23 15</cell></row><row><cell cols="2">Hobbiton 138513 3821</cell><cell>14084</cell><cell cols="3">163 2493 24</cell><cell>18</cell><cell cols="2">1065 58</cell></row><row><cell>Edoras</cell><cell>47864 1186</cell><cell>4684</cell><cell>224</cell><cell>956</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>0</cell></row><row><cell cols="2">Mordor 139364 4542</cell><cell>68459</cell><cell cols="3">2594 1492 111</cell><cell>154</cell><cell cols="2">165 26</cell></row><row><cell cols="2">Fangorn 249967 3126</cell><cell>45520</cell><cell cols="3">1017 1991 50</cell><cell>30</cell><cell cols="2">27 11</cell></row><row><cell>Valley</cell><cell>219712 4845</cell><cell>46062</cell><cell cols="3">1362 3358 89</cell><cell>21</cell><cell>10</cell><cell>5</cell></row><row><cell>Total</cell><cell cols="2">929499 19564 185281</cell><cell cols="3">6364 11216 333</cell><cell>244</cell><cell cols="2">1292 115</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Pedestrian dataset -Our 3 main evaluation methods, ordered as: Mean Average Displacement on all trajectories | Mean Average Displacement on collisions avoidance | Average displacement of the predicted final position (after 4.8 s).</figDesc><table><row><cell cols="2">Methods Lin</cell><cell>LTA</cell><cell>SF [1]</cell><cell></cell><cell cols="2">IGP [45]</cell><cell>Our SF-mc</cell></row><row><cell>eth</cell><cell cols="3">0.80 0.95 1.31 0.54 0.70 0.77 0.41 0.49</cell><cell>0.59</cell><cell cols="3">0.20 0.39 0.43 0.41</cell><cell>0.46</cell><cell>0.59</cell></row><row><cell>Hotel</cell><cell cols="3">0.39 0.55 0.63 0.38 0.49 0.64 0.25 0.38</cell><cell>0.37</cell><cell cols="2">0.24 0.34</cell><cell>0.37</cell><cell>0.24 0.32 0.37</cell></row><row><cell>Zara 1</cell><cell cols="4">0.47 0.56 0.89 0.37 0.39 0.66 0.40 0.41 0.60</cell><cell>0.39</cell><cell>0.54</cell><cell>0.39 0.35 0.41 0.60</cell></row><row><cell>Zara 2</cell><cell cols="3">0.45 0.44 0.91 0.40 0.41 0.72 0.40 0.40</cell><cell>0.68</cell><cell>0.41</cell><cell>0.43</cell><cell>0.42</cell><cell>0.39 0.39 0.67</cell></row><row><cell>UCY</cell><cell cols="3">0.57 0.62 1.14 0.51 0.57 0.95 0.48 0.54</cell><cell>0.78</cell><cell>0.61</cell><cell>0.62</cell><cell>1.82</cell><cell>0.45 0.51 0.76</cell></row><row><cell>Average</cell><cell cols="3">0.54 0.62 0.97 0.44 0.51 0.75 0.39 0.44</cell><cell cols="3">0.60 0.37 0.46</cell><cell>0.69</cell><cell>0.37 0.42 0.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Campus Dataset -Our 3 main evaluation methods, ordered as: Mean Average Displacement on all trajectories | Mean Average Displacement on collisions avoidance | Average displacement of the predicted final position (after 4.8 s).</figDesc><table><row><cell cols="2">Methods Lin</cell><cell>SF</cell><cell></cell><cell cols="2">IGP [45]</cell><cell></cell><cell>SF-Physical</cell><cell></cell><cell>Our SF-mc</cell></row><row><cell>Isengard</cell><cell cols="2">1.69 1.00 2.84 1.60</cell><cell>0.99</cell><cell>2.32 1.57</cell><cell>1.14</cell><cell>2.64</cell><cell>1.56 0.86</cell><cell>1.83</cell><cell>1.53 0.84 1.81</cell></row><row><cell>Hobbiton</cell><cell cols="3">1.17 1.01 1.81 1.11 0.82</cell><cell cols="3">1.70 1.11 0.81 2.25</cell><cell cols="3">1.12 0.81 1.70 1.12</cell><cell>0.83</cell><cell>1.70</cell></row><row><cell>Edoras</cell><cell cols="2">0,91 0.83 1.03 0.80</cell><cell cols="2">0.81 0.89 1.33</cell><cell>0.85</cell><cell>2.61</cell><cell cols="3">0.79 0.81 0.89 0.78 0.82</cell><cell>0.89</cell></row><row><cell>Mordor</cell><cell cols="2">1.72 1.10 3.80 1.38</cell><cell>0.89</cell><cell cols="2">2.30 0.95 0.69</cell><cell cols="2">1.78 1.37 0.65</cell><cell>2.30</cell><cell>1.37</cell><cell>0.60 2.30</cell></row><row><cell>Fangorn</cell><cell cols="2">1.02 0.75 2.00 0.94</cell><cell>0.41</cell><cell>1.66 0.96</cell><cell>0.69</cell><cell>1.67</cell><cell>0.90 0.40</cell><cell cols="2">1.51 0.89 0.36 1.51</cell></row><row><cell>Valley</cell><cell cols="2">1.38 0.86 2.45 1.29</cell><cell>0.87</cell><cell>2.02 1.20</cell><cell>0.75</cell><cell>2.46</cell><cell cols="3">1.01 0.65 1.65 0.99 0.66</cell><cell>1.65</cell></row><row><cell>Average</cell><cell cols="2">1.32 0.93 2.32 1.29</cell><cell>0.79</cell><cell>1.82 1.19</cell><cell>0.82</cell><cell>2.24</cell><cell>1.14 0.70</cell><cell>1.65</cell><cell>1.11 0.69 1.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Forecasting error with respect to the number of clusters in our new campus dataset.</figDesc><table><row><cell></cell><cell>1 [1] 2</cell><cell>4</cell><cell>7</cell><cell>12</cell><cell>18</cell></row><row><cell>Mean error</cell><cell cols="5">1.14 1.16 1.15 1.11 1.12 1.20</cell></row><row><cell>Collision error</cell><cell cols="5">0.72 0.68 0.69 0.69 0.73 0.75</cell></row><row><cell cols="6">Final position error 1.84 1.74 1.70 1.64 1.69 1.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>MTT tracking results.</figDesc><table><row><cell></cell><cell>Rcll Prcn MT</cell><cell>ML</cell><cell cols="2">MOTA MOTP MOTAL</cell></row><row><cell>MDP [39] + Lin</cell><cell cols="3">74.1 80.1 44.18 % 20.9 % 51.5</cell><cell>74.2</cell><cell>55.4</cell></row><row><cell>MDP [39] + SF [1]</cell><cell cols="3">84.4 91.5 58.13 % 25.5 % 73.5</cell><cell>77.1</cell><cell>76.3</cell></row><row><cell cols="2">MDP [39] + our SF-mc 86.1 92.6 60 %</cell><cell cols="2">23.2 % 75.6</cell><cell>78.2</cell><cell>79.3</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Who are you with and where are you going?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving data association by joint modeling of pedestrian trajectories and groupings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010, Part I</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6311</biblScope>
			<biblScope unit="page" from="452" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3542" to="3549" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified framework for multi-target tracking and collective activity recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012, Part IV</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7575</biblScope>
			<biblScope unit="page" from="215" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual tracking: an experimental survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inferring dark matter and dark energy from videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2224" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding collective activitiesof people from videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1242" to="1257" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond actions: discriminative models for contextual group activities</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1216" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What are they doing? Collective activity classification using spatio-temporal relationship among people</title>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1282" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012, Part IV</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7575</biblScope>
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="655" to="664" />
			<date type="published" when="2007">2007</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robot navigation in dense human crowds: the case for cooperation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Trautman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2153" to="2160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic people tracking for occlusion handling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Pattern Recognition, ICPR 2004</title>
		<meeting>the 17th International Conference on Pattern Recognition, ICPR 2004</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="132" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The flow of human crowds</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Fluid Mech</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dependent gaussian processes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="217" to="224" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modelling smooth paths using gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K C</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Field and Service Robotics. Springer Tracts in Advanced Robotics</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Laugier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="381" to="390" />
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: modeling social behavior for multi-target tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Continuum crowds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1160" to="1168" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discrete choice pedestrian behavior model for pedestrian detection in visual tracking systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Thiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bierlaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Concepts for Intelligent Vision Systems, ACIVS 2004. Number EPFL-CONF-87109</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discrete choice models of pedestrian walking behavior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Res. Part B Methodological</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="667" to="687" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Planning-based prediction for pedestrians</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="3931" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to navigate through crowded environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="981" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1433" to="1438" />
		</imprint>
		<respStmt>
			<orgName>AAAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear inverse reinforcement learning with gaussian processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A probabilistic model of human motion and navigation intent for mobile robot path planning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Horiuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kagami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 4th International Conference on Autonomous Robots and Agents</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="663" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">People tracking with human motion predictions from social forces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Tipaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="464" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Everybody needs somebody: modeling social and grouping behavior on a linear programming multiple people tracker</title>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Socially-aware large-scale crowd forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to predict trajectories of cooperatively navigating agents</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4015" to="4020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding pedestrian behaviors from stationary crowd groups</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3488" to="3496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multicamera people tracking with a probabilistic occupancy map</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lengagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="282" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A sparsity constrained inverse problem to locate people in a network of cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boursier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on Digital Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparsity driven people localization with a heterogeneous network of cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boursier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GMCP-tracker: global tracking using generalized minimum clique graphs</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012, Part II</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7573</biblScope>
			<biblScope unit="page" from="343" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to track: online multi-object tracking by decision making</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4705" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">MOTChallenge 2015: towards a benchmark for multi-target tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<imprint>
			<date type="published" when="2015-04">April 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cost-sensitive topdown/bottom-up inference for multiscale activity recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012, Part IV</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7575</biblScope>
			<biblScope unit="page" from="187" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint inference of groups, events and human roles in aerial videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust real-time pedestrians detection in urban environments with low-resolution cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bierlaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Res. Part C Emerg. Technol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="113" to="128" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object detection and matching with mobile cameras collaborating with fixed cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bierlaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-camera and Multi-modal Sensor Fusion Algorithms and Applications-M2SFA</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unfreezing the robot: navigation in dense, interacting crowds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Trautman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="797" to="803" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
