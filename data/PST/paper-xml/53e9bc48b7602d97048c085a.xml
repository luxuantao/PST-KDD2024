<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fusion of Infrared and Visible Images for Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aglika</forename><surname>Gyaourova</surname></persName>
							<email>aglika@cs.unr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Bebis</surname></persName>
							<email>bebis@cs.unr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><surname>Pavlidis</surname></persName>
							<email>pavlidis@cs.uh.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visual Computing Laboratory</orgName>
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fusion of Infrared and Visible Images for Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">38B723AE5D4DAFA481B0E0A451EC56BC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A number of studies have demonstrated that infrared (IR) imagery offers a promising alternative to visible imagery due to it's insensitive to variations in face appearance caused by illumination changes. IR, however, has other limitations including that it is opaque to glass. The emphasis in this study is on examining the sensitivity of IR imagery to facial occlusion caused by eyeglasses. Our experiments indicate that IR-based recognition performance degrades seriously when eyeglasses are present in the probe image but not in the gallery image and vice versa. To address this serious limitation of IR, we propose fusing the two modalities, exploiting the fact that visible-based recognition is less sensitive to the presence or absence of eyeglasses. Our fusion scheme is pixel-based, operates in the wavelet domain, and employs genetic algorithms (GAs) to decide how to combine IR with visible information. Although our fusion approach was not able to fully discount illumination effects present in the visible images, our experimental results show substantial improvements recognition performance overall, and it deserves further consideration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Considerable progress has been made in face recognition research over the last decade <ref type="bibr" target="#b0">[1]</ref> especially with the development of powerful models of face appearance (e.g., eigenspaces <ref type="bibr" target="#b1">[2]</ref>). Despite the variety of approaches and tools studied, however, face recognition has shown to perform satisfactorily in controlled environments but it is not accurate or robust enough to be deployed in uncontrolled environments. Several factors affect face recognition performance including pose variation, facial expression changes, face occlusion, and most importantly, illumination changes.</p><p>Previous studies have demonstrated that IR imagery offers a promising alternative to visible imagery for handling variations in face appearance due to illumination changes more successfully. In particular, IR imagery is nearly invariant to changes in ambient illumination <ref type="bibr" target="#b2">[3]</ref>, and provides a capability for identification under all lighting conditions including total darkness <ref type="bibr" target="#b3">[4]</ref>. Thus, while visible-based algorithms opt for pure algorithmic solutions into inherent phenomenology problems, IR-based algorithms have the potential to offer simpler and more robust solutions, improving performance in uncontrolled environments and deliberate attempts to obscure identity <ref type="bibr" target="#b4">[5]</ref>.</p><p>Despite its advantages, IR imagery has other limitations including that it is opaque to glass. Objects made of glass act as a temperature screen, completely hiding the face parts located behind them. In this study, we examine the sensitivity of IR imagery to facial occlusion due to eyeglasses. To address this serious limitation of IR, we propose fusing IR with visible information in the wavelet domain using GAs <ref type="bibr" target="#b5">[6]</ref>. To demonstrate the results of our fusion strategy, we performed extensive recognition experiments using the popular method of eigenfaces <ref type="bibr" target="#b1">[2]</ref>, although any other recognition method could have been used. Our results show overall substantial improvements in recognition performance using IR and visible imagery fusion than either modality alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review of Face Recognition in the Infrared Spectrum</head><p>An overview of identification in the IR spectrum can be found in <ref type="bibr" target="#b6">[7]</ref>. Below, we review several studies comparing the performance of visible and IR based face recognition. The effectiveness of visible versus IR was compared using several recognition algorithms in <ref type="bibr" target="#b7">[8]</ref>. Using a database of 101 subjects without glasses, varying facial expression, and allowing minor lighting changes, they concluded that there are no significant performance differences between visible and IR recognition across all the algorithms tested. They also concluded that fusing visible and IR decision metrics represents a viable approach for enhancing face recognition performance. In <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, several different face recognition algorithms were tested under various lighting conditions and facial expressions. Using radiometrically calibrated thermal imagery, they reported superior performance for IR-based recognition than visible-based recognition. In <ref type="bibr" target="#b10">[11]</ref>, the effect of lighting, facial expression, and passage of time between the gallery and probe images were examined. Although IR-based recognition outperformed visible-based recognition assuming lighting and facial expression changes, their experiments demonstrated that IR-based recognition degrades when there is substantial passage of time between the gallery and probe images. Using fusion strategies at the decision level based on ranking and scoring, they were able to develop schemes that outperformed either modality alone. IR has also been used recently in face detection <ref type="bibr" target="#b11">[12]</ref>. This approach employs multi-band feature extraction and capitalizes on the unique reflectance characteristics of the human skin in the near-IR spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fusion of Infrared and Visible Imagery</head><p>Despite its robustness to illumination changes, IR imagery has several drawbacks. First, it is sensitive to temperature changes in the surrounding environment. Currents of cold or warm air could influence the performance of systems using IR imagery. As a result, IR images should be captured in a controlled environment. Second, it is sensitive to variations in the heat patterns of the face. Factors that could contribute to these variations include facial expressions (e.g. open mouth), physical conditions (e.g. lack of sleep), and psychological conditions (e.g. fear, stress, excitement). Finally, IR is opaque to glass. As a result, a large part of the face might be occluded (e.g. by wearing eyeglasses).</p><p>In contrast to IR imagery, visible imagery is more robust to the above factors. This suggests that effective algorithms to fuse information from both spectra have the potential to improve the state of the art in face recognition. In the past, fusion of visible and IR images has been successfully used for visualization purposes <ref type="bibr" target="#b12">[13]</ref>.</p><p>In this study, we consider the influence of eyeglasses to IR-based face recognition. Our experiments demonstrate that eyeglasses pose a serious problem to recognition performance in the IR spectrum. To remedy this problem, we propose fusing IR with visible imagery. Visible imagery can suffer from highlights on the glasses under certain illumination conditions, but the problems are considerably less severe than with IR. Since IR and visible imagery capture intrinsically different characteristics of the observed faces, intuitively, a better face description could be found by utilizing the complimentary information present in the two spectra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fusion at Multiple Resolutions</head><p>Pixel by pixel fusion does not preserve the spatial information in the image. In contrast, fusion at multiple resolution levels allows features with different spatial extend to be fused at the resolution at which they are most salient. In this way, important features appearing at lower resolutions can be preserved in the fusion process.</p><p>Multiple resolution features have been used in several face recognition systems in the past (e.g. <ref type="bibr" target="#b13">[14]</ref>). The advantages of using different frequencies is that high frequencies are relatively independent of global changes in the illumination, while the low frequencies take into account the spatial relationships among the pixels and are less sensitive to noise and small changes, such as facial expression.</p><p>The slow heat transfer through the human body causes natural low resolution of IR images of human face. Thus, we decided to implement our fusion strategy in the wavelet domain, taking into consideration the benefits of multi-resolution representations and the differences in resolution between the IR and visible-light images. Our fusion strategy is thus different from fusion strategies implemented at the decision level, reported earlier in the literature (i.e., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method Overview</head><p>The proposed method contains two major steps: (a) fusion of IR and visible images and (b) recognition based on the fused images. Fusion is performed by combining the coefficients of Haar wavelet <ref type="bibr" target="#b14">[15]</ref> decompositions of a pair of IR and visible images having equal size. The fusion strategy is found during a training phase using GAs <ref type="bibr" target="#b5">[6]</ref>. The coefficients selected from each spectrum are put together and the fused face image is reconstructed using the inverse wavelet transform. To demonstrate the effectiveness of the fusion solutions found by GAs, we perform recognition using the popular method of eigenfaces <ref type="bibr" target="#b1">[2]</ref> although any other recognition technique could have been used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Eigenfaces</head><p>The eigenface approach uses Principal Components Analysis (PCA), a classical multivariate statistics method, to linearly project face images in a lowdimensional space. This space is spanned by the principal components (i.e., eigenvectors corresponding to the largest eigenvalues) of the distribution of the training images. After a face image has been projected in the eigenspace, a feature vector containing the coefficients of the projection is used to represent the face image. Representing each image I(x, y) as a N × N vector Γ i , first the average face Ψ is computed:</p><formula xml:id="formula_0">Ψ = 1 R R i=1 Γ i</formula><p>where R is the number of faces in the training set. Next, the difference Φ of each face from the average face is computed:</p><formula xml:id="formula_1">Φ i = Γ i -Ψ . Then the covariance matrix is estimated by: C = 1 R R i=1 Φ i Φ T i = AA T , where, A = [Φ 1 Φ 2 . . . Φ R ].</formula><p>The eigenspace can then be defined by computing the eigenvectors µ i of C. Usually, we need to keep a smaller number of eigenvectors R k corresponding to the largest eigenvalues. Each image Γ is transformed by first subtracting the mean image (Φ = Γ -Ψ ), and then projecting in the eigenspace w i = µ T i Γ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Wavelet Transform(WT)</head><p>Wavelets are a type of multi-resolution function approximation that allow for the hierarchical decomposition of a signal or image. In particular, they decomposes a given signal onto a family of functions with finite support. This family of functions is constructed by the translations and dilations of a single function called mother wavelet. The finite support of the mother wavelet gives exact time localization while the scaling allows extraction of different frequency components. The discrete wavelet transform (DWT) is defined in terms of discrete dilations and translations of the mother wavelet function:</p><formula xml:id="formula_2">ψ jk (t) = 2 -j/2 ψ(2 -jt -k) ,</formula><p>where the scaling factor j and the translation factor k are integers: j, k ∈ Z. The wavelet decomposition of a function f (t) ∈ L 2 (R) is given by: f (t) = j k h j,k ψ jk (t) , where the coefficients h j,k are the inner products of f (t) and ψ jk (t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Genetic Algorithms (GAs)</head><p>GAs are a class of randomized, parallel search optimization procedures inspired by the mechanisms of natural selection, the process of evolution <ref type="bibr" target="#b5">[6]</ref>. They were designed to efficiently search large, non-linear, poorly-understood search spaces. In the past, GAs have been used in target recognition <ref type="bibr" target="#b15">[16]</ref>, object recognition <ref type="bibr" target="#b16">[17]</ref>, face detection/verification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and feature selection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>GAs operate iteratively on a population of structures, each of which represents a candidate solution to the problem, encoded as a string of symbols (i.e., chromosome). A randomly generated set of such strings forms the initial population from which the GA starts its search. Three basic genetic operators guide this search: selection, crossover and mutation. Evaluation of each string is based on a fitness function which is problem-dependent. The fitness function determines which of the candidate solutions are better. Selection probabilistically filters out poor solutions and keeps high performance solutions for further investigation. Mutation is a very low probability operator that plays the role of restoring lost genetic material. Crossover in contrast is applied with high probability. It is a randomized yet structured operator that allows information exchange between the stings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evolutionary IR and Visible Image Fusion</head><p>Our fusion strategy operates in the wavelet domain. The goal is to find an appropriate way to combine the wavelet coefficients from the IR and visible images. The key question is which wavelet coefficients to choose and how to combine them. Obviously, using un-weighted averages is not appropriate since it assumes that the two spectra are equally important and, even further, that they have the same resolution which is not true. Several experiments for fusing the wavelet coefficients of two images have been reported in <ref type="bibr" target="#b21">[22]</ref>. Perhaps, the most intuitive approach is picking the coefficients with maximum absolute value <ref type="bibr" target="#b22">[23]</ref>. The higher the absolute value of a coefficient is, the higher is the probability that it encodes salient image features. Our experiments using this approach showed poor performance.</p><p>In this paper, we propose using GAs to fuse the wavelet coefficients from the two spectra. Our decision to use GAs for fusion was based on several factors. First, the search space for the image fusion task at hand is very large. In the past, GAs have demonstrated good performance when searching large solution spaces. Much work in the genetic and evolutionary computing communities has led to growing understanding of why they work well and plenty of empirical evidence to support this claim <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Second, the problem at hand appears to have many suboptimal solutions. Although, GAs cannot guarantee finding a global optimum, they have shown to be successful in finding good local optima. Third, they suitable for parallelization and linear speedups are the norm, not the exception <ref type="bibr" target="#b25">[26]</ref>. Finally, we have applied GAs in the past for feature selection, a problem very much related to fusion, with good success <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Encoding: In our encoding scheme, the chromosome is a bit string whose length is determined by the number of wavelet coefficients in the image decomposition. Each bit in the chromosome is associated with a wavelet coefficient at a specific location. The value of a bit in this array determines whether the corresponding wavelet coefficient is selected from the IR (e.g., 0) or from the visible spectrum (e.g., 1).</p><p>Fitness Evaluation: Each individual in a generation represents a possible way to fuse IR with visible images. To evaluate its effectiveness, we perform the fusion based on the information encoded by this individual and apply the eigenface approach. Recognition accuracy is computed using a validation dataset (see <ref type="bibr">Section 7)</ref> and is used to provide a measure of fitness.</p><p>Initial Population: In general, the initial population is generated randomly, (e.g., each bit in an individual is set by flipping a coin). In this way, however, we will end up with a population where each individual contains the same number of 1's and 0's on average. To explore subsets of different numbers of wavelet coefficients chosen from each domain, the number of 1's for each individual is generated randomly. Then, the 1's are randomly scattered in the chromosome.</p><p>Selection: Our selection strategy was cross generational. Assuming a population of size N, the offspring double the size of the population and we select the best N individuals from the combined parent-offspring population Crossover: In general, we do not know how different wavelet coefficients depend on each other. If dependent coefficients are far apart in the chromosome, it is more probable that traditional 1-point crossover, will destroy the schemata. To avoid this problem, uniform crossover is used here. The crossover probability used in our experiments was 0.96.</p><p>Mutation: Mutation is a very low probability operator which flips the values of randomly chosen bit. The mutation probability used here was 0.02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Face Dataset</head><p>In our experiments, we used the face database collected by Equinox Corporation under DARPA's HumanID program <ref type="bibr">[27]</ref>. Specifically, we used the long-wave infrared (LWIR) (i.e., 8µ-12µ) and the corresponding visible spectrum images from this database. The data was collected during a two-day period. Each pair of LWIR and visible light images was taken simultaneously and co-registered with 1/3 pixel accuracy (see Fig. <ref type="figure" target="#fig_0">1</ref>). The LWIR images were radiometrically calibrated and stored as grayscale images with 12 bits per pixels. The visible images are also grayscale images represented with 8 bits per pixel. The size of the images in the database is 320×240 pixels.</p><p>The database contains frontal faces under the following scenarios: (1) three different light direction -frontal and lateral (right and left); (2) three facial expression -"frown", "surprise" and "smile"; (3) vocals pronunciation expressions -subjects were asked to pronounce several vocals from which three representative frames are chosen; and (4) presence of glasses -for subjects wearing glasses, all of the above scenarios were repeated with and without glasses. Both IR and visible face images were preprocessed prior to experimentation by following a procedure similar to that described in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. The goal of preprocessing was to align and scale the faces, remove background, and account for some illumination variations (see Fig. <ref type="figure" target="#fig_0">1</ref>). In this study, we attempted to test the effect on recognition performance of each factor available in the Equinox database. In addition, we have performed experiments focusing on the effect of eyeglasses. For comparison purposes, we have attempted to evaluate our fusion strategy using a similar experimental protocol to that given in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Our evaluation methodology employs a training set (i.e., used to compute the eigenfaces), a gallery set (i.e., set of persons enrolled in the system), a validation set (i.e., used in the fitness evaluation of the GA), and a test set (i.e., probe image set containing the images to be identified).</p><p>Our training set contains 200 images, randomly chosen from the entire Equinox database.</p><p>For recognition, we used the Euclidean distance and the first 100 principal components as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Recognition performance was measured by finding the percentage of the images in the test set, for which the top match is an image of the same person from the gallery. To mitigate for the relatively small number of images in the database, the average error was recorded using a three-fold cross-validation procedure. In particular, we split each dataset used for testing randomly three times by keeping only 75% of the images for testing purposes and the rest 25% for validation purposes. To account for performance variations due to random GA initialization, we averaged the results over three different GA runs for each test, choosing a different random seed each time. Thus, we performed a total of 9 runs for each gallery/test set experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Facial Expression Tests</head><p>The test sets for the facial expression experiments include the images containing the three expression frames and three vocal pronunciation frames. There are 90 subjects with a total of 1266 pairs of images for the expression frames and 1299 for the vocal frames. Some of the subjects in these tests sets wear glasses while others not. Following the terminology in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> we have created the following test sets: EA (expression frames, all illuminations), EL (expression frames, lateral illuminations), EF (expression frames, frontal illumination), VA (vocal frames, all illumination), VL (vocal frames, lateral illumination), VF (vocal frames, frontal illumination). The inclusion relations among these sets are as follows: EA = EL ∪ EF, VA = VL ∪ VF, and VA ∩ EA = ∅.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Eyeglasses Tests</head><p>Measuring the effect of eyeglasses is done by using the expression frames. There are 43 subjects wearing glasses in the EA set making a total of 822 images. Following the terminology in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> we created the following test sets: EG (expression frames with glasses, all illuminations), EnG (expression frames without glasses, all illuminations), EFG (expression frames with glasses, frontal illumination), ELG (expression frames with glasses, lateral illumination), EFnG (expression frames without glasses, frontal illumination), ELnG (expression frames without glasses, lateral illumination). The inclusion relations among these sets are as follows: EG = ELG ∪ EFG, EnG = ELnG ∪ EFnG and EG ∩ EnG = ∅.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Eyeglasses</head><p>The results shown in Table <ref type="table">1</ref> illustrate that IR-based recognition is robust to illumination changes but performs poorly when glasses are present in the gallery set but not in the test set and vice versa. Considerable improvements in recognition performance have been achieved in this case by fusing IR with visible images. The improvement was even greater when, in addition to eyeglasses, the test and the gallery set contained images taken under different illuminations. For example, in the EFG/ELnG test case the fusion approach improved recognition performance by 46% compared to recognition using visible-light images and by 82% compared to recognition using LWIR images.</p><p>Recognition using LWIR images outperformed recognition using fused images when the only difference between the images in the test and gallery sets was the direction of illumination. This is accounted to the inability of our fusion scheme to fully discount illumination effects contributed by the visible-light images. Recognition performance using visible-light images was always worse than using fused images.</p><p>Table <ref type="table">1</ref>. Averages and standard deviations for the eyeglasses experiments. The columns represent the gallery set and the rows represent the test set. The first entry in each cell shows the performance measured from the visible-light images, the second entry is from the LWIR images, and the third entry is from the fused images. The bottom entry shows the minimum and maximum recognition performances from the three cross-validation runs achieved when using the fused images. Test scenarios for which the test and the gallery sets had common subsets were not performed.</p><formula xml:id="formula_3">EG ELG EFG EnG ELnG EFnG EG × × × (84.8, 1<label>.4) (15.1, 1.0) (92.5, 1.3) 91.0 -93.6 (84.8, 1.4) (13.1, 1.0) (88.9, 1.4) 88.0 -90.5 (64.3, 1.7) (21.7, 1.0) (82.1, 3.1) 79.2 -85.4</label></formula><p>ELG × × ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Facial Expression</head><p>The facial expression tests had varying success as shown in Table <ref type="table" target="#tab_1">2</ref>. In general, fusion led to improved recognition compared to recognition using visible-light images. In several cases, however, the accuracy using LWIR images was higher than using fused images. These were cases again where the illumination dierctions between the gallery and the test sets were different. This result is consistent with that of the eyeglasses tests and was caused by the inability of our fusion scheme to fully discount the illumination effects in the visible images. Note that we did not performed experiments when the intersection between gallery and test sets is not empty. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion</head><p>The presence/absence of eyeglasses proved to be a big obstacle for IR-based recognition. To better understand this, let's take a closer look of the results shown in Table <ref type="table">1</ref>. The horizontal and vertical double lines through the center of the table divide the table into four quadrants (i.e., I to IV , starting from the upper-right corner and moving counterclockwise). Each quadrant represents a set of experiments testing some specific difference between the gallery and the test sets: (1) Experiments in quadrant I evaluate the effect of eyeglasses being present in the probe but not in the gallery; <ref type="bibr" target="#b1">(2)</ref> Experiments in quadrant III evaluate the effect of eyeglasses being present in the gallery but not in the probe; (3) Experiments along the off-diagonals within each of these two quadrants represent tests where the illumination conditions between the gallery and probe sets are the same; (4) Experiments in quadrants II and IV evaluate the effect illumination changes only.</p><p>To illustrate the performance of our fusion approach, we have interpolated the results from Table <ref type="table">1</ref> and used a simple visualization scheme to remove small differences and emphasize major trends in recognition performance (see Fig. <ref type="figure">2</ref>). Our visualization scheme assigns a grayscale value to each average from Table <ref type="table">1</ref>) with black implying 0% recognition and white 100% recognition. The empty cells from Table <ref type="table">1</ref> are also shown in black.</p><p>By observing Fig. <ref type="figure">2</ref>, several interesting conclusions can be made. As expected, face recognition success based on IR images (see Fig. <ref type="figure">2</ref>.(b))is not influenced by lighting conditions. This is supported by the prevailing white color in quadrants II and IV (case (3)) and by the high recognition rates in quadrants II and IV (case (4)). However, IR yielded very low success when eyeglasses were present in the gallery but not in the probe and vice-versa (cases (1) and ( <ref type="formula">2</ref>)). The success of visible-based face recognition was relatively insensitive to subjects' wearing glasses (see Fig. <ref type="figure">2</ref>.(c)). This follows from the relatively uniform color in quadrants I and III (cases (1) and ( <ref type="formula">2</ref>)). Lighting conditions had big influence on the success of face recognition in the visible domain. There are distinguishable bright lines along the main diagonals in quadrants I and II (case (3)). The success of face recognition based on fused images was similar in all four quadrants of the image (see Fig. <ref type="figure">2.(d)</ref>). This implies that we were able to achieve relative insensitivity to both eyeglasses and variable illumination. The image fusion approach led to higher recognition performance compared to recognition in the visible spectrum but was not able to completely compensate for the effects of illumination direction in the visible images. We have noticed that in all the cases where LWIR performed better than fusion, the illumination direction in the gallery set was different from that in the test set (assuming no difference in glasses). The presence of illumination effects in the fused images can be visually confirmed by observing the reconstructed fused images shown in Fig. <ref type="figure">3</ref>, and their first eigenfaces shown in i.e., Fig. <ref type="figure" target="#fig_2">4</ref>. Fused images had higher resolution compared to LWIR images, however, they were also affected by illumination effects present in the visible images. Obviously, the first eigenfaces of the fused images still encode the effects of illumination direction, present in the visible images. More effective fusion schemes (e.g., weighted averages of wavelet coefficients) and more powerful fitness functions (i.e., add extra terms to control the number of coefficients selected from different bands of spectrum) might help to overcome these problems and improve fusion overall.</p><p>Also, further consideration should be given to the existence of many optimal solutions found by the GA. Although optimal in the training phase, these solutions showed different recognition performances when used for testing. In investigating these solutions, we were not able to distinguish any pattern in the content of the chromosomes that might have revealed why some chromosomes were better than others. On the average, half of the coefficients were selected from the visible spectrum and the other half from the IR spectrum. The use of larger validation sets and more selective fitness functions might help to address these issues more effectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusions and Future Work</head><p>We presented a fusion method for combining IR and visible light images for the purposes of face recognition. The algorithm aims at improved and robust recognition performance across variable lighting, facial expression, and presences of eyeglasses. Future work includes addressing the issues mentioned in the previous section, considering fitness approximation schemes <ref type="bibr" target="#b26">[28]</ref> to reduce the computational requirements of fitness evaluation, and investigating the effect of environmental (e.g., temperature changes), physical (e.g., lack of sleep) and physiological conditions (e.g., fear, stress) to IR performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of visible and IR image pairs and preprocessed images 7 Experimental Procedure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. The average performance values from Table 1, visualized as a grayscale image. See text for details. (a) ideal case (b) visible images (c) IR images, (d) fused images.</figDesc><graphic coords="10,309.08,208.42,68.01,54.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The first few eigenfaces of a fused image data set. The second and third eigenfaces show clear influence of the right and left lateral illumination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Recognition results for the facial expression tests.</figDesc><table><row><cell>VA</cell><cell cols="2">VA × (65.0, 1.0) EA (92.3, 0.7) (93.6, 0.7)</cell><cell cols="4">VF × × × × EF VL EL</cell></row><row><cell>EA</cell><cell>(65.2, 1.2) (93.7, 0.4) (93.1, 0.4)</cell><cell cols="5">92.8 -94.2 × × × × ×</cell></row><row><cell>VF</cell><cell cols="4">92.7 -93.4 × × × (65.3, 1.8) (90.5, 0.4) (93.3, 0.9)</cell><cell>(62.4, 0.4) (99.3, 0) (98.1, 0.5)</cell><cell>(36.4, 0.7) (87.9, 0.6) (91.4, 1.4)</cell></row><row><cell>EF</cell><cell cols="3">× × (59.2, 0.9) (91.3, 0.7) (91.2, 1.3)</cell><cell cols="2">92.5 -94.2 × (30.4, 0.7) 97.7 -98.6 (91.2, 0.4) (90.0, 1.4)</cell><cell>89.8 -92.3 (68.5, 1.1) (99.5, 0.4) (98.2, 0.4)</cell></row><row><cell>VL</cell><cell cols="3">90.3 -92.8 × × (60.5, 0.7) (98.4, 0.2) (97.6, 0.8)</cell><cell>(28.0, 1.2) (90.4, 0.1) (86.9, 0.8)</cell><cell cols="2">88.6 -91.5 × (66.4, 0.4) 97.7 -98.4 (91.3, 0.4) (94.5, 1.5)</cell></row><row><cell>EL</cell><cell cols="3">97.0 -98.4 × × (31.6, 1.1) (85.8, 0.7) (84.4, 0.3)</cell><cell>85.9 -87.4 (63.0, 0.9) (98.6, 0.2) (96.4, 0.7)</cell><cell>(68.9, 1.0) (91.7, 0.2) (93.7, 0.4)</cell><cell>92.9 -95.7 ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell>84.2 -84.7</cell><cell>95.8 -97.2</cell><cell>93.3 -94.1</cell><cell></cell></row><row><cell>number of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>images</cell><cell>1299</cell><cell>1266</cell><cell>435</cell><cell>429</cell><cell>864</cell><cell>837</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R P P</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CogNeuro</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="96" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantitative measurement of illumination invariance for face recognition using thermal infrared imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Computer Vision Beyond the Visible Spectrum: Methodsand Applications</title>
		<meeting><address><addrLine>Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Biometrics: Personal Identification in Networked Society</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The imaging issue in an automatic face/disguise detection system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Symosek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Computer Vision Beyond the Visible Spectrum: Methods and Applications</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Genetic algorithms in search, optimization, and machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">History, current status, and future of infrared identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Prokoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Computer Vision Beyond the Visible Spectrum</title>
		<imprint>
			<publisher>Hilton Head</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparison of visible and infra-red imagery for recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Automatic Face and Gesture Recognition</title>
		<meeting><address><addrLine>Killington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Appearance-based facial recognition using visible and thermal imagery: A comparative study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Socolinsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="2" to="3" />
		</imprint>
		<respStmt>
			<orgName>Equinox Corporation n</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparative study of face recognition performance with visible and thermal infrared imagery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Socolinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="217" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pca-based face recognition in infrared imagery: Baseline and comparative studies</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Analysis and Modeling of Faces and Gestures</title>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face detection in the near-ir spectrum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dowdall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="565" to="578" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local mapping for multispectral image visualization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scheunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="971" to="978" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A feature based approach to face recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Von Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="373" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An introduction to wavelets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating image filters for target recognition by genetic learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thrift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Genetic object recognition using combinations of views</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yfantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="132" to="146" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Genetic algorithms for object localization in a complex scene</title>
		<author>
			<persName><forename type="first">D</forename><surname>Swets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="595" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face detection and verification using genetic search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uthiram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Georgiopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Genetic feature subset selection for gender classification: A comparison study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Louis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boosting object detection using feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wavelets and image fusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Orr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="248" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multisensor image fusion using the wavelet transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="51" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Evolutionary Computation, Toward a New Philosophy of Machine Intelligence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fogel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Genetic Programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient and Accurate Parallel Genetic Algorithms and Evolutionary Computation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cantu-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fitness approximation in evolutionary computation -a survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sendhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computation Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
