<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A self-organizing neural network using ideas from the immune system to solve the traveling salesman problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Thiago</forename><forename type="middle">A S</forename><surname>Masutti</surname></persName>
							<email>thiago.masutti@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory of Intelligent Systems (LSIN)</orgName>
								<orgName type="institution">Catholic University of Santos (UNISANTOS)</orgName>
								<address>
									<addrLine>R. Dr. Carvalho de Mendonça</addrLine>
									<postCode>144</postCode>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vila Mathias Santos/SP</orgName>
								<address>
									<postCode>11070-906</postCode>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leandro</forename><forename type="middle">N</forename><surname>De Castro</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Graduate Program in Electrical Engineering</orgName>
								<orgName type="institution">Mackenzie University, R. da Consolação</orgName>
								<address>
									<addrLine>896 São Paulo/SP</addrLine>
									<postCode>01302-907</postCode>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A self-organizing neural network using ideas from the immune system to solve the traveling salesman problem</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7D6BD582EC7C4455666BE64135C984D3</idno>
					<idno type="DOI">10.1016/j.ins.2008.12.016</idno>
					<note type="submission">Received 26 July 2007 Accepted 18 December 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Traveling salesman problem Self-organizing networks Artificial immune systems Artificial neural networks Combinatorial optimization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most combinatorial optimization problems belong to the NP-complete or NP-hard classes, which means that they may require an infeasible processing time to be solved by an exhaustive search method. Thus, less expensive heuristics in respect to the processing time are commonly used. These heuristics can obtain satisfactory solutions in short running times, but there is no guarantee that the optimal solution will be found. Artificial Neural Networks (ANNs) have been widely studied to solve combinatorial problems, presenting encouraging results. This paper proposes some modifications on RABNET-TSP, an immune-inspired self-organizing neural network, for the solution of the Traveling Salesman Problem (TSP). The modified algorithm is compared with other neural methods from the literature and the results obtained suggest that the proposed method is competitive in relation to the other ones, outperforming them in many cases with regards to the quality (cost) of the solutions found, though demanding a greater time for convergence in many cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The study of combinatorial optimization problems is relevant from a theoretical and practical perspective. They are commonly found in real-world situations, such as routing and scheduling, and usually belong to the NP-complete and NP-hard classes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>. Thus, the time required to find a satisfactory solution may be infeasible, and the use of heuristics <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> is often preferred, for they are capable of providing a good trade-off between the quality of the solution and the time spent to find it.</p><p>Probably the most widely studied combinatorial optimization problem is the Traveling Salesman Problem (TSP), which can be described as follows: given a set of n cities (nodes in a graph), a salesman has to visit all the cities once and return to its origin minimizing the cost of the trip <ref type="bibr" target="#b23">[24]</ref>. It is well-known that a symmetric TSP with n cities has (n À 1)!/2 possible routes to perform, a value that grows factorially with the number of cities. Several algorithms have already been devised to solve TSP-like problems, such as branch and cut algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref> which uses cutting-plane and branch and bound <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref> techniques to find the global solution; the k-opt algorithm, which uses a technique that permutes k edges of the current route to find better routes; and Lin-Kerninghan <ref type="bibr" target="#b25">[26]</ref>, which adapts k-opt such that an ideal k is found. For the symmetric TSP, Lin-Kerninghan is considered one of the most effective methods to generate optimal or quasi-optimal solutions <ref type="bibr" target="#b19">[20]</ref>. Concerning a broader range of heuristics within the Natural Computing <ref type="bibr" target="#b10">[11]</ref> area of research, it is important to mention the use of Ant Colony Optimization <ref type="bibr" target="#b12">[13]</ref> and Evolutionary Algorithms <ref type="bibr" target="#b27">[28]</ref> to solve the TSP. Further information about the TSP and its solution approaches can be found in <ref type="bibr" target="#b1">[2]</ref>.</p><p>Still under the natural computing umbrella, a class of algorithms that have been broadly applied to combinatorial problems, particularly to the TSP, are the self-organized artificial neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> or slight variations of Kohonen's self-organizing feature map (SOM) <ref type="bibr" target="#b22">[23]</ref>. The application of these networks to optimization problems in general is important and intriguing. This is because they usually do not involve a quality measure to assess the performance of the solutions they generate, but are still capable of finding high quality solutions that can be used either as the final solution to the problem or that can be combined with another technique to find the global optimal. A self-organized competitive process is the main mechanism used to adjust the network weights and, thus, find a solution to the problem. In the particular case of competitive networks, which will be the focus of this paper, the network architecture is almost invariably of single layer with a circular neighborhood. In SOM-like networks for solving the TSP, the number N of neurons in the network is usually greater than or equal to the number n of cities in the TSP (N P n) and, usually, never greater than 2n.</p><p>The present paper proposes some modifications in the RABNET-TSP <ref type="bibr" target="#b33">[34]</ref> algorithm aiming at improving its performance in terms of computational efficiency (time to solve the problem) and optimality or efficacy(quality of the solution). The modified algorithm is directly compared with the original RABNET-TSP, termed here oRABNET-TSP, and two other well-known self-organized algorithms designed to solve TSP-like problems, namely Angeniol's network [1] and Somhom's network <ref type="bibr" target="#b39">[40]</ref>. The performance of all methods in terms of optimality is compared with the best values in the literature for a number of TSP instances taken from the TSPLIB <ref type="bibr" target="#b36">[37]</ref>. Besides, RABNET-TSP is compared with other approaches whose performances were taken directly from the literature.</p><p>This paper is organized as follows. Section 2 reviews the literature on self-organized neural networks applied to the TSP, and Section 3 describes RABNET-TSP stressing the modifications introduced. Section 4 presents the simulation results, including the heuristics used to tune the main parameters of all algorithms, and Section 5 concludes the paper with a discussion about the results presented and some perspectives for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Self-organized networks applied to the TSP: a brief review</head><p>This section makes a brief review of self-organized neural nets to solve TSP problems, focusing on the main differences between the proposals and describing in greater detail the works of Angeniol et al. [1] and Somhom et al. <ref type="bibr" target="#b39">[40]</ref>, which were re-implemented and used for direct performance comparisons.</p><p>In the work of Durbin and Wilshaw <ref type="bibr" target="#b11">[12]</ref>, the authors proposed the Elastic Net, in which the neurons are moved according to two strengths that force the network to be expanded as an elastic band until all cities be covered by the neurons, thus forming a TSP route. These forces act upon the neurons aiming at minimizing the size of the band. The elastic net was used to solve 50-city instances of the traveling salesman problem with results 2% worse than those obtained with the simulated annealing algorithm <ref type="bibr" target="#b21">[22]</ref>, regarding the quality of the solutions.</p><p>The work of Angeniol et al.</p><p>[1] was one of the pioneers in the use of self-organized neural networks to solve TSP-like problems. In their algorithm, the network is initialized with a single neuron and, through a process of duplication and pruning, the network architecture is allowed to vary with time. Based on experimental results the authors observed that the number of neurons is never greater than twice the number of cities in the instance investigated. Before the iterative process starts, the order of the cities is randomized and maintained fixed throughout all epochs. For each city (input pattern), the winner neuron is the one closest to it. All neurons in the network are moved towards the input city based on a function that takes into account the neighborhood between the neurons in a circular ring and the winner neuron:</p><formula xml:id="formula_0">f ðG; dÞ ¼ ð1=sqrtð2ÞÞ expðÀd 2 =G 2 Þ;<label>ð1Þ</label></formula><p>where d is the neighborhood degree between a given neuron and the winner neuron, and G is a parameter that controls the neighborhood influence. Parameter G usually starts with high values and decreases along the iterative procedure of adaptation.</p><p>The updating rule of neuron j is given by:</p><formula xml:id="formula_1">w j ¼ w j þ af ðG; dÞðx i À w j Þ;<label>ð2Þ</label></formula><p>where w j is the weight vector of neuron j, a is a learning rate, and x i is the coordinates' vector of city i. Angeniol's algorithm does not explicit use a learning rate, what is equivalent to making a = 1 in Eq. ( <ref type="formula" target="#formula_1">2</ref>) above.</p><p>After each epoch, all neurons that won the competition for more than one city are duplicated. The new copy is inserted in the network as a direct neighbor of the duplicated neuron and both neurons are inhibited in the following epoch; that is, if they win the competition for a certain city they will produce no updating in the network. This inhibition makes that 'twin' neurons be separated by the movements of their neighbors before they promote changes in the network. For a neuron to be excluded from the network, it must spend three iterations in a row without winning for any city.</p><p>Along with the work of Angeniol et al., the paper by Fort <ref type="bibr" target="#b14">[15]</ref> was pioneer in applying self-organizing networks to the TSP. Unlike [1], however, Fort uses a fixed number of neurons in the network, being 2n neurons for small instances, n 2 <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>, and 2.5n for larger instances. One of the main problems found by the author was the high computational time required to find reasonable solutions. The author also stresses the dynamic nature of the algorithm; that is, its capability of dealing with variations in the TSP instance.</p><p>Kim et al. <ref type="bibr" target="#b20">[21]</ref> proposed a SOM-based network for TSP in which the number of neurons is always the same as the number of cities. A restriction was imposed in the competitive phase such that a neuron wins for a single city each epoch. The author compares his proposal with [1] and concludes that his algorithm is superior in terms of efficiency and optimality.</p><p>In the work of Favata and Walker <ref type="bibr" target="#b13">[14]</ref>, a modified SOM with a number of neurons equal to the number of cities in the TSP instance to be solved is proposed. In the updating phase, only the winner and its two direct neighbors are updated. No constraint in relation to the competition phase is used and, thus, one neuron can be related to more than one city. According to the author, this makes that the network does not become sensitive to the order of the cities in the final solution and, in order to make this solution feasible, a heuristic is used to sort the cities. Their proposal was compared with a simulated annealing algorithm <ref type="bibr" target="#b21">[22]</ref> and concluded that, although their approach presented lower quality solutions, the processing time was smaller.</p><p>Burke and Damani <ref type="bibr" target="#b6">[7]</ref> proposed the Guilty Net, in which the number of neurons used is equal to the number of cities in the TSP. In order to guarantee a feasible solution (one neuron mapping each city), the authors used a bias to avoid that a neuron wins many competitions. This bias is called conscience. In a later work <ref type="bibr" target="#b4">[5]</ref>, Burke proposed a new network, called Vigilant Net, to improve the separation among neurons, so that each neuron maps a single city in the TSP. To reach that goal, a vigilance parameter is used to avoid that a single neuron wins too many competitions.</p><p>In the work of Somhom et al. <ref type="bibr" target="#b39">[40]</ref>, a SOM-like neural network is proposed with a number of neurons fixed as twice the number of cities in the problem. To guarantee the convergence of the algorithm to feasible solutions, no neuron is allowed to win twice within the same epoch. The updating rule follows that of Eq. ( <ref type="formula" target="#formula_1">2</ref>), with a neighborhood function given by:</p><formula xml:id="formula_2">f ðG; dÞ ¼ expðÀd 2 =G 2 Þ; d &lt; 0:2n; 0 otherwise; (<label>ð3Þ</label></formula><p>where n is the number of cities in the TSP instance, d is the neighborhood degree between a given neuron and the winning neuron, and G is the parameter that controls the neighborhood influence. Based on this neighborhood function, only 40% of the neurons are updated and, according to the authors, this restriction does not affect the quality of the solution, but improves the efficiency of the algorithm reducing the processing time.</p><p>To assess the performance of their algorithm, the authors applied it to several instances of the TSPLIB <ref type="bibr" target="#b36">[37]</ref> and compared its results with those obtained by the Guilty Net, the Elastic Net and the approach introduced by Matsuyama <ref type="bibr" target="#b29">[30]</ref>. The results presented in their paper showed a superior performance in terms of efficiency and optimality for most instances tested.</p><p>In the work of Aras et al. <ref type="bibr" target="#b2">[3]</ref> the authors propose a neural network in which the number of neurons varies according to duplication and pruning procedures similar to that of <ref type="bibr">[1]</ref>. An important feature of this paper is that the authors not only take into account the topological distribution of the cities in the TSP instance, they also account for the average position of the cities. The training algorithm has two adaptive modules: (1) an attraction module in which the winner neuron and its neighbors belonging to a certain ring move in the direction of the city presented; and (2) a dispersion module in which the neurons that were not updated in the attraction module are moved so that the neurons have the same average position as the cities. The authors assessed their method using the TSPLIB and compared the performance with the Guilty Net, the Angeniol's net and with a standard self-organizing map.</p><p>More recently, Cochrane and Beasley <ref type="bibr" target="#b7">[8]</ref> proposed a network named Co-Adaptive Net, characterized by the use of a heuristic that decides if the winner neuron and its neighbors will be adapted or not. The algorithm also has a heuristics that determines one route per epoch. Thus, the best route found by the algorithm is not necessarily the one at the end of the iterative procedure of adaptation, but the best one found so far. The algorithm allows one neuron to map more than one city and, in this case, a heuristics has to be used to determine the proposed route. The performance of their algorithm is compared to that of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref> with very good results.</p><p>The work of Reilly and Tchimev <ref type="bibr" target="#b35">[36]</ref> compares the Continuous Hopfield neural network <ref type="bibr" target="#b41">[42]</ref> with SOM. To compare the performance between the algorithms, the authors proposed a 52-city instance based on a map of Germany. As the Hopfield network was unable to process the full 52 cities instance, a 15 cities subset was used. With the obtained results the authors concluded that the SOM algorithm is clearly superior to the Hopfield algorithm, with a 62% better solution than the Hopfield network. Moreover, the SOM was able to solve the original 52 cities instance, achieving a good solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Modified RABNET-TSP</head><p>The constructive self-organizing network to be presented here is a slight variation of the Real-Valued Antibody Network designed to solve the Traveling Salesman Problem (RABNET-TSP) <ref type="bibr" target="#b33">[34]</ref>. The modifications proposed in the original algorithm aim at improving its efficacy (quality of the solutions found) and reducing the computational time of the algorithm. From now on, the acronym RABNET-TSP refers to the modified version to be presented here and oRABNET-TSP refers to its original version.</p><p>The main features of RABNET-TSP are: (1) feedforward neural network with no hidden layer; (2) competitive and unsupervised learning based on some immune principles <ref type="bibr" target="#b9">[10]</ref>; (3) constructive network architecture with growing and pruning phases based on some immune principles; and (4) pre-defined circular neighborhood.</p><p>The goal of RABNET-TSP is to position one network cell on top of each city of the TSP instance to be solved. The sequence of labeled network cells will correspond to the sequence of cities to be traversed by the salesman. The adaptation algorithm can be divided into nine distinct phases: (1) network initialization; (2) presentation of input patterns; (3) competition; (4) cooperation; (5) adaptation; <ref type="bibr" target="#b5">(6)</ref> growing; (7) stabilization of the winners; (8) network convergence; and (9) pruning. Each of these phases will be explained separately in the next sections and the proposed modifications will be stressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network initialization</head><p>In RABNET-TSP the network is initialized with only one antibod (in artificial neural networks, the analogue for an antibody is the neuron). The attribute vector (weight vector in the neural network literature) describing this antibody is equal to the centroid of the spatial distribution of the cities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Presentation of antigens (cities)</head><p>During the immune system evolution, an organism can meet a certain antigen several times. As the problem to be solved by the self-organizing network is the TSP, each city corresponds to one antigen (in artificial neural networks, the analogue for an antigen is an input pattern) and they are iteratively presented to the antibody network, simulating the meeting between the organism and an antigen. Prior to each epoch, the order of the cities is randomized so as to avoid that this order influences the network adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Competition</head><p>The competitive phase consists of finding the network antibody that is most similar to the antigen presented. Thus, that antibody J whose Euclidean distance to the antigen is minimal wins the competition and is selected:</p><formula xml:id="formula_3">J ¼ arg min j kAg i À Ab j k 8j:<label>ð4Þ</label></formula><p>where the antigen Ag i corresponds to the coordinate vector of city i and Ab j is the coordinates vector of antibody j. Each antibody in the network is allowed to recognize zero, one, or more antigens. This information is used in the growing and pruning procedures of the algorithm, and is stored in two variables: (1) one, c j , containing the number of cities mapped by antibody j; and (2) another, t i , containing the index of the antibody that won the competition for city i. The main idea we use here from artificial immune systems <ref type="bibr" target="#b9">[10]</ref> is that the most stimulated cell; that is, the one that wins the competition, will be the one with the highest affinity (most similar) to the city presented and that recognizes the largest number of cities (largest concentration of antigens).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Cooperation</head><p>The cooperative procedure used in RABNET-TSP is the same as the one for Kohonen's self-organizing maps <ref type="bibr" target="#b22">[23]</ref>. The antibodies in RABNET-TSP are arranged in a linear array of cells with a one-cell ring neighborhood, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> and expressed by Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_4">d jJ ¼ min½j j À J j; NÀ j j À J j;<label>ð5Þ</label></formula><p>where d jJ is the neighborhood degree between the winner antibody J and the present antibody j, and N is the number of antibodies in the ring. In Fig. <ref type="figure" target="#fig_0">1</ref>, cell 1 is a one-degree neighbor of cells 2 and 5, and a two-degree neighbor of cells 3 and 4.</p><p>When an antibody J wins the competition, it is moved towards the antigen, and its neighbors are moved as well, but with a smaller step. The farther the neighbor, the smaller the movement. One form of simulating the influence of a cell with its neighbors is to define a general kernel function h iJ that allows a decay of the cells' updating step smoothly with lateral distance <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>. This can be implemented by giving function h iJ a 'bell curve' shape. More specifically, let h iJ denote the topological neighborhood centered on the winning cell J and encompassing a set of neighbor cells, and let d iJ denote the topological lateral distance between the winning antibody J and its neighbor j: </p><formula xml:id="formula_5">f ðr; d jJ Þ ¼ h jJ ¼ expðÀd 2 jJ =2r 2 Þ;<label>ð6Þ</label></formula><p>where f(r,d jJ ) plays the same role as f(G,d) in Eq. ( <ref type="formula" target="#formula_1">2</ref>), and the value of r = r(t) will be responsible for controlling the neighborhood influence during the iterations, as will be explained in details shortly. The parameter r(t) is iteratively reduced until the network weights stabilize, as follows:</p><formula xml:id="formula_6">rðtÞ ¼ rð0Þ expðÀt=s 1 Þ;<label>ð7Þ</label></formula><p>where r(0) is the initial value of r, t is the epoch counter, and s 1 is defined as s 1 = 1000/log(r(0)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Adaptation</head><p>To present an effective response to pathogenic agents, the immune system counts with a learning process that involves the increasing of the Ag-Ab affinity at each meeting of the organism with a determined antigen. In RABNET-TSP, the adaptation phase performs this increase in Ab-Ag affinity updating the attribute vectors Ab i by moving them in the direction of the antigen presented Ab i , taking into account the neighborhood relationship defined in the cooperative phase. The adaptation rule to be proposed here takes into account the size of the topological neighborhood given by Eq. ( <ref type="formula" target="#formula_5">6</ref>), as follows:</p><p>Ab j ðt þ 1Þ ¼ Ab j ðtÞ þ a:h jJ ðAg i À Ab j ðtÞÞ; h jJ &gt; j;</p><formula xml:id="formula_7">Ab j ðtÞ otherwise;<label>ð8Þ</label></formula><p>where Ag i corresponds to the coordinates' vector of city i, a is the learning rate, and j is a parameter that defines the minimum value of h jJ so that Ab j is updated. This is one of the modifications we performed in relation to the original oRABNET-TSP. By constraining the updating to only those cases in which updating will be significant (achieved by setting j), the computational time of the algorithm can be substantially reduced, as will be empirically investigated in Section 4.1.</p><p>It is also common in self-organizing networks that the learning rate decreases with the iterations. Here we follow the same schedule as described in <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_8">aðtÞ ¼ að0Þ expðÀt=s 2 Þ;<label>ð9Þ</label></formula><p>where a(0) will depend on the general feature of the TSP instance to be solved and s 2 = 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Network growing</head><p>The growing process is inspired by the clonal selection and affinity maturation principles from immunology <ref type="bibr" target="#b9">[10]</ref>. Basically, the most stimulated antibody in the immune system is selected for cloning (splitting). The stimulation level of an antibody is determined by two parameters: (1) the affinity level of antibody j in relation to the antigen presented j, which will be measured here by their Euclidean distance; and (2) the concentration (number), c j , of antigens recognized by antibody j.</p><p>The possibility of growing is tested every iteration, as follows:</p><p>Antibody I that recognizes the highest concentration of antigens is selected: I = arg max i (c j ) "j. If two antibodies have the same highest concentration c, then one of them is selected randomly.</p><p>Among all antigens (cities) recognized by antibody I, Ag i with the greatest Euclidean distance to Ab I is selected.</p><p>If the distance between the selected antibody and Ag i is greater than a pre-defined threshold e, then Ab I is cloned:</p><formula xml:id="formula_9">If kAg i -Ab I k &gt; e, then clone.</formula><p>The vector describing the features of the newly created antibody is the same as the one from its parent antibody, and its neighborhood is of degree one in relation to it. The whole network neighborhood is then reconstructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Stabilization of the winners</head><p>To reduce the processing time of the algorithm, we propose the Winners Stabilization phase which suppresses cooperation as soon as no variation, Dv, in the winners is detected. This is obtained when the winners for each antigen in a given epoch are the same as the ones in the previous epoch:</p><formula xml:id="formula_10">Dv ¼ X n i¼1 jJ i ðt À 1Þ À J i ðtÞj;<label>ð10Þ</label></formula><p>where J i (t À 1) and J i (t) are the indices of the winners for city i at epoch t À 1 and t, respectively; and n is the number of cities.</p><p>To quantify the gain in performance with the addition of this phase, for a TSP instance with 76 cities, the average computational time was 5.5% smaller than with the original algorithm (oRABNET-TSP, 9.987s; RABNET-TSP, 9.462s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Convergence criterion</head><p>In order for the algorithm to converge to a valid solution, a well-defined route has to be determined by the network antibodies. Therefore, at least one antibody has to map each city. To guarantee that this is going to occur, every antibody will have to be at least at a distance k from a city. When this condition is met, the learning algorithm automatically stops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9.">Network pruning</head><p>Pruning in RABNET-TSP is very simple. After the iterative procedure of adaptation is concluded, all antibodies with concentration level c j = 0 "j, are removed from the network. The expected result is an antibody mapping each city in the instance. This way, the pre-defined neighborhood will indicate the TSP route determined.</p><p>Algorithm 1 summarizes the pseudo-code for the modified RABNET-TSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance evaluation</head><p>To assess the performance of RABNET-TSP, several experiments were conducted using TSP instances available at TSPLIB <ref type="bibr" target="#b36">[37]</ref>. Another instance with 64 cities symmetrically distributed, ncit64, proposed in <ref type="bibr" target="#b33">[34]</ref> was also employed. To compare the performance of the modified RABNET-TSP, the following algorithms were implemented: (1) the original algorithm oRABNET-TSP <ref type="bibr" target="#b33">[34]</ref>; (2) Angeniol's algorithm [1], named here AVT; (3) Somhom's algorithm <ref type="bibr" target="#b39">[40]</ref>, named here SME. The reason why we chose Angeniol's algorithm for comparison is because it is one of the pioneers in the literature and it also incorporates a growing mechanism in the network. The choice of Somhom's goes in the opposite direction, it constitutes a more recent approach with a fixed number of neurons in the network and with good results reported.</p><p>The algorithms were implemented in Matlab and run on a PIV 3.0 GHz with 1 GB RAM. The results presented are the minimum, average and standard deviation of the cost (tour length) over 30 runs. The average number of epochs, time for convergence and number of cells generated are also listed. Furthermore, the performance of other algorithms from the literature is considered when available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The influence of tunable parameters</head><p>All the algorithms investigated have some parameters to be adjusted before they can be applied to the problems at hand. This section briefly investigates the influence of the tunable parameters of all algorithms used for comparisons, with emphasis on the proposed RABNET-TSP, namely, a(0), r(0), e, k and j. The analyses were undertaken using the eil76 TSP instance <ref type="bibr" target="#b36">[37]</ref>, and all results presented involve 30 independent runs.</p><p>RABNET-TSP is initialized with a single antibody and is allowed do grow during training. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the behavior of the network during the iterative procedure of adaptation. Note that the network behaves like a growing ring that molds itself to cover all nodes in the graph.</p><p>When an antibody wins for more than one city, it will be cloned when its distance from the farthest city is greater than or equal to e, thus avoiding that an antibody is cloned unnecessarily. According to <ref type="bibr" target="#b33">[34]</ref>, a reasonable value for e is:</p><formula xml:id="formula_11">e ¼ 0:2 Ã md;<label>ð11Þ</label></formula><p>where md is the smallest distance among all the cities in the map. Furthermore, in order for a route to emerge, each antibody must be at a minimum distance from each city. Parameter k is used to guarantee this approximation and its value is also a function of md, as follows:</p><formula xml:id="formula_12">k ¼ 0:01 Ã md:<label>ð12Þ</label></formula><p>Parameter r is responsible for controlling the influence of the neighborhood in the adaptation of the antibodies. Its initial value, r(0), directly influences the total number of antibodies used in the network and its convergence time, as can be observed from Fig. <ref type="figure">3</ref>. As illustrated in Fig. <ref type="figure">3d</ref>, one can note that, on average, the initial value of r does not influences significantly the quality of the solution found, but its right tune had some crucial effect on founding the best solutions that will be presented in this paper. An appropriate value for r(0) varies according to the TSP instance to be tackled and should be obtained with some preliminary experiments. When this is not possible we suggest the use of r(0) = 16, which performed well for most of our experiments.</p><p>Large values for parameter a(0) imply in major changes in the network structure at each epoch, and vice-versa. As can be observed from Fig. <ref type="figure">4</ref>, large values for a(0) result in large numbers of antibodies in the network and large numbers of epochs for convergence, thus requiring large convergence time. Except for low values of a(0) (e.g., 0.1 or 0.3), the quality of the average solution remains almost unalterable, indicating a low influence of this parameter on the quality of the solution. As with the case of r(0), an optimized value for parameter a(0) can only be obtained empirically and for each case independently. However, as suggested in <ref type="bibr" target="#b33">[34]</ref>, the use of a(0) = 0.1 is appropriate for symmetric TSP instances, and a(0) = 1.25 for the other instances.</p><p>The value of parameter h in Eq. ( <ref type="formula" target="#formula_7">8</ref>) directly influences the step size of the adaptation of a given antibody's attribute vector. In Fig. <ref type="figure" target="#fig_4">5</ref> we observe that after some epochs the h value is considerably small for large neighborhoods d, thus suggesting that the updating procedure could be constrained so as to avoid unnecessary computational effort. Some authors restrict the adaptation, decreasing the effective neighborhood of the winner neuron by performing the adaptation only to values of d lower than a pre-determined threshold <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>. This approach does not consider that, at the beginning of the learning process, some neighbors far from the winner neuron carry out a significant adaptation to the network and that, at the end of the adaptive process, neighbors close to the winner neuron no longer carry out significant adaptations. According to this, we propose that this restriction occurs on h, adding a threshold j to it, thus allowing a higher flexibility of the method. Therefore, the standard value we propose for the threshold j in Eq. ( <ref type="formula" target="#formula_7">8</ref>) is j = 0.01. To illustrate the gain in performance obtained with this approach for the example studied in this section, eil76, the average computational time was reduced from 17.80 to 14.43 s, a gain in performance of approximately 20%.</p><p>For the AVT algorithm [1], G i = round(n/2), where round(Á) is the operator that rounds the value within parenthesis towards the closest integer and n is the number of cities in the instance. The parameter a(0) was chosen to be 0.002 for ncit64, 0.02 for all other instances of size less than 500, and 0.2 for the instances with more than 500 cities.</p><p>In <ref type="bibr" target="#b39">[40]</ref> the authors suggest that G(0) has to be defined according to the problem being tested, and in <ref type="bibr" target="#b40">[41]</ref> the authors proposed the following heuristics to define G(0):</p><formula xml:id="formula_13">Gð0Þ ¼ 0:06 þ 12:41 Ã n:<label>ð13Þ</label></formula><p>Although the context of <ref type="bibr" target="#b40">[41]</ref> is slightly different from that of their previous paper, we chose this heuristics to tune G(0).</p><p>Parameter a was set to 0.03 for all problems and the learning rate was set to 0.6 for all instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental evaluation</head><p>The comparison to be performed here will take into account the total cost of the solutions found by the algorithms, the computational cost (in terms of time and epochs for convergence) of each algorithm, and the percentage deviation of the solutions found in relation to the best known solutions. All algorithms were run 30 times for each instance and the results presented include the best and average solutions found. The number of cities in each instance is the number following the letters that name the instances, for example, in fl1400 the number of cities is 1400. Sigma</p><formula xml:id="formula_14">1 4 7 1 0 1 3 1 6 1 9 2 2 2 5 2 8</formula><p>Sigma</p><formula xml:id="formula_15">1 4 7 1 0 1 3 1 6 1 9 2 2 2 5 2 8</formula><p>Sigma Table <ref type="table" target="#tab_0">1</ref> compares the performance of RABNET-TSP, oRABNET-TSP <ref type="bibr" target="#b33">[34]</ref>, AVT [1], and SME <ref type="bibr" target="#b39">[40]</ref> in terms of the best and average solutions found for thirty independent runs. The best results found are detached in bold, and the table is shadowed for the five instances with more 500 cities. In this table, all algorithms were re-implemented using Matlab 7.0. We chose to re-implement the algorithms in order to reduce the bias introduced by the implementation and programming language when comparing the computational cost of the different algorithms. It is possible to observe from Table <ref type="table" target="#tab_0">1</ref> that for the smaller instances the modified algorithm presents slightly superior results in absolute terms. For the instances with more than 1000 cities, however, AVT and SME performed slightly better.</p><formula xml:id="formula_16">1 4 7 1 0 1 3 1 6 1 9 2 2 2 5 2 8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sigma</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Antibodies</head><p>Table <ref type="table" target="#tab_1">2</ref> compares the computational time and number of epochs for convergence for each algorithm. It also brings the percentage deviation of the cost of each solution found by the algorithms when compared to the best known solutions. In this table it is possible to observe that the SME algorithm is the least computational intensive one in terms of time and number of epochs for convergence, but it is the least efficient one in terms of quality (tour length) of the solutions found. By  comparing the results of RABNET-TSP with those of oRABNET-TSP the effectiveness of the modifications introduced can be observed in the reduced time and number of epochs for convergence without a loss in performance.</p><p>In Table <ref type="table" target="#tab_2">3</ref> a trade-off between the performance of RABNET-TSP and the results of other proposals from the literature is presented. Again it is possible to observe that RABNET-TSP is competitive when the target is to find the lowest cost routes for the TSP. Besides, the average solution obtained with RABNET-TSP is of good quality, generally, better than the results presented with other algorithms from the literature for the set of instances used in this work, as can be observed in Fig. <ref type="figure" target="#fig_5">6</ref>. Fig. <ref type="figure" target="#fig_6">7</ref> illustrates some of the best routes found by RABNET-TSP and their costs. Note that the way the network grows, like an expanding ring, reduces the possibility of crossings in the routes, which are characteristic of locally optimal routes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and future investigations</head><p>Aiming at improving the efficacy and the efficiency of an immune-inspired network for discrete optimization, this paper proposed two modifications in the original algorithm: (1) the addition of a threshold to the use of the kernel function, Eq. ( <ref type="formula" target="#formula_7">8</ref>), for the antibodies updating; and (2) the use of a winners' stabilization mechanism. The resultant and original algorithms were tested on a much larger set of TSP instances than in the original publication <ref type="bibr" target="#b33">[34]</ref> and the results were directly compared to that of two other well-known algorithms from the literature and to the best results available to date. Comparisons with results available in the literature were also made.</p><p>It is important to remark that the networks assessed in this work are unsupervised, in the sense that no information about the quality of the solutions found is used to guide the search. In spite of that, the modified algorithm proposed here was capable of finding solutions that are less than 1% worse than the best results found to date for almost all instance with less than 500 cities, thus showing the strength of the self-organizing learning processes. For larger instances, a percentage deviation of up to 14% was found in relation to the best known solutions, which means that either further tuning/improvements could be made in the algorithm or these would only be suitable for smaller scale problems. But this type of conclusion requires further investigation.</p><p>In relative terms, the results obtained show that RABNET-TSP is superior when compared with the other approaches implemented in terms of quality of the solutions found (tour length), but in most cases it required a greater computational time to solve the problems. However, for larger TSP instances, with n P 1000, RABNET-TSP was less computational intensive but was not the most efficient method in terms of tour length.</p><p>Although the better solutions for each instance were found with an appropriate set of parameters, empirical tests indicated that small variations of these values do not influence significantly the quality of the average solution found. Moreover, the average solution obtained with the proposed algorithm was satisfactory, usually better than those found by similar algorithms. These two characteristics suggest that, even with suboptimal values of the parameters, the algorithm is capable of providing, on average, good quality solutions. However, this also requires further experiments for a more solid conclusion.</p><p>The encouraging results presented here led us to adapt the algorithm to solve more complex routing problems, such as the multiple traveling salesmen problem (MTSP) <ref type="bibr" target="#b28">[29]</ref>, and the capacitated vehicle routing problem (CVRP). Also, further investigations must be performed with larger TSP instances and the hybrid use of the proposed algorithm with route improvement heuristics, such as k-opt and simulated annealing. //r(0)-&gt;initial value for r;</p><p>//e-&gt;cloning threshold; //k-&gt;convergence threshold; //j-&gt;cooperation threshold; //itMax-&gt;maximum number of epochs; n number of cities;</p><p>s 1 1000/log(r o ); Ab initialize a single antibody; N 1;//initial number of antibodies neighborhood 1; for(it = 1:itMax) a(it) a(it À 1) * exp(Àit/s 1 ); r(it) r(it À 1) * exp(Àit/s 2 ); c 0;//concentration vector t 0;//winners vector Ag random order for(i = 1:n)//for all antigens ag Ag i ;//current antigen J competition(ag,Ab);// Eq. ( <ref type="formula" target="#formula_3">4</ref>) c(J) c(J) + 1;//increases the concentration of the winner t(k) J;//winner for antigen k if(neighborhood == 1) H = cooperation();// Eq. ( <ref type="formula" target="#formula_5">6</ref>) Ab = adaptation();// Eq. ( <ref type="formula" target="#formula_7">8</ref>) else Ab J = adaptation();// Eq. ( <ref type="formula" target="#formula_7">8</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Ring structure and neighborhood in RABNET-TSP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. RABNET-TSP learning for the instance eil76. (a) 200 epochs; (b) 400 epochs; (c) 600 epochs; (d) final solution after 837 epochs.</figDesc><graphic coords="6,68.03,374.57,403.38,298.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 . 6 AlfaFig. 4 .</head><label>364</label><figDesc>Fig. 3. Influence of the initial value of the parameter r(0) on the proposed algorithm. The black curve represents the average values and the edges of the bars represent the minimum and maximum values, all obtained after 30 runs. (a) Number of antibodies before the pruning phase; (b) Epochs for convergence; (c) convergence time; and (d) route cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Influence of the neighborhood function h along the iterations for different neighborhood sizes d.</figDesc><graphic coords="10,160.50,56.13,219.25,165.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Percentage deviation of the average solution to the best known solution of each TSP instance to the following algorithms: RABNET-TSP, Somhom et al. [40] and Cochrane and Beasley [8].</figDesc><graphic coords="12,125.30,115.76,291.28,105.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Best routes found by RABNET-TSP: (a) ncit64; (b) eil51; (c) lin105; (d) kroA100; (e) berlin52 and (f) kroB150.</figDesc><graphic coords="13,75.18,54.71,397.60,456.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Computational results of RABNET-TSP, oRABNET-TSP, AVT and SME. BKS is the best known solution; Mean is the average solution; SD is the standard deviation; and Best is the best solution found. All results are presented for 30 runs.</figDesc><table><row><cell>TSP</cell><cell>BKS</cell><cell>RABNET-TSP</cell><cell></cell><cell></cell><cell cols="2">oRABNET-TSP</cell><cell></cell><cell>AVT</cell><cell></cell><cell></cell><cell>SME</cell><cell></cell></row><row><cell>Instance</cell><cell></cell><cell>Mean</cell><cell>SD</cell><cell>Best</cell><cell>Mean</cell><cell>SD</cell><cell>Best</cell><cell>Mean</cell><cell>SD</cell><cell>Best</cell><cell>Mean</cell><cell>SD</cell><cell>Best</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Computational effort for each algorithm and percentile deviation in relation to the best known solution. Time is the average running time in seconds, Epochs refers to the average number of epochs for convergence and PDbest is the percentile deviation of the best solution found by each algorithm in relation to the best known solution. All results are presented for 30 runs.</figDesc><table><row><cell>TSP Instance</cell><cell>RABNET-TSP</cell><cell></cell><cell></cell><cell cols="2">oRABNET-TSP</cell><cell></cell><cell>AVT</cell><cell></cell><cell></cell><cell>SME</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Time (s)</cell><cell>Epochs</cell><cell>PDbest</cell><cell>Time (s)</cell><cell>Epochs</cell><cell>PDbest</cell><cell>Time (s)</cell><cell>Epochs</cell><cell>PDbest</cell><cell>Time (s)</cell><cell>Epochs</cell><cell>PDbest</cell></row><row><cell>ncit64</cell><cell>6.87</cell><cell>556.2</cell><cell>0.00</cell><cell>16.72</cell><cell>1028.6</cell><cell>0.00</cell><cell>50.90</cell><cell>4339.0</cell><cell>1.28</cell><cell>5.22</cell><cell>262.0</cell><cell>1.28</cell></row><row><cell>eil51</cell><cell>7.95</cell><cell>747.3</cell><cell>0.23</cell><cell>10.64</cell><cell>764.3</cell><cell>0.70</cell><cell>3.24</cell><cell>412.4</cell><cell>1.41</cell><cell>3.34</cell><cell>249.9</cell><cell>1.64</cell></row><row><cell>eil76</cell><cell>15.07</cell><cell>818.6</cell><cell>0.56</cell><cell>19.09</cell><cell>843.9</cell><cell>0.74</cell><cell>6.91</cell><cell>450.2</cell><cell>2.97</cell><cell>6.77</cell><cell>262.6</cell><cell>2.60</cell></row><row><cell>eil101</cell><cell>21.72</cell><cell>845.8</cell><cell>1.43</cell><cell>30.60</cell><cell>943.1</cell><cell>1.91</cell><cell>12.14</cell><cell>477.8</cell><cell>4.13</cell><cell>11.37</cell><cell>271.6</cell><cell>1.75</cell></row><row><cell>berlin52</cell><cell>8.91</cell><cell>730.8</cell><cell>0.00</cell><cell>11.43</cell><cell>772.4</cell><cell>2.31</cell><cell>3.33</cell><cell>415.4</cell><cell>3.13</cell><cell>3.48</cell><cell>254.7</cell><cell>2.29</cell></row><row><cell>bier127</cell><cell>32.02</cell><cell>872.3</cell><cell>0.58</cell><cell>47.48</cell><cell>1027.3</cell><cell>0.40</cell><cell>19.57</cell><cell>507.5</cell><cell>1.55</cell><cell>17.97</cell><cell>286.9</cell><cell>1.32</cell></row><row><cell>ch130</cell><cell>33.31</cell><cell>864.1</cell><cell>0.57</cell><cell>51.43</cell><cell>1037.3</cell><cell>0.52</cell><cell>20.70</cell><cell>503.4</cell><cell>2.54</cell><cell>18.55</cell><cell>283.2</cell><cell>1.52</cell></row><row><cell>ch150</cell><cell>39.90</cell><cell>894.4</cell><cell>1.13</cell><cell>64.90</cell><cell>1117.8</cell><cell>1.55</cell><cell>27.63</cell><cell>513.2</cell><cell>1.62</cell><cell>26.02</cell><cell>288.1</cell><cell>1.58</cell></row><row><cell>rd100</cell><cell>19.88</cell><cell>803.2</cell><cell>0.91</cell><cell>31.34</cell><cell>960.0</cell><cell>0.47</cell><cell>11.91</cell><cell>475.8</cell><cell>2.25</cell><cell>11.31</cell><cell>275.8</cell><cell>1.49</cell></row><row><cell>lin105</cell><cell>20.62</cell><cell>800.9</cell><cell>0.00</cell><cell>31.75</cell><cell>927.2</cell><cell>0.00</cell><cell>13.27</cell><cell>485.4</cell><cell>4.31</cell><cell>12.44</cell><cell>277.3</cell><cell>0.00</cell></row><row><cell>lin318</cell><cell>110.28</cell><cell>1038.4</cell><cell>1.92</cell><cell>204.44</cell><cell>1393.9</cell><cell>2.25</cell><cell>155.07</cell><cell>586.3</cell><cell>6.76</cell><cell>115.27</cell><cell>314.0</cell><cell>2.68</cell></row><row><cell>kroA100</cell><cell>15.87</cell><cell>693.2</cell><cell>0.24</cell><cell>30.77</cell><cell>944.9</cell><cell>0.41</cell><cell>12.09</cell><cell>479.9</cell><cell>8.11</cell><cell>11.35</cell><cell>277.0</cell><cell>0.60</cell></row><row><cell>kroA150</cell><cell>39.89</cell><cell>616.0</cell><cell>0.58</cell><cell>63.27</cell><cell>1075.2</cell><cell>1.54</cell><cell>28.00</cell><cell>518.0</cell><cell>9.14</cell><cell>26.31</cell><cell>290.2</cell><cell>1.53</cell></row><row><cell>kroA200</cell><cell>61.88</cell><cell>954.2</cell><cell>0.79</cell><cell>101.91</cell><cell>1215.2</cell><cell>0.77</cell><cell>52.20</cell><cell>542.3</cell><cell>7.84</cell><cell>45.58</cell><cell>299.5</cell><cell>2.64</cell></row><row><cell>kroB100</cell><cell>22.06</cell><cell>630.0</cell><cell>0.91</cell><cell>29.94</cell><cell>928.6</cell><cell>2.06</cell><cell>11.90</cell><cell>478.2</cell><cell>8.51</cell><cell>11.34</cell><cell>277.0</cell><cell>1.84</cell></row><row><cell>kroB150</cell><cell>40.61</cell><cell>892.7</cell><cell>0.51</cell><cell>64.09</cell><cell>1093.8</cell><cell>1.01</cell><cell>27.93</cell><cell>514.7</cell><cell>6.72</cell><cell>26.05</cell><cell>290.1</cell><cell>0.81</cell></row><row><cell>kroB200</cell><cell>57.91</cell><cell>894.0</cell><cell>0.68</cell><cell>107.51</cell><cell>1258.1</cell><cell>1.34</cell><cell>52.49</cell><cell>542.2</cell><cell>9.90</cell><cell>45.48</cell><cell>299.4</cell><cell>0.90</cell></row><row><cell>kroC100</cell><cell>21.65</cell><cell>817.1</cell><cell>0.80</cell><cell>30.96</cell><cell>947.0</cell><cell>0.80</cell><cell>11.93</cell><cell>476.7</cell><cell>7.69</cell><cell>11.34</cell><cell>276.9</cell><cell>0.83</cell></row><row><cell>kroD100</cell><cell>21.92</cell><cell>814.5</cell><cell>0.38</cell><cell>31.24</cell><cell>948.1</cell><cell>0.77</cell><cell>12.05</cell><cell>475.7</cell><cell>8.37</cell><cell>11.36</cell><cell>277.4</cell><cell>0.97</cell></row><row><cell>kroE100</cell><cell>21.49</cell><cell>834.4</cell><cell>1.48</cell><cell>30.48</cell><cell>940.6</cell><cell>1.63</cell><cell>11.92</cell><cell>475.0</cell><cell>7.13</cell><cell>11.43</cell><cell>277.1</cell><cell>1.41</cell></row><row><cell>rat575</cell><cell>215.28</cell><cell>1037.4</cell><cell>4.05</cell><cell>427.10</cell><cell>1473.4</cell><cell>3.93</cell><cell>48.76</cell><cell>61.0</cell><cell>19.70</cell><cell>372.08</cell><cell>330.4</cell><cell>4.68</cell></row><row><cell>rat783</cell><cell>352.03</cell><cell>1152.9</cell><cell>5.00</cell><cell>609.14</cell><cell>1499.9</cell><cell>4.30</cell><cell>112.66</cell><cell>63.2</cell><cell>19.60</cell><cell>709.82</cell><cell>340.8</cell><cell>5.79</cell></row><row><cell>rl1323</cell><cell>946.32</cell><cell>1502.6</cell><cell>11.31</cell><cell>1222.16</cell><cell>1590.0</cell><cell>9.20</cell><cell>969.18</cell><cell>70.63</cell><cell>8.57</cell><cell>2124.02</cell><cell>362.1</cell><cell>9.47</cell></row><row><cell>fl1400</cell><cell>1105.80</cell><cell>1552.0</cell><cell>3.60</cell><cell>1273.23</cell><cell>1551.1</cell><cell>3.07</cell><cell>502.64</cell><cell>69.8</cell><cell>2.59</cell><cell>2353.00</cell><cell>359.0</cell><cell>2.14</cell></row><row><cell>d1655</cell><cell>1569.25</cell><cell>1766.3</cell><cell>14.15</cell><cell>1839.29</cell><cell>1763.3</cell><cell>13.19</cell><cell>1798.43</cell><cell>71.27</cell><cell>12.63</cell><cell>3346.77</cell><cell>367.4</cell><cell>8.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Computational results of RABNET-TSP in comparison with other results directly taken from the literature (when available). BKS is the best known solution, PDav is the percentage deviation of the average solution found in relation to the best known solution, and PDbest is the percentage deviation of the best solution found in relation to the best known solution. All results for the RABNET-TSP are taken from 30 runs.</figDesc><table><row><cell>TSP Instance</cell><cell>BKS</cell><cell>RABNET-TSP</cell><cell></cell><cell>SME</cell><cell></cell><cell cols="2">Aras et al. [3]</cell><cell cols="2">Cochrane and Beasley [8]</cell></row><row><cell></cell><cell></cell><cell>PDav</cell><cell>PDbest</cell><cell>PDav</cell><cell>PDbest</cell><cell>PDav</cell><cell>PDbest</cell><cell>PDav</cell><cell>PDbest</cell></row><row><cell>eil51</cell><cell>426</cell><cell>2.69</cell><cell>0.23</cell><cell>3.43</cell><cell>1.64</cell><cell>-</cell><cell>2.86</cell><cell>2.89</cell><cell>0.94</cell></row><row><cell>eil76</cell><cell>538</cell><cell>3.41</cell><cell>0.56</cell><cell>5.46</cell><cell>1.49</cell><cell>-</cell><cell>4.98</cell><cell>4.35</cell><cell>2.04</cell></row><row><cell>eil101</cell><cell>629</cell><cell>3.12</cell><cell>1.43</cell><cell>4.17</cell><cell>1.27</cell><cell>-</cell><cell>4.65</cell><cell>3.78</cell><cell>1.11</cell></row><row><cell>berlin52</cell><cell>7542</cell><cell>5.18</cell><cell>0.00</cell><cell>5.81</cell><cell>0.00</cell><cell>-</cell><cell>-</cell><cell>7.01</cell><cell>0.00</cell></row><row><cell>bier127</cell><cell>118,282</cell><cell>2.20</cell><cell>0.58</cell><cell>3.41</cell><cell>0.84</cell><cell>-</cell><cell>-</cell><cell>3.00</cell><cell>0.69</cell></row><row><cell>ch130</cell><cell>6110</cell><cell>2.82</cell><cell>0.57</cell><cell>2.82</cell><cell>1.02</cell><cell>-</cell><cell>-</cell><cell>2.82</cell><cell>1.13</cell></row><row><cell>ch150</cell><cell>6528</cell><cell>3.22</cell><cell>1.13</cell><cell>3.91</cell><cell>1.76</cell><cell>-</cell><cell>-</cell><cell>3.23</cell><cell>1.78</cell></row><row><cell>rd100</cell><cell>7910</cell><cell>3.66</cell><cell>0.91</cell><cell>3.99</cell><cell>1.28</cell><cell>-</cell><cell>2.09</cell><cell>3.64</cell><cell>1.19</cell></row><row><cell>lin105</cell><cell>14,379</cell><cell>0.15</cell><cell>0.00</cell><cell>2.75</cell><cell>0.62</cell><cell>-</cell><cell>1.98</cell><cell>1.08</cell><cell>0.00</cell></row><row><cell>lin318</cell><cell>42,029</cell><cell>3.97</cell><cell>1.92</cell><cell>4.16</cell><cell>2.17</cell><cell>-</cell><cell>-</cell><cell>4.31</cell><cell>2.65</cell></row><row><cell>kroA100</cell><cell>21,282</cell><cell>1.13</cell><cell>0.24</cell><cell>2.62</cell><cell>0.31</cell><cell>-</cell><cell>-</cell><cell>1.31</cell><cell>0.57</cell></row><row><cell>kroA150</cell><cell>26,524</cell><cell>3.14</cell><cell>0.58</cell><cell>4.61</cell><cell>1.47</cell><cell>-</cell><cell>-</cell><cell>3.06</cell><cell>1.55</cell></row><row><cell>kroA200</cell><cell>29,368</cell><cell>2.80</cell><cell>0.79</cell><cell>3.70</cell><cell>0.86</cell><cell>-</cell><cell>5.72</cell><cell>3.27</cell><cell>0.92</cell></row><row><cell>kroB100</cell><cell>22,141</cell><cell>2.35</cell><cell>0.91</cell><cell>2.91</cell><cell>1.43</cell><cell>-</cell><cell>-</cell><cell>2.20</cell><cell>1.53</cell></row><row><cell>kroB150</cell><cell>26,130</cell><cell>1.92</cell><cell>0.51</cell><cell>2.82</cell><cell>1.67</cell><cell>-</cell><cell>-</cell><cell>2.60</cell><cell>1.06</cell></row><row><cell>kroB200</cell><cell>29,437</cell><cell>2.37</cell><cell>0.68</cell><cell>4.83</cell><cell>1.55</cell><cell>-</cell><cell>-</cell><cell>2.31</cell><cell>0.88</cell></row><row><cell>kroC100</cell><cell>20,749</cell><cell>1.07</cell><cell>0.80</cell><cell>3.18</cell><cell>0.85</cell><cell>-</cell><cell>-</cell><cell>1.70</cell><cell>0.80</cell></row><row><cell>kroD100</cell><cell>21,294</cell><cell>1.89</cell><cell>0.38</cell><cell>2.28</cell><cell>0.89</cell><cell>-</cell><cell>-</cell><cell>1.87</cell><cell>0.80</cell></row><row><cell>kroE100</cell><cell>22,068</cell><cell>2.93</cell><cell>1.48</cell><cell>3.35</cell><cell>1.71</cell><cell>-</cell><cell>-</cell><cell>2.56</cell><cell>1.52</cell></row><row><cell>rat575</cell><cell>6773</cell><cell>5.06</cell><cell>4.05</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.20</cell><cell>4.89</cell></row><row><cell>rat783</cell><cell>8806</cell><cell>6.11</cell><cell>5.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.57</cell><cell>5.66</cell></row><row><cell>rl1323</cell><cell>270,199</cell><cell>13.00</cell><cell>11.31</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.85</cell><cell>11.29</cell></row><row><cell>fl1400</cell><cell>20,127</cell><cell>4.88</cell><cell>3.60</cell><cell>2.78</cell><cell>1.68</cell><cell>-</cell><cell>-</cell><cell>4.26</cell><cell>2.12</cell></row><row><cell>d1655</cell><cell>62,128</cell><cell>16.07</cell><cell>14.15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10.09</cell><cell>9.10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>T.A.S. Masutti, L.N. de Castro / Information Sciences 179 (2009) 1454-1468</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank CNPq and Fapesp for the financial support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-organizing feature maps and the travelling salesman problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Angeniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Croix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y. Le</forename><surname>Vaubois</surname></persName>
		</author>
		<author>
			<persName><surname>Texier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="289" to="293" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Traveling Salesman Problem: A Computational Study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Applegate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bixby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chvatal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Kohonen network incorporating explicit statistics and its application to the travelling salesman problem</title>
		<author>
			<persName><forename type="first">N</forename><surname>Aras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Altinel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1273" to="1284" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Schaum&apos;s Outline of Theory and Problems of Operations Research</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bronson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Naadimuthu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conscientious&quot; neural nets for tour construction in the traveling salesman problem: the vigilant net</title>
		<author>
			<persName><forename type="first">L</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Operations Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural methods for the traveling salesman problem: insights from operations research</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="681" to="690" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The guilty net for the traveling salesman problem</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Damany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Operations Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="255" to="265" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The co-adaptive neural network approach to the Euclidean travelling salesman problem</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Cochrane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Beasley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1499" to="1525" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solution of a large-scale traveling salesman problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dantzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fulkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="393" to="410" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Timmis</surname></persName>
		</author>
		<title level="m">Artificial Immune Systems: A New Computational Intelligence Approach</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fundamentals of natural computing: an overview</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>De Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics of Life Reviews</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An analogue approach to the travelling salesman problem using an elastic net method</title>
		<author>
			<persName><forename type="first">R</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Willshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">326</biblScope>
			<biblScope unit="page" from="689" to="691" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exchange strategies for multiple ant colony system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ellabib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Calamai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Basir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="1248" to="1264" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A study of the application of Kohonen-type neural networks to the travelling salesman problem</title>
		<author>
			<persName><forename type="first">F</forename><surname>Favata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="463" to="468" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solving a combinatorial problem via self-organizing process: an application of the Kohonen algorithm to the traveling salesman problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Fort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="33" to="40" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FLEXMAP -a neural network for the traveling salesman problem with linear time and space complexity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fritzke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wilke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks, IJCNN&apos;91</title>
		<meeting>the International Joint Conference on Neural Networks, IJCNN&apos;91</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="929" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Computers and Intractability: A Guide to the Theory of NP-Completeness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>WH Freeman &amp; Co</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Handbook of Metaheuristics</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Glover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">International Series in Operations Research and Management Science</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kochenberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural Networks: A Comprehensive Foundation</title>
		<meeting><address><addrLine>Upper Saddle River, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An effective implementation of the Lin-Kernighan traveling salesman heuristic</title>
		<author>
			<persName><forename type="first">K</forename><surname>Helsgaun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="106" to="130" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An efficient algorithm for traveling salesman problems based on self-organizing feature maps</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second IEEE International Conference on Fuzzy Systems</title>
		<meeting>the Second IEEE International Conference on Fuzzy Systems</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1085" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-Organizing Maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lawler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lenstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H G</forename><surname>Rinnooy Kan</surname></persName>
		</author>
		<title level="m">The Traveling Salesman Problem: A Guided Tour of Combinatorial Optimization</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Shmoys</surname></persName>
		</editor>
		<meeting><address><addrLine>Chichester</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<title level="m">Elements of the Theory of Computation</title>
		<meeting><address><addrLine>Upper Saddle River, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An effective heuristic algorithm for the traveling-salesman problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Kernighan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="498" to="516" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural network methods in combinatorial optimization</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Looi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Operations Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="191" to="208" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Case injected genetic algorithms for traveling salesman problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="201" to="225" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Uma Abordagem Neuro-Imune para a Solução do Problema de Múltiplos Caixeiros Viajantes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A S</forename><surname>Masutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>De Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VIII Brazilian Conference on Neural Networks</title>
		<meeting>the VIII Brazilian Conference on Neural Networks<address><addrLine>Florianópolis, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>CD-ROM</orgName>
		</respStmt>
	</monogr>
	<note>in Portuguese</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-organizing neural networks and various Euclidean traveling salesman problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems and Computers in Japan</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="101" to="112" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An efficient multivalued Hopfield network for the traveling salesman problem</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mérida-Casermeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Galán-Marı ´n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muñoz-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="203" to="216" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Michalewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Fogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">How to Solve It: Modern Heuristics</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Padberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rinaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="60" to="100" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Neuro-immune network for solving the traveling salesman problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>De Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Neural Networks, IJCNN&apos;06</title>
		<meeting>International Joint Conference on Neural Networks, IJCNN&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3760" to="3766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rardin</surname></persName>
		</author>
		<title level="m">Optimization in Operations Research</title>
		<meeting><address><addrLine>Upper Saddle River, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural network approach to solving the traveling salesman problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tchimev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computing Sciences in Colleges</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="41" to="61" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">TSPLIB -a traveling salesman problem library</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ORSA Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="376" to="384" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural techniques for combinatorial optimization with applications</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palaniswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krishnamoorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1301" to="1318" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural networks for combinatorial optimization: a review of more than a decade of research</title>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="15" to="34" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A self-organising model for the travelling salesman problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somhom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Enkawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="919" to="928" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Competition-based neural network for the multiple travelling salesmen problem with minmax objective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somhom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Enkawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Operations Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the dynamics of discrete-time, continuous-state Hopfield neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="747" to="749" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
