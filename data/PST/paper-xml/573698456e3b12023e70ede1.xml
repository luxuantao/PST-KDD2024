<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Committee of Deep CNNs with Exponentially-Weighted Decision Fusion for Static Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo-Kyeong</forename><surname>Kim</surname></persName>
							<email>bokyeong1015@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Hwaran</forename><surname>Lee</surname></persName>
							<email>hwaran.lee@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Jihyeon</forename><surname>Roh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Soo-Young</forename><surname>Lee</surname></persName>
							<email>sylee@kaist.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Advanced Institute of Science and Technology (KAIST) Daejeon</orgName>
								<address>
									<country>Korea, Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Committee of Deep CNNs with Exponentially-Weighted Decision Fusion for Static Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7370687A596B40A8035F2801128C4F86</idno>
					<idno type="DOI">10.1145/2818346.2830590</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hierarchical Committee</term>
					<term>Exponentially-Weighted Decision Fusion</term>
					<term>Deep Convolutional Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a pattern recognition framework to improve committee machines of deep convolutional neural networks (deep CNNs) and its application to static facial expression recognition in the wild (SFEW). In order to generate enough diversity of decisions, we trained multiple deep CNNs by varying network architectures, input normalization, and weight initialization as well as by adopting several learning strategies to use large external databases. Moreover, with these deep models, we formed hierarchical committees using the validation-accuracy-based exponentially-weighted average (VA-Expo-WA) rule. Through extensive experiments, the great strengths of our committee machines were demonstrated in both structural and decisional ways. On the SFEW2.0 dataset released for the 3rd Emotion Recognition in the Wild (EmotiW) sub-challenge, a test accuracy of 57.3% was obtained from the best single deep CNN, while the single-level committees yielded 58.3% and 60.5% with the simple average rule and with the VA-Expo-WA rule, respectively. Our final submission based on the 3-level hierarchy using the VA-Expo-WA achieved 61.6%, significantly higher than the SFEW baseline of 39.1%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Committee machines (also known as classifier ensembles) generally yield a better performance than a single classifier <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>, and thus have extensively applied in various research fields including vision <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref>, speech <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>, text <ref type="bibr" target="#b7">[7]</ref>, and bio-data <ref type="bibr" target="#b8">[8]</ref>. From previous studies on designing a good committee to outperform its individual members, generating diverse decisions from various individuals has been shown to be crucial in providing complementary information about input data <ref type="bibr">[9,</ref><ref type="bibr" target="#b10">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. The overall system for facial expression recognition</head><p>With recent advances in deep learning and parallel computing, forming a committee of multiple deep neural networks was presented in <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>, has attained impressive successes <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref>, and now becomes a widely used approach <ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref>. Particularly, in this paper, we investigate the multi-column deep neural network (MCDNN) <ref type="bibr" target="#b14">[14]</ref> for static facial expression recognition in the wild (SFEW). The standard MCDNN is a committee of deep convolutional neural networks (deep CNNs) with a simple averaging decision rule in a single structure level. Since the MCDNN has already proved its superiority in many visual classifications, we expect that its excellence in recognition could also be demonstrated in SFEW.</p><p>More importantly, we present 2 simple yet effective ways to improve the MCDNN: 'training more diverse individuals' and 'forming a better committee in both decisional and structural aspects'. The former is achieved by designing various network architectures in addition to applying the commonly-used schemes (e.g., different input normalizations and different random weight initialization). Furthermore, we adopt several strategies for using external data in training deep CNNs in order to pursue more diverse decisions and errors. The latter is achieved with a better ensemble rule based on an exponentially-weighted decision fusion. Moreover, we build a hierarchical committee which can make more reliable decisions. As structural levels in the committee become higher, the consensus of multiple sub-groups could be formed and thus enhance the reliability of decisions. The overall proposed system is shown in Figure <ref type="figure">1</ref>.</p><p>Our framework based on the improved committee machines of deep CNNs is tested on the SFEW 2.0 database <ref type="bibr" target="#b19">[19]</ref>, released for a sub-competition in the 3 rd Emotion Recognition in the Wild 2015 (EmotiW2015) challenge. The remainder of this paper describes the proposed approach in detail, experimental results (including our test-label submissions for this SFEW competition), and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED APPROACH 2.1 Deep CNNs as Individual Members</head><p>A deep CNN consists of several feature extraction stages (with alternating convolutional and pooling layers), followed by a recognition stage (with fully-connected layers) <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b20">20]</ref>. Because of its excellent classification ability as well as hierarchical feature development mimicking the human visual system, we selected the deep CNN for the base member of a committee as in a standard MCDNN.</p><p>To build diverse deep CNNs in standard MCDNNs, being trained with 'different training data sets' was mainly focused rather than using 'different classifiers'. The effect of 'different training data' was achieved by several preprocessing methods on the original data such as deformation and normalization. For the effect of 'different classifiers', the MCDNNs applied multiple random seeds for weight initialization, but the identical network architectures were used for all individual members. We believed that various network architectures also largely contributed to obtaining different classifiers and thus to increasing diversity of decisions in forming a committee. Therefore, we applied various architectures for deep CNNs as well as differently preprocessed data and different weight initialization. Furthermore, we explored several training strategies for making use of external data along with the SFEW data in order to pursue more diverse errors. The experimental details about how to build the individual deep CNNs and their recognition accuracies are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Exponentially-Weighted Decision Fusion</head><p>When forming a committee, how to combine decisions from individual members has been extensively investigated. In this paper, we first explored 3 widely-used rules for decision fusion <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>: the majority voting, median rule, and simple average rule. Then, we introduced an effective combination rule based on exponential weighting to give more weights on well-performed individuals.</p><p>The 'majority voting' directly uses the predicted class labels to select a class with the largest number of votes. On the other hands, instead of using the labels, the 'median rule' and 'simple average rule' use the class-related continuous confidences or scores. In our experiments, the median/simple average rule decided a class with the highest median/average of posterior class probabilities yielded from deep CNNs. For these 3 rules, the individuals have equal rights for participation so that any reliability or importance on each of their decision is not considered.</p><p>A straightforward way to regard the importance of members' decisions is to compute a weighted mean of class scores with assigning the weights as validation performances. We denoted it as the 'VA-Simp-WA' rule, short for the validation-accuracybased simple weighted average. However, when the committee members yield the similar accuracies and thus almost equal weights are used, the VA-Simp-WA does not differ from the 'simple average rule'. Our exponentially-weighted decision fusion has been motivated by considering the aforementioned case. In determining the weights, we adopted an exponential function which influences on the differences between numbers (e.g. '3 1 -2 1 =1' &lt; '3 2 -2 2 =5'). We expected that this characteristic of exponent can give more weights on the members with (even slightly) higher accuracies.</p><p>Let us denote our method as the 'VA-Expo-WA' rule, short for the validation-accuracy-based exponentially-weighted average, and continue our discussion with mathematical notations. Suppose a member model m (=1,...,M) with its validation accuracy of zm provides a posteriori class probability vector sm for an input pattern. Then, the final ensemble of M models' decisions in the VA-Expo-WA becomes <ref type="bibr" target="#b1">(1)</ref> where a decision weight dm reflects the normalized significance of the model m's decision (0 dm 1) and an exponent q is a hyperparameter to determine how much the qualified members are emphasized (q&gt;1) or de-emphasized (q 1). Finally, a class with the highest value in exponentially-weighted class probabilities is chosen. Here, the value of q is found by a simple uniform search: scanned over [-50:0.1:150] and selected to provide the maximum performance on validation data after the fusion. The scanning procedure and the corresponding decision weights for the selected q are illustrated in Figure <ref type="figure">2</ref>. We confirmed that this searching method required little additional computation, while it found a proper q which can improve the generalization on validation data. Note that the VA-Expo-WA rules with q=0 and q=1 are identical to the simple average rule and the VA-Simp-WA, respectively. The superior performance of our VA-Expo-WA rule compared to the commonly-used decision fusion rules are shown in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hierarchical Committee</head><p>The existing literatures using hierarchical architectures of committees aimed to divide a hard problem into the easier ones (based on the divide-and-conquer strategy) with a statistical framework <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref> and/or to efficiently combine the outputs of different classifiers trained on heterogeneous features <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b23">23]</ref>. Meanwhile, we constructed hierarchical committees of deep CNNs with the following procedure according to the 2 expected merits.</p><p>1) Organize the M0 individual members into the 1 st level sub-groups, having some overlapped members. After that, make a decision for each group according to the 1 st level decision fusion rule.</p><p>2) Collect all sub-groups' decisions in the l th level, where l (= 1, …, L-1) and ml are indices for the level and the subgroup, respectively. Then, re-organize them into the (l+1) th level groups, and make a decision for each group according to the (l+1) th level decision fusion rule. 3) Repeat '2)' until reaching to get a final decision at the last L th level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. (a)</head><p>The training and validation accuracies (%) as an exponent 'q' is scanned in the exponentially-weighted decision fusion and (b) the corresponding decision weights for the selected q. This result was obtained when 108 models formed a committee with the VA-Expo-WA rule (see <ref type="bibr">Section 5)</ref>.</p><p>The first merit is that, more reliable decisions could come from the strong consensus of multiple sub-groups in higher structural levels. Second, the increased diversity of errors could be obtained by setting some members to be overlapped in certain sub-groups. Then, depending on other members in these groups, the overlapping members differently contribute to the next level decision. Since the former is quite intuitive, let us explain the latter with a toy example. Suppose a 2-class problem and 5 member classifiers of (a, b, c, d, e) who claim the class label for an input sample as (1, 1, 1, 2, 2), respectively. When they are divided into 2 sub-groups, G1: {a, b, c} and G2: {c, d, e}, with an overlapping member 'c' and the majority voting is applied, the 'c' differently contributes to the final decision. More specifically, without grouping, there is no doubt of the selection of class 1 by 3 votes from a, b, and c among 5 members. However, with grouping, both classes get the equal number of mid-level decisions (the class 1 from G1 and the class 2 from G2) due to the different impacts of c's claim on both sub-groups, so the final decision depends on the mean class probabilities. We expected that these groups with overlapping members could lead to more various decisions in the low structural levels, finally serving as diverse errors in the last level. See Section 6 for the experimental details and favorable classification results of our hierarchical committees, which outperform the standard single-level committees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FACE REGISTRATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SFEW 2.0 Database</head><p>The SFEW 2.0 database <ref type="bibr" target="#b19">[19]</ref> was created by extracting frames from emotional movie clips in the AFEW data corpus <ref type="bibr" target="#b24">[24]</ref>. The task was to assign 7 expression labels (angry, disgust, fear, happy, sad, surprise, and neutral) to these frames in close-to-real world conditions. For the training, validation, and test set, the SFEW database contains 958, 436, and 372 images, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Face Registration</head><p>For face registration, we conducted a conventional 2-D alignment based on eye locations as illustrated in Figure <ref type="figure" target="#fig_0">3</ref>(a). To improve robustness in face/landmark detection, multiple detection pipelines were designed to produce different landmark estimations, and the best estimation among them was finally used for the 2-D alignment. We used the Viola-Jones (V-J) model <ref type="bibr" target="#b25">[25]</ref> and the Zhu-Ramanan (Z-R) model <ref type="bibr" target="#b26">[26]</ref> for face detection along with the IntraFace model <ref type="bibr" target="#b27">[27]</ref> for landmark detection. As shown in Figure <ref type="figure" target="#fig_0">3</ref>(b), we considered 4 single pipelines based on the following observations: i) some faces, failed by the V-J, could be detected by the Z-R, and vice versa, ii) depending on face locations from the V-J and Z-R, the landmark estimation of the IntraFace became different, iii) the V-J and IntraFace sometimes yielded complementary outputs when histogram-equalized images were used as input. Among 4 possible landmark sets from those pipelines, the landmark set with the highest confidence provided from the IntraFace was eventually selected for alignment.</p><p>In Table <ref type="table" target="#tab_0">1</ref>, we compared the performances between our multipipeline-based alignment and single-pipeline-based ones. For each data type, we computed the ratio of successful alignments to the whole number of samples as well as the ratio of cases where only face detection succeeded. Our 4-pipeline-based alignment performed better than the single-pipeline ones, implying that complementary detection results were obtained from 4 single pipelines. Therefore, combining them can lead to the robust face registration in real-world conditions. Note that, for training and validation data, the erroneous and failed alignments were semiautomatically processed by hands for a later usage in training deep models. However, for testing data, any human intervention was not applied in the context of a fully-automatic system.  <ref type="table" target="#tab_1">2</ref>. However, in addition to oA, we also used pA in training deep models for the following reasons: for giving deformation effects (such as translation and rotation) to ours and for providing complementary information when either oA or pA failed (as shown in the 3 rd and 4 th rows of the figure). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DESIGNING AND TRAINING INDIVIDUAL COMMITTEE MEMBERS</head><p>In this section, we first described how to design multiple deep CNNs to pursue diverse errors in forming a good committee. Next, for training these models, how to make use of external data along with the SFEW data was presented with other learning details. Finally, classification results of individual models were examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Designing Individual Deep CNNs</head><p>With the aim of getting diversity of decisions and errors from individual members, we designed 216 deep CNNs using different input preprocessing methods, random weight initializations, and network architectures. A single deep CNN is denoted as (2) and the detailed explanations for sub-notations are followed.</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Preprocessing type of data</head><p>We considered various normalization techniques on differently aligned faces. Each raw image was rescaled from 0 to 1 via a minmax normalization. To reduce illumination variation in images, as used in <ref type="bibr" target="#b28">[28]</ref>, the isotropic diffusion based normalization <ref type="bibr" target="#b29">[29]</ref> from INface toolbox [30] was applied with the default parameter setting. Moreover, to enhance contrast for each image, as used in <ref type="bibr" target="#b14">[14]</ref>, we applied the histogram equalization implemented in MATLAB. The examples of normalized images are shown in Figure <ref type="figure" target="#fig_2">4(a)</ref>.</p><p>For different input deformations (e.g., translation and rotation), we used the aligned faces provided from EmotiW2015 (pA) as well as our aligned faces (oA), as previously introduced in Section 3.2. Furthermore, some faces erroneously aligned by our method can be compensated by the provided alignments, thus leading to more diverse and complementary information about input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Architectures of deep CNNs</head><p>As the baseline architecture, we referred to Tang's deep CNN <ref type="bibr" target="#b30">[31]</ref>, the winner model of ICMLW2013's facial expression recognition challenge <ref type="bibr" target="#b31">[32]</ref>. It consisted of the 1 input-transform and 3 convolution+pooling stages, followed by fully-connected hidden and output layers. In the input-transform stage for image mirroring and translating, data were augmented by extracting 42x42 patches from the 48x48 faces. The subsequent layers corresponded to a configuration of {CNNM -FC3072} in our notation (see Table <ref type="table" target="#tab_2">3</ref>), except for average-pooling in the 2 nd and 3 rd stages of Tang's model. To get more specific settings, see his implementation at <ref type="bibr">[33]</ref>.  <ref type="table" target="#tab_2">3</ref>. The CNNM had a medium-size receptive field (with 5x5, 4x4, and 5x5 filters for each conv layer, respectively), the CNNL had a relatively large receptive field (with 7x7 filters for all conv layers), and the CNNS had a relatively small one (with 3x3 filters for all conv layers). For all CNN types, the strides and pads were properly set to ensure the same sizes of output maps (5x5 @64) in the max-pool 3 layer. Moreover, for each CNN type, 4 kinds of fully-connected hidden layer (FC) were used. In this FC layer, the dropout <ref type="bibr" target="#b32">[34]</ref> was applied to reduce over-fitting in training deep models. Notice that, from Tang's model, we modified the pooling layers in the 2 nd and 3 rd stages from average-pooling to max-pooling, since it provided better classification results in our preliminary experiments. For the nonlinearity, Rectified Linear Unit (ReLU) activations were applied for all conv and penultimate layers, and a softmax activation was for the output layer. FC3072: 3072 neurons with a dropout probability = 0.8, FC2048: 2048 neurons with a dropout probability = 0.5, FC1024: 1024 neurons with a dropout probability = 0.5, or FC512: 512 neurons with a dropout probability = 0.5 fc output 7 neurons (one per class) a conv, max-pool, fc: convolutional, max-pooling, fully-connected. b maps: the size of output maps @ the number of output maps. c kernel: the size of kernels, (stride, pad) where 'stride' refers to spacing size of kernels, 'pad without an asterisk*' refers to zero-padding to all 4 spatial directions (top, bottom, left and right directions) of input maps, and 'pad with an asterisk*' refers to zero-padding to the top and left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Individual Deep CNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Usage of external data</head><p>The size of SFEW data is quite small to train deep CNNs. Inspired by <ref type="bibr" target="#b28">[28]</ref>, we also decided to use 2 external databases along with the SFEW data for training models: the Facial Expression Recognition 2013 database (FER-2013 DB) [35] and the Toronto Face Dataset (TFD) <ref type="bibr" target="#b33">[36]</ref>. The FER-2013 DB, released for ICMLW2013's sub-challenge <ref type="bibr" target="#b31">[32]</ref>, was created using the Google image search API. Since realistic facial expressions were collected from the internet, large variations reflecting realworld conditions existed in the FER-2013 DB. From this dataset, 28,698 training faces (after removing 11 non-number-filled images from original training data) and 3,589 private testing faces were used for our experiment. The TFD was constructed by merging together 30 pre-existing face datasets. The faces in TFD were strictly aligned and almost all of them were fully-frontal. From the TFD, 4,178 labelled faces were used. Notice that both datasets contained 48x48 gray-scale faces labelled with the identical 7 expression categories used in the SFEW data.</p><p>After determining the external databases to be used, we explored how to use them together with the SFEW data for training models. The following 3 strategies were considered: In <ref type="bibr" target="#b28">[28]</ref>, the strategies 'i' and 'ii' were discussed to train a deep CNN which yielded per-frame predictions for video-based emotion recognition. In their experiment, the strategy 'i' was finally selected based on a better validation performance. In addition to the 'i' and 'ii', we also investigated one type of transfer learning scheme as denoted in the strategy 'iii' <ref type="bibr" target="#b34">[37]</ref>. To figure out the most proper usage of external data for this SFEW competition, we evaluated the performances of several models trained differently with the aforementioned 3 strategies. Specifically, the following 12 deep CNNs having various architectures were used:</p><p>(3) Figure <ref type="figure" target="#fig_4">5</ref>(a) depicts learning curves during training a deep CNN of {CNNM -FC2048} with 3 strategies. The strategy 'iii' provided superior performances not only at the initial epoch but also at the convergence. Figure <ref type="figure" target="#fig_4">5(b)</ref> shows validation accuracies for all examined training strategies and network architectures. Regardless of architectures, the 'iii' outperformed the other two. It indicates that the transfer learning scheme is effective because of representing more similar and suitable feature distributions between training and validation data. Therefore, we eventually decided to use the strategy 'iii'. We first pre-trained 108 deep CNNs using two external data of FER-2013 DB and TFD. Then, 216 models were fine-tuned using the SFEW data: 108 models fine-tuned using oA + 108 using pA. Note that, for the last two submissions, in addition to these 216 models trained with the 'iii', we also incorporated the 24 models with the 'i' and 'ii' to form a committee for a better handling of various facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Other training details</head><p>We used the MatConvNet toolbox <ref type="bibr" target="#b35">[38]</ref> on NVIDIA GeForce GTX 690 GPUs. Each deep CNN was trained using the stochastic gradient descent with a batch size of 200 and momentum of 0.9. Except for the last fully-connected layer with weight decay of 0.002, weight decay of 0.0001 was applied for all other layers. Moreover, the learning rate was equal for all layers, while its value started from 0.004 and became half at every 25 epoch. During total 100 epochs, we selected a model yielding the max validation performance.</p><p>To avoid over-fitting, the dropout and data augmentation were applied. A dropout probability of 0.8 was used for deep CNNs with FC3072, while 0.5 was used for FC2048, FC1024, and FC512. The training data were augmented by 10 times, through using 5 crops of size 42x42 (1 from resizing an original 48x48 face and 4 from extracting its 4 corners) and their horizontal flopping. At the test phase, to maintain consistency with the training, 10 patches extracted from each face were fed to the model and the corresponding 10 predictions were averaged to produce a final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification Performance of Deep CNNs</head><p>For 216 deep CNNs trained with the strategy 'iii' (a transfer learning scheme), their classification rates to validation data were reported in Table <ref type="table" target="#tab_3">4</ref>. Our best single model with the highest validation accuracy of 52.5% was PREPiNor,oA -{CNNL -FC3072}R1. This model became the 1 st submission for the SFEW competition, yielding a test accuracy of 57.3%.</p><p>We also analyzed general tendencies in performances of deep CNNs. In the aspect of face alignments, the 108 models trained using oA showed a better mean accuracy than those using pA. Furthermore, to examine the trends according to preprocessing types and CNN architectures, we computed the mean accuracy of 36 models for each preprocessing (per-PREP group) and for each CNN (per-CNN group). The illumination normalization was superior to other preprocessing types, and the CNNM with the medium size of receptive field performed better than other CNN architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPONENTIALLY-WEIGHTED DECISION FUSION</head><p>Before moving on examining a hierarchical committee, we demonstrated the superiority of our VA-Expo-WA rule in a conventional single-level committee, by comparing it to the widely-used rules for decision fusion. Here, we formed the 3 committees consisting of 108 models trained using our aligned faces (oA; corresponding to the case of Figure <ref type="figure">2</ref>), 108 models using provided alignments (pA), and 216 using both alignments of oA and pA. As shown in Table <ref type="table">5</ref>, regardless of the committee types, our VA-Expo-WA rule outperformed all other rules. In addition, as we expected, the VA-Simp-WA did not much differ from the simple average rule since individual models produced even and similar validation accuracies as denoted in Table <ref type="table" target="#tab_3">4</ref>.  Meanwhile, it is worth noting that even though the committee of 216 models with the VA-Expo-WA provided the highest classification rate on validation data, its test rate was not the best. The best test accuracy of 60.5% was achieved by the committee of 108 models using oA. It may imply that, at some level yielding a maximum validation performance, adding other models to the committee did not work properly with the VA-Expo-WA. Rather, it seemed to harm the generalization on test data because too many models were participated to improve the final validation accuracy of the committee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5. Validation (&amp; testing, if available) accuracy (%) of single-level committees with various decision fusion rules</head><p>We further investigated the strength of our VA-Expo-WA rule that can reveal the importance or contribution of each individual on a final ensemble. Figure <ref type="figure">2(b)</ref> shows the decision weights of 108 models using oA, and we computed the threshold above which a certain portion of the total weights was covered. The 6, 18, and 42 models were in charge of about 50%, 75% and 95% of the total, respectively. These information could be used for model selection and pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">HIERARCHICAL COMMITTEE</head><p>To construct hierarchical committees, we organized 216 deep CNNs (obtained with the training strategy 'iii') into 12 subgroups for the 1 st level, having some overlapping members: Each sub-group consisted of 36 deep CNNs. The 3 per-PREP groups (G1-G3) and 3 per-CNN groups (G7-G9) were formed from 108 models trained using our aligned faces (oA), and similarly the 3 per-PREP (G4-G6) and 3 per-CNN (G10-G12) groups were from 108 models using provided alignments (pA).</p><p>There were several ways to build the hierarchy in 'structural' aspects (regarding the number of hierarchical levels and the structure for re-combination of higher-level decisions) and in 'decisional' aspects (regarding the decision fusion rule for each level). At an earlier phase of experiments, we submitted the predicted test labels obtained from adopting the VA-Expo-WA rule for all structural levels in the hierarchy. From these submissions, we got some unexpected results; validation accuracies were higher than our previous submissions, but testing accuracies dropped. However, we learned an empirical lesson; applying the VA-Expo-WA rule for all levels of a hierarchy may have a negative impact on generalization for test data, since the exponent selection was optimized for the validation performance. Therefore, we decided to use the VA-Expo-WA rule only for the 1 st level. For decision fusions in higher levels, the majority voting and simple average rules were used for a better generalization.</p><p>We first considered a simple 2-level hierarchical structure as illustrated in Figure <ref type="figure" target="#fig_6">6(a)</ref>. With a fixed VA-Expo-WA rule of the 1 st level, we varied decision fusion rules of the 2 nd level as the majority voting or simple average rules. Moreover, as mentioned in Section 4.2.1, for handling more various face expressions and pursuing more diverse errors, we additionally examined 24 models from the training strategies 'i' and 'ii'. These models were also formed into the 1 st level groups, having the 12 models each:  By adding these 2 groups' decisions to the 1 st level, we built another 2-level hierarchy as shown in Figure <ref type="figure" target="#fig_6">6</ref>(b). For clarity in the subsequent discussion, we shall to name each hierarchy as each index with a parenthesis in Figure <ref type="figure" target="#fig_6">6</ref>. The top part of Table <ref type="table" target="#tab_5">6</ref> denotes classification performances of the 2-level hierarchies. For both (a) and (b), the majority voting in the 2 nd level performed better than the simple average rule. However, the hierarchy (b) with a great validation accuracy of 56.2% did not yield a better test accuracy compared to previous submissions. We suggested the following reason; the added 2 groups, g1-g2, to the 1 st level were expected to produce more diverse decisions based on different training strategies, but their impacts on the 2 nd level were quite small due to competing with 12 decisions from G1-G12.</p><p>Hence, we decided to reduce the influence of decisions from G1-G12 on the final prediction by forming the 3-level hierarchical committee as shown in Figure <ref type="figure" target="#fig_6">6</ref>(c) and 6(d). Note that these 3 levels on the side of G1-G12 not only reduced the number of decisions (from 12 in the hierarchy (b) to 4 in the (c) or to 2 in the (d)) but also made compact and reliable decisions passing through multiple levels. Here, 2 types of decision fusions in the 2 nd and 3 rd levels were considered, as demonstrated in the bottom part of Table <ref type="table" target="#tab_5">6</ref>. Applying majority voting for both 2 nd and 3 rd levels showed better validation accuracies than using the simple average rule for the 2 nd level with the majority voting for the 3 rd level. More importantly, in the test performances, these 2 types of 3level hierarchies did not differ from each other but were superior to the 2-level hierarchies. It indicates that, as forming a hierarchical committee with higher levels, the structural consideration is more important than the decision fusion method to give enough diversity in decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper, we present a framework based on committee machines of deep CNNs. To generate diverse errors for a better committee, we first constructed multiple deep CNNs as individual committee members. Here, deep models were trained by applying various network architectures, several strategies to use external data, and different input preprocessing and random initialization. With these individuals, we formed hierarchical committees which adopted the valid-accuracy-based exponentially-weighted average. This exponentially-weighted decision fusion was superior to other commonly-used ensemble methods by increasing a generalization capability. Furthermore, the hierarchical structure indeed made more reliable decisions with the consensus of various sub-groups.</p><p>Our proposed approach was demonstrated on the SFEW competition data released for the EmotiW 2015 sub-challenge. To sum up our submissions, the test accuracy of the best single deep CNN was 57.3%, while the single-level committees of 108 models trained using our aligned faces yielded 58.3% with the simple average rule and 60.5% with the exponentially-weighted decision fusion. Furthermore, the last two submissions based on 3-level hierarchical committees of total 240 deep CNNs achieved 61.6%, greatly outperforming the SFEW baseline of 39.1%. We believe that the superiority of our committee machines could further be drawn in other pattern recognition problems as well as SFEW.</p><p>In our future works, we will design various and good objective functions in training individual deep CNNs in order to get more diverse decisions. Moreover, how to determine the structure of hierarchical committees will be intensively studied in both academic and engineering manners.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) Face registration based on a 2-D alignment, (b) our multi-pipeline-based alignment, and (c) examples of aligned faces for test data. In (b), the terms 'IMG', 'HistEq', 'LOC', and 'Conf' are short for 'image', 'histogram equalized', 'location', and 'confidence', respectively. In (c), faces in the corresponding positions between oA and pA are processed from the same test image.</figDesc><graphic coords="3,63.70,429.50,220.70,224.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 (</head><label>3</label><figDesc>Figure 3(c) depicts examples for test faces processed by our alignment method and provided from EmotiW2015. As shown in the 1 st and 2 nd rows of the figure, the faces processed from our method (oA) were more similarly aligned each other compared to the provided alignments (pA). It could lead to superior accuracies of oA as denoted in Table2. However, in addition to oA, we also used pA in training deep models for the following reasons: for giving deformation effects (such as translation and rotation) to ours and for providing complementary information when either oA or pA failed (as shown in the 3 rd and 4 th rows of the figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) Various normalizations on the aligned validation faces and (b) a deep CNN architecture Based on Tang's architecture, we designed diverse deep CNNs by changing the sizes of filters for various receptive fields and by changing the number of neurons in a fully-connected hidden layer as denoted in Table3. The CNNM had a medium-size receptive field (with 5x5, 4x4, and 5x5 filters for each conv layer, respectively), the CNNL had a relatively large receptive field (with 7x7 filters for all conv layers), and the CNNS had a relatively small one (with 3x3 filters for all conv layers). For all CNN types, the strides and pads were properly set to ensure the same sizes of output maps (5x5 @64) in the max-pool 3 layer. Moreover, for each CNN type, 4 kinds of fully-connected hidden layer (FC) were used. In this FC layer, the dropout<ref type="bibr" target="#b32">[34]</ref> was applied to reduce over-fitting in training deep models. Notice that, from Tang's model, we modified the pooling layers in the 2 nd and 3 rd stages from average-pooling to max-pooling, since it provided better classification results in our preliminary experiments. For the nonlinearity, Rectified Linear Unit (ReLU) activations were applied for all conv and penultimate layers, and a softmax activation was for the output layer.</figDesc><graphic coords="4,318.10,54.00,239.65,129.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>i. Random initialization ⇒ In learning, using data as follows: {FER-2013 DB + TFD} for 'training' {SFEW Train + SFEW Valid} for 'validation' ii. Random initialization ⇒ In learning, using data as follows: {FER-2013 DB + TFD + SFEW Train} for 'training' {SFEW Valid} for 'validation' iii. Initialization from a pre-trained model constructed by using {FER-2013 DB Train + TFD} for 'training' {FER-2013 DB Test} for 'validation' ⇒ In learning, using data as follows: {SFEW Train} for 'training' {SFEW Valid} for 'validation'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of 3 strategies for using external data along with SFEW data in training: (a) learning curves from a deep CNN and (b) validation accuracies for 12 deep CNNs</figDesc><graphic coords="5,59.90,498.10,228.35,185.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>7) b 56.4 (60.0) c a, b, c For single-level committees with the VA-Expo-WA rule, the values of exponent q were selected as 43.8, 58.1, and 60.5, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Diagrams of 2-and 3-level hierarchical committees</figDesc><graphic coords="7,54.25,484.70,239.65,220.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Alignment-success rate (%) of our alignments Alignment Method Success Rate (%)</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Alignment (Both Face &amp;</cell><cell></cell><cell>Only</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Landmark Detection)</cell><cell cols="3">Face Detection</cell></row><row><cell></cell><cell></cell><cell>Train</cell><cell>Valid</cell><cell>Test</cell><cell cols="2">Train Valid</cell><cell>Test</cell></row><row><cell>Single-Pipeline-Based</cell><cell>① ② ③ ④</cell><cell>70.5 71.6 58.6 56.6</cell><cell>70.4 74.1 60.8 56.4</cell><cell>73.1 77.2 57.5 53.5</cell><cell>1.4 2.6 25.5 27.5</cell><cell>1.1 1.8 29.8 34.2</cell><cell>2.4 3.5 30.9 34.9</cell></row><row><cell cols="2">Multi-Pipeline-Based</cell><cell>81.8</cell><cell>83.5</cell><cell>90.1</cell><cell>9.5</cell><cell>6.0</cell><cell>4.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Validation (&amp; testing, if available) accuracy (%) of our alignments (oA) and provided alignments (pA) Classification Method oA pA</head><label>2</label><figDesc></figDesc><table /><note><p>{LPQ-pHOG} + rbfSVM: baseline [19] -36.0 (39.1) A single deep CNN a : PREPiNor -{CNNL -FC3072}R1 52.5 (57.3) 46.8 A committee of 108 deep CNNs b with the VA-Expo-WA rule 54.6 (60.5) 52.2 (56.7) a, b For the detailed information, see Section 4.3 and 5, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . Configuration of deep CNNs Layer a CNNS CNNM CNNL</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">maps b kernel c</cell><cell>maps</cell><cell>kernel</cell><cell>maps</cell><cell>kernel</cell></row><row><cell>input</cell><cell>42x42 @1</cell><cell>-</cell><cell>42x42 @1</cell><cell>-</cell><cell>42x42 @1</cell><cell>-</cell></row><row><cell>conv 1</cell><cell>42x42 @32</cell><cell>3x3, (1,1)</cell><cell>42x42 @32</cell><cell>5x5, (1, 2)</cell><cell>42x42 @32</cell><cell>7x7, (1, 3)</cell></row><row><cell>max</cell><cell>21x21</cell><cell>2x2,</cell><cell>21x21</cell><cell>3x3,</cell><cell>21x21</cell><cell>2x2,</cell></row><row><cell>-pool 1</cell><cell>@32</cell><cell>(2, 0)</cell><cell>@32</cell><cell>(2, 1*)</cell><cell>@32</cell><cell>(2, 0)</cell></row><row><cell>conv 2</cell><cell>19x19 @32</cell><cell>3x3, (1, 0)</cell><cell>20x20 @32</cell><cell>4x4, (1, 1)</cell><cell>19x19 @32</cell><cell>7x7, (1, 2)</cell></row><row><cell>max</cell><cell>10x10</cell><cell>2x2,</cell><cell>10x10</cell><cell>3x3,</cell><cell>10x10</cell><cell>2x2,</cell></row><row><cell>-pool 2</cell><cell>@32</cell><cell>(2, 1*)</cell><cell>@32</cell><cell>(2, 1*)</cell><cell>@32</cell><cell>(2, 1*)</cell></row><row><cell>conv 3</cell><cell>10x10 @64</cell><cell>3x3, (1, 1)</cell><cell>10x10 @64</cell><cell>5x5, (1, 2)</cell><cell>10x10 @64</cell><cell>7x7, (1, 3)</cell></row><row><cell>max</cell><cell>5x5</cell><cell>2x2,</cell><cell>5x5</cell><cell>3x3,</cell><cell>5x5</cell><cell>2x2,</cell></row><row><cell>-pool 3</cell><cell>@64</cell><cell>(2, 0)</cell><cell>@64</cell><cell>(2, 1*)</cell><cell>@64</cell><cell>(2, 0)</cell></row><row><cell>fc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hidden</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 . Validation accuracy (%) of individual deep CNNs. 108</head><label>4</label><figDesc>models (top) trained using our aligned faces and 108 (bottom) using provided alignments from EmotiW2015. The highest accuracy for a given architecture (each column) is written in bold. The asterisk* denotes the single best model.</figDesc><table><row><cell cols="2">Aligned Faces</cell><cell></cell><cell cols="2">CNNS</cell><cell></cell><cell></cell><cell cols="2">CNNM</cell><cell></cell><cell></cell><cell cols="2">CNNL</cell><cell></cell><cell>Mean (Std) of</cell></row><row><cell cols="2">From Our Method (oA)</cell><cell>FC 3072</cell><cell>FC 2048</cell><cell>FC 1024</cell><cell>FC 512</cell><cell>FC 3072</cell><cell>FC 2048</cell><cell>FC 1024</cell><cell>FC 512</cell><cell>FC 3072</cell><cell>FC 2048</cell><cell>FC 1024</cell><cell>FC 512</cell><cell>per-PREP group: 36 models / group</cell></row><row><cell>PREP raw, oA</cell><cell>R1 R2 R3</cell><cell>45.6 46.3 45.0</cell><cell>45.0 45.0 43.6</cell><cell>45.4 46.8 47.0</cell><cell>47.7 45.0 45.6</cell><cell>49.8 47.5 49.1</cell><cell>48.4 48.4 47.0</cell><cell>45.4 49.1 47.3</cell><cell>47.7 49.1 50.0</cell><cell>50.0 47.5 48.4</cell><cell>44.7 46.1 45.9</cell><cell>45.9 45.0 47.3</cell><cell>47.9 46.6 46.1</cell><cell>46.9 (1.7)</cell></row><row><cell>PREP iNor, oA</cell><cell>R1 R2 R3</cell><cell>48.6 47.9 48.9</cell><cell>49.5 46.3 46.3</cell><cell>48.6 48.4 46.6</cell><cell>49.1 50.7 49.5</cell><cell>49.1 52.1 48.6</cell><cell>46.3 49.5 48.9</cell><cell>50.9 49.8 47.9</cell><cell>50.2 50.5 50.7</cell><cell>52.5* 50.2 47.5</cell><cell>49.5 47.9 47.0</cell><cell>50.5 48.6 50.5</cell><cell>52.3 48.2 50.5</cell><cell>49.2 (1.6)</cell></row><row><cell>PREP cEnh, oA</cell><cell>R1 R2 R3</cell><cell>45.9 45.2 43.4</cell><cell>44.5 44.7 43.4</cell><cell>45.0 43.8 46.3</cell><cell>45.9 44.0 44.7</cell><cell>47.5 49.8 48.2</cell><cell>45.2 47.5 44.7</cell><cell>48.2 48.2 45.9</cell><cell>49.1 49.8 45.2</cell><cell>42.9 45.2 45.6</cell><cell>41.3 42.9 42.2</cell><cell>43.4 39.7 43.8</cell><cell>43.1 41.3 45.2</cell><cell>45.1 (2.4)</cell></row><row><cell cols="2">Mean (Std) of per-CNN group: 36 models / group</cell><cell></cell><cell cols="2">46.3 (1.9)</cell><cell></cell><cell></cell><cell cols="2">48.4 (1.7)</cell><cell></cell><cell></cell><cell cols="2">46.5 (3.2)</cell><cell></cell><cell>Total 108 models 47.0 (2.5)</cell></row><row><cell cols="2">Aligned Faces</cell><cell></cell><cell cols="2">CNNS</cell><cell></cell><cell></cell><cell cols="2">CNNM</cell><cell></cell><cell></cell><cell cols="2">CNNL</cell><cell></cell><cell>Mean (Std) of</cell></row><row><cell cols="2">Provided From EmotiW2015 (pA)</cell><cell>FC 3072</cell><cell>FC 2048</cell><cell>FC 1024</cell><cell>FC 512</cell><cell>FC 3072</cell><cell>FC 2048</cell><cell>FC 1024</cell><cell>FC 512</cell><cell>FC 3072</cell><cell>FC 2048</cell><cell>FC 1024</cell><cell>FC 512</cell><cell>per-PREP group: 36 models / group</cell></row><row><cell>PREP raw, pA</cell><cell>R1 R2 R3</cell><cell>43.8 43.1 43.8</cell><cell>43.6 41.9 39.6</cell><cell>40.8 42.9 42.9</cell><cell>45.7 40.5 41.7</cell><cell>48.0 45.9 47.1</cell><cell>45.2 45.4 45.7</cell><cell>46.1 46.8 46.1</cell><cell>45.4 46.4 44.5</cell><cell>44.5 45.4 45.0</cell><cell>44.3 43.8 44.3</cell><cell>45.7 42.9 45.0</cell><cell>46.8 45.4 47.3</cell><cell>44.5 (2.0)</cell></row><row><cell>PREP iNor, pA</cell><cell>R1 R2 R3</cell><cell>46.4 47.5 46.6</cell><cell>43.8 44.3 45.2</cell><cell>45.4 43.6 43.6</cell><cell>42.6 46.4 46.1</cell><cell>47.5 46.8 46.8</cell><cell>44.7 46.6 45.7</cell><cell>48.7 48.7 45.4</cell><cell>45.4 45.4 52.2</cell><cell>46.8 47.3 47.8</cell><cell>44.0 45.9 45.7</cell><cell>47.1 49.0 43.3</cell><cell>45.0 47.8 46.1</cell><cell>46.1 (1.9)</cell></row><row><cell>PREP cEnh, pA</cell><cell>R1 R2 R3</cell><cell>44.0 43.8 43.3</cell><cell>39.1 41.9 40.8</cell><cell>42.2 44.3 42.4</cell><cell>41.2 43.6 40.5</cell><cell>46.4 47.5 46.4</cell><cell>46.1 45.4 41.0</cell><cell>46.6 44.7 44.0</cell><cell>47.8 47.8 46.8</cell><cell>42.2 41.7 40.3</cell><cell>42.4 42.2 39.3</cell><cell>42.2 43.6 41.9</cell><cell>43.8 43.1 44.3</cell><cell>43.5 (2.4)</cell></row><row><cell cols="2">Mean (Std) of per-CNN group: 36 models / group</cell><cell></cell><cell cols="2">43.3 (2.0)</cell><cell></cell><cell></cell><cell cols="2">46.3 (1.8)</cell><cell></cell><cell></cell><cell cols="2">44.5 (2.2)</cell><cell></cell><cell>Total 108 models 44.7 (2.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 . Validation (&amp; testing, if available) accuracy (%) of hierarchical committees with using the VA-Expo-WA rule as the 1 st level decision fusion 2-Level Hierarchical Committee Decision Fusion in the 2 nd Level</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell>Simple Ave. Rule</cell><cell>Majority Voting</cell></row><row><cell>(a)</cell><cell>53.4</cell><cell>56.2</cell></row><row><cell>(b)</cell><cell>53.9</cell><cell>56.2 (60.2)</cell></row><row><cell>3-</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Level Hierarchical Committee Decision Fusion in the 2 nd &amp; 3 rd Levels</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Simple Ave. Rule &amp;</cell><cell>Majority Voting &amp;</cell></row><row><cell></cell><cell>Majority Voting</cell><cell>Majority Voting</cell></row><row><cell>(c)</cell><cell>53.9 (61.6)</cell><cell>56.2</cell></row><row><cell>(d)</cell><cell>52.5</cell><cell>52.8 (61.6)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENT</head><p>The authors thank Suh-Yeon Dong for her valuable discussions and comments on this work (suhyeon.dong@gmail.com). This work was supported by the Industrial Strategic Technology Development Program (10044009), funded by the Ministry of Knowledge Economy (MKE, Korea).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ensemble based systems in decision making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="45" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical ensemble of global and local classifiers for face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1885" to="1896" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Hopfield Neural Network for combining classifiers applied to textured images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pajares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="144" to="153" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On combining classifiers for speaker authentication</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rodrı Ǵuez-Liñares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="347" to="359" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emotion recognition of affective speech based on multiple classifiers using acousticprosodic information and semantic labels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On combining classifier mass functions for text</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1307" to="1319" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Microarray-based classification and clinical predictors: on combined classifiers and additional predictive value</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Boulesteix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1698" to="1706" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relationships between combination methods and measures of diversity in combining classifiers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Shipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Fusion</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="148" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using diversity of errors for selecting members of a committee classifier</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aksela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="608" to="623" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3207" to="3220" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural network committees for handwritten character classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1135" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-column deep neural network for traffic sign classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="333" to="338" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive multi-column deep neural networks with application to robust image denoising</title>
		<author>
			<persName><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1493" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep dynamic neural networks for gesture segmentation and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014 Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="552" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the EM algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mixture of experts classification using a hierarchical mixture model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2221" to="2244" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical ensemble of multi-level classifiers for diagnosis of alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLMI 2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7588</biblScope>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI 2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An image preprocessing algorithm for illumination invariant face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Brajovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVBPA 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Learning with Linear Support Vector Machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2013 Workshop on Representational Learning</title>
		<meeting><address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="59" to="63" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Toronto face database</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<idno>TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Toronto, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<idno>CoRR., abs/1412.4564</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
