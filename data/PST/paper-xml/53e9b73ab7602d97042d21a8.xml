<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An efficient and robust line segment matching approach based on LBD descriptor and pairwise geometric consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lilian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">University of Kiel</orgName>
								<address>
									<postCode>24098</postCode>
									<settlement>Kiel</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reinhard</forename><surname>Koch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">University of Kiel</orgName>
								<address>
									<postCode>24098</postCode>
									<settlement>Kiel</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An efficient and robust line segment matching approach based on LBD descriptor and pairwise geometric consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">20EB8CEEA24E87F0F359EA6E6D4C6046</idno>
					<idno type="DOI">10.1016/j.jvcir.2013.05.006</idno>
					<note type="submission">Preprint submitted to Journal of Visual Communication and Image RepresentationMay 20, 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Line segment matching</term>
					<term>Multi-scale line detection</term>
					<term>Line band descriptor</term>
					<term>Pairwise geometric consistency</term>
					<term>Relational graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a line matching algorithm which utilizes both the local appearance of lines and their geometric attributes. To overcome the problem of segment fragmentation and geometric variation, we extract lines in the scale space. To depict the local appearance of lines, we design a novel line descriptor called Line Band Descriptor (LBD). To evaluate the pairwise geometric consistency, we define the pairwise geometric attributes between line pairs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Then we built a relational graph for candidate line matches and employ a spectral technique to solve this matching problem efficiently. The advantages of the proposed algorithm are as follows: (1) it's robust to image transformations because of the multi-scale line detection strategy; (2) it's efficient because the designed LBD descriptor is fast to compute and the appearance similarities reduce the dimension of the graph matching problem; <ref type="bibr">(3)</ref> it's accurate even for low-texture images because of the pairwise geometric consistency evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the challenging areas in computer vision is feature matching, which is a basic tool for applications in scene reconstruction <ref type="bibr">[1]</ref>, pattern recognition and retrieval <ref type="bibr">[2]</ref>, stereo SLAM <ref type="bibr">[3]</ref> and so on. Most of the existing matching methods in the literature are based on local points or region features <ref type="bibr" target="#b3">[4]</ref> which are deficient for low-texture scenes <ref type="bibr" target="#b4">[5]</ref>. On the contrary, line features are often abundant in these situations. Moreover, line features and other local features provide complementary information about scenes. Therefore, line segment matching is both desirable and indispensable in many applications.</p><p>Although some progress was achieved recently for the line matching problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, these approaches are quite computationally expensive, prohibiting their usage in many applications. This paper addresses the problem of robust and efficient line feature matching based on the results of our previous work <ref type="bibr" target="#b6">[7]</ref> with following extensions: (i), the Line Band Descriptor (LBD) is presented in detail; (ii), the descriptor performance is experimentally evaluated; (iii), the robustness of the histogram based rotation estimation method is discussed; (iv), in this paper, we also conduct an experiment to reveal the influence of the multi-scale line detection strategy and the geometric constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Existing approaches to match lines are of three types: those that match individual line segments, those that match groups of line segments and those that perform line matching by employing point correspondences.</p><p>For matching lines in image sequences or small baseline stereos where extracted corresponding segments are similar, approaches based on matching individual lines are suitable <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> because of their better computational performance. Among the first group, Wang et al. <ref type="bibr" target="#b10">[11]</ref> propose a descriptor named Mean-Standard deviation Line Descriptor (MSLD) for line matching based on the appearance of the pixel support region. This approach achieves good matching results for moderate image variations in textured scenes.</p><p>Generally, approaches which match groups of line segments have the advantage that more geometric information is available for disambiguation. A large number of methods have been developed around the idea of graphmatching <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> while most of them are for small baseline stereo image pairs. Bay et al. <ref type="bibr" target="#b15">[16]</ref> present a wide baseline stereo line matching method which compares the histograms of neighboring color profiles and iteratively eliminates mismatches by a topological filter. The results shown in their work are for structured scenes with small number of lines, thus the performance on images featuring a larger range of conditions is not clear. Wang et al. <ref type="bibr" target="#b4">[5]</ref> use line signatures to match lines between wide baseline images.</p><p>To overcome the unreliable line detection problem, a multi-scale line extraction strategy which extracts lines by verifying multiple merging thresholds from the edge image is employed. A line signature is constructed for each extracted line. This approach significantly improves the repeatability of line signatures and therefore has a good matching performance. However, this method is quite computationally expensive because of the huge number of line signatures.</p><p>Given a set of point correspondences, Schmid and Zisserman <ref type="bibr" target="#b16">[17]</ref> take the epipolar constraint of line endpoints for short baseline matching and present a plane sweep algorithm for wide baseline matching. Lourakis et al. <ref type="bibr" target="#b17">[18]</ref> use two lines and two points to construct a projective invariant for matching planar surfaces. Kim and Lee <ref type="bibr" target="#b18">[19]</ref> present a line matching method by using coplanar Line Intersection Context Features (LICF). More recently, Fan et al. <ref type="bibr" target="#b5">[6]</ref> explore an affine invariant from two points and one line. They utilize this affine invariant to match lines with known point correspondences. The main drawback of these approaches is the requirement of known epipolar geometry or point correspondences. Besides, their performance in low texture scenes is limited because of the lack of good point correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Our approach</head><p>Several reasons make line matching a difficult problem <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>, including: inaccurate locations of line endpoints, fragmentation of lines, lack of strongly disambiguating geometric constraints (e.g. Epipolar Constraints), lack of distinctive appearance in low-texture scenes, instabilities for large image transformations. To deal with these challenges, the approach in this paper is built on three strategies.</p><p>The first is to extract lines in the scale space making the matching algorithm robust to the scale changes. Though there is some work on detecting and tracking scale invariant lines <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, the proposed multi-scale line extraction approach simply apply the EDLine <ref type="bibr" target="#b21">[22]</ref> detector to a scale-space pyramid consisting a set of octave images, because it is more efficient to detect features in the scale space <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> than to directly extract scale invariant regions <ref type="bibr" target="#b26">[27]</ref>.</p><p>The second strategy is to characterize the local appearance of line segments by the Line Band Descriptor (LBD) which is more efficient to compute than MSLD <ref type="bibr" target="#b10">[11]</ref>. Different from the edge descriptors proposed in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, the proposed line descriptors are not designed to overcome the large scale changes because it is inefficient to adjust the scale of support region for each line segment. Instead, the multi-scale line extraction approach is adopted to solve this problem more efficiently.</p><p>The third novel part is to combine the local appearance of lines and the geometric constraints between line pairs to build a relational graph. The dimension of the graph matching problem is reduced by checking the appearance similarities and geometric consistencies in a loose way. A spectral method <ref type="bibr" target="#b29">[30]</ref> is employed to solve the matching problem which avoids the combinatorial explosion inherent to the graph matching problem. The geometric relationship of corresponding line pairs in two images may be not exactly affine invariant because they are often not coplanar. However, for images without strong view point changes, most of the correctly corresponding line pairs tend to establish strong agreement links among each other while the incorrect assignments have weak links in the graph and few of them have strong links by accident. This property makes the spectral technique a promising strategy to efficiently solve the matching problem.</p><p>Compared to state-of-the-art methods, experiments validate that the proposed line matching approach is faster to generate the matching results. It's also robust against various image transformations including occlusion, rotation, blurring, illumination changes, scale changes, and moderate view point changes even for non-planar scenes or low-texture scenes.</p><p>The rest of this paper is organized as follows. Sec.2 presents the way to extract lines in the scale space and construct the line descriptors. Sec.3 introduces the processes to generate candidate matching pairs, to build the relational graph and to solve the graph matching problem via spectral technique. The descriptor performance evaluation is presented in Sec.4 and the experimental matching results are reported in Sec.5. Finally, we draw the conclusion in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Line detection and description</head><p>In this section, we first present the approach to detect lines in the scale space. Then the way to construct the line descriptor is introduced. The main reason for proposing this new line descriptor is to depict the local appearance of line more efficiently than MSLD <ref type="bibr" target="#b10">[11]</ref> without losing the matching performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Detecting lines in the scale space</head><p>To overcome the fragmentation problem of line detection and to improve the performance for large scale changes, in our line detection framework we employ a scale-space pyramid consisting of N octave images which are generated by down-sampling the original image with a set of scale factors and Gaussian blurring. There is no intra-layer between two consecutive octaves.</p><p>We first apply the EDLine <ref type="bibr" target="#b21">[22]</ref> algorithm to each octave producing a set of lines in the scale space. Each line has a direction which is given by making the gradients of most edge pixels pointing from its left side to its right side.</p><p>Then we re-organize them by finding corresponding lines in the scale space.</p><p>For all lines extracted in the scale space, they are assigned a unique ID and stored into a vector called LineVec if they are related to the same event in the image (i.e. the same region of the image with the same direction). The final extracted results are a set of LineVecs as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. The line detection approach is different from Wang's <ref type="bibr" target="#b4">[5]</ref> by re-organizing all the line segments extracted in the scale space to form LineVecs which reduces the dimension of the graph matching problem.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, each LineVec may include more than one line in the scale space. To depict the local appearance of a LineVec, for each line in it, we will generate a line descriptor from the octave image where the line is extracted. The representation of the line support region and the construction of the line descriptor are introduced in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The band representation of the line support region</head><p>Given a line segment in the octave image, the descriptor will be computed from the line support region (LSR) which is a local rectangular region centered at the line as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. This support region is divided into a set of bands {B 1 , B 2 , . . . , B m } where each band is a sub-region of the LSR and parallel with the line. The numbers of bands m and the width of each band w will be discussed in Sec.4.1. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates an example of the LSR when m = 5, w = 3. The length of the band naturally equals to the length of the segment. Similar to MSLD <ref type="bibr" target="#b10">[11]</ref>, two directions which form a local 2D coordinate frame are introduced to distinguish parallel lines with opposite gradient directions and to make the descriptor rotation invariant. According to the line direction d L , the orthogonal direction d ⊥ is defined as the clockwise orthogonal direction of d L . The middle point of the line is chosen as the origin of this local coordinate frame. The gradient of each pixel in the LSR is projected into this local frame g = (g</p><formula xml:id="formula_0">T • d ⊥ , g T • d L ) T (g d ⊥ , g d L )</formula><p>T in which g and g are the pixel gradients in the image frame and the local frame respectively. Motivated by SIFT <ref type="bibr" target="#b22">[23]</ref> and MSLD <ref type="bibr" target="#b10">[11]</ref>, two Gaussian functions are applied to each row of the LSR along the d ⊥ direction. First, a global weighting</p><formula xml:id="formula_1">coefficient f g (i) = (1/ √ 2πσ g )e -d 2 i /2σ 2</formula><p>g is assigned to the i th row in the LSR, in which d i is the distance of the i th row to the center row of LSR and σ g = 0.5(m • w -1). Second, considering a band B j , for rows in the band B j and in its nearest neighbor bands B j-1 , B j+1 , a local weighting coefficient</p><formula xml:id="formula_2">f l (k) = (1/ √ 2πσ l )e -d 2 k /2σ 2</formula><p>l is assigned to the k th row, in which d k is the the Gaussian weights are applied to each row rather than each pixel directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The construction of the line band descriptor</head><p>For a band B j in the LSR, the band descriptor BD j is computed from rows of B j and its nearest two neighbor bands B j-1 , B j+1 . Specially, for the top and bottom bands B 1 and B m , rows which are outside of the LSR won't be considered when computing the band descriptor of B 1 and B m . After computing {BD j }, the line band descriptor LBD is simply generated by concatenating them:</p><formula xml:id="formula_3">LBD = (BD T 1 , BD T 2 , . . . , BD T m ) T . (<label>1</label></formula><formula xml:id="formula_4">)</formula><p>Now, we construct the band descriptor BD j . For the k th row in the band B j or its neighbors, we accumulate the gradients of pixels within this row as:</p><formula xml:id="formula_5">v1 k j = λ g d ⊥ &gt;0 g d ⊥ , v2 k j = λ g d ⊥ &lt;0 -g d ⊥ , v3 k j = λ g d L &gt;0 g d L , v4 k j = λ g d L &lt;0</formula><p>-g d L .</p><p>(</p><formula xml:id="formula_6">)<label>2</label></formula><p>where the Gaussian coefficient</p><formula xml:id="formula_7">λ = f g (k)f l (k).</formula><p>By stacking these four accumulated gradients of all rows associated with the band B j , the band description matrix (BDM) is constructed as:</p><formula xml:id="formula_8">BDM j = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ v1 1 j v1 2 j • • • v1 n j v2 1 j v2 2 j • • • v2 n j v3 1 j v3 2 j • • • v3 n j v4 1 j v4 2 j • • • v4 n j ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ ∈ 4×n , (<label>3</label></formula><formula xml:id="formula_9">)</formula><p>where n is the number of rows associated with B j :</p><formula xml:id="formula_10">n = ⎧ ⎨ ⎩ 2w, j = 1 m; 3w, else.</formula><p>Now BD j is simply constructed using the mean vector M j and the standard deviation vector S j of the matrix BDM j : BD j = (M T j , S T j ) T ∈ 8 . Substituting in Eq.1, yields:</p><formula xml:id="formula_11">LBD = (M T 1 , S T 1 , M T 2 , S T 2 , • • • , M T m , S T m ) T ∈ 8m . (<label>4</label></formula><formula xml:id="formula_12">)</formula><p>Similar to <ref type="bibr" target="#b10">[11]</ref>, the mean part</p><formula xml:id="formula_13">(M T 1 , M T 2 , • • • , M T m ) and the standard deviation part (S T 1 , S T 2 , • • • , S T m )</formula><p>of LBD are normalized separately because of their different magnitudes. Furthermore, to reduce the influence of non-linear illumination changes, the value of each dimension of LBD is restrained such that it's no larger than a threshold (0.4 is empirically found to be a good value). Finally, we re-normalize the restrained vector to get a unit LBD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph matching using spectral technique</head><p>After introducing the line detection and description, in this section we present the method to construct the relational graph between two groups of LineVecs and to establish the matching results from this graph. Before that, some pre-processes are introduced first to reduce the dimension of the graph matching problem by excluding the clear non-matches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generating the candidate matching pairs</head><p>LineVecs detected in the reference and query images are deemed to be non-matches if they fail to pass the following tests according to their unary geometric attributes and their local appearance similarities.</p><p>• Unary Geometric Attribute: The unary geometric attribute considered in our work is the direction of LineVecs. Note that lines in the same LineVec have the same direction, so each LineVec has a unique direction. At first glance, the directions of corresponding LineVecs in the image pair are ambiguous and unreliable as image pairs can have arbitrary rotation changes.</p><p>Though this is exactly true, there is often an approximate global rotation angle between image pairs. We could employ this attribute whenever it is available to reduce the number of candidate matches.</p><p>In <ref type="bibr" target="#b5">[6]</ref>, the approximate rotation relationship between the reference and query images are calculated from the point feature correspondences. Inspired by this, although we don't have such point correspondence information, we can directly compute the LineVec direction histograms of the reference and query images. We first calculate the two direction histograms of LineVecs from two images, then get the normalized histograms (h r , h q ) in which the subscript r denotes the reference image and q denotes the query image. Then we shift h q by an angle θ varying from 0 to 2π and search for the approximate global rotation angle θ g . By taking the angle as index in the histogram for simplicity, θ g is estimated as:</p><formula xml:id="formula_14">θ g = argmin 0≤θ≤2π h r (x) -h q (x -θ) . (<label>5</label></formula><formula xml:id="formula_15">)</formula><p>Since it's not always suitable to approximate the perspective transformation of images by a global rotation change, we have to check whether the estimated rotation angle is genuine. In practice, if the perspective transformation can be approximated by a rotation, then the shifted histogram distance h r (x)h q (xθ g ) is small. Fig. <ref type="figure" target="#fig_2">3</ref> gives an example of line direction histograms of an image pair. The estimated θ g is 0.349 rad and the shifted histogram distance is 0.243. Besides, if the repeatability of the extracted lines in the images is low, then histogram based method may fail, i.e. a wrong rotation angle may be accepted by the algorithm. To improve the robustness of this method, for lines falling in the same bin of the direction histogram, their length are accumulated as well. So, corresponding to a direction histogram, there is a length vector whose i th element is the accumulated length of all lines falling in the i th bin of the direction histogram.</p><p>In our implementation, we accept the estimated global rotation angle when the minimal shifted histogram distance is smaller than a threshold t h and the minimal shifted length vector distance is smaller than a threshold t l . In Sec.5.1, we will experimentally discuss these two thresholds. Once θ g is accepted, for a pair of LineVecs to be matched, if |αθ g | &gt; t θ in which α is the angle between their directions, they are considered to be a non-match without further checking their appearance similarities. If there is no accepted rotation angle between two images, then only the appearance similarities will be tested.</p><p>• Local Appearance Similarity: The local appearance similarity is measured by the distance of line descriptors. For each line in the LineVec, we generate a LBD descriptor vector V from the octave image where the line is extracted. When matching two sets of LineVecs extracted from an image pairs, the distances between all descriptors of a reference LineVec and a test LineVec are evaluated, and the minimal descriptor distance is used to measure the LineVec appearance similarity s. If s &gt; t s in which t s is the local appearance dissimilarity tolerance, then the corresponding two LineVecs won't be considered further.</p><p>After checking the unary geometric attribute of LineVecs and their local appearance similarities, the pairs passing these tests are taken as candidate matches. A set of loose thresholds should be chosen, otherwise there will be a larger chance of missing correct matches. In our implementation, the thresholds are empirically set as t θ = π/4, and t s = 0.35. The number of candidate matches is quite larger than the number of real matches because one can not only rely on the aforementioned verifications to decide the final matching results. However, the checking still significantly reduces the dimension of the following graph matching problem compared with direct combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Building the relational graph</head><p>For a set of candidate matches, we build a relational graph whose nodes represent the potential correspondences and the weights on the links represent pairwise consistencies between them.</p><p>Given a set of κ candidate matches, the relational graph is represented by an adjacency matrix A with a size of κ × κ following the terminology in <ref type="bibr" target="#b29">[30]</ref>. The value of the element in row i and column j of A is the consistent score of candidate LineVec matches (L i r , L i q ) and (L j r , L j q ) where L i r , L j r are LineVecs in the reference image and L i q , L j q are LineVecs in the query image. The consistent score is computed from the pairwise geometric attributes and appearance similarities of the candidate matched pairs. For describing the pairwise geometric attributes of two LineVecs (L i , L j ), we choose two lines (l i , l j ) which lead to the minimal descriptor distance between these two LineVecs and locate their endpoint positions in the original images. Then we describe the geometric attributes of (l i , l j ) by their intersection ratios (I i , I j ), projection ratios (P i , P j ) and relative angle Θ ij as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. I i and P i are computed as:</p><formula xml:id="formula_16">I i = --→ S i C • --→ S i E i | --→ S i E i | 2 , P i = | --→ S i S i p | + | ---→ E i E i p | | --→ S i E i | . (<label>6</label></formula><formula xml:id="formula_17">)</formula><p>I j and P j can be calculated in the same way. The relative angle Θ ij is easily calculated from the line directions. These three attributes are invariant to changes of translation, rotation, and scale.</p><p>As introduced in Sec.3.1, we use the LBD descriptor vector V to represent the local appearance of a line. Supposing the descriptors with minimal distances for LineVecs (L i r , L i q ) in the reference and query images are (V i r , V i q ) and for LineVecs (L j r , L j q ) are (V j r , V j q ) respectively, we get two sets of pairwise geometric attributes and local appearances for two candidate matches (L i r , L i q ) and (L j r , L j q ) as: {I i r , I j r , P i r , P j r , Θ ij r , V i r , V j r } and {I i q , I j q , P i q , P j q , Θ ij q , V i q , V j q }. Then the consistent score A ij is computed as:</p><formula xml:id="formula_18">A ij = ⎧ ⎨ ⎩ 5 -d I -d P -d Θ -s i V -s j V , if Γ is true; 0, else,<label>(7)</label></formula><p>where d I , d P and d Θ are the geometric similarities; s i V , s j V are the local appearance similarities; and Γ is the condition. They are defined as:</p><formula xml:id="formula_19">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ d I = min( |I i r -I i q | t I , |I j r -I j q | t I ); d P = min( |P i r -P i q | t P , |P j r -P j q | t P ); d Θ = |Θ ij r -Θ ij q | t Θ ; s i V = V i r -V i q ts ; s j V = V j r -V j q ts ; Γ ≡ {d I , d P , d Θ , s i V , s j V } ≤ 1,<label>(8)</label></formula><p>where Γ ≤ 1 means that each element in Γ is not larger than 1. Compared with <ref type="bibr" target="#b4">[5]</ref>, the definition of d I in our work is more robust against the fragmentation problem of line detection because only if one pair of matched lines in the reference and query images is well extracted, then d I could be very small no matter how bad the other pair is extracted. The definition of d P shares the same advantage. t I , t P , t Θ and t s are thresholds. In our implementation, they are set as t I = 1, t P = 1, t Θ = π/4 and t s = 0.35. For all the candidate matches, we compute the consistent score among them and obtain the adjacency matrix A. The diagonal elements of A equal zero as suggested by <ref type="bibr" target="#b30">[31]</ref> for better results and let A ji = A ij to keep the symmetry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generating the Final Matching Results</head><p>The matching problem is now reduced to finding the cluster of matches LM that maximizes the total consistent scores (L i r ,L i q ),(L j r ,L j q )∈LM A ij such that the mapping constraints are met. We use an indicator vector x to represent the cluster such that x(i) = 1 if (L i r , L i q ) ∈ LM and zero otherwise. Thus, the problem is formulated as:</p><formula xml:id="formula_20">x * = argmax(x T A x) (<label>9</label></formula><formula xml:id="formula_21">)</formula><p>where x is subject to the mapping constraints. The general quadratic programming techniques are too computationally expensive to solve this problem. We employ the spectral technique which relaxes both the mapping constraints and the integral constraints on x such that its elements can take real values in [0, 1].</p><p>By the Raleigh's ratio theorem <ref type="bibr" target="#b29">[30]</ref>, the x * that will maximize x T Ax is the principal eigenvector of A. What still remains is to binarize the eigenvector using mapping constraints and obtain a robust approximation of the optimal solution. The mapping constraints applied here are the sidedness constraint <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref> and the one-to-one constraint. Details of the algorithm are as follows:</p><p>1. Extract LineVecs from the reference and query images by EDLine <ref type="bibr" target="#b21">[22]</ref> in the scale space to obtain two sets of LineVecs L r and L q ;</p><p>2. Estimate the global rotation angle θ g of the image pair from the direction histograms of L r and L q ;</p><p>3. Compute the LBD descriptors of LineVecs in L r and L q ;</p><p>4. Generate a set of candidate matches CM = {(L 1 r , L 1 q ), (L 2 r , L 2 q ), . . . , (L κ r , L κ q )} by checking the unary geometric attribute and local appearance similarities of LineVecs in L r and L q ; 5. Build the adjacency matrix A with a size of κ × κ according to the consistence scores of pairs in CM; 6. Get the principal eigenvector x * of A by using ARPACK <ref type="bibr" target="#b31">[32]</ref>; 7. Initialize the matching result: LM ← ; 8. Find a = argmax 1≤i≤κ (x * (i)). If x * (a) = 0, then stop and return the matching result LM. Otherwise, set LM = LM ∪ {(L a r , L a q )}, CM = CM -{(L a r , L a q )} and x * (a) = 0. 9. Check all the candidates in CM. If (L j r , L j q ) conflicts with (L a r , L a q ), then set CM = CM -{(L j r , L j q )} and x * (j) = 0. 10. If CM is empty, then return LM. Otherwise go back to Step 8.</p><p>The final line matches can be directly retrieved from the matching results of LineVecs LM. Notice that, lines in the LineVec are located in the same region of image with the same direction, hence, for each pair of LineVec matches, it's enough to retrieve only one pair of line matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The descriptor performance evaluation</head><p>Before testing the proposed graph matching algorithm, we first analyze the influence of the LSR parameters, i.e. the number of bands m and the width of each band w, then evaluate the performance of LBD by comparing it with the well-known MSLD <ref type="bibr" target="#b10">[11]</ref> descriptor.</p><p>Mikolajczyk and Schmid <ref type="bibr" target="#b32">[33]</ref> established a benchmark to evaluate the performance of the local descriptors. We employ this framework to compare the performance of the line descriptors. The dataset in this experiment includes 8 groups of images with following transformations: illumination changes, inplane rotation, JPEG compression, image blurring, image occlusion, view point changes in the low-texture scene and the texture scene, and scale variations. There are 6 images in each group of sequence raising from small to large image transformations. Fig. <ref type="figure" target="#fig_12">5</ref> shows example images of our dataset, image sets of (a), (c) and (d) are from <ref type="bibr" target="#b32">[33]</ref>, and the rest are captured by ourselves to make sure images contain some line features. The images are either of planar scenes or the camera position was fixed during acquisition. Therefore, they are always related by a homography (plane projective transformation). The ground truth homographies are known. In order to better evaluate the descriptor performance for different image transformations, in this section we only consider lines extracted in the original image rather than in the octave images.</p><p>Since the ground truth of the image homographies is available, we first transfer the extracted lines in the query image into the reference image, then establish the ground truth of line correspondences by searching the parallel and close reference lines of the transferred lines. For the matching perfor- mance of descriptors reported in this section, we choose the nearest neighbor matching criterion to match lines according to their descriptor distance avoiding the prejudice of a distance threshold because different kinds of descriptors prefer different thresholds. Another advantage of this matching criterion is that the recall ratio (the number of correct matches divided by the number of ground correspondences) and the matching precision (the number of correct matches divided by the number of total matches) are only decided by the number of correct matches because the denominators for different descriptors are equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The descriptor dimension</head><p>The influence of the LSR parameters are analyzed experimentally. We vary m and w from 3 to 13, respectively. Fig. <ref type="figure" target="#fig_14">6</ref> shows how the number of correct matches of all images is influenced by these two parameters. It's clear that LBD and MSLD share similar rules: the performance increases fast at the beginning with the increment of m or w, then reaches the best performance when m = 9 and w is about 7 or 9, after that there is a steady performance decrease. The results are well explained by the fact that larger values of m and w (i.e. larger LSR) make the descriptor more distinctive while they also reduce the repeatability of the LSR.</p><p>We also evaluate the time performances of these two descriptors which are given in Tab.1. Although the time performances may change from image to image, their relative relationship will keep the same. We only show the results which are generated from an example image with the size of 900 × 600 and 573 extracted lines . Basically, the larger the m and w are, the more computational time is consumed. LBD is less sensitivity to the increase of m and w than that of MSLD, especially for the increase of w.</p><p>Based on the aforementioned evaluation, through the rest of the paper, the descriptor will be computed from a LSR with m = 9 and w = 7, resulting in a 72-dimensional descriptor. Then the computational times of LBD and MSLD for the example image are 28ms, and 137ms, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Further comparison of MSLD and LBD</head><p>In this section, we report the comparison details of the descriptor performance for each group of images in our dataset (Fig. <ref type="figure" target="#fig_12">5</ref>). For each group of images, the recall ratios of MSLD and LBD are given in Fig. <ref type="figure" target="#fig_4">7</ref>.    Conclusively, for the most kinds of image transformations (Fig. <ref type="figure" target="#fig_4">7</ref>.(a-f)), it's clear that LBD performs better than MSLD. For the large scale change (Fig. <ref type="figure" target="#fig_4">7</ref>.(h)), as explained in Sec.1.2, all these two descriptors perform badly though MSLD is slightly better, because in this experiment, lines are only extracted in the original image. However, this can be made up by extracting lines in the scale space as addressed in Sec.2.1 and will be illustrated in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Line matching experiments</head><p>In this section, we first experimentally analyze the direction histogram based rotation estimation method proposed in Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The analysis of the rotation estimation method</head><p>In Sec.3.1, we propose the direction histogram based rotation estimation method which is employed to reduce the number of candidate matches when the estimated rotation angle is accepted. The robustness of this method is very important for the matching algorithm. Generally, there are two kinds of errors: false negative (i.e. a correct rotation angle is rejected) and false positive (i.e. a wrong rotation angle is accepted). For false negative errors, the matching algorithm will be less efficient because more candidate line matches c 0, 0 0, 0 0, 0 0, 0 0, 0 0.088, 0.087 0.049, 0.406 0.075, 0.052 0.138, 0.081 0.221, 0.161 d 0, 0 0, 0 0, 0 0, 0 0, 0 0.077, 0.067 0.189, 0.134 0.339, 0.251 0.367, 0.259 0.375, 0.301 e 0, 0 0, 0 0, 0 0, 0 0, 0 0.043, 0.043 0.036, 0.055 0.076, 0.145 0.090, 0.150 0.065, 0.122 f 0, 0 0, 0 0, 0 0, 0 0, 0 0.167, 0.103 0.235, 0.286 0.182, 0.196 0.169, 0.208 0.365, 0.299 g 0, 80 0, 0 0, 0 0, 0 0, 0 0.578, 0.626 0.292, 0.366 0.085, 0.099 0.207, 0.437 0.222, 0.448 h 0, 0 0, 0 0, 0 0, 0 0, 0 0.132, 0.105 0.157, 0.240 0.290, 0.299 0.357, 0.331 0.296, 0.344 will be generated without checking their directions, but the matching algorithm can still match lines accurately. For false positive errors, the matching algorithm may fail because the wrong rotation angle is employed to generate wrong candidate line matches. Therefore, our priority goal is to control the false positive error as low as possible while keep small false negative error.</p><p>In Sec.3.1, there are two thresholds: the minimal shifted histogram distance threshold t h and the minimal shifted length vector distance threshold t l . It's hard to give a theoretic analysis about the optimal setting of these thresholds because they depends on the scene environment, the image transformation and the line detection method. Here, we use the image set in Fig. <ref type="figure" target="#fig_12">5</ref> to experimentally choose the proper values of these thresholds. For each image transformation, 5 images are compared to the reference image. Tab.2 reports the ground truth and the estimated rotation angle, the minimal shifted histogram distance and the minimal shifted length vector distance between two images. Notice that the precision of the estimated rotation angle equals to the resolution of the direction histogram. In our implementation, it's 20 degrees. In this experiment, for the image pair g.1, the estimated rotation angle is obviously wrong. Hence, it should be rejected. In order to control the false positive error, we choose a pair of conservative thresholds (t h = t l = 0.5) although there is a false negative error (the correct estimated rotation angle is rejected for image pair b.5). This threshold setting generally works well even for a challenging image set as shown in the following experiment. If the efficiency of the matching algorithm is not important while matching failure is definitely not acceptable, then one can set t h and t l to zero, i.e., the estimated rotation angle will always be rejected and the unary attribute of lines will never be employed to generate the candidate matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">The improvement of the matching performance</head><p>As introduced in Sec.1.2, in order to mitigate the problem of segment fragmentation and image variation, we propose to extract lines in the scale space (marked as +S). Besides, the geometric constraints are applied to improve the matching robustness (marked as +G). The sign +S&amp;G denotes both strategies are applied. We now show the influence of these two strategies on the matching performance. Limited by the space, we only illustrate the performance improvement for the large scale changes although the approaches are generally effective for other image transformations. Fig. <ref type="figure" target="#fig_16">8</ref> shows the improved matching performance of the local appearance based algorithms (LBD and MSLD). Taken Fig. <ref type="figure" target="#fig_16">8</ref>.(a) for example, when the scale variation is large, the performance gain of LBD+S&amp;G is obvious. Although when the scale variation is small, the recall ratio of LBD+S&amp;G is slightly smaller than that of LBD or LBD+G because a larger number of lines are extracted in the scale space than in the original image. The results also show that LBD+G is better than LBD, but it is still not robust to scale changes because we employ the geometric consistency verification as post process. When both strategies are applied, the matching performance of LBD+S&amp;G is less sensitive to the scale changes and always better than LBD+S. The proposed two strategies are also effective for the MSLD based matching algorithms as shown in Fig. <ref type="figure" target="#fig_16">8</ref>.(b). The performance comparison between LBD+S&amp;G and MSLD+S&amp;G together with other state-of-the-art methods are presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with state-of-the-art methods</head><p>For a fair comparison with previous work, in this section we conduct the comparison experiment on the images demonstrated in the literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> (except the occlusion image pair which is captured in our office). The differences between the image set in Fig. <ref type="figure" target="#fig_17">9</ref> and the previous image set (Fig. <ref type="figure" target="#fig_12">5</ref>) are that: <ref type="bibr">(1)</ref>, this image set only includes an image pair rather than an image sequence for each group of transformation; (2), most of the image pairs in this image set are more challenging. As introduced in Sec.1, the existing approaches to match lines are mainly of three types. Hence, the proposed algorithm LBD+S&amp;G, is compared with three representatives from three groups which are recently reported to feature remarkable performance: the Line Signature (LS) <ref type="bibr" target="#b4">[5]</ref>, the Line matching leveraged by Point correspon-dences (LP) <ref type="bibr" target="#b5">[6]</ref> and the Mean-Standard deviation Line Descriptor (MSLD) <ref type="bibr" target="#b10">[11]</ref> ( here, we use MSLD+S&amp;G instead because it performs better than MSLD as illustrated in Fig. <ref type="figure" target="#fig_16">8(b)</ref>). The implementations of LS and LP are by the courtesy of their authors while MSLD+S&amp;G is implemented by ourselves with parameter settings as recommended by its authors.</p><p>In this experiment, the line detectors used by LBD+S&amp;G and MSLD+S&amp;G are the same as described in Sec.   The last image pair in Fig. <ref type="figure" target="#fig_10">10</ref> is a textured scene with strong scale and rotation variations. The matching algorithm is less performing for these three image pairs than for the rest of image pairs. Nevertheless, the results shown in Fig. <ref type="figure" target="#fig_10">10</ref> are still quite acceptable and establish many line correspondences with few mismatches.</p><p>It's worth to note that similar to the parameter detection methods adopted in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref>, it's empirical to find the good parameter settings. However, these parameter settings are fixed as presented in Tab.3 for all the experiments. The results that the algorithm works well for a large range of image variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and discussion</head><p>We address the problem of line matching for image pairs under various situations: low-texture scenes, partial occlusion, rotation changes, blurred images, illumination changes, moderate viewpoint changes, and scale changes.</p><p>We show the robustness and the efficiency of our graph matching process.</p><p>The good performance achieved by the proposed algorithm is mainly because we detect lines in the scale space and combine the local appearance and geometric constraints together which eliminates lots of mismatches. The source code of the proposed algorithm, the image dataset and the matching results are available on our website<ref type="foot" target="#foot_0">1</ref> .</p><p>The dimension of the LBD descriptor to characterize the local appearance of line segment is a 72-vector with float elements. For retrieving line features from a large data base, it may be not efficient enough. The idea of Brief <ref type="bibr" target="#b33">[34]</ref> or its enhancements Brisk <ref type="bibr" target="#b24">[25]</ref> and ORB <ref type="bibr" target="#b25">[26]</ref> could be employed to improve the matching process. In order to conduct a set of meaningful binary tests, the LSR should be normalized to a regular region with a fixed size. It's worth to address this in the future.</p><p>Besides, the geometric constraints are enforced globally in this paper by using the spectral technique. For images undergoing a moderate view point transformation, the global geometric constraints are maintained well. For strong wide baseline images of the non-planar scenes, the global constraints may be violated, then it's better to enforce the local geometric constraints like the approach in <ref type="bibr" target="#b4">[5]</ref> although it is more time consuming.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the line detection in the scale space. The original image is downsampled to generate a set of octave images. For each octave image, line segments are extracted by EDLine [22] detector. For all extracted lines, they are re-organized to form a set of LineVecs. Lines in the same LineVec have the same direction and are corresponding to the same region in the original image.</figDesc><graphic coords="8,148.92,118.84,328.33,203.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the band representation. A local rectangular region around the line is chosen as the line support region (LSR). Two directions d L and d ⊥ are introduced. The LSR is divided into m bands with the width of w, (e.g. m = 5, w = 3). A global Gaussian function f g is applied to all rows in the LSR. For each band (e.g. Band2), a local Gaussian function f l is applied to rows in the band and its nearest two neighbor bands.Small arrows represent the gradients of pixels in the LSR.</figDesc><graphic coords="10,148.89,118.89,328.15,209.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of line direction histograms. The first two images show the reference and query images with detected lines and the plot shows their direction histograms. The resolution of each bin is 20 degrees, so there are 18 bins for each histogram.</figDesc><graphic coords="13,145.17,118.87,109.86,82.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the pairwise geometric attributes. C is the intersection of two lines. (S i , E i ) are endpoints of the line l i and (S ip , E i p ) are their projections onto the line l j . Similarly, (S j , E j ) are endpoints of the line l j and (S j p , E j p ) are their projections onto the line l i .</figDesc><graphic coords="17,221.92,118.99,182.42,104.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7.(a) shows the performances of MSLD and LBD for the image illumination changes. From image 1 to image 5, the lighting condition gets worse. The recall ratios decrease with the increment of the lighting distortion. Fig.7.(b) shows the results for images which are generated by a set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7.(e) shows the descriptor performance against image occlusion. To evaluate the occlusion effect, we first artificially add some vertical line features in a background image, then shift the region of interest along the vertical direction of the artificial image to generate a set of smaller images as shown in Fig.5.(e). This process makes sure that for the most of the lines, their LSR in the image sequence will change gradually (some part of the LSR moves out or in). The results show that the descriptor performance decreases with the increment of the image occlusion. Fig.7.(f) shows the descriptor performance in the low-texture scene. Images in this sequence are captured in front of the window with small view point changes. The results don't show drastic change in performance because of the small baseline between images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7.(g) shows the descriptor performance against large view point change.The view angles between the query images and the reference image range approximately from -70 circ to 60 • . No doubt, the descriptors perform better when the absolute value of the view angle is smaller (image3 and image4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7.(h) shows the most challenging case for the descriptors, i.e, the large scale change. The scale ratio between the query images and the reference image range from 0.9 to 0.3. The performance decreases fast with the scale change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3 . 1 .</head><label>31</label><figDesc>Then we illustrate the performance improved by the multi-scale line extraction strategy and the geometric consistency verification. At last, we compare the proposed line matching algorithm with the state-of-the-art methods. The following experiments are performed on a 3.4GHz Intel(R) Core 2 processor with 8 GB of RAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 . 1 .</head><label>21</label><figDesc>The extracted Linevecs are also used as input to LP. For LS, it uses its own multi-scale line detector because the structure of line signature in<ref type="bibr" target="#b4">[5]</ref> is quite different from the structure of Linevec. The parameter settings of the proposed algorithm are summarized in Tab.3. The comparison results are given in Tab.4. All the matched lines are checked one by one manually to test whether a matched line pair is correct or not. It's clear that LP is less performing for the low-texture scene, because the local appearances of lines are indistinguishable and the images lack stable point correspondences. The results also show that LBD+S&amp;G performs slightly better than MSLD+S&amp;G even for images with large scale variations because the drawback of LBD is made up by the multi-scale line extraction strategy. Surprisingly, LS has a bad matching result for image pair (h) which is inconsistent with the result presented in<ref type="bibr" target="#b4">[5]</ref>. If we change the role of reference and query images, then we can get the same good result. This illustrates that the matching results of LS are depending on the order of images in a pair. Compared to MSLD+S&amp;G, LP and LS, the most superior feature of LBD+S&amp;G is its time performance. Here, the time of LP given in Tab.4 is its complete processing time which includes generating point correspondences and matching lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 illustrates</head><label>10</label><figDesc>Fig.10 illustrates the matching results of LBD+S&amp;G in three challenging scenes. The matched lines in each pair are assigned the same color and one of their endpoints are connected to illustrate their correspondences. These figures are better viewed in color. The first image pair in Fig.10 is a lowtexture planar scene with illumination and view point changes. The second image pair in Fig.10 is a non-planar scene with moderate view point changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples in the image dataset including 8 groups of image transformations. For each group, there are six images in the dataset raising from small to large transformations (Images in (f) are generated by increasing baseline between views in the low texture environment). The first and the last images of each group are shown here. The left image in each pair is chosen as the reference image.</figDesc><graphic coords="36,136.19,419.14,87.86,65.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The analysis of the descriptor dimension. The numbers of bands and the width of each band both vary from 3 to 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The comparison of the descriptor performance in terms of the recall ratio over 5 test images for each image transformation in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Illustration of the performance improvement. In (a), LBD is the original descriptor performance, LBD+G is generated by applying geometric constraints, LBD+S is generated by detecting lines in the scale space and LBD+S&amp;G is generated by employing both strategies. (b) is related to MSLD descriptor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Image dataset for comparison experiment. In each group, there are two images with large transformations.</figDesc><graphic coords="39,326.17,442.80,87.82,65.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Illustration of the LBD+S&amp;G matching results.</figDesc><graphic coords="40,139.72,424.15,347.34,138.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The time performance of descriptors (LBD, MSLD). The results are given in ms by varying the number of bands m and the width of each band w. The time is measured on a 3.4GHz Intel(R) Core2 processor with 8 GB of RAM.</figDesc><table><row><cell>@ m @ @ w @ @</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>13</cell></row><row><cell>3</cell><cell cols="2">4, 13 7, 27</cell><cell>9, 41</cell><cell cols="3">12, 62 14, 85 17, 115</cell></row><row><cell>5</cell><cell cols="6">6, 23 12, 44 15, 72 20, 107 24, 151 28, 208</cell></row><row><cell>7</cell><cell cols="6">9, 33 15, 63 21, 103 28, 154 34, 217 42, 303</cell></row></table><note><p>9 12, 42 20, 82 28, 137 37, 200 45, 281 52, 383 11 15, 52 24, 100 34, 165 44, 245 53, 346 63, 470 13 17, 61 29, 119 40, 195 51, 296 65, 418 74, 566</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The experimental analysis of the direction histogram based rotation estimation method. For each image pair, the following results are reported: the ground truth and the estimated rotation angle in degrees, the minimal shifted histogram distance and length vector distance.</figDesc><table><row><cell></cell><cell cols="5">ground truth and estimated angle</cell><cell></cell><cell cols="3">histogram distance and length vector distance</cell><cell></cell></row><row><cell>Img</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>a</cell><cell>0, 0</cell><cell>0, 0</cell><cell>0, 0</cell><cell>0, 0</cell><cell>0, 0</cell><cell cols="5">0.064, 0.059 0.065, 0.054 0.048, 0.052 0.079, 0.129 0.108, 0.101</cell></row><row><cell cols="11">b 15, 10 30, 30 45, 40 60, 60 75, 70 0.493, 0.482 0.415, 0.472 0.379, 0.420 0.152, 0.133 0.528, 0.561</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The summary of parameters of the LBD+S&amp;G algorithm and their settings in</figDesc><table><row><cell>the experiments</cell><cell></cell><cell></cell></row><row><cell></cell><cell>m number of bands</cell><cell>9</cell></row><row><cell>Descriptor</cell><cell></cell><cell></cell></row><row><cell></cell><cell>w width of band</cell><cell>7</cell></row><row><cell>Histogram</cell><cell>t h histogram distance threshold</cell><cell>0.5</cell></row><row><cell></cell><cell>t l length vector distance threshold</cell><cell>0.5</cell></row><row><cell></cell><cell cols="2">t I intersection ratio difference threshold 1</cell></row><row><cell>Consistency</cell><cell cols="2">t P projection ratio difference threshold 1</cell></row><row><cell></cell><cell>t θ relative angle difference threshold</cell><cell>π/4</cell></row><row><cell></cell><cell>t s appearance dissimilarity threshold</cell><cell>0.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Img LBD MSLD LP LS</cell><cell></cell><cell cols="4">Img LBD MSLD LP LS</cell><cell>Img LBD MSLD LP LS</cell></row><row><cell></cell><cell></cell><cell cols="2">+S&amp;G+S&amp;G</cell><cell></cell><cell></cell><cell></cell><cell cols="2">+S&amp;G+S&amp;G</cell><cell>+S&amp;G+S&amp;G</cell></row><row><cell></cell><cell>a</cell><cell>54</cell><cell>44</cell><cell>12 54</cell><cell></cell><cell>a</cell><cell>94</cell><cell>92</cell><cell>67 96</cell><cell>a 0.11 0.35 5.5 8</cell></row><row><cell></cell><cell>b</cell><cell>54</cell><cell>57</cell><cell>50 76</cell><cell></cell><cell cols="4">b 100 100 94 100</cell><cell>b 0.04 0.07 4.5 1</cell></row><row><cell>Total Matches</cell><cell cols="4">c 263 240 253 188 d 106 121 101 43 e 245 223 262 241 f 446 445 422 281 g 87 78 117 151</cell><cell>Match Precision (%)</cell><cell cols="4">c 100 100 100 100 d 100 98 100 100 e 100 100 100 100 f 100 100 100 100 g 100 100 91 98</cell><cell>Time(s)</cell><cell>c 0.38 0.48 13 26 d 0.55 0.59 38 5 e 0.59 0.65 28 8 f 1.75 2.49 31 10 g 0.20 0.42 22 8</cell></row><row><cell></cell><cell>h</cell><cell>44</cell><cell>33</cell><cell>54 14</cell><cell></cell><cell>h</cell><cell>95</cell><cell>88</cell><cell>76 29</cell><cell>h 0.51 0.54 54 8</cell></row></table><note><p><p><p><p><p><p><p>Comparison of our approach (LBD+S&amp;G) with three line matching algorithms (MSLD</p><ref type="bibr" target="#b10">[11]</ref></p>+S&amp;G, LP</p><ref type="bibr" target="#b5">[6]</ref></p>, LS</p><ref type="bibr" target="#b4">[5]</ref></p>). For each image pair, the following results are reported: the number of total matches, the matching precision and the computational time.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.mip.informatik.uni-kiel.de/tiki-index.php?page=Lilian+Zhang.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by China Scholarship Council (No.2009611008).</p><p>The authors thank the anonymous reviewers for their valuable comments that helped to improve the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structure and motion from line segments in multiple images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1021" to="1032" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object recognition in high clutter images using line features</title>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dementhon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1581" to="1588" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Moving in stereo: Efficient structure and motion using lines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1741" to="1748" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Wide-baseline image matching using line signatures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1311" to="1318" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Line matching leveraged by point correspondences</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="390" to="397" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Line matching using appearance similarities and geometric constraints, DAGM-OAGM</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">7476</biblScope>
			<biblScope unit="page" from="236" to="245" />
			<date type="published" when="2012">2012</date>
			<pubPlace>Graz</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tracking line segments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A fast visual line segment tracker</title>
		<author>
			<persName><forename type="first">P</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Protzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vidal-Calleja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacroix</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ETFA</publisher>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">2d line matching using geometric and intensity data</title>
		<author>
			<persName><forename type="first">D.-M</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>AICI</publisher>
			<biblScope unit="page" from="99" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A robust descriptor for line matching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Msld</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="941" to="953" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient registration of stereo images by matching graph descriptions of edge segments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Faverjon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="107" to="131" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structural matching in computer vision using probabilistic relaxation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="749" to="764" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stereo correspondence through feature grouping and maximal cliques</title>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Skordas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1168" to="1180" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural matching by discrete relaxation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="634" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Wide-baseline stereo matching with line segments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic line matching across views</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="666" to="671" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matching disparate views of planar surfaces using projective invariants</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I A</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Halkidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Orphanoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="673" to="683" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Wide-baseline image matching based on coplanar line intersections</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1157" to="1164" />
		</imprint>
		<respStmt>
			<orgName>IROS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scale and rotation invariance of the evidence accumulation-based line detection algorithm</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Chmielewski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
		<respStmt>
			<orgName>CORES</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scale invariant segment detection and tracking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L C</forename><surname>Amaury Negre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ISER</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Edlines: A real-time line segment detector with a false detection control</title>
		<author>
			<persName><forename type="first">C</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Topal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1633" to="1642" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stefan Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<title level="m">Brisk: Binary robust invariant scalable keypoints, in: ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Orb: an efficient alternative to sift or surf</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Ethan Rublee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vincent Rabaud</surname></persName>
		</author>
		<author>
			<persName><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Matching widely separated views based on affine invariant regions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="61" to="85" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Shape recognition with edgebased features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Edge descriptors for robust wide-baseline correspondence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A spectral technique for correspondence problems using pairwise constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1482" to="1489" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning for graph matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Lehoucq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maschhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://www.caam.rice.edu/software/ARPACK/" />
		<title level="m">Arpack software</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Brief: Binary robust independent elementary features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-scale line detection strategy is applied</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Line Band Descriptor is developed to depict the local appearance of line</title>
	</analytic>
	<monogr>
		<title level="m">A novel</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A graph is build which combines appearance similarities and geometric consistencies</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
