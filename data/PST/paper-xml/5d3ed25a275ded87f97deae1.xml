<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Graph Convolutional Networks Against Adversarial Attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Beijing National Research Center for Information Science and Technology(BNRist)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<email>zw-zhang16@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
							<email>cuip@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Graph Convolutional Networks Against Adversarial Attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330851</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Convolutional Networks</term>
					<term>Robustness</term>
					<term>Adversarial Attacks</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) are an emerging type of neural network model on graphs which have achieved state-ofthe-art performance in the task of node classification. However, recent studies show that GCNs are vulnerable to adversarial attacks, i.e. small deliberate perturbations in graph structures and node attributes, which poses great challenges for applying GCNs to real world applications. How to enhance the robustness of GCNs remains a critical open problem.</p><p>To address this problem, we propose Robust GCN (RGCN), a novel model that "fortifies" GCNs against adversarial attacks. Specifically, instead of representing nodes as vectors, our method adopts Gaussian distributions as the hidden representations of nodes in each convolutional layer. In this way, when the graph is attacked, our model can automatically absorb the effects of adversarial changes in the variances of the Gaussian distributions. Moreover, to remedy the propagation of adversarial attacks in GCNs, we propose a variance-based attention mechanism, i.e. assigning different weights to node neighborhoods according to their variances when performing convolutions. Extensive experimental results demonstrate that our proposed method can effectively improve the robustness of GCNs. On three benchmark graphs, our RGCN consistently shows a substantial gain in node classification accuracy compared with state-of-the-art GCNs against various adversarial attack strategies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are ubiquitous in the real world, representing complex relationships between objects such as social networks, e-commerce networks, biological networks and traffic networks. How to mine the rich value underlying graph data has long been an important research direction. Graph Convolutional Networks (GCNs) are a type of neural network model for graphs that recently attracts considerable research attention <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18]</ref>. State-of-the-art GCNs usually follow a "message-passing" framework <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref> where each node aggregates information from its immediate neighbors in each convolutional layer. GCNs have been shown to achieve promising results in many graph analytic tasks such as node classification.</p><p>Despite their remarkable performance, recent studies show that GCNs are vulnerable to adversarial attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, i.e. carefully designed small perturbations in graph structures and node attributes. By changing only a few links or node attributes which are unnoticeable to the users, the performance of GCNs can be drastically degraded, which poses great challenges for applying GCNs to real world applications, especially those risk-sensitive scenarios such as finance networks or medical networks. Therefore, how to design a robust GCN model in the presence of adversarial attacks is a crucial open question.</p><p>The challenges of this problem are two-fold. Firstly, we need to enhance the robustness of GCNs while maintaining its effectiveness, which in turn requires that we need to empower GCNs to tolerate adversarial information while not changing its backbone architecture. Secondly, since nodes in the graphs are entangled, the adversarial attacks can propagate to affect many other nodes besides the directly attacked nodes. How to prevent such propagation in the "message-passing" framework is another key issue.</p><p>In this paper, we propose Robust GCN (RGCN), a novel model to improve the robustness of GCNs in the presence of adversarial attacks. Specifically, rather than representing nodes by plain vectors as in all existing GCNs, we propose adopting Gaussian distributions as the hidden representations of nodes in all graph convolutional layers. The motivation is that when the graph data is attacked, the perturbations in graph structures and node attributes are usually abnormal compared to the existing information. For example, the attacker tends to connect nodes from different communities to confuse the classifier <ref type="bibr" target="#b6">[7]</ref>. While plain vectors cannot adapt to such changes, Gaussian distributions can automatically absorb the effects of such unexpected adversarial changes in the variances <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36]</ref>. As a result, using Gaussian distributions can enhance the robustness of GCNs. Furthermore, in order to remedy the propagation of adversarial attacks in GCNs, we propose a variance-based attention mechanism, i.e. assigning different weights to node neighborhoods according to their variances in the convolution operation. More concretely, when one node is attacked in previous GCNs, the adversarial effects will propagate to its neighbors through the convolutions, affecting a large proportion of the graph. In our model, the effect of such propagation will be greatly reduced since the attacked nodes tend to have large variances and small weights in affecting other nodes.</p><p>To verify the efficacy of our proposed method, we conduct extensive experiments on several benchmark datasets for the task of node classification. Experimental results show that our RGCN consistently outperforms state-of-the-art GCNs under various adversarial attack strategies, demonstrating the robustness of our proposed method. We also empirically analyze the reasons behind the effectiveness of our proposed method.</p><p>The contributions of this paper are summarized as follows:</p><p>• We propose RGCN, a novel model that explicitly enhances the robustness of GCNs against adversarial attacks. To the best of our knowledge, we are the first to investigate this critical and challenging problem. • We propose using Gaussian distributions in graph convolutional layers to absorb the effects of adversarial attacks and introduce a variance-based attention mechanism to prevent the propagation of adversarial attacks in GCNs. • Extensive experimental results demonstrate the effectiveness of our proposed method under various adversarial attack strategies.</p><p>The rest of the paper is organized as follows. In Section 2, we review related works. In Section 3, we summarize the notations and give some preliminaries. We introduce our proposed method in Section 4 and report experimental results in Section 5. Finally, we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work builds upon two categories of recent research: graph convolutional networks and graph adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Convolutional Networks</head><p>Graph convolutional networks (GCNs), aiming to generalize convolutional neural networks to graph data, have drawn increasing research interests in the past few years. Next, we briefly review some representative GCNs, and readers are referred to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref> for some comprehensive surveys. Bruna et al. <ref type="bibr" target="#b2">[3]</ref> first introduce convolutions for graph data using graph signal processing <ref type="bibr" target="#b24">[25]</ref>. By using the eigen-decomposition of the graph Laplacian matrix, the convolution is defined in the graph spectral domain. However, since the full eigenvectors of the Laplacian matrix are needed, the time complexity of such graph convolution is very high. To solve the efficiency problem, Cheb-Net <ref type="bibr" target="#b7">[8]</ref> proposes to use a K-order Chebyshev polynomial <ref type="bibr" target="#b12">[13]</ref> to approximate the convolutional filters in the spectral domain. By using the recurrence relation of Chebyshev polynomials, the exact eigen-decomposition of the Laplacian matrix is avoided, thus reducing the overall time complexity. Kipf and Welling <ref type="bibr" target="#b17">[18]</ref> further propose to simplify the graph convolution using only the 1 st order polynomial, i.e. the immediate neighborhoods. By stacking multiple convolutional layers, this GCN variant achieves state-ofthe-art performance in the semi-supervised node classification task. They also design a variational graph autoencoder using GCN as the encoder <ref type="bibr" target="#b16">[17]</ref>. MPNNs <ref type="bibr" target="#b9">[10]</ref> and GraphSAGE <ref type="bibr" target="#b11">[12]</ref> unify these approaches using the "message-passing" framework, i.e. defining the graph convolution as nodes aggregating information from immediate neighborhoods. Further improvements include adding an attention mechanism to assign different weights in aggregating node neighborhoods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>, adding residual and jumping connections <ref type="bibr" target="#b31">[32]</ref>, sampling to improve efficiency <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref> considering edge attributes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>, disentangling node representations <ref type="bibr" target="#b19">[20]</ref> and automatically selecting hyper-parameters <ref type="bibr" target="#b28">[29]</ref>. However, all of these works do not consider the robustness of the GCN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Adversarial Attacks</head><p>Recently, to show the vulnerability of GCNs, some adversarial attack methods have been proposed. The basic idea is to perturb graph structures and nodes attributes so that GCNs cannot correctly classify certain nodes. Meanwhile, the perturbations (i.e. attacks) should be made unnoticeable to users. Based on different settings, graph adversarial attacks can be divided into following categories <ref type="bibr" target="#b26">[27]</ref>:</p><p>• Poisoning or Evasion. Poisoning attacks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> try to attack the model by changing training data and evasion attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref> try to generate fake samples for a trained model, i.e. the attacks are categorized by whether they are before (poisoning attacks) or after (evasion attacks) the training phase of GCNs. • Targeted or Non-targeted. In targeted attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, the attacker focus on misclassifying some target nodes while in non-targeted attacks <ref type="bibr" target="#b37">[38]</ref>, the attacker aims to reduce the overall model performance. • Direct or Influence. Targeted attacks can be further divided into two categories based on attack settings. In direct attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, the attacker can directly manipulate the edges or features of the target nodes. In influence attacks <ref type="bibr" target="#b36">[37]</ref>, the attacker can only manipulate other nodes except the targets.</p><p>Graph adversarial attacks have posed great challenges to the robustness of GCNs, which severely limits the applicability of GCNs in real world applications. To the best of our knowledge, there is no study on improving the robustness of GCNs or preventing them from adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NOTATIONS AND PRELIMINARIES</head><p>In this section, we summarize the notations used in this paper and introduce some preliminaries of GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>In this paper, a graph is defined as G = (V, E), where V = {v 1 , ..., v N } denotes the set of nodes, N = |V| is the number of nodes, and E ⊆ V × V is the set of edges between nodes. Let A denote the adjacency matrix and D i,i = j A i, j denote the diagonal degree matrix. Let X be a matrix of node feature vectors. We define</p><formula xml:id="formula_0">H (l ) = h (l ) 1 , h (l ) 2 , ..., h<label>(l )</label></formula><p>N as the hidden representations of nodes in the l t h layer of a deep model where h</p><formula xml:id="formula_1">(l )</formula><p>i is the representation of node v i and [•, •] is concatenation. We further define f l as the dimensionality of h (l ) i and L as the number of layers. For convenience, we also denote X as H (0) . The immediate neighborhoods of node v i , including v i itself, are denoted as ne(i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Convolutional Networks</head><p>Though a number of different GCN methods have been proposed, here we focus on a representative one proposed by Kipf and Welling <ref type="bibr" target="#b17">[18]</ref>. Here, the (l + 1) t h convolutional layer is defined as:</p><formula xml:id="formula_2">h (l +1) i = ρ j ∈ne(i) 1 Di,i Dj, j h (l ) j W (l ) ,<label>(1)</label></formula><p>or in the equivalent matrix form:</p><formula xml:id="formula_3">H (l +1) = ρ D− 1 2 Ã D− 1 2 H (l ) W (l ) ,<label>(2)</label></formula><p>where Ã = A + I N , D = D + I N , I N is the identity matrix, ρ(•) is a non-linear activation function such as ReLU and W (l ) are trainable parameters. The general philosophy is that nodes should exchange information with their immediate neighbors in each convolutional layer, followed by applying learnable filters and some non-linear transformation. This architecture can be trained end-to-end using task-specific loss function, for example, the cross entropy loss in semi-supervised node classification as follows:</p><formula xml:id="formula_4">L cl s = − v ∈V L C c=1 Y vc log Ŷvc ,<label>(3)</label></formula><p>where V L is the set of labeled nodes, C is the number of classes, Y is the label matrix and Ŷ = so f tmax(H (L) ) are predictions of GCN by passing the hidden representation in the final layer H (L) to a softmax function.</p><p>Although this GCN variant is shown extremely effective in the node classification task, later works show that it can be easily fooled by adversarial attacks, as discussed in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ROBUST GRAPH CONVOLUTIONAL NETWORK</head><p>In this section, we introduce our proposed method. We first show the overall framework, and then elaborate on the technical details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework</head><p>To enhance the robustness of GCNs against adversarial attacks, we propose Robust Graph Convolutional Network (RGCN) and the framework is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Compared with existing methods, we introduce two novel modifications: we use Gaussian distributions as the hidden representations of nodes in graph convolutional layers and assign attention weights to node neighbors according to their variances. Meanwhile, our model explicitly considers the mathematical relevance between means and variances by the sampling process and regularizations. In the following subsections, we will introduce how to realize our model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gaussian-based Graph Convolution Layer</head><p>Since we use Gaussian distributions in the hidden layers, the existing graph convolutions are no longer applicable. Next, we formally define a Gaussian-based graph convolution layer to perform convolution operations between Gaussian distributions. Denote h</p><formula xml:id="formula_5">(l ) i = N (µ (l ) i , diaд(σ (l ) i ))</formula><p>as the latent representation of node v i in layer l, where µ (l ) i ∈ R f l is the mean vector and diaд(σ</p><formula xml:id="formula_6">(l ) i ) ∈ R f l ×f l is the diagonal variance matrix 1 . We use M (l ) = µ (l ) 1 , ..., µ (l ) N ∈ R N ×f l and Σ (l ) = σ (l ) 1 , ..., σ<label>(l )</label></formula><p>N ∈ R N ×f l to denote the matrix of means and variances for all nodes, respectively.</p><p>Consider n independent random variables following Gaussian distributions. Their weighted sum also follows a Gaussian distribution, as we show in the following theorem. Theorem 4.1. If x i ∼ N µ i , diaд(σ i ) i = 1, ..., n, and they are independent, then for any fixed weights w i , we have:</p><formula xml:id="formula_7">n i=1 w i x i ∼ N n i=1 w i µ i , diaд n i=1 w 2 i σ i .<label>(4)</label></formula><p>Proof. The proof can be found in probability textbooks such as <ref type="bibr" target="#b21">[22]</ref> and is omitted for brevity. □</p><p>Using this theorem and assuming all hidden representations of nodes are independent 2 , we can aggregate node neighbors as follows:</p><formula xml:id="formula_8">h (l ) ne(i) = j ∈ne(i) 1 Di,i Dj, j h (l ) j ∼ N j ∈ne(i) 1 Di,i Dj, j µ (l ) j , diaд j ∈ne(i) 1 Di,i Dj, j σ (l ) j .</formula><p>(5) In other words, the aggregation of node neighbors also follows a Gaussian distribution.</p><p>In addition, to prevent the propagation of adversarial attacks in GCNs, we propose an attention mechanism to assign different weights to neighbors based on their variances since larger variances indicate more uncertainties in the latent representations and larger probability of having been attacked. Specifically, we use a smooth exponential function to control the effect of variances on weights</p><formula xml:id="formula_9">α (l ) j = exp(−γ σ (l ) j ),<label>(6)</label></formula><p>where α (l ) j are the attention weights of node v j in the layer l and γ is a hyper-parameter. Then, we modify Eq. ( <ref type="formula">5</ref>) as follows:</p><formula xml:id="formula_10">h (l ) ne(i) = j ∈ne(i) 1 Di,i Dj, j (h (l ) j ⊙ α (l ) j ) ∼ N j ∈ne(i) µ (l ) j ⊙ α (l ) j Di,i Dj, j , diaд j ∈ne(i) σ (l ) j ⊙ α (l ) j ⊙ α (l ) j Di,i Dj, j ,<label>(7)</label></formula><p>where ⊙ is the element-wise product. Note that the attention weights are exerted for different dimensions separately. Following the "message-passing" framework, next we need to apply learnable filters and non-linear activation functions to h</p><formula xml:id="formula_11">(l ) ne(i) to get h (l +1) i</formula><p>. However, this is mathematically intractable since h (l ) ne(i) is a Gaussian distribution. Alternatively, we directly impose layer-specific parameters and non-linear activation functions to the 2 We make this assumption to make the computations tractable. means and variances respectively. Formally, we define Gaussianbased graph convolution as follows:</p><formula xml:id="formula_12">µ (l +1) i = ρ j ∈ne(i) 1 Di,i Dj, j µ (l ) j ⊙ α (l ) j W (l ) µ σ (l +1) i = ρ j ∈ne(i) 1 Di,i Dj, j σ (l ) j ⊙ α (l ) j ⊙ α (l ) j W (l ) σ ,<label>(8)</label></formula><p>or equivalently in the matrix form:</p><formula xml:id="formula_13">M (l +1) = ρ D− 1 2 Ã D− 1 2 M (l ) ⊙ A (l ) W (l ) µ Σ (l +1) = ρ D−1 Ã D−1 Σ (l ) ⊙ A (l ) ⊙ A (l ) W (l ) σ ,<label>(9)</label></formula><p>where W µ , W σ are parameters for the means and the variances respectively and</p><formula xml:id="formula_14">A (l ) = exp(−γ Σ (l ) ).</formula><p>For the first layer, since input features are vectors rather than Gaussian distributions, we adopt a fully connected layer as follows:</p><formula xml:id="formula_15">M (1) = ρ H (0) W (0) µ , Σ (1) = ρ H (0) W (0) σ . (<label>10</label></formula><formula xml:id="formula_16">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss Functions</head><p>Next, we introduce the loss functions of our proposed method. In this paper, we focus on the task of node classification. Considering that the hidden representations of our method are Gaussian distributions, we first adopt a sampling process in the last hidden layer</p><formula xml:id="formula_17">z i ∼ N µ (L) i , diaд σ (L) i ,<label>(11)</label></formula><p>i.e. z i is sampled from h (L) i . Next, z i is passed to a softmax function to get the predicted labels:</p><formula xml:id="formula_18">Ŷ = so f tmax(Z), Z = [z 1 , .., z N ] .<label>(12)</label></formula><p>Then, we can use the same cross entropy loss L cls defined in Eq. (3) as the objective function for the task of node classification. Moreover, we can use the "reparameterization trick" <ref type="bibr" target="#b8">[9]</ref> to optimize this loss function. Mathematically, we first sample ϵ ∼ N (0, I) and</p><formula xml:id="formula_19">then compute z i = µ (L) i + ϵ ⊙ σ (L) i . Given a fixed µ (L) i , σ<label>(L)</label></formula><p>i and ϵ, the loss function is deterministic and continuous with respect to model parameters.</p><p>In addition, to ensure that the learned representations are indeed Gaussian distributions, we use an explicit regularization to constrain the latent representations in the first layer as follows:</p><formula xml:id="formula_20">L r eд1 = N i=1 KL N (µ (1) i , diaд(σ (1) i ))||N (0, I) ,<label>(13)</label></formula><p>where KL(•||•) is the KL-divergence between two distributions <ref type="bibr" target="#b18">[19]</ref>.</p><p>Note that we only need to regularize H (1) as deeper layers are naturally Gaussian distributions by using our Gaussian-based Graph Convolutions.</p><p>Following the original GCN model <ref type="bibr" target="#b17">[18]</ref>, we also impose L 2 regularization on parameters of the first layer as follows:</p><formula xml:id="formula_21">L r eд2 = W (0) µ 2 2 + W (0) σ 2 2 . (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>Algorithm 1 Robust Graph Convolutional Network (RGCN) Calculate M (1) , Σ (1) using Eq. <ref type="bibr" target="#b9">(10)</ref> 4:</p><formula xml:id="formula_23">Input: Graph G = (V, E), Feature Matrix X, Number of Layers L, Dimensionalities { f 1 , ..., f L }, Hyper-parameters {γ , β 1 , β 2 }, Task Specific Loss L cl s Output: Latent representations {z i } N i=1 and RGCN parameters Θ = {W (i) µ , W (i) σ } L−1</formula><p>for l ← 2 to L do 5:</p><p>Calculate M (l ) , Σ (l ) using Eq. ( <ref type="formula" target="#formula_13">9</ref>) 6: end for 7:</p><p>Sample Z from Gaussian distributions H (L) using Eq. ( <ref type="formula" target="#formula_17">11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Calculate L using Eq. ( <ref type="formula" target="#formula_24">15</ref>) 9:</p><p>Update Θ using back-propagation 10: end while Finally, we jointly minimize the loss function by combining the above terms:</p><formula xml:id="formula_24">L = L cl s + β 1 L r eд1 + β 2 L r eд2 ,<label>(15)</label></formula><p>where β 1 and β 2 are hyper-parameters that control the impact of different regularizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Optimization and Complexity Analysis</head><p>We show the pseudo-code of RGCN in Algorithm 1. By using the "reparameterization trick" introduced in the last subsection, our model can be trained end-to-end using back-propagation, and thus we can use gradient descent to optimize the model. The time complexity of our method is O M L i=0 f l + N L i=1 f i−1 f i where N = |V| is the number of nodes, M = |E| is the number of edges and f l is the dimensionality of the l t h hidden layer, i.e. our method is linear with respect to the number of nodes and number of edges in the graph respectively, which is in the same order as other GCNs. Note that since our method also follows the "message-passing" framework, many sampling strategies for GCNs can be directly applied, e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we empirically evaluate the effectiveness of our proposed method. We first introduce the experimental settings and then present our experimental results. Some additional experiment details can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Baselines and Adversarial Attack Methods.</head><p>To evaluate the robustness of RGCN, we compare it with two state-of-the-art GCN models:</p><p>• GCN <ref type="bibr" target="#b17">[18]</ref>: As introduced in Section 3.2 , this is the original GCN model which defines graph convolution as aggregating features from neighborhoods. • GAT <ref type="bibr" target="#b29">[30]</ref>: This model enhances GCN by introducing multihead self-attention to assign different weights to different neighbors.</p><p>We compare these two GCNs and our proposed RGCN under three attacking methods:</p><p>• Random Attack: We randomly generate fake edges and add them into the graph. We regard this method as an illustrating example of non-targeted attacks. • RL-S2V <ref type="bibr" target="#b6">[7]</ref> <ref type="foot" target="#foot_2">3</ref> : This method generates adversarial attacks on graph-structured data based on reinforcement learning. It is designed for evasion and targeted attacks and can only perform direct attacks.</p><p>• NETTACK <ref type="bibr" target="#b36">[37]</ref>: This method also generates adversarial perturbations targeting GCNs. It is designed for targeted attacks and can perform both direct and influence attacks, which we denote as NETTACK-Di and NETTACK-In respectively. We focus on the poisoning attack of NETTACK since it better matches the transductive node classification setting. For all attacking methods, we focus on changing graph structures since it is more related to the graph setting, but our method can be directly applied to changing node attributes as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Datasets.</head><p>In order to comprehensively evaluate the effectiveness of our proposed method, we adopt three citation networks commonly used in previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>: Cora, Citeseer and Pubmed <ref type="bibr" target="#b23">[24]</ref>, where nodes represent documents and edges represent citations. Nodes are also associated with bag-of-words features and corresponding ground-truth labels. The statistics of the datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We closely follow the experimental setting in previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. Specifically, we use all node features and 20 labels per class as the training set and another 500 labels for validation and early-stopping. The performance of different methods is evaluated on a separate test set of 1000 labels. We adopt the same dataset splits as in <ref type="bibr" target="#b17">[18]</ref> and report the average results of 10 runs. In experiments, we set the number of layers as two for all methods as suggested by previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. For GCN and RGCN, we set the number of hidden units as 32.</p><p>Note that our method RGCN has both mean and variance vectors. For a fair comparison, we set their length as 16 so that the total number of hidden units matches 32. For GAT, we use 8 attention heads with each head containing 8 features, i.e. a total of 64 hidden units as suggested by the authors.</p><p>For RGCN, the hyper-parameters are set as follows: γ = 1, β 1 = 5 • 10 −4 , β 2 = 5 • 10 −4 on all datasets. We set the dropout rate for RGCN as 0.6 and use Xavier initialization <ref type="bibr" target="#b10">[11]</ref> for all weight matrices. The non-linear activation is ELU (•) <ref type="bibr" target="#b5">[6]</ref> and ReLU (•) <ref type="bibr" target="#b20">[21]</ref> for means and variances respectively since variances are required to be non-negative while means can take negative values. For the optimization, we use Adam <ref type="bibr" target="#b15">[16]</ref> with a fixed learning rate of 0.01 and set epoch number as 200 with early stopping on the validation set. For GCN and GAT, we use the default optimization settings in the authors implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Node Classification on Clean Datasets</head><p>To build a reference line, we first conduct experiments on the clean datasets, i.e. datasets that are not attacked. The experimental results are shown in Table <ref type="table" target="#tab_1">2</ref>. We can see that our proposed method RGCN slightly outperforms the baseline methods on Pubmed, while having comparable performance on Cora and Citeseer. The strong performance of RGCN on clean datasets shows that our proposed Gaussian-based graph convolution is as effective as traditional graph convolutions in capturing the graph structure, which lays the foundation for applying it to the adversarial settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cora</head><p>Citeseer Pubmed GCN 81.5 ± 0.5 70.9 ± 0.5 79.0 ± 0.3 GAT 83.0 ± 0.7 72.5 ± 0.7 79.0 ± 0.3 RGCN 82.8 ± 0.6 71.2 ± 0.5 79.1 ± 0.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Against Non-targeted Adversarial Attacks</head><p>In this section, we first evaluate the classification accuracy of different methods against non-targeted adversarial attacks, i.e. perturbations that aim to reduce the overall classification accuracy of the model. To the best of our knowledge, there is no publicly available non-targeted attack method. Thus we use Random Attack as an illustrating example of the non-targeted attack method.</p><p>Specifically, we focus on adding edges and performing poisoning attacks, i.e. we randomly choose some node pairs without edges as noise edges to add into the graph, train different GCN methods on the modified graph and evaluate the classification accuracy on the test set. We vary the ratio of noise edges to clean edges from 0.1 to 1 and report the experimental results in Figure <ref type="figure">2</ref>.</p><p>The figure shows that RGCN consistently outperforms both baselines on all datasets, demonstrating that we can improve the robustness of GCNs under non-targeted attacks. By using Gaussian distributions as hidden representations and assigning variancebased attention weights to neighborhoods, RGCN can absorb the effects of noise edges and is thus less sensitive to such adversarial information. On the other hand, although GAT also has attention mechanisms and achieves the best results on the clean Cora and Citeseer dataset, the performance of GAT drops rapidly, indicating that GAT is very sensitive to adversarial attacks and its attention mechanism cannot adapt to prevent the adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Against Targeted Adversarial Attacks</head><p>In this section, we continue to evaluate the node classification accuracy of different methods against targeted adversarial attacks. Specifically, we first use adversarial attack methods to generate different numbers of perturbations for the targeted nodes, where one perturbation is defined as adding or deleting one edge in the graph. Then, we either retrain the GCN model for poisoning attacks (i.e. when use NETTACK) or keep the GCN model unchanged for evasion attacks (i.e. when use RL-S2V). Finally, we test the performance of different GCNs on the targeted nodes, i.e. whether we successfully defend the attacks. Considering that adversarial attacks are designed to be unnoticeable and high-degree nodes usually have richer values, we set the targeted nodes as all nodes in the test set with degree larger than 10. For Cora and Citeseer, we repeat the above process for all targeted nodes. For Pubmed, since running the adversarial methods for all targeted nodes is very time consuming, we randomly sample 10% of them.</p><p>Next, we show the results when adopting different attack methods in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">RL-S2V.</head><p>In this subsection, we report the results when adopting RL-S2V as the attack method. Recall that RL-S2V is an evasion attack and can only perform direct attacks. We use the default parameter settings of RL-S2V in the authors implementation. The experimental results are shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>The results show that RGCN again consistently achieves better results compared to all the baselines, demonstrating the robustness of RGCN under targeted attacks. Note that RGCN does not utilize any information of the attack method, including which nodes are the targets. So rather than specifically "defending" certain attacks, the architecture of RGCN actually improves the overall robustness of the model and is general to any adversarial attack strategy, which is desirable in practice since we may not know the attack method or which nodes are the targets in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">NETTACK.</head><p>In this section, we adopt NETTACK as the attack method and keep all the default parameter settings in the authors implementation. Since NETTACK can handle both direct and influence attacks, we report the results of NETTACK-Di and NETTACK-In in Figure <ref type="figure" target="#fig_3">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref> respectively. In Figure <ref type="figure" target="#fig_3">4</ref>, we focus on the first 5 perturbations because more perturbations will lead to too low performance for all methods which is unbearable in practice.</p><p>From Figure <ref type="figure" target="#fig_3">4</ref>, we can see that the performance of all methods decays rapidly with respect to the number of perturbations, demonstrating that NETTACK-Di is a very strong attack method. Despite that, we can see that RGCN is still consistently more robust than both baselines on all datasets. On the other hand, for influence attacks shown in Figure <ref type="figure" target="#fig_4">5</ref>, all methods have relative higher accuracy, proving that influence attacks are usually less effective than direct attacks, Nevertheless, our method still outperforms all baselines.</p><p>The experimental results show that no matter how strong the attacks are, RGCN consistently outperforms the baselines, demonstrating that our proposed architecture can prevent GCNs from various targeted attack strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Parameter Analysis</head><p>In this section, we conduct some parameter analysis to further investigate the reasons behind the robustness of RGCN.   5.5.1 Analysis of Variances. While designing the model, we claim that using Gaussian distributions as hidden layers can absorb the effects of adversarial attacks in the variances to enhance robustness.</p><p>To verify that, we analyze how the variances change when the graph is attacked. Specifically, we follow the experimental setting in Section 5.4 and plot the variances of the attacked nodes with respect to the number of perturbations. For brevity, we only report the variances of the first layer, i.e. σ (1) , when using NETTACK-Di as the attack model, but similar patterns are observed in other cases.</p><p>From Figure <ref type="figure" target="#fig_5">6a</ref>, we can see that with the number of perturbations increasing, the variances also increase significantly, which is in accordance with our intuition. By absorbing the effects of adversarial attacks in variances, RGCN is more robust than only using plain vectors. This also lays the foundation for using variance-based attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Analysis of Imposing</head><p>Variance-based Attention Weights. Moreover, we also conduct some experiments to analyze whether imposing variance-based attention weights in our model can help remedy the propagation of adversarial attacks. Specifically, we follow the experimental setting of non-targeted attacks in Section 5.3 and vary γ , the hyper-parameter in setting the attention weights. For the ease of presentation, we only show the results on Cora when setting  the ratio of noise edges between 0 and 0.5, while other datasets show consistent results. Figure <ref type="figure" target="#fig_5">6b</ref> shows that imposing variance-based attention weights (γ &gt; 0) outperforms not using attention (γ = 0), demonstrating that such attention mechanism can indeed improve the robustness of RGCN. Moreover, we can observe that the margin becomes larger as the ratio of noise edges increases, indicating the attention mechanism is more effective when there are more noise edges. Moreover, setting γ too large will degrade the performance, probably because the "message-passing" in real edges is also blocked. We find that uniformly setting γ = 1 works well in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Analysis of Regularizations.</head><p>In this section, we investigate the effectiveness of two regularization terms. More specifically, we evaluate how different values of hyper-parameter β 1 and β 2 affect our method. We report the results of node classification on clean dataset Cora, while other experiments exhibit similar patterns.</p><p>Figure <ref type="figure" target="#fig_5">6c</ref> and Figure <ref type="figure" target="#fig_5">6d</ref> show that choosing an appropriate value for both β 1 and β 2 can increase the accuracy of RGCN, which is in line with our expectations of regularization constraints. However, setting β 1 or β 2 too large will also hurt the performance. In our experiments, setting β 1 = β 2 = 5 • 10 −4 on all datasets lead to satisfying results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we propose RGCN, a novel graph convolution model to explicitly enhance the robustness of GCNs against adversarial attacks. By using Gaussian distributions in hidden layers and introducing variance-based attention weights in aggregating node neighborhoods, our proposed method can effectively reduce the impacts of adversarial attacks. Experimental results demonstrate that our proposed method can consistently improve the robustness of GCNs under various adversarial attack strategies. To the best of our knowledge, this is the first study on this critical and challenging problem. Future directions include conducting more experiments besides citation networks. It is also interesting to extend this framework to other deep learning models on graphs and more complicated graph structures such as heterogeneous graphs or graphs with edge attributes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of our proposed RGCN. GGCL represents Gaussian-based Graph Convolution Layer introduced in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>i=0 1 :</head><label>1</label><figDesc>Initialize all parameters Θ 2: while L does not converge do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of different methods when adopting RL-S2V as the attack method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of different methods when adopting NETTACK-Di as the attack method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of different methods when adopting NETTACK-In as the attack method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of parameter analysis: (a) the average variance of targeted nodes w.r.t the number of perturbations; (b) the parameter sensitivity with respect to γ ; (c) the parameter sensitivity with respect to β 1 ; (d) the parameter sensitivity with respect to β 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets. |V | denotes the number of nodes, |E| denotes the number of edges, |C | denotes the number of classes and |F | denotes the number of features.</figDesc><table><row><cell></cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell cols="2">|V | 2,708</cell><cell>3,327</cell><cell>19,717</cell></row><row><cell cols="2">|E| 5,429</cell><cell>4,732</cell><cell>44,338</cell></row><row><cell>|C |</cell><cell>7</cell><cell>6</cell><cell>3</cell></row><row><cell cols="2">|F | 1,433</cell><cell>3,703</cell><cell>500</cell></row><row><cell cols="2">5.1.3 Parameter Settings.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The results of node classification accuracy (in percentages) on clean datasets.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this paper, we focus on diagonal variance matrices, but it can be extended to more general cases<ref type="bibr" target="#b21">[22]</ref>. We also use σ rather than σ</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">to represent variances for the ease of presentation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The original paper proposes three different attack methods under different settings. We choose the most effective method, RL-S2V, in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by Beijing Academy of Artificial Intelligence (BAAI), National Program on Key Basic Research Project (No. 2015CB352300), National Natural Science Foundation of China Major Project (No.U1611461), National Natural Science Foundation of China (No. 61772304, No. 61521002, No. 61531006), the research fund of Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology, and the Young Elite Scientist Sponsorship Program by CAST. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL EXPERIMENT DETAILS A.1 Hardware and Software Configurations</head><p>All experiments are conducted on a server with the following configurations:</p><p>• Operating System: Ubuntu 18.04.1 LTS • CPU: Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz</p><p>• GPU: GeForce GTX TITAN X • Software: Python 3.6.7, TensorFlow 1.12.0, SciPy 1.1.0, NumPy 1.15.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Baselines and Adversarial Attack Methods</head><p>We use the following publicly available implementation of baseline methods and adversarial attack methods:</p><p>• GCN: https://github.com/tkipf/gcn • GAT: https://github.com/PetarV-/GAT • RL-S2V: https://github.com/Hanjun-Dai/graph_adversarial_attack • NETTACK: https://github.com/danielzuegner/nettack</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Dataset Splits</head><p>We use the following publicly available dataset splits for all three datasets: https://github.com/tkipf/gcn/tree/master/gcn/data</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
				<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FastGCN: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic Training of Graph Convolutional Networks with Variance Reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial Attack on Graph Structured Data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>David K Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive Sampling Towards Fast Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4563" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
				<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951. 1951</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Disentangled Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ICML-10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
				<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Vladimirovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petrov</forename></persName>
		</author>
		<title level="m">Sums of independent random variables</title>
				<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10528</idno>
		<title level="m">Adversarial Attack and Defense on Graph Data: A Survey</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Attention-based graph neural network for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Kiran K Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03735</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AutoNE: Hyperparameter Optimization for Massive Network Embedding</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07293</idno>
		<title level="m">Heterogeneous Graph Attention Network</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs</title>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<title level="m">Deep Learning on Graphs: A Survey</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep variational network embedding in wasserstein space</title>
		<author>
			<persName><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarial Attacks on Graph Neural Networks via Meta Learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
