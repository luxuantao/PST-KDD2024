<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CardioGAN: Attentive Generative Adversarial Network with Dual Discriminators for Synthesis of ECG from PPG</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pritam</forename><surname>Sarkar</surname></persName>
							<email>pritam.sarkar@queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Dept. of ECE &amp; Ingenuity Labs Research Institute Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Etemad</surname></persName>
							<email>ali.etemad@queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Dept. of ECE &amp; Ingenuity Labs Research Institute Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CardioGAN: Attentive Generative Adversarial Network with Dual Discriminators for Synthesis of ECG from PPG</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Electrocardiogram (ECG) is the electrical measurement of cardiac activity, whereas Photoplethysmogram (PPG) is the optical measurement of volumetric changes in blood circulation. While both signals are used for heart rate monitoring, from a medical perspective, ECG is more useful as it carries additional cardiac information. Despite many attempts toward incorporating ECG sensing in smartwatches or similar wearable devices for continuous and reliable cardiac monitoring, PPG sensors are the main feasible sensing solution available. In order to tackle this problem, we propose Car-dioGAN, an adversarial model which takes PPG as input and generates ECG as output. The proposed network utilizes an attention-based generator to learn local salient features, as well as dual discriminators to preserve the integrity of generated data in both time and frequency domains. Our experiments show that the ECG generated by CardioGAN provides more reliable heart rate measurements compared to the original input PPG, reducing the error from 9.74 beats per minute (measured from the PPG) to 2.89 (measured from the generated ECG).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>According to the World Health Organization (WHO) in 2017, Cardiovascular Deceases (CVDs) are reported as the leading causes of death worldwide <ref type="bibr">(WHO 2017)</ref>. The report indicates that CVDs cause 31% of global deaths, out of which at least three-quarters of deaths occur in the low or medium-income countries. One of the primary reasons behind this is the lack of primary healthcare support and the inaccessible on-demand health monitoring infrastructure. Electrocardiogram (ECG) is considered as one of the most important attributes for continuous health monitoring required for identifying those who are at serious risk of future cardiovascular events or death. Vast amount of research is being conducted with the goal of developing wearable devices capable of continuous ECG monitoring and feasible for daily life use, largely to no avail. Currently, very few wearable devices provide wrist-based ECG monitoring, and those who do, require the user to stand still and touch the watch with both hands in order to close the circuit in order Copyright c 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>to record an ECG segment of limited duration (usually 30 seconds), making these solutions non-continuous and sporadic.</p><p>Photoplethysmogram (PPG), an optical method for measuring blood volume changes at the surface of the skin, is considered as a close alternative to ECG, which contains valuable cardiovascular information <ref type="bibr" target="#b7">(Gil et al. 2010;</ref><ref type="bibr" target="#b27">Schäfer and Vagedes 2013)</ref>. For instance, studies have shown that a number of features extracted from PPG (e.g., pulse rate variability) are highly correlated with corresponding metrics extracted from ECG (e.g., heart rate variability) <ref type="bibr" target="#b7">(Gil et al. 2010)</ref>, further illustrating the mutual information between these two modalities. Yet, through recent advancements in smartwatches, smartphones, and other similar wearable and mobile devices, PPG has become the industry standard as a simple, wearable-friendly, and low-cost solution for continuous heart rate (HR) monitoring for everyday use. Nonetheless, PPG suffers from inaccurate HR estimation and several other limitations in comparison to conventional ECG monitoring devices <ref type="bibr" target="#b2">(Bent et al. 2020</ref>) due to factors like skin tone, diverse skin types, motion artefacts, and signal crossovers among others. Moreover, the ECG waveform carries important information about cardiac activity. For instance, the Pwave indicates the sinus rhythm, whereas a long PR interval is generally indicative of a first-degree heart blockage <ref type="bibr" target="#b1">(Ashley and Niebauer 2004)</ref>. As a result, ECG is consistently being used by cardiologists for assessing the condition and performance of the heart.</p><p>Based on the above, there is a clear discrepancy between the need for continuous wearable ECG monitoring and the available solutions in the market. To address this, we propose CardioGAN, a generative adversarial network (GAN) <ref type="bibr" target="#b10">(Goodfellow et al. 2014)</ref>, which takes PPG as inputs and generates ECG. Our model is based on the CycleGAN architecture <ref type="bibr" target="#b31">(Zhu et al. 2017</ref>) which enables the system to be trained in an unpaired manner. Unlike CycleGAN, Car-dioGAN is designed with attention-based generators and equipped with multiple discriminators. We utilize attention mechanisms in the generators to better learn to focus on specific local regions such as the QRS complexes of ECG. To generate high fidelity ECG signals in terms of both time and frequency information, we utilize a dual discriminator strategy where one discriminator operates on signals in the time domain while the other uses frequency-domain spectrograms of the signals. We show that the generated ECG outputs are very similar to the corresponding real ECG signals. Finally, we perform HR estimation using our generated ECG as well as the input PPG signals. By comparing these values to the HR measured from the ground-truth ECG signals, we observe a clear advantage in our proposed method. While to demonstrate the efficacy of our solution we focus on single-lead ECG, we believe our approach can be used for multi-lead ECG through training the system on other desired leads. Our contributions in this paper are summarised below:</p><p>• We propose a novel framework called CardioGAN for generating ECG signals from PPG inputs. We utilize attention-based generators and dual time and frequency domain discriminators along with a CycleGAN backbone to obtain realistic ECG signals. To the best of our knowledge, no other studies have attempted to generate ECG from PPG (or in fact any cross-modality signal-to-signal translation in the biosignal domain) using GANs or other deep learning techniques.</p><p>• We perform a multi-corpus subject-independent study, which proves the generalizability of our model to data from unseen subjects and acquired in different conditions.</p><p>• The generated ECG obtained from the CardioGAN provides more accurate HR estimation compared to HR values calculated from the original PPG, demonstrating some of the benefits of our model in the healthcare domain. We make the final trained model publicly available<ref type="foot" target="#foot_0">1</ref> .</p><p>The rest of this paper is organized as follows. Section 2 briefly mentions the prior studies on ECG signal generation. Next, our proposed method is discussed in Section 3. Section 4 discusses the details of our experiments, including datasets and training procedures. Finally, the results and analyses are presented in Section 5, followed by a summary of our work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generating synthetic ECG Signal</head><p>The idea of synthesizing ECG has been explored in the past, utilizing both model-driven (e.g. signal processing or mathematical modelling) and data-driven (machine learning and deep learning) techniques. As examples of earlier works, <ref type="bibr" target="#b14">(McSharry et al. 2003;</ref><ref type="bibr" target="#b25">Sayadi, Shamsollahi, and Clifford 2010)</ref> proposed solutions based on differential equations and Gaussian models for generating ECG segments.</p><p>Despite deep learning being employed to process ECG for a wide variety of different applications, for instance biometrics <ref type="bibr" target="#b30">(Zhang, Zhou, and Zeng 2017)</ref>, arrhythmia detection <ref type="bibr" target="#b11">(Hannun et al. 2019)</ref>, emotion recognition <ref type="bibr">(Sarkar and Etemad 2020a,b)</ref>, cognitive load analysis <ref type="bibr" target="#b24">(Sarkar et al. 2019;</ref><ref type="bibr" target="#b21">Ross et al. 2019)</ref>, and others, very few studies have tackled synthesis of ECG signals with deep neural networks <ref type="bibr" target="#b31">(Zhu et al. 2019a;</ref><ref type="bibr" target="#b9">Golany and Radinsky 2019;</ref><ref type="bibr" target="#b8">Golany et al. 2020</ref>). Synthesizing ECG with GANs was first studied in <ref type="bibr" target="#b31">(Zhu et al. 2019a)</ref>, where a bidirectional LSTM-CNN architecture was proposed to generate ECG from Gaussian noise. The study performed by <ref type="bibr" target="#b9">(Golany and Radinsky 2019)</ref>, proposed PGAN or Personalized GAN to generate patient-specific synthetic ECG signals from input noise. A special loss function was proposed to mimic the morphology of ECG waveforms, which was a combination of cross-entropy loss and mean squared error between real and fake ECG waveforms.</p><p>A few other studies have targeted this area, for example, EmotionalGAN was proposed in <ref type="bibr" target="#b3">(Chen et al. 2019)</ref>, where synthetic ECG was used to augment the available ECG data in order to improve emotion classification accuracy. The proposed GAN generated the new ECG based on input noise. Lastly, in a similar study performed by <ref type="bibr" target="#b8">(Golany et al. 2020)</ref>, ECG was generated from input noise to augment the available ECG training set, improving the performance for arrhythmia detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ECG Synthesis from PPG</head><p>With respect to the very specific problem of PPG-to-ECG translation, to the best of our knowledge, only <ref type="bibr" target="#b32">(Zhu et al. 2019b</ref>) has been published. This work did not use deep learning, instead used discrete cosine transformation (DCT) technique to map each PPG cycle to its corresponding ECG cycle. First, onsets of the PPG signals were aligned to the R-peaks of the ECG signals, followed by a de-trending operation in order to reduce noise. Next, each cycle of ECG and PPG was segmented, followed by temporal scaling using linear interpolation in order to maintain a fixed segment length. Finally, a linear regression model was trained to learn the relation between DCT coefficients of PPG segments and corresponding ECG segments. In spite of several contributions, this study suffers from few limitations. First, the model failed to produce reliable ECG in a subjectindependent manner, which limits its application to only previously seen subject's data. Second, often the relation between PPG segments and ECG segments are not linear, therefore in several cases, this model failed to capture the non-linear relationships between these 2 domains. Lastly, no experiments have been performed to indicate any performance enhancement gained from using the generated ECG as opposed to the available PPG (for example a comparison of measured HR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Objective and Proposed Architecture</head><p>In order to not be constrained by paired training where both types of data are needed from the same instance in order to train the system, we are interested in an unpaired GAN, i.e. CycleGAN-based architectures. We propose Car-dioGAN whose main objective is to learn to estimate the mapping between PPG (P ) and ECG (E) domains. In order to force the generator to focus on regions of the data with significant importance, we incorporate an attention mechanism into the generator. We implement generator G E : P → E to learn forward mapping, and G P : learn the inverse mapping. We denote generated ECG and generated PPG from CardioGAN as E and P respectively, where E = G E (P ) and P = G P (E). According to <ref type="bibr" target="#b18">(Penttilä et al. 2001</ref>) and a large number of other studies, cardiac activity is manifested in both time and frequency domains. Therefore, in order to preserve the integrity of the generated ECG in both domains, we propose the use of a dual discriminator strategy, where D t is employed to classify time domain and D f is used to classify the frequency domain response of real and generated data.</p><formula xml:id="formula_0">E → P to D f E D t E P E' f(E') D t P D f P P'' f(P) R/F R/F GE GP Input (A) R/F R/F Output D f P D t P E P' f(P') D t E D f E E'' f(E) R/F R/F GP GE (B) R/F R/F Input Output</formula><p>Figure <ref type="figure" target="#fig_0">1</ref> shows our proposed architecture, where G E takes P as an input and generates E as the output. Similarly, E is given as an input to G P where P is generated as the output. We employ D t E and D t P to discriminate E versus E , and P versus P , respectively. Similarly, D f E and D f P are developed to discriminate f (E) versus f (E ), as well as f (P ) versus f (P ), respectively, where f denotes the spectrogram of the input signal. Finally, E and P are given as inputs to G P and G E respectively, in order to complete the cyclic training process.</p><p>In the following subsections, we expand on the dual discriminator, the notion of integrating an attention mechanism into the generator, and the loss functions used to train the overall architecture. The details and architectures of each of the networks used in our proposed solution are provided in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual Discriminators</head><p>As mentioned above, to preserve both time and frequency information in the generated ECG, we use a dual discriminator approach. Dual discriminators have been used earlier in <ref type="bibr" target="#b15">(Nguyen et al. 2017)</ref>, showing improvements in dealing with mode collapse problems. To leverage the concept of dual discriminators, we perform Short-Time Fourier Transformation (STFT) on the ECG/PPG time series data. Let's denote x[n] as a time-series, then ST F T (x[n]) can be denoted as  <ref type="figure" target="#fig_0">1</ref> the time-domain and frequency-domain discriminators operate in parallel, and as we will discuss in Section 3.4, to aggregate the outcomes of these two networks, the loss terms of both of these networks are incorporated into the adversarial loss.</p><formula xml:id="formula_1">X(m, ω) = ∞ n=−∞ x[n]w[n − m]e −jωn</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention-based Generators</head><p>We adopt Attention U-Net as our generator architecture, which has been recently proposed and used for image classification <ref type="bibr" target="#b17">(Oktay et al. 2018;</ref><ref type="bibr" target="#b12">Jetley et al. 2018</ref>). We chose attention-based generators to learn to better focus on salient features passing through the skip connections. Let's assume x l are features obtained from the skip connection originating from layer l, and g is the gating vector that determines the region of focus. First, x l and g are mapped to an intermediatedimensional space R Fint where F int corresponds to the dimensions of the intermediate-dimensional space. Our objective is to determine the scalar attention values (α l i ) for each temporal unit x l i ∈ R F l , utilizing gating vector g i ∈ R Fg , where F l and F g are the number of feature maps in x l and g respectively. Linear transformations are performed on x l and g as θ <ref type="figure">and b</ref> x , b g refer to the bias terms. Next, non-linear activation function ReLu (denoted by σ 1 ) is applied to obtain the sum feature activation f = σ 1 (θ x + θ g ), where σ 1 (y) is formulated as max(0, y). Next we perform a linear mapping of f onto the R Fint dimensional space by performing channel-wise 1 × 1 convolutions, followed by passing through a sigmoid activation function (σ 2 ) in order to obtain the attention weights in the range of [0, 1]. The attention map corresponding to x l is obtained by α l i = σ 2 (ψ * f ) where σ 2 (y) can be formulated as 1 1+exp −y , ψ ∈ R Fint and * denotes convolution. Next, we perform element-wise multiplication between x l i and α l i to obtain the final output from the attention layer.</p><formula xml:id="formula_2">x = W x x l i + b x and θ g = W g g i + b g respec- tively, where W x ∈ R F l ×Fint , W g ∈ R Fg×Fint ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss</head><p>Our final objective function is a combination of an adversarial loss and a cyclic consistency loss as presented below.</p><p>Adversarial Loss We apply adversarial loss in both forward and inverse mappings. Let's denote individual PPG segments as p and the corresponding ground-truth ECG segments as e. For the mapping function G E : P → E, and discriminators D t E and D f E , the adversarial losses are defined as:</p><formula xml:id="formula_3">L adv (G E , D t E ) = E e∼E [log (D t E (e))] + E p∼P [log (1 − D t E (G E (p)))]</formula><p>(1)</p><formula xml:id="formula_4">L adv (G E , D f E ) = E e∼E [log (D f E (f (e)))] + E p∼P [log (1 − D f E (f (G E (p))))]<label>(2)</label></formula><p>Similarly, for the inverse mapping function G P : E → P , and discriminators D t P and D f P , the adversarial losses are defined as:</p><formula xml:id="formula_5">L adv (G P , D t P ) = E p∼P [log (D t P (p))] + E e∼E [log (1 − D t P (G P (e)))]</formula><p>(3)</p><formula xml:id="formula_6">L adv (G P , D f P ) = E p∼P [log (D f P (f (p)))] + E e∼E [log (1 − D f P (f (G P (e))))]<label>(4)</label></formula><p>Finally, the adversarial objective function for the mapping</p><formula xml:id="formula_7">G E : P → E is obtained as min G E max D t E L adv (G E , D t E ) and min G E max D f E L adv (G E , D f E ).</formula><p>Similarly, for the mapping G P : E → P , can be calculated as min G P max D t P L adv (G P , D t P ) and min G P max D f P L adv (G P , D f P ). Cyclic Consistency Loss The other component of our objective function is the cyclic consistency loss or reconstruction loss as proposed by <ref type="bibr" target="#b31">(Zhu et al. 2017)</ref>. In order to ensure that forward mappings and inverse mappings are consistent, i.e., p → G E (p) → G P (G E (p)) ≈ p, as well as e → G P (e) → G E (G P (e)) ≈ e, we minimize the cycle consistency loss calculated as:</p><formula xml:id="formula_8">L cyclic (G E , G P ) = E e∼E [||G E (G P (e)) − e|| 1 ] + E p∼P [||G P (G E (p)) − p|| 1 ]<label>(5)</label></formula><p>Final Loss The final objective function of CardioGAN is computed as:</p><formula xml:id="formula_9">L CardioGAN = αL adv (G E , D t E ) + αL adv (G P , D t P ) + βL adv (G E , D f E ) + βL adv (G P , D f P ) + λL cyclic (G E , G P ),<label>(6)</label></formula><p>where α and β are adversarial loss coefficients corresponding to D t and D f respectively, and λ is the cyclic consistency loss coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the datasets used in this study, followed by the description of the data preparation steps. Next, we present our implementation and architecture details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use 4 very popular ECG-PPG datasets, namely BIDMC <ref type="bibr" target="#b19">(Pimentel et al. 2016)</ref>, CAPNO <ref type="bibr" target="#b13">(Karlen et al. 2013)</ref>, DALIA <ref type="bibr" target="#b20">(Reiss et al. 2019)</ref>, and WESAD <ref type="bibr" target="#b28">(Schmidt et al. 2018)</ref>. We combine these 4 datasets in order to enable a multi-corpus approach leveraging large and diverse distributions of data for different factors such as activity (e.g. working, driving, walking, resting), age (e.g. 29 children, 96 adults), and others. The aggregate dataset contains a total of 125 participants with a balanced male-female ratio.</p><p>BIDMC <ref type="bibr" target="#b19">(Pimentel et al. 2016</ref>) was obtained from 53 adult ICU patients (32 females, 21 males, mean age of 64.81) where each recording was 8 minutes long. PPG and ECG were both sampled at a frequency of 125 Hz. It should be noted this dataset consists of three leads of ECG (II, V, AVR). However, we only use lead II in this study.</p><p>CAPNO <ref type="bibr" target="#b13">(Karlen et al. 2013</ref>) consists of data from 42 participants, out of which 29 were children (median age of 8.7) and 13 were adults (median age of 52.4). The recordings were collected while the participants were under medical observation. Single-lead ECG and PPG recordings were sampled at a frequency of 300 Hz and were 8 minutes in length.</p><p>DALIA <ref type="bibr" target="#b20">(Reiss et al. 2019)</ref> was recorded from 15 participants (8 females, 7 males, mean age of 30.60), where each recording was approximately 2 hours long. ECG and PPG signals were recorded while participants went through different daily life activities, for instance sitting, walking, driving, cycling, working and so on. Single-lead ECG signals were recorded at a sampling frequency of 700 Hz while the PPG signals were recorded at a sampling rate of 64 Hz.</p><p>WESAD <ref type="bibr" target="#b28">(Schmidt et al. 2018)</ref> was created using data from 15 participants (12 male, 3 female, mean age of 27.5), while performing activities such as solving arithmetic tasks, watching video clips, and others. Each recording was over 1 hour in duration. Single-lead ECG was recorded at a sampling rate of 700 Hz while PPG was recorded at a sampling rate of 64 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Preparation</head><p>Since the above-mentioned datasets have been collected at different sampling frequencies, as a first step we re-sampled (using interpolation) both the ECG and PPG signals with a sampling rate of 128 Hz. As the raw physiological signals contain a varying amounts and types of noise (e.g. power line interference, baseline wandering, motion artefacts), we perform very common filtering techniques on both the ECG and PPG signals. We apply a band-pass FIR filter with a pass-band frequency of 3 Hz and stop-band frequency of 45 Hz on the ECG signals. Similarly, a band-pass Butterworth filter with a pass-band frequency of 1 Hz and a stopband frequency of 8 Hz is applied on the PPG signals. Next, person-specific z-score normalization is performed on both ECG and PPG. Then, the normalized ECG and PPG signals are segmented into 4-second windows (128 Hz ×4 seconds = 512 samples), with a 10% overlap to avoid missing any peaks. Finally, we perform min-max [−1, 1] normalization on both ECG and PPG segments to ensure all the input data are in a specific range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Architecture</head><p>Generator As mentioned earlier an Attention U-Net architecture is used as our generator, where self-gated softattention units are used to filter the features passing through the skip connections. G E and G P take 1×512 data points as input. The encoder consists of 6 blocks, where the number of filters is gradually increased <ref type="bibr">(64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">512,</ref><ref type="bibr">512)</ref> with a fixed kernel size of 1 × 16 and a stride of 2. We apply layer normalization and leaky-ReLu activation after each convolution layers except the first layer, where no normalization is used. A similar architecture is used in the decoder, except de-convolutional layers with ReLu activation functions are used and the number of filters is gradually decreased in the same manner. The final output is then obtained from a de-convolutional layer with a single-channel output followed by tanh activation.</p><p>Discriminator Dual discriminators are used to classify real and fake data in time and frequency domains.  <ref type="bibr">(64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512)</ref> with a fixed kernel of 1 × 16 for D t and 7 × 7 for D f . Both networks use a stride of 2. Each convolution layer is followed by layer normalization and leaky ReLu activation, except the first layer where no normalization is used. Finally, the output is obtained from a single-channel convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>Our proposed CardioGAN network is trained from scratch on an Nvidia Titan RTX GPU, using TensorFlow 2.2. We divide the aggregated dataset into a training set and test set.</p><p>We randomly select 80% of the users from each dataset (a total of 101 participants, equivalent to 58K segments) for training, and the remaining 20% of users from each dataset (a total of 24 participants, equivalent to 15K segments) for testing. The training time was approximately 50 hours. To enable CardioGAN to be trained in an unpaired fashion, we shuffle the ECG and PPG segments from each dataset separately eliminating the couplings between ECG and PPG followed by a shuffling of the order of datasets themselves for ECG and PPG separately. We use a batch size of 128, unlike the original CycleGAN where a batch size of 1 is used. We notice performance gain with a larger batch size. Adam optimizer is used to train both the generators and discriminators. We train our model for 15 epochs, where the learning rate (1e −4 ) is kept constant for the initial 10 epochs and then linearly decayed to 0. The values of α, β, and λ are empirically set to 3, 1 and 30 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance</head><p>CardioGAN produces two main signal outputs, generated ECG (E ) and generated PPG (P ). As our goal is to generate the more important and elusive ECG, we utilize E and ignore P in the following experiments. In this section, we present the quantitative and qualitative results of our proposed CardioGAN network. Next, we perform an ablation study in order to understand the effects of the different components of the model. Further, we perform several analyses, followed by a discussion of potential applications using our proposed solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Results</head><p>Heart rate is measured as number of beats per minutes (BPM) by dividing the length of ECG or PPG segments in seconds by the average of the peak intervals multiplied by 60 (seconds). Let's define the mean absolute error (MAE) metric for the heart rate (in BPM) obtained from a given ECG or PPG signal (HR Q ) with respect to a ground-truth HR (HR GT ) as</p><formula xml:id="formula_10">M AE HR (Q) = 1 N N i=1 |HR GT i −HR Q i |,</formula><p>where N is the number of segments for which the HR measurements have been obtained. In order to investigate the merits of CardioGAN, we measure M AE HR (E ), where E is the ECG generated by CardioGAN. We compare these MAE values to M AE HR (P ) (where P denotes the available input PPG) as reported by other studies on the 4 datasets. The results are presented in  for 3 of the 4 datasets, the HR measured from the ECG generated by CardioGAN is more accurate than the HR measured from the input PPG signals. For CAPNO dataset in which our ECG shows higher error compared to other works based on PPG, the difference is quite marginal, especially in comparison to the performance gains achieved across the other datasets. Different studies in this area have used different window sizes for HR measurement which we report in Table <ref type="table" target="#tab_0">1</ref>. To evaluate the impact of our solution based on different window sizes, we measure M AE HR (E ) over different 4, 8, 16, 32, and 64 second windows and present the results in comparison to M AE HR (P ) across all the subjects available in the 4 datasets in Table <ref type="table" target="#tab_1">2</ref>. In these experiments, we utilize two popular algorithms for detecting peaks from ECG <ref type="bibr" target="#b11">(Hamilton 2002)</ref> and PPG <ref type="bibr" target="#b5">(Elgendi et al. 2013)</ref> signals. We observe a clear advantage in measuring HR from E as opposed to P . We notice a very consistent performance gain across different window sizes, which further demonstrates the stability of the results produce by CardioGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Results</head><p>In Figure <ref type="figure" target="#fig_3">2</ref>  network is able to learn to reconstruct the shape of the original ECG signals from corresponding PPG inputs. Careful observation shows that in some cases, the generated ECG signals exhibit a small time lag with respect to the original ECG signals. The root cause of this time delay is the Pulse Arrival Time (PAT), which is defined as the time taken by the PPG pulse to travel from the heart to a distal site (from where PPG is collected, for example, wrist, fingertip, ear, or others) <ref type="bibr" target="#b4">(Elgendi et al. 2019)</ref>. Nonetheless, this time-lag is consistent for all the beats across a single generated ECG signal as a simple offset, and therefore does not impact HR measurements or other cardiovascular-related metrics. This is further evidenced by the accurate HR measurements presented earlier in Tables <ref type="table" target="#tab_1">1 and 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>The proposed CardioGAN consists of attention-based generators and dual discriminators, as discussed earlier. In order to investigate the usefulness of the attention mechanisms and dual discriminators, we perform an ablation study of 2 variations of the network by removing each of these components individually. To evaluate these components, we perform the same M AE HR along with a number of other metrics to quantify the quality of ECG waveforms. We use metrics similar to those used in <ref type="bibr" target="#b31">(Zhu et al. 2019a)</ref>, which are Root Mean Squared Error (RMSE), Percentage Root Mean Squared Difference (PRD), and Fréchet Distance (FD). We briefly defined these metrics as follows: RMSE: In order to understand the stability between E and E , we calculate RM SE =</p><formula xml:id="formula_11">1 N N i=1 (E i − E i )</formula><p>2 where E i and E i refer to the i th point of E and E respectively.</p><p>PRD: To quantify the distortion between E and E , we</p><formula xml:id="formula_12">calculate P RD = N i=1 (Ei−E i ) 2 N i=1 (Ei) 2</formula><p>× 100.</p><p>FD: Fréchet distance <ref type="bibr" target="#b0">(Alt and Godau 1995)</ref> is calculated to measure the similarity between the E and E . While calculating the distance between two curves, this distance considers the location and order of the data points, hence, giving a more accurate measure of similarity between two timeseries signals. Let's assume E, a discrete signal, can be expressed as a sequence of {e 1 , e 2 , e 3 , . . . , e N }, and similarly  The results of our ablation study are presented in Table <ref type="table" target="#tab_3">3</ref>. We present the performance of different variants of Cardio-GAN for all the subjects across all 4 datasets. CardioGAN w/o DD is the variant with only the time domain discriminator and no change in the generator architecture. CardioGAN w/o attn is the variant where the generator does not contain an attention mechanism. The results presented in the table evidently show the benefit of using the proposed Car-dioGAN over it's ablation variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>Attention Map In order to better understand what has been learned through the attention mechanism in the generators, we visualize the attention maps applied to the very last skip connection of the generator (G E ). We choose the attention applied to the last skip connection since this layer is the closest to the final output and there more interpretable. For better visualization, we superimpose the attention map on top of the output of the generator as shown in Figure <ref type="figure" target="#fig_5">3</ref>. This shows that our model learns to generally focus on the PQRST complexes, which in turn helps the generator to learn the shapes of ECG waveform better as evident from qualitative and quantitative results presented earlier.    examples in Figure <ref type="figure">5</ref> where for highly noisy PPG inputs, the generated ECG samples also exhibit very low quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unpaired</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Potential Applications and Demonstration</head><p>Apart from the interest to the AI community, we believe our proposed solution has the potential to make a larger impact in the healthcare and wearable domains, notably for continuous health monitoring. Monitoring cardiac activity is an essential part of continuous health monitoring systems, which could enable early diagnosis of cardiovascular diseases, and in turn, early preventative measures that can lead to overcoming severe cardiac problems. Nonetheless, as discussed earlier, there are no suitable solutions for every-day continuous ECG monitoring. In this study we bridge this gap by utilizing PPG signals (which can be easily collected from almost every wearable devices available in the market) in our proposed CardioGAN to capture the cardiac information of users and generate accurate ECG signals. We perform a multi-corpus subject-independent study, where the subjects have gone through a wide range of activities including dailylife tasks, which assures us of the usability of our proposed solution in practical settings. Most importantly, our pro-posed solution can be integrated into an existing PPG-based wearable device to extract ECG data without any required additional hardware. To demonstrate this concept, we have implemented our model to perform in real-time and used a wrist-based wearable device to feed it with PPG data. The video 2 presented as supplementary material demonstrates CardioGAN producing realistic ECG from wearable PPG in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Future Work</head><p>In this paper, we propose CardioGAN, a solution for generating ECG signals from input PPG signals to aid with continuous and reliable cardiac monitoring. Our proposed method takes 4-second PPG segments and generates corresponding ECG segments of equal length. Self-gated soft-attention is used in the generator to learn important regions, for example the QRS complexes of ECG waveforms. Moreover, a dual discriminator strategy is used to learn the mapping in both time and frequency domains. Further, we evaluate the merits of the generated ECG by calculating HR and comparing the results to HR obtained from the real PPG. The analysis shows a clear advantage of using CardioGAN as more accurate HR values are obtained as a result of using the model.</p><p>For future work, the advantages of using the generated ECG data in other areas where the use of PPG is limited may be evaluated. These areas include identification of cardiovascular diseases, detection of abnormal heart rhythms, and others. Furthermore, generating multi-lead ECG can also be studied in order to extract more useful cardiac information often missing in single-channel ECG recordings. Finally, we hope our research can open a new path towards cross-modality signal-to-signal translation in the biosignal domain, allowing for less available physiological recording to be generated from more affordable and readily available signals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of the proposed CardioGAN is presented. The original ECG (E) and PPG (P ) signals are shown in the color 'orange'; the generated outputs (E and P ) are represented with the color 'green'; and the reconstructed or cyclic outputs (E and P ) are marked with the color 'black' for better visibility. Moreover, connections to the generators are marked with solid lines, whereas, connections to the discriminators are marked with dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, where m is the step size and w[n] denotes Hann window func-tion. Finally, the spectrogram is obtained by f (x[n]) = log(|X(m, ω)| + θ), where we use θ = 1e −10 to avoid infinite condition. As shown in Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>D t E and D t P take time-series signals of size 1 × 512 as inputs, whereas, spectrograms of size 128 × 128 are given as inputs to D f E and D f P . Both D t and D f use 4 convolution layers, where the number of filters are gradually increased</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: We present ECG samples generated by our proposed CardioGAN. We show 2 different samples from each dataset to better demonstrate the qualitative performance of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Training vs. Paired Training We further investigate the performance of CardioGAN while training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of attention maps are presented where the brighter parts indicate regions to which the generator pays more attention compared to the darker regions. We present 4 samples of generated ECG segments corresponding to different subjects.</figDesc><graphic url="image-8.png" coords="7,177.49,138.78,114.92,74.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Samples obtained from paired training of Cardio-GAN are presented.Generated ECG PPG Input (A) (B) (C) Original ECG Figure 5: Few failed ECG examples generated by Cardio-GAN are presented.</figDesc><graphic url="image-6.png" coords="7,177.08,54.09,114.92,74.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Table1where we observe that We compare the M AE HR calculated from the generated ECG with M AE HR calculated from the real input PPG.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell></cell><cell cols="2">Window (sec.) M AEHR</cell></row><row><cell></cell><cell cols="2">(Nilsson et al. 2005)</cell><cell></cell><cell>4.6</cell></row><row><cell></cell><cell cols="2">(Shelley et al. 2006)</cell><cell></cell><cell>2.3</cell></row><row><cell>BIDMC</cell><cell cols="2">(Fleming et al. 2007) (Karlen et al. 2013)</cell><cell>64</cell><cell>5.5 5.7</cell></row><row><cell></cell><cell cols="2">(Pimentel et al. 2016)</cell><cell></cell><cell>2.7</cell></row><row><cell></cell><cell>CardioGAN</cell><cell></cell><cell></cell><cell>0.7</cell></row><row><cell></cell><cell cols="2">(Nilsson et al. 2005)</cell><cell></cell><cell>10.2</cell></row><row><cell></cell><cell cols="2">(Shelley et al. 2006)</cell><cell></cell><cell>2.2</cell></row><row><cell>CAPNO</cell><cell cols="2">(Fleming et al. 2007) (Karlen et al. 2013)</cell><cell>64</cell><cell>1.4 1.2</cell></row><row><cell></cell><cell cols="2">(Pimentel et al. 2016)</cell><cell></cell><cell>1.9</cell></row><row><cell></cell><cell>CardioGAN</cell><cell></cell><cell></cell><cell>2.0</cell></row><row><cell></cell><cell cols="2">(Schäck et al. 2017)</cell><cell></cell><cell>20.5</cell></row><row><cell>Dalia</cell><cell cols="2">(Reiss et al. 2019) (Reiss et al. 2019)</cell><cell>8</cell><cell>15.6 11.1</cell></row><row><cell></cell><cell>CardioGAN</cell><cell></cell><cell></cell><cell>8.3</cell></row><row><cell></cell><cell cols="2">(Schäck et al. 2017)</cell><cell></cell><cell>19.9</cell></row><row><cell>WESAD</cell><cell cols="2">(Reiss et al. 2019) (Reiss et al. 2019)</cell><cell>8</cell><cell>11.5 9.5</cell></row><row><cell></cell><cell>CardioGAN</cell><cell></cell><cell></cell><cell>8.6</cell></row><row><cell cols="5">Window (sec.) M AEHR(E ) M AEHR(P )</cell></row><row><cell></cell><cell>4</cell><cell>4.86</cell><cell>10.67</cell></row><row><cell></cell><cell>8</cell><cell>3.54</cell><cell>10.23</cell></row><row><cell></cell><cell>16</cell><cell>3.27</cell><cell>10.00</cell></row><row><cell></cell><cell>32</cell><cell>3.08</cell><cell>9.77</cell></row><row><cell></cell><cell>64</cell><cell>2.89</cell><cell>9.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>A comparison of M AE HR between generated ECG and real PPG is presented for different window sizes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of CardioGAN and it's ablation variations across all the subjects of the 4 datasets are presented. E can be expressed as {e 1 , e 2 , e 3 , . . . , e N }. We can create a 2-D matrix M of corresponding data points by preserving the order of sequence E and E , where M ⊆ {(e, e )|e ∈ E, e ∈ E }. The discrete Fréchet distance of E and E is calculated as F D = min M max (e,e )∈M d(e, e ), where d(e, e ) denotes the Euclidean distance between corresponding samples of e and e .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The results obtained from CardioGAN-Paired are presented.</figDesc><table><row><cell>with paired ECG-PPG inputs as opposed to our original ap-</cell></row><row><cell>proach which is based on unpaired training. To train Cardio-</cell></row><row><cell>GAN in a paired manner, we follow the same training pro-</cell></row><row><cell>cess mentioned in Section 4.4, except we keep the coupling</cell></row><row><cell>between the ECG and PPG pairs intact in the input data. The</cell></row><row><cell>results are presented in Table 4, and a few samples of gen-</cell></row><row><cell>erated ECG are shown in Figure 4. By comparing these re-</cell></row><row><cell>sults to those presented in Table 4, we observe that unpaired</cell></row><row><cell>training of CardioGAN shows superior performance com-</cell></row><row><cell>pared to paired training. In particular, we notice that while</cell></row><row><cell>CardioGAN-Paired does learns well to generate ECG beats</cell></row><row><cell>from PPG inputs, it fails to learn the exact shape of the orig-</cell></row><row><cell>inal ECG waveforms. This might be because an unpaired</cell></row><row><cell>training scheme forces the network to learn stronger user-</cell></row><row><cell>independent mappings between PPG and ECG, compared to</cell></row><row><cell>user-dependant paired training. While it can be argued that</cell></row><row><cell>utilizing paired data using other GAN architectures might</cell></row><row><cell>perform well, it should be noted that the goal of this experi-</cell></row><row><cell>ment is to evaluate the performance when paired training is</cell></row><row><cell>performed without any fundamental changes to the architec-</cell></row><row><cell>ture. We design CardioGAN with the aim of being able to</cell></row><row><cell>leverage datasets that do not necessarily contain both ECG</cell></row><row><cell>and PPG, hence, unpaired training, even though we resort to</cell></row><row><cell>datasets that do contain both (ECG and PPG) so that ground-</cell></row><row><cell>truth measurements can be used for evaluation purposes.</cell></row></table><note>Failed CasesWe notice there are instances where Cardio-Gan fails to generate ECG samples that resemble the original ECG data very closely. Such cases arise only when the PPG input signals are of very poor quality. We show a few</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://code.engineering.queensu.ca/17ps21/ppg2ecgcardiogan</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computing the Fréchet distance between two polygonal curves</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Godau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Geometry &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">01n02</biblScope>
			<biblScope unit="page" from="75" to="91" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niebauer</surname></persName>
		</author>
		<title level="m">Conquering the ECG</title>
				<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Remedica</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Investigating sources of inaccuracy in wearable optical heart rate sensors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Kibbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion-alGAN: Generating ECG to Enhance Emotion State Classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Computer Science</title>
				<meeting>the International Conference on Artificial Intelligence and Computer Science</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="309" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The use of photoplethysmography for assessing hypertension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elgendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
		<ptr target="https://youtu.be/z0Dr4k24t7U" />
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Systolic peak detection in acceleration photoplethysmograms measured from emergency responders in tropical conditions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elgendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Norton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brearley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e76585</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of signal processing techniques for the extraction of breathing rate from the photoplethysmogram</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Fleming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Biological and Medical Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="232" to="236" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Photoplethysmography pulse rate variability as a surrogate measurement of heart rate variability during non-stationary conditions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Vergara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mainardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laguna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological Measurement</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1271</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving ECG Classification Using Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Golany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lavee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Yarden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Radinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13280" to="13285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PGANs: Personalized generative adversarial networks for ECG synthesis to improve patient-specific deep ECG classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Golany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Radinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haghpanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Tison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bourn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Turakhia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers in Cardiology</title>
				<imprint>
			<date type="published" when="2002">2002. 2019</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
	<note>Open source ECG analysis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learn to Pay Attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiparameter respiratory rate estimation from the photoplethysmogram</title>
		<author>
			<persName><forename type="first">W</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ansermino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Dumont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1946" to="1953" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A dynamical model for generating synthetic electrocardiogram signals</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Mcsharry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="294" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Respiration can be monitored by photoplethysmography with high sensitivity and specificity regardless of anaesthesia and ventilatory mode</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Anaesthesiologica Scandinavica</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1157" to="1162" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention u-net: Learning where to look for the pancreas</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time domain, geometrical and frequency domain analysis of cardiac vagal outflow: effects of various respiratory patterns</title>
		<author>
			<persName><forename type="first">J</forename><surname>Penttilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Helminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jartti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuusela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Huikuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Tulppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Coffeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scheinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Physiology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="376" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toward a robust estimation of respiratory rate from pulse oximeters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Birrenkott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Watkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1914" to="1923" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep PPG: large-scale heart rate estimation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Indlekofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Laerhoven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">3079</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward Dynamically Adaptive Simulation: Multimodal Classification of User Expertise Using Wearable Devices</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rodenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hungler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szulewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">4270</biblScope>
			<date type="published" when="2019">2019. 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised ECG Representation Learning for Emotion Recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning for ECG-Based Emotion Recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="3217" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classification of Cognitive Load and Expertise for Adaptive Simulation using Deep Multitask Learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ruberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rodenbura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hungler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Affective Computing and Intelligent Interaction</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Synthetic ECG generation and Bayesian filtering using a Gaussian wave-based dynamical model</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Shamsollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological Measurement</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1309</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computationally efficient heart rate estimation during physical exercise using photoplethysmographic signals</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2478" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How accurate is pulse rate variability as an estimate of heart rate variability?: A review on studies comparing photoplethysmographic technology with an electrocardiogram</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vagedes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Cardiology</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="29" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Introducing wesad, a multimodal dataset for wearable stress and affect detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duerichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Laerhoven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimodal Interaction</title>
				<meeting>the International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="400" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The use of joint time frequency analysis to quantify the effect of ventilation on the pulse oximeter waveform</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Shelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Stout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Silverman</surname></persName>
		</author>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds" />
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Monitoring and Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="87" />
			<date type="published" when="2006">2006. 2017. 2020</date>
		</imprint>
	</monogr>
	<note>Cardiovascular Diseases</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HeartID: A Multiresolution Convolutional Neural Network for ECG-Based Biometric Human Identification in Smart Health Applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="11805" to="11816" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Electrocardiogram generation with a bidirectional LSTM-CNN generative adversarial network</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2019a. 2017</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
	<note>Unpaired image-to-image translation using cycle-consistent adversarial networks</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Learning Your Heart Actions From Pulse: ECG Waveform Reconstruction From PPG. bioRxiv 815258</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
