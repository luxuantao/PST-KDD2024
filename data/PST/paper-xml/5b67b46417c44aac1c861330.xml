<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Pruning of Large Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
							<email>stefano.faralli@unitelma.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rome Unitelma Sapienza</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Irene</forename><surname>Finocchi</surname></persName>
							<email>finocchi@di.uniroma1.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Rome Sapienza</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Mannheim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paola</forename><surname>Velardi</surname></persName>
							<email>velardi@di.uniroma1.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Rome Sapienza</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Pruning of Large Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present an efficient and highly accurate algorithm to prune noisy or over-ambiguous knowledge graphs given as input an extensional definition of a domain of interest, namely as a set of instances or concepts. Our method climbs the graph in a bottom-up fashion, iteratively layering the graph and pruning nodes and edges in each layer while not compromising the connectivity of the set of input nodes. Iterative layering and protection of pre-defined nodes allow to extract semantically coherent DAG structures from noisy or over-ambiguous cyclic graphs, without loss of information and without incurring in computational bottlenecks, which are the main problem of stateof-the-art methods for cleaning large, i.e., Webscale, knowledge graphs. We apply our algorithm to the tasks of pruning automatically acquired taxonomies using benchmarking data from a SemEval evaluation exercise, as well as the extraction of a domain-adapted taxonomy from the Wikipedia category hierarchy. The results show the superiority of our approach over state-of-art algorithms in terms of both output quality and computational efficiency. * Contributions made while he was still at the University of Mannheim.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the age of information, the Web provides a goldmine of data from which knowledge can be harvested on an unprecedented scale. As a matter of fact, efforts in information extraction and knowledge acquisition from the past decade have been able to produce knowledge resources on a scale that was arguably hard to imagine a few years ago <ref type="bibr">[Carlson et al., 2010;</ref><ref type="bibr" target="#b6">Wu et al., 2012;</ref><ref type="bibr" target="#b1">Fader et al., 2011;</ref><ref type="bibr" target="#b1">Gupta et al., 2014;</ref><ref type="bibr">Dong et al., 2014, inter alia]</ref>. Large coverage, however, comes with new challenges associated with the noise in the input data, as well as errors in the output knowledge base.</p><p>In this paper, we address the problem of pruning large knowledge graphs, removing "noisy" edges and relations. We specifically focus on a high-performing, yet efficient algorithm since in this task we are typically faced with complexity issues that arise from the large size of the graph to be pruned. Examples include open-domain Web mining <ref type="bibr" target="#b4">[Seitner et al., 2016]</ref> or pruning large crowdsourced knowledge bases -e.g., those considered in <ref type="bibr" target="#b1">[Faralli et al., 2015a]</ref> and <ref type="bibr">[Kapanipathi et al., 2014]</ref>. We present a new algorithm, named CRUMB-TRAIL, to efficiently and effectively mine a taxonomic structure "hidden" within a large graph, starting from a number of constraints (or "crumbs", as in the fairy tale of Hansel and Gretel) that a user can select to characterize a domain of interest, which help identifying promising paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement</head><p>We start by formally defining the task of pruning a knowledge graph, along with an intuition and real-case examples to justify the utility of such a task. We define a knowledge graph as a typed directed graph KG = (V, E, T ): nodes V identify a set of concepts or entities and E is a set of relations across nodes such that (a, b, t) ∈ E, where a, b ∈ V and t ∈ T is the relation type. Relations types may vary due to the model specification and the scopes of the representation. Definition 1 (Strict weak order relation). A relation ≺ of type t is a strict weak order (SWO) if it is irreflexive, antisymmetric, and transitive <ref type="bibr">[Roberts and Tesman, 2009]</ref>.</p><p>The edges of a knowledge graph KG induce a strict weak order if the edges in the transitive closure of KG satisfy the three properties of Definition 1. In this paper, we restrict to the case in which all edges are of a given type t, where t is a SWO relation. Common types of SWO relations in a knowledge graph are hypernymy, meronymy and topical <ref type="bibr">[Ponzetto and Strube, 2011]</ref> relations.</p><p>Given a knowledge graph KG, we identify two undesired phenomena, which we denote as "noise": i) infringements of SWO relations: if, for some relation, at least one of the three conditions in Definition 1 does not hold; ii) unessential nodes and edges: assuming that KG is the graph representation of a specific knowledge domain (like, for example, health, tourism, or architecture), unessential nodes and edges are those which are either redundant or even harmful to describe the domain semantics, for instance due to their ambiguity and the resulting potential for semantic shifts. Accordingly, we define a noisy knowledge graph N KG as a graph where one or both undesired phenomena occur.</p><p>Pruning a noisy knowledge graph N KG is viewed throughout the paper as the process of searching a subgraph of N KG such that: 1) its edges induce a strict weak order and 2) it does not contain unessential nodes and edges. Condition 1 is satisfied if the subgraph does not contain cycles: in fact, for each pair of nodes u and v in a cycle, both u ≺ v and v ≺ u hold by transitivity (u ≺ v means that edge (u, v) exists), since the two nodes can reach each other along cycle edges, violating the property of being antisymmetric. This can be formally verified using only topological properties of the graph. On the contrary, testing condition 2 should in principle involve the use of knowledge-based methods, which are heuristic in nature. Our challenge in this paper is to adopt a notion of (un)essentiality which can be formally tested on the graph topology without resorting to methods for assessing loosely defined constraints such as "domain semantics". To this end, our approach is to assume the availability of an initial set of nodes P , defined as follows:</p><p>Definition 2 (Primitive essential nodes). Given a noisy knowledge graph N KG = (V, E), let P ⊆ V be a set of primitive essential nodes containing: a) terminological nodes: a subset of nodes of N KG which belong to a target domain of interest (i.e., domain-specific instances and terms); a) categorical nodes (the "crumbs"): some (even few) domain-pertinent concepts of N KG expected to be located in "higher" positions in the strict weak order; c) root: a concept r that is a common ancestor of all nodes in P , i.e., a node from which all nodes of P can be reached (in short, we call this property r-connectivity).</p><p>Given P , we characterize essential nodes in terms of connectivity of P with respect to the root r: Definition 3 (Essential nodes). Essential nodes in a NKG are those nodes that are strictly needed to preserve set P during the pruning process, i.e., those nodes which, if removed, would compromise r-connectivity.</p><p>The identification of (un)essential nodes is at the core of the CRUMBTRAIL algorithm. The intuition is that, provided that set P implicitly defines one or even more domains of interest, noisy nodes and edges -either originated by extraction errors or out-of-domain -should mostly lie outside the relevant paths connecting terminological and crumb nodes to the root r, i.e., they are not essential.</p><p>Further note that the output subgraph is different from the subgraph of N KG induced by P : in particular, it might contain nodes that are not in P . Similarly, while resembling the maximal acyclic subgraph <ref type="bibr" target="#b0">[Berger and Shor, 1990]</ref> and the minimum feedback arc set <ref type="bibr">[Demetrescu and Finocchi, 2003]</ref> problems, which are often used for untangling the structure of complex networks <ref type="bibr" target="#b4">[Sugiyama et al., 1981;</ref><ref type="bibr">Demetrescu and Finocchi, 2001]</ref>, computing such subgraphs would not be appropriate for our pruning task, since connectivity properties would not be necessarily preserved. On the other side directed Steiner tree <ref type="bibr" target="#b0">[Charikar et al., 1999]</ref> algorithms, besides computational complexity issues (see Sections 3 and 5), would maintain connectivity but would prune N KG very aggressively, removing all multiple paths from r to any node in P , at the risk of omitting nodes that could be semantically relevant to describe domain-consistent and equally valid classifications of a concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of noise.</head><p>In N KG, the presence of noise might be the result of automatic <ref type="bibr" target="#b6">[Velardi et al., 2013;</ref><ref type="bibr" target="#b3">Mitchell et al., 2015]</ref> or collaborative <ref type="bibr">[Ponzetto and Strube, 2011]</ref> knowledge graph construction. In fact, it is acknowledged that the process of creating large and dynamically updated knowledge resources <ref type="bibr" target="#b2">[Hoffart et al., 2016]</ref> cannot be entrusted to a small team of domain experts, and is therefore subject to errors. Some common examples of "noise", which illustrate the utility of N KG cleaning, include: Our hunch is that both extraction errors and out-of-domain nodes are expected to be unessential for preserving the connectivity of the set P . For example, if we aim at building an animal taxonomy, with crumbs such as mammal and/or animal, and a text mining algorithm extracts multiple hypernymy relations (some of which are either wrong or out-of-domain) such as cats ← f eline, cats ← example, cats ← musical, it is very unlikely that the nodes example and musical lie on hypernymy chains connecting cat with mammal or animal, and even more unlikely that they are essential to preserve the connectivity between these nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Approaches to knowledge graphs cleaning in literature differ both in the method to identify relevant and irrelevant nodes, and in the higher or lower impact of the pruning policy. 1) Domain-aware "soft" pruning. Most taxonomy pruning approaches require that the user selects the relevant concepts by hand <ref type="bibr" target="#b5">[Swartout, 1996]</ref> or describes in some way the domain of interest <ref type="bibr" target="#b0">[Best and Lebiere, 2010]</ref>. These approaches are rather conservative, and the number of deleted nodes is actually quite low <ref type="bibr" target="#b3">[Kim et al., 2007]</ref>. Furthermore, none of these methods is able to detect and remove cycles, and complexity may also be an issue, since it is necessary to compute all the paths top-down from root to leaf nodes.</p><p>2) Domain-aware "aggressive" pruning. A number of papers rely on more "aggressive" pruning policies based on topological graph pruning methods. <ref type="bibr" target="#b3">[Kozareva and Hovy, 2010]</ref> propose an algorithm to induce a taxonomy from a graph structure. It uses a root term, a set of seed examples of hypernymy relations (e.g., cats ← f eline) and lexicosyntactic patterns to learn automatically from the Web hyponyms and hypernyms subordinated to the root. It then uses cycle pruning and "longest path" heuristics to induce a DAG structure. <ref type="bibr">Similarly, In Ontolearn Reloaded [Velardi et al., 2013]</ref> the algorithm starts from a set of automatically extracted terms and iteratively extracts hypernym relations thus building an hypernymy graph. To induce a taxonomy from the graph, the authors use a variant of Chu-Liu Edmonds' (CLE) optimal branching algorithm <ref type="bibr">[Edmonds, 1967]</ref>, in which the node weighting strategy is based on preserving both longest paths and the highest coverage of input terms.</p><p>3) Domain-unaware "soft" pruning approaches. Studies like <ref type="bibr">[Kapanipathi et al., 2014] and</ref><ref type="bibr" target="#b1">[Faralli et al., 2015a]</ref> look at the problem of removing cycles in Wikipedia in a user recommendation task. The first paper uses simulated annealing to identify relevant upper categories starting from a set of Wikipages representing users' interests. The latter uses an efficient variant of Tarjan's topological sorting <ref type="bibr" target="#b6">[Tarjan, 1972]</ref> for cycle pruning. To avoid a random selection of edges to prune, <ref type="bibr">[Sun et al., 2017]</ref> combine different heuristics to approximate an "aggressive" node ranking function and experiment different strategies to select an edge to be removed and break cycles.</p><p>All previous methods, setting aside the precision of the pruning process, present at least one of the following two problems, if not both: 1) Computational complexity: Some of the above approaches rely on time or space expensive techniques. For example, the complexity of CLE, and other Steiner algorithms, is affected by the need to compute the weight of alternative branches, which in general implies a depth first search (DFS). Similar problems arise in soft pruning methods, since they need to compute all paths from root to leaf nodes using DFS. Parallelization techniques such as MapReduce would not help and would rather be harmful, due to the parallelization overhead, since DFS is inherently sequential <ref type="bibr" target="#b4">[Reif, 1985]</ref>; 2) Information loss: Edge or path pruning based either on topological (nodes outdegree, longest or shortest paths, transitive closure, etc.) or random selection, may cause the loss of possibly relevant hierarchical relations (especially if the shortest path heuristics is adopted, like in <ref type="bibr">[Kapanipathi et al., 2014]</ref>), and even the disconnection of selected seed nodes <ref type="bibr" target="#b1">[Faralli et al., 2015a]</ref>.</p><p>In marked contrast, as shown in this paper, our CRUMB-TRAIL algorithm does not incur in space and time limits (even when pruning extremely large and dense graphs such as the full Wikipedia), and is both domain aware and "aggressive", while preserving all the available information on the domain (the set P ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The CrumbTrail Algorithm</head><p>In this section we summarize our pruning algorithm, called CRUMBTRAIL. In line with the problem description provided in Section 1, given a (directed) noisy knowledge graph N KG(V, E) (hereafter denoted G for brevity), a set P ⊆ V of terminological nodes and crumbs to be preserved (referred to as protected nodes), and a root r ∈ P , CRUMBTRAIL prunes G to obtain an acyclic subgraph G P that contains all nodes of P , as well as possibly other nodes to guarantee connectivity properties as explained below. Unessential nodes, namely redundant nodes that hinder exposing the domainfocused structure due to the presence of multiple alternative paths (Section 1), are eliminated by CRUMBTRAIL, resulting in a more aggressive pruning. The output graph G P is layered, with top-down edges connecting upper to lower layers. Nodes of P can appear on any layer, including intermediate ones: this is in line with the fact that P contains both terminological nodes and crumbs. Moreover, under the assumption Algorithm 1: CRUMBTRAIL Input: preprocessed graph G(V, E), P ⊆ V , root r ∈ P Output: acyclic and r-connected graph GP 1 Initialize sets Ground, Intermediate, and postponed table</p><formula xml:id="formula_0">2 = 0 3 V0 = Ground 4 repeat 5 let P = V ∩ P 6 Ground = Ground ∪ P 7 Intermediate = Intermediate \ P 8 = + 1 9 createNewLayer(G, Ground, Intermediate, r, V −1, postponed) 10 postponeNodes(G,V , postponed)</formula><p>11 pruneUnessential(G, V , Ground, Intermediate, r) 12 until r ∈ Ground and postponed is empty that G is r-connected with respect to P (i.e., contains directed paths from the root r to every other node of P ), CRUMB-TRAIL also preserves the r-connectivity in G P .</p><p>Overview. The algorithm is called after the fairy tale of Hansel and Gretel: intuitively, it finds its path towards home -the root r -following a trail of "breadcrumbs" -the set P of protected nodes. Differently from previous approaches, CRUMBTRAIL climbs G bottom-up, rather than top-down, traversing crumbs from a bottom layer V 0 containing a subset of P up to a layer V h containing the root r. The number h of layers of G P is not known a priori. Instead, due to the presence of cycles and multiple paths between nodes, layers V i of G P are unfolded incrementally while climbing G upwards, according to the following criteria: a) no path in G P can connect any pair of nodes in V i . If such a path is found during the construction of V i , its start node is deferred (postponed) to an upper layer; b) cycles involving edges incident to nodes of V i are broken, taking care of eliminating cycle edges whose removal does not disconnect any of the nodes P from the root r; c) for each node in the new layer V i , incoming edges start from unprocessed or postponed nodes and outgoing edges end in lower layers (i.e., layers V t such that t &lt; i); d) nodes in V i (and their incident edges) are pruned whenever unessential to preserve the r-connectivity.</p><p>Traversing and pruning G bottom-up guarantees that the number of alternative paths decreases progressively as the graph is incrementally unfolded, due to criteria b and d. As demonstrated in Section 5, this results in lower space consumption and faster execution with respect to previous approaches, even when processing large and highly connected graphs.</p><p>Preprocessing. As a preliminary step, we eliminate from G all self loops, all edges leading to the root (if any), and all nodes of V \ P with indegree or outdegree equal to 0. For each node in P , we also break cycles passing through its outgoing edges. None of these operations harms the connectivity between r and nodes in P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data structures. CRUMBTRAIL maintains a layer counter</head><p>≥ 0 and a hash table of postponed nodes consisting of pairs v, i , where i is an estimate of the layer at which node v will be analyzed and is used as key in the dictionary. With a slight abuse of notation, throughout this section we denote by postponed(i) the subset of nodes temporarily postponed to layer i. Throughout the execution of CRUMB-TRAIL, P is partitioned into two sets, called Ground and Intermediate = P \ Ground. At the beginning, Ground contains only P nodes with outdegree 0. The remaining P nodes are added to Intermediate as well as to the postponed table, using as a tentative layer the length of a shortest path to a ground node. In subsequent iterations, P nodes not yet analyzed are transferred from Intermediate to Ground as they are processed and assigned to layers. The first layer, V 0 , coincides with Ground nodes (line 3 of Algorithm 1).</p><p>Main iteration. After data structure initialization (lines 1-3 of Algorithm 1), graph pruning is performed by an iterative procedure (lines 4-12) that repeats the following steps, until the root has been visited and the postponed table is empty (termination condition at line 12):</p><p>1) createNewLayer: build a new layer V and remove cycles passing through edges outgoing from nodes of V (line 9); 2) postponeNodes: postpone nodes of the current layer V that reach each other (line 10). Namely, if two nodes of V are connected by a path of length k, the starting node of the path is temporarily postponed to layer V +k ; 3) pruneUnessential: remove unessential nodes in V (line 11) while preserving the r-connectivity with respect to nodes in P .</p><p>In more details, the three subroutines work as follows.</p><p>Algorithm CreateNewLayer. Besides selecting candidate nodes to be added to the current layer V , this subroutine also removes cycles passing through incident edges. Nodes added to V can be either starting nodes of edges whose target is in V −1 or postponed nodes. First, every node u with an outgoing edge to V −1 is added to V , provided that there is no directed path from u to any postponed node p. This check is done to satisfy the node layering criteria: all edges should flow top-down, but adding u to V would create at least a bottom-up edge if u is connected to a postponed node. It may be the case that after this phase the current layer V remains empty. If this is the case, and if there are no nodes postponed to layer , the algorithm skips to the first non-empty layer.</p><p>Next, createNewLayer removes cycles passing through edges outgoing from nodes in V . The cycle breaking procedure iterates over these edges and detects a cycle across an edge (x, y) whenever it finds a path π = y ; x starting at node y and ending in x. To implement this check, the algorithm first computes a BFS tree T r rooted at r. Notice that, if an edge f does not belong to T r , it can be safely removed from the cycle involving (x, y) without compromising r-connectivity. Moreover, since T r is acyclic, such an edge must necessarily exist in cycle x, y ; x identified by the cycle breaking procedure. Hence, every cycle can be safely broken while maintaining r-connectivity.</p><p>We remark that, after the execution of this subroutine, it may be possible that there are still edges between nodes in V : if such an edge (x, y) exists, we are guaranteed that it is not part of a cycle, and x will be postponed to an upper layer immediately later by subroutine postponeNodes. It may be also the case that there are edges outgoing from V and reaching nodes that are not yet assigned to a layer: if such an edge (x, y) exists, with x ∈ V and y unlayered, it can happen neither that y is postponed nor that there is a path from y to a postponed node. In that case, x would have not been added to V . Since postponed nodes include intermediate nodes (see the data structure description), if upward edges exist starting from V , they must lead to nodes that are unessential to preserve r-connectivity and can be later removed by subroutine pruneUnessential.</p><p>Algorithm PostponeNodes. This subroutine is invoked by CRUMBTRAIL immediately after building a new layer and identifies nodes of V to be postponed to subsequent layers, updating the postponed table. As previously observed, CRUMBTRAIL aims at creating layers so that edges flow topdown. Hence, nodes of the current layer V who are the starting points of paths ending in V itself must be shifted to higher layers. In more details, given two nodes u and v in V connected by a path π = u ; v, u is postponed |π| levels higher than v, where |π| denotes the path length. Since there could be multiple paths connecting u and v, we do not attempt at establishing a priori the exact layer distance among the two nodes. Instead, π is chosen arbitrarily and the layer at which u is postponed might be later increased due to the discovery of additional (longer) paths originating from u. The postponed table is updated accordingly and all the rescheduled nodes are removed from V .</p><p>Algorithm PruneUnessential. Nodes that are not essential to preserve the connectivity between intermediate and ground nodes can be removed by the procedure pruneUnessential. This algorithm finds nodes of the current layer either reachable from intermediate nodes or reaching ground nodes. Nodes of V that cannot be removed unless compromising the connectivity between Intermediate and Ground are called essential. For any v ∈ V , let G(v) be the set of ground nodes reachable from v and let I(v) be the set of intermediate nodes from which v is reachable. A node v ∈ V is essential if there is at least one pair of nodes, x ∈ I(v) and y ∈ G(v), that can be connected only through v: i.e., x and y would be disconnected by deleting v. It follows that, if G(v) = ∅ or I(v) = ∅, then v does not connect any intermediate-ground pair and can be safely removed. Unessential nodes are then ranked based on |G(v)| and |I(v)| and the highest ranked node is removed (ties are broken arbitrarily). Since the removal of any node changes the rank of the others, the procedure is iterated until no more nodes can be deleted. Hence, only essential nodes survive in V . As a last step, the algorithm deletes nodes with indegree or outdegree equal to 0 that might have been created during the previous steps.</p><p>Example. Figure <ref type="figure" target="#fig_1">1</ref> presents a step-by-step execution of an iteration of CRUMBTRAIL on a 21-node noisy graph. As shown in Figure <ref type="figure" target="#fig_1">1</ref>.i, P = {r, a, b, c, d, e}. During preprocessing (Figure <ref type="figure" target="#fig_1">1</ref>.ii), self-loops on nodes 3 and 9, as well as nodes 10, 11, and 12, whose in/out-degree is 0, are removed. Edge (6, r) points to the root and is thus deleted. Edges (2, e) and (d, 1) are eliminated in order to break cycles e, 4, 2, e and d, 1, d , respectively. The protected nodes e and r are marked as intermediate and postponed to upper layers, based on the length of a shortest path to ground nodes {a, b, c, d}. In Figure <ref type="figure" target="#fig_1">1</ref>.iii, createNewLayer builds a new layer V 1 containing all nodes with an outgoing edge to V 0 . The algorithm then breaks three cycles <ref type="bibr">7, 6, 8, 7 , 6, 8, 6 , and 14, 4, 14 by removing edges (7, 6), (6, 8)</ref>, and (14, 4), respectively. Notice that after edge deletion the connectivity between intermediate and ground nodes is still maintained. In Figure <ref type="figure" target="#fig_1">1</ref>.iv, postponeNodes moves node 4 out of V 1 , deferring its processing to layer V 2 due to the existence of paths 4, 5, b and 4, 2, c . Nodes 7, 6, 5, 2, and 14 are unessential to preserve the connectivity of r and e with ground nodes, and are thus eliminated by pruneUnessential in Figure <ref type="figure" target="#fig_1">1</ref>.v.</p><p>After the first iteration, only nodes 1 and 15 remain in V 1 , as shown in Figure <ref type="figure" target="#fig_1">1</ref>.vi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In order to benchmark CRUMBTRAIL (hereafter, CRU), we consider the following competing approaches: 1) the algorithm from <ref type="bibr" target="#b1">[Faralli et al., 2015a]</ref> (TAR), which is based on Tarjan's topological sorting <ref type="bibr" target="#b6">[Tarjan, 1972]</ref>; 2) the algorithm from <ref type="bibr" target="#b6">[Velardi et al., 2013]</ref> (CLE), which is based on Chu-Liu/Edmond's optimal branching <ref type="bibr">[Edmonds, 1967]</ref> and an ad-hoc node weighting policy; 3) the Wikipedia hierarchical edge pruning algorithm proposed in <ref type="bibr">[Kapanipathi et al., 2014]</ref> (HPA).</p><p>To the best of our knowledge, these three algorithms are the only ones that tackle the problem of fully automated pruning of large Web-size knowledge graphs. As summarized in Section 3: TAR and HPA are "soft" methods only aimed at eliminating cycles, while CLE and CRU belong to the category of "aggressive" pruning algorithms. Furthermore, both TAR and CLE are lossy, i.e., they are not guaranteed to preserve the connectivity of nodes in P . HPA is not lossy with respect to leaf nodes, but given the simple pruning strategy, it may end up eliminating relevant edges.</p><p>Experimental setting. We evaluate the performance of the aforementioned methods for two different tasks, namely ontology pruning and ontology domain adaptation. We are given a noisy knowledge graph N KG(V, E) (G for brevity) a gold-standard graph GS(V , E ) and a reference graph R(V , E ) = G(V, E) ∩ GS(V , E ). In ontology learning G(V, E) is an automatically learned structure that may have noisy and missing elements with respect to the gold standard. Instead, in ontology adaptation G(V, E) is a possibly very dense knowledge graph, and the aim is to extract a domain taxonomy which is fully embedded in G.</p><p>Performance metrics. We apply our four pruning algorithms to G(V, E) and obtain a pruned graph G p (V p , E p ). Importantly, for the sake of comparison with CRU (see line 3 of Algorithm 1), we remove from the pruned graphs obtained from each of the compared algorithms the "pending" leaf and root nodes. We recall that pending leaf nodes in a noisy graph are those leaf nodes not in P and pending roots are those different from r. As a consequence, even though HPA is not lossy -since its simple edge pruning strategy guarantees that all nodes v ∈ V are preserved -in our implementation a number of peripheral nodes are eventually pruned.</p><p>Building V 1 and removal of cycles passing through outgoing edges.  Ideally, we would like to have the pruned graph to perfectly match the reference graph, namely G p (V p , E p ) = R(V , E ) or to be able to quantify different degrees of similarity between the two graphs when no perfect match is attained. To this end, we use the Jaccard distance over the two node sets as evaluation measure:</p><formula xml:id="formula_1">V 0 V 1 V 2 {r,4} V 3 {e} V 2 {r,4} V 3 {e} V 2 {r,4} V 3 {e} V 2 {r} V 3</formula><formula xml:id="formula_2">J V = |V ∪ V p | − |V ∩ V p | |V ∪ V p |<label>(1)</label></formula><p>Since in both tasks of ontology pruning and domain adaptation, the algorithms are provided with a number of initial nodes P , a second objective is that the ontology pruning task is not lossy, i.e. that all nodes in P are preserved in G p . Accordingly, we compute the coverage of P nodes as:</p><formula xml:id="formula_3">C P = |P ∩Vp| |P | .</formula><p>Next, in order to provide a measure of the difficulty of the pruning task, we define the following indexes to compute the amount of noise of G(V, E) with respect to the reference graph R(V , E ):</p><formula xml:id="formula_4">N oise V = (1 − |V | |V | ), N oise E = (1 − |E | |E| )</formula><p>. Finally, we compute the familiar metrics of precision, recall and balanced F-measure of the node P V , R V and F 1 V and edge P E , R E and F 1 E pruning tasks.</p><p>Experiment 1: Cleaning automatically learned taxonomies. In our first experiment, we use the gold-standard taxonomies and the taxonomy learning systems from Task 17 of the SemEval 2015 challenge 2 . In this shared task, competing 2 http://alt.qcri.org/semeval2015/task17/ systems were required to learn a taxonomic structure given an input terminology T and a root node r (P = T ∪ r). Four domains were considered, namely Chemical, Food, Equipment and Science. For each domain, the participants were asked to automatically induce their own taxonomies, using the terminology P as the leaf nodes of the taxonomy. The participating systems thus output automatically built hypernymy graphs, with some amount of noisy and missing nodes and edges with respect to the four gold-standard taxonomies.</p><p>We apply the four previously listed pruning algorithms (HPA, TAR, CLE and CRU) to the output of each of the ontology learning systems participating in the SemEval 2015 challenge, giving a total of 31 runs (note that not all systems submitted an N KG for each of the 4 domains). Table <ref type="table" target="#tab_1">2</ref> provides an overview of the characteristics of the data used in our nodes edges  <ref type="figure">E</ref> ). Table <ref type="table" target="#tab_2">3</ref> shows the performance of the different pruning algorithms. In the SemEval challenge, the submitted noisy hypernymy graphs are not very large (cf. Table <ref type="table" target="#tab_1">2</ref>): consequently none of the algorithms incurs in complexity problems. However, both TAR and CLE are lossy -i.e., some of the nodes in P are disconnected from the resulting taxonomy (cf. the coverage of P nodes C P in column 2). In particular, TAR and CLE are lossy in 11 and 19 out of 31 runs, respectively. Concerning the quality of the resulting pruned taxonomies, the best performance figures are obtained by CRU and HPA. HPA shows a higher recall, primarily due to the fact that it performs only cycle pruning. CRU instead achieves the best overall results when using precision, F 1 measure and the Jaccard distance as evaluation metric -in the case of Jaccard, the smaller the value, the better the taxonomy, since we measure here the distance of the pruned taxonomy from the goldstandard one.</p><formula xml:id="formula_5">C P J V P V R V F 1 V P E R E F 1 E HPA 1.0 .</formula><p>We note that all approaches perform worst on edge pruning than on node pruning. This is because reference taxonomies R do not cover many of the edges of the gold-standard taxonomies in the first place, as shown in Table <ref type="table" target="#tab_1">2</ref>. That is, since pruning algorithms cannot, and are not meant to be able to retrieve missing nodes and edges in GS \R, limited coverage of the edges of the gold-standard taxonomy GS in the reference taxonomy R heavily impacts overall performance on edge structuring. Missing edges are also the main cause for pruning preserved nodes in lossy algorithms (i.e., TAR and CLE). In Table <ref type="table" target="#tab_2">3</ref>, for the sake of fair comparison, we do not test the unique feature of CRUMBTRAIL, which is able to identify promising paths in the noisy graph, when given a number of intermediate nodes as additional hints (the "crumbs"). However, we found that, when adding to the set P an increasing but small number of "crumbs" randomly selected from the intermediate nodes of the reference taxonomies R, performance indicators that were already high achieve only very small improvements, whereas performance on edge pruning, which was lower, increases of about 30% before saturating.</p><p>Experiment 2: Domain adaptation of the Wikipedia category hierarchy. In our second experiment, we apply the four pruning algorithms to the entire Wikipedia category graph, thus testing the full power of the CRUMBTRAIL algorithm. For this, we use different subgraphs of the Wikipedia category graph as silver-standard datasets, namely the category hierarchies rooted in the categories Singers, Entertainers and Companies. The silver-standard category hierarchies are obtained as follows: 1) starting from the full Wikipedia category graph, we remove all incoming edges in the selected root node (e.g., for Singers, we remove edges starting in Singing, Vocal Ensembles and Musicians); 2) we compute the transitive closure of the root node r, e.g., Closure(Singers); 3) we add to the gold-standard all the nodes in Closure(r) and all edges (v i , v j ) such that v i , v j ∈ Closure(r).</p><p>Note that this approach is not guaranteed to produce an error-free, gold-standard category hierarchy. For example, back to Table <ref type="table" target="#tab_0">1</ref>, selecting the root node Geography, we would oddly reach both David Lynch and University of Tokyo. However, for very focused intermediate concepts such as those selected in this experiment, we manually verified on large samples of the datasets that nodes are indeed mostly 'golden' (e.g., they can be considered as specializations of the three roots according to commonsense). The four compared algorithms are provided with: i) the set P = T ∪r, where T are the Wikipages at the leaf nodes of Closure(r) (e.g., Diamanda Galás under category Singers), and ii) the full Wikipedia category graph rooted in Main topic classifications. The task for each algorithm is then to induce from the "noisy" Wikipedia category graph a domain-focused hierarchy, i.e., a directed acyclic graph "embedded" in it, with root r and leaf nodes T . The result of each algorithm is then compared with the silver standard, which, in contrast to the previous experiment, is completely included in the Wikipedia category graph.</p><p>Table <ref type="table" target="#tab_4">5</ref> compares the four algorithms and shows a striking superiority of CRU over the other methods, both in terms of Jaccard distance and F-measure, for all domains. Though the Jaccard distance is always remarkably low, the relative rankings (not shown for sake of space) reflect the order of generality of the domain, and thus, the impact of multiple inheritance: from the most focused domain Singers, to the most generic Entertainers. Table <ref type="table" target="#tab_3">4</ref> shows that the amount of noise in this second experiment is much higher than in the previous experiment using SemEval, and that the dimensions of both noisy and silver taxonomies are order of magnitude higher. The Table <ref type="table">also</ref> shows that CRU and CLE (the latter, whenever it does not run out of memory) are much more aggressive in pruning nodes and edges, as expected, given that HPA and TAR are "soft" algorithms. In terms of efficiency CRU is always faster, followed by the naive HPA pruning strategy. Instead, for CLE it was necessary to limit the maximum depth h of depth-first search to 5 (denoted as CLE(5) in Table <ref type="table" target="#tab_3">4</ref>), and even with this limit, a solution was actually produced only for the first domain, namely Singers. In the other two cases, CLE runs out of memory on a multi-core machine. CRU is beaten in terms of recall by some of the other methods, since, as we repeatedly remarked, TAR and HPA only remove cycles. Note that, in terms of Jaccard distance, CRU almost perfectly retrieves the silver category hierarchies from the noisy graphs. The only competitive system in terms of quality of the pruned graph is CLE: however, computational complexity prevents from obtaining a solution as the dimension of the taxonomy increases, a problem that cannot be mitigated with parallelization algorithms. The higher performance obtained by CRU in the Wikipedia experiment is also due to the fact that, in contrast to the SemEval experiment, the silver category graph is fully embedded in the N KG: therefore, all the necessary information is available to the pruning algorithm. Further note that in this second experiment we did not test the additional feature of CRUMBTRAIL of expanding the set P with intermediate categories: however, the performances are already extremely good and there is quite a limited space for improvements.</p><p>To summarize, our results indicate that, as the dimension and connectivity of the N KG and the amount of noise increase, so does the superiority of CRUMBTRAIL over the other graph pruning methods, both in terms of quality of the results and speed of execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>To the best of our knowledge, CRUMBTRAIL is the first algorithm that has been shown to perform well in the task of removing multiple inheritance in the Wikipedia graph. This problem has prevented so far from fully exploiting Wikipedia hierarchical relations in many relevant applications -including user recommendation <ref type="bibr">[Kapanipathi et al., 2014;</ref><ref type="bibr" target="#b1">Faralli et al., 2015b;</ref><ref type="bibr" target="#b1">Elgohary et al., 2011]</ref>, document categorization <ref type="bibr">[Gabrilovich and Markovitch, 2006]</ref> or query understanding <ref type="bibr" target="#b4">[Schuhmacher et al., 2015]</ref>, to name a few.</p><p>We also acknowledge some limits, which we aim to address in the future. First, the pruning strategy is limited to knowledge graphs with a single relation type that satisfies the SWO constraint. Although SWO relations have a predominant role, this does not fit to more general cases that might occur in the Web of Data, in which more relation types are available. Moreover, CRUMBTRAIL relies on a number terminological and categorical nodes (the"crumbs") that a user can select to characterize a domain of interest. Since the availability of categorical nodes is a tighter constraint, a better solution could be to automatically infer seed categories from terminological nodes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A complete walkthrough example of the application of CRUMBTRAIL to a noisy graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example of multiple inheritance in the Wikipedia category graph: note that top categories are almost completely overlapping.</figDesc><table><row><cell>• Infringement of SWO relations: cycles, which represent</cell></row><row><cell>an infringement of SWO relations, are very common in</cell></row><row><cell>the Wikipedia category graph 1 (e.g., P ersian books →</cell></row><row><cell>Iranian books → P ersian books) and in dictionaries.</cell></row><row><cell>Circularity of definitions is a well-known issue in lexicog-</cell></row><row><cell>raphy and is considered to be a problem since the earlier</cell></row><row><cell>history of computational linguistics [Richens, 2008].</cell></row><row><cell>• Extraction errors: the problem of automated taxonomy</cell></row><row><cell>learning is commonly addressed by extracting SWO rela-</cell></row><row><cell>tions such as hypernymy relations from glossaries [Navigli</cell></row><row><cell>and Velardi, 2010] or definitional patterns [Hearst, 1992]</cell></row><row><cell>like: "x are y" ("cats are felines"). Although more or less</cell></row><row><cell>sophisticated, all algorithms are prone to extraction errors.</cell></row><row><cell>As an example, the sentence "cats are examples of highly</cell></row><row><cell>refined adaptation and evolution" may lead to extracting</cell></row><row><cell>the following hypernymy relations: cats ← example,</cell></row><row><cell>cats ← ref ined, cats ← adaptation, which are all</cell></row><row><cell>wrong according to commonsense knowledge.</cell></row><row><cell>• Out-of-domain nodes: in Wikipedia, Freebase, DBpedia</cell></row><row><cell>and other large knowledge bases, categorical information</cell></row><row><cell>is freely generated by contributors with limited editorial</cell></row><row><cell>verification, which leads to an excessive multiple inheri-</cell></row><row><cell>tance, a problem that may cancel the advantage of adding</cell></row><row><cell>semantics [Matykiewicz and Duch, 2014]. A typical ex-</cell></row><row><cell>ample from Wikipedia's category graph is shown in Table</cell></row><row><cell>1: note that two very different entities, a director (David</cell></row><row><cell>Lynch) and an education institution (University of Tokyo),</cell></row><row><cell>end up almost in the same set of upper Wikipedia cate-</cell></row><row><cell>gories (oddly, they both reach the categories Education</cell></row><row><cell>and P eople). The problem however is not so much the</cell></row><row><cell>quality of semantic relations, but rather the coexistence of</cell></row><row><cell>many different perspectives. For example, the page Uni-</cell></row><row><cell>versity of Tokyo is classified (see column 2 of Table 1)</cell></row><row><cell>as National universities and Visitor attractions in Tokyo.</cell></row><row><cell>These are both reasonable classifications, however the first</cell></row><row><cell>would be an appropriate category for an Education tax-</cell></row><row><cell>onomy, and the second for a Tourism and places taxon-</cell></row><row><cell>omy. Furthermore, these different "semantic threads" do</cell></row><row><cell>not remain separated while climbing towards upper cate-</cell></row><row><cell>gories, but they interweave over and over again in the cate-</cell></row><row><cell>gory graph, making the (useful) task of generating domain</cell></row><row><cell>views particularly complex.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Structural analysis of the dataset used for ontology pruning.</figDesc><table><row><cell>Taxonomy type</cell><cell cols="2">Avg. # nodes Avg. # edges</cell></row><row><cell>Gold (GS)</cell><cell>557</cell><cell>634</cell></row><row><cell>Reference (R)</cell><cell>558</cell><cell>225</cell></row><row><cell>SemEval submitted runs (N KGs)</cell><cell>754</cell><cell>1385</cell></row><row><cell>average N oise V</cell><cell>0.26%</cell><cell></cell></row><row><cell>average N oise E</cell><cell>0.84%</cell><cell></cell></row><row><cell>Pruning algorithm</cell><cell cols="2">Avg. # nodes Avg. # edges</cell></row><row><cell>HPA</cell><cell>289</cell><cell>739</cell></row><row><cell>TAR</cell><cell>288</cell><cell>764</cell></row><row><cell>CLE</cell><cell>263</cell><cell>500</cell></row><row><cell>CRU</cell><cell>299</cell><cell>823</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance in the ontology pruning task (best results for each evaluation metric are bolded).first experiment, covering structural properties of the goldstandard, reference noisy (i.e., SemEval submitted runs) and pruned taxonomies. All the data shown in the table have been averaged over each type of taxonomy, considering for each submitted N KG only the main connected component (the one which contains the root node). Note that, as mentioned in the table, in some cases TAR and CLE could not guarantee the connectivity of any of the nodes in P . Also note that, as shown by the average size of the reference graph, in the average, N KGs submitted to the SemEval challenge include the majority of nodes in the gold taxonomy (V ≈ V ) but they are unable to capture many hypenymy relations (E</figDesc><table><row><cell></cell><cell>24 .90 .98</cell><cell>.93</cell><cell>.23 .94</cell><cell>.34</cell></row><row><cell>TAR</cell><cell>.81 .35 .76 .81</cell><cell>.77</cell><cell>.18 .80</cell><cell>.28</cell></row><row><cell cols="2">CLE .72 .42 .75 .73</cell><cell>.73</cell><cell>.22 .63</cell><cell>.29</cell></row><row><cell cols="2">CRU 1.0 .21 .97 .94</cell><cell>.95</cell><cell>.24 .93</cell><cell>.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Structural analysis of the Wikipedia category graphs. nodes edgesC P J V P V R V F 1 V P E R E F 1 E</figDesc><table><row><cell cols="2">Wikipedia category graph (G)</cell><cell></cell><cell># nodes</cell><cell># edges</cell></row><row><cell cols="2">root:Main topic classifications</cell><cell cols="3">5,398,723 18,006,438</cell></row><row><cell cols="4">Silver category hierarchy (GS = R) # nodes</cell><cell># edges</cell></row><row><cell>root: Singers</cell><cell></cell><cell></cell><cell>49,076</cell><cell>93,243</cell></row><row><cell>N oise V</cell><cell></cell><cell></cell><cell>99.09%</cell><cell></cell></row><row><cell>N oise E</cell><cell></cell><cell></cell><cell>99.48%</cell><cell></cell></row><row><cell>algorithms</cell><cell></cell><cell></cell><cell># nodes</cell><cell># edges</cell><cell>run time</cell></row><row><cell>HPA</cell><cell></cell><cell></cell><cell cols="2">167,268 588,681</cell><cell>0.5 h</cell></row><row><cell>TAR</cell><cell></cell><cell></cell><cell cols="2">167,279 589,564</cell><cell>6 h</cell></row><row><cell>CLE(5)</cell><cell></cell><cell></cell><cell>48,317</cell><cell>84,945</cell><cell>5 h</cell></row><row><cell>CRU</cell><cell></cell><cell></cell><cell>48,632</cell><cell>91,686</cell><cell>0.1 h</cell></row><row><cell cols="4">Silver category hierarchy (GS = R) # nodes</cell><cell># edges</cell></row><row><cell cols="2">root: Entertainers</cell><cell></cell><cell cols="2">338,853 844,708</cell></row><row><cell>N oise V</cell><cell></cell><cell></cell><cell>93.72%</cell><cell></cell></row><row><cell>N oise E</cell><cell></cell><cell></cell><cell>95.30%</cell><cell></cell></row><row><cell>algorithms</cell><cell></cell><cell></cell><cell># nodes</cell><cell># edges</cell><cell>run time</cell></row><row><cell>HPA</cell><cell></cell><cell></cell><cell cols="2">529,839 1,859,316</cell><cell>1 h</cell></row><row><cell>TAR</cell><cell></cell><cell></cell><cell cols="2">538,682 2,099,215</cell><cell>1.5 days</cell></row><row><cell>CLE(5)</cell><cell></cell><cell></cell><cell>n.a</cell><cell cols="2">n.a 2 days (fail)</cell></row><row><cell>CRU</cell><cell></cell><cell></cell><cell cols="2">324,576 774,508</cell><cell>0.2 h</cell></row><row><cell cols="4">Silver category hierarchy (GS = R) # nodes</cell><cell># edges</cell></row><row><cell cols="2">root: Companies by stock exchange</cell><cell></cell><cell>7,953</cell><cell>9,436</cell></row><row><cell>N oise V</cell><cell></cell><cell></cell><cell>99.85%</cell><cell></cell></row><row><cell>N oise E</cell><cell></cell><cell></cell><cell>99.94%</cell><cell></cell></row><row><cell>algorithms</cell><cell></cell><cell></cell><cell># nodes</cell><cell># edges</cell><cell>run time</cell></row><row><cell>HPA</cell><cell></cell><cell></cell><cell cols="2">58,543 177,279</cell><cell>1 h</cell></row><row><cell>TAR</cell><cell></cell><cell></cell><cell cols="2">54,312 159,432</cell><cell>1.5 days</cell></row><row><cell>CLE(5)</cell><cell></cell><cell></cell><cell>n.a</cell><cell cols="2">n.a 2 days (fail)</cell></row><row><cell>CRU</cell><cell></cell><cell></cell><cell>7,934</cell><cell>9,395</cell><cell>0.14 h</cell></row><row><cell>HPA</cell><cell cols="2">1.0 .65 .35 .99</cell><cell>.48</cell><cell>.19 .90</cell><cell>.28</cell></row><row><cell>TAR</cell><cell cols="2">.99 .53 .35 .99</cell><cell>.40</cell><cell>.61 .99</cell><cell>.31</cell></row><row><cell cols="3">CLE 5 .33 .67 .33 .33</cell><cell>.33</cell><cell>.33 .30</cell><cell>.32</cell></row><row><cell>CRU</cell><cell cols="2">1.0 .02 1.0 .98</cell><cell>.99</cell><cell>1.0 .98</cell><cell>.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance in the task of domain adaptation of the Wikipedia category graph. Measures are averaged on the 3 domains.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">https://en.wikipedia.org/wiki/Wikipedia: Dump_reports/Category_cycles Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been partially supported by grant PO 1900/1-1 of the Deutsche Forschungsgemeinschaft (DFG) under the JOIN-T project, by the IBM Faculty Award #2305895190, and by the MIUR under grant "Dipartimenti di eccellenza 2018-2022" of the Department of Computer Science of Sapienza University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Demetrescu and Finocchi, 2001] Camil Demetrescu and Irene Finocchi. Breaking cycles for minimizing crossings</title>
		<author>
			<persName><forename type="first">Shor</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Shor ; Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of BRiMS</title>
				<meeting>of BRiMS</meeting>
		<imprint>
			<date type="published" when="1990">1990. 1990. 2010. 2010. 2010. 2010. 1999. 1999. 2001. 2003</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
	<note>Information Processing Letters</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overcoming the brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge</title>
		<author>
			<persName><forename type="first">Dong</forename></persName>
		</author>
		<idno>25:1-25:23</idno>
	</analytic>
	<monogr>
		<title level="m">Gabrilovich and Markovitch</title>
				<imprint>
			<publisher>Alon Halevy</publisher>
			<date type="published" when="1967">2014. 2014. 1967. 1967. 2011. 2011. 2011. 2011. 2015a. 2015. 2015b. 2015. 2006. 2006. 2014. 2014</date>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="505" to="516" />
		</imprint>
		<respStmt>
			<orgName>Jack Edmonds. Optimum branchings. Journal of Research of the National Bureau of Standards</orgName>
		</respStmt>
	</monogr>
	<note>Proc. of PVLDB</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">User interests identification on Twitter using a hierarchical knowledge base</title>
		<author>
			<persName><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><surname>Hoffart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web: Trends and Challenges</title>
				<editor>
			<persName><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chitra</forename><surname>Venkataramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amit</forename><surname>Sheth</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992. 1992. 2016. 2016. 2014</date>
			<biblScope unit="volume">8465</biblScope>
			<biblScope unit="page" from="99" to="113" />
		</imprint>
	</monogr>
	<note>Proc. of COLING</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Navigli and Velardi, 2010] Roberto Navigli and Paola Velardi. Learning word-class lattices for definition and hypernym extraction</title>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simone Paolo Ponzetto and Michael Strube</title>
				<imprint>
			<publisher>Ponzetto and Strube</publisher>
			<date type="published" when="2007">2007. 2007. 2010. 2010. 2014. 2014. 2015. 2015. 2010. 2011. 2011</date>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="1737" to="1756" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP-10. Taxonomy induction based on a collaboratively built knowledge repository</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Methods for visual understanding of hierarchical system structures</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">H</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><surname>Schuhmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<title level="s">Fred Roberts and Barry Tesman. Applied combinatorics</title>
		<editor>
			<persName><forename type="first">Deepak</forename><surname>Ajwani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Patrick</forename><forename type="middle">K</forename><surname>Nicholson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alessandra</forename><surname>Sala</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</editor>
		<meeting>of COLING</meeting>
		<imprint>
			<publisher>Jiankai Sun</publisher>
			<date type="published" when="1981">1985. 1985. 2008. 2009. 2009. 2015. 2015. 2016. 2016. 1981. 2017. 2017</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
		<respStmt>
			<orgName>Roberts and Tesman</orgName>
		</respStmt>
	</monogr>
	<note>Proc. of WebSci</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward distributed use of large-scale ontologies</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">R</forename><surname>Swartout</surname></persName>
		</author>
		<author>
			<persName><surname>Swartout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Tenth Knowledge Acquisition for Knowledge-Based Systems Workshop</title>
				<meeting>of the Tenth Knowledge Acquisition for Knowledge-Based Systems Workshop</meeting>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">OntoLearn Reloaded: A graph-based algorithm for taxonomy induction</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><surname>Velardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGMOD</title>
				<editor>
			<persName><forename type="first">Hongsong</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Haixun</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenny</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Zhu</surname></persName>
		</editor>
		<meeting>of SIGMOD</meeting>
		<imprint>
			<date type="published" when="1972">1972. 1972. 2013. 2013. 2012. 2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
	<note>Probase: A probabilistic taxonomy for text understanding</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
