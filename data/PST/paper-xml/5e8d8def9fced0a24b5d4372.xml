<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
							<email>haitiansun@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@google.com</email>
						</author>
						<title level="a" type="main">PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider open-domain queston answering (QA) where answers are drawn from either a corpus, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a corpus is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., "multi-hop") reasoning. We describe PullNet, an integrated framework for ( <ref type="formula">1</ref>) learning what to retrieve (from the KB and/or corpus) and ( <ref type="formula">2</ref>) reasoning with this heterogeneous information to find the best answer. PullNet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or "pull") operations on the corpus and/or KB. After the subgraph is complete, a similar graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-ofthe art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open domain Question Answering (QA) is the task of finding answers to questions posed in natural language, usually using text from a corpus <ref type="bibr" target="#b7">(Dhingra et al., 2017;</ref><ref type="bibr" target="#b12">Joshi et al., 2017;</ref><ref type="bibr" target="#b8">Dunn et al., 2017)</ref>, or triples from a knowledge base (KB) <ref type="bibr" target="#b39">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b40">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b37">Yih et al., 2015)</ref>. Both of these approaches have limitations. Even the largest KBs are incomplete <ref type="bibr" target="#b22">(Min et al., 2013)</ref>, which limits recall of a KB-based QA system. On the other hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult <ref type="bibr" target="#b18">(Chen et al., 2017;</ref><ref type="bibr">Welbl et al., 2018;</ref><ref type="bibr" target="#b15">Kwiatkowski et al., 2019;</ref><ref type="bibr" target="#b36">Yang et al., 2018)</ref>.</p><p>In this paper we follow previous research <ref type="bibr" target="#b24">(Sawant et al., 2019;</ref><ref type="bibr" target="#b30">Sun et al., 2018)</ref> in deriving answers using both a corpus and a KB. We focus on tasks in which questions require compositional (sometimes called "multi-hop") reasoning, and a setting in which the KB is incomplete, and hence must be supplemented with information extracted from text. We also restrict ourselves in this paper to answers which correspond to KB entities. For this setting, we propose an integrated framework for ( <ref type="formula">1</ref>) learning what to retrieve, from either a corpus, a KB, or a combination, and (2) combining this heterogeneous information into a single data structure that allows the system to reason and find the best answer. In prior work, this approach was termed an early fusion approach, and shown to improve over late fusion methods, in which two QA systems, one corpus-based and one KB-based, are combined in an ensemble.</p><p>The system we describe, PullNet, builds on the GRAFT-Net 1 early fusion system. GRAFT-Net uses heuristics to build a question-specific subgraph which contains sentences from the corpus, entities from the KB, and facts from the KB. A graph <ref type="bibr">CNN (Kipf and Welling, 2016;</ref><ref type="bibr" target="#b17">Li et al., 2016;</ref><ref type="bibr" target="#b26">Schlichtkrull et al., 2017)</ref> variant is then used to reason over this graph and select an answer. However, as we will show experimentally, GRAFT-Net's heuristics often produce subgraphs that far from optimal: they are generally much larger than necessary, and often do not contain the answer at all. 1 Graphs of Relations Among Facts and Text Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1904.09537v1 [cs.CL] 21 Apr 2019</head><p>PullNet also uses a reasoning process based on a graph CNN to find answers. However, PullNet learns how to construct the subgraph, rather than using an ad hoc subgraph-building strategy. More specifically, PullNet relies on a small set of retrieval operations, each of which takes a node in an existing subgraph, and then expand the node by retrieving new information from the KB or the corpus. PullNet learns when and where to apply these "pull" operations with another graph CNN classifier. The "pull" classifier is weakly supervised , using question-answer pairs for supervision.</p><p>The end result is a learned iterative process for subgraph construction, which begins with a small subgraph containing only the question text and the entities which it contains, and gradually expands the subgraph to contain information from the KB and corpus that are likely to be useful. The incremental question-guided subgraph construction process results in high-recall subgraphs that are much smaller than the ones created heuristically, making the final answer extraction process easier. The process is especially effective for multi-hop questions, which naively would require expanding the subgraph to include all corpus and KB elements that are k hops away from the question.</p><p>PullNet improves over the current state-ofthe-art for KB-only QA on several benchmark datasets, and is superior to, or competitive with, corpus-only QA on several others. For multi-hop questions, this improvement is often dramatic: for instance, MetaQA <ref type="bibr" target="#b41">(Zhang et al., 2018)</ref> contains multi-hop questions based on a small movie KB, originally associated with the WikiMovies dataset <ref type="bibr" target="#b21">(Miller et al., 2016)</ref>. In a KB-only setting, Pull-Net improves hits-at-one performance for 3-hop MetaQA questions from 62.5% to 91.4%. Perhaps more interestingly, PullNet obtains performance of 85.2% hits-at-one with a KB from which half of the triples have been removed, if that KB is supplemented with a corpus. We note that this result improves by 7% (absolute improvement) over a pure corpus-based QA system, and by more than 25% over a pure KB-based QA system. In a similar incomplete-KB setting, PullNet improves over GRAFTNet by 6.8% on the ComplexWebQuestions dataset <ref type="bibr" target="#b31">(Talmor and Berant, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This paper has focused on QA for multi-hop questions using large KBs and text corpora as the information sources from which answers can be drawn. The main technical contribution is an iterative question-guided retrieval mechanism that retrieves information from KBs, corpora, or combinations of both. The iterative retrieval makes it possible to follow long paths of reasoning on large KBs, and as we show below, this leads to new state-of-the art results on two datasets of this sort. The hybrid KB/text retrieval, and the ability to reason over combinations of text and KBs, lets us extend these results to settings where only text is available, and to settings where text and an incomplete KB is available. These contributions are based on much prior work.</p><p>A long line of QA models have been developed which answer questions based on a single passage of text <ref type="bibr" target="#b6">(Dhingra et al., 2016;</ref><ref type="bibr">Yu et al., 2018;</ref><ref type="bibr" target="#b27">Seo et al., 2016;</ref><ref type="bibr" target="#b10">Gao et al., 2018;</ref><ref type="bibr" target="#b19">Liu et al., 2017;</ref><ref type="bibr" target="#b5">Devlin et al., 2018)</ref>. Generally, these "reading comprehension" systems operate by encoding the passage and question into an embedding space, and due to memory limitations cannot be applied to a large corpus instead of a short passage. To address this limitation a number of systems have been designed which use a "retrieve and read" pipeline <ref type="bibr" target="#b18">(Chen et al., 2017;</ref><ref type="bibr" target="#b7">Dhingra et al., 2017;</ref><ref type="bibr" target="#b12">Joshi et al., 2017;</ref><ref type="bibr" target="#b8">Dunn et al., 2017;</ref><ref type="bibr" target="#b33">Wang et al., 2018</ref><ref type="bibr" target="#b34">Wang et al., , 2017))</ref>, in which a retrieval system with high recall is piped into a reading comprehension system that can find the answer. An alternative approach is "phrase-indexed" QA <ref type="bibr" target="#b28">(Seo et al., 2018)</ref>, where embedded phrases in a document are indexed and searched over. Such systems differ from PullNet in that only a single round of retrieval is used; however, for questions that requires multi-hop reasoning, we believe is difficult for a single retrieval step to find the relevant information. These systems are also not able to use both KB and text for QA.</p><p>SplitQA <ref type="bibr" target="#b31">(Talmor and Berant, 2018)</ref> is a textbased QA system that cam decompose complex questions (e.g. conjunction, composition) into simple subquestions, and perform retrieval on them sequentially. Although it uses iterative retrieval for multi-hop questions, unlike PullNet, SplitQA does not also use a KB as an information source. Also, since SplitQA has been applied only to the Complex WebQuestions dataset, it is unclear how general its question decomposition methods are.</p><p>There has also been much work on QA from KBs alone, often using methods based on memory networks <ref type="bibr" target="#b29">(Sukhbaatar et al., 2015)</ref>, semantic parsing <ref type="bibr" target="#b39">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b40">Zettlemoyer and Collins, 2005)</ref> or reinforcement learning <ref type="bibr" target="#b3">Das et al. (2017a)</ref>. Extending such KB-based work to also use text, however, is non-trivial. In <ref type="bibr" target="#b4">(Das et al., 2017b)</ref> one approach for a hybrid text-based and KB-based QA system is described, where keyvalue memory networks are used to store text as well as KB facts encoded with a universal schema representation <ref type="bibr" target="#b23">(Riedel et al., 2013)</ref>. We use a similar method as one of our baselines, following <ref type="bibr" target="#b30">(Sun et al., 2018)</ref>. Another line of QA work from text and KBs is exemplified by AQQU <ref type="bibr" target="#b1">(Bast and Haussmann, 2015)</ref> and its successors <ref type="bibr" target="#b24">(Sawant et al., 2019)</ref>. These systems focus on questions that, like the questions in the SimpleWebQuestions dataset, can be interpreted as identifying an entity based on a relationship and related entity, plus (potentially) additional restrictions described in text, and it is unclear how to extend such approaches to multi-hop questions.</p><p>GRAFT-Net <ref type="bibr" target="#b30">(Sun et al., 2018)</ref> supports multihop reasoning on both KBs and text by introducing a question subgraph built with facts and text, and uses a learned graph representation <ref type="bibr" target="#b14">(Kipf and Welling, 2016;</ref><ref type="bibr" target="#b17">Li et al., 2016;</ref><ref type="bibr" target="#b26">Schlichtkrull et al., 2017;</ref><ref type="bibr" target="#b25">Scarselli et al., 2009)</ref> to perform the "reasoning" required to select the answer. We use the same representation and reasoning scheme as GRAFT-Net, but do not require that the entire graph be retrieved in a single step. In our experimental comparisons, this gives significant performance gains for multi-hop reasoning tasks.</p><p>Combinations of KBs and text have also been used for relation extraction and Knowledge Base Completion (KBC) <ref type="bibr" target="#b16">(Lao et al., 2012;</ref><ref type="bibr" target="#b32">Toutanova et al., 2015;</ref><ref type="bibr" target="#b3">Das et al., 2017a)</ref>. The QA task differs from KBC in that in QA, the inference process must be conditioned on a natural-language question, which leads to different constraints on which methods can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The PullNet Model</head><p>PullNet retrieves from two "knowledge sources", a text corpus and a KB. Given a question, PullNet will use these to construct a question subgraph that can be used to answer the question. The question subgraph is constructed iteratively. Initially the subgraph depends only on the question. PullNet then iteratively expands the subgraph by choos-ing nodes from which to "pull" information about, from the KB or corpus as appropriate. The question subgraph is heterogeneous, and contains both entities, KB triples, and entity-linked text.</p><p>In this section, we will first introduce notation defining the heterogeneous graph structure we use. Then we will introduce the general iterative retrieval process. Finally, we will discuss the retrieval operations used on the corpus and KB, and the classification operations on the graph which determine where to perform the retrievals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Question Subgraph</head><p>A question subgraph for question q, denoted G q = {V, E}, is a hetogeneous graph that contains information from both the text corpus and the KB relevant to q. Let V denote the set of vertices, which we also call nodes. Following GRAFT-Net <ref type="bibr" target="#b30">(Sun et al., 2018)</ref>, there are three types of nodes: entity nodes V e , text nodes V d , and fact nodes V f , with V = V e ∪V d ∪V f . A entity node v e ∈ V e represents an entity from the knowledge base. A text node v d ∈ V d represents a document from the corpus, with a sequence of tokens denoted (w 1 , . . . , w |d| ). In this paper, a document is always a single sentence, to which an entity linker <ref type="bibr" target="#b11">(Ji et al., 2014)</ref> has been applied to detect and ground entity mentions. A fact node v f ∈ V f represents a triplet (v s , r, v o ) from the KB, with subject and objects v s , v o ∈ V e and relation r. We let E denote the set of edges between nodes. An edge connects a fact node v f and an entity node v e exists iff fact v f has v e as its subject or object. An edge connects a text node v d with entity node v e iff the entity is mentioned in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Iterative subgraph expansion and classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Overview</head><p>We start with a question subgraph G 0 q = {V 0 , E 0 } where V 0 = {e q i } is the list of entities mentioned in the question and E 0 is an empty set. We iteratively expand the question subgraph G 0 q until it contains the information required to answer the question.</p><p>The algorithm is shown in Alg 1. Briefly, we expand the graph in T iterations. In each iteration, we choose k entities to expand, and then for each selected entity, we retrieve a set of related documents, and also a set of related facts. The new documents are then passed through an entity-linking system to identify entities that occur in them, and the head and tail entities of each fact will also be extracted. The last stage in the iteration is to update the question graph by adding all these new edges. After the t-th iteration of expansion, an additional classification step is applied to the final question subgraph which predicts the answer entity.</p><p>Algorithm 1 PullNet 1: Initialize question graph G 0 q with question q and question entities, with</p><formula xml:id="formula_0">V 0 = {eq i } and E 0 = ∅. 2: for t = 1, • • • , T do 3:</formula><p>Classify and select the entity nodes in the graph with probability larger than</p><formula xml:id="formula_1">{ve i } = classify pullnodes(G t q , k) 4:</formula><p>for all ve in {ve i } do 5:</p><p>Perform pull operation on selected entity nodes</p><formula xml:id="formula_2">{v d i } = pull docs(ve, q) {v f i } = pull facts(ve, q) 6: for all v d in {v d i } do 7: Extracted entities in new document nodes {v e(d) i } = pull entities(v d ) 8: for all v f in {v f i } do 9:</formula><p>Extract head and tail of new fact nodes {v e(f</p><formula xml:id="formula_3">) i } = pull headtail(v f ) 10:</formula><p>Add new nodes and edges to question graph</p><formula xml:id="formula_4">G t+1 q</formula><p>= update(G t q ) 11: Select entity node in final graph that is the best answer vans = classify answer(G T q )</p><p>We will now describe the operations used in Alg 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Pull Operations</head><p>Pull operations either retrieve information from a knowledge source, or extract entities from a fact or document.</p><p>The two extraction operations are relatively simple. The pull entities(v d ) operation inputs a document node v d , calls an entity linker and returns all entities mentioned in v d . The pull headtail(v f ) operation inputs a fact node v f and returns the subject and object entity of fact v f . The retrieval operations are more complex. The pull docs(v e , q)) operation retrieves relevant documents from the corpus. We use an IDF-based retrieval system, Lucene <ref type="bibr" target="#b20">(McCandless et al., 2010)</ref> and assume that all sentences have been entitylinked prior to being indexed. The retrieved documents are constrained to link to entity v e , and are ranked by their IDF similarity to the question q.</p><p>Only the top N d documents in this ranking are returned.</p><p>The pull facts(v e , q) operation retrieves the top N f facts from the KB about entity v e . The retrieved facts are constrained to have v e as their subject or object, and are ranked based on the similarity S(r, q) between the fact's relation r and the question q. Since it is not obvious how to assess relevance of a fact to a question q, we learn a S(r, q) as follows. Let h r be an embedding of relation r, which is looked up from an embedding table, and let q = (w 1 , . . . , w |q| ) be the sequence of words for question q. Similarity is defined as the dot-product of the last-state LSTM representation for q with the embedding for r. This dot-product is then passed through a sigmoid function to bring it into a range of [0, 1]: as we explain below, we will train this similarity function as a classifier which predicts which retrieved facts are relevant to the question q. The final ranking method for facts is thus</p><formula xml:id="formula_5">h q = LSTM(w 1 , . . . , w |q| ) ∈ R n S(r, q) = sigmoid(h T r h q )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Classify Operations</head><p>Two types of classify operations are applied to the nodes in a subgraph G t q . These operations are applied only to the entity nodes in the graph, but they are based on node representations computed by the graph CNN, so the non-entity nodes and edges also affect the classification results.</p><p>During subgraph construction, the classify pullnodes(G t q ) operation returns a probability an entity node v e should be expanded in the next iteration. In subgraph construction, we choose the k nodes with the highest probability in each iteration. After the subgraph is complete, the classify answer(G t q ) operation predicts whether an entity node answers the question. The highest-scoring entity node is returned as the final answer.</p><p>We use the same CNN architecture used by GRAFT-Net <ref type="bibr" target="#b30">(Sun et al., 2018)</ref> for classification. GRAFT-Net's source is open-domain and it supports node classification on heterogeneous graphs containing facts, entities, and documents. Graft-Net differs from other graph CNN implementations, in using special mechanisms to distribute representations across different types of nodes and edges: notably, document nodes are represented with LSTM encodings, extended by mechanisms that allow the representations for entity nodes v e to be passed into a document v d that mentions v e , and mechanisms that allow the LSTM hidden states associated with an entity mention to be passed out of v d to the associated entity node v e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">The Update Operation</head><p>The update operation takes the question subgraph G t−1 q from the previous iteration and updates it by adding the newly retrieved entity nodes {v e(f ) i } ∪ {v e(d) i }, the text nodes {v d i }, and the fact nodes {v f i }. It also updates the set of edges E based on the definitions of Section 3.1. Note that some new edges are derived when pull operations are performed on text nodes and fact node, but other new edges may connect newly-added nodes with nodes that already exist in the previous subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>To train PullNet, we assume that we only observe question and answer pairs, i.e., the actual inference chain required to answer the question is latent. We thus need to use weak supervision to train the classifiers and similarity scores described above.</p><p>To train these models, we form an approximation of the ideal question subgraph for question q as follows. Note that in training, the answer entities are available. We use these to find all shortest paths in the KB between the question entities and answer entities. Each entity e that appears in such a shortest path will be marked as a candidate intermediate entities. For each candidate intermediate entity e we record its minimal distance t e from the question nodes.</p><p>When we train the classify pullnodes classifier in iteration t, we treat as positive examples only those entities e that are connected to a candidate intermediate entity e with distance e t = t+1. Likewise in training the similarity function S(h r , q) we treat as positive relations leading to candidate intermediate entities e at distance e t = t + 1. This encourages the retrieval to focus on nodes that lie on shortest paths to an answer.</p><p>In training we use a variant of teacher forcing. When training, we pull from all entity nodes with a predicted score larger than some threshold , rather than only the top k nodes. If, during training, a candidate intermediate entity is not retrieved in iteration t e , we add it to the graph anyway. The values T and are hyperparameters, but here we always pick for T , the number of retrieval steps, maximum length of the inference chain needed to ensure full coverage of the answers.</p><p>We use the same classifier on the graph in the retrieval step as in answer selection, except that we change last fully-connected layer. The classifiers used for retrieval in the different iterations are identical.</p><p>The learned parts of the model are implemented in PyTorch, using an ADAM optimizer (Kingma and Ba, 2014), and the full retrieval process of Alg 1 is performed on each minibatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>MetaQA <ref type="bibr" target="#b41">(Zhang et al., 2018)</ref> consists of more than 400k single and multi-hop (up to 3-hop) questions in the domain of movies. The questions were constructed using the knowledge base provided by the WikiMovies <ref type="bibr" target="#b21">(Miller et al., 2016)</ref> dataset. We use the "vanilla" version of the queries: for this version, the 1-hop questions in MetaQA are exactly the same as WikiMovies. We use the KB and text corpus that are supplied with the WikiMovies dataset <ref type="bibr" target="#b21">(Miller et al., 2016)</ref>, and run use a simple exact match on surface forms to perform entity linking. The KB used here is relatively small, with about 43k entities and 135k triples. WebQuestionsSP <ref type="bibr" target="#b37">(Yih et al., 2015)</ref> contains 4737 natural language questions that are answerable using Freebase<ref type="foot" target="#foot_0">2</ref> . The questions require up to 2-hop reasoning from knowledge base, and 1-hop reasoning using the corpus. We use Freebase as our knowledge base but for ease of experimentation restrict it to a subset of Freebase which contains all facts that are within 2-hops of any entity mentioned in the questions of WebQuestionsSP. This smaller KB contains 164.6 million facts and 24.9 million entities. We use Wikipedia as our text corpus and again use a simple entity-linker: we link entities by exact matching to any surface form annotated in FACC1 project <ref type="bibr" target="#b9">(Gabrilovich et al., 2013)</ref>.<ref type="foot" target="#foot_1">3</ref> Complex WebQuestions 1.1 (Complex WebQ) <ref type="bibr" target="#b31">(Talmor and Berant, 2018)</ref> is generated from Web-QuestionsSP by extending the question entities or adding constraints to answers, in order to construct more complex multi-hop questions<ref type="foot" target="#foot_2">4</ref> . There are four types of question: composition (45%), conjunction (45%), comparative (5%), and superlative (5%). The questions require up to 4-hops of reasoning on knowledge base and 2-hops on the corpus. We use the same KB and corpus as used for WebQuestionsSP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tasks</head><p>We explored several different QA settings: KB only (complete), corpus only, incomplete KB only, and incomplete KB paired with the corpus.</p><p>In the KB only complete setting, the answer always exists in knowledge base: for all of these datasets, this is true because the questions were crowd-sourced to enforce this conditions. This is the easiest setting for QA, but arguably unrealistic, since with a more natural distribution of questions, a KB is likely to be incomplete. Note that PullNet can be used in this setting by simply removing the pull docs operation.</p><p>In the text only setting we use only the corpus. Again, PullNet can operate in this setting by rethe pull facts operation. In the incomplete KB setting, we simulate KBbased QA on anincomplete KB by randomly discarding some of the triples in the KB: specifically, we randomly drop a fact from the knowledge base with probability p = 50%.</p><p>In the incomplete KB plus text setting, we pair the same incomplete knowledge base with the corpus. In principle this allows a learned QA system to adopt many different hybrid strategies. For simple 1-hop queries, a model might use the KB when the required fact exists, and "back off" to text when the KB is missing information. In multihop or conjunctive queries, the reasoning done in answering the question might involve combining inferences done with text and inferences done with KB triples.</p><p>Comment. One point to note is that our training procedure is based on finding shortest paths in a complete KB, and we use this same procedure on the incomplete KB setting as well. Thus the weak training that we use should, in the incomplete-KB settings, be viewed as a form of weak supervision, with labels that are intermediate in informativeness between pure distant training (with only question-answer pairs) and gold inference paths (a setting that has been extensively investigated on some of these datasets, in particular Web-QuestionsSP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We choose Key-Value Memory Network <ref type="bibr" target="#b21">(Miller et al., 2016)</ref> and GRAFT-Net <ref type="bibr" target="#b30">(Sun et al., 2018)</ref> as our baseline models: to the best of our knowledge, these are the only ones that can use both text and KBs for question answering. However, both models are limited in the number of facts and text that can fit into memory. Thus, we create a separate retrieval process as a pre-processing step, which will be discussed below.</p><p>Key-Value Memory Network (KVMem) <ref type="bibr" target="#b21">(Miller et al., 2016)</ref> maintains a memory table which stores KB facts and text encoded into key-value pairs. The encoding of KB facts is the same as is presented in <ref type="bibr" target="#b21">Miller et al. (2016)</ref>. For text, we use an bi-directional LSTM to encode the text for the key and take the entities mentioned in the text as values. Our implementation shows comparable performance on the WikiMovies (MetaQA 1-hop) datasets as reported previously, as shown in Table <ref type="table">2</ref>.</p><p>For GRAFT-Net <ref type="bibr" target="#b30">(Sun et al., 2018)</ref>, we use the implementation published by the author<ref type="foot" target="#foot_3">5</ref> ; however, we run it on somewhat different data as described in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Subgraph Retrieval for Baseline Models</head><p>Text was retrieved (non-iteratively) using the IDFbased similarity to the question. It is less obvious how to perform KB retrieval: we would like to retrieve as many facts as possible to maximize the recall of answers, but it is infeasible to take all facts that are within k-hop away from question entities since the number grows exponentially, and there is no widely-used heuristic, corresponding to IDF-based text retrieval, for extracting subsets of Table <ref type="table">2</ref>: Hits@1 on MetaQA compared to baseline models. Number below the double line are from original papers: KV-Mem (KB) <ref type="bibr" target="#b21">(Miller et al., 2016)</ref>, KV-Mem (Text) <ref type="bibr" target="#b35">(Watanabe et al., 2017)</ref>, GraftNet <ref type="bibr" target="#b30">(Sun et al., 2018)</ref>, and VRN <ref type="bibr" target="#b41">(Zhang et al., 2018)</ref>. * Reimplemented or different data.</p><p>a KB. We elected to first run the PageRank-Nibble algorithm <ref type="bibr" target="#b0">(Andersen et al., 2006</ref>) with = 1e −<ref type="foot" target="#foot_4">6</ref> and pick the top m entities. PageRank-Nibble efficiently approximates a personalized PageRank (aka random walk with reset) seeded from the questions (not the answers, which are of course not available at test time). We then find all entities from these top m entities that are within k-hops of the question entities. Finally, we collect all facts from the knowledge base that connect any pair of the retrieved entities. This allows us to easily vary the number of KB facts that are retrieved. The retrieval results are shown in Table <ref type="table">3 and  4</ref>. For the smaller MetaQA KB, the proposed retrieval method finds high-coverage graphs. For ComplexWebQuestions, the recall is only 64%, even with a graph with nearly 2000 nodes: this is expected, since retrieving the relevant entities for a multi-hop question from a KB with millions of entities is a difficult task. 4.5 Main Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">MetaQA</head><p>The experimental results for MetaQA are shown in Table <ref type="table">2</ref>. For 1-hop questions in MetaQA (which is identical to WikiMovies), our model is compara-ble to the state-of-the-art 6 . For the other three settings, the performance of our re-implementation is slightly worse than the results reported in by original GRAFT-Net paper <ref type="bibr" target="#b30">(Sun et al., 2018)</ref>; this is likely because we use a simpler retrieval module.</p><p>In the KB-only setting, PullNet shows a large improvement over the baseline models on 2-hop and 3-hop questions. For the text only setting, PullNet also improves on the baselines, and other prior models, by a large margin. Finally, PullNet also shows significant improvements over baselines in the text and incomplete-KB-plus-text settings. We also see that PullNet is able to effectively combine KB and text information, as this setting also greatly outperforms the incomplete KB alone or the text alone.<ref type="foot" target="#foot_5">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">WebQuestionsSP</head><p>Table <ref type="table" target="#tab_4">5</ref> presents the results on the WebQuestion-sSP dataset. PullNet is comparable with GRAFT-Net on the complete KB only setting and slightly worse on the text only setting, but is consistently better than GRAFT-Net on the incomplete KB setting or the incomplete KB plus text setting. It is also significantly better than the reimplemented GRAFT-Net, which uses the less highly-engineered retrieval module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Complex WebQuestions</head><p>Complex WebQuestions contains multi-hop questions against FreeBase: intuitively, one would expect that single-shot retrieval of facts and text would not be able to always find efficient information to answer such questions.  Researchers are only allowed limited submissions on the Complex WebQuestions test set, however, some results for the test set are shown in Table 7. Results with the text-only (Wikipedia) setting are comparable to the dev set results (13.8% instead of 13.1%) as are results with the KB-only setting (45.9% instead of 47.2%).</p><p>For completeness, we also compare to SplitQA <ref type="bibr" target="#b31">(Talmor and Berant, 2018)</ref>. SplitQA takes a multihop question and decomposes it into several simple sub-questions, and sends each of these to subquestions to the Google search engine. After that, it applies a reading comprehension model to gather information from the web snippets returned by Google to find answers. Using this same collection of Google snippets as the corpus, our model has 4.5% lower hists-at-one than SplitQA. However, the snippet corpus is arguably biased toward the SplitQA model, since it was collected specifically to support it. We also note unlike SplitQA, PullNet relies only on open-source components and corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Further Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Retrieval Performance of PullNet</head><p>We compare the retrieval performance of PullNet and PageRank-Nibble on multi-hop questions with a complete KB, varying the number of entities retrieved by PageRank-Nibble. The results in Figure <ref type="figure" target="#fig_0">1</ref> show that PullNet retrieves far fewer entities but obtains higher recall. We also further explored the effectiveness of iterative retrieval for multi-hop questions on a text corpus. The results are shown in Figure <ref type="figure" target="#fig_1">2</ref>. Again, PullNet with multiple iterations of retrieval, obtains higher recall than a single iteration of IDFbased retrieval on both the MetaQA (3-hop) and the Complex WebQuestions dataset.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Training Efficiency</head><p>We also analyze the training efficiency of PullNet. PullNet's algorithm is quite different from prior systems, since learning and retrieval are interleaved: in most prior systems, including GRAFT-Net, retrieval is performed only once, before learning. Intuitively, interleaving learning with the relatively slow operation of retrieval is potentially slower; on the other hand, PullNet's final question subgraph is smaller than GRAFT-Net, which makes learning potentially faster.</p><p>To study these issues, we plot the Hits@1 performance of learned model versus wall clock time in Figure <ref type="figure" target="#fig_4">4</ref>. This experiment is run on Complex WebQuestions in the KB-only setting, using one high-end GPU. To be fair to GRAFT-Net, we used a fast in-memory implementation of PageRank-Nibble (based on SciPy sparse matrices), which takes about 40 minutes to complete. GRAFT-Net takes an average of 31.9 minutes per epoch, while PullNet is around four times slower, taking an average of 114 minutes per epoch.</p><p>As the graph shows, initially PullNet's performance is better, since GRAFT-Net cannot start learning until the preprocessing finishes. GRAFT-Net's faster learning speed then allows it to dominates for some time. GRAFT-Net reaches its peak performance in about 3.6 hours. PullNet passes GRAFT-Net in hits-at-one after around 6 hours, or about 3 epochs for PullNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>PullNet is a novel integrated QA framework for ( <ref type="formula">1</ref>) learning what to retrieve from a KB and/or corpus and (2) reasoning with this heterogeneous to find the best answer. Unlike prior work, PullNet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph-CNN is used to identify subgraph nodes that should be expanded using retrieval (or "pull") operations on the corpus and/or KB. This iterative process makes it possible to retrieve a small graph that contains just the information relevant to a multi-hop question-a task that is in general difficult.</p><p>Experimentally PullNet improves over the prior state-of-the art for the setting in which questions are answered with a corpus plus an incomplete KB, or in settings in which questions need complex "multi-hop" reasoning. Sometimes the performance improvements are dramatic: e.g., an improvement from 62.5% hits-at-one to 91.4% hitsat-one for 3-hop MetaQa with a KB, or improvements from 32.8% to 47.2% for Complex Web-Questions with a KB.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Recall of graphs retrieved by PageRank-Nibble compared with PullNet. Left: MetaQA (3-hop). Right: Complex WebQuestions.</figDesc><graphic url="image-1.png" coords="8,308.92,260.10,104.77,147.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Recall of a single round of retrieval with Apache Lucene compared with PullNet. Left: MetaQA (3-hop). Right: Complex WebQuestions.</figDesc><graphic url="image-3.png" coords="8,308.92,569.75,104.77,147.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 shows the recall of question subgraphs on MetaQA (3-hop) questions as training proceeds. Performance of the retrieval components of PullNet converges relatively quickly, with recall saturating after 10-20,000 examples (about 10-20% of a single epoch).</figDesc><graphic url="image-5.png" coords="9,82.91,157.07,196.44,154.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recall of question subgraph on MetaQA 3hop questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of PullNet and GRAFT-Net under wall clock training time.</figDesc><graphic url="image-6.png" coords="9,318.19,86.93,196.44,154.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of all datasets.</figDesc><table><row><cell></cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>MetaQA 1-hop</cell><cell>96,106</cell><cell>9,992</cell><cell>9,947</cell></row><row><cell>MetaQA 2-hop</cell><cell cols="3">118,980 14,872 14,872</cell></row><row><cell>MetaQA 3-hop</cell><cell cols="3">114,196 14,274 14,274</cell></row><row><cell>WebQuestionsSP</cell><cell>2,848</cell><cell>250</cell><cell>1,639</cell></row><row><cell>Complex WebQ</cell><cell>27,623</cell><cell>3,518</cell><cell>3,531</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 shows</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">WebQuestionsSP</cell><cell></cell></row><row><cell></cell><cell>KB</cell><cell>Text</cell><cell>50% KB</cell><cell>50% KB + Text</cell></row><row><cell>KV-Mem*</cell><cell>46.7</cell><cell cols="2">23.2 32.7</cell><cell>31.6</cell></row><row><cell>GraftNet*</cell><cell>66.4</cell><cell cols="2">24.9 48.2</cell><cell>49.7</cell></row><row><cell>PullNet (Ours)</cell><cell>68.1</cell><cell cols="2">24.8 50.3</cell><cell>51.9</cell></row><row><cell>GraftNet</cell><cell>67.8</cell><cell cols="2">25.3 47.7</cell><cell>49.9</cell></row><row><cell>NSM</cell><cell>69.0 (F1)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hits@1 on WebQuestionsSP compared to baseline models. Number below the double line are from original papers: GraftNet<ref type="bibr" target="#b30">(Sun et al., 2018)</ref>, and NSM<ref type="bibr" target="#b18">(Liang et al., 2017)</ref> (which only reports F1 in their paper). * Reimplemented or different data.our results for Complex WebQuestions on the development set. An expected, PullNet shows significant improvement over GRAFT-Net and KV-Mem on all four settings. Once again we see some improvement when pairing the incomplete knowledge base with text, compared to using the incomplete knowledge base only or the text only.</figDesc><table><row><cell></cell><cell cols="4">Complex WebQuestions (dev)</cell></row><row><cell></cell><cell>KB</cell><cell>Text</cell><cell>50% KB</cell><cell>50% KB + Text</cell></row><row><cell>KV-Mem*</cell><cell>21.1</cell><cell>7.4</cell><cell>14.8</cell><cell>15.2</cell></row><row><cell>GraftNet*</cell><cell cols="3">32.8 10.6 26.1</cell><cell>26.9</cell></row><row><cell cols="4">PullNet (Ours) 47.2 13.1 31.5</cell><cell>33.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Complex WebQuestions (test)</cell></row><row><cell></cell><cell cols="3">GoogleSnippet Wikipedia Freebase</cell></row><row><cell>SplitQA</cell><cell>34.2</cell><cell>-</cell><cell>-</cell></row><row><cell>PullNet</cell><cell>29.7</cell><cell>13.8</cell><cell>45.9</cell></row></table><note>Hits@1 on Complex WebQuestions (dev) compared to baseline models. * Reimplemented or different data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Hits@1 on Complex WebQuestions (test).</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We use the same train/dev/test splits as GRAFT-Net<ref type="bibr" target="#b30">(Sun et al., 2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">If two overlapping spans are possible matches, we match only the longer of them.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We use Complex WebQuestions v1.1, where the author re-partition the train/dev/test data to prevent the leakage of information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">https://github.com/OceanskySun/ GraftNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">6.7% questions in Wikimovies are ambiguous, e.g. questions about movies with remakes without specifying years.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">  7  Another way to perform QA in the incomplete KB plus text setting would be to ensemble two QA systems, one which uses the incomplete KB, and one which uses the text. Although we do not make that comparison here, prior work<ref type="bibr" target="#b30">(Sun et al., 2018)</ref> did show that GRAFT-Net outperforms such "late fusion" approaches.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Annual IEEE Symposium on Foundations of Computer Science (FOCS&apos;06)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">More accurate question answering on freebase</title>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elmar</forename><surname>Haussmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1431" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05851</idno>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Akshay Krishnamurthy, Alex Smola, and Andrew McCallum</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Question answering on knowledge bases and text using universal schema and memory networks</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>CoRR, abs/1704.08384</idno>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Quasar: Datasets for question answering by search and reading</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Ugur Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Facc1: Freebase annotation of clueweb corpora, version 1 (release date 2013-06-26</title>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>format version 1, correction level 0</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic zoom-in network for fast object detection in large images</title>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6926" to="6935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of tac-kbp2014 entity discovery and linking tasks</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Text Analysis Conference (TAC2014)</title>
				<meeting>Text Analysis Conference (TAC2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1333" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Transactions of the Association of Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1017" to="1026" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Phase conductor on multi-layered attentions for machine comprehension</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiguang</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Chikina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10504</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccandless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otis</forename><surname>Gospodnetic</surname></persName>
		</author>
		<title level="m">Lucene in action: covers Apache Lucene 3.0</title>
				<imprint>
			<publisher>Manning Publications Co</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2013 -2013 Conference of the North American Chapter</title>
				<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
	<note>Proceedings of the Main Conference. Association for Computational Linguistics (ACL</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter</title>
				<meeting>the 2013 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural architecture for question answering using a knowledge graph and web corpus</title>
		<author>
			<persName><forename type="first">Uma</forename><surname>Sawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Phraseindexed question answering: A new challenge for scalable document comprehension</title>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="559" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>American Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">R 3: Reinforced ranker-reader for open-domain question answering</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerry</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Evidence aggregation for answer reranking in open-domain question answering</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<idno>CoRR, abs/1711.05116</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08885</idno>
	</analytic>
	<monogr>
		<title level="m">Pontus Stenetorp, and Sebastian Riedel</title>
				<imprint>
			<date type="published" when="2017">2017. 2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Question answering from unstructured text by retrieval and comprehension. Transactions of the Association of Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName><forename type="first">Adams</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09541</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National conference on Artificial intelligence</title>
				<meeting>the Thirteenth National conference on Artificial intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: structured classification with probabilistic categorial grammars</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Variational reasoning for question answering with knowledge graph</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
