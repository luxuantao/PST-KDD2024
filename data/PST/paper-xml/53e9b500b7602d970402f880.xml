<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Adjusting Computation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Umut</forename><forename type="middle">A</forename><surname>Acar</surname></persName>
							<email>umut@tti-c.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Adjusting Computation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A5F1FED3B986C523A02CC6EAF83213C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.3.0 [Programming Languages]: General; D.3.3 [Programming Languages]: Language Constructs and Features General Terms Languages</term>
					<term>Design</term>
					<term>Algorithms</term>
					<term>Performance</term>
					<term>Experimentation Incremental Computation</term>
					<term>Self-adjusting Computation</term>
					<term>Compilers</term>
					<term>Run-Time Systems</term>
					<term>Memory Management</term>
					<term>Computational Geometry</term>
					<term>Scientific Computing</term>
					<term>Machine Learning</term>
					<term>Computational Biology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many applications need to respond to incremental modifications to data. Being incremental, such modification often require incremental modifications to the output, making it possible to respond to them asymptotically faster than recomputing from scratch. In many cases, taking advantage of incrementality therefore dramatically improves performance, especially as the input size increases. As a frame of reference, note that in parallel computing speedups are bounded by the number of processors, often a (small) constant.</p><p>Designing and developing applications that respond to incremental modifications, however, is challenging: it often involves developing highly specific, complex algorithms. Self-adjusting computation offers a linguistic approach to this problem. In selfadjusting computation, programs respond automatically and efficiently to modifications to their data by tracking the dynamic data dependences of the computation and incrementally updating their output as needed. In this invited talk, I present an overview of self-adjusting computation and briefly discuss the progress in developing the approach and some recent advances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Motivation</head><p>Since the early days of computer science, researchers realized that many uses of computer applications are incremental by nature. We often start with some initial input and obtain some initial output. We then repeatedly observe the output, make some small modifications to the input, and re-compute the output. In many cases, such incremental modifications cause only small modifications to the output making it feasible to update the output asymptotically more efficiently than recomputing from scratch when the input is Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. PEPM'09, January <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr">2009</ref>   modified. In some cases, this asymptotic improvement is crucial to making computations tractable in practice.</p><p>Examples of incrementality abound. In some applications, input modifications arise as a result of interaction with external agents. For example, applications that interact with or model the physical world (e.g., robots, traffic control systems, scheduling systems) must respond to modifications in the world as it evolves. Similarly applications that interact with the user must respond to his/her modifications, e.g., in software development, the compiler is invoked repeatedly after the user makes small modifications to the program code. In other examples, input modifications can be inherent to the application. For example, in motion simulation, objects move continuously over time causing the property being computed to change continuously as well. For example, we can simulate the flow of a fluid by modeling its constituent particles and computing properties of the moving particles (e.g., their triangulation) while updating them as points move. Since motion often causes small combinatorial modifications to computed properties, we can often model it as a sequence of incremental modifications.</p><p>I would like to start by going through a "back-of-the-envelope" calculation to illustrate the potential improvements offered by incremental changes. Let's assume that an interesting application takes at least linear time in the size of the input<ref type="foot" target="#foot_0">1</ref> and that we can have it respond to incremental modifications in logarithmic time by performing an incremental update. Figure <ref type="figure" target="#fig_0">1</ref> shows how the times for re-computing from scratch (linear) and updating (logarithmic time) grow with the input size. Since the logarithmic update time is reasonably expected to have a larger constant factor, we consider the constant factors 10 and 100. Note that re-computing fromscratch dominates incremental updates starting at small input sizes (in 100's) and the speedup obtained by incremental updates increases linearly with the input size. For example, when the input exceeds three orders of magnitude so does the speedup. As a frame of reference compare this to parallel computing, were speedups are bounded by the number of processor, often a (small) constant. Note also that re-computing from-scratch is exponentially slower than updating incrementally, i.e., n = 2 log n (Figure <ref type="figure" target="#fig_2">2</ref>). Thus if we can achieve (poly-) logarithmic time incremental update times, then we can hugely speedup computations. This can also make realistic some applications that are otherwise impractical.</p><p>Having realized the potential offered by incrementality, previous work devised techniques for enabling computations respond efficiently to incremental modifications (e.g., see <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> for surveys). Except in some special cases, this requires designing specific algorithms or data structures for remembering and re-using results so that computed properties of interest may be updated efficiently. Until recently no broadly effective general-purpose technique or language was known for developing applications that can interact with changing data. Self-adjusting computation proposes a solution to this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Self-Adjusting Computation</head><p>The goal of self-adjusting computation is to enable the development of software that can take advantage of incremental updates. The approach is language centric: instead of expecting the programmer to develop software for a particular application and a particular class of incremental modifications, we offer a languagebased approach where any program can automatically respond to incremental modifications.</p><p>In self-adjusting computation, programs consist of two components: a self-adjusting core and a top-or meta-level mutator. The self-adjusting core performs a single run of the intended application with fixed, unchanging input data. The mutator drives the selfadjusting core by supplying the initial input and by subsequently modifying the input or other computation data. The mutator can modify computation data in a variety of forms depending on the application. For example, it can insert a new object into a set of objects that make up the input to a self-adjusting program, or it can change the outcome of a comparison performed during the execution. After performing such modifications, the mutator can update the output and the computation by requesting change propagation to be performed. Change propagation automatically updates the computation and the output by propagating the data modifications through the core.</p><p>By supplying an automatic change-propagation mechanism, self-adjusting computation shifts the burden of designing and implementing an incremental-update mechanism from the programmer to the language designer and implementor. The language designer in turns needs to answer a number of questions:</p><p>1. Can we design a general-purpose mechanism that can guarantee efficient response times under a broad range application domains?</p><p>2. What should be the primitives of the language for writing selfadjusting programs?</p><p>3. Can we compile self-adjusting programs efficiently?</p><p>4. Can we provide a cost model for reasoning about the asymptotic complexity of self-adjusting programs?</p><p>5. Can we extend existing languages to support self-adjusting computation?</p><p>6. Can the approach be effective in practice, e.g., compared to ad hoc approaches?</p><p>I give an overview of the advances that my collaborators and I have made in answering these questions and point out some of the remaining challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DDGs, Memoization, and Change Propagation</head><p>To update computations efficiently, we represent executions of programs with dynamic dependence graphs or DDGs for short. At a high-level, DDGs record all changing data, input and intermediate, in a computation and the dependences between the data and the code that use the data. Given the DDG of an execution of a program P with some inputs I and modified input I , change propagation updates the DDG as if P is executed from-scratch with the modified input I . To achieve this, change propagation finds and re-executes the code that depend on the modified data. Since reexecuting code can modify other data, change propagation continues until all modifications are processed.</p><p>DDGs and change propagation were proposed in a paper in 2002 <ref type="bibr" target="#b4">[5]</ref>. The same paper proposed language facilities for writing so called adaptive programs that can automatically respond to modifications via change propagation. The key idea behind these facilities is the notion of modifiable references which hold data that changes over time. The language facilities enable tracking dependences selectively by providing a particular interface to modifiables that makes explicit the dependences between modifiable data and the code. More specifically, the mutator can update only the contents of modifiables, and the contents of modifiables can be accessed only within the scope of a special read primitive that can "return" a value only by writing to another modifiable. With these restrictions, change propagation can be performed by recording in the DDG only the operations on modifiable references. This selective tracking of dependences is critical to ensure that the approach can be supported efficiently in practice.</p><p>A major limitation of DDGs is that change propagation can re-execute code unnecessarily, because re-executed code cannot re-use previously performed computations. Subsequent work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> remedy this limitation by integrating memoization with DDGs and change propagation by showing that they are essentially duals that, when combined, provide for efficient re-use. Due to a number of conflicting requirements between memoization and DDGs, (e.g., memoization requires purely functional code whereas propagation imperatively updates memory), this integration is conceptually complex. We propose a particular technique for integrating them and prove it correct by providing mechanically checked proofs <ref type="bibr" target="#b9">[10]</ref>. We prove that the proposed approach is also efficient <ref type="bibr" target="#b3">[4]</ref>. The term "self-adjusting computation" was first used to refer to computations that use this approach to update computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Effects</head><p>Initial work on self-adjusting computation assumed a purely functional setting: the core would not be allowed to side-effect the memory or perform other effectful computations such as writing to a file. The difficulty with effects is that change propagation can skip them because it only re-executes parts of the computation-thus, the se-mantics differs from that of a from-scratch run. This do not seem to be a problem for certain benign effects such as writing to the screen, but not for other effects such as writing to a file or updating memory.</p><p>Follow up work generalized self-adjusting computation primitives to support modifiable references that can be updated imperatively in the core <ref type="bibr" target="#b2">[3]</ref>. (Note that the mutator is always allowed to update modifiables destructively.) The idea is to record different versions (contents) that a modifiable may have and track dependences at the granularity of versions instead of modifiables themselves. We proposed techniques for performing dependence tracking and change propagation in the presence of memory effects but there appears to be room for asymptotic improvements (currently they can cause a logarithmic time slowdown). Apart from memory effects, the interaction between self-adjusting computation and other effects such as I/O is not well understood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Language primitives</head><p>Although self-adjusting computation can be applied without having to change existing code by tracking all data and all dependences between code and data <ref type="bibr" target="#b1">[2]</ref>, this is prohibitively expensive in practice. Early work on self-adjusting computation therefore proposed primitives for operating on modifiable references and for memoizing functions to enable tracking dependences selectively <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>. These primitives effectively require the programmer to mark explicitly the dependences between code and data.</p><p>These proposals suffer from several major limitations: 1) they can require significantly restructuring existing code, 2) they can make reasoning about performance and complexity difficult (there are many ways to write a program with significantly different performance), 3) they are higher-order, which makes them unsuitable for lower level languages such as C or Java. In the past several years, we therefore have been developing an alternative, compilation-based approach that eliminates the difference between self-adjusting computation primitives and the primitives for conventional imperative languages <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref>. The idea is to allow the programmer to operate on modifiable references just like conventional references by reading/dereferencing and writing them without restrictions. The approach shifts the burden of finding the dependences between code and data from the programmer to the compiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Compilation</head><p>To compile a conventional (imperative) program into a self-adjusting program, we need to find the part of the code that depends on each use of a (modifiable) references contents. One idea is to use the continuation<ref type="foot" target="#foot_1">2</ref> of each dereference operation as a conservative approximation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>. Continuations, however, approximate actual dependences coarsely: a small modification can require change propagation to re-execute a continuation completely, which extends to the end of the computation. We therefore memoize continuations themselves and devise a technique for memoizing functions with continuation arguments such that they can be re-used even if their continuation arguments do not match. We make this intuitive description precise by formulating a program translation, based on a continuation-passing-style (cps) transform, for compiling conventional imperative programs with first-class mutable references into self-adjusting programs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>. It is likely that the proposed translation is optimal for a reasonably large class of computations, i.e., it enables re-use of computation as much as possible, but we have no proof of this.</p><p>Continuations and cps-translation come naturally in languages such as ML or Haskell, but they are cumbersome in imperative languages, e.g., in C. We have therefore been developing static analyses for identifying dependences between code and data directly from program code <ref type="bibr" target="#b17">[18]</ref>. At a high level, the idea is to delimit the part of the program code that depends on each access to a modifiable and functionalize it by creating a function from the delimited code. Change propagation uses these functions to update the computation when the contents of a modifiable changes. Since this code transformation re-organizes the code by altering the control flow, care must be taken to ensure that the translation does not change the semantics of the program itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Implementations</head><p>There are two major ongoing efforts in realizing self-adjusting computation by extending the SML and the C languages. These extended languages called SaSML and CEAL (respectively) use the described compilation techniques (based on continuations or static analysis respectively) to compile conventional programs written in a natural style into self-adjusting programs. We have implemented (prototype) compilers for these languages. For SaSML <ref type="bibr" target="#b20">[21]</ref>, we extended the MLton compiler <ref type="bibr" target="#b0">[1]</ref>. For CEAL <ref type="bibr" target="#b17">[18]</ref>, we developed a compiler by using the CIL framework <ref type="bibr" target="#b22">[23]</ref>. The CEAL language performs very efficient automatic garbage collection by integrating garbage collection and change propagation <ref type="bibr" target="#b16">[17]</ref>. These implementations can incur moderate overheads (in SaSML, overhead sometimes exceeds an order of magnitude, in CEAL it is less than a factor of five) when re-executing from scratch but can respond to changes asymptotically faster (often by a linear factor) than recomputing from scratch. In practice, we measure orders of magnitude speedups (we measured up to 6 orders).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Cost Model</head><p>Since self-adjusting programs are written like conventional programs but can respond to data modifications automatically via change propagation, their run-time behavior is quite different than their conventional interpretation would imply. It is therefore critical to provide a cost model that makes it possible for the programmer to reason about the efficiency of self-adjusting programs under incremental modifications. Before the development of compilation techniques, this was difficult because the previously proposed language facilities could obscure the run-time behavior. But now this goal is within reach. In recent work, we develop a notion of derivation or trace distance for analyzing the asymptotic complexity of self-adjusting programs under change propagation <ref type="bibr" target="#b19">[20]</ref>. The idea is to compare the derivations for from-scratch runs of the program with differing inputs and compute the distance between them. We then show that change propagation takes time proportional to this distance.</p><p>An interesting aspect of the proposed cost semantics is that it allows reasoning about asymptotic complexity by making it possible to generalize and compose distances-since languagebased cost models traditionally consider concrete evaluations, this is generally difficult. This work is still a first step. The cost model is nondeterministic-there are many distances between two derivations-and the implementation (the run-time system) does not necessarily choose the smallest distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Applications</head><p>To evaluate the effectiveness of self-adjusting computation, we have been investigating its applications to several problem domains. I review some of our work in the areas of computational geometry &amp; scientific computing, and machine learning &amp; computational biology.</p><p>Many scientific computing applications such as physical simulations requiring computing various geometric properties of changing data, e.g., a blood flow simulation requires computing various forces between molecules as they flow through the veins. In such applications, it is critical, for efficiency reasons, to update incrementally the computed properties. Unfortunately, these applications often require sophisticated algorithms making ad hoc incrementalization difficult. Thus many of these problems remain unsolved from a theoretical perspective and/or pose significant implementation challenges. Motivated by these examples, we have considered a number of fundamental computational-geometry problems such as computation of convex hulls and triangulations in both 2 and 3 dimensions. Our work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref> show that selfadjusting programs for various computational geometry problems respond to incremental modifications due to motion or external actions by a near linear factor faster than recomputing from scratch, yielding orders of magnitude speedups in practice. Using our approach we have also made progress on some difficult problems such as motion simulation of three-dimensional convex hulls <ref type="bibr" target="#b6">[7]</ref> that has remained open for nearly a decade.</p><p>As another major application area we have been investigating the problem of incrementally updating statistical or Bayesian inferences under modifications to the assumed models. Statistical inference is a fundamental problem in machine learning that has broad applications. Our original motivation was to facilitate modifying the structure of proteins or other molecules to help enhance scientific understanding of their crucial functions. We show that statistical inferences can be updated under incremental modifications to underlying models in logarithmic time (as opposed to the linear time required by a from-scratch run) in the number of nodes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. This problem has been previously formulated but remained open except for simple modifications that do not change the structure of the model. Our experiments with actual protein data show that these approaches can speedup the computation of various biological properties of proteins (total energy, three-d confirmation) by as much as an order of magnitude. Some of this work is based on our earlier work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> that showed that self-adjusting version of an algorithm for computing properties of trees <ref type="bibr" target="#b21">[22]</ref> is competitive with optimal algorithms for incremental updating of tree computations <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Status and Future Work</head><p>In the past five years, we have developed some of the foundations for self-adjusting computation and investigated its practical effectiveness by extending existing languages and considering some applications. We can now compile (imperative or purely functional) programs written in a conventional, direct style to self-adjusting programs. Our compilers produce code that incurs moderate overheads when running from scratch but can respond to incremental updates (the common case) very quickly often by orders of magnitude faster than recomputing from scratch. For some of these application, we can also prove that self-adjusting computation closely matches best known or optimal bounds, even for problems that resisted ad hoc approaches. In the future, we plan to explore a number of problems, which I briefly mention below.</p><p>Language Design. We need a more expressive, type safe meta language. The current meta language is not, e.g., it allows the mutator to reference data that may become invalid, or garbage, during change propagation. In addition, the language does not allow self-adjusting computations to be treated as first class values or establish interesting connections between them, other than treating them as one large computation.</p><p>We need a better understanding of input/output (I/O) and how to support it efficiently. Existing languages do not allow I/O, though in practice, printing to the screen seems to "work." Currently, our languages require programmer guidance to ensure efficient change propagation: the programmer must provide reasonably fine-grained hints indicating how memory locations should be re-used during change propagation, because the choice of locations affect computation re-use. We wish to reduce the need for programmer provided hints by expanding the run-time system and the language.</p><p>Language Implementation. Our existing implementations target a high-level language, SML, and a low-level language C. The C implementation is particularly efficient. But both implementations leave a lot of room for further optimizations-except for a few basic optimizations they support none. One particularly interesting optimization is to allow the run-time system to control the granularity of dependence tracking so that we can trade time for space. Self-adjusting computation makes it critical to support automatic memory management, because during change propagation allocated objects may become unreachable/garbage making explicit memory management difficult. Our C implementation provides for efficient garbage collection by integrating it with change propagation. The technique, however, is conservative: it can treat memory objects live even if they are not.</p><p>The SML implementations use the memory management facilities of their host compiler (SML/NJ or MLton). Since selfadjusting computation violates some critical assumptions made by conventional approaches to garbage collection, with both compilers we observe that performance degrades significantly as the resident-size of the computation approaches the size of the physical memory. We need efficient techniques to support mutable references, which, in the current design, can require non-constant overheads, and an evaluation of the effectiveness of the approach with highly imperative programs.</p><p>Algoritmic Aspects and Cost Semantics. Our recent work shows that self-adjusting computation can yield optimal update times for a range of applications even for problems that resist ad hoc approaches. These results indicate that it may be possible to study incremental problems more systematically. They are, however, somewhat indirect: they often analyze self-adjusting programs by treating them as algorithms integrated with change propagation. Our recent proposal on cost semantics alleviates some of these concerns by facilitating source-level reasoning. It does not, however, provide a provably efficient implementation that can guarantee optimal response times. The approach suggests that it may be possible to provide a fully formal cost semantics for algorithmic analysis, e.g., making it possible for mechanized theorem provers or other mechanized tools to aid in analyzing the complexity of self-adjusting programs.</p><p>Parallelism. Parallel programs tend to be amenable to self-adjusting computation (in ways that are not fully understood) suggesting that it may be possible to reap the benefits of both. Except for some preliminary results <ref type="bibr" target="#b15">[16]</ref>, it is not known how selfadjusting computation may be made to work in the parallel setting. Also, it might be possible for self-adjusting computation to aid in parallelization of effectful programs by allowing change propagation to update computations instead of aborting due to side effects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Linear time versus logarithmic time (linear scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Linear time versus logarithmic time (logarithmic scale).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This assumption is justified because an interesting application should at least inspect all of its input (thus requiring linear time).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Intuitively, a continuation at a point in the computation is a closed function that represents the rest of the computation.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://mlton.org/" />
		<title level="m">MLton</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Self-Adjusting Computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><surname>Acar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imperative selfadjusting computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amal</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><surname>Blume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual ACM Symposium on Principles of Programming Languages (POPL)</title>
		<meeting>the 25th Annual ACM Symposium on Principles of Programming Languages (POPL)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An experimental analysis of self-adjusting computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanat</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName><surname>Tangwongsan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
		<meeting>the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Functional Programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual ACM Symposium on Principles of Programming Languages</title>
		<meeting>the 29th Annual ACM Symposium on Principles of Programming Languages</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamizing static algorithms with applications to dynamic trees and history independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">L</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maverick</forename><surname>Vittes</surname></persName>
		</author>
		<author>
			<persName><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM Symposium on Discrete Algorithms (SODA)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust Kinetic Convex Hulls in 3D</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanat</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duru</forename><surname>Tangwongsan</surname></persName>
		</author>
		<author>
			<persName><surname>Türkoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual European Symposium on Algorithms (ESA)</title>
		<meeting>the 16th Annual European Symposium on Algorithms (ESA)</meeting>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kinetic Algorithms via Self-Adjusting Computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanat</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">L</forename><surname>Tangwongsan</surname></persName>
		</author>
		<author>
			<persName><surname>Vittes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual European Symposium on Algorithms (ESA)</title>
		<meeting>the 14th Annual European Symposium on Algorithms (ESA)</meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
			<biblScope unit="page" from="636" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An experimental analysis of change propagation in dynamic trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">L</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><surname>Vittes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Algorithm Engineering and Experimentation (ALENEX)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A consistent semantics of self-adjusting computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName><surname>Donham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual European Symposium on Programming (ESOP)</title>
		<meeting>the 16th Annual European Symposium on Programming (ESOP)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Maintaining well-spaced point sets under dynamic changes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanat</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duru</forename><surname>Tangwongsan</surname></persName>
		</author>
		<author>
			<persName><surname>Türkoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-11">November 2008</date>
		</imprint>
	</monogr>
	<note>In preparation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive Bayesian Inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramgopal</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Özgür</forename><surname>Mettu</surname></persName>
		</author>
		<author>
			<persName><surname>Sümer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive Inference on General Graphical Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramgopal</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Özgür</forename><surname>Mettu</surname></persName>
		</author>
		<author>
			<persName><surname>Sümer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algorithmic issues in modeling motion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pankaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sariel</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hershberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lydia</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Kavraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Koehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mirtich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisha</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Sacks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhash</forename><surname>Snoeyink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ouri</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><surname>Wolefson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="550" to="572" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Algorithms and Theory of Computation Handbook, chapter 8</title>
		<author>
			<persName><forename type="first">David</forename><surname>Eppstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zvi</forename><surname>Galil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><forename type="middle">F</forename><surname>Italiano</surname></persName>
		</author>
		<editor>Mikhail J. Atallah</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
	<note>Dynamic graph algorithms</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A proposal for parallel self-adjusting computation</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><forename type="middle">A</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwar</forename><surname>Ghuloum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAMP &apos;07: Proceedings of the first workshop on Declarative Aspects of Multicore Programming</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Memory management for self-adjusting computation</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><forename type="middle">A</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMM &apos;08: Proceedings of the 7th international symposium on Memory management</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">CEAL: A C-based language for self-adjusting computation</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><forename type="middle">A</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-11">November 2008</date>
			<publisher>Toyota Technological Institute</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dynamic Mesh Refinement</title>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Hudson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-12">December 2007</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University Computer Science Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A cost semantics for self-adjusting computation</title>
		<author>
			<persName><forename type="first">Ruy</forename><surname>Ley-Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><forename type="middle">A</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Fluet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual ACM Symposium on Principles of Programming Languages</title>
		<meeting>the 26th Annual ACM Symposium on Principles of Programming Languages</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compiling self-adjusting programs with continuations</title>
		<author>
			<persName><forename type="first">Ruy</forename><surname>Ley-Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Fluet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><forename type="middle">A</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Functional Programming (ICFP)</title>
		<meeting>the International Conference on Functional Programming (ICFP)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parallel tree contraction and its application</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Reif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 26th Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="487" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cil: Intermediate language and tools for analysis and transformation of C programs</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">C</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mcpeak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Westley</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Compiler Construction</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Categorized Bibliography on Incremental Computation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual ACM Symposium on Principles of Programming Languages (POPL)</title>
		<meeting>the 20th Annual ACM Symposium on Principles of Programming Languages (POPL)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="502" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A data structure for dynamic trees</title>
		<author>
			<persName><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sleator</surname></persName>
		</author>
		<author>
			<persName><surname>Endre Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="362" to="391" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
