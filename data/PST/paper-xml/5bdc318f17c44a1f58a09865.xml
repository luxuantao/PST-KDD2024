<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Road Extraction from High-Resolution Remote Sensing Imagery Using Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-09-13">13 September 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongyang</forename><surname>Xu</surname></persName>
							<email>yongyangxu@cug.edu.cn</email>
							<idno type="ORCID">0000-0001-7421-4915</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhong</forename><surname>Xie</surname></persName>
							<email>xiezhong@cug.edu.cn</email>
							<idno type="ORCID">0000-0001-7421-4915</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center of Geographic Information System</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaxing</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhanlong</forename><surname>Chen</surname></persName>
							<email>chenzl@cug.edu.cn</email>
							<idno type="ORCID">0000-0001-6373-3162</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center of Geographic Information System</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Road Extraction from High-Resolution Remote Sensing Imagery Using Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-13">13 September 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">2AABBCF94290886BAC66352DC002997B</idno>
					<idno type="DOI">10.3390/rs10091461</idno>
					<note type="submission">Received: 20 August 2018; Accepted: 11 September 2018;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>road network extraction</term>
					<term>deep learning</term>
					<term>pyramid attention</term>
					<term>global attention</term>
					<term>high resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The road network plays an important role in the modern traffic system; as development occurs, the road structure changes frequently. Owing to the advancements in the field of high-resolution remote sensing, and the success of semantic segmentation success using deep learning in computer version, extracting the road network from high-resolution remote sensing imagery is becoming increasingly popular, and has become a new tool to update the geospatial database. Considering that the training dataset of the deep convolutional neural network will be clipped to a fixed size, which lead to the roads run through each sample, and that different kinds of road types have different widths, this work provides a segmentation model that was designed based on densely connected convolutional networks (DenseNet) and introduces the local and global attention units. The aim of this work is to propose a novel road extraction method that can efficiently extract the road network from remote sensing imagery with local and global information. A dataset from Google Earth was used to validate the method, and experiments showed that the proposed deep convolutional neural network can extract the road network accurately and effectively. This method also achieves a harmonic mean of precision and recall higher than other machine learning and deep learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the rapid development of remote sensing technology, high-resolution remote sensing imagery has been widely used in many applications, including disaster management, urban planning, and building footprint extraction <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Roads network play a key role in the development of transportation systems, including the addition of automatic road navigation and unmanned vehicles, and urban planning <ref type="bibr" target="#b3">[4]</ref>, which is important in both industry and daily living. Therefore, developing a new method to extract road networks from high-resolution remote sensing imagery would be beneficial to geographical information systems (GIS) and intelligent transportation systems (ITS) <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Extracting road networks has become one of the main research topics in the field of remote sensing imagery processing, and high-resolution imagery has become an important data source to update roads network in the geospatial database in real-time <ref type="bibr" target="#b7">[8]</ref>.</p><p>The structure of roads is complex and the road segments are irregular; the shadows of trees or buildings on roadsides and the vehicles on the roads can be observed from high resolution imagery <ref type="bibr" target="#b8">[9]</ref>, on the other hand, insufficient context of the roads in the remote sensing imagery is similar with the roof of the buildings. The aforementioned issues make it more difficult to extract the road networks from high-resolution imagery.</p><p>the shadows and vehicles on the road as well as the turning information of the road, and the global information about the roads continuity and the morphological structure should be extracted effectively. A semantic labelling method is required to account for the global and local context and to increase the accuracy of extraction of road networks from remote sensing imagery.</p><p>Considering the feature extractor of densely connected convolutional networks (DenseNet) <ref type="bibr" target="#b36">[37]</ref> is powerful enough, and it can take full advantage of features with less training time <ref type="bibr" target="#b36">[37]</ref>; the symmetric architecture of U-Net is good at semantic segmentation, this study attempts to improve the performance of road extraction from remote sensing imagery based on DenseNet and U-Net. This work defined the local information as detailed local context, such as the shadows of buildings and trees, vehicles on the road and also the turning information of the road. In a sample, the global information is defined as the continuity and the morphological structure of the roads. To extract the local and global information defined above accurately, the local and global feature attention blocks are proposed, which were designed by operating the convolutional and pooling layers (Sections 2.2 and 2.3) and aim to pay attention to the local and global information respectively. They were designed to extract the local and global road information richly and accurately. The method comprised of two steps: first, all the remote sensing imagery, as well as the corresponding ground truth labels, were pre-processing to prepare the dataset to train the designed model. Then, a deep neural network was designed to extract roads from the pre-processed images. The proposed model produced binary maps, where the roads were treated as the foreground and all the other objects were treated as the background. All the challenges have resulted in an improvement in the extraction accuracy of the road network from remote sensing imagery. The major contribution of this work is proposing a new model, which learnt from the symmetric architecture of U-Net and was designed as contracting and expansive. DenseNet feature extractor was used to extract the road features in contracting. Two units, the local and global feature attention modules, were designed in the model of expansive. The proposed architecture defined as global and local attention model based on U-Net and DenseNet (GL-Dense-U-Net). This paper explores a novel supervised learning framework to extract roads from remote sensing imagery, which was confirmed as accurate and effective by experimental results.</p><p>The paper is organized as follows. Section 2 presents the proposed approach. Section 3 describes the experiment results and the parameters. Section 4 is a discussion of the method and Section 5 presents the concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods for Roads Extraction from High Resolution Remote Sensing Imagery</head><p>In this paper, a new deep neural network of image semantic segmentation model was proposed to extract the roads from remote sensing imagery. First, the original remote sensing imagery were pre-processed. To prepare the dataset for training the designed model, all the remote sensing imagery and corresponding ground truth images were clipped by a fixed-size sliding-window. Then, the designed deep neural network GL-Dense-U-Net model was introduced to extract the roads. All the pre-processed samples were treated as the input of the model, and the output of the trained model was the two-category classification maps. The categories were "road" and "others", which represent the road extraction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Structure of Deep Convolution Neural Network</head><p>The proposed model in this paper was designed based on the DenseNet, which has shown good performance in image classification, and is famous for several advantages, such as: alleviating the vanishing-gradient problem in most deep neural networks; being powerful enough to extract features and strengthen the feature propagation during training and evaluation, at the same time, it can encourage feature reuse for classification or semantic segmentation; most importantly, the DenseNet can reduce the number of parameters, which makes it easy to be trained. Meanwhile, encouraged by the performance of the symmetrical structure of U-Net <ref type="bibr" target="#b32">[33]</ref> in semantic segmentation, the GL-Dense-U-Net was designed with two parts (Figure <ref type="figure" target="#fig_0">1</ref>). The first part is designed to extract the features using the DenseNet (top of Figure <ref type="figure" target="#fig_0">1</ref>), known as the contracting part. The second (expansive) part is designed to generate the classification map based on the extracted features at different stages of the contracting part (bottom of Figure <ref type="figure" target="#fig_0">1</ref>). Each box represents the feature map with size of w × h × c existing in the top part of Figure <ref type="figure" target="#fig_0">1</ref>, where w and h represent the width and high, respectively, and c is the channels number of the feature map. To take full advantage of the features in different stages and obtain good performance of road extraction, these two parts were connected by a proposed local attention unit (LAU). features at different stages of the contracting part (bottom of Figure <ref type="figure" target="#fig_0">1</ref>). Each box represents the feature map with size of w × h × c existing in the top part of Figure <ref type="figure" target="#fig_0">1</ref>, where w and h represent the width and high, respectively, and c is the channels number of the feature map. To take full advantage of the features in different stages and obtain good performance of road extraction, these two parts were connected by a proposed local attention unit (LAU). Compared with some other traditional deep convolution neural network structures, for each feature extractor layer of the DenseNet, all the preceding layers were treated as the input of the layer. Therefore, there are L (L + 1)/2 connections in an L-layer instead of only L, as is the case for some traditional structures. In this way, The DenseNet will require fewer parameters during training, because there is no need to re-learn redundant features. At the same time, each layer in the contracting part has access to the gradients which form a loss at both the end of the model and at the beginning of the structure, which improves the flow of information between layers and makes the weights and biases easy to be trained.</p><p>The direct connection pattern is used in the dense block, where all the layers are connected, and this structure improves the information flow between layers. To make sure all the layers in a dense block are with same size, a 3 × 3 convolution with a padding operation is used in the block following the BN layer <ref type="bibr" target="#b38">[38]</ref> and ReLU layer <ref type="bibr" target="#b39">[39]</ref>, which is defined as non-linear transformation Tl(). Any feature map xl can be calculated by the preceding layers, including x0,…xl-1 by Tl(), as follows:</p><formula xml:id="formula_0">    0 1 1 , ,..., l l l x T x x x   (1)</formula><p>where  </p><formula xml:id="formula_1">0 1 1 , ,..., l x x</formula><p>x  is the concatenation operation of all the output layers 0,…l -1.</p><p>As an important operation in the deep neural network, pooling <ref type="bibr" target="#b40">[40]</ref> changes the size of the feature maps, which aids in extracting the information from different levels during training, and acts as a component between the convolution (Conv) layers. To facilitate down-sampling in the DenseNet model and to take full advantage of the architecture, the dense blocks were connected by the pooling operation following a 1 × 1 convolution with padding operation. Compared with some other traditional deep convolution neural network structures, for each feature extractor layer of the DenseNet, all the preceding layers were treated as the input of the layer. Therefore, there are L (L + 1)/2 connections in an L-layer instead of only L, as is the case for some traditional structures. In this way, The DenseNet will require fewer parameters during training, because there is no need to re-learn redundant features. At the same time, each layer in the contracting part has access to the gradients which form a loss at both the end of the model and at the beginning of the structure, which improves the flow of information between layers and makes the weights and biases easy to be trained.</p><p>The direct connection pattern is used in the dense block, where all the layers are connected, and this structure improves the information flow between layers. To make sure all the layers in a dense block are with same size, a 3 × 3 convolution with a padding operation is used in the block following the BN layer <ref type="bibr" target="#b38">[38]</ref> and ReLU layer <ref type="bibr" target="#b39">[39]</ref>, which is defined as non-linear transformation T l (). Any feature map x l can be calculated by the preceding layers, including x 0 , . . . x l-1 by T l (), as follows:</p><formula xml:id="formula_2">x l = T l ([x 0 , x 1 , . . . , x l-1 ])<label>(1)</label></formula><p>where [x 0 , x 1 , . . . , x l-1 ] is the concatenation operation of all the output layers 0, . . . l -1.</p><p>As an important operation in the deep neural network, pooling <ref type="bibr" target="#b40">[40]</ref> changes the size of the feature maps, which aids in extracting the information from different levels during training, and acts as a component between the convolution (Conv) layers. To facilitate down-sampling in the DenseNet model and to take full advantage of the architecture, the dense blocks were connected by the pooling operation following a 1 × 1 convolution with padding operation.</p><p>The architecture of the proposed deep neural convolution network is symmetrical. The expansive part is used to recover the road networks from feature maps extracted by contracting part. Every dense block in the contracting part corresponds to a local attention unit (LAU) and a global attention unit (GAU) in the expansive part, where a LAU is designed to extract the roads local information from remote sensing imagery during the different neural network training stages. GAUs are used to extract the global road information when recovering the information from deep level feature maps. The designed extraction model is end-to-end, and the size of the output is the same as the input remote sensing image. Therefore, some up-sampling operations are required in the expansive part. During expanding, a LAU and a GAU are regarded as a group, which is followed by a deconvolution layer <ref type="bibr" target="#b41">[41]</ref> to enlarge the size of the feature maps. At the end of the model, a 1 × 1 convolution is used to map the feature maps into two classes: road and non-road; the problem of extracting roads can be resolved by binary classification. Training the deep neural convolution network is a process of minimizing the energy function via the gradient descent <ref type="bibr" target="#b42">[42]</ref>. Because the output of the softmax function can be used to represent a categorical distribution, the energy function of model is defined as the cross-entropy between the estimated class probabilities and the "true" distribution. To train the convolutional neural layers and classifier coherently, soft-max layer and cross entropy <ref type="bibr" target="#b32">[33]</ref> were used in the network, where the soft-max layer is defined to calculate the classification probability, as follows:</p><formula xml:id="formula_3">p i = exp(a i ) K ∑ k=1 exp(a k )<label>(2)</label></formula><p>where p i represents the probability that a pixel is predicted to belong to class i. Because the images in this work are classified into the foreground and background, the number of classes K is set as 2 in the experiment. a is the output of the last layer in the model. In this work, the energy function can be defined as follows:</p><formula xml:id="formula_4">E = - 1 N N ∑ n=1 K ∑ i=1 [y n i ln p n i + (1 -y n i ) ln(1 -p n i )]<label>(3)</label></formula><p>where, N represents the number of samples in training dataset, y is the expected output and p is the probability mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Local Attention Unit</head><p>Inspired by pyramid feature maps extraction, the local attention unit was designed to provide precise pixel-level attention to the feature maps extracted by DenseNet from deep levels. Benefitting from the special structure, pyramid pooling can extract information from different scale feature maps; at the same time, this design method helps to increase the receptive field, and is widely used in semantic segmentation <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44]</ref>. However, the pyramid structure does not give significant attention to the global context information, and the channel-wise attention vector used in the structure is limited to extract pixel-wise information <ref type="bibr" target="#b45">[45]</ref>.</p><p>Considering the analysis above, in order to extract the local pixel-level information of road networks from remote sensing, the LAU is designed to fuse different scale feature maps and draw attention to the pixel-level information from deep-level of the DenseNet. To improve the performance of the LAU in extracting information from different scale feature maps, this paper applied four different convolution operations with kernel sizes of 1 × 1, 3 × 3, 5 × 5, 7 × 7. The features are integrated by the LAU from bottom to top in a stepwise fashion (Figure <ref type="figure" target="#fig_2">2</ref>), in this way, the context information from neighboring scales can be incorporated precisely. At the top of the LAU, the 1 × 1 convolution is designed to be multiplied pixel-wise by the feature information extracted from bottom convolution operations. The pyramid structure to fuse different scale information, while the pixel-wise multiplication allows for better extraction of local pixel-level information for road extraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Global Attention Unit</head><p>A road is a continuum, and so runs through all the images. Therefore, the global information is important in the expansive part of road extraction from remote sensing imagery. There are some semantic segmentation models designed for this part using bilinearly up-sampling to directly generate the results <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b47">47]</ref>. The one-step decoder is limited to recovering location information and will lose some global road features due to a lack of different scale low level feature-maps.</p><p>Encouraged by recent research, in which the contracting part was combined with the pyramid structure to improve the performance of semantic segmentation, the global attention unit (GAU) was designed in the expansive part. This GAU introduces global average pooling (GAP) into the unit to extract global road information, which is connected to the result of deconvolution and is used as a guide to recover the information (Figure <ref type="figure" target="#fig_3">3</ref>). In detail, firstly, a 1 × 1 convolution and a dense block, which corresponds with the block in contracting part, are applied to operate the feature maps from a high-level; then, the features are operated in two ways, one using a deconvolution layer, and the other employing a GAP operation followed by a 1 × 1 convolution and deconvolution. Finally, features from the two methods are added together as the output of the GAU. This proposed unit considers the feature maps from both low-levels and high-levels, effectively, and provides the global information to guide feature recovery in the expansive part of the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The high-resolution remote sensing imagery collected by Cheng, et al. <ref type="bibr" target="#b48">[48]</ref> from screenshots taken from Google Earth <ref type="bibr" target="#b49">[49]</ref> were used in this work. The ground truth was manually labeled by the reference maps, and the dataset is publicly available. There are 224 images in the dataset, and at least 600 × 600 pixels in each image. The spatial resolution of every pixel in the remote sensing imagery is 1.2 m as description in <ref type="bibr" target="#b48">[48]</ref>. The dataset covers urban, suburban and rural regions. There are waters, mountains and hills and lands covered by the vegetation. Most original images are under complex grounds, which make the road extraction task very challenging. The ground truth images have two kinds of pixels: including road and unknown (clutter); the road widths in the ground truth images are about 12-15 pixels (Figure <ref type="figure" target="#fig_8">4</ref>). To avoid the network learns using pixels that are reserved to the independent testing dataset, all the samples and ground truth images were divided into two parts randomly, where eighty percent were used to train the GL-Dense-U-Net model and twenty percent were used to validate the trained model. The whole validating dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Global Attention Unit</head><p>A road is a continuum, and so runs through all the images. Therefore, the global information is important in the expansive part of road extraction from remote sensing imagery. There are some semantic segmentation models designed for this part using bilinearly up-sampling to directly generate the results <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b47">47]</ref>. The one-step decoder is limited to recovering location information and will lose some global road features due to a lack of different scale low level feature-maps.</p><p>Encouraged by recent research, in which the contracting part was combined with the pyramid structure to improve the performance of semantic segmentation, the global attention unit (GAU) was designed in the expansive part. This GAU introduces global average pooling (GAP) into the unit to extract global road information, which is connected to the result of deconvolution and is used as a guide to recover the information (Figure <ref type="figure" target="#fig_3">3</ref>). In detail, firstly, a 1 × 1 convolution and a dense block, which corresponds with the block in contracting part, are applied to operate the feature maps from a high-level; then, the features are operated in two ways, one using a deconvolution layer, and the other employing a GAP operation followed by a 1 × 1 convolution and deconvolution. Finally, features from the two methods are added together as the output of the GAU. This proposed unit considers the feature maps from both low-levels and high-levels, effectively, and provides the global information to guide feature recovery in the expansive part of the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Global Attention Unit</head><p>A road is a continuum, and so runs through all the images. Therefore, the global information is important in the expansive part of road extraction from remote sensing imagery. There are some semantic segmentation models designed for this part using bilinearly up-sampling to directly generate the results <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b47">47]</ref>. The one-step decoder is limited to recovering location information and will lose some global road features due to a lack of different scale low level feature-maps.</p><p>Encouraged by recent research, in which the contracting part was combined with the pyramid structure to improve the performance of semantic segmentation, the global attention unit (GAU) was designed in the expansive part. This GAU introduces global average pooling (GAP) into the unit to extract global road information, which is connected to the result of deconvolution and is used as a guide to recover the information (Figure <ref type="figure" target="#fig_3">3</ref>). In detail, firstly, a 1 × 1 convolution and a dense block, which corresponds with the block in contracting part, are applied to operate the feature maps from a high-level; then, the features are operated in two ways, one using a deconvolution layer, and the other employing a GAP operation followed by a 1 × 1 convolution and deconvolution. Finally, features from the two methods are added together as the output of the GAU. This proposed unit considers the feature maps from both low-levels and high-levels, effectively, and provides the global information to guide feature recovery in the expansive part of the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The high-resolution remote sensing imagery collected by Cheng, et al. <ref type="bibr" target="#b48">[48]</ref> from screenshots taken from Google Earth <ref type="bibr" target="#b49">[49]</ref> were used in this work. The ground truth was manually labeled by the reference maps, and the dataset is publicly available. There are 224 images in the dataset, and at least 600 × 600 pixels in each image. The spatial resolution of every pixel in the remote sensing imagery is 1.2 m as description in <ref type="bibr" target="#b48">[48]</ref>. The dataset covers urban, suburban and rural regions. There are waters, mountains and hills and lands covered by the vegetation. Most original images are under complex grounds, which make the road extraction task very challenging. The ground truth images have two kinds of pixels: including road and unknown (clutter); the road widths in the ground truth images are about 12-15 pixels (Figure <ref type="figure" target="#fig_8">4</ref>). To avoid the network learns using pixels that are reserved to the independent testing dataset, all the samples and ground truth images were divided into two parts randomly, where eighty percent were used to train the GL-Dense-U-Net model and twenty percent were used to validate the trained model. The whole validating dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The high-resolution remote sensing imagery collected by Cheng, et al. <ref type="bibr" target="#b48">[48]</ref> from screenshots taken from Google Earth <ref type="bibr" target="#b49">[49]</ref> were used in this work. The ground truth was manually labeled by the reference maps, and the dataset is publicly available. There are 224 images in the dataset, and at least 600 × 600 pixels in each image. The spatial resolution of every pixel in the remote sensing imagery is 1.2 m as description in <ref type="bibr" target="#b48">[48]</ref>. The dataset covers urban, suburban and rural regions. There are waters, mountains and hills and lands covered by the vegetation. Most original images are under complex grounds, which make the road extraction task very challenging. The ground truth images have two kinds of pixels: including road and unknown (clutter); the road widths in the ground truth images are about 12-15 pixels (Figure <ref type="figure" target="#fig_8">4</ref>). To avoid the network learns using pixels that are reserved to the independent testing dataset, all the samples and ground truth images were divided into two parts randomly, where eighty percent were used to train the GL-Dense-U-Net model and twenty percent were used to validate the trained model. The whole validating dataset were clipped into 256 × 256 non-overlapping samples. To enhance the training samples, all the original images, as well as the corresponding ground truth images, were clipped by a 256 × 256 sliding window with a stride of 64 pixels. To increase the size of our dataset and avoid padding or null values, all the clipped square samples were rotated by 90 were clipped into 256 × 256 non-overlapping samples. To enhance the training samples, all the original images, as well as the corresponding ground truth images, were clipped by a 256 × 256 sliding window with a stride of 64 pixels. To increase the size of our dataset and avoid padding or null values, all the clipped square samples were rotated by 90°, 180° and 270°. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Setup and Results</head><p>All the prepared samples, as well as the ground truth images, were treated as inputs to train the proposed GL-Dense-U-Net model. The architecture and the parameters, including the size of every block and number of blocks, etc., used in this work are shown in Figure <ref type="figure" target="#fig_0">1</ref> Adam (Adaptive Moment Estimation), one of the most commonly used algorithms <ref type="bibr" target="#b50">[50]</ref>, was treated as the network optimizer to minimize the losses and update the parameters, including weights, biases, and so on. To obtain a better performance and speed up the processing, during training, the learning rate of the GL-Dense-U-Net model was set to 0.001, and was divided by 10 every 10,000 iterations.</p><p>Edges are important for roads extracting from the remote sensing images. Edge enhancement is designed to reduce the noise by a filter and decrease the computation complexity. There are some filters used in edge enhancing including contour filter, detail filter, edge enhance filter and so on. All of these filters have been implemented by some image processing libraries such as python imaging library (PIL) and the OpenCV. In essence, these filters are convolutional filter, they enhance the edges of images by sum-weighted value of the convolution region. It is known that convolutional layers, the extractor of DenseNet designed in the contracting part of proposed model, tend to extract low-level features in the first layer, such as edges <ref type="bibr" target="#b51">[51]</ref>. As shown in Figure <ref type="figure" target="#fig_10">5</ref>, the edges can be extracted clearly, which would play the role of edge enhancement and be beneficial for the roads extraction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Setup and Results</head><p>All the prepared samples, as well as the ground truth images, were treated as inputs to train the proposed GL-Dense-U-Net model. The architecture and the parameters, including the size of every block and number of blocks, etc., used in this work are shown in Figure <ref type="figure" target="#fig_0">1</ref>  <ref type="bibr" target="#b50">[50]</ref>, was treated as the network optimizer to minimize the losses and update the parameters, including weights, biases, and so on. To obtain a better performance and speed up the processing, during training, the learning rate of the GL-Dense-U-Net model was set to 0.001, and was divided by 10 every 10,000 iterations.</p><p>Edges are important for roads extracting from the remote sensing images. Edge enhancement is designed to reduce the noise by a filter and decrease the computation complexity. There are some filters used in edge enhancing including contour filter, detail filter, edge enhance filter and so on. All of these filters have been implemented by some image processing libraries such as python imaging library (PIL) and the OpenCV. In essence, these filters are convolutional filter, they enhance the edges of images by sum-weighted value of the convolution region. It is known that convolutional layers, the extractor of DenseNet designed in the contracting part of proposed model, tend to extract low-level features in the first layer, such as edges <ref type="bibr" target="#b51">[51]</ref>. As shown in Figure <ref type="figure" target="#fig_10">5</ref>, the edges can be extracted clearly, which would play the role of edge enhancement and be beneficial for the roads extraction.</p><p>were clipped into 256 × 256 non-overlapping samples. To enhance the training samples, all the original images, as well as the corresponding ground truth images, were clipped by a 256 × 256 sliding window with a stride of 64 pixels. To increase the size of our dataset and avoid padding or null values, all the clipped square samples were rotated by 90°, 180° and 270°. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Setup and Results</head><p>All the prepared samples, as well as the ground truth images, were treated as inputs to train the proposed GL-Dense-U-Net model. The architecture and the parameters, including the size of every block and number of blocks, etc., used in this work are shown in Figure <ref type="figure" target="#fig_0">1</ref> Adam (Adaptive Moment Estimation), one of the most commonly used algorithms <ref type="bibr" target="#b50">[50]</ref>, was treated as the network optimizer to minimize the losses and update the parameters, including weights, biases, and so on. To obtain a better performance and speed up the processing, during training, the learning rate of the GL-Dense-U-Net model was set to 0.001, and was divided by 10 every 10,000 iterations.</p><p>Edges are important for roads extracting from the remote sensing images. Edge enhancement is designed to reduce the noise by a filter and decrease the computation complexity. There are some filters used in edge enhancing including contour filter, detail filter, edge enhance filter and so on. All of these filters have been implemented by some image processing libraries such as python imaging library (PIL) and the OpenCV. In essence, these filters are convolutional filter, they enhance the edges of images by sum-weighted value of the convolution region. It is known that convolutional layers, the extractor of DenseNet designed in the contracting part of proposed model, tend to extract low-level features in the first layer, such as edges <ref type="bibr" target="#b51">[51]</ref>. As shown in Figure <ref type="figure" target="#fig_10">5</ref>, the edges can be extracted clearly, which would play the role of edge enhancement and be beneficial for the roads extraction.  To assess the quantitative performance of the proposed GL-Dense-U-Net model in road network extraction from remote sensing imagery, the precision (P) and recall (R) <ref type="bibr" target="#b52">[52]</ref> are introduced, as well as the F 1 score <ref type="bibr" target="#b53">[53]</ref> and the overall accuracy (OA) metrics. The F 1 score is calculated by P and R, and it is a powerful evaluation metric for the harmonic mean of P and R, and it can be calculated as follows:</p><formula xml:id="formula_5">F 1 = 2 × P × R P + R<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">P = TP TP + FP , R = TP TP + FN<label>(5)</label></formula><p>Here, R measures the proportion of matched pixels in the ground truth and P is the percentage of matched pixels in the extraction results. TP, FP and FN represent the number of true positives, false positives and false negatives, respectively. OA measures the precision of road and non-road at pixel level and it can be calculated as follows:</p><formula xml:id="formula_7">OA = Pos r + Pos n N<label>(6)</label></formula><p>where Pos r and Pos n represent the positive number for road and non-road at pixel level, and N is the pixels number of test imagery. All the metrics mentioned above can be calculated by the pixel-based confusion matrices <ref type="bibr" target="#b54">[54]</ref>.</p><p>In this work, the proposed GL-Dense-U-Net model was implemented using the open source framework Tensorflow, provided by Google. The code was executed in the Linux platform with four TITAN GPUs (12 GB RAM per GPU). There were 100,000 iterations; the trained model achieved state-of-the-art results (Table <ref type="table" target="#tab_2">1</ref>). Figure <ref type="figure" target="#fig_18">6</ref> shows the changes in accuracy and losses with increasing iterations during the training of the model. To assess the quantitative performance of the proposed GL-Dense-U-Net model in road network extraction from remote sensing imagery, the precision (P) and recall (R) <ref type="bibr" target="#b52">[52]</ref> are introduced, as well as the F1 score <ref type="bibr" target="#b53">[53]</ref> and the overall accuracy (OA) metrics. The F1 score is calculated by P and R, and it is a powerful evaluation metric for the harmonic mean of P and R, and it can be calculated as follows:</p><formula xml:id="formula_8">1 2 P R F P R    <label>(4)</label></formula><p>where</p><formula xml:id="formula_9">TP P TP FP   , TP R TP FN  <label>(5)</label></formula><p>Here, R measures the proportion of matched pixels in the ground truth and P is the percentage of matched pixels in the extraction results. TP, FP and FN represent the number of true positives, false positives and false negatives, respectively. OA measures the precision of road and non-road at pixel level and it can be calculated as follows:</p><formula xml:id="formula_10">r n Pos Pos OA N   (6)</formula><p>where Posr and Posn represent the positive number for road and non-road at pixel level, and N is the pixels number of test imagery. All the metrics mentioned above can be calculated by the pixel-based confusion matrices <ref type="bibr" target="#b54">[54]</ref>.</p><p>In this work, the proposed GL-Dense-U-Net model was implemented using the open source framework Tensorflow, provided by Google. The code was executed in the Linux platform with four TITAN GPUs (12 GB RAM per GPU). There were 100,000 iterations; the trained model achieved state-of-the-art results (Table <ref type="table" target="#tab_2">1</ref>). Figure <ref type="figure" target="#fig_18">6</ref> shows the changes in accuracy and losses with increasing iterations during the training of the model.  The trained GL-Dense-U-Net reached a 97.82% overall accuracy, proving that the deep convolutional neural network performs well in extracting roads from high-resolution remote sensing imagery. To prove that the proposed method works in a generic sense, some data in commercial areas, rural areas, deserts and imagery covered by vegetation were used to validate the proposed method, respectively (Figure <ref type="figure" target="#fig_20">7</ref>). There are five rows and three columns of subfigures. The The trained GL-Dense-U-Net reached a 97.82% overall accuracy, proving that the deep convolutional neural network performs well in extracting roads from high-resolution remote sensing imagery. To prove that the proposed method works in a generic sense, some data in commercial areas, rural areas, deserts and imagery covered by vegetation were used to validate the proposed method, respectively (Figure <ref type="figure" target="#fig_20">7</ref>). There are five rows and three columns of subfigures. The first column represents the original image, the second column represents the corresponding ground truth and the last column is the prediction by the proposed method. The performance of designed method in commercial areas was illustrated by the first row. From the second row to the last row were used to illustrate the results of deserts, rural, tropical and residential areas, separately.</p><p>Remote Sens. 2018, 10, x FOR PEER REVIEW 9 of 16 first column represents the original image, the second column represents the corresponding ground truth and the last column is the prediction by the proposed method. The performance of designed method in commercial areas was illustrated by the first row. From the second row to the last row were used to illustrate the results of deserts, rural, tropical and residential areas, separately.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison of the Proposed Method</head><p>This work uses the Dense Convolutional Network as the feature extractor in the contracting part, which allows direct connections between two layers in a block, while keeping layers in the same block within the same feature map size. During training, the DenseNet can achieve state-of-the-art results with fewer parameters and less computation. The feature extractor follows a simple connectivity rule, which is beneficial to integrating identity maps, deep supervision and diversified depth. In this way, the deep neural convolutional network can consequently learn more information and reduce feature redundancy for road extraction. The designed structure has shown good performance in image classification. To demonstrate the function of the DenseNet in semantic segmentation, this work compared the model which use DenseNet as the feature extractor with the model using a general convolutional neural network as the feature extractor (U-Net). The comparison results were shown in Figure <ref type="figure" target="#fig_22">8</ref>. The recall of U-Net is clear higher than the model using DenseNet as the feature extractor, while the road extraction precision and F1 scores have improved by 10.59% and 4.33%, respectively. By analyzing the results, it is clear that DenseNet is able to extract more road information, allowing more precise pixel classification.</p><p>The proposed GL-Dense-U-Net model in this work is powerful for extracting roads from high-resolution remote sensing imagery. The network extracts the road features via the DenseNet in the contracting part, which shows drastic improvements over previous models. Because the original remote sensing image must to be clipped to train the deep neural convolutional network, this paper designed the LAU and GAU to obtain the local and global road information, while combining multiple scales in different blocks in the expansive part. All of the improvements helped to recover the road by the extracted information and to classify roads of different sizes.</p><p>To evaluate the effectiveness of the proposed GL-Dense-U-Net model on road extraction from remote sensing imagery, this work was compared with some state-of-the-art methods, including the deep learning methods like FCN <ref type="bibr" target="#b42">[42]</ref> and U-Net <ref type="bibr" target="#b32">[33]</ref>. At the same time, the newest deep convolutional neural network DeepLab V3+ <ref type="bibr" target="#b55">[55]</ref> was used to compare the proposed GL-Dense-U-Net model with. To make the comparisons objective and fair, all the of the methods mentioned were tested with the same set of images. The comparison results are exhibited in Table <ref type="table" target="#tab_3">2</ref>, where the best values of each column were in bold font.</p><p>To show the improvements to individual images and the whole dataset, this work chose three sampled images in the first three columns and the all the test dataset in the last column in Table <ref type="table" target="#tab_3">2</ref>. The precision, recall and F1 score were calculated by state-of-the-art methods. From comparing results of different methods (Table <ref type="table" target="#tab_3">2</ref>) we can see that the U-Net model (based on FCN), achieve higher recall values. However, their comprehensive performances are unsatisfying and obtain a lower F1 score. The recall of U-Net is clear higher than the model using DenseNet as the feature extractor, while the road extraction precision and F 1 scores have improved by 10.59% and 4.33%, respectively. By analyzing the results, it is clear that DenseNet is able to extract more road information, allowing more precise pixel classification.</p><p>The proposed GL-Dense-U-Net model in this work is powerful for extracting roads from high-resolution remote sensing imagery. The network extracts the road features via the DenseNet in the contracting part, which shows drastic improvements over previous models. Because the original remote sensing image must to be clipped to train the deep neural convolutional network, this paper designed the LAU and GAU to obtain the local and global road information, while combining multiple scales in different blocks in the expansive part. All of the improvements helped to recover the road by the extracted information and to classify roads of different sizes.</p><p>To evaluate the effectiveness of the proposed GL-Dense-U-Net model on road extraction from remote sensing imagery, this work was compared with some state-of-the-art methods, including the deep learning methods like FCN <ref type="bibr" target="#b42">[42]</ref> and U-Net <ref type="bibr" target="#b32">[33]</ref>. At the same time, the newest deep convolutional neural network DeepLab V3+ <ref type="bibr" target="#b55">[55]</ref> was used to compare the proposed GL-Dense-U-Net model with. To make the comparisons objective and fair, all the of the methods mentioned were tested with the same set of images. The comparison results are exhibited in Table <ref type="table" target="#tab_3">2</ref>, where the best values of each column were in bold font.</p><p>To show the improvements to individual images and the whole dataset, this work chose three sampled images in the first three columns and the all the test dataset in the last column in Table <ref type="table" target="#tab_3">2</ref>. The precision, recall and F 1 score were calculated by state-of-the-art methods. From comparing results of different methods (Table <ref type="table" target="#tab_3">2</ref>) we can see that the U-Net model (based on FCN), achieve higher recall values. However, their comprehensive performances are unsatisfying and obtain a lower F 1 score. U-Net model is robust to occlusions for convolutional operations after up-sampling in the expansive part of the model. Though this method can achieve satisfactory results and improved recall results, some false positives are introduced in some areas, such as the turning of the roads (the region B in Figure <ref type="figure" target="#fig_24">9</ref>), so that the comprehensive performances are unsatisfying. FCN and DeepLab V3+ are sensitive to noise, roads in some regions like the shadows of trees (the region A in Figure <ref type="figure" target="#fig_24">9</ref>) cannot be extracted accurately. Benefited from the global and local attention units, the proposed GL-Dense-U-Net model achieved relatively satisfactory performances in recall and the best values both in precision and F 1 score, therefore, the designed model generally achieves the best result. Specifically, the F 1 score of our designed model is 2.11% higher than the next best compared method (DeepLab V3+ <ref type="bibr" target="#b55">[55]</ref>). The proposed GL-Dense-U-Net model obtained the highest F 1 score and precision in all the separated samples mentioned. All of them demonstrate that the combination of the proposed attention units and the use of DenseNet as the feature extractor in the GL-Dense-U-Net model helped to achieve better performance than other state-of-the-art methods. At the same time, the comparison validated this work in terms of road network extraction from high-resolution remote sensing imagery.  Where P stands for precision, R represents recall and F1 score for the roads extraction, respectively.</p><p>U-Net model is robust to occlusions for convolutional operations after up-sampling in the expansive part of the model. Though this method can achieve satisfactory results and improved recall results, some false positives are introduced in some areas, such as the turning of the roads (the region B in Figure <ref type="figure" target="#fig_24">9</ref>), so that the comprehensive performances are unsatisfying. FCN and DeepLab V3+ are sensitive to noise, roads in some regions like the shadows of trees (the region A in Figure <ref type="figure" target="#fig_24">9</ref>) cannot be extracted accurately. Benefited from the global and local attention units, the proposed GL-Dense-U-Net model achieved relatively satisfactory performances in recall and the best values both in precision and F1 score, therefore, the designed model generally achieves the best result. Specifically, the F1 score of our designed model is 2.11% higher than the next best compared method (DeepLab V3+ <ref type="bibr" target="#b55">[55]</ref>). The proposed GL-Dense-U-Net model obtained the highest F1 score and precision in all the separated samples mentioned. All of them demonstrate that the combination of the proposed attention units and the use of DenseNet as the feature extractor in the GL-Dense-U-Net model helped to achieve better performance than other state-of-the-art methods. At the same time, the comparison validated this work in terms of road network extraction from high-resolution remote sensing imagery.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Local and global road information play important roles for extracting roads from the complex remote sensing imagery. However, most of the road detection methods are limited for information. To improve the performance of road extraction, this work proposes two modules: the local and global attention units, which help to improve the ability to extract local and global information, respectively. Profiting from these powerful units, the precision, recall and the F 1 scores have been significantly improved. To illustrate the effect of the two units, this work compared the results of the proposed model while excluding either the LAU or the GAU; in both cases, the same training dataset was used. All the assessed metrics are shown in Table <ref type="table" target="#tab_6">3</ref>: The OA improved by 1.89% and by 1.38% when the designed model only including the LAU and GAU, respectively. At the same time, the F 1 score for road extraction improved by 1.37% and 1.03% for the models using only the LAU or GAU, respectively. When both the LAU and GAU were designed in the deep convolutional neural network, the OA and F 1 scores did not improve by the addition of the aforementioned results, but did improves slightly over the single attention unit models. The comparison illustrates that some road information in the original remote sensing imagery can be extracted by both of the proposed units during model training, which proves that both of the proposed units have the ability to extract features at different scales.</p><p>Global information plays an important role in extracting the roads from remote sensing imagery, because the roads' structure can run through almost all of the images. GAP operation followed by a 1 × 1 convolution and deconvolution were designed in the GAU, which can extract global information and guide feature recovery. Therefore, the proposed GAU helps the model by improving all the metrics of road extraction. The roads in the remote sensing imagery are complex, which makes it easy to miss the local information during road extraction. The LAU is designed using the pyramid structure to fuse different scales feature-maps and draw attention to the pixel-level information, aids the model in road extraction, as shown in Figure <ref type="figure" target="#fig_27">10</ref>. The results show that the performance is poor in some local structures like A, C (which includes a tree shadow) and B (where the spectral characteristics are similar to the road), when the LAU is excluded from the designed model. It is clear that the designed GL-Dense-U-Net model is powerful enough to extract the local information of the road, and the pixels were classified into different groups correctly.</p><p>As remote sensing technology develops, more satellites are being launched, which make it easier to access high-resolution remote sensing imagery. Semantic segmentation of remote sensing imagery plays an important role in practical applications, such as navigation and urban planning. This work designed a new model, which improved the image classification performance for road extraction. The trained model can be used to extract roads from other remote sensing imagery datasets directly or just fine-tuning. On the other hand, the designed model can be retrained by the datasets of different land uses and then extract their relative information. The model is adapted to imagery with more than 3 bands by modifying the input channels in the first convolutional layer. However, the proposed method is a novel supervised learning framework, and the dataset used in the experiment does not contain the unpaved roads. Therefore, the trained model is invalid for extracting the unpaved roads. The extraction results cannot directly function as vector data for navigation. In the future, a more optimized deep neural network is required to extract the road to a vector format. At the same time, benefited from the performance of this work in extracting local and global information, the semantic segmentation results can be used as additional information, like the road width, for the extracted roads vector data to improve the development of transportation systems. Therefore, future work will focus on how to generate reliable vector road networks using remote sensing imagery with more information, which can be directly used for practical applications. global information, the semantic segmentation results can be used as additional information, like the road width, for the extracted roads vector data to improve the development of transportation systems. Therefore, future work will focus on how to generate reliable vector road networks using remote sensing imagery with more information, which can be directly used for practical applications.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The architecture of the GL-Dense-U-Net used in this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The architecture of the GL-Dense-U-Net used in this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of the local attention unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The architecture of the global attention unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of the local attention unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of the local attention unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The architecture of the global attention unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The architecture of the global attention unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Samples of the remote sensing imagery used in the experiments. The RGB image (a) and the corresponding label (b).</figDesc><graphic coords="7,132.65,154.81,328.80,124.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>. To improve the speed of the training model, this work restored the weights of DenseNet which were pre-trained by the ImageNet dataset for the contracting part. During training the model, an excellent optimization is required to minimize the energy function and update the parameters of the model algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The original image (a) and a part of feature maps (b-d) of first layer of designed model.</figDesc><graphic coords="7,104.54,649.45,385.56,104.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Samples of the remote sensing imagery used in the experiments. The RGB image (a) and the corresponding label (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>. To improve the speed of the training model, this work restored the weights of DenseNet which were pre-trained by the ImageNet dataset for the contracting part. During training the model, an excellent optimization is required to minimize the energy function and update the parameters of the model algorithm. Adam (Adaptive Moment Estimation), one of the most commonly used algorithms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Samples of the remote sensing imagery used in the experiments. The RGB image (a) and the corresponding label (b).</figDesc><graphic coords="7,132.92,125.35,328.80,124.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>. To improve the speed of the training model, this work restored the weights of DenseNet which were pre-trained by the ImageNet dataset for the contracting part. During training the model, an excellent optimization is required to minimize the energy function and update the parameters of the model algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The original image (a) and a part of feature maps (b-d) of first layer of designed model.Figure 5. The original image (a) and a part of feature maps (b-d) of first layer of designed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The original image (a) and a part of feature maps (b-d) of first layer of designed model.Figure 5. The original image (a) and a part of feature maps (b-d) of first layer of designed model.</figDesc><graphic coords="7,104.27,678.91,385.56,104.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Table 1 .</head><label>1</label><figDesc>The metrics of the model, including overall accuracy for the classification, precision and recall, as well as the F1 score for road extraction from remote sensing imagery. for roads, and C represent clutter, and OA represents overall accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Plots showing the accuracy (a) and loss (b) of the GL-Dense-U-Net model while training the datasets with increasing iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Plots showing the accuracy (a) and loss (b) of the GL-Dense-U-Net model while training the datasets with increasing iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Results of roads extraction using the proposed GL-Dense-U-Net model. The original images (a,d,g,j,m), while the corresponding ground truths (b,e,h,k,n) and predictions (c,f,i,l,o) are also given. Green, red and blue represents the TP, FP and FN, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 7 . 16 3. 3 .</head><label>7163</label><figDesc>Figure 7. Results of roads extraction using the proposed GL-Dense-U-Net model. The original images (a,d,g,j,m), while the corresponding ground truths (b,e,h,k,n) and predictions (c,f,i,l,o) are also given. Green, red and blue represents the TP, FP and FN, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The results of road extraction from high-resolution remote sensing imagery using U-Net and DenseNet as feature extractors in the designed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The results of road extraction from high-resolution remote sensing imagery using U-Net and DenseNet as feature extractors in the designed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visual comparisons of road extraction results with different comparing algorithms. (a) The original remote sensing image. (b) The corresponding ground truth image of this region. (c) The results using the GL-Dense-U-Net model. (d) Results of FCN. (e) Results of U-Net. (f) Results of DeepLab V3+. Green, red and blue represents the TP, FP and FN, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visual comparisons of road extraction results with different comparing algorithms. (a) The original remote sensing image. (b) The corresponding ground truth image of this region. (c) The results using the GL-Dense-U-Net model. (d) Results of FCN. (e) Results of U-Net. (f) Results of DeepLab V3+. Green, red and blue represents the TP, FP and FN, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>Remote Sens. 2018, 10, x FOR PEER REVIEW 13 of 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Results of road extraction from the remote sensing imagery with and without LAU. (a,e) The original remote sensing image. (b,f) The corresponding ground truth image of this region. (c,g) The prediction results using the GL-Dense-U-Net model. (d,h) The prediction results without LAU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>In this paper, a novel deep convolutional neural network was presented to perform road extraction from high-resolution remote sensing imagery. The major contribution of this work is the designed road extraction model based on the construction of U-Net and the introduction of the DenseNet as the feature extractor. At the same time, this work designed the local attention unit and global attention unit in the expansive part of the model, which improved the precision and F1 score. The proposed model aimed to extract the local and global information of roads in the remote sensing imagery and improve the accuracy of road network extraction. The proposed GL-Dense-U-Net model did well in labeling different scale roads in the remote sensing imagery because the DenseNet blocks in different stages were used to guide the feature recovery in the expansive part. Experiments were carried out on a high-resolution remote sensing imagery dataset.The roads were extracted successfully via the deep convolutional neural network proposed in this work, and the results showed the effectiveness and feasibility of the proposed framework in improving the performance of semantic segmentation of remote sensing imagery. Qualitative comparisons were performed with some state-of-the-art methods for semantic segmentation, such as the fully convolutional network (FCN), the U-Net, as well as the new DeepLab V3+ method. Experimental results demonstrated that the proposed model performed better than the other methods. The proposed method in this work can obtain improvements in terms of the comprehensive evaluation metric, the F1 score, over the aforementioned semantic segmentation systems.Author Contributions: Y.X., Z.C. proposed the network architecture design and the framework of extracting roads. Y.X. and Y.F. performed the experiments and analyzed the data. Y.X., Z.X. wrote the paper. Y.F. revised the paper and provided valuable advices for the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Funding:</head><label></label><figDesc>This study was financially supported by the National key R &amp; D program of China (No. 2017YFB0503600, 2017YFC0602204, 2018YFB0505500), National Natural Science Foundation of China</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Results of road extraction from the remote sensing imagery with and without LAU. (a,e) The original remote sensing image. (b,f) The corresponding ground truth image of this region. (c,g) The prediction results using the GL-Dense-U-Net model. (d,h) The prediction results without LAU.</figDesc><graphic coords="13,102.57,169.36,390.42,208.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>In this paper, a novel deep convolutional neural network was presented to perform road extraction from high-resolution remote sensing imagery. The major contribution of this work is the designed road extraction model based on the construction of U-Net and the introduction of the DenseNet as the feature extractor. At the same time, this work designed the local attention unit and global attention unit in the expansive part of the model, which improved the precision and F 1 score. The proposed model aimed to extract the local and global information of roads in the remote sensing imagery and improve the accuracy of road network extraction. The proposed GL-Dense-U-Net model did well in labeling different scale roads in the remote sensing imagery because the DenseNet blocks in different stages were used to guide the feature recovery in the expansive part. Experiments were carried out on a high-resolution remote sensing imagery dataset. The roads were extracted successfully via the deep convolutional neural network proposed in this work, and the results showed the effectiveness and feasibility of the proposed framework in improving the performance of semantic segmentation of remote sensing imagery. Qualitative comparisons were performed with some state-of-the-art methods for semantic segmentation, such as the fully convolutional network (FCN), the U-Net, as well as the new DeepLab V3+ method. Experimental results demonstrated that the proposed model performed better than the other methods. The proposed method in this work can obtain improvements in terms of the comprehensive evaluation metric, the F 1 score, over the aforementioned semantic segmentation systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>• , 180 • and 270 • .</figDesc><table /><note><p>Remote Sens. 2018, 10, x FOR PEER REVIEW 7 of 16</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The metrics of the model, including overall accuracy for the classification, precision and recall, as well as the F 1 score for road extraction from remote sensing imagery.</figDesc><table><row><cell>Remote Sens. 2018, 10, x FOR PEER REVIEW</cell><cell></cell><cell></cell><cell></cell><cell>8 of 16</cell></row><row><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F 1</cell><cell>OA</cell></row><row><cell>R C</cell><cell>0.9630 0.9936</cell><cell>0.9515 0.9956</cell><cell>0.9572 0.9946</cell><cell>0.9782</cell></row></table><note><p>Where R stands for roads, and C represent clutter, and OA represents overall accuracy.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the results of the proposed method with other methods, where the values in bold are the best.Where P stands for precision, R represents recall and F 1 score for the roads extraction, respectively.</figDesc><table><row><cell></cell><cell></cell><cell>Image 1</cell><cell></cell><cell></cell><cell>Image 2</cell><cell></cell><cell></cell><cell>Image 3</cell><cell></cell><cell></cell><cell>ALL</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell></row><row><cell>FCN [42]</cell><cell cols="12">0.8027 0.9397 0.8658 0.8437 0.9624 0.8991 0.9756 0.8459 0.9061 0.8478 0.9307 0.8873</cell></row><row><cell>U-Net [33]</cell><cell cols="12">0.8303 0.9848 0.9010 0.8432 0.9942 0.9125 0.7953 0.9696 0.8738 0.8326 0.9708 0.8964</cell></row><row><cell cols="13">DeepLab V3+ [55] 0.9287 0.9211 0.9249 0.9552 0.9255 0.9401 0.9407 0.9458 0.9432 0.9415 0.9308 0.9361</cell></row><row><cell>Ours</cell><cell cols="12">0.9516 0.9537 0.9527 0.9797 0.9420 0.9605 0.9576 0.9589 0.9582 0.9630 0.9515 0.9572</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the results of the proposed method with other methods, where the values in bold are the best.</figDesc><table><row><cell></cell><cell></cell><cell>Image 1</cell><cell></cell><cell></cell><cell>Image 2</cell><cell></cell><cell></cell><cell>Image 3</cell><cell></cell><cell></cell><cell>ALL</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>FCN [42]</cell><cell cols="12">0.8027 0.9397 0.8658 0.8437 0.9624 0.8991 0.9756 0.8459 0.9061 0.8478 0.9307 0.8873</cell></row><row><cell>U-Net [33]</cell><cell cols="12">0.8303 0.9848 0.9010 0.8432 0.9942 0.9125 0.7953 0.9696 0.8738 0.8326 0.9708 0.8964</cell></row><row><cell>DeepLab V3+ [55]</cell><cell cols="12">0.9287 0.9211 0.9249 0.9552 0.9255 0.9401 0.9407 0.9458 0.9432 0.9415 0.9308 0.9361</cell></row><row><cell>Ours</cell><cell cols="12">0.9516 0.9537 0.9527 0.9797 0.9420 0.9605 0.9576 0.9589 0.9582 0.9630 0.9515 0.9572</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Comparison of the results while excluding either the LAU or the GAU, where the values in bold are the best.</figDesc><table><row><cell>Elements</cell><cell>OA</cell><cell>Precision (R)</cell><cell>Recall (R)</cell><cell>F 1 (R)</cell><cell>Precision (C)</cell><cell>Recall (C)</cell><cell>F 1 (C)</cell></row><row><cell>LAU and GAU</cell><cell>0.9782</cell><cell>0.9630</cell><cell>0.9515</cell><cell>0.9572</cell><cell>0.9936</cell><cell>0.9956</cell><cell>0.9946</cell></row><row><cell>Only LAU</cell><cell>0.9750</cell><cell>0.9565</cell><cell>0.9504</cell><cell>0.9534</cell><cell>0.9941</cell><cell>0.9938</cell><cell>0.9940</cell></row><row><cell>Only GAU</cell><cell>0.9699</cell><cell>0.9458</cell><cell>0.9543</cell><cell>0.9500</cell><cell>0.9942</cell><cell>0.9933</cell><cell>0.9938</cell></row><row><cell>Without both</cell><cell>0.9561</cell><cell>0.9385</cell><cell>0.9409</cell><cell>0.9397</cell><cell>0.9914</cell><cell>0.9921</cell><cell>0.9917</cell></row></table><note><p>Where R stand for buildings and C represent clutter, and OA means overall accuracy.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>The authors thank Guangliang Cheng providing datasets. The authors also thank Mingyu Xie (University of California, Santa Barbara) for helping improve the language.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This study was financially supported by the National key R &amp; D program of China (No. 2017YFB0503600, 2017YFC0602204, 2018YFB0505500), National Natural Science Foundation of China (41671400), HuBei Natural Science Foundation of China (2015CFA012) and Fundamental Research Funds for National Universities China University of Geosciences (Wuhan).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: Y.X. and Z.C. proposed the network architecture design and the framework of extracting roads. Y.X. and Y.F. performed the experiments and analyzed the data. Y.X. and Z.X. wrote the paper. Y.F. revised the paper and provided valuable advices for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Remote sensing of impervious surfaces in the urban areas: Requirements, methods, and trends. Remote Sens. Environ</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Weng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2011.02.030</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="34" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incorporating generic and specific prior knowledge in a multiscale phase field model for road extraction from VHR images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2008.922318</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="139" to="146" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building Extraction in Very High Resolution Remote Sensing Imagery Using Deep Learning and Guided Filters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10010144</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Road Extraction by Deep Residual U-Net</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2802944</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The recognition of road network from high-Resolution satellite remotely sensed data using image morphological characteristics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pesaresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431160500300354</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="5493" to="5508" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Road network extraction: A neural-dynamic framework based on deep learning and a finite state machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2015.1054049</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3144" to="3169" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An integrated method for urban main-road centerline extraction from optical remotely sensed imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Debayle</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2013.2272593</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="3359" to="3372" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Road Recognition From Remote Sensing Imagery Using Incremental Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2017.2665658</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2993" to="3005" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Road extraction from very high resolution remote sensing optical images based on texture analysis and beamlet transform</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Sghaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lepage</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2015.2449296</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Road Extraction from High-Resolution SAR Images via Automatic Local Detecting and Human-Guided Global Tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1155/2012/989823</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Antenn. Propag</title>
		<imprint>
			<biblScope unit="volume">989823</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic Road Extraction from High-Resolution Remote Sensing Image Based on Bat Model and Mutual Information Matching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.4304/jcp.6.11.2417-2426</idno>
	</analytic>
	<monogr>
		<title level="j">JCP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2417" to="2426" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A semi-automatic method for road centerline extraction from VHR images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2014.2312000</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1856" to="1860" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Road network detection using probabilistic and graph theoretical methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Unsalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sirmacek</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2012.2190078</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="4441" to="4453" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural damage assessments from Ikonos data using change detection, object-oriented segmentation, and classification techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Al-Khudhairy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Caravaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giada</surname></persName>
		</author>
		<idno type="DOI">10.14358/PERS.71.7.825</idno>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="825" to="837" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review of road extraction from remote sensing images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eklund</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jtte.2016.05.005</idno>
	</analytic>
	<monogr>
		<title level="j">J. Traffic Transp. Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="271" to="282" />
			<date type="published" when="2016">2016</date>
			<publisher>Engl</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Random forest classification of wetland landcovers from multi-sensor data in the arid region of Xinjiang</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8110954</idno>
		<imprint>
			<pubPlace>China</pubPlace>
		</imprint>
	</monogr>
	<note>Remote Sens. 2016, 8, 954. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Binary patterns encoded convolutional neural networks for texture recognition and remote sensing scene classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Molinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2018.01.023</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="74" to="85" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Support vector machines for road extraction from remotely sensed images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Analysis of Images and Patterns</title>
		<meeting>the International Conference on Computer Analysis of Images and Patterns<address><addrLine>Groningen, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08">August 2003</date>
			<biblScope unit="page" from="285" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An improved road and building detector on VHR images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Simler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<meeting>the 2011 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="507" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving SAR-based urban change detection by combining MAP-MRF classifier and nonlocal means similarity weights</title>
		<author>
			<persName><forename type="first">O</forename><surname>Yousif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2014.2347171</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4288" to="4300" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Road extraction based on the algorithms of MRF and hybrid model of SVM and FCM</title>
		<author>
			<persName><forename type="first">D.-M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Symposium on Image and Data Fusion (ISIDF)</title>
		<meeting>the 2011 International Symposium on Image and Data Fusion (ISIDF)<address><addrLine>Taiyuan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.6382</idno>
	</analytic>
	<monogr>
		<title level="m">CNN Features Off-the-Shelf: An Astounding Baseline for Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quality assessment of building footprint data using a deep autoencoder network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1080/13658816.2017.1341632</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Geogr. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1929" to="1951" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Road network extraction via deep learning and line integral convolution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<meeting>the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
			<biblScope unit="page" from="1599" to="1602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to detect roads in high-resolution aerial images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="210" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Road crack detection using deep convolutional neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the 2016 IEEE International Conference on Image Processing (ICIP)<address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
			<biblScope unit="page" from="3708" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for building and road extraction: Preliminary results</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<meeting>the IEEE International Geoscience and Remote Sensing Symposium (IGARSS)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
			<biblScope unit="page" from="1591" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Classification for High Resolution Remote Sensing Imagery Using a Fully Convolutional Network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9050498</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2017, 9, 498. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Large-Scale Remote-Sensing Image Classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2612821</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="645" to="657" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">UFCN: A fully convolutional neural network for road extraction in RGB imagery acquired by remote sensing from an unmanned aerial vehicle</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kestur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mehraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Narasipura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mudigere</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.12.016020</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl</title>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2018, 12, 016020. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10">October 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02364</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Attention-over-attention neural networks for reading comprehension</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1461" to="1477" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Ft. Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04">April 2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Léon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the 2011 IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">November 2011</date>
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1337" to="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pyramid Attention Network for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10180</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2017.2669341</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="3322" to="3337" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Google</forename><forename type="middle">Google</forename><surname>Earth</surname></persName>
		</author>
		<ptr target="http://www.google.cn/intl/zh-CN/earth/" />
		<imprint>
			<date type="published" when="2015-09-12">12 September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="7" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pixel-wise deep learning for contour detection</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01989</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evaluation of automatic road extraction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jamet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2004.1273918</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Gated Convolutional Neural Network for Semantic Segmentation in High-Resolution Images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9050446</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2017, 9, 446. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
