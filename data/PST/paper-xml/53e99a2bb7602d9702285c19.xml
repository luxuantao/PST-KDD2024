<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Thread Fork/Join Techniques for Multi-level Parallelism Exploitation in NUMA Multiprocessors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Martorell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. d&apos;Arquitectura de Computadors</orgName>
								<address>
									<addrLine>Universitat Polithcnica de Cataiunya cr. Jordi Girona 1-3, Campus Nord, Mbdui C6</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Eduard</forename><surname>Ayguade</surname></persName>
							<email>eduard@ac.upc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. d&apos;Arquitectura de Computadors</orgName>
								<address>
									<addrLine>Universitat Polithcnica de Cataiunya cr. Jordi Girona 1-3, Campus Nord, Mbdui C6</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nacho</forename><surname>Navarro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. d&apos;Arquitectura de Computadors</orgName>
								<address>
									<addrLine>Universitat Polithcnica de Cataiunya cr. Jordi Girona 1-3, Campus Nord, Mbdui C6</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julita</forename><surname>Corbalbn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. d&apos;Arquitectura de Computadors</orgName>
								<address>
									<addrLine>Universitat Polithcnica de Cataiunya cr. Jordi Girona 1-3, Campus Nord, Mbdui C6</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Gonz&amp;lez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. d&apos;Arquitectura de Computadors</orgName>
								<address>
									<addrLine>Universitat Polithcnica de Cataiunya cr. Jordi Girona 1-3, Campus Nord, Mbdui C6</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jestis</forename><surname>Labarta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. d&apos;Arquitectura de Computadors</orgName>
								<address>
									<addrLine>Universitat Polithcnica de Cataiunya cr. Jordi Girona 1-3, Campus Nord, Mbdui C6</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Thread Fork/Join Techniques for Multi-level Parallelism Exploitation in NUMA Multiprocessors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">457CE4FB0104922B1683FB10E6297E0A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents some techniques for efficient thread forking and joining in parallel execution environments, taking into consideration the physical structure of NUMA machines and the support for multi-level parallelization and processor grouping. Two work generation schemes and one join mechanism are designed, implemented, evaluated and compared with the ones used in the IFUX MP library, an efficient implementation which supports a single level of parallelism.</p><p>Supporting multiple levels of parallelism is a current research goal, both in shared and distributed memory machines. Our proposals include a first work generation scheme (GWD, or global work descriptor) which supports multiple levels of parallelism, but not processor grouping. The second work generation scheme (LWD, or local work descriptor) has been designed to support multiple levels of parallelism and processor grouping. Processor grouping is needed to distribute processors among different parts of the computation and maintain the working set of each processor across different parallel constructs.</p><p>The mechanisms are evaluated using synthetic benchmarks, two SPEC95fp applications and one NAS application. The performance evaluation concludes that: i) the overhead of the proposed mechanisms is similar to the overhead of the existing ones when exploiting a single level of parallelism, and ii) a remarkable improvement in performance is obtained for applications that have multiple levels of parallelism. The comparison with the traditional single-level parallelism exploitation gives an improvement in the range of 3065% for these applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Several current multiprocessor machines are based on building blocks (modules) consisting of 2 to 8 processors along with 128 to 5 12 Mb. of local memory. For instance, the SGI Origin 2000 <ref type="bibr">[ I]</ref> and the SUN Enterprise 10000 [2] are built using this approach. By joining several blocks, current machines can scale to a large number of processors. Although in these machines the physical Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed Ibr profit or commercial advantage and that copies hear this notice and the full citation on the tirst page. To copy othcrwisc, to republish, to post on scrvcrs or to redistribute to lists. requires prior specific permission andior a fee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICS '99 Rhodes Greece</head><p>Copyright ACM 1999 I-581 13-164~x/99/06...$5.00 memory is globally shared, the access from a processor to a remote memory location can be 2 to 3 times slower than the access to a local memory location due to the non-uniform memory access (NUMA) architecture or memory contention.</p><p>When the access to remote memory is slow, the processor utilization is very influenced by the amount of data shared among the processors. When data sharing increases, the actual processor utilization decreases due to the search for data in remote memory. Even when there are no data sharing problems at application level, the way the parallel tasks are supplied to processors and the way the threads join at the end of parallel regions are the aspects that have to be carefully tuned in order to achieve good performance. These aspects become specially important when support for both fine-grain and multi-level parallelism are required.</p><p>Current multiprocessor systems provide parallel execution environments mostly targeted to loop-level parallelism that try to obtain good processor utilization when running parallel programs. Current trends in the development of parallel execution environments include the possibility of expressing and exploiting multiple-levels of parallelism. The OpenMP [3] proposal for Fortran and C/C++ is a good example, although by now, no implementation of OpenMP is able to exploit multiple levels of parallelism.</p><p>Wc are working in the design and implementation of a parallel execution environment (called nano-threads <ref type="bibr">[4]</ref>) which allows the exploitation of multiple levels of parallelism, along with some mechanisms to group processors for the execution of parallel regions. The execution environment is targeted to Fortran applications containing both parallel sections and loops. In this paper, several techniques for supplying work to processors and to implement thread joining in NUMA machines are first compared. Our proposed techniques are aware of the cache behavior to minimize memory conflicts, although they may incur overheads higher than existing implementations that only support a single level of parallelism. We use the performance counters of the MIPS RlOOOO processor <ref type="bibr">[5]</ref> to keep track of the cache behaviour in the different implementations. Then, two applications (the SPEC95fp Hydro2D and the NAS APPBT) are used to validate the proposed fork/join implementations. Both applications benefit from exploiting multiple levels of parallelism, achieving higher speedup than their corresponding single-level versions.</p><p>The rest of the paper is structured as follows: Section 2 summarizes the functiondities related to thread forking and joining supported by current parallelization environments. Section 3 describes the proposed fork/join techniques, enlightening the differences with the existing ones. Section 4 presents an evaluation of the overhead introduced by the proposed techniques. Section 5 shows the structure of the applications used to validate our proposals and Section 6 presents their evaluation. Finally, Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CURRENT PARALLELIZATION ENVIRONMENTS</head><p>Current parallelization environments are based on highly tuned and customized thread packages. Each package provides mechanisms to spawn and join parallelism. The implementation of such mechanisms greatly influences the type of parallelism which can be exploited at application level. Current implementations do not allow the exploitation of more than one level of parallelism, because the run-time execution environment forbids spawning parallelism when already running in a parallel region.</p><p>For instance, in the SGI MP library, when the master thread spawns parallelism in a parallel construct, it sets the starting program counter and the arguments for the slave threads and assigns a sequence number to the parallel construct. It collects all this information in a common and fixed memory area, known as the work descriptor. Then, the slave threads pick up their work from this descriptor. They all participate in the parallel construct identified by the current sequence number, executing the same function with the same arguments. This mechanism supports all the work-sharing constructs defined in OpenMP-like extensions to sequential languages such as Fortran or C/C++. However, it restricts the parallelism that can be exploited by the application to a single level because the descriptor cannot be reused till the previous parallelism has been joined.</p><p>The SGI MP library provides two different implementations for thread joining. The first one, used by default, is a global join structure in shared memory (SHM). The slave threads use the global sequence number assigned to the current parallel construct to mark in the global join structure, located in the work descriptor, that the parallel work has been finished. Meanwhile, the master thread, after participating in the spawned work, waits for all the sequence numbers in the join structure to reach the current one. Usually, the join structure is distributed along several cache lines in order to minimize false sharing when the slave threads access it.</p><p>The second implementation replaces the join structure by a counter on uncached atomic memory (FOP's [6]) based on specialized hardware present in the memory modules of the Origin2000 system. The use of a shared counter or a single joining structure and a global sequence number to implement thread joining or barrier synchronization also disables multi-level parallelism exploitation because only one thread may act as the master thread for managing the joining process.</p><p>The parallelization environment based on the SUIF compiler [7] is also based in a run-time package which restricts the parallelism to a single level. The limitation of the SUIF run-time library is also motivated by the use of a shared work descriptor to supply work to the slave processors and only one join structure with a global sequence number to identify which parallel construct is active.</p><p>The Illinois-Intel Multithreading Library (IML [S]) is also targeted to shared memory multiprocessors. This one does support multiple levels of general, unstructured parallelism. Application tasks are inserted in task queues before execution, allowing several task descriptions to be active at the same time. IML focuses on the design alternatives for implementing such task queues (centralized and/or distributed). The library is also in charge of mapping the tasks to the available processors and of load balancing issues.</p><p>There are other research projects targeting at the exploitation of multiple levels of parallelism. They focus on providing some kind of coordination support to allow the interaction of a set of program modules in the framework of data parallel programs for distributed memory architectures. For instance, [9] propose a library-based approach that provides a set of functions for coupling multiple HPF tasks to form task-parallel computations. The Fx [lo] and PARADIGM [ 1 I] projects propose extensions to integrate task and data parallelism in an HPF environment. The use of task parallelism is proposed to improve the performance when data parallelism is not enough. The NthLib package interface is designed in such a way that it provides different primitives to spawn parallelism, depending on the hierarchy level in which the application is working. The deepest level is generated using the most efficient thread creation primitives, based on work descriptors and the techniques presented in Section 3. This level is the one which contains the finest grained parallelism. Higher (less deeper) levels are generated using nanothreads, a different (more costly) interface which provides threads with a stack. Local variables residing in the thread stack can be maintained (as in a local address space) to be used by the threads executing inner levels of parallelism. In addition, the latter interface also provides support for the general unstructured parallelism found in the hierarchical task graph. A detailed description of this interface and its implementation can be found elsewhere [ 121.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THREAD FORK/JOIN TECliNIQUES</head><p>The main goal of our proposals for implementing efficient thread fork/join in the nano-threads environment is the support for multiple levels of parallelism and processor grouping. In this section, two different techniques for supplying work to processors and an improved thread joining scheme are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Forking Threads</head><p>Forking threads efficiently at the inner-most level of parallelism is based on supplying a work descriptor to the participating processors. The work descriptor consists of a pointer to the function encapsulating the work that has to be executed and its arguments. When the same work descriptor is supplied to a group of processors, each one decides the portion of work that has to execute, based on the arguments, the number of processors working in the group and its own identifier inside the group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">I Functionality</head><p>The forking techniques are GWD (Global Work Descriptors) and LWD (Local Work Descriptors), WD for short. The GWD can be used in spawning the inner-most level in a multilevel parallel application. It supports multiple levels of parallelism because it allows the coexistence of multiple opened parallel constructs, solving the limitation of a single work descriptor found in previous implementations. All processors share a single GWD structure, they all can simultaneously supply work to the GWD and they execute the same work. GWD is expected to perform comparable to existing highly tuned implementations when exploiting a single level of parallelism. Figure <ref type="figure">1A</ref> shows the execution of a parallel construct consisting of a parallel loop, four independent sections (also containing parallel loops) and another parallel loop. In this case, the master thread executing each section spawns the parallelism associated to the parallel loops inside the section to all the processors. Since the same descriptor is supplied to all the processors for each parallel loop, each processor in the system will execute a chunk of iterations of all the loops (probably always the same, if the compiler exploits loop afftnity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loop is executed on P processors</head><p>Figure <ref type="figure">1</ref>: Two alternatives for the exploitation of multiple levels of parallelism LWD is a more advanced technique that inherits most of the characteristics of the GWD and also allows processor grouping. Using processor grouping, the application is able to drive several processors to work on a independent task or set of tasks. Each group has a master and (possibly) several slave processors. The master processor starts the task and it is in charge of spawning the parallelism encountered inside it. After that, the slave processors cooperate with the master to execute the parallelism. At the end, the master processor waits for the slaves to complete the work. This execution model requires that any processor can supply work to any other processor. There exists one LWD structure for each processor, allowing different work to be supplied to different processors from different parts of the parallel application. In Figure <ref type="figure">lB</ref>, the master thread executing each section spawns the parallelism associated to the parallel loops inside the section to just a subset of all the processors (in this case, each group of four processors is supplied with a different work descriptor). Due to the definition of these groups, now each processor executes a chunk of iterations for a subset of all the loops inside the parallel sections. However, for loops outside them, all processors cooperate in the execution of the parallel loops. Care should be taken into account about how this change in the structure of the parallelism can influence data locality.</p><p>As a result, the LWD overhead can be higher than that of GWD due to the individual supply of work descriptors, but it is expected that limiting the number of processors participating in an inner parallel construct makes worthwhile the exploitation of multiple levels of parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Implementation</head><p>The two WD proposals are implemented as arrays of pointers to work descriptors, behaving as circular lists (see Figure <ref type="figure" target="#fig_2">2, A</ref> and<ref type="figure">B</ref>).</p><p>There is a shared GWD structure and one per-processor LWD structure. Each processor searches for work first in its own LWD and then in the GWD. The size of the WD structures is a multiple of the secondary cache line size and they are aligned to cache line boundaries to avoid false sharing. Both implementations use oneway communication from the master processor to the slave processors. This means that the master processor writes pointers and the slaves read them. After the master processor writes a pointer to a WD location, it takes advantage of having exclusive access to the cache line to also clear a previously used location.  Both the DJS and the PJV are allocated in the stack of the master processor in order to allow the existence of several master processors at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION OF THE FORK/JOIN TECHNIQUES</head><p>The goal of this section is to evaluate the proposed fork/join techniques and compare them with the ones currently implemented in the SGI MP library. Later in this paper we show the usefulness of the multiple levels of parallelism exploitation and processor grouping. This will validate the fork/join techniques and justify the additional overhead they introduce.</p><p>All the experimental evaluation in this paper has been done on a dedicated SGI Origin2000 machine [ 161 containing 64 RlOOOO processors (250 Mhz., chip revision 3.4) and 8 Gb. of main memory. Each processor uses separate primary caches for instructions (32 Kb.) and data (32 Kb.) and a unified 4 Mb. secondary cache. The operating system is IRIX release 6.5. Unless noted otherwise, all benchmarks have been compiled with the MIPSpro FORTRAN 77 release 7.2.1. Im, using the following flags: -Ofast=ip27 -LNO:prefetch-ahead= 1 :auto-dist=ON. These are the options used to compile the base versions of the SPEC95fp benchmarks.</p><p>A synthetic benchmark (called "overhead") is used to evaluate the overhead introduced by the techniques presented in Section 3 when executing fine-grain parallelism. "Overhead" consists of 1,000 iterations spawning and joining parallelism. Several experiments have been carried out in the Origin2000 machine with a work size ranging from 1,000 to 1 ,OOO,OOO iterations. From them, we have selected the most relevant ones: Experiment #2 executes 4,000 iterations, taking around 100 us.; Experiment #6 does 50,000 iterations, taking 1.2 ms.; And experiment #9 is 400,000 iterations, taking 10 ms. The benchmark is run on both the SGI MP library and NthLib 8-e. Each bar represents the execution time of the benchmark on 32 processors, running the experiment indicated by the associated label. For instance, NNTH-LWD2 corresponds to the experiment #2 using the LWD technique. Along with the execution times of the benchmark, the plot presents the normalized numbers of primary and secondary data cache misses and the normalized number of store operations requiring exclusive access to a shared cache line. These events have been collected using the RlOOOO hardware event counters, through thepe&amp;x analysis tool, provided by SGI.</p><p>Execution times in Figure <ref type="figure" target="#fig_3">3</ref> show that all techniques are comparable with respect performance. Differences arise when comparing the behaviour of the cache. Observe that for the FOP, SHM and GWD techniques, the amount of cache events are similar. In LWD, the number of stores that reclaim exclusive access to a cache line increases. This is due to the way the work is supplied to processors, one to one. The store event is caused each time the master processor generates work, getting exclusive access to the cache line where it stores the pointer to the work descriptor. The cache line is, then, read by the destination slave processor, which requests shared access to the line. Thus, one cache line per processor exchanges twice its status each time work is generated. This movement between cache memories is not affecting the performance of the overhead benchmark, but it can affect the performance of parallel applications.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> shows the execution times obtained when running the overhead benchmark (4096 iterations, from 1 to 64 processors).</p><p>The results confirm that the GWD and LWD implementations are comparable to the MP library implementation. In addition, the figure also presents results for a multilevel version of the overhead benchmark. This version spawns a first level of paralielism consisting of four sections and an inner level of loop parallelism executing 1,024 iterations (using LWD), so the total amount of work is the same. This is represented by the bar labeled NNTH-2L2. It shows that the overhead of the multilevel version is comparable to the overhead of the single level one. Figure <ref type="figure" target="#fig_5">5</ref> shows the execution times for a parallel version of the SPEC95fp Swim application (with one level of parallelism and using the GWD technique) using from 1 to 64 processors. The execution times of the Swim application show that the GWD approach is comparable to the SGI implementation also when running comDlete aonlications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MULTI-LEVEL PARALLELISM EXPLOITATION</head><p>In the rest of the paper, we use two different applications to validate the implementation of the fork/join techniques presented in Section 3. First, we show that their efficiency is good enough to support multiple-levels of parallelism. A second result is that multiple-levels of parallelism can be effectively exploited inside applications. The applications selected are the SPEC95@ Hydro2D and the NAS APPBT benchmarks. The structure of the applications is described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SPEC95fp Hydro2D Benchmark</head><p>The Hydro2D application solves the hydrodynamical Navier Stokes equations to compute galactical jets. The reference input for Hydro2D executes 200 iterations of a time-step loop.</p><p>Figure <ref type="figure" target="#fig_6">6</ref> shows the structure of one of the two computational steps in the main time-consuming routine (advnce) executed inside the time-step loop. Up to a maximum of three levels of parallelism can be detected in this application. In the advnce subroutine, a set of functions can be executed in parallel since they access different data (nodes 2, 3 and 4); these functions contain two levels of parallelism as shown in the right part of Figure <ref type="figure" target="#fig_6">6</ref>. Nodes 5 and 6 corresponds to the invocation of functions transl and trans2, respectively; both functions also contain two-levels of parallelism as shown in the left part of Figure <ref type="figure" target="#fig_6">6</ref>. Nodes 7 to 10 contain a call to thefir subroutine. This subroutine has parallel loops.</p><p>Two different parallelization strategies have been tested. The first one exploits a single level of loop parallelism. The second one is a two-level version which exploits all the parallelism inside co@ stagf7lstugf2, transfltrans2 and the parallelism of the parallel invocations to fit subroutine including parallel loops. Although nodes 2, 3 and 4 can be also executed in parallel (so three levels of parallelism could be exploited), we decided to execute them sequentially in order to improve data locality. Spawning at the outer level distributes work to the master processors for each group. For instance, in the 16 processor execution, processors numbered 0, 4, 8 and 12 are the group master processors. At the inner level, each group master processor entering a new worksharing construct spawns parallelism on its group. For example, processor 4 is going to spawn parallelism on processors 4 (itself), 5,6 and 7.</p><p>As the application structure is regular, Hydro2D is useful to demonstrate how a well balanced allocation of the work at the different levels of parallelism to processors can result in a better 'ocessor utilization. The structure of the application is as follows: An iterative loop sequentiaIly calls to routines compute-r&amp; x-solve, y-solve and z-solve. The dependences in these routines determine which loops can be parallelized. For instance, x-solve carries the dependence in the first dimension being the loops that traverse the second and third dimension completely parallel; similarly, y-solve carries the dependence in the second dimension and z-solve in the third dimension. A possible strategy would be to parallelize the loop that traverses the third dimension in routines x-solve and y-solve and parallelize the loop that traverses the second dimension in routine z-solve. Although this parallelization shategy implies totally parallel loops, it suffers from the data movement overhead (transposition) that occurs when going from ydolve to z-solve and back again to x-solve.</p><p>The data movement overhead of the transposition can be avoided if the third dimension is also parallelized in z-solve; this requires the use of the OpenMP ORDERED clause and directive that forces the sequential execution of the distributed loop iterations. In order to allow a pipelined execution of the ORDERED dimension, loop blocking is applied. In this way, a chunk of iterations in processor p+I is executed when the same chunk of iterations finish its execution in processor p. Figure <ref type="figure" target="#fig_7">7</ref> shows the data distribution among processors and the resulting execution model for the onedimensional parallelization in the z-solve routine, Although this introduces the overhead of blocking and synchronization, the overlap of different chunks in different processors can result in an improved performance.</p><p>When the number of iterations is small to fed a large number of processors, two dimensions are worth to be parallelized. In order to avoid data movement, our strategy parallelizes the second and third dimension in all the routines. This implies that two dimensions are executed in parallel in x-solve, but one of the two dimensions parallelized are executed in an ORDERED way in both the y-solve and z-solve routines. Figure <ref type="figure">8</ref> shows the data distribution performed among processors and the resulting execution model for the multidimensional parallelization in the -solve routine. Four different versions of the Hydro2D benchmark have been executed using the reference input file, as provided in the SPEC benchmarks. Figure <ref type="figure" target="#fig_8">9</ref> shows the speedup obtained. Sequential execution time is 154.71 seconds, which agrees with the 154 seconds reported in the SPEC ,benchmark CFP95 summaries <ref type="bibr">[ 191.</ref> Bar labeled MP-SHM correspond to the speedup obtained when the application is run on top of the SGI MP library. In this case, the application has been manually parallelized using GpenMP directives to exploit loop parallelism and compiled with the native SGI compiler. Results are better than those reported in the SPEC summaries because of the manual parallelization. For instance, reported results using automatic parallelization through PFA (Parallel Fortran Analyzer) are 39.1 seconds on 8 processors and 35.9 seconds on 16 processors. Results obtained through manual parallelization are 32.7 seconds on 8 processors and 23.1 seconds on 16 processors. These results validate the parallelization we have manually performed on this application.</p><p>Bars labelled NNTH-ILVL have been obtained by executing the same loop level parallelization on top of the NthLib threads package using GWD. In this case, the application has been first parallelized using the NANOS compiler. The resulting parallel Fortran code, containing calls to the NthLib threads package is then compiled using the MIPSpro Fortran 77 compiler with the usual compilation options. As can be observed in Figure <ref type="figure" target="#fig_8">9</ref>, the execution time of this version is worse than the SGI MP library version. The main reason for this poor performance is that the quality of the code generated by the SGI OpenMP compiler is better than the quality of the code generated for the NtbLib. This is because the MIPSpro Fortran 77 compiler applies some code optimizations before and after the parallelization takes effect. This behaviour is inhibited in our approach because the task of parallelism extraction is done by the NANOS compiler, which is not applying any optimization before parallelizing. This difference in the quality of the generated code has a greater influence when the number of processors is small. However, up to a certain number of processors (32), the speedup obtained by the two onele implementations is similar, The last bar (labelled NNTH-MLV) is the result of the multi-level execution. The application exploits two levels of parallelism as explained in Section 5. Results show that the overhead introduced by the two-level parallelization is noticeable when running on a small number of processors. A fair comparison can be established with the NNTH-1 LVL version, in which the quality of the code generated is the same. As a result, for up to 8 processors, the overhead of the two-level parallelization motivates an increment in the execution time below 10%. In this application, four different processor groups are established at the outer parallelization level, so at least 8 processors are required to effectively exploit multiple levels of parallelism. Four groups of 2 processors give an speedup of 4.0 compared to the 4.5 achieved by the one-level version running in 8 processors. When 16 processors (four groups of 4 processors) are used, neariy the same speedup is achieved by both NNTH versions. When using more than 16 processors, the onelevel version is unable to scale, while the multi-level version scales till a speedup of 9.3 on 48 processors and giving an improvement of 30% in 32 processors with respect the single level version.</p><p>Processor grouping promotes scalability in this application because the parallel loops in the inner parallel level are distributed among less processors when executed in parallel. This causes each processor to keep a larger working set belonging to each matrix used in the loop body, thus reducing the amount of cache conflicts among the processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">NAS APPBT Benchmark</head><p>Figure <ref type="figure" target="#fig_9">10</ref> shows the execution time and speed-up of the NAS APPBT benchmark. We have selected a small version (class W) of the APPBT application in order to better appreciate the influence of the parallelization overhead. All versions of the APPBT application have been compiled with -03 compilation option instead of -Ofast=ip27 because this is the option used in the standard compilation of the NAS benchmarks in SGI machines. Due to the parallelization scheme and the application class, the ication can be executed on as much as 24 processors. In Figure <ref type="figure" target="#fig_9">10</ref>, MP-SHM stands for the one-level version executed on the SC1 MP Library. This result is provided as a reference. Two NNTH versions have been developed, one for single dimension parallelization (NNTH-1 LVL) and another for multidimensional parallelization (NNTH-MLV), achieved through two-levels of parallelism (as shown in Figure <ref type="figure">8</ref>).</p><p>Again, comparing the results in the nano-threads environment, the results on a small number of processors show that the multi-level version is worse than the one level version. This is due to the extra overhead introduced by the multi-level version. However, when using more than 8 processors, the multiple-level version achieves higher speedup, reaching 14.5 on 24 processors. The gain in the speedup reaches 65% with respect to the one dimensional parallelization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLJJSIONS</head><p>Current parallelization environments for shared memory multiprocessor systems are based on highly tuned and customized thread packages offering the mechanisms required for thread creation (fork) and joining, thread identification and so forth, Such environments (either commercial or experimental) do not allow the exploitation of more than one level of parallelism.</p><p>This paper presents techniques for efficient thread forking and joining in parallel execution environments, taking into consideration the physical structure of NUMA machines and the support for multi-level parallelization and processor grouping. Two work generation schemes and one join mechanism are designed, implemented in the NthLib package, evaluated and compared with the ones used in the IRIX MP library, an efficient implementation which supports a single level of parallelism.</p><p>For a single level of parallelism, the MP library uses single data structures located at fixed memory locations that do not enable several processors to coordinate (spawn and join) parallel activities, thus restricting the parallelism that can be exploited at the application level. In order to overcome this lack of functionality, we provide per-thread data structures that enable the coordination of some slave threads with a master thread, set when spawning an outer level of parallelism. Per-thread data structures also allow multiple threads to coordinate parallel activities coming from different parts of the application at a time.</p><p>Both mechanisms are evaluated and validated using a synthetic benchmark for measuring their overhead, two SPEC95fp applications (Swim and Hydro2D), and one NAS benchmark (APPBT). The overhead benchmark and Swim (with a single level of parallelism around loops) allow us to conclude that the additional functionalities in our fork/join mechanisms do not result in a noticeable reduction of the thread package efficiency. The other two benchmarks expose multiple levels of parallelism and benefit from its exploitation. The comparison with the traditional one-level parallelism exploitation for these programs gives an improvement of 30% and 65% in the speedup of Hydro2d (on 32 processors) and APPBT (on 24 processors), respectively. These results validate the exploitation of multiple levels of parallelism based on the thread fork/join techniques described in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGEMENTS</head><p>We would like to thank to the partners of the NANOS project for the insightful discussions on various aspects of this paper.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Our thread package implementation (NthLib [ 121) is built based on the Nano-Threads Programming Model [13][ 141. It is targeted to shared memory and supports multiple levels of parallelism. The user expresses the multiple levels of parallelism through nested OpenMP directives. The NANOS compiler [4] (based on Parafiase-2 [15]) analyses the directives and generates a hierarchical task graph structure which represents the application. Each level of the hierarchy represents one possible level of parallelism. Each nested parallel construct makes the hierarchy one level deeper. The compiler generates code based on this internal representation. At run-time, tasks are mapped to nano-threads and these ones are dynamically mapped one-to-one to the currently available processors. The words thread and processor are used indistinctly along the paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This one-way communication mechanism saves several cache misses and invalidations while generating work, thus speeding up part of the critical path of the run-time library. Each processor has its own local index to the WD structures to extract work from them. It knows that there is no work in a WD structure when the location pointed by its index is NULL. The master processor uses a global index (WDP) to store a new pointer in the next available location of the WD structures. The global index is necessary to allow several processors to add work at the same time. Mutual exclusion through load-linked and store conditional instruction sequences is used to update this global index. For example, in Figure 2A, four work descriptor pointers (shaded area) are currently stored in the GWD, possibly from different parts of the application. Not all processors have executed the same number of descriptors and some of them are extracting work from different locations. Each processor has a local index for work extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FigureFigure 2 :</head><label>2</label><figDesc>Figure 2B presents the implementation of the LWD. It shows four LWD structures for four different processors. In the example, two groups of two processors are already spawned. Each master processor (0 and 2) uses an index associated to each LWD to insert new work. This index is accessed in mutual exclusion to allow several processors to add work to a LWD at the same time. Again, clearing an already used location during work insertion improves the cache behaviour. Each processor waits for work in its LWD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evaluation of the fork/join mechanisms Figure 3 compares the four techniques for work generation with respect to the execution time and cache behaviour. MP-FOP and MP-SHM stand for the two different joining mechanisms inside the MP library. NNTH-GWD and NNTH-LWD stand for the two</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Fork/join evaluation (overhead)</figDesc><graphic coords="5,50.00,463.12,247.68,181.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fork/join evaluation (SPEC95fp Swim)</figDesc><graphic coords="5,330.77,66.49,248.16,181.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Multiple levels of parallelism inside the SPEC95fp HydroZD benchmark 5.2 NAS APPBT Benchmark The NAS APPBT benchmark [I 7][ 1 S] solves three sets of uncoupled block tridiagonal systems of equations, first in the x, then in they and finally in the I direction. Each block contains 5x5 elements. These systems arise in many CFD applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: One-dimensional data distribution and pipelined ORDERED execution in the APPBT application _xxw,"---,..--I -.~</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Execution time and speedup of the SPEC95fp HydrotD benchmark</figDesc><graphic coords="7,58.97,431.40,224.16,186.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Execution time and speedup of the NAS APPT3T benchmark</figDesc><graphic coords="7,335.90,404.79,226.56,183.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>This work has been supported by the European Community under the ESPRIT project E21907 (NANOS) and the Ministry of Education of Spain (CICYT) under contracts TIC97-1445-CE and TIC98-05 11.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The SGI Origin: A ccNUMA Highly Scalable Server</title>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lenoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Charlesworth</surname></persName>
		</author>
		<ptr target="www.openmp.org" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>of the 24th Int. Symposium on Computer Architecture (ISCA)<address><addrLine>IEEE Micro</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997. Jan/Feb 1998</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>STARFIRE: Extending the SMP Envelope</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting Parallelism Through Directives in the Nano-Threads Programming Model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguadc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martorell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Labarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzfilez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">10th Workshop on Programming Languages and Compilers for Parallel Computing</title>
		<imprint>
			<date type="published" when="1997-01">August 1997. January 1997</date>
			<publisher>MIPS Technologies Inc</publisher>
			<pubPlace>Minneapolis (USA</pubPlace>
		</imprint>
	</monogr>
	<note>MIPS RlOOOO Microprocessor User&apos;s Manual. version 2.0</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">IRIX Device Driver Programmer&apos;s Guide</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cortesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raithel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tuthill</surname></persName>
		</author>
		<idno>dot. 007-09 1 l-l 20</idno>
	</analytic>
	<monogr>
		<title level="j">SGI</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximizing Multiprocessor Performance with the SUIF Compiler</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="1996-12">December 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Illinois-Intel Multithreading Library: Multithreading Support for Intel Architecture Based Multiprocessor Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Haghighat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stavrakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal, Q 1 issue</title>
		<imprint>
			<date type="published" when="1998-02">February 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Double Standards: Bringing Task Parallelism to HPF Via the Message Passing Interface</title>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnaiyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-11">November 1996</date>
		</imprint>
	</monogr>
	<note>Supercomputing&apos;96</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Task Parallelism in a High Performance Fortran framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'halloran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Subhlok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Parallel and Distributed Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simultaneous Exploitation of Task and Data Parallelism in Regular Scientific Computations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Library Implementation of the Nano-Threads Programming Model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Martorell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Labarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguadc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europar&apos;96</title>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kleiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nano-Threads: A user-level threads architecture</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>CSRD, Univ. of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">wane-threads: Compiler Driven Multithreading</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Int. Workshop on Compilers for Parallel Computing</title>
		<meeting><address><addrLine>Delhi (The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-12">December 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parafiase-2: An Environment for Parallelizing, Partitioning, Synchronizing and Scheduling Programs on Multiprocessors</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haghighat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schouten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Parallel Processing</title>
		<meeting><address><addrLine>St. Charles, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Silicon Graphics Computer Systems SGI</title>
		<imprint>
			<date type="published" when="1996">2000. 1996</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Origin 200 and Origin</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parallelization of NAS Benchmarks for Shared Memory Multiprocessors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno>NAS-98- 010</idno>
	</analytic>
	<monogr>
		<title level="j">NASA</title>
		<imprint>
			<date type="published" when="1998-03">March 1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The NAS Parallel Benchmarks 2.0</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saphir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wijngaart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yarrow</surname></persName>
		</author>
		<idno>NAS-95-020</idno>
	</analytic>
	<monogr>
		<title level="j">NASA</title>
		<imprint>
			<date type="published" when="1995-12">December 1995</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Standard Performance Evaluation Corporation</title>
		<ptr target="www.spec.org" />
	</analytic>
	<monogr>
		<title level="m">SPEC Organization</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
