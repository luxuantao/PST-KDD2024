<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DELIBERATION MODEL BASED TWO-PASS END-TO-END SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-17">17 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ke</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
							<email>tsainath@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
							<email>rpang@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
							<email>prabhavalkar@google.com</email>
						</author>
						<title level="a" type="main">DELIBERATION MODEL BASED TWO-PASS END-TO-END SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-17">17 Mar 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2003.07962v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end (E2E) models have made rapid progress in automatic speech recognition (ASR) and perform competitively relative to conventional models. To further improve the quality, a two-pass model has been proposed to rescore streamed hypotheses using the nonstreaming Listen, Attend and Spell (LAS) model while maintaining a reasonable latency. The model attends to acoustics to rescore hypotheses, as opposed to a class of neural correction models that use only first-pass text hypotheses. In this work, we propose to attend to both acoustics and first-pass hypotheses using a deliberation network. A bidirectional encoder is used to extract context information from first-pass hypotheses. The proposed deliberation model achieves 12% relative WER reduction compared to LAS rescoring in Google Voice Search (VS) tasks, and 23% reduction on a proper noun test set. Compared to a large conventional model, our best model performs 21% relatively better for VS. In terms of computational complexity, the deliberation decoder has a larger size than the LAS decoder, and hence requires more computations in second-pass decoding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>E2E ASR has gained a lot of popularity due to its simplicity in training and decoding. An all-neural E2E model eliminates the need to individually train components of a conventional model (i.e., acoustic, pronunciation, and language models), and directly outputs subword (or word) symbols <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. In large scale training, E2E models perform competitively compared to more sophisticated conventional systems on Google traffic <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Given its all-neural nature, an E2E model can be reasonably downsized to fit on mobile devices <ref type="bibr" target="#b5">[6]</ref>.</p><p>Despite the rapid progress made by E2E models, they still face challenges compared to state-of-the-art conventional models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. To bridge the quality gap between a streaming recurrent neural network transducer (RNN-T) <ref type="bibr" target="#b5">[6]</ref> and a large conventional model <ref type="bibr" target="#b7">[8]</ref>, a two-pass framework has been proposed in <ref type="bibr" target="#b9">[10]</ref>, which uses a non-streaming LAS decoder to rescore the RNN-T hypotheses. The rescorer attends to audio encoding from the encoder, and computes sequence-level log-likelihoods of first-pass hypotheses. The two-pass model achieves 17%-22% relative WER reduction (WERR) compared to RNN-T <ref type="bibr" target="#b5">[6]</ref> and has a similar WER to a large conventional model <ref type="bibr" target="#b7">[8]</ref>.</p><p>A class of neural correction models post-process hypotheses using only the text information, and can be considered as second-pass models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. The models typically use beam search to generate new hypotheses, compared to rescoring where one leverages external language models trained with large text corpora <ref type="bibr" target="#b13">[14]</ref>. For example, a neural correction model in <ref type="bibr" target="#b10">[11]</ref> takes first-pass text hypotheses and generates new sequences to improve numeric utterance recognition <ref type="bibr" target="#b14">[15]</ref>. A transformer-based spelling correction model is proposed in <ref type="bibr" target="#b11">[12]</ref> to correct the outputs of a connectionist temporal classification model in Mandarin ASR. In addition, <ref type="bibr" target="#b12">[13]</ref> leverages text-to-speech (TTS) audio to train an attention-based neural spelling corrector to improve LAS decoding. These neural correction models typically use only text as inputs, while the aforementioned two-pass model attends to acoustics alone for second-pass processing.</p><p>In this work, we propose to combine acoustics and first-pass text hypotheses for second-pass decoding based on the deliberation network <ref type="bibr" target="#b15">[16]</ref>. The deliberation model has been used in state-of-the-art machine translation <ref type="bibr" target="#b16">[17]</ref>, or generating intermediate representation in speech-to-text translation <ref type="bibr" target="#b17">[18]</ref>. Our deliberation model has a similar structure as <ref type="bibr" target="#b15">[16]</ref>: An RNN-T model generates the first-pass hypotheses, and deliberation attends to both acoustics and first-pass hypotheses for a second-pass decoding. We encode first-pass hypotheses bidirectionally to leverage context information for decoding. Note that the first-pass hypotheses are sequences of wordpieces <ref type="bibr" target="#b18">[19]</ref> and are usually short in VS, and thus the encoding should have limited impact on latency.</p><p>Our experiments are conducted using the same training data as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, which is from multiple domains such as Voice Search, YouTube, Farfield and Telephony. We first analyze the behavior of the deliberation model, including performance when attending to multiple RNN-T hypotheses, contribution of different attention, and rescoring vs. beam search. We apply additional encoder (AE) layers and minimum WER (MWER) training <ref type="bibr" target="#b21">[22]</ref> to further improve quality. The results show that our MWER trained 8-hypothesis deliberation model performs 11% relatively better than LAS rescoring <ref type="bibr" target="#b9">[10]</ref> in VS WER, and up to 15% for proper noun recognition. Joint training further improves VS slightly (2%) but significantly for a proper noun test set: 9%. As a result, our best deliberation model achieves a WER of 5.0% on VS, which is 21% relatively better than the large conventional model <ref type="bibr" target="#b7">[8]</ref> (6.3% VS WER). Lastly, we analyze the computational complexity of the deliberation model, and show some decoding examples to understand its strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DELIBERATION BASED TWO-PASS E2E ASR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model Architecture</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, our deliberation network consists of three major components: A shared encoder, an RNN-T decoder <ref type="bibr" target="#b0">[1]</ref>, and a deliberation decoder, similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. The shared encoder takes log-mel filterbank energies, x = (x1, ..., xT ), where T denotes the number of frames, and generates an encoding e. The encoder output e is then fed to an RNN-T decoder to produce first-pass decoding results yr in a streaming fashion. Then the deliberation decoder attends to both e and yr to predict a new sequence y d . We use a bidirectional encoder to further encode yr for useful context information, and the output is denoted as h b . Note that we could use mul- </p><formula xml:id="formula_0">+ O 5 H C S O C s A n L O / z W 9 R W k r 1 E v r 1 8 = " &gt; A A A B 7 X i c b Z B N S w M x E I Z n / a z 1 q + r R S 7 A I n s q u C H o s e v F Y w X 5 A W 0 o 2 z b a x 2 W R J Z s W y 9 D 9 4 8 a C I V / + P N / + N a b s H b X 0 h 8 P D O D J l 5 w 0 Q K i 7 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D h t W p Y b z O t N S m F V L L p V C 8 j g I l b y W G 0 z i U v B m O b q b 1 5 i M 3 V m h 1 j + O E d 2 M 6 U C I S j K K z G p 0 w y p 4 m v V L Z r / g z k W U I c i h D r l q v 9 N X p a 5 b G X C G T 1 N p 2 4 C f Y z a h B w S S f F D u p 5 Q l l I z r g b Y e K x t x 2 s 9 m 2 E 3 L q n D 6 J t H F P I Z m 5 v y c y G l s 7 j k P X G V M c 2 s X a 1 P y v 1 k 4 x u u p m Q i U p c s X m H 0 W p J K j J 9 H T S F 4 Y z l G M H l B n h d i V s S A 1 l 6 A I q u h C C x Z O X o X F e C R z f X Z S r 1 3 k c B T i G E z i D A C 6 h C r d Q g z o w e I B n e I U 3 T 3 s v 3 r v 3 M W 9 d 8 f K Z I / g j 7 / M H 1 2 O P S g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + O 5 H C S O C s A n L O / z W 9 R W k r 1 E v r 1 8 = " &gt; A A A B 7 X i c b Z B N S w M x E I Z n / a z 1 q + r R S 7 A I n s q u C H o s e v F Y w X 5 A W 0 o 2 z b a x 2 W R J Z s W y 9 D 9 4 8 a C I V / + P N / + N a b s H b X 0 h 8 P D O D J l 5 w 0 Q K i 7 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D h t W p Y b z O t N S m F V L L p V C 8 j g I l b y W G 0 z i U v B m O b q b 1 5 i M 3 V m h 1 j + O E d 2 M 6 U C I S j K K z G p 0 w y p 4 m v V L Z r / g z k W U I c i h D r l q v 9 N X p a 5 b G X C G T 1 N p 2 4 C f Y z a h B w S S f F D u p 5 Q l l I z r g b Y e K x t x 2 s 9 m 2 E 3 L q n D 6 J t H F P I Z m 5 v y c y G l s 7 j k P X G V M c 2 s X a 1 P y v 1 k 4 x u u p m Q i U p c s X m H 0 W p J K j J 9 H T S F 4 Y z l G M H l B n h d i V s S A 1 l 6 A I q u h C C x Z O X o X F e C R z f X Z S r 1 3 k c B T i G E z i D A C 6 h C r d Q g z</formula><p>o w e I B n e I U 3 T 3 s v 3 r v 3 M W 9 d 8 f K Z I / g j 7 / M H 1 2 O P S g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><formula xml:id="formula_1">+ O 5 H C S O C s A n L O / z W 9 R W k r 1 E v r 1 8 = " &gt; A A A B 7 X i c b Z B N S w M x E I Z n / a z 1 q + r R S 7 A I n s q u C H o s e v F Y w X 5 A W 0 o 2 z b a x 2 W R J Z s W y 9 D 9 4 8 a C I V / + P N / + N a b s H b X 0 h 8 P D O D J l 5 w 0 Q K i 7 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D h t W p Y b z O t N S m F V L L p V C 8 j g I l b y W G 0 z i U v B m O b q b 1 5 i M 3 V m h 1 j + O E d 2 M 6 U C I S j K K z G p 0 w y p 4 m v V L Z r / g z k W U I c i h D r l q v 9 N X p a 5 b G X C G T 1 N p 2 4 C f Y z a h B w S S f F D u p 5 Q l l I z r g b Y e K x t x 2 s 9 m 2 E 3 L q n D 6 J t H F P I Z m 5 v y c y G l s 7 j k P X G V M c 2 s X a 1 P y v 1 k 4 x u u p m Q i U p c s X m H 0 W p J K j J 9 H T S F 4 Y z l G M H l B n h d i V s S A 1 l 6 A I q u h C C x Z O X o X F e C R z f X Z S r 1 3 k c B T i G E z i D A C 6 h C r d Q g z</formula><p>o w e I B n e I U 3 T 3 s v 3 r v 3 M W 9 d 8 f K Z I / g j 7 / M H 1 2 O P S g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><formula xml:id="formula_2">+ O 5 H C S O C s A n L O / z W 9 R W k r 1 E v r 1 8 = " &gt; A A A B 7 X i c b Z B N S w M x E I Z n / a z 1 q + r R S 7 A I n s q u C H o s e v F Y w X 5 A W 0 o 2 z b a x 2 W R J Z s W y 9 D 9 4 8 a C I V / + P N / + N a b s H b X 0 h 8 P D O D J l 5 w 0 Q K i 7 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D h t W p Y b z O t N S m F V L L p V C 8 j g I l b y W G 0 z i U v B m O b q b 1 5 i M 3 V m h 1 j + O E d 2 M 6 U C I S j K K z G p 0 w y p 4 m v V L Z r / g z k W U I c i h D r l q v 9 N X p a 5 b G X C G T 1 N p 2 4 C f Y z a h B w S S f F D u p 5 Q l l I z r g b Y e K x t x 2 s 9 m 2 E 3 L q n D 6 J t H F P I Z m 5 v y c y G l s 7 j k P X G V M c 2 s X a 1 P y v 1 k 4 x u u p m Q i U p c s X m H 0 W p J K j J 9 H T S F 4 Y z l G M H l B n h d i V s S A 1 l 6 A I q u h C C x Z O X o X F e C R z f X Z S r 1 3 k c B T i G E z i D A C 6 h C r d Q g z</formula><p>o w e I B n e I U 3 T 3 s v 3 r v 3 M W 9 d 8 f K Z I / g j 7 / M H 1 2 O P S g = = &lt; / l a t e x i t &gt; e &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N 3 a d 6 f 1 u a</p><formula xml:id="formula_3">+ R Z z 7 n D 8 V O X P Z U d q L c = " &gt; A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F 8 F R 2 R d B j 0 Y v H C v Y D 2 l K y 6 W w b m 0 2 W J C u U p f / B i w d F v P p / v P l v T N s 9 a O s L g Y d 3 Z s j M G y a C G + v 7 3 1 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e N Y 1 K N c M G U 0 L p d k g N C i 6 x Y b k V 2 E 4 0 0 j g U 2 A r H t 7 N 6 6 w m 1 4 U o + 2 E m C v Z g O J Y 8 4 o 9 Z Z z W 4 Y Z T j t l y t + 1 Z + L r E K Q Q w V y 1 f v l r + 5 A s T R G a Z m g x n Q C P 7 G 9 j G r L m c B p q Z s a T C g b 0 y F 2 H E o a o + l l 8 2 2 n 5 M w 5 A x I p 7 Z 6 0 Z O 7 + n s h o b M w k D l 1 n T O 3 I L N d m 5 n + 1 T m q j 6 1 7 G Z Z J a l G z x U Z Q K Y h W Z n U 4 G X C O z Y u K A M s 3 d r o S N q K b M u o B K L o R g + e R V a F 5 U A 8 f 3 l 5 X a T R 5 H E U 7 g F M 4 h g C u o w R 3 U o Q E M H u E Z X u H N U 9 6 L 9 + 5 9 L F o L X j 5 z D H / k f f 4 A u o S P N w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N 3 a d 6 f 1 u a + R Z z 7 n D 8 V O X P Z U d q L c = " &gt; A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F 8 F R 2 R d B j 0 Y v H C v Y D 2 l K y 6 W w b m 0 2 W J C u U p f / B i w d F v P p / v P l v T N s 9 a O s L g Y d 3 Z s j M G y a C G + v 7 3 1 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e N Y 1 K N c M G U 0 L p d k g N C i 6 x Y b k V 2 E 4 0 0 j g U 2 A r H t 7 N 6 6 w m 1 4 U o + 2 E m C v Z g O J Y 8 4 o 9 Z Z z W 4 Y Z T j t l y t + 1 Z + L r E K Q Q w V y 1 f v l r + 5 A s T R G a Z m g x n Q C P 7 G 9 j G r L m c B p q Z s a T C g b 0 y F 2 H E o a o + l l 8 2 2 n 5 M w 5 A x I p 7 Z 6 0 Z O 7 + n s h o b M w k D l 1 n T O 3 I L N d m 5 n + 1 T m q j 6 1 7 G Z Z J a l G z x U Z Q K Y h W Z n U 4 G X C O z Y u K A M s 3 d r o S N q K b M u o B K L o R g + e R V a F 5 U A 8 f 3 l 5 X a T R 5 H E U 7 g F M 4 h g C u o w R 3 U o Q E M H u E Z X u H N U 9 6 L 9 + 5 9 L F o L X j 5 z D H / k f f 4 A u o S P N w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N 3 a d 6 f 1 u a + R Z z 7 n D 8 V O X P Z U d q L c = " &gt; A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F 8 F R 2 R d B j 0 Y v H C v Y D 2 l K y 6 W w b m 0 2 W J C u U p f / B i w d F v P p / v P l v T N s 9 a O s L g Y d 3 Z s j M G y a C G + v 7 3 1 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e N Y 1 K N c M G U 0 L p d k g N C i 6 x Y b k V 2 E 4 0 0 j g U 2 A r H t 7 N 6 6 w m 1 4 U o + 2 E m C v Z g O J Y 8 4 o 9 Z Z z W 4 Y Z T j t l y t + 1 Z + L r E K Q Q w V y 1 f v l r + 5 A s T R G a Z m g x n Q C P 7 G 9 j G r L m c B p q Z s a T C g b 0 y F 2 H E o a o + l l 8 2 2 n 5 M w 5 A x I p 7 Z 6 0 Z O 7 + n s h o b M w k D l 1 n T O 3 I L N d m 5 n + 1 T m q j 6 1 7 G Z Z J a l G z x U Z Q K Y h W Z n U 4 G X C O z Y u K A M s 3 d r o S N q K b M u o B K L o R g + e R V a F 5 U A 8 f 3 l 5 X a T R 5 H E U 7 g F M 4 h g C u o w R 3 U o Q E M H u E Z X u H N U 9 6 L 9 + 5 9 L F o L X j 5 z D H / k f f 4 A u o S P N w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N 3 a d 6 f 1 u a + R Z z 7 n D 8 V O X P Z U d q L c = " &gt; A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F 8 F R 2 R d B j 0 Y v H C v Y D 2 l K y 6 W w b m 0 2 W J C u U p f / B i w d F v P p / v P l v T N s 9 a O s L g Y d 3 Z s j M G y a C G + v 7 3 1 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e N Y 1 K N c M G U 0 L p d k g N C i 6 x Y b k V 2 E 4 0 0 j g U 2 A r H t 7 N 6 6 w m 1 4 U o + 2 E m C v Z g O J Y 8 4 o 9 Z Z z W 4 Y Z T j t l y t + 1 Z + L r E K Q Q w V y 1 f v l r + 5 A s T R G a Z m g x n Q C P 7 G 9 j G r L m c B p q Z s a T C g b 0 y F 2 H E o a o + l l 8 2 2 n 5 M w 5 A x I p 7 Z 6 0 Z O 7 + n s h o b M w k D l 1 n T O 3 I L N d m 5 n + 1 T m q j 6 1 7 G Z Z J a l G z x U Z Q K Y h W Z n U 4 G X C O z Y u K A M s 3 d r o S N q K b M u o B K L o R g + e R V a F 5 U A 8 f 3 l 5 X a T R 5 H E U 7 g F M 4 h g C u o w R 3 U o Q E M H u E Z X u H N U 9 6 L 9 + 5 9 L F o L X j 5 z D H / k f f 4 A u o S P N w = = &lt; / l a t e x i t &gt; y r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k a 1 w y 5 s P O U i T o b R C Z I w d + b L k f x g = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p Z h N 3 N 0 I J / R N e P C j i 1 b / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U 6 Q V h N u m r a b 9 c c a v u H G S V e D m p Q I 5 G v / z V G 8 Q s j V A a J q j W X c 9 N j J 9 R Z T g T O C 3 1 U o 0 J Z W M 6 x K 6 l k k a o / W x + 7 5 S c W W V A w l j Z k o b M 1 d 8 T G Y 2 0 n k S B 7 Y y o G e l l b y b + 5 3 V T E 1 7 7 G Z d J a l C y x a I w F c T E Z P Y 8 G X C F z I i J J Z Q p b m 8 l b E Q V Z c Z G V L I h e M s v r 5 J W r e p d V G v 3 l 5 X 6 T R 5 H E U 7 g F M 7 B g y u o w x 0 0 o A k M B D z D K 7 w 5 j 8 6 L 8 + 5 8 L F o L T j 5 z D H / g f P 4 A Z l S Q N A = = &lt; / l a t e x i t &gt; c e &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 H 0 q u a A k 5 S H U K y W 5 X 2 U z P j B h C L E = " &gt; A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a R d u t n E 3 Y 1 Q Q v + E F w + K e P X v e P P f u G 1 z 0 N Y X F h 7 e m W F n 3 j A V X B v X / X Z K a + s b m 1 v l 7 c r O 7 t 7 + Q f X w q K W T T D H 0 W S I S 1 Q m p R s E l + o Y b g Z 1 U I Y 1 D g e 1 w f D u r t 5 9 Q a Z 7 I B z N J M Y j p U P K I M 2 q s 1 e m F U c 7 6 O O 1 X a 2 7 d n Y u s g l d A D Q o 1 + 9 W v 3 i B h W Y z S M E G 1 7 n p u a o K c K s O Z w G m l l 2 l M K R v T I X Y t S h q j D v L 5 v l N y Z p 0 B i R J l n z R k 7 v 6 e y G m s 9 S Q O b W d M z U g v 1 2 b m f 7 V u Z q L r I O c y z Q x K t v g o y g Q x C Z k d T w Z c I T N i Y o E y x e 2 u h I 2 o o s z Y i C o 2 B G / 5 5 F V o X d Q 9 y / e X t c Z N E U c Z T u A U z s G D K 2 j A H T T B B w Y C n u E V 3 p x H 5 8 V 5 d z 4 W r S W n m D m G P 3 I + f w A v r Z A N &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 H 0 q u a A k 5 S H U K y W 5 X 2 U z P j B h C L E = " &gt; A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a R d u t n E 3 Y 1 Q Q v + E F w + K e P X v e P P f u G 1 z 0 N Y X F h 7 e m W F n 3 j A V X B v X / X Z K a + s b m 1 v l 7 c r O 7 t 7 + Q f X w q K W T T D H 0 W S I S 1 Q m p R s E l + o Y b g Z 1 U I Y 1 D g e 1 w f D u r t 5 9 Q a Z 7 I B z N J M Y j p U P K I M 2 q s 1 e m F U c 7 6 O O 1 X a 2 7 d n Y u s g l d A D Q o 1 + 9 W v 3 i B h W Y z S M E G 1 7 n p u a o K c K s O Z w G m l l 2 l M K R v T I X Y t S h q j D v L 5 v l N y Z p 0 B i R J l n z R k 7 v 6 e y G m s 9 S Q O b W d M z U g v 1 2 b m f 7 V u Z q L r I O c y z Q x K t v g o y g Q x C Z k d T w Z c I T N i Y o E y x e 2 u h I 2 o o s z Y i C o 2 B G / 5 5 F V o X d Q 9 y / e X t c Z N E U c Z T u A U z s G D K 2 j A H T T B B w Y C n u E V 3 p x H 5 8 V 5 d z 4 W r S W n m D m G P 3 I + f w A v r Z A N &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 H 0 q u a A k 5 S H U K y W 5 X 2 U z P j B h C L E = " &gt; A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a R d u t n E 3 Y 1 Q Q v + E F w + K e P X v e P P f u G 1 z 0 N Y X F h 7 e m W F n 3 j A V X B v X / X Z K a + s b m 1 v l 7 c r O 7 t 7 + Q f X w q K W T T D H 0 W S I S 1 Q m p R s E l + o Y b g Z 1 U I Y 1 D g e 1 w f D u r t 5 9 Q a Z 7 I B z N J M Y j p U P K I M 2 q s 1 e m F U c 7 6 O O 1 X a 2 7 d n Y u s g l d A D Q o 1 + 9 W v 3 i B h W Y z S M E G 1 7 n p u a o K c K s O Z w G m l l 2 l M K R v T I X Y t S h q j D v L 5 v l N y Z p 0 B i R J l n z R k 7 v 6 e y G m s 9 S Q O b W d M z U g v 1 2 b m f 7 V u Z q L r I O c y z Q x K t v g o y g Q x C Z k d T w Z c I T N i Y o E y x e 2 u h I 2 o o s z Y i C o 2 B G / 5 5 F V o X d Q 9 y / e X t c Z N E U c Z T u A U z s G D K 2 j A H T T B B w Y C n u E V 3 p x H 5 8 V 5 d z 4 W r S W n m D m G P 3 I + f w A v r Z A N &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 H 0 q u a A k 5 S H U K y W 5 X 2 U z P j B h C L E = " &gt; A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a R d u t n E 3 Y 1 Q Q v + E F w + K e P X v e P P f u G 1 z 0 N Y X F h 7 e m W F n 3 j A V X B v X / X Z K a + s b m 1 v l 7 c r O 7 t 7 + Q f X w q K W T T D H 0 W S I S 1 Q m p R s E l + o Y b g Z 1 U I Y 1 D g e 1 w f D u r t 5 9 Q a Z 7 I B z N J M Y j p U P K I M 2 q s 1 e m F U c 7 6 O O 1 X a 2 7 d n Y u s g l d A D Q o 1 + 9 W v 3 i B h W Y z S M E G 1 7 n p u a o K c K s O Z w G m l l 2 l M K R v T I X Y t S h q j D v L 5 v l N y Z p 0 B i R J l n z R k 7 v 6 e y G m s 9 S Q O b W d M z U g v 1 2 b m f 7 V u Z q L r I O c y z Q x K t v g o y g Q x C Z k d T w Z c I T N i Y o E y x e 2 u h I 2 o o s z Y i C o 2 B G / 5 5 F V o X d Q 9 y / e X t c Z N E U c Z T u A U z s G D K 2 j A H T T B B w Y C n u E V 3 p x H 5 8 V 5 d z 4 W r S W n m D m G P 3 I + f w A v r Z A N &lt; / l a t e x i t &gt; Bidi- LSTMs h b &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C E k u c d J c e I J 9 u e D p e u a a M 7 R U N c = " &gt; A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z d O O y g r 1 A G 8 J k O m m H T i Z h 5 k Q s I e C r u H G h i F u f w 5 1 v 4 7 S N o K 0 / D H z 8 5 x z O m T 9 I B N f g O F / W 0 v L K 6 t p 6 a a O 8 u b W 9 s 2 v v 7 b d 0 n C r K m j Q W s e o E R D P B J W s C B 8 E 6 i W I k C g R r B 6 P r S b 1 9 z 5 T m s b y D c c K 8 i A w k D z k l Y C z f P u w B e 4 A g z I a 5 / 4 N B 7 t s V p + p M h R f B L a C C C j V 8 + 7 P X j 2 k a M Q l U E K 2 7 r p O A l x E F n A q W l 3 u p Z g m h I z J g X Y O S R E x 7 2 f T 8 H J 8 Y p 4 / D W J k n A U / d 3 x M Z i b Q e R 4 H p j A g M 9 X x t Y v 5 X 6 6 Y Q X n o Z l 0 k K T N L Z o j A V G G I 8 y Q L 3 u W I U x N g A o Y q b W z E d E k U o m M T K J g R 3 / s u L 0 K p V 3 b N q 7 f a 8 U r 8 q 4 i i h I 3 S M T p G L L l A d 3 a A G a i K K M v S E X t C r 9 W g 9 W 2 / W + 6 x 1 y S p m D t A f W R / f a v a W b A = = &lt; / l a t e x i t &gt; y d</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x 7 p C q 9 n D + 9 5 Q H tiple hypotheses {y i r }, where i = 1, ..., H and H is the number of hypotheses, and in this scenario we encode each hypothesis y i r separately using the same bidirectional encoder, and then concatenate their outputs in time to form h b . We keep the audio encoder unidirectional due to latency considerations. Then, two attention layers are followed to attend to acoustic encoding and first-pass hypothesis encoding separately. The two context vectors, c b and ce, are concatenated as inputs to a LAS decoder.</p><formula xml:id="formula_4">d F U q 6 g X g I 3 l Y l E = " &gt; A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z d O O y g r 1 A G 8 J k M m m H T i 7 M n I g h B H w V N y 4 U c e t z u P N t n L Y R t P W H g Y / / n M M 5 8 3 u J 4 A o s 6 8 t Y W l 5 Z X V u v b F Q 3 t 7 Z 3 d s 2 9 / Y 6 K U 0 l Z m 8 Y i l j 2 P K C Z 4 x N r A Q b B e I h k J P c G 6 3 v h 6 U u / e M 6 l 4 H N 1 B l j A n J M O I B 5 w S 0 J Z r H g 6 A P Y A X 5 F n h / q B f u G b N q l t T 4 U W w S 6 i h U i 3 X / B z 4 M U 1 D F g E V R K m + b S X g 5 E Q C p 4 I V 1 U G q W E L o m A x Z X 2 N E Q q a c f H p + g U + 0 4 + M g l v p F g K f u 7 4 m c h E p l o a c 7 Q w I j N V + b m P / V + i k E l 0 7 O o y Q F F t H Z o i A V G G I 8 y Q L 7 X D I K I t N A q O T 6 V k x H R B I K O r G q D s G e / / I i d B p 1 + 6 z e u D 2 v N a / K O C r o C B 2 j U 2 S j C 9 R E N 6 i F 2 o i i H D 2 h F / R q P B r P x p v x P m t d M s q Z A / R H x s c 3 i J C W f w = = &lt; / l a t e x i t &gt; c b &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 0 C Y J k g o f 8 6 f i k I 2 U 9 r / I 3 Z J s L 0 = " &gt; A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z d O O y g r 1 A G 8 J k O m m H T i Z h 5 k Q s I e C r u H G h i F u f w 5 1 v 4 7 S N o K 0 / D H z 8 5 x z O m T 9 I B N f g O F / W 0 v L K 6 t p 6 a a O 8 u b W 9 s 2 v v 7 b d 0 n C r K m j Q W s e o E R D P B J W s C B 8 E 6 i W I k C g R r B 6 P r S b 1 9 z 5 T m s b y D c c K 8 i A w k D z k l Y C z f P u w B e 4 A g z G j u / 2 C Q + 3 b F q T p T 4 U V w C 6 i g Q g 3 f / u z 1 Y 5 p G T A I V R O u u 6 y T g Z U Q B p 4 L l 5 V 6 q W U L o i A x Y 1 6 A k E d N e N j 0 / x y f G 6 e M w V u Z J w F P 3 9 0 R G I q 3 H U W A 6 I w J D P V + b m P / V u i m E l 1 7 G Z Z I C k 3 S 2 K E w F h h h P s s B 9 r h g F M T Z A q O L m V k y H R B E K J r G y C c G d / / I i t G p V 9 6 x a u z 2 v 1 K + K O E r o C B 2 j U + S i C 1 R H N 6 i B m o i i D D 2 h F / R q P V r P 1 p v 1 P m t d s o q Z A / R</formula><p>There are two major differences between our model and the LAS rescoring <ref type="bibr" target="#b9">[10]</ref>. First, the deliberation model attends to both e and yr, while <ref type="bibr" target="#b9">[10]</ref> only attends to the acoustic embedding, e. Second, our deliberation model encodes yr bidirectionally, while <ref type="bibr" target="#b9">[10]</ref> only relies on unidirectional encoding e for decoding. <ref type="bibr" target="#b9">[10]</ref> shows that the incompatibility between an RNN-T encoder and a LAS decoder leads to a gap between the rescoring model and LASonly model. To help adaptation, we introduce a 2-layer LSTM as an additional encoder (dashed box in Fig. <ref type="figure" target="#fig_0">1</ref> to indicate optional) to further encode e. We show in Sect. 4 that additional encoder layers improve both deliberation and LAS rescoring models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Additional Encoder Layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training</head><p>A deliberation model is typically trained from scratch by jointly optimizing all components <ref type="bibr" target="#b15">[16]</ref>. However, we find training a two-pass model from scratch tends to be unstable in practice <ref type="bibr" target="#b9">[10]</ref>, and thus use a two-step training process: Train the RNN-T as in <ref type="bibr" target="#b5">[6]</ref>, and then fix the RNN-T parameters and only train the deliberation decoder and additional encoder layers as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">MWER Loss</head><p>We apply the MWER loss <ref type="bibr" target="#b21">[22]</ref> in training which optimizes the expected word error rate by using n-best hypotheses:</p><formula xml:id="formula_5">LMWER(x, y * ) = B i=1 P (y i d |x)[W (y i d , y * ) − Ŵ ]<label>(1)</label></formula><p>where y i d is the ith hypothesis from the deliberation decoder, and W (y i d , y * ) the number of word errors for y i d w.r.t the ground truth target y * . P (y i d |x) is the probability of the ith hypothesis normalized over all other hypotheses to sum to 1. B is the beam size. In practice, we combine the MWER loss with cross-entropy (CE) loss to stabilize training: L MWER (x, y * ) = LMWER(x, y * ) + αLCE(x, y * ), where α = 0.01 as in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Joint Training</head><p>Training the deliberation decoder while fixing RNN-T parameters is not optimal since the model components are not jointly updated. We propose to use a combined loss to train all modules jointly:</p><formula xml:id="formula_6">Ljoint(θe, θ1, θ2) = LRNNT(θe, θ1) + λLCE(θe, θ2)<label>(2)</label></formula><p>where LRNNT(•) is the RNN-T loss, and LCE(•) the CE loss for the deliberation decoder. θe, θ1, and θ2 denote the parameters of shared encoder, RNN-T decoder, and deliberation decoder, respectively. Note that a jointly trained model can be further trained with MWER loss. The joint training is similar to "deep finetuning" in <ref type="bibr" target="#b9">[10]</ref> but without a pre-trained decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Decoding</head><p>Our decoding consists of two passes: 1) Decode using the RNN-T model to obtain the first-pass sequence yr, and 2) Attend to both yr and e, and perform the second beam search to generate y d . We are also curious how rescoring performs given bidirectional encoding from yr. In rescoring, we run the deliberation decoder on yr in a teacher-forcing mode <ref type="bibr" target="#b9">[10]</ref>. Note the difference from <ref type="bibr" target="#b9">[10]</ref> when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses. We compare rescoring and beam search in Sect. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>For training, we use the same multidomain datasets as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> which include anonymized and hand-transcribed English utterances from general Google traffic, far-field environments, telephony conversations, and YouTube. We augment the clean training utterances by artificially corrupting them by using a room simulator, varying degrees of noise, and reverberation such that the signal-to-noise ratio (SNR) is between 0dB and 30dB <ref type="bibr" target="#b22">[23]</ref>. We also use mixed-bandwidth utterances at 8kHz or 16 kHz for training <ref type="bibr" target="#b23">[24]</ref>.</p><p>Our main test set includes ~14K anonymized hand-transcribed VS utterances sampled from Google traffic. To evaluate the performance of proper noun recognition, we report performance on a side-by-side (SxS) test set, and 4 voice command test sets <ref type="bibr" target="#b5">[6]</ref>. The SxS set contains utterances where the LAS rescoring model <ref type="bibr" target="#b9">[10]</ref> performs inferior to a state-of-the-art conventional model <ref type="bibr" target="#b7">[8]</ref>, and one reason is due to proper nouns. The voice command test sets include 3 TTS test sets created using parallel-wavenet <ref type="bibr" target="#b24">[25]</ref>: Songs, Contacts-TTS, and Apps, where the commands include song, contact, and app names, respectively. The Contacts-Real set contains anonymized and hand-transcribed utterances from Google traffic to communicate with a contact, for example, "Call Jon Snow".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture Details and Training</head><p>Our first-pass RNN-T model has the same architecture as <ref type="bibr" target="#b5">[6]</ref>. The encoder of the RNN-T consists of an 8-layer Long Short-Term Memory (LSTM) <ref type="bibr" target="#b25">[26]</ref> and the prediction network contains 2 layers. Each LSTM layer has 2,048 hidden units followed by 640-dimensional projection. A time-reduction layer is added after the second layer to improve the inference speed without accuracy loss. Outputs of the encoder and prediction network are fed to a joint-network with 640 hidden units, which is followed by a softmax layer predicting 4,096 mixed-case wordpieces.</p><p>The deliberation decoder can attend to multiple hypotheses, and RNN-T hypotheses with different lengths are thus padded with end-of-sentence label \s to a length of 120. Each subword unit in a hypothesis is then mapped to a vector by a 96-dimensional embedding layer, and then encoded by a 2-layer bidirectional LSTM encoder, where each layer has 2,048 hidden units followed by 320-dimensional projection. Each of the two attention models is a multi-headed attention <ref type="bibr" target="#b26">[27]</ref> with four attention heads. The two output context vectors are concatenated and fed to a 2-layer LAS decoder (2,048 hidden units followed by 640-dimensional projection per layer). The LAS decoder has a 4,096-dimensional softmax layer to predict the same mixed-case wordpieces <ref type="bibr" target="#b18">[19]</ref> as the RNN-T.</p><p>For feature extraction, we use 128-dimensional log-Mel features from 32-ms windows at a rate of 10 ms. Each feature is stacked with three previous frames to form a 512-dimensional vector, and then downsampled to a 30-ms frame rate. Our models are trained in Tensorflow <ref type="bibr" target="#b27">[28]</ref> using the Lingvo framework <ref type="bibr" target="#b28">[29]</ref> on 8×8 Tensor Processing Units (TPU) slices with a global batch size of 4,096.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Computational Complexity</head><p>We estimate the computational complexity of the deliberation decoder using the number of floating-point operations (FLOPS) required:</p><formula xml:id="formula_7">FLOPS = MB • N • H + MD • N • B + FLOPSatten (<label>3</label></formula><formula xml:id="formula_8">)</formula><p>where MB is the size of the bidirectional encoder, N the number of decoded tokens, and H the number of first-pass hypotheses. MD denotes the size of the LAS decoder, and B the second beam search size. FLOPSatten is the FLOPS required for two attention layers, and we compute it as the sum of multiplying the sizes of source and query matrices with the number of time frames and N , respectively. Our deliberation decoder contains roughly 66M parameters, where the size of the bidirectional encoder is MB = 22M, LAS decoder is MD = 42M, and attention layers have 2M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>In this section we analyze the importance of certain components of the deliberation model by ablation studies, improve the model by MWER and AE layers, and select one of our best deliberation models for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Number of RNN-T Hypotheses</head><p>The deliberation decoder may attend to multiple first-pass hypotheses. We encode the hypotheses separately, and then concatenate them as the input to the attention layer. We use a beam size of 8 for RNN-T decoding. Unless stated otherwise, the WER we report is for VS test set. The third row in Table <ref type="table" target="#tab_1">1</ref> shows that the WER improves slightly when increasing the number of RNN-T hypotheses from 1 to 8. However, after applying MWER training, the WER improves continuously: 5.4% to 5.1%. We suspect that MWER training specifically helps deliberation attend to relevant parts of first-pass hypotheses. Since 8-hypothesis model gives the best performance, we use that for subsequent experiments. MWER training is not used for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Acoustics vs. Text</head><p>We are curious about how different attention (c b vs ce) contribute to deliberation, and thus train separate models where we attend to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Additional Encoder Layers</head><p>To help the deliberation decoder better adapt to the shared encoder, we add AE layers for dedicated encoding for the deliberation decoder. The AE consists of a 2-layer LSTM with 2,048 hidden units followed by 640-dimensional projection per layer. Beam search is used for decoding. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Rescoring</head><p>We propose to use the deliberation decoder to rescore first-pass RNN-T results, and expect bidirectional encoding to help compared to LAS rescoring <ref type="bibr" target="#b9">[10]</ref>. Table <ref type="table" target="#tab_6">5</ref> shows that the deliberation rescoring (E8) performs 5% relatively better than LAS rescoring (B3). AE layers are added to both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparisons</head><p>From the above analysis, an MWER trained 8-hypothesis deliberation model with AE layers performs the best, and thus we use that for comparison below.</p><p>In Table <ref type="table" target="#tab_5">4</ref>, we compare deliberation models with an RNN-T [6] and LAS rescoring model <ref type="bibr" target="#b9">[10]</ref>  AE layers (E9), and a jointly trained version (E10). For LAS twopass model, we add AE layers to the model in <ref type="bibr" target="#b9">[10]</ref> and evaluate both rescoring (B4) and beam search (B5). We note that all models are MWER trained except the RNN-T model, which we find little improvement. First, we note that two-pass models perform substantially better than RNN-T (B0) in both VS task (15%-25% WERR) and rare word test sets (e.g. up to 30% in E10 for the SxS set). This confirms that second-pass decoding brings additional benefits. Second, the MWER trained 8-hypothesis deliberation model with AE layers (E9) performs significantly better than LAS rescoring (B4) or beam search (B5). When beam search is used for both of the deliberation and LAS models, the WERR is 7% for VS, and 8% for the SxS set. We observe significant improvements for voice command test sets too. Third, joint training (E10) brings an additional 2% relative improvement for VS, 9% for the SxS set, and uniform improvements for voice command test sets.</p><p>To understand where the improvement comes from, in Fig. <ref type="figure">2</ref> we show an example of deliberation attention distribution on the RNN-T hypotheses (x-axis) at every step of the second-pass decoding (yaxis). We can see the attention selects mainly one wordpiece when the first-pass result is correct (e.g. " weather", " in", etc). However, when the first-pass output is wrong (e.g. "ond" and "on"), the attention looks ahead at " Nevada" for context information for correction. We speculate that the attention functions similarly as a context-aware language model on the first-pass sequence.</p><p>In Table <ref type="table" target="#tab_5">4</ref>, we also report gigaFLOPS (GFLOPS) estimated using Eq. ( <ref type="formula" target="#formula_7">3</ref>) on the 90%-tile VS set, where an utterance has roughly 109 audio frames and a decoded sequence of 14 tokens. Since the deliberation decoder has a larger size than LAS decoder (67MB vs. 33MB), it requires around 1.8 times GFLOPS as LAS rescoring. The increase mainly comes from the bidirectional encoder for 8 first-pass hypotheses. However, we note that the computation can be parallelized across hypotheses <ref type="bibr" target="#b9">[10]</ref> and should have less impact on latency. Latency estimation is complicated, and we will quantify that in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Decoding Examples</head><p>Lastly, we compare some decoding examples between deliberation and LAS rescoring in Table <ref type="table" target="#tab_7">6</ref>. One type of wins for deliberation is URL, where the deliberation model corrects and concatenates string pieces to a single one since it sees the whole first-pass hypothesis. Second type is proper noun. Leveraging the context, deliberation re-Fig. <ref type="figure">2</ref>. Example attention probabilities on a first-pass RNN-T hypothesis: "Weather in London Nevada", for generating the second-pass result "Weather in Lund Nevada". Brighter colors correspond to higher probabilities. A beginning wordpiece starts with a space marker (i.e., " "). s denotes start of sentence, and \s the end of sentence. alizes the previous word should be a proper noun (i.e. Walmart). Third, the deliberation decoder corrects semantic errors (china → train). On the other hand, we also see some losses of deliberation due to over-correction of proper nouns or spelling difference. The former is probably from knowledge in training, and the latter is benign and does not affect semantics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAS rescoring</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Diagram of the deliberation model with an optional additional encoder (dashed box).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>WERs (%) of deliberation with different number of RNN-T hypotheses.</figDesc><table><row><cell>ID</cell><cell>E1</cell><cell>E2</cell><cell>E3</cell><cell>E4</cell></row><row><cell>Model</cell><cell cols="4">1 hyp 2 hyp 4 hyp 8 hyp</cell></row><row><cell>Deliberation</cell><cell>5.5</cell><cell>5.4</cell><cell>5.4</cell><cell>5.4</cell></row><row><cell>+ MWER</cell><cell>5.4</cell><cell>5.3</cell><cell>5.2</cell><cell>5.1</cell></row><row><cell cols="5">either acoustics (E5) or text (E6) alone in training and inference.</cell></row><row><cell cols="5">Table 2 shows that either E5 or E6 perform significantly better than</cell></row><row><cell cols="5">the baseline RNN-T model (B0) with a 9% WERR. By using both</cell></row><row><cell cols="5">attentions (E4), the model gains another 11% relative improvement.</cell></row><row><cell cols="5">It seems surprising that E6 performs equally to E5. We note this</cell></row><row><cell cols="5">could be because E6 has a bidirectional encoder while E5 does not.</cell></row><row><cell>ID</cell><cell>Model</cell><cell></cell><cell>WER (%)</cell><cell></cell></row><row><cell>B0</cell><cell>RNN-T</cell><cell></cell><cell>6.7</cell><cell></cell></row><row><cell>E5</cell><cell cols="2">Acoustics alone</cell><cell>6.1</cell><cell></cell></row><row><cell>E6</cell><cell cols="2">Text alone</cell><cell>6.1</cell><cell></cell></row><row><cell>E4</cell><cell>Both</cell><cell></cell><cell>5.4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>WERs (%) of baseline RNN-T model and deliberation models with different attention setup.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>, we show that with AE layers (E7)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>WERs (%) with or without AE layers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>in different recognition tasks including VS and proper noun recognition. We include two deliberation models: An MWER trained 8-hypothesis deliberation model with Comparison of RNN-T, LAS two-pass and deliberation models in WERs (%) and GFLOPS. LAS two-pass and deliberation models are augmented with AE layers. All models are trained with MWER loss except the RNN-T model.</figDesc><table><row><cell>ID</cell><cell>Model</cell><cell></cell><cell>Decoding</cell><cell cols="5">WER (%) VS SxS Songs Contacts-Real Contacts-TTS Apps</cell><cell>Estimated GFLOPS</cell></row><row><cell>B0</cell><cell cols="2">RNN-T</cell><cell cols="2">Beam search 6.7 35.2</cell><cell>11.9</cell><cell>15.9</cell><cell>24.3</cell><cell>7.8</cell><cell>3.5</cell></row><row><cell>B4</cell><cell cols="2">LAS [10]</cell><cell>Rescoring</cell><cell>5.7 31.4</cell><cell>10.9</cell><cell>14.7</cell><cell>22.6</cell><cell>7.5</cell><cell>4.8</cell></row><row><cell>B5</cell><cell cols="2">LAS [10]</cell><cell cols="2">Beam search 5.5 29.0</cell><cell>11.7</cell><cell>14.7</cell><cell>22.9</cell><cell>8.3</cell><cell>4.8</cell></row><row><cell>E9</cell><cell cols="2">Deliberation</cell><cell cols="2">Beam search 5.1 26.6</cell><cell>9.9</cell><cell>13.7</cell><cell>22.3</cell><cell>7.1</cell><cell>8.8</cell></row><row><cell>E10</cell><cell cols="4">+ Joint training Beam search 5.0 24.3</cell><cell>9.6</cell><cell>13.4</cell><cell>22.0</cell><cell>6.4</cell><cell>8.8</cell></row><row><cell></cell><cell>ID</cell><cell>Rescoring</cell><cell>WER (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>E8</cell><cell>Deliberation</cell><cell>5.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>B3</cell><cell>LAS + AE</cell><cell>6.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison of rescoring models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Decoding examples of deliberation and LAS rescoring. Deliberation wins are in green and losses in red.We presented a new two-pass E2E ASR based on the deliberation network, and our best model obtained significant improvements over LAS rescoring in both VS tasks and proper noun recognition: 12% and 23% WERR, respectively. The model also performs 21% relatively better than a large conventional model for VS. Although the model requires more computation than LAS rescoring, batching across hypotheses can improve latency.</figDesc><table><row><cell></cell><cell>Deliberation</cell></row><row><cell>Quality times.com</cell><cell>quadcitytimes.com</cell></row><row><cell cols="2">Where my job application Walmart job application</cell></row><row><cell>china near me</cell><cell>train near me</cell></row><row><cell>bio of Chesty Fuller</cell><cell>bio of Chester Fuller</cell></row><row><cell>2016 Kia Forte5</cell><cell>2016 Kia Forte 5</cell></row><row><cell cols="2">5. CONCLUSION</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring architectures, data and units for streaming end-to-end speech recognition with RNNtransducer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Endto-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint CTC-attention based end-toend speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6381" to="6385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">State-of-theart speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lower frame rate neural network acoustic models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="22" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RWTH ASR systems for LibriSpeech: Hybrid vs attention</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two-pass end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2773" to="2777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural models of text normalization for speech applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="337" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Investigation of transformer based spelling correction model for CTC-based end-to-end mandarin speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2180" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A spelling correction model for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5651" to="5655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An analysis of incorporating an external language model into a sequence-to-sequence model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5824" to="5828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving performance of end-to-end ASR on numeric sequences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2185" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1784" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Achieving human parity on automatic Chinese to English news translation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05567</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech-to-text translation with two-pass decoding</title>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7175" to="7179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Japanese and Korean voice search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toward domain-invariant speech recognition via large scale training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elfeky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT 2018</title>
				<meeting>SLT 2018</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="441" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing long-form speech using streaming end-toend models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum word error rate training for attention-based sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4839" to="4843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in google home</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="379" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3605</idno>
		<title level="m">Feature learning in deep neural networks-studies on speech recognition tasks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V D</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10433</idno>
		<title level="m">Parallel wavenet: Fast high-fidelity speech synthesis</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lingvo: A modular and scalable framework for sequence-to-sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08295</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
