<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Text-Conditional Image Generation with CLIP Latents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
							<email>aramesh@openai.com</email>
						</author>
						<author>
							<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
							<email>prafulla@openai.com</email>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Hierarchical Text-Conditional Image Generation with CLIP Latents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent progress in computer vision has been driven by scaling models on large datasets of captioned images collected from the internet [9, <ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b12">14]</ref>. Within this framework, CLIP <ref type="bibr" target="#b32">[34]</ref> has emerged as a successful representation learner for images. CLIP embeddings have a number of desirable properties: they are robust to image distribution shift, have impressive zero-shot capabilities, and have been fine-tuned to achieve state-of-the-art results on a wide variety of vision and language tasks <ref type="bibr" target="#b38">[40]</ref>. Concurrently, diffusion models <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b21">23]</ref> have emerged as a promising generative modeling framework, pushing the state-of-the-art on image and video generation tasks <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b20">22]</ref>. To achieve best results, diffusion models leverage a guidance technique <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b20">22]</ref> which improves sample fidelity (for images, photorealism) at the cost of sample diversity.</p><p>In this work, we combine these two approaches for the problem of text-conditional image generation. We first train a diffusion decoder to invert the CLIP image encoder. Our inverter is non-deterministic, and can produce multiple images corresponding to a given image embedding. The presence of an encoder and its approximate inverse (the decoder) allows for capabilities beyond text-to-image translation. As in GAN inversion <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b48">50]</ref>, encoding and decoding an input image produces semantically similar output images. We can also interpolate between input images by inverting interpolations of their image embeddings. However, one notable advantage of using the CLIP latent space is the ability to semantically modify images by moving in the direction of any encoded text vector, whereas discovering these directions in GAN latent space involves luck and diligent manual examination. vibrant portrait painting of Salvador Dalí with a robotic half face a shiba inu wearing a beret and black turtleneck a close up of a handpalm with leaves growing from it an espresso machine that makes coffee from human souls, artstation panda mad scientist mixing sparkling chemicals, artstation a corgi's head depicted as an explosion of a nebula a dolphin in an astronaut suit on saturn, artstation a propaganda poster depicting a cat dressed as french emperor napoleon holding a piece of cheese a teddybear on a skateboard in times square  Furthermore, encoding and decoding images also provides us with a tool for observing which features of the image are recognized or disregarded by CLIP.</p><p>To obtain a full generative model of images, we combine the CLIP image embedding decoder with a prior model, which generates possible CLIP image embeddings from a given text caption. We compare our text-to-image system with other systems such as DALL-E <ref type="bibr" target="#b33">[35]</ref> and GLIDE <ref type="bibr" target="#b28">[30]</ref>, finding that our samples are comparable in quality to GLIDE, but with greater diversity in our generations. We also develop methods for training diffusion priors in latent space, and show that they achieve comparable performance to autoregressive priors, while training much more efficiently. We refer to our full text-conditional image generation stack as unCLIP, since it generates images by inverting the CLIP image encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our training dataset consists of pairs (x, y) of images x and their corresponding captions y. Given an image x, let z i and z t be its CLIP image and text embeddings, respectively. We design our generative stack to produce images from captions using two components:</p><p>• A prior P (z i |y) that produces CLIP image embeddings z i conditioned on captions y. The first equality holds because z i is a deterministic function of x. The second equality holds because of the chain rule. Thus, we can sample from the true conditional distribution P (x|y) by first sampling z i using the prior, and then sampling x using the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Decoder</head><p>We use diffusion models <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b41">43]</ref> to produce images conditioned on CLIP image embeddings (and optionally text captions). Specifically, we modify the 3.5 billion parameter GLIDE <ref type="bibr" target="#b28">[30]</ref> model by projecting and adding CLIP embeddings to the existing timestep embedding, and by projecting CLIP embeddings into four extra tokens of context that are concatenated to the sequence of outputs from the GLIDE text encoder. We retained the text conditioning pathway present in the original GLIDE model, hypothesizing that it could allow the diffusion model to learn aspects of natural language that CLIP fails to capture (e.g. variable binding), but find that it offers little help in this regard (Section 6). We enable classifier-free guidance <ref type="bibr" target="#b20">[22]</ref> by randomly setting the CLIP embeddings to zero (or a learned embedding) 10% of the time, and randomly dropping the text caption 50% of the time.</p><p>To generate high resolution images, we train two diffusion upsampler models <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b36">38]</ref>: one to upsample images from 64 × 64 to 256 × 256, and another to further upsample those to 1024 × 1024. To improve the robustness of our upsamplers, we slightly corrupt the conditioning images during training using gaussian blur <ref type="bibr" target="#b36">[38]</ref> for upsampling to 256 × 256 images, and a more diverse BSR degradation <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b52">54]</ref> for upsampling to 1024 × 1024 images. To reduce training compute and improve numerical stability, we follow Rombach et al.</p><p>[37] and train on random crops of images that are one-fourth the target size. We use only spatial convolutions in the model and at inference time directly apply the model at the target resolution, observing that it readily generalizes to the higher resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prior</head><p>While a decoder can invert CLIP image embeddings z i to produce images x, we need a prior model that produces z i from captions y to enable image generations from text captions. We explore two different model classes for the prior model:</p><p>• Autoregressive (AR) prior: the CLIP image embedding z i is converted into a sequence of discrete codes and predicted autoregressively conditioned on the caption y. • Diffusion prior: The continuous vector z i is directly modelled using a Gaussian diffusion model conditioned on the caption y</p><p>In addition to the caption, we can condition on the CLIP text embedding z t since it is a deterministic function of the caption. We additionally allow for sampling using classifier-free guidance for both the AR and diffusion prior by randomly dropping this text conditioning information during training.</p><p>To train and sample from the AR prior more efficiently, we first reduce the dimensionality of the CLIP image embeddings z i by applying Principal Component Analysis (PCA) <ref type="bibr" target="#b30">[32]</ref>. In particular, we find that the rank of the CLIP representation space is drastically reduced when training CLIP with SAM <ref type="bibr" target="#b11">[13]</ref> while slightly improving evaluation metrics. We are able to preserve nearly all of the information<ref type="foot" target="#foot_0">2</ref> by retaining only 319 principal components out of the original 1,024. After applying PCA, we order the principal components by decreasing eigenvalue magnitude, quantize each of the 319 dimensions into 1,024 discrete buckets, and predict the resulting sequence using a decoder-only Transformer <ref type="bibr" target="#b46">[48]</ref> model with a causal attention mask.</p><p>We condition the AR prior on the text caption and the CLIP text embedding by encoding them as a prefix to the sequence. Additionally, we prepend a token indicating the (quantized) dot product between the text embedding and image embedding, z i • z t . This allows us to condition the model on a higher dot product, since higher text-image dot products correspond to captions which better describe the image. In practice, we find it beneficial to sample the dot product from the top half of the distribution. <ref type="foot" target="#foot_1">3</ref>For the diffusion prior, we train a decoder-only Transformer with a causal attention mask on a sequence consisting of encoded text, the CLIP text embedding, an embedding for the diffusion timestep, the noised CLIP image embedding, and a final embedding whose output from the Transformer is used to predict the unnoised CLIP image embedding. Instead of using the ϵ-prediction formulation from Ho et al. <ref type="bibr" target="#b21">[23]</ref>, we find it better to train our model to predict the unnoised z i directly, and use a mean-squared error loss on this prediction:</p><formula xml:id="formula_0">L prior = E t∼[1,T ],z (t) i ∼qt ∥f θ (z (t) i , t, y) − z i ∥ 2</formula><p>We choose not to condition the diffusion prior on z i • z t like in the AR prior; instead, we improve quality during sampling time by generating two samples of z i and selecting the one with a higher dot product with z t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Variations</head><p>Given an image x, we can obtain its CLIP image embedding z i and then use our decoder to "invert" z i , producing new images that we call variations of our input. To make the variations more similar to the original image x, we first perform DDIM inversion <ref type="bibr" target="#b8">[10]</ref> of x with the decoder conditioned on z i to obtain its DDIM inverted latent z T . We can then use z T as the initial noise when generating the images through the decoder.</p><p>We use DDIM <ref type="bibr" target="#b40">[42]</ref> with η &gt; 0 for sampling, with higher values of η giving rise to more diverse variations.</p><p>It is also possible to combine two images for variations. To do so, we perform spherical interpolation of their CLIP embeddings z i and z j to obtain intermediate z θ = slerp(z i , z j , θ), and produce variations of z θ by passing it through the decoder.</p><p>3 Exploring the CLIP Latent Space Using the unCLIP Decoder</p><p>Our decoder model provides a unique opportunity to explore CLIP latent space by allowing us to directly visualize what the CLIP image encoder is seeing. In Figure <ref type="figure" target="#fig_1">2</ref>, we encode an image using CLIP, and then decode its image embedding using the diffusion decoder, obtaining variations of the image (Section 2.3).</p><p>These variations tell us what information was captured in the CLIP image embedding (and thus is preserved across samples), and what was lost (and thus changes across the samples). One can also interpolate between CLIP embeddings to mix information between two images, as shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>As an example use case, we can revisit cases where CLIP makes incorrect predictions, such as typographic attacks <ref type="bibr" target="#b16">[18]</ref>. In these adversarial images, a piece of text is overlayed on top of an object, which causes CLIP to predict the object described by the text rather than the object depicted in the image. This piece of text essentially hides the original object in terms of output probabilities. In Figure <ref type="figure">5</ref>, we show an example of this attack from <ref type="bibr" target="#b16">[18]</ref>, wherein an apple can be misclassified as an iPod. Surprisingly, we find that our decoder still generates pictures of apples with high probability even though the predicted probability of "Granny Smith" is near zero. Even more notable, the model never produces pictures of iPods, despite the very high relative predicted probability of this caption.</p><p>Another possibility is to probe the structure of the CLIP latent space itself. In Figure <ref type="figure" target="#fig_4">6</ref>, we take the CLIP image embeddings of a handful of source images and reconstruct them with progressively more PCA dimensions, and then visualize the reconstructed image embeddings using our decoder with DDIM on a fixed seed. This allows us to see what semantic information the different dimensions encode. We observe that the early PCA dimensions preserve coarse-grained semantic information such as what types of objects are in the scene, whereas the later PCA dimensions encode finer-grained detail such as the shapes and exact form of the objects. For example, in the first scene, the earlier dimensions seem to encode that there is food and perhaps Granny Smith: 100% iPod: 0% Pizza: 0%</p><p>Granny Smith: 0.02% iPod: 99.98% Pizza: 0%</p><p>Granny Smith: 94.33% iPod: 0% Pizza: 5.66% Figure <ref type="figure">5</ref>: Variations of images featuring typographic attacks <ref type="bibr" target="#b16">[18]</ref> paired with the CLIP model's predicted probabilities across three labels. Surprisingly, the decoder still recovers Granny Smith apples even when the predicted probability for this label is near 0%. We also find that our CLIP model is slightly less susceptible to the "pizza" attack than the models investigated in <ref type="bibr" target="#b16">[18]</ref>. a container present, whereas the later dimensions encode tomatoes and a bottle specifically. Figure <ref type="figure" target="#fig_4">6</ref> also serves as a visualization of what the AR prior is modeling, since the AR prior is trained to explicitly predict these principal components in this order.</p><p>4 Text-to-Image Generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Importance of the Prior</head><p>Although we train a prior to generate CLIP image embeddings from captions, the prior is not strictly necessary for caption-to-image generation. For instance, our decoder can condition on both CLIP image embeddings and captions, but the CLIP image embedding is dropped 5% of the time during training in order to enable classifier-free guidance. Therefore, at sampling time, we can condition on only the caption, although this underperforms a model trained fully in this way (this model is GLIDE, and we do a thorough comparison with GLIDE in Sections 4.2 and 4.3). Another possibility is to feed the decoder the CLIP text embedding as if it were an image embedding, as previously observed <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b47">49]</ref>. The first two rows of Figure <ref type="figure" target="#fig_5">7</ref> depicts samples obtained in these two ways; the third row depicts samples obtained with a prior. Conditioning the decoder Table <ref type="table">1</ref>: Human evaluations comparing unCLIP to GLIDE. We compare to both the AR and diffusion prior for unCLIP. Reported figures are 95% confidence intervals of the probability that the unCLIP model specified by the row beats GLIDE. Sampling hyperparameters for all models were swept to optimize an automated proxy for human photorealism evaluations.</p><p>on just the caption is clearly worst, but conditioning on text embeddings zero-shot does produce reasonable results. Building on this observation, another approach would be to train the decoder to condition on CLIP text embeddings <ref type="bibr" target="#b7">[8]</ref> instead of CLIP image embeddings (although we would lose the capabilities mentioned in Section 3).</p><p>To quantify the effectiveness of these alternate approaches, we train two models: a small decoder conditioned on CLIP text embeddings, and a small unCLIP stack (diffusion prior and decoder). We then compare samples from the text-embedding decoder, samples from the unCLIP stack, and samples obtained from feeding text embeddings to the unCLIP decoder zero-shot, sweeping across guidance scales for all models. We find that these approaches respectively score FIDs of 9.16, 7.99, and 16.55 on a test set, suggesting the unCLIP approach is best. We also run human evaluations comparing the first two settings, sweeping over sampling hyperparameters for each using our human evaluation proxy model (Appendix A). We find that humans prefer the full unCLIP stack 57.0% ± 3.1% of the time for photorealism and 53.1% ± 3.1% of the time for caption similarity.</p><p>Given the importance of the prior, it is worth evaluating different approaches to train the prior. We compare both the AR and diffusion priors throughout our experiments. In all cases (Sections 4.2, 4.4, and 4.5), we find that the diffusion prior outperforms the AR prior for comparable model size and reduced training compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Evaluations</head><p>We observe in Figure <ref type="figure" target="#fig_0">1</ref> that unCLIP is capable of synthesizing complex, realistic images. While we can compare sample quality to past models using FID, its not always aligned with human judgment. To better gauge the generation capabilities of our system, we conduct systematic human evaluations comparing unCLIP to GLIDE for photorealism, caption similarity, and sample diversity.</p><p>We follow the protocol of Ramesh et al., Nichol et al. <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b28">30]</ref> for the first two evaluations: for photorealism, users are presented with pairs of images and must choose which looks more photorealistic; for caption similarity, users are additionally prompted with a caption, and must choose which image better matches the caption. In both evaluations, there is a third "Not sure" option. For diversity, we propose a new evaluation protocol in which humans are presented with two 4 × 4 grids of samples and must choose which is more diverse (with a third option, "Not sure"). For this evaluation, we produce sample grids using 1,000 captions from the MS-COCO validation set, and always compare sample grids for the same caption. Before running human comparisons, we swept over sampling hyperparameters for each model using a CLIP linear probe trained to be a proxy for human photorealism evaluations (Appendix A). These hyperparameters are fixed across all three types of evaluation.</p><p>We present our results in Table <ref type="table">1</ref>. In general, the diffusion prior performs better than the AR prior in pairwise comparisons against GLIDE. We find that humans still slightly prefer GLIDE to unCLIP in terms of photorealism, but the gap is very small. Even with similar photorealism, unCLIP is strongly preferred over GLIDE in terms of diversity, highlighting one of its benefits. : Samples when increasing guidance scale for both unCLIP and GLIDE, using the prompt, "A green vase filled with red roses sitting on top of table." For unCLIP, we fix the latent vectors sampled from the prior, and only vary the guidance scale of the decoder. For both models, we fix the diffusion noise seed for each column. Samples from unCLIP improve in quality (more realistic lighting and shadows) but do not change in content as we increase guidance scale, preserving semantic diversity even at high decoder guidance scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Improved Diversity-Fidelity Trade-off with Guidance</head><p>Compared to GLIDE, we qualitatively observe that unCLIP is able to generate more diverse images while leveraging the guidance technique to improve sample quality. To understand why, consider Figure <ref type="figure">8</ref> where we increase guidance scale for both GLIDE and unCLIP. For GLIDE, the semantics (camera angle, color, size) converge as we increase guidance scale, whereas for unCLIP the semantic information of the scene is frozen in the CLIP image embedding and therefore does not collapse when guiding the decoder.</p><p>In Section 4.2, we observed that unCLIP achieves similar photorealism as GLIDE while maintaining more diversity, but that its caption matching capabilities were slightly worse. It is natural to ask whether GLIDE's guidance scale can be lowered to obtain the same diversity level as unCLIP while maintaining better caption matching. In Figure <ref type="figure">9</ref>, we conduct a more careful study of this question by performing human evaluations across several GLIDE guidance scales. We find that GLIDE at guidance scale 2.0 is very close to the photorealism and caption similarity of unCLIP, while still producing less diverse samples.</p><p>Finally, in Figure <ref type="figure" target="#fig_0">10</ref> we compute MS-COCO zero-shot FID <ref type="bibr" target="#b19">[21]</ref> while sweeping over guidance scale for both unCLIP and GLIDE, finding that guidance hurts the FID of unCLIP much less so than for GLIDE. In this evaluation, we fix the guidance scale of the unCLIP prior and only vary the guidance scale of the decoder. This is another indication that guidance hurts the diversity of GLIDE much more than unCLIP, since FID heavily penalizes non-diverse generations.  <ref type="figure">9</ref>: When comparing unCLIP (with our best sampling settings) to various settings of guidance scale for GLIDE, unCLIP was preferred by human evaluators on at least one axis among photorealism, caption similarity, and diversity for each comparison. At the higher guidance scales used to generate photorealistic images, unCLIP yields greater diversity for comparable photorealism and caption similarity. Figure <ref type="figure" target="#fig_0">10</ref>: FID versus guidance scale for unCLIP and GLIDE. For the unCLIP priors, we swept over sampling hyperparameters and fixed to the settings with the best minimum FID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison on MS-COCO</head><p>In the text-conditional image generation literature, it has become standard practice to evaluate FID on the MS-COCO <ref type="bibr" target="#b23">[25]</ref> validation set. We present results on this benchmark in Table <ref type="table" target="#tab_1">2</ref>. Like GLIDE and DALL-E, unCLIP is not directly trained on the MS-COCO training set, but can still generalize to the validation set zero-shot. We find that, compared to these other zero-shot models, unCLIP achieves a new state-of-the-art FID of 10.39 when sampling with the diffusion prior. In Figure <ref type="figure" target="#fig_9">11</ref>, we visually compare unCLIP to various recent text-conditional image generation models on several captions from MS-COCO. We find that, like the other methods, unCLIP produces realistic scenes that capture the text prompts. For our model, we achieve the best results using the diffusion prior with guidance scale 1.25 for both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Aesthetic Quality Comparison</head><p>We additionally perform automated aesthetic quality evaluations comparing unCLIP to GLIDE. Our goal with this evaluation is to assess how well each model produces artistic illustrations and photographs. To this end, we generated 512 "artistic" captions using GPT-3 <ref type="bibr" target="#b2">[3]</ref> by prompting it with captions for existing artwork (both real and AI generated). Next, we trained a CLIP linear probe to predict human aesthetic judgments using the AVA dataset <ref type="bibr" target="#b26">[28]</ref> (Appendix A). For each model and set of sampling hyperparameters, we produce four images for each prompt, and report the mean predicted aesthetic judgment over the full batch of 2048 images.</p><p>In Figure <ref type="figure" target="#fig_1">12</ref>, we present results on our aesthetic quality evaluation. We find that guidance improves aesthetic quality for both GLIDE and unCLIP. For unCLIP, we only guide the decoder (we found that guiding the prior hurt results). We also plot the aesthetic quality against Recall<ref type="foot" target="#foot_2">4</ref> , since guidance typically induces a trade-off between fidelity and diversity. Interestingly, we find that guiding unCLIP does not decrease Recall while still improving aesthetic quality according to this metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Synthetic image generation is a well studied problem, and most popular techniques for unconditional image generation have also been applied to the text-conditional setting. Many previous works have trained GANs <ref type="bibr" target="#b17">[19]</ref> on publicly available image captioning datasets to produce text-conditional image samples <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b50">52]</ref>. Other works have adapted the VQ-VAE approach <ref type="bibr" target="#b45">[47]</ref> to text-conditional image generation by training autoregressive transformers on sequences of text tokens followed by image tokens <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b0">1]</ref>. Finally, some works have applied diffusion models to the problem, training either continuous <ref type="bibr" target="#b28">[30]</ref> or discrete <ref type="bibr" target="#b18">[20]</ref> diffusion models with auxiliary text encoders to handle textual input.</p><p>Previous works have leveraged hierarchical generative processes to create high-quality synthetic images. Razavi et al. <ref type="bibr" target="#b34">[36]</ref> trains a multi-layer discrete autoencoder, allowing them to first sample coarse-grained latent codes and then use this as conditioning information when sampling higher-resolution latent codes. Child, Vahdat and Kautz <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">45]</ref> generate images using VAEs with a hierarchy of latent codes that increase progressively with resolution. Concurrently with our work, Gafni et al. <ref type="bibr" target="#b13">[15]</ref> conditions a generative image  model on segmentation masks, allowing for a generative process that first samples a semantic map of an image and then conditions the generated image on this information.</p><p>The computational benefits of using diffusion to model a latent space has been noted by previous works. Preechakul et al. <ref type="bibr" target="#b31">[33]</ref> propose an autoencoder framework where diffusion models are used to render latent variables as images, and a second diffusion model is used to generate these latents (similar to our diffusion Figure <ref type="figure" target="#fig_1">12</ref>: Aesthetic quality evaluations comparing GLIDE and unCLIP using 512 auto-generated artistic prompts. We find that both models benefit from guidance, but unCLIP does not sacrifice recall for aesthetic quality.</p><p>prior). Vahdat et al. <ref type="bibr" target="#b44">[46]</ref> use a score-based model for the latent space of a VAE, while Rombach et al. <ref type="bibr" target="#b35">[37]</ref> use diffusion models on the latents obtained from a VQGAN <ref type="bibr" target="#b10">[12]</ref> like autoencoder.</p><p>Since its release, CLIP <ref type="bibr" target="#b32">[34]</ref> has been used extensively to steer generative image models towards text prompts.  <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> use an unnoised CLIP model to guide unconditional or class-conditional diffusion models. Ho Salimans <ref type="bibr" target="#b20">[22]</ref> introduced classifier-free guidance and showed that one can perform guidance implictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. Nichol et al. <ref type="bibr" target="#b28">[30]</ref> showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation.</p><p>Several previous works have trained generative image models that are directly conditioned on CLIP embeddings. Zhou et al. <ref type="bibr" target="#b54">[56]</ref> condition GAN models on randomly perturbed CLIP image embeddings, finding that these models can generalize to CLIP text embeddings to produce text-conditional images. Crowson <ref type="bibr" target="#b7">[8]</ref> trained diffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional image generation. Wang et al. <ref type="bibr" target="#b47">[49]</ref> train an autoregressive generative model conditioned on CLIP image embeddings, finding that it generalizes to CLIP text embeddings well enough to allow for text-conditional image synthesis.</p><p>Bordes et al. <ref type="bibr" target="#b1">[2]</ref> train diffusion models conditioned on image representations from contrastive models. While the diffusion models themselves cannot generate images unconditionally, the authors experimented with a simple approach for two-stage image generation by employing Kernel Density Estimation to sample image representations. By feeding these generated representations to the diffusion model, they can generate images end-to-end in a way similar to our proposed technique. However, our work differs from this in two ways: first, we use multimodal contrastive representations rather than image-only representations; second, we employ much more powerful generative models for the first stage of the generation hierarchy, and these generative models are conditioned on text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Although conditioning image generation on CLIP embeddings improves diversity, this choice does come with certain limitations. In particular, unCLIP is worse at binding attributes to objects than a corresponding GLIDE  model. In Figure <ref type="figure" target="#fig_11">13</ref>, we find that unCLIP struggles more than GLIDE with a prompt where it must bind two separate objects (cubes) to two separate attributes (colors). We hypothesize that this occurs because the CLIP embedding itself does not explicitly bind attributes to objects, and find that reconstructions from the decoder  often mix up attributes and objects, as shown in Figure <ref type="figure" target="#fig_12">14</ref>. A similar and likely related issue is that unCLIP struggles at producing coherent text, as illustrated in Figure <ref type="figure" target="#fig_13">15</ref>; it is possible that the CLIP embedding does not precisely encode spelling information of rendered text. This issue is likely made worse because the BPE encoding we use obscures the spelling of the words in a caption from the model, so the model needs to have independently seen each token written out in the training images in order to learn to render it.</p><p>We also note that our stack still a hard time producing details in complex 16). hypothesize that is a limitation decoder hierarchy producing image base resolution of 64 × 64 and then upsampling it. Training our unCLIP decoder at a higher base resolution should be to alleviate this, cost needing more training and compute. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Random samples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Selected 1024 × 1024 samples from a production version of our model.</figDesc><graphic url="image-7.png" coords="2,17.44,499.15,189.72,189.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Variations of an input image by encoding with CLIP and then decoding with a diffusion model. The variations preserve both semantic information like presence of a clock in the painting and the overlapping strokes in the logo, as well as stylistic elements like the surrealism in the painting and the color gradients in the logo, while varying the non-essential details.</figDesc><graphic url="image-12.png" coords="3,93.79,171.14,209.91,209.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: A high-level overview of unCLIP. Above the dotted line, we depict the CLIP training process, through which we learn a joint representation space for text and images. Below the dotted line, we depict our text-to-image generation process: a CLIP text embedding is first fed to an autoregressive or diffusion prior to produce an image embedding, and then this embedding is used to condition a diffusion decoder which produces a final image. Note that the CLIP model is frozen during training of the prior and decoder.</figDesc><graphic url="image-14.png" coords="4,117.50,95.04,377.00,161.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Variations between two images by interpolating their CLIP image embedding and then decoding with a diffusion model. We fix the decoder seed across each row. The intermediate variations naturally blend the content and style from both input images.</figDesc><graphic url="image-16.png" coords="6,141.07,95.04,329.85,141.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of reconstructions of CLIP latents from progressively more PCA dimensions (20, 30, 40, 80, 120, 160, 200, 320 dimensions), with the original source image on the far right. The lower dimensions preserve coarse-grained semantic information, whereas the higher dimensions encode finer-grained details about the exact form of the objects in the scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Samples using different conditioning signals for the same decoder.In the first row, we pass the text caption to the decoder, and pass a zero vector for the CLIP embedding. In the second row, we pass both the text caption and the CLIP text embedding of the caption. In the third row, we pass the text and a CLIP image embedding generated by an autoregressive prior for the given caption. Note that this decoder is only trained to do the text-to-image generation task (without the CLIP image representation) 5% of the time.</figDesc><graphic url="image-40.png" coords="9,110.37,255.77,77.10,77.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure8: Samples when increasing guidance scale for both unCLIP and GLIDE, using the prompt, "A green vase filled with red roses sitting on top of table." For unCLIP, we fix the latent vectors sampled from the prior, and only vary the guidance scale of the decoder. For both models, we fix the diffusion noise seed for each column. Samples from unCLIP improve in quality (more realistic lighting and shadows) but do not change in content as we increase guidance scale, preserving semantic diversity even at high decoder guidance scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>train is coming down the tracks" "a group of skiers are preparing to ski down a mountain.""a small kitchen with a low ceiling" "a group of elephants walking in muddy water.""a living area with a television and a table"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Random image samples on MS-COCO prompts.</figDesc><graphic url="image-73.png" coords="13,104.02,433.72,81.39,81.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Samples from unCLIP and GLIDE for the prompt "a red cube on top of a blue cube".</figDesc><graphic url="image-80.png" coords="15,122.04,337.45,85.68,85.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure14: Reconstructions from the decoder for difficult binding problems. We find that the reconstructions mix up objects and attributes. In the first two examples, the model mixes up the color of two objects. In the rightmost example, the model does not reliably reconstruct the relative size of two objects.</figDesc><graphic url="image-83.png" coords="15,104.90,437.31,119.94,119.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Samples from unCLIP for the prompt, "A sign that says deep learning."</figDesc><graphic url="image-87.png" coords="16,123.93,217.01,364.13,182.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: unCLIP samples showing low levels of detail for some complex scenes.</figDesc><graphic url="image-88.png" coords="16,123.93,417.18,364.13,182.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :Figure 18 :Figure 19 :</head><label>171819</label><figDesc>Figure 17: Random samples from unCLIP for prompt "Vibrant portrait painting of Salvador Dali with a robotic half face"</figDesc><graphic url="image-89.png" coords="22,91.80,126.13,428.34,428.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of FID on MS-COCO 256 × 256.</figDesc><table><row><cell>Model</cell><cell>FID</cell><cell cols="2">Zero-shot FID Zero-shot FID (filt)</cell></row><row><cell>AttnGAN (Xu et al., 2017)</cell><cell>35.49</cell><cell></cell></row><row><cell>DM-GAN (Zhu et al., 2019)</cell><cell>32.64</cell><cell></cell></row><row><cell>DF-GAN (Tao et al., 2020)</cell><cell>21.42</cell><cell></cell></row><row><cell>DM-GAN + CL (Ye et al., 2021)</cell><cell>20.79</cell><cell></cell></row><row><cell>XMC-GAN (Zhang et al., 2021)</cell><cell>9.33</cell><cell></cell></row><row><cell>LAFITE (Zhou et al., 2021)</cell><cell>8.12</cell><cell></cell></row><row><cell>Make-A-Scene (Gafni et al., 2022)</cell><cell>7.55</cell><cell></cell></row><row><cell>DALL-E (Ramesh et al., 2021)</cell><cell></cell><cell>∼ 28</cell></row><row><cell>LAFITE (Zhou et al., 2021)</cell><cell></cell><cell>26.94</cell></row><row><cell>GLIDE (Nichol et al., 2021)</cell><cell></cell><cell>12.24</cell><cell>12.89</cell></row><row><cell>Make-A-Scene (Gafni et al., 2022)</cell><cell></cell><cell></cell><cell>11.84</cell></row><row><cell>unCLIP (AR prior)</cell><cell></cell><cell>10.63</cell><cell>11.08</cell></row><row><cell>unCLIP (Diffusion prior)</cell><cell></cell><cell>10.39</cell><cell>10.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Galatolo et al., Patashnik et al., Murdock, Gal et al. [17,<ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b14">16]</ref> guide GANs using gradients from a CLIP model. For diffusion models, Dhariwal and Nichol<ref type="bibr" target="#b8">[10]</ref> introduced classifier guidance as a way to use gradients from a classifier trained on noised images to steer the model towards higher quality generations. Nichol et al.<ref type="bibr" target="#b28">[30]</ref> train a CLIP model on noised images and guide a text-conditional diffusion model, whileCrowson, Crowson  </figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">I.e., less than 1% average mean-squared error in reconstructing the image representations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">We swept over percentiles 50%, 70%, 85%, 95% and found 50% to be optimal in all experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Recall is computed with respect to the training dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We'd to thank Jong Hyeonwoo Alec Radford, Pranav Shyam, and Ilya helpful discussions and contributions our work. We'd also like to thank Yunxin Jiao for creating several figures used in the paper. We are grateful to the Acceleration and Supercomputing teams at OpenAI for their work on software and hardware infrastructure this project used.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Linear Probes for Evaluations</head><p>For our evaluations, we leverage two new linear probes on top of a CLIP ViT-L/14 model. To automate aesthetic quality evaluations, we follow the procedure used by Crowson <ref type="bibr" target="#b4">[5]</ref>, training a linear regression model on images and mean ratings from the AVA dataset <ref type="bibr" target="#b26">[28]</ref>. To reduce the cost of hyperparameter sweeps before conducting human evaluations, we train a logistic regression model to predict win probabilities between pairs of images. To train this model, we used 15,000 pairwise image comparisons gathered from all of our previous human evaluations. For each comparison i, we computed CLIP image embeddings x i and y i for the two images in the pair. We then trained a linear model f (x) such that 1 1+e f (x i )−f (y i ) approximates the probability that a human prefers the image for y i . This can be reduced to a logistic regression problem with inputs equal to y i − x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Error Bars for Human Evaluation</head><p>When computing error bars for human evaluations, we use the normal approximation interval with p = 0.95. We expect the normal approximation to be accurate for such a large sample size of n = 1000.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Candace</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07520</idno>
		<title level="m">Cm3: A causal masked multimodal model of the internet</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High fidelity visualization of what your self-supervised representation knows about</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09164</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep vaes generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10650</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Crowson</surname></persName>
		</author>
		<ptr target="https://twitter.com/RiversHaveWings/status/1472346186728173568?s=20&amp;t=T-HRr3Gw5HRGjQaMDtRe3A" />
		<title level="m">Ava linear probe</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Crowson</surname></persName>
		</author>
		<ptr target="https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj" />
		<title level="m">Clip guided diffusion hq</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Clip guided diffusion 512x512, secondary model method</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Crowson</surname></persName>
		</author>
		<ptr target="https://twitter.com/RiversHaveWings/status/1462859669454536711" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Crowson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06666</idno>
		<ptr target="https://github.com/crowsonkb/v-diffusion-pytorch" />
		<title level="m">Virtex: Learning visual representations from textual annotations</title>
				<editor>
			<persName><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13290</idno>
		<title level="m">Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09841</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01412</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Fürst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Rumetshofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuong</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D P</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><surname>Klambauer</surname></persName>
		</author>
		<ptr target="https://openreview.net/" />
		<title level="m">Angela Bitto-Nemling, and Sepp Hochreiter. CLOOB: Modern hopfield networks with infoLOOB CLIP</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Make-ascene: generation human priors</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taigman</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13131</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Rinon</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00946</idno>
		<title level="m">Stylegan-nada: Clipguided domain adaptation of generators</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generating images from caption and vice versa via clip-guided generative latent space search</title>
		<author>
			<persName><forename type="first">Federico</forename><forename type="middle">A</forename><surname>Galatolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C A</forename><surname>Mario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gigliola</forename><surname>Cimino</surname></persName>
		</author>
		<author>
			<persName><surname>Vaglini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal neurons in artificial neural networks</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">†</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">†</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00030</idno>
		<ptr target="https://distill.pub/2021/multimodal-neurons" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vector quantized diffusion model for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14822</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classifier-free diffusion guidance</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qw8AKxfYbI" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15282</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft coco: Common objects in context</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Slip: Self-supervision meets languageimage pre-training</title>
		<author>
			<persName><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12750</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Murdock</surname></persName>
		</author>
		<ptr target="https://twitter.com/advadnoun/status/1351038053033406468" />
		<title level="m">The big sleep</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ava: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName><forename type="first">Naila</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2012.6247954</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Alex Prafulla Improved denoising diffusion probabilistic models</title>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17249</idno>
		<title level="m">Styleclip: Text-driven manipulation of stylegan imagery</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><surname>Liii</surname></persName>
		</author>
		<idno type="DOI">10.1080/14786440109462720</idno>
		<ptr target="https://doi.org/10.1080/14786440109462720" />
		<imprint>
			<date type="published" when="1901-11">November 1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Konpat</forename><surname>Preechakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nattanat</forename><surname>Chatthee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suttisak</forename><surname>Wizadwongsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15640</idno>
		<title level="m">Diffusion autoencoders: Toward a meaningful and decodable representation</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00446</idno>
		<title level="m">Generating diverse high-fidelity images with VQ-VAE-2</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10752</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:2104.07636</idno>
		<title level="m">Image super-resolution via iterative refinement</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning visual representations with caption annotations</title>
		<author>
			<persName><forename type="first">Mert</forename><surname>Bulent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01392</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">How much can clip benefit vision-and-language tasks?</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liunian</forename><forename type="middle">Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06383</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<title level="m">Deep unsupervised learning using nonequilibrium thermodynamics</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songsong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingkun</forename><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<title level="m">Deep fusion generative adversarial networks for text-to-image synthesis</title>
				<meeting><address><addrLine>Df-gan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Neural discrete representation learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Clip-gen: Language-free training of a text-to-image generator with clip</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinglong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zili</forename><surname>Yi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00386</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05278</idno>
		<title level="m">Gan inversion: A survey</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10485</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Improving text-to-image synthesis using contrastive learning</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiulong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Takac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajshekhar</forename><surname>Sunderraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02423</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Cross-modal contrastive learning for text-to-image generation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04702</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Designing a practical degradation model for deep blind image super-resolution</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00475</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV48922.2021.00475" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2021-10">2021. Oct 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00747</idno>
		<title level="m">Contrastive learning of medical visual representations from paired images and text</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Lafite: Towards language-free training for text-to-image generation</title>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13792</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03552</idno>
		<title level="m">Generative visual manipulation on the natural image manifold</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01310</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
