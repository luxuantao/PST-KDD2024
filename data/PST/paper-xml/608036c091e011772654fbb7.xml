<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GMLP: Building Scalable and Flexible Graph Neural Networks with Feature-Message Passing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-20">20 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
							<email>wentao.zhang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Shen</surname></persName>
							<email>shenyu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheyu</forename><surname>Lin</surname></persName>
							<email>linzheyu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaosen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Ouyang</surname></persName>
							<email>gdpouyang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
							<email>yangzhi@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
							<email>bin.cui@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University ‡ Data Platform</orgName>
								<orgName type="institution" key="instit3">Tencent Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GMLP: Building Scalable and Flexible Graph Neural Networks with Feature-Message Passing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-20">20 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.09880v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent studies, neural message passing has proved to be an effective way to design graph neural networks (GNNs), which have achieved state-of-the-art performance in many graph-based tasks. However, current neural-message passing architectures typically need to perform an expensive recursive neighborhood expansion in multiple rounds and consequently suffer from a scalability issue. Moreover, most existing neural-message passing schemes are inflexible since they are restricted to fixed-hop neighborhoods and insensitive to the actual demands of different nodes. We circumvent these limitations by a novel feature-message passing framework, called Graph Multi-layer Perceptron (GMLP), which separates the neural update from the message passing. With such separation, GMLP significantly improves the scalability and efficiency by performing the message passing procedure in a pre-compute manner, and is flexible and adaptive in leveraging node feature messages over various levels of localities. We further derive novel variants of scalable GNNs under this framework to achieve the best of both worlds in terms of performance and efficiency. We conduct extensive evaluations on 11 benchmark datasets, including large-scale datasets like ogbn-products and an industrial dataset, demonstrating that GMLP achieves not only the state-of-art performance, but also high training scalability and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b30">[31]</ref> have become the state-of-theart method in many supervised and semi-supervised graph representation learning scenarios such as node classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>, link prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, recommendation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>, and knowledge graphs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. The majority of these GNNs can be described in terms of the neural message passing (NMP) framework <ref type="bibr" target="#b8">[9]</ref>, which is based on the core idea of recursive neighborhood aggregation. Specifically, during each iteration, the representation of each node is updated (with neural networks) based on messages received from its neighbors. Despite their success, existing GNN algorithms suffer from two major drawbacks:</p><p>First, the current NMP procedure needs to repeatedly perform a recursive neighborhood expansion at each training iteration in order to compute the hidden representations of a given node, which are inherently less scalable due to the expensive computation and communication cost, especially for large-scale graphs. For example, a recent snapshot of the Tencent Wechat friendship graph is comprised of 1.2 billion nodes and more than 100 billion edges. Also, the high-dimensional features (typically ranging from 300 to 600) are associated with nodes, making GNNs difficult to be trained efficiently under time and resource constraints. While there has been an ever-growing interest in graph sampling strategies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> to alleviate the scalability and efficiency issues, sampling strategy may cause information loss. Also, our empirical observation shows that sampling cannot effectively reduce the high ratio of communication time over computation time, leading to IO bottleneck. This requires a new underlying NMP to achieve the best of both worlds.</p><p>Moreover, the existing NMP schemes are inflexible since they are restricted to a fixed-hop neighborhood and insensitive to actual demands of messages. This either makes that long-range dependencies cannot be fully leveraged due to limited hops/layers, or loses local information due to introducing many irrelevant nodes and unnecessary messages when increasing the number of hops (i.e., over-smoothing issue <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref>). Moreover, no correlations are exploited on different hops of locality. As a result, most stateof-the-art GNN models are designed with two layers (i.e., steps). These issues prevent existing methods from unleashing their full potential in terms of prediction performance.</p><p>To address the above limitations, we propose a novel GNN framework called Graph Multi-layer Perceptron (GMLP), which pushes the boundary of the prediction performance towards the direction of scalable graph deep learning. Figure <ref type="figure">1</ref> illustrates the architecture of our framework. The main contribution of GMLP is a new feature message passing (FMP) abstraction, described by graph_aggregate, message_aggregate, and update functions. This abstraction separates the neural network update from the message passing and leverages multiple messages over different levels of localities to update the node's final representation. Specifically, instead of messages updated by neural networks, GMLP passes node features Figure <ref type="figure">1</ref>: The architecture of GMLP framework. and avoids repeated expensive message passing procedure by preprocessing it in a distributed manner, thus achieving high scalability and efficiency. Meanwhile, GMLP is highly flexible by providing a variety of graph aggregators and message aggregators for generating and combining messages at different localities, respectively. Under this new abstraction, we could explore many novel scalable GNN variations within this framework, permitting a more flexible and efficient accuracy-efficiency tradeoff. For instance, through adaptively combining the message on multiple localities for each node with a new self-guidance attention scheme, our novel variant could achieve much better accuracy than existing GNNs while maintaining high scalability and efficiency. We also find recently emerging scalable algorithms, such as SGC <ref type="bibr" target="#b29">[30]</ref> and SIGN <ref type="bibr" target="#b6">[7]</ref>, are special variants of our GMLP framework.</p><p>To validate the effectiveness of GMLP, we conduct extensive experiments on 11 benchmark datasets against various GNN baselines. Experimental results demonstrate that GMLP outperforms the state-of-the-art methods APPNP and GAT by a margin of 0.2-3.0% and 0.2%-3.6% in terms of predictive accuracy, while achieving up to 15.6× and 74.4× training speedups, respectively. Moreover, GMLP achieves a nearly linear speedup in distributed training.</p><p>Our contributions can be summarized as follows: (1) Abstraction. We present a novel message passing abstraction that supports efficient and scalable feature aggregation over node neighborhoods in an adaptive manner. (2) Algorithms. We explore various scalable and efficient GMLP variants under the new abstraction with carefully designed message passing and aggregation strategies. (3) Performance and efficiency. The experimental results on massive datasets demonstrate that GMLP framework outperforms the existing GNN methods and meanwhile achieves better efficiency and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>In this section, we introduce GNNs from the view of message passing (MP) along with the corresponding scalability challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GNNs and Message-Passing</head><p>Considering a graph G(V, E) with nodes V, edges E and features for all nodes x 𝑣 ∈ R 𝑑 , ∀𝑣 ∈ V, many proposed GNN models can be analyzed using the message passing (MP) framework.</p><p>Neural message passing (NMP).. This NMP framework is widely adopted by mainstream GNNs, like GCN <ref type="bibr" target="#b11">[12]</ref>, GraphSAGE <ref type="bibr" target="#b8">[9]</ref>, GAT <ref type="bibr" target="#b26">[27]</ref>, and GraphSAINT <ref type="bibr" target="#b33">[34]</ref>, where each layer adopts a neighborhood and an updating function. At timestep 𝑡, a message vector m 𝑡 𝑣 for node 𝑣 ∈ V is computed with the representations of its neighbors N 𝑣 using an aggregate function, and m 𝑡 𝑣 is then updated by a neural-network based update function, which is:</p><formula xml:id="formula_0">m 𝑡 𝑣 ← aggregate h 𝑡 −1 𝑢 |𝑢 ∈ N 𝑣 , h 𝑡 𝑣 ← update(m 𝑡 𝑣 ).<label>(1)</label></formula><p>Messages are passed for 𝑇 timesteps so that the steps of message passing correspond to the network depth. Taking the vanilla GCN <ref type="bibr" target="#b11">[12]</ref> as an example, we have:</p><formula xml:id="formula_1">GCN-aggregate h 𝑡 −1 𝑢 |𝑢 ∈ N 𝑣 = ∑︁ 𝑢 ∈N 𝑣 h 𝑡 −1 𝑢 / √︃ d𝑣 d𝑢 , GCN-update(m 𝑡 𝑣 ) = 𝜎 (𝑊 m 𝑡 𝑣 ),</formula><p>where d𝑣 is the degree of node 𝑣 obtained from the adjacency matrix with self-connections Ã = 𝐼 + 𝐴.</p><p>Decoupled neural message passing (DNMP). Note that the aggregate and update operations are inherently intertwined in equation (1), i.e., each aggregate operation requires a neural layer to update the node's hidden state in order to generate a new message for the next step. Recently, some researches show that such entanglement could compromise performance on a range of benchmark tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>, and suggest separating GCN from the aggregation scheme. We reformulate these models into a single decoupled neural MP framework: Neural prediction messages are first generated (with update function) for each node utilizing only that node's own features, and then aggregated using aggregate function.</p><formula xml:id="formula_2">h 0 𝑣 ← update(x v ), h 𝑡 𝑣 ← aggregate h 𝑡 −1 𝑢 |𝑢 ∈ N 𝑣 . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where 𝑥 𝑣 is the input feature of node 𝑣. Existing methods, such as PPNP <ref type="bibr" target="#b13">[14]</ref>, APPNP <ref type="bibr" target="#b13">[14]</ref>, AP-GCN <ref type="bibr" target="#b24">[25]</ref> and etc., follows this decoupled MP. Taking APPNP as an example:</p><formula xml:id="formula_4">APPNP-update(x v ) = 𝜎 (𝑊 x 𝑣 ), APPNP-aggregate h 𝑡 −1 𝑢 |𝑢 ∈ N 𝑣 = 𝛼h 0 𝑣 + (1 − 𝛼) ∑︁ 𝑢 ∈N 𝑣 h 𝑡 −1 𝑢 √︃ d𝑣 d𝑢</formula><p>, where aggregate function adopts personalized PageRank with the restart probability 𝛼 ∈ (0, 1] controlling the locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenges and Motivation</head><p>Scalable training. Most researches on GNNs are tested on small benchmark graphs, while in practice, graphs contain billions of nodes and edges. However, we find that neural message passing formulated in equations ( <ref type="formula" target="#formula_0">1</ref>) and (2) do not scale well to large graphs since they typically need to repeatedly perform a recursive neighborhood expansion to gather neural messages. In other words, both MP and decoupled MP generate new messages (e.g., h v , ∀𝑣 ∈ V) during each training epoch, and the expensive aggregate procedure need to be performed again, which requires gathering from its neighbors, and the neighbors in turn, have to gather from their own neighbors and so on. This process leads to a costly recursive neighborhood expansion growing with the number of layers. For large graphs that can not be entirely stored on each worker's local   storage <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, gathering the required neighborhood neural messages leads to massive data communication costs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>To demonstrate the scalability issue, we utilize distributed training functions provided by DGL <ref type="bibr" target="#b1">[2]</ref> to test the scalability of Graph-SAGE <ref type="bibr" target="#b8">[9]</ref> (batch size = 8192). We partition the Reddit dataset across multiple machines, and treat each GPU as a worker. Figure <ref type="figure" target="#fig_2">2</ref> illustrates the training speedup along with the number of workers and the bottleneck in distributed settings. In particular, Figure <ref type="figure" target="#fig_2">2(a)</ref> shows that the scalability of GraphSAGE is significantly limited even when the mini-batch training and graph sampling method are adopted. Note that the speedup is calculated relative to the runtime of two workers. Model Flexibility. We find that both NMP and DNMP is inflexible given a fixed 𝑇 step of aggregation for all nodes. It is difficult to determine a suitable 𝑇 . Few steps may fail to capture sufficient neighborhood information, while more steps may bring too much information which leads to the over-smoothing issue. Moreover, nodes with different neighborhood structure require different message passing steps to fully capture the structural information. As shown in Figure <ref type="figure" target="#fig_4">3</ref>, we apply standard GCN with different layers to conduct node classification on the Cora dataset. We observe that most of the nodes are well classified with two steps, and as a result, most state-of-the-art GNN models are designed with two layers (i.e., steps). In addition, the predictive accuracy on 13 of the 20 sampled nodes increases with a certain step larger than two. The above observation motivates us to design a flexible and adaptive aggregate function for each node. However, this becomes even more challenging when complex graphs and sparse features are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GMLP ABSTRACTION</head><p>To address the above challenges, we propose a new GMLP abstraction under which more flexible and scalable GNNs can be derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature-message Passing Interfaces</head><p>GMLP consists of a message aggregation phase over graph followed by message combination and applying phases.   </p><formula xml:id="formula_5">m 𝑡 𝑣 ← graph_aggregator m 𝑡 −1 𝑢 |𝑢 ∈ N 𝑣 ,<label>(3)</label></formula><p>where m 0 𝑣 = x 𝑣 . The messages could be passed for 𝑇 timesteps, where 𝑚 𝑡 𝑣 at timestep 𝑡 could gather the neighborhood information from nodes that are 𝑡-hop away. The multi-hop messages M 𝑣 = {m 𝑡 𝑣 | 0 ≤ 𝑡 ≤ 𝑇 } are then aggregated into a single combined message vector 𝑐 𝑣 for a node 𝑣 such that the model learns to combine different scales for a given node:</p><formula xml:id="formula_6">c 𝑇 𝑣 ← message_aggregator(M 𝑣 , r 𝑣 ),<label>(4)</label></formula><p>where r 𝑣 is a reference vector for adaptive message aggregation on node 𝑣 (null by default). The GMLP learns node representation h 𝑣 by applying an MLP network to the combined message vector c 𝑇 𝑣 :</p><formula xml:id="formula_7">h 𝑣 ← udpate(c 𝑣 ).<label>(5)</label></formula><p>There are two key differences of the GMLP abstraction from existing GNN message passing (MP).</p><p>Message type. To address the scalability challenge, the message type in our abstraction is different from the previous GNN MP. We propose to pass node feature messages instead of neural messages by making the message aggregation independent of the update of hidden state. In particular, MP in the existing GNNs needs to update the hidden state h 𝑡 𝑣 by applying the message vector m 𝑡 𝑣 with neural networks, in order to perform the aggregate function for next step. Decoupled-GNN MP also needs to first update the hidden state h 𝑡 𝑣 with neural networks in order to get the neural message for the following multi-step aggregate procedure. By contrast, GMLP allows passing the node feature message without applying messages on hidden state in graph_aggregator. This message passing procedure is independent of learnable model parameters and can be easily pre-computed, thus leading to high scalability and speedup.</p><p>Multi-scale Messages. The existing MP framework only utilizes the last message vector m 𝑇 𝑣 to compute the final hidden state h 𝑇 𝑣 . Motivated by the observation in Section 2.2, GMLP assumes that the optimal neighborhood expansion size should not be the same for each node 𝑣, and thus retaining all the messages {m 𝑡 𝑣 |𝑡 ∈ [1, 𝑇 ]} that a node 𝑣 receives over different steps (i.e., localities). The multiscale messages are then aggregated per node into a single vector with message_aggregator, such that we could balance the preservation of information from both local and extended (multi-hop) neighborhood for a given node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Aggregators</head><p>To capture the information of nodes that are several hops away, GMLP adopts a graph aggregator to combine the nodes with their neighbors during each timestep. Intuitively, it is unsuitable to use a fixed graph aggregator for each task since the choice of graph aggregators depends on the graph structure and features. Thus GMLP provides three different graph aggregators to cope with different scenarios, and one could add more aggregators following the semantic of graph_aggregator interface.</p><p>Normalized adjacency. The augmented normalized adjacency (Aug. NA) <ref type="bibr" target="#b11">[12]</ref> and random walk are simple yet effective on a range of GNNs. The only difference between the two aggregators lies in its normalization method. The former uses the renormalization trick while the latter applies the random walk normalization. We denote d𝑣 as the degree of node 𝑣 obtained from the augmented adjacency matrix Ã = 𝐼 + 𝐴, the normalized graph aggregator is:</p><formula xml:id="formula_8">m 𝑡 𝑣 = ∑︁ 𝑢∈N𝑣 1 √︃ d𝑣 d𝑢 m 𝑡 −1 𝑢 .<label>(6)</label></formula><p>Personalized PageRank. Personalized PageRank (PPR) <ref type="bibr" target="#b12">[13]</ref> focuses on its local neighborhood using a restart probability 𝛼 ∈ (0, 1] and performs well on graphs with noisy connectivity. While the calculation of the fully personalized PageRank matrix is computationally expensive, we apply its approximate computation <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_9">m 𝑡 𝑣 = 𝛼m 0 𝑣 + (1 − 𝛼) ∑︁ 𝑢∈N𝑣 1 √︃ d𝑣 d𝑢 m 𝑡 −1 𝑢 ,<label>(7)</label></formula><p>where the restart probability 𝛼 allows to balance preserving locality (i.e., staying close to the root node to avoid over-smoothing) and leveraging the information from a large neighborhood.</p><p>Triangle-induced adjacency. Triangle-induced adjacency (Triangle. IA) matrix <ref type="bibr" target="#b21">[22]</ref> accounts for the higher order structures and helps distinguish strong and weak ties on complex graphs like social graphs. We assign each edge a weight representing the number of different triangles it belongs to, which forms a weight metrix 𝐴 𝑡𝑟𝑖 . We denote 𝑑 𝑡𝑟𝑖 𝑣 as the degree of node 𝑣 from the weighted adjacency matrix 𝐴 𝑡𝑟𝑖 . The aggregator is then calculated by applying a row-wise normalization:</p><formula xml:id="formula_10">m 𝑡 𝑣 = ∑︁ 𝑢∈N𝑣 1 𝑑 𝑡𝑟𝑖 𝑣 m 𝑡 −1 𝑢 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Message Aggregators</head><p>Before updating the hidden state of each node, GMLP proposes to apply a message aggregator to combine messages obtained by graph aggregators per node into a single vector, such that the subsequent model learns from the multi-scale neighborhood of a given node. We summarize the message aggregators GMLP supports, and one can also include more aggregators with the future state-of-the-arts.</p><p>Non-adpative aggregators. The main characteristic of these aggregators is that they do not consider the correlation between messages and the center node. The messages are directly concatenated or summed to obtain the combined message vector.</p><formula xml:id="formula_11">𝑐 𝑚𝑠𝑔 ← ⊕ 𝑚 𝑖 𝑣 ∈𝑀 𝑣 𝑓 (𝑚 𝑖 𝑣 ),<label>(9)</label></formula><p>where 𝑓 can be a function used to reduce the dimension of message vectors, and the ⊕ can be concatenating or pooling operators including average pooling or max pooling. The max used in the max-pooling aggregator is an element-wise operator. Compared with the pooling operators, although the concatenating operator keeps all the input message information, the dimension of its outputs increases as 𝑇 grows, leading to additional computational costs on the following MLP network.</p><p>Adaptive aggregators. The observations in Section 2.2 imply that messages of different hops make different contributions to the final performance. This motivates the design of adaptive-aggregation functions, which determines the importance of a node's message at different ranges rather than fixing the same weights for all nodes. To this end, we propose attention and gating, which generate retainment scores, indicating how much the corresponding messages should be retained in the final combined message.</p><p>As we shall describe in Sec. 4, the attention aggregator takes as input a self-guided reference vector r 𝑣 which captures the personalized property of the each target node 𝑣:</p><formula xml:id="formula_12">c msg ← ∑︁ m 𝑖 𝑣 ∈𝑀𝑣 𝑤 𝑖 m 𝑖 𝑣 , 𝑤 𝑖 = exp(𝜑 (m 𝑖 𝑣 , r 𝑣 )) m 𝑖 𝑣 ∈𝑀𝑣 exp(𝜑 (m 𝑖 𝑣 , r 𝑣 )) ,<label>(10)</label></formula><p>where 𝜑 (m 𝑖 𝑣 , r 𝑣 ) = tanh(𝑾 1 m 𝑖 𝑣 + 𝑾 2 r 𝑣 ). Like attention-based aggregator, the gating aggregator also aggregates the messages adaptively. The main difference is that it adopts a trainable global reference vector that are uniform for all nodes. The formula of the gating aggregator is given as follows,</p><formula xml:id="formula_13">c msg ← ∑︁ m 𝑖 𝑣 ∈𝑀𝑣 𝑤 𝑖 m 𝑖 𝑣 , 𝑤 𝑖 = 𝜎 (sm 𝑖 𝑣 ),<label>(11)</label></formula><p>where s is a trainable vector shared by all nodes to generate gating scores, and 𝜎 denotes the sigmoid function.</p><p>By utilizing these adaptive message aggregators, GMLP is capable of balancing the messages from the multi-scale neighborhood for each node, at the expense of training attention parameters. Given various aggregators, GMLP provides various algorithms in Sec. 4 balancing high flexible performance-efficiency tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GMLP ALGORITHMS</head><p>Based on the above message passing abstraction, we propose the following GMLP algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: GMLP Framework</head><p>Input: Graph 𝐺, propagation number 𝐾, feature matrix X. </p><formula xml:id="formula_14">𝑚 ! 𝑚 " 𝑚 # 𝑚 $ ⋯ Precomputed Graph Messages MLP ⋯ ⋯ ⋯ ⋯ … MLP … × × × × +</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-guided Attention Aggregator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-adaptive Aggregator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference Vector</head><p>Classification Loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Loss</head><p>Figure <ref type="figure">5</ref>: The self-guided network architecture of GMLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Algorithm Framework</head><p>Self-guided GMLP. As shown in Algorithm 1, the sequence of operations is "graph_aggregator → message_aggregator →update→message_aggregator→update". Specifically, we first apply a 𝑇 -steps graph_aggregator to derive the messages over different hops of nodes (Line 2-5). Both long-range dependencies and local information are captured by multiple messages. The messages are aggregated per node into a single message vector and the vector is then fed to update (e.g., MLP) to obtain the hidden state vector ℎ 𝑣 (Line 7-8). Note that ℎ 𝑣 is obtained by aggregating messages uniformly over the fixed aggregation steps for each node. However, the most suitable aggregation steps for each node should be different. As ℎ 𝑣 captures the relation of different messages of a node 𝑣 and model performance, we include an adaptive, self-guided adjustment by introducing ℎ 𝑣 as a reference vector of 𝑣 (Line 9-10) to generate retainment scores. These scores are used for messages that carry information from a range of neighborhoods. These retainment scores measure how much information derived by different propagation layers should be retained to generate the final combined message for each node.</p><p>Figure <ref type="figure">5</ref> shows the corresponding model architecture of Algorithm 1, which includes two branches: the non-adaptive aggregation (NA) branch (corresponding to Line 7-8 in the algorithm) and selfguided attention aggregation (SGA) branch (corresponding to Line 9-10), which share the same multi-scale graph messages. The NA branch aims to create a multi-scale semantic representation of the target node over its neighborhoods with different size, which helps to recognize the correlation among node representations of different propagation steps. This multi-scale feature representation is Algorithm 2: GMLP-GU Input: Graph 𝐺, propagation number 𝐾, feature matrix X. then fed into the SGA branch to generate the refined attention feature representation for each node. The SGA branch will gradually remove the noisy level of localities and emphasize those neighborhood regions that are more relevant to the semantic descriptions of the targets. These duple branches are capable of modeling a wider neighborhood while enhancing correlations, which brings a better feature representation for each node.</p><p>Model Training. We combine the loss of two branches as follows:</p><formula xml:id="formula_15">L = 𝛼 𝑡 L 𝑁 𝐴 + (1 − 𝛼 𝑡 ) L 𝑆𝐺𝐴 ,<label>(12)</label></formula><p>where the time-sensitive parameter 𝛼 𝑡 = cos 𝜋𝑡 2𝑇 gives higher weight to the NA branch to find a better reference vector at the beginning and gradually shift the focus to the SGA branch for adaptive adjustment as the training process progresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GMLP Variants</head><p>It is worth pointing out that the above GMLP algorithm framework derives a family of scalable model variants to tackle flexible performance-efficiency tradeoff, through either adopting different provided graph/message aggregators or dropping certain message aggregators. Both adaptive and non-adaptive message aggregators are included in our general GMLP model. We wish to open up several interesting special cases of our model. GMLP-GU. This is the most simplified variant, the sequence of operations is reduce to "graph_aggregator→update", as shown in Algorithm 2. No message aggregators are performed and features are aggregated over a single scale m T for applying message. However, the GMLP-GU still allows a variety of graph aggregators provided in Section 3.2. SGC <ref type="bibr" target="#b29">[30]</ref> can be taken as a special case of this version with the normalized-adjacency graph aggregator. This variant has the highest efficiency by dropping message aggregators, but it ignores multiple neighborhood scales and their correlations which lead to higher performance. ) APPNP <ref type="bibr" target="#b12">[13]</ref> decoupled neural ✓ × O (𝐿𝑝 𝑀𝑑 + 𝐿𝑢 𝑁𝑑 2 ) AP-GCN <ref type="bibr" target="#b24">[25]</ref> decoupled neural</p><formula xml:id="formula_16">✓ × O (𝐿𝑝 𝑀𝑑 + 𝐿𝑢 𝑁𝑑 2 ) SGC feature ✓ ✓ O (𝐿𝑢 𝑁𝑑 2 ) GMLP-GU feature ✓ ✓ O (𝐿𝑢 𝑁𝑑 2 ) GMLP-GMU feature ✓ ✓ O (𝐿𝑢 𝑁𝑑 2 ) GMLP feature ✓ ✓ O (𝐿𝑢 𝑁𝑑 2 )</formula><p>GMLP-GMU. In this variant, the model performs operations "graph_aggregator→message_aggregator→update", as shown in Algorithm 3. The message aggregators are used to combine node features over the multi-scale neighborhood to improve performance. If non-adaptive message aggregators are used, the multi-scale messages are indiscriminately aggregated, without effectively exploring their correlation. SIGN <ref type="bibr" target="#b6">[7]</ref> can be viewed as a special case of using the message aggregator of concatenation. Moreover, GMLP provides a gating aggregator given by equation ( <ref type="formula" target="#formula_13">11</ref>) for node-adaptive message aggregation. However, the reference vector s is the same for each node, preventing it from unleashing the full potential to capture the correlations between nodes. The architecture of GMLP-GMU retains the non-adaptive aggregation branch in Figure <ref type="figure">5</ref>, leading to moderate efficiency in GMLP model family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Advantages</head><p>Efficiency. As shown in Section 2.1, passing neural messages requires propagating the outputs obtained from neural transformations, i.e., the forward complexity of the neural network inevitably includes the complexity of both state update and propagation. However, as GMLP updates the hidden state after the propagation procedure is done, it could perform propagation only once as a preprocessing stage. Therefore, the forward complexity of the overall model is significantly reduced to that of training an MLP, which is O (𝐿 𝑢 𝑁𝑑 2 ) as shown in Table <ref type="table" target="#tab_2">1</ref>.</p><p>Scalability. For large graphs that can not be stored locally, each worker requires to gather neighborhood neural messages from shared (distributed) storage, which leads to high communication cost dominating training cycles. For example, let 𝑇 be the number of training epochs, the communication cost of GCN is O (𝐿 𝑝 𝑀𝑇𝑑). However, since propagation is taken in advance as pre-computation, GMLP reduces the total communication cost from O (𝐿 𝑝 𝑀𝑇𝑑)) to O (𝐿 𝑝 𝑀𝑑), thus scaling to large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GMLP IMPLEMENTATION</head><p>Different from the existing GNNs, the training of GMLP is clearly separated into a pre-processing stage and a training stage: First, we pre-compute the message vectors for each node over the graph, and then we combine the messages and train the model parameters with SGD. Both stages can be implemented in a distributed fashion.</p><p>Graph message pre-processing. For the first stage, we implement an efficient batch data processing pipeline function over distributed The nodes are partitioned into batches, and the computation of each batch is implemented by workers in parallel with matrix multiplication. As shown in Figure <ref type="figure" target="#fig_7">6</ref>, for each node in a batch, we firstly pull all the 𝑖-th step messages of its 1-hop neighbors from the message distributed storage and then compute the (𝑖 + 1)-th step messages of the batch in parallel. Next, We push these aggregated messages back for reuse in the calculation of the (𝑖 + 2)-step messages. In our implementation, we treat GPUs as workers for fast pre-processing, and the graph data are partitioned and stored on host memory across machines. Since we compute the message vectors for each node in parallel, our implementation could scale to large graphs and significantly reduce the runtime.</p><p>Distributed training. For the second stage, we implement GMLP by PyTorch and optimize the parameters with distributed SGD. For algorithms that can be expressed by the GMLP interfaces, we translate it into the corresponding model architecture containing message aggregator operations such as attention. Then, the model parameters are stored on a parameter server and multiple workers (GPU) process the data in parallel. We adopt asynchronous training to avoid the communication overhead between many workers. Each worker fetches the most up-to-date parameters and computes the gradients for a mini-batch of data, independent of the other workers.   <ref type="bibr" target="#b11">[12]</ref>, two social networks (Flickr and Reddit) in <ref type="bibr" target="#b33">[34]</ref>, four co-authorship graphs (Amazon and Coauthor) in <ref type="bibr" target="#b22">[23]</ref>, the copurchasing network (ogbn-products) in <ref type="bibr" target="#b9">[10]</ref> and the tencent video dataset from our industry partner -Tencent Inc. Table <ref type="table" target="#tab_3">2</ref> provides the overview of the 11 datasets and the detailed description is in Section A.1 of the supplemental material. Parameters. For GMLP and other baselines, we use random search or follow the original papers to get the optimal hyperparameters. To eliminate random factors, we run each method 20 times and report the mean and variance of the performance. More details can be found in Sec. A.3 in the supplementary material. Environment. We run our experiments on four machines, each with 14 Intel(R) Xeon(R) CPUs (Gold 5120 @ 2.20GHz) and 4 NVIDIA TITAN RTX GPUs. All the experiments are implemented in Python 3.6 with Pytorch 1.7.1 on CUDA 10.1. Baselines. In the transductive settings, we compare GMLP with GCN <ref type="bibr" target="#b11">[12]</ref>, GAT <ref type="bibr" target="#b26">[27]</ref>, JK-Net <ref type="bibr" target="#b31">[32]</ref>, Res-GCN <ref type="bibr" target="#b11">[12]</ref>, APPNP <ref type="bibr" target="#b14">[15]</ref>, AP-GCN <ref type="bibr" target="#b24">[25]</ref>, SGC <ref type="bibr" target="#b29">[30]</ref>, SIGN <ref type="bibr" target="#b23">[24]</ref>, which are the state-of-the-art models of different message passing types. Besides, we also compare GMLP with its two variants: GMLP-GU with the personalized pagerank graph aggregators and GMLP-GMU with the gating message aggregators. In the inductive settings, the compared baselines are GraphSAGE <ref type="bibr" target="#b8">[9]</ref>, FastGCN <ref type="bibr" target="#b4">[5]</ref>, ClusterGCN <ref type="bibr" target="#b5">[6]</ref> and GraphSAINT <ref type="bibr" target="#b33">[34]</ref>. The detailed introduction of these baselines are shown in Section A.2 of the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance-Efficiency Analysis</head><p>To demonstrate the overall performance in both transductive and inductive settings, we compare GMLP with the other state-of-theart methods. The results are summarized in Table <ref type="table" target="#tab_5">3 and 4</ref>  We observe that GMLP obtains quite competitive performance in both transductive and inductive settings. In inductive settings, Table <ref type="table" target="#tab_5">4</ref> shows that GMLP outperforms the best baseline GraphSAINT by a margin of 1.2% on Flickr while achieves the same performance on Reddit. In transductive settings, GMLP outperforms the best baseline of each dataset by a margin of 0.3% to 1.0%. Remarkably, our simplified variant GMLP-GMU also achieves the best performance among the baselines that pass neural messages, which shows the superiority of passing node features followed by message aggregation. In addition, GMLP improves SIGN, the best scalable baselines, by a margin of 0.3% to 2.0%. We attribute this improvement to the application of the self-guided adaptive message aggregator.</p><p>We also evaluate the efficiency of each method in the real production environment. Figure <ref type="figure" target="#fig_9">7</ref> illustrates the performance over training time on Tencent Video. In particular, we pre-compute the graph messages of each scalable method, and the training time takes into account the pre-computation time. We observe that GCN, along with its variants which pass neural messages requires a rather larger training time than the variants of GMLP that pass node features. Among considered baselines, GMLP achieves the best performance with 5× training time compared with GMLP-GU and SGC. It's worth pointing out that GMLP-GU and GMLP-GMU varients, outperform SGC and SIGN respectively while requiring less training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Training Scalability</head><p>To examine the training scalability of GMLP, we compare it with GraphSAGE, a widely used method in industry on two large-scale datasets, and the results are shown in Figure <ref type="figure" target="#fig_11">8</ref>. We run the two methods in both stand-alone and distributed scenarios, and then   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Model Scalability</head><p>We examine the model scalability by observing how the model performance changes along with the message passing step 𝑇 . As shown in Figure <ref type="figure" target="#fig_12">9</ref>, the vanilla GCN gets the best results with two aggregation steps, but its performance drops rapidly along with the increased steps due to the over-smoothing issue. Both Res-GCN and SGC show better performance than GCN with larger aggregation steps. SGC alleviates this problem by removing the non-linear transformation and ResGCN carries information from the previous step by introducing the residual connections. However, their performance still degrades as they are unable to balance the needs of preserving locality (i.e. staying close to the root node to avoid over-smoothing) and leveraging the information from a large neighborhood. In contrast, GMLP achieves consistent performance improvement across steps, which indicates that GMLP is able to adaptively and effectively combine multi-scale neighborhood messages for each node.</p><p>To demonstrate this, Figure <ref type="figure" target="#fig_13">10</ref> shows the average attention weights of graph messages according to the number of steps and degrees of input nodes, where the maximum step is 6. In this experiment, we randomly select 20 nodes for each degree range (1-4, 5-8, 9-12) and plot the relative weight based on the maximum value. We get two observations from the heat map: 1) The 1-step and 2step graph messages are always of great importance, which shows that GMLP captures the local information as those widely 2-layer methods do; 2) The weights of graph messages with larger steps drop faster as the degree grows, which indicates that the attentionbased aggregator could prevent high-degree nodes from including excessive irrelevant nodes which lead to over-smoothing. From the two observations, we conclude that GMLP is able to identify the different message passing demands of nodes and explicitly weight each graph message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We present GMLP, a new GNN framework that achieves the best of both worlds of scalability and performance via a novel feature message passing abstraction. In comparison to previous approaches, GMLP decouples message passing and neural update and solves the limited scalability and flexibility problems inherent in previous neural message passing models. Under this abstraction, GMLP provides a variety of graph and message aggregators as cornerstones for developing scalable GNNs. By exploring different aggregators under the framework, we derive novel GMLP model variants that allow for efficient feature aggregation over adaptive node neighborhoods. Experiments on 11 real-world benchmark datasets demonstrate that GMLP consistently outperforms the other state-of-the-art methods on performance, efficiency, and scalability. For future work, we are building GMLP and integrating it as a key part of Angel-Graph<ref type="foot" target="#foot_0">1</ref> .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The speedup and bottleneck of a two-layer Graph-SAGE along with the increased workers on Reddit dataset.storage<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, gathering the required neighborhood neural messages leads to massive data communication costs<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>.To demonstrate the scalability issue, we utilize distributed training functions provided by DGL<ref type="bibr" target="#b1">[2]</ref> to test the scalability of Graph-SAGE<ref type="bibr" target="#b8">[9]</ref> (batch size = 8192). We partition the Reddit dataset across multiple machines, and treat each GPU as a worker. Figure2illustrates the training speedup along with the number of workers and the bottleneck in distributed settings. In particular, Figure2(a)shows that the scalability of GraphSAGE is significantly limited even when the mini-batch training and graph sampling method are adopted. Note that the speedup is calculated relative to the runtime of two workers. Figure (b) further demonstrates that the scalability is mainly bottlenecked by the aggregation procedure in which high data loading cost is required to gather multi-scale neural messages. Other GNNs under the NMP (or DNMP) framework suffer from the same scalability issue as they perform such costly aggregation during each training epoch. This motivates us to separate the neural update and message passing towards scalable training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2: The speedup and bottleneck of a two-layer Graph-SAGE along with the increased workers on Reddit dataset.storage<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, gathering the required neighborhood neural messages leads to massive data communication costs<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>.To demonstrate the scalability issue, we utilize distributed training functions provided by DGL<ref type="bibr" target="#b1">[2]</ref> to test the scalability of Graph-SAGE<ref type="bibr" target="#b8">[9]</ref> (batch size = 8192). We partition the Reddit dataset across multiple machines, and treat each GPU as a worker. Figure2illustrates the training speedup along with the number of workers and the bottleneck in distributed settings. In particular, Figure2(a)shows that the scalability of GraphSAGE is significantly limited even when the mini-batch training and graph sampling method are adopted. Note that the speedup is calculated relative to the runtime of two workers. Figure (b) further demonstrates that the scalability is mainly bottlenecked by the aggregation procedure in which high data loading cost is required to gather multi-scale neural messages. Other GNNs under the NMP (or DNMP) framework suffer from the same scalability issue as they perform such costly aggregation during each training epoch. This motivates us to separate the neural update and message passing towards scalable training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The influence of aggregation steps on 20 randomly sampled nodes on Citeseer dataset. The X-axis is the node id and Y-axis is the aggregation steps (number of layers in GCN). The color from white to blue represents the ratio of being predicted correctly in 50 different runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 interface GMLP { 2 / 4 /</head><label>24</label><figDesc>/ neighborhood message aggregation 3 msg = g r a p h _ a g g r e g a t o r ( Nbr_messages ); / node 's message aggregation 5 c_msg = m e s s a g e _ a g g r e g a t o r ( msgList , reference ); 6 // apply the combined message with MLP 7 h = update ( c_msg ) 8 };</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The GMLP Abstraction. Interfaces. At timestep 𝑡, a message vector m 𝑡 𝑣 is collected from the messages of the neighbors N 𝑣 of 𝑣:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: The workflow of GMLP, consisting of message precompute stage and message aggregation/update stage . graph storage: The nodes are partitioned into batches, and the computation of each batch is implemented by workers in parallel with matrix multiplication. As shown in Figure6, for each node in a batch, we firstly pull all the 𝑖-th step messages of its 1-hop neighbors from the message distributed storage and then compute the (𝑖 + 1)-th step messages of the batch in parallel. Next, We push these aggregated messages back for reuse in the calculation of the (𝑖 + 2)-step messages. In our implementation, we treat GPUs as workers for fast pre-processing, and the graph data are partitioned and stored on host memory across machines. Since we compute the message vectors for each node in parallel, our implementation could scale to large graphs and significantly reduce the runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance over training time on Tencent Video.We observe that GMLP obtains quite competitive performance in both transductive and inductive settings. In inductive settings, Table4shows that GMLP outperforms the best baseline GraphSAINT by a margin of 1.2% on Flickr while achieves the same performance on Reddit. In transductive settings, GMLP outperforms the best baseline of each dataset by a margin of 0.3% to 1.0%. Remarkably, our simplified variant GMLP-GMU also achieves the best performance among the baselines that pass neural messages, which shows the superiority of passing node features followed by message aggregation. In addition, GMLP improves SIGN, the best scalable baselines, by a margin of 0.3% to 2.0%. We attribute this improvement to the application of the self-guided adaptive message aggregator.We also evaluate the efficiency of each method in the real production environment. Figure7illustrates the performance over training time on Tencent Video. In particular, we pre-compute the graph messages of each scalable method, and the training time takes into account the pre-computation time. We observe that GCN, along with its variants which pass neural messages requires a rather larger training time than the variants of GMLP that pass node features. Among considered baselines, GMLP achieves the best performance with 5× training time compared with GMLP-GU and SGC. It's worth pointing out that GMLP-GU and GMLP-GMU varients, outperform SGC and SIGN respectively while requiring less training time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Scalability comparison on Reddit and ogbn-product datasets. The stand-alone scenario means the graph has only one partition stored on a multi-GPU server, whereas the distributed scenario means the graph is partitioned and stored on multi-servers. In the distributed scenario, we run two workers per machine.</figDesc><graphic url="image-3.png" coords="8,367.34,231.72,130.51,56.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Test accuracy of different models along with the increased aggregation steps.measure their corresponding speedups. The batch size is set to 8192 for Reddit and 16384 for ogbn-product, and the speedup is calculated by runtime per epoch relative to that of one worker in the stand-alone scenario and two workers in the distributed scenario. Without considering extra costs, the speedup will increase linearly in an ideal condition. For GraphSAGE, since it requires aggregating the neighborhood nodes during training, it meets the I/O bottleneck when transmitting a large number of required neural messages. Thus, the speedup of GraphSAGE increases slowly as the number of workers grows. The speedup of GraphSAGE is less than 2× even with 4 workers in the stand-alone scenario and 8 workers in the distributed scenario. It's worth recalling that the only extra communication cost of GMLP is to synchronize parameters with different workers, which is essential to all distributed training methods. As a result, GMLP performs more scalable than GraphSAGE and behaves close to the ideal circumstance in both two scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The average attention weights of graph messages of different steps on 60 randomly selected nodes from Cora. adaptively and effectively combine multi-scale neighborhood messages for each node.To demonstrate this, Figure10shows the average attention weights of graph messages according to the number of steps and degrees of input nodes, where the maximum step is 6. In this experiment, we randomly select 20 nodes for each degree range (1-4, 5-8, 9-12) and plot the relative weight based on the maximum value. We get two observations from the heat map: 1) The 1-step and 2step graph messages are always of great importance, which shows that GMLP captures the local information as those widely 2-layer methods do; 2) The weights of graph messages with larger steps drop faster as the degree grows, which indicates that the attentionbased aggregator could prevent high-degree nodes from including excessive irrelevant nodes which lead to over-smoothing. From the two observations, we conclude that GMLP is able to identify the different message passing demands of nodes and explicitly weight each graph message.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 𝑀 = {X}; 2 for 1 ≤ 𝑡 ≤ 𝑇 do 𝑣 ←update( 𝑐 𝑣 ); 𝑐 𝑣 ←message_aggregator(𝑀 𝑣 , ℎ 𝑣 );</figDesc><table><row><cell>3</cell><cell>for 𝑣 ∈ 𝑉 do</cell><cell></cell></row><row><cell>4</cell><cell>𝑚 𝑡 𝑣 ← graph_aggregator(𝑚 𝑡 −1 N 𝑣</cell><cell>);</cell></row><row><cell>5</cell><cell>𝑀 𝑣 = 𝑀 𝑣 ∪ {𝑚 𝑡 𝑣 };</cell><cell></cell></row><row><cell cols="2">6 for 𝑣 ∈ 𝑉 do</cell><cell></cell></row><row><cell>7</cell><cell cols="2">𝑐 𝑣 ←message_aggregator(𝑀 𝑣 , 𝑛𝑢𝑙𝑙);</cell></row><row><cell>9</cell><cell></cell><cell></cell></row><row><cell>10</cell><cell>ℎ 𝑣 ←update(𝑐 𝑣 );</cell><cell></cell></row></table><note>8ℎ</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Algorithm analysis. We denote 𝑁 , 𝑀 and 𝑑 as the number of nodes, edges and features respectively. 𝐿 𝑝 and 𝐿 𝑢 are the number of graph convolution and update layers, and 𝑘 refers to the number of sampled nodes in GraphSAGE.</figDesc><table><row><cell>Algorithm</cell><cell>Message typle</cell><cell cols="2">Decoupled Scalable</cell><cell>Forward pass</cell></row><row><cell>GCN [11]</cell><cell>neural</cell><cell>×</cell><cell>×</cell><cell>O (𝐿𝑝 𝑀𝑑 + 𝐿𝑢 𝑁𝑑 2 )</cell></row><row><cell>GraphSAGE [9]</cell><cell>neural</cell><cell>×</cell><cell>×</cell><cell>O (𝑘 𝐿𝑝 𝑁𝑑 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overview of datasets (T and I represents transductive and inductive, respectively).</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Nodes #Features</cell><cell>#Edges</cell><cell cols="2">#Classes Task</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>1,433</cell><cell>5,429</cell><cell>7</cell><cell>T</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>3,703</cell><cell>4,732</cell><cell>6</cell><cell>T</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>500</cell><cell>44,338</cell><cell>3</cell><cell>T</cell></row><row><cell>Amazon Computer</cell><cell>13,381</cell><cell>767</cell><cell>245,778</cell><cell>10</cell><cell>T</cell></row><row><cell>Amazon Photo</cell><cell>7,487</cell><cell>745</cell><cell>119,043</cell><cell>8</cell><cell>T</cell></row><row><cell>Coauthor CS</cell><cell>18,333</cell><cell>6,805</cell><cell>81,894</cell><cell>15</cell><cell>T</cell></row><row><cell>Coauthor Physics</cell><cell>34,493</cell><cell>8,415</cell><cell>247,962</cell><cell>5</cell><cell>T</cell></row><row><cell>ogbn-products</cell><cell>2,449,029</cell><cell>100</cell><cell>61,859,140</cell><cell>47</cell><cell>T</cell></row><row><cell>Flickr</cell><cell>89,250</cell><cell>500</cell><cell>899,756</cell><cell>7</cell><cell>I</cell></row><row><cell>Reddit</cell><cell>232,965</cell><cell>602</cell><cell>11,606,919</cell><cell>41</cell><cell>I</cell></row><row><cell>Tencent</cell><cell>1,000,000</cell><cell>64</cell><cell>1,434,382</cell><cell>253</cell><cell>T</cell></row><row><cell cols="2">6 EXPERIMENTS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">6.1 Experimental Settings</cell><cell></cell><cell></cell><cell></cell></row></table><note>Datasets. We conduct the experiments on public partitioned datasets, including three citation networks (Citeseer, Cora, and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy (%) in transductive settings (Dcp. neural means decoupled neural).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Amazon</cell><cell>Amazon</cell><cell>Coauthor</cell><cell>Coauthor</cell><cell>Tencent</cell></row><row><cell>Type</cell><cell>Models</cell><cell>Cora</cell><cell>Citeseer Pubmed</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Computer</cell><cell>Photo</cell><cell>CS</cell><cell>Physics</cell><cell>Video</cell></row><row><cell></cell><cell>GCN</cell><cell cols="2">81.8±0.5 70.8±0.5 79.3±0.7</cell><cell>82.4±0.4</cell><cell>91.2±0.6</cell><cell>90.7±0.2</cell><cell>92.7±1.1</cell><cell>45.9±0.4</cell></row><row><cell>Neural</cell><cell>GAT JK-Net</cell><cell cols="2">83.0±0.7 72.5±0.7 79.0±0.3 81.8±0.5 70.7±0.7 78.8±0.7</cell><cell>80.1±0.6 82.0±0.6</cell><cell>90.8±1.0 91.9±0.7</cell><cell>87.4±0.2 89.5±0.6</cell><cell>90.2±1.4 92.5±0.4</cell><cell>46.8±0.7 47.2±0.3</cell></row><row><cell></cell><cell>ResGCN</cell><cell cols="2">82.2±0.6 70.8±0.7 78.3±0.6</cell><cell>81.1±0.7</cell><cell>91.3±0.9</cell><cell>87.9±0.6</cell><cell>92.2±1.5</cell><cell>46.8±0.5</cell></row><row><cell>Dcp. neural</cell><cell>APPNP AP-GCN</cell><cell cols="2">83.3±0.5 71.8±0.5 80.1±0.2 83.4±0.3 71.3±0.5 79.7±0.3</cell><cell>81.7±0.3 83.7±0.6</cell><cell>91.4±0.3 92.1±0.3</cell><cell>92.1±0.4 91.6±0.7</cell><cell>92.8±0.9 93.1±0.9</cell><cell>46.7±0.6 46.9±0.7</cell></row><row><cell>Feature</cell><cell>SGC SIGN</cell><cell cols="2">81.0±0.2 71.3±0.5 78.9±0.5 82.1±0.3 72.4±0.8 79.5±0.5</cell><cell>82.2±0.9 83.1±0.8</cell><cell>91.6±0.7 91.7±0.7</cell><cell>90.3±0.5 91.9±0.3</cell><cell>91.7±1.1 92.8±0.8</cell><cell>45.2±0.3 46.3±0.5</cell></row><row><cell></cell><cell>GMLP</cell><cell cols="2">84.1±0.5 72.7±0.4 80.3±0.6</cell><cell>84.7±0.7</cell><cell cols="2">92.6±0.8 92.5±0.5</cell><cell cols="2">93.7±0.9 47.7±0.3</cell></row><row><cell>Feature</cell><cell>GMLP-GU(PPR)</cell><cell cols="2">81.7±0.6 71.1±0.5 79.1±0.6</cell><cell>82.5±0.8</cell><cell>91.7±0.8</cell><cell>90.3±0.3</cell><cell>91.4±0.7</cell><cell>45.7±0.4</cell></row><row><cell></cell><cell cols="3">GMLP-GMU(Gating) 83.6±0.3 72.1±0.3 79.8±0.7</cell><cell>83.8±0.6</cell><cell>91.8±0.6</cell><cell>91.6±0.4</cell><cell>93.1±0.7</cell><cell>47.4±0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy (%) in inductive settings.</figDesc><table><row><cell>Models</cell><cell>Flickr</cell><cell>Reddit</cell></row><row><cell>GraphSAGE</cell><cell cols="2">50.1±1.3 95.4±0.0</cell></row><row><cell>FastGCN</cell><cell cols="2">50.4±0.1 93.7±0.0</cell></row><row><cell>ClusterGCN</cell><cell cols="2">48.1±0.5 95.7±0.0</cell></row><row><cell>GraphSAINT</cell><cell cols="2">51.1±0.1 96.6±0.1</cell></row><row><cell>GMLP</cell><cell cols="2">52.3±0.2 96.6±0.1</cell></row><row><cell>GMLP-GU(PPR)</cell><cell cols="2">50.3±0.3 95.2±0.1</cell></row><row><cell cols="3">GMLP-GMU(Gating) 51.6±0.2 96.1±0.0</cell></row><row><cell>Pubmed) in</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/Angel-ML/angel</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A multi-scale approach for graph link prediction</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3308" to="3315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
				<editor>
			<persName><forename type="first">Ankur</forename><surname>Teredesai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rómer</forename><surname>Rosales</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evimaria</forename><surname>Terzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</editor>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-08-04">2019. August 4-8, 2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SIGN: Scalable Inception Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Personalized Embedding Propagation: Combining Neural Networks on Graphs with Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>CoRR abs/1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName><forename type="first">Chung-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Kuan</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1576" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PaGraph: Scaling GNN training on large graphs via computation-aware caching</title>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinlong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Symposium on Cloud Computing</title>
				<meeting>the 11th ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="401" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neugraph: parallel deep neural network computation on large graphs</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafei</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="443" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04844</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06803</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">MotifNet: a motifbased Graph Convolutional Network for directed graphs</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno>CoRR abs/1802.01572</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SIGN: Scalable Inception Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno>CoRR abs/2004.11198</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive propagation graph convolutional network</title>
		<author>
			<persName><forename type="first">Indro</forename><surname>Spinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Alok</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Yelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydin</forename><surname>Buluc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03300</idno>
		<title level="m">Reducing communication in graph neural network training</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6857" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00504</idno>
		<title level="m">Deep reasoning with knowledge graph for social relationship understanding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">AGL: a scalable system for industrial-purpose graph machine learning</title>
		<author>
			<persName><forename type="first">Dalong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02454</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09691</idno>
		<title level="m">Link prediction based on graph neural networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qidong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05337</idno>
		<title level="m">DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baole</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08730</idno>
		<title level="m">Aligraph: A comprehensive graph neural network platform</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
