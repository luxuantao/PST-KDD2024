<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 COMPOFA: COMPOUND ONCE-FOR-ALL NETWORKS FOR FASTER MULTI-PLATFORM DEPLOYMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 COMPOFA: COMPOUND ONCE-FOR-ALL NETWORKS FOR FASTER MULTI-PLATFORM DEPLOYMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The emergence of CNNs in mainstream deployment has necessitated methods to design and train efficient architectures tailored to maximize the accuracy under diverse hardware &amp; latency constraints. To scale these resource-intensive tasks with an increasing number of deployment targets, Once-For-All (OFA) proposed an approach to jointly train several models at once with a constant training cost. However, this cost remains as high as 40-50 GPU days and also suffers from a combinatorial explosion of potentially sub-optimal model configurations. We find that the cost of this one-shot training is dependent on the size of the model design space, and hence seek to speed up the training by constraining the design space to configurations with better accuracy-latency trade-offs. We incorporate the insights of compound relationships between model dimensions to build CompOFA, a design space smaller by several orders of magnitude. Through experiments on Ima-geNet, we demonstrate that even with simple heuristics we can achieve a 2x reduction in training time 1 and 216x speedup in model search/extraction time compared to the state of the art, without loss of Pareto optimality! We also show that this smaller design space is dense enough to support equally accurate models for similar diversity of hardware and latency targets, while also reducing the complexity of the training and subsequent extraction algorithms. Source code is at https: //github.com/compofa-blind-review/compofa-iclr21</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>CNNs are emerging in mainstream deployment across diverse hardware platforms, latency requirements, and/or workload characteristics. The available processing power, memory, and latency requirements may vary vastly across deployment platforms -say, from server-grade GPUs to lowpower embedded devices, cycles of high or low workload, etc. Hence, it becomes vital to design or search models tailored to the deployment scenario, maximizing accuracy constrained by the desired model inference latency. Building such architectures (either manually or by searching) and then training them are resource-intensive tasks -requiring massive computational resources, expertise of both ML and underlying systems, time, dollar cost, and CO 2 emissions every time they are performed. Repeating such intensive processes for each deployment target is prohibitively expensive w.r.t. multiple metrics of cost and doesn't scale.</p><p>Once-For-All (OFA) <ref type="bibr" target="#b4">(Cai et al., 2020)</ref> proposed to address this challenge by decoupling the search and training phases through a novel progressive shrinking algorithm. In this approach, OFA trains a family of models in a single-shot by sharing weights between a family of 10 19 sub-networks of varying depth, width, kernel size, and image resolution. Subsequently, it carries out a Neural Architecture Search (NAS) to extract a specialized sub-network for a specific deployment target -a task performed independently from the former training phase.</p><p>This massive search space leads to a training cost that remains prohibitively expensive. Though this cost can be amortized over a number of deployment targets, it is still significant (e.g., reaching 1200 GPU hours for OFA). This exhaustive approach of training all model configurations leaves out opportunities for any accuracy or latency guided exploration. Thus, it suffers a clear inefficiency in training a large number of models of a sub-optimal accuracy-latency trade-off, lying below the Pareto frontier. These extraneous models not only go un-utilized but also add interference, which necessitates phased training to stabilize their simultaneous optimization. Finally, searching &amp; extracting an optimal model from this space can only be done via indirect estimations of latency and accuracy lookup tables.</p><p>On the other hand, common practices as well as empirical studies <ref type="bibr">(Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b12">Radosavovic et al., 2020)</ref> have shown that model dimensions such as depth, width, and resolution are not orthogonal -models that follow some compound couplings between these dimensions produce a better accuracy-latency trade-off than those with unconstrained settings.</p><p>In this work, we show that the same or better accuracy-latency trade-offs can be achieved by model families smaller by several orders of magnitude. We reduce the search space (and, correspondingly, the train time) from O(10 19 ) models to 243, by leveraging two key insights: (1) architecture dimensions (such as depth and width) can be coupled, eliminating models which are suboptimal in the accuracy-latency tradeoff space; (2) 1ms density granularity on the accuracy/latency Pareto frontier is sufficient for practical systems deployments. We show that this size reduction allows for simpler one-shot training at merely half the cost in GPU hours, dollars, and CO 2 emissions, while also speeding up model extraction by up to 216x. This new reduced search space, named CompOFA, is further shown to be still dense enough to produce models covering the same range and density of latency targets for hardware platforms as diverse as OFA, eliminating orders of magnitude of suboptimal model configurations from training and NAS search. The generality of CompOFA is finally validated on another base architecture (Section 5.6), achieving similar gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Efficient neural network design has been an active area of research due to the high computational complexity of CNNs. NAS is increasingly used to guide or replace previously manual design processes. Early NAS techniques <ref type="bibr" target="#b23">(Zoph et al., 2018;</ref><ref type="bibr" target="#b22">Zoph &amp; Le, 2016)</ref> sampled several architectures and trained them from scratch every time, making them extremely compute-hungry. The technique of weight-sharing has emerged as one way to address these inefficiencies <ref type="bibr" target="#b1">(Berman et al., 2020;</ref><ref type="bibr" target="#b0">Bender et al., 2018;</ref><ref type="bibr" target="#b2">Brock et al., 2017;</ref><ref type="bibr" target="#b6">Guo et al., 2019;</ref><ref type="bibr" target="#b9">Liu et al., 2018;</ref><ref type="bibr" target="#b10">Pham et al., 2018)</ref>. These methods slice candidate sub-networks from a larger super-network, thereby sharing weights between them. Simultaneously, latency-guided NAS methods <ref type="bibr">(Tan et al., 2019;</ref><ref type="bibr" target="#b3">Cai et al., 2018;</ref><ref type="bibr" target="#b1">Berman et al., 2020;</ref><ref type="bibr" target="#b14">Stamoulis et al., 2019;</ref><ref type="bibr" target="#b18">Wu et al., 2019)</ref> have sought to incorporate model complexity into their search to find efficient models for a given target latency on a given hardware target.</p><p>Nevertheless, these methods yield a single model per run -both search and training must be repeated for new deployment targets. With compute costs reaching as high as O(10 4 ) GPU hours, this linear scaling is infeasible for an ever-growing need for multi-platform, multi-latency deployment. Once-For-All (OFA) <ref type="bibr" target="#b4">(Cai et al., 2020)</ref> proposed to reduce this cost by using weight-sharing for a large model family that collectively supports a diverse range of latencies. They perform one-time training of O(10 19 ) sub-networks, which can then be independently searched to support a given deployment target later, thus amortizing the training cost. However, this cost is still prohibitively expensive, reaching 1200 GPU hours. The unnecessarily large search space complicates both the training and the searching, stemming from an uninhibited combinatorial explosion of model configurations.</p><p>On the other hand, empirical studies on neural network design spaces <ref type="bibr">(Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b12">Radosavovic et al., 2020)</ref> have recently shown that model dimensions (e.g., depth, width, resolution) are not independent -underlying relations between them can be used to obtain an optimal accuracy-latency trade-off. In other words the number of degrees of freedom used to induce the architecture search space can be reduced without loss of Pareto optimality. This insight forms the basis of our work to constrain the design space of OFA networks while maintaining the same quality (w.r.t. accuracy) as well as diversity of models with reduced train and search costs.</p><p>Dynamic Neural Networks with weight-sharing across models of varying latencies have been explored before <ref type="bibr" target="#b20">(Yu et al., 2018;</ref><ref type="bibr" target="#b19">Yu &amp; Huang, 2019)</ref> but with much fewer models (e.g. 4 in <ref type="bibr" target="#b20">Yu et al. (2018)</ref>) and few dimensions (e.g. only width) which make for much sparser and narrower support for diverse latency targets. We explore a middle ground between these works and <ref type="bibr" target="#b4">Cai et al. (2020)</ref> to build design spaces that are tractable yet sufficiently diverse to support many latency targets and varying in multiple model dimensions. <ref type="bibr" target="#b21">Yu et al. (2020)</ref> proposed replacing OFA's multi-stage training by a single-stage one. However, its training cost remains high at over 2300 TPU hours for O(10 12 ) models. We propose a similar reduction of OFA's multi-stage training but halve the training cost in this process. While <ref type="bibr" target="#b21">Yu et al. (2020)</ref> challenges the conventional training practices of weight-shared NAS, our work emphasizes on insights from <ref type="bibr">Tan &amp; Le (2019)</ref> and <ref type="bibr" target="#b12">Radosavovic et al. (2020)</ref> to refine the network design space to good models and show that an intractably large cardinality is not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DESIGN SPACE PARAMETRIZATION</head><p>Consider a network architecture N composed of m micro-architectural blocks, B 1 , B 2 , . . . , B m . Each block is parametrized by its depth, per-layer width, and per-layer kernel size as</p><formula xml:id="formula_0">B i (d i , W i , K i ).</formula><p>Here d i denotes the number of layers (depth) in the block, and W i and K i are lists denoting the width &amp; kernel sizes of each of these d i layers.</p><p>Once-For-All <ref type="bibr" target="#b4">(Cai et al., 2020)</ref> builds a family of networks N 1 , N 2 , . . . of varying accuracies and latencies. The weights of common layers and channels of a block B i are shared across all the networks. The "block" used in OFA is the Inverted Residual from MobileNetV3 <ref type="bibr" target="#b8">(Howard et al., 2019)</ref> and hence "width" here refers to the channel expansion ratio.</p><formula xml:id="formula_1">d i , w ij , k ij are sampled independently from sets D = [2, 3, 4], W = [3, 4, 6], K = [3, 5, 7] respectively.</formula><p>In OFA, each of these dimensions is treated as orthogonal. Hence, the resulting number of possible networks is enormous, with O(10 19 ) models for m = 5 blocks <ref type="bibr" target="#b4">(Cai et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">COMPOUND RELATION OF MODEL DIMENSIONS</head><p>The combinatorial explosion from just 3 independent model dimensions of D, W, K yields a model family with an enormous number of models. While the aim for jointly training "every possible model" in this large design space is to support a diverse range of hardware platforms, we note the following concerns that arise from this:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Model dimensions are not orthogonal</head><p>Scaling model dimensions like depth, width, and resolution to higher FLOP regimes is a common practice and Tan &amp; Le (2019) systematically showed that compound relations exist between these dimensions for achieving an optimal accuracy-latency trade-off. In particular, increasing model dimensions in accordance with a compound scaling rule gives better results than doing so independently. <ref type="bibr" target="#b12">Radosavovic et al. (2020)</ref> elevated this concept to the model population level and found that a quantized linear relation between model depth and width yielded design spaces with a better concentration of well-performing models. These works solidified the common practice of jointly increasing model depth and width. Yet, all these other sub-optimal models are still included in the search space when all dimensions are sampled independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Large model families complicate training &amp; searching</head><p>The interference between OFA's sub-networks complicates their simultaneous optimization and necessitates techniques like progressive shrinking <ref type="bibr" target="#b4">(Cai et al., 2020)</ref>, increasing training time. Further, extracting models for a desired target cannot rely on simple enumeration and instead needs to rely on predicting model accuracy/latency. This requires additional training for these accuracy predictors specific to the trained model family and collecting latency look-up tables <ref type="bibr" target="#b3">(Cai et al., 2018)</ref>. With a more tractable cardinality, we could rely on more standard, faster approaches during training and achieve "off-the-shelf" usability during search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hardware latency differences below a certain granularity are noisy</head><p>Finally, the difference between unique architectures' accuracy or latency needs to be distinguishable during search. The original OFA design space covers a FLOP range of 120-560 MFLOPs. Within this range, there are O(10 19 ) architecture choices. Even if these models were uniformly distributed in buckets of 1 FLOP, we would still have an O(10 11 ) models in each FLOP target. This is not accounting for the fact that each of these models support variable resolution, which would further multiply the number of choices at each FLOP target. On any hardware, inference time below a certain threshold is expected to be indistinguishable from noise. This threshold can vary depending on application context or hardware (e.g. it can be upto 1-5 ms in ML serving scenarios). Irrespective of the threshold, having these many models with such fine-grained differences in their compute requirement is well below the minimum error resolution at which they can be meaningfully compared. This motivates us to consider that a design space that is sparser by several orders of magnitude might still support the same density and range of deployment targets.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows heatmaps comparing accuracy and latency of OFA sub-networks with uniform depth and width for a fixed kernel size. A closer look into these heatmaps, depicts a monotonically increasing gradient along the diagonal of each of these two dimensional heatmaps. This observation showcases that when the configurations are increased along both the depth and width dimensions we retrieve a class of sub-networks that achieve a better accuracy-latency trade-off as opposed to those formulated through a single dimensional configuration change. Thereby, emphasizing the existence of a coefficient that dictates the relationship between depth and width configurations of a model. This suggests that by quantitatively conforming to this coupling between the model dimensions, we can aim toward reducing the network design space of an architecture without sacrificing its accuracy. Subsequently, this further implies that OFA is unnecessarily training a lot more models than required and we additionally show that OFA's model distribution suffers from high variance as well (Appendix A.1). Conclusively, this leads us toward designing a solution that leverages these insights thereby pruning out the redundant and ineffective sub-networks otherwise generated by the unconstrained OFA architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COMPOUND OFA</head><p>4.1 COUPLING HEURISTIC Motivated by the above observations, we devise a simple heuristic to constrain the network design space by scaling or shrinking the depth and width dimensions together and not independently. Specifically, in each block, whenever we sample the i th largest depth d i ∈ D, we correspondingly sample the i th largest width w i ∈ W for all d i layers in the block. For instance, with D = [2, 3, 4] and W = [3, 4, 6], each block can have either two layers of channel expansion ratio 3, or three layers of ratio 4, or four layers of ratio 6. Once this modified search space is defined by the heuristic, the method of extracting a given model configuration remains the same -we slice out a sub-network of the specified dimensions from the largest network, thus sharing common weights between all sub-networks.</p><p>This significantly reduces the degrees of freedom in the design space. An additional consequence of this is that all layers within a block now have the same width, causing an extra reduction in the design space. Nevertheless, we will show in Section 5 that the design space is still diverse and dense enough to provide models for different deployment requirements.</p><p>We fix the kernel size by default, but for fair comparison we also show a variant with Elastic-Kernel. With a fixed-kernel, we create a simplified search space with kernel sizes fixed to either 3 or 5 in each block, to match the original setting used in MobileNetV3. Thereby in this design space, depth (or width) alone can fully specify a block's configuration and this yields a family of 3 5 = 243 models.</p><p>In the elastic-kernel design space, we expand the kernel size, sampled from K = [3, 5, 7] per layer, same as in OFA, and this results in a family of (3 2 + 3 3 + 3 4 ) 5 ≈ 10 10 models. Unless otherwise specified, we use "CompOFA" to refer to the fixed-kernel design space.</p><p>As we show in the following section, the source of our training speedup is the reduction in cardinality of the search space. The input resolution to the model affects the model inference time but does not affect the number of unique trainable architectures. Hence, we keep the resolution elastic, which allows us one architecture to support multiple latencies for "free", without increasing our search space cardinality (and therefore, training budget). Our goal is to reduce T f amily while achieving the same level of accuracy-latency trade-off and supporting the same range of deployment scenarios. Since we do not wish to train bigger or smaller models themselves, we try to keep E(t p ) unchanged and target a reduction in the number of phases to reduce T f amily .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TRAINING SPEEDUP</head><p>While the number of models does not explicitly factor in T f amily , note that it implicitly affects the count &amp; duration of training phases -progressive shrinking was required in the first place due to interference between a large number of models. The maximum number of simultaneously trainable models in CompOFA's search space is smaller by 17 orders of magnitude. With this significantly smaller family size, we find that the interference between these models is reduced to the extent that we are now able to remove progressive shrinking altogether, and instead achieve the same accuracy with all trainable sub-networks included in training (see Section 5).</p><p>In Section 5 we show that this allows us to reduce the training time by 50%. Additionally, we show an ablation to confirm that this reduction in phases is only possible due to the reduced number of models in CompOFA, and not in the original large design space of OFA. Next, for CompOFA with fixed kernel sizes, we unlock all permissible model configurations (D, W = [(2, 3), (3, 4), (4, 6)]) in one stage, as opposed to progressively adding more configurations. We train this for 25 epochs with an initial learning rate of 0.06, followed by 120 epochs with an initial learning rate of 0.18 to obtain our final network. We sample N sample = 4 models in each batch. The full network serves as a teacher model for knowledge distillation <ref type="bibr" target="#b7">(Hinton et al., 2015)</ref>. The comparison of the possible configurations and epoch durations for each training phase is summarized in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TRAINING TIME AND COST</head><p>By using just one stage of training after the teacher model (in the default fixed kernel setting), instead of multiple stages of the same duration, we are able to reduce the training cost of CompOFA by 50%.  Our reproduction of OFA's training scheme on 6 GPUs takes 978 GPU hours while CompOFA-Fixed Kernel finishes in 493 GPU hours, as shown in Table <ref type="table" target="#tab_1">1</ref>. Table <ref type="table" target="#tab_2">2</ref> shows the translation of this time reduction in terms of dollar cost and CO 2 emissions. Note that the total duration of an epoch with the same design space configuration is comparable, as we do not target a speedup by training smaller models. Instead, the speedup stems largely from halving the number of epochs, by way of reducing the phases for training. This reduction is in turn possible due to a smaller, constrained design space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ACCURACY-LATENCY TRADE-OFF</head><p>After training CompOFA networks, we carried out an evolutionary search <ref type="bibr" target="#b13">(Real et al., 2019)</ref> to retrieve specialized sub-networks for diverse hardware deployment scenarios, as done in OFA. The evolutionary search fetches sub-networks that maximize accuracy subject to a target efficiency (latency or FLOPs) constraint. Similar to OFA, we use a 3-layer accuracy predictor common across all platforms. For latency, we use a lookup-table based latency estimator for Samsung Note 10 CPU provided by <ref type="bibr" target="#b4">Cai et al. (2020)</ref>. For other hardware platforms -namely NVIDIA GeForce RTX 2080 Ti GPU and the Intel(R) Xeon(R) Gold 6226 CPU -we measured actual latency with a batch size of 64 and 1 respectively. The estimated accuracies of the best models returned by this search were then verified on the actual ImageNet validation set.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> reports the performances of accuracy-latency trade-off for CompOFA and OFA networks.</p><p>We observe that the best model in CompOFA for evaluated latency targets are at least as accurate, despite their significantly smaller family size and training budget. This result validates our intuition behind the simple heuristic that aids in creating a smaller design space without losing out on Paretooptimality or density of the generated models.  Figure <ref type="figure">3</ref>: CDF comparisons of 50 randomly sampled models sampled in latency buckets of 5ms each. CompOFA has a higher fraction of its population at or better than a given classification error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">DESIGN SPACE COMPARISON</head><p>Apart from individual models, we further evaluate CompOFA at a population level through a statistical sampling of design space as introduced by <ref type="bibr" target="#b12">Radosavovic et al. (2020;</ref><ref type="bibr">2019)</ref>. Using Samsung Note 10 as sample hardware, we divide the supported range of latencies into buckets of 5ms each. For each latency bucket, we randomly sample 50 models in OFA and CompOFA. Figure <ref type="figure">3</ref> plots the cumulative distribution function (CDF) of classification error for each of these latency buckets. In each bucket, CompOFA's CDFs lie above those of OFA, showing that CompOFA has a higher fraction of models that exceed a given accuracy. Additionally in Figures 4, we show 20 of these models in each bucket for each search space, demonstrating that the average accuracy of a randomly picked model from CompOFA is higher than that in OFA. The takeaway of both these evaluations is that not only does CompOFA yield competitive best models at a given latency, but a better concentration of accurate models with fewer sub-optimal models -i.e. a more accurate overall model population. <ref type="bibr" target="#b4">Cai et al. (2020)</ref> showed that OFA networks suffer a top-1 accuracy drop of up to 3.7% when progressive shrinking is removed, showing its role in resolving interference. With O(10 19 ) models interfering with each other for simultaneous optimization of their weights, a phased approach is needed to stabilize training by gradually train model configurations. In CompOFA, our primary method of reducing the training time is by reducing the number of phases in training. We repeat a similar ablation to compare the effect of progressive shrinking in OFA &amp; CompOFA. For both design spaces, we train with &amp; without progressive shrinking and compare the accuracies of common subnetworks. Figure <ref type="figure">5</ref> shows CompOFA achieves the same accuracy with and without progressive shrinking. We attribute this to lower interference between sub-networks in a design space smaller by 17 orders of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">EFFECT OF NUMBER OF PHASES</head><p>Next, we build a CDF of model configurations in CompOFA and then extract the same sub-networks from OFA. Figure <ref type="figure">5</ref> shows that CompOFA still maintains a slightly higher accuracy despite comparing the same models in both spaces. Thus, models discarded in the process of constraining the design space do not add to the accuracy of OFA or CompOFA -the retained models can achieve the same (if not better) accuracy independent of any potential collaboration from these discarded models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">GENERALIZING TO OTHER ARCHITECTURES</head><p>The guiding principle behind our heuristic -namely, the optimality of coupling depth &amp; width over the unconstrained search space -is expected to apply to other architectures, datasets, and tasks.  In this section, we demonstrate similar savings in training budget with our heuristic applied to a different base architecture.</p><p>We train OFA and CompOFA with the base architecture changed from MobileNetV3 to Proxyless-NAS <ref type="bibr" target="#b3">(Cai et al., 2018)</ref>. We keep the same depths and widths <ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">6]</ref>) and build the search spaces of OFA &amp; CompOFA using the cartesian product or the heuristic, respectively. In CompOFA, we fix the kernel size to 3 in all layers. Both networks use a width multiplier of 1.3.</p><formula xml:id="formula_2">(D = [2, 3, 4], W = [</formula><p>The training hyperparameters, schedule, and search spaces are identical to those described in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">SEARCH TIME</head><p>The intractable cardinality of OFA necessitated the use of latency estimators as a proxy to real latency measurement. With a significantly smaller design space, this need is removed -avoiding the time and effort of building these latency tables that eases "off-the-shelf" practical usability of CompOFA. We introduce a simple memoization technique during the evolutionary search to cache the measured latencies of model architectures and hence avoid its re-measurement, which is only practical with the smaller search space. Table <ref type="table" target="#tab_3">3</ref> reports the average run time of NAS algorithms for a single latency target with this optimization added to both CompOFA and OFA, reducing the search time to just 75 seconds for CompOFA Fixed-Kernel. We also show that the evolutionary search converges in fewer iterations for CompOFA, in Appendix A.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We build upon Once-For-All (OFA) networks to propose CompOFA -a design space for OFA networks using compound couplings between model dimensions, which speeds up the process of oneshot training &amp; neural architecture search with hardware efficiency in the loop.</p><p>We leverage the insight of compound coupling between model dimensions to significantly reduce the architecture search space induced. Even with a simple coupling heuristic proposed, we achieve the same Pareto optimality and sufficient Pareto density at a fraction of the state-of-the-art cost. 2x budget reduction and up to 216x search/extraction latency reduction is shown through elimination of unnecessary models from the design space.</p><p>We make an observation that intractably large search spaces have latency granularity that is unnecessarily fine for realistic hardware deployments, and that CompOFA's significantly smaller search space is sufficiently dense to produce efficient models at the same range, density, and diversity of latency/hardware targets. This smaller cardinality reduces interference in weight-shared training to the extent that we can afford to train CompOFA to the same accuracy with a 50% reduction in the training budget.</p><p>Finally, during the search stage, we demonstrate that this reduced search space eliminates the time and effort needed to build latency lookup tables, thus improving the ease of "off-the-shelf" usability of our method in the real-world settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 EXTRA MODELS IN OFA Figure <ref type="figure">7</ref> depicts the stratification of the latency levels into multiple buckets of size 5ms each and uniformly sampled 100 sub-networks of OFA for each bucket. This box-plot helps in uncovering the considerable variance between the maximum and minimum accuracies of the models of the same latency bucket in OFA. This variance is mainly caused due to OFA's enormous search space that contains redundant models that do not improve the overall accuracy of the model distribution belonging to a certain latency bucket.</p><p>A.2 TRAINING SCHEDULE FOR COMPOFA-ELASTIC KERNEL </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Accuracy and latency heatmaps for kernel size 5 with varying uniform depth and expansion ratios, as measured in MobileNetV3 architecture. Latency is measured on NVIDIA RTX 2080 Ti GPU with BS=64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5. 1</head><label>1</label><figDesc>TRAINING SETUPWe train the full-sized CompOFA network (D = [4], W = [6]) on ImageNet<ref type="bibr" target="#b5">(Deng et al., 2009)</ref> using the same base architecture of MobileNetV3 as OFA. We use a batch size of 1536 and a learning rate of 1.95 to train on 6 NVIDIA V100 GPUs. All other training hyperparameters are kept the same as OFA for accurate comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: CompOFA networks consistently achieve comparable and higher ImageNet accuracies for similar latency and FLOP constraints on CPU, GPU and mobile platforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Random sampling of 20 models in each latency bucket of 5ms for CompOFA &amp; OFA actual latency (left) and bucketed latency (right). CompOFA yields a higher average accuracy, i.e. as a population it has a higher concentration of accurate models. The shaded regions in the left plot show the 95% confidence interval of the average accuracy.</figDesc><graphic url="image-3.png" coords="8,109.47,91.79,194.04,125.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Cumulative distribution function of accuracies of model configurations common to OFA &amp; CompOFA with the base architecture changed from MobileNetV3 to ProxylessNAS. Despite the change in architecture, the same heuristic allows CompOFA to train to the same or marginally higher accuracies with half the training budget.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b4">Cai et al. (2020)</ref> proposed a "progressive shrinking" approach to train the OFA sub-networks in a phased manner, starting with the largest model and then progressively including smaller subnetworks in training. At each batch, a different sub-network is sampled for forward/backward pass. If the expected time to complete one epoch in phase p is E(t p ) and the training is run for e p number of epochs, then the total time to train a model family becomes T f amily ∝ p∈P hases e p × E(t p ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Training schedule, duration, and GPU hour comparisons for OFA and CompOFA. Com-pOFA reduces the training time of OFA by 50% with a fixed kernel space. The columns K, D, W represent the sets of possible model dimensions (as in Section 3.1). While CompOFA allows all sub-networks to be trained after the teacher network, OFA progresses to the full search space with multiple phases of similar duration. See Appendix A.2 for CompOFA with Elastic Kernel</figDesc><table><row><cell></cell><cell cols="6">(a) Once-For-All (Cai et al., 2020) (Elastic Kernel)</cell><cell></cell></row><row><cell>Phase</cell><cell>K</cell><cell>D</cell><cell cols="5">W N sample Epochs Wall Time GPU Hours</cell></row><row><cell>Teacher</cell><cell>7</cell><cell>4</cell><cell>6</cell><cell>1</cell><cell>180</cell><cell>28h 45m</cell><cell>172h 30m</cell></row><row><cell>Elastic Kernel</cell><cell>3, 5, 7</cell><cell>4</cell><cell>6</cell><cell>1</cell><cell>125</cell><cell>26h 51m</cell><cell>161h 06m</cell></row><row><cell cols="2">Elastic Depth-1 3, 5, 7</cell><cell>3, 4</cell><cell>6</cell><cell>2</cell><cell>25</cell><cell>7h 46m</cell><cell>46h 36m</cell></row><row><cell cols="3">Elastic Depth-2 3, 5, 7 2, 3, 4</cell><cell>6</cell><cell>2</cell><cell>125</cell><cell>38h 32m</cell><cell>231h 12m</cell></row><row><cell cols="3">Elastic Width-1 3, 5, 7 2, 3, 4</cell><cell>4, 6</cell><cell>4</cell><cell>25</cell><cell>10h 06m</cell><cell>60h 36m</cell></row><row><cell cols="4">Elastic Width-2 3, 5, 7 2, 3, 4 3, 4, 6</cell><cell>4</cell><cell>125</cell><cell>51h 03m</cell><cell>306h 18m</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>605</cell><cell>163h 03m</cell><cell>978h 18m</cell></row><row><cell></cell><cell></cell><cell cols="4">(b) CompOFA (Fixed Kernel)</cell><cell></cell><cell></cell></row><row><cell>Phase</cell><cell>K</cell><cell>D</cell><cell cols="5">W N sample Epochs Wall Time GPU Hours</cell></row><row><cell>Teacher</cell><cell>7</cell><cell>4</cell><cell>6</cell><cell>1</cell><cell>180</cell><cell>28h 45m</cell><cell>172h 30m</cell></row><row><cell>Compound</cell><cell cols="3">-2, 3, 4 3, 4, 6</cell><cell>4</cell><cell>25</cell><cell>8h 43m</cell><cell>52h 18m</cell></row><row><cell>Compound</cell><cell cols="3">-2, 3, 4 3, 4, 6</cell><cell>4</cell><cell>125</cell><cell>44h 47m</cell><cell>268h 42m</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>330</cell><cell>82h 15m</cell><cell>493h 30m</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Monetary cost and CO 2 emission comparison for OFA &amp; CompOFA. Monetary cost is based on hourly price of 1 NVIDIA V100 on Google Cloud. CO 2 emission estimation is based on<ref type="bibr" target="#b15">Strubell et al. (2019)</ref> </figDesc><table><row><cell>Method</cell><cell cols="2">Cost CO2 emission</cell></row><row><cell>OFA</cell><cell>$2.4k</cell><cell>277 lbs</cell></row><row><cell cols="2">CompOFA (Elastic Kernel) $1.7k</cell><cell>196 lbs</cell></row><row><cell>CompOFA (Fixed Kernel)</cell><cell>$1.2k</cell><cell>138 lbs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average search duration for a single latency target in OFA and CompOFA, when measuring real latency without lookup tables</figDesc><table><row><cell>Network Architecture</cell><cell>Avg. Search Time</cell></row><row><cell>OFA</cell><cell>4.5 hours</cell></row><row><cell>CompOFA (Elastic Kernel)</cell><cell>2.25 hours</cell></row><row><cell>CompOFA (Fixed Kernel)</cell><cell>75 seconds</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1 and Table 1. Hence, CompOFA again requires half the training epochs -and thererfore half the training time, cost, and CO 2 emissions. Figure 6 compares the CDFs of models common to both OFA &amp; CompOFA under this setting, showing that the CompOFA has the same or marginally higher accuracies for the same model configurations despite half the training budget.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>shows the training schedule for CompOFA with Elastic Kernel. Compared to OFA, the training budget is reduced by 31%.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>CompOFA (Elastic Kernel) FASTER CONVERGENCE OF NASAn evolutionary algorithm is set to converge after a certain number of iterations (N ) beyond which the fitness value of the population (P ) does not improve significantly. OFA runs NAS with a setting of N = 500 iterations for a population of size |P | = 100. Figure8demonstrates that the search time of NAS could further be reduced by reducing the number of iterations to N = 300 and N = 50</figDesc><table><row><cell>Phase</cell><cell>K</cell><cell>D</cell><cell cols="5">W N sample Epochs Wall Time GPU Hours</cell></row><row><cell>Teacher</cell><cell>7</cell><cell>4</cell><cell>6</cell><cell>1</cell><cell>180</cell><cell>28h 45m</cell><cell>172h 30m</cell></row><row><cell cols="2">Elastic Kernel 3, 5, 7</cell><cell>4</cell><cell>6</cell><cell>1</cell><cell>125</cell><cell>26h 51m</cell><cell>161h 06m</cell></row><row><cell>Compound</cell><cell cols="3">3, 5, 7 2, 3, 4 3, 4, 6</cell><cell>4</cell><cell>25</cell><cell>9h 21m</cell><cell>56h 06m</cell></row><row><cell>Compound</cell><cell cols="3">3, 5, 7 2, 3, 4 3, 4, 6</cell><cell>4</cell><cell>125</cell><cell>48h 01m</cell><cell>288h 06m</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>455</cell><cell>112h 58m</cell><cell>677h 48m</cell></row><row><cell>A.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">and, therefore, dollar cost and CO2 emissions</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for CompOFA Elastic-Kernel and CompOFA Fixed-Kernel respectively, without losing out on their Pareto-optimality. Coupled with the removal of the latency predictor, these CompOFA-specific optimizations to the search are effective in reducing the search time and improving direct usability of CompOFA. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aows: Adaptive and optimal network width search with latency constraints</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11217" to="11226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Smash: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1908.09791.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00420</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On network design spaces for visual recognition</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1882" to="1890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
				<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single-path nas: Designing hardware-efficient convnets in less than 4 hours</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="481" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universally slimmable networks and improved training techniques</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1803" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08928</idno>
		<title level="m">Slimmable neural networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11142</idno>
		<title level="m">Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
