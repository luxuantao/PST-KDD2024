<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-31">31 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaobo</forename><surname>Li</surname></persName>
							<email>shli@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
							<email>lixiaoguang11@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
							<email>shang.lifeng@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
							<email>dongzhenhua@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
							<email>sunchengjie@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
							<email>jizhenzhou@hit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<email>jiang.xin@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-31">31 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.16747v1[cs.CL]</idno>
					<note type="submission">Columbus born between 25 August and 31 October 1451, died 20 May 1506 was an Italian explorer. Positionally Close: Columbus born between 25 August and 31 October 1451, died 20 May 1506 was an Italian explorer. Highly Co-occurred: Columbus born between 25 August and 31 October 1451, died 20 May 1506 was an Italian explorer.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs' ability to fill in the missing factual words in cloze-style prompts such as "Dante was born in [MASK]." However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the wordlevel patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Do Pre-trained Language Models (PLMs) capture factual knowledge? LAMA benchmark <ref type="bibr" target="#b18">(Petroni et al., 2019)</ref> answers this question by quantitatively measuring the factual knowledge captured in PLMs: query PLMs with cloze-style prompts such as "Dante was born in <ref type="bibr">[MASK]</ref>?" Filling in the mask with the correct word "Florence" is considered a successful capture of the corresponding factual knowledge. The percentage of correct fillings over all the prompts can be used to estimate the amount of factual knowledge captured. PLMs Figure <ref type="figure">1</ref>: The associations we investigated. The underlined words are the missing words that need to be generated. The bold words, which hold specific associations with the missing words, are considered as the word-level patterns that PLMs may use to generate the missing words.</p><p>show a surprisingly strong ability to capture factual knowledge in such probings <ref type="bibr" target="#b10">(Jiang et al., 2020;</ref><ref type="bibr" target="#b21">Shin et al., 2020;</ref><ref type="bibr" target="#b27">Zhong et al., 2021)</ref>, which elicits further research on a more in-depth question <ref type="bibr" target="#b1">(Cao et al., 2021;</ref><ref type="bibr">Elazar et al., 2021a)</ref>: How do PLMs capture the factual knowledge? In this paper, we try to answer this question with a two-fold analysis:</p><p>Research Question 1 Which association do PLMs depend on to capture factual knowledge?</p><p>Research Question 2 Is the association on which PLMs depend effective in capturing factual knowledge?</p><p>We use association to refer to the explicit association between the missing words and the remaining words in the context. We define three typical associations between words. Figure <ref type="figure">1</ref> illustrates these associations in a mask-filling sample.</p><p>Definition 1 Knowledge-Dependent (KD): According to a Knowledge Base (KB), the missing words can be deterministically predicted when providing the remaining words. Figure <ref type="figure" target="#fig_0">2</ref>: The overview of the proposed analysis framework. The dependence measure quantifies how much the PLMs depend on each association to capture factual knowledge when per-training. The effectiveness measure evaluates whether the dependence on an association is good for the factual knowledge performance in probing.</p><p>Definition 3 Highly Co-occurred (HC): The remaining words have a higher co-occurrence frequency with the missing words.</p><p>Question 1 investigates how much PLMs depend on a specific group of remaining words to predict the missing words in pre-training samples. We select the remaining words to be investigated according to their association with the missing words.</p><p>We propose a causal-inspired method to quantify the word-level dependence in each sample. The average dependence on the remaining words that hold the same association with the missing words over all the samples indicates how PLMs rely on this association to predict the missing words. We refer to this average dependence as dependence on the association. The above analysis is named dependence measure.</p><p>In Question 2, we reveal the effectiveness of dependence by the correlation between the quantified dependence on associations and the factual knowledge capturing performance. The performance is probed with additionally crafted clozestyle prompts <ref type="bibr">(Elazar et al., 2021a)</ref>. The more the dependence on an association positive correlates with the probing performance, the more effective this association is. We refer to the second analysis as effectiveness measure. According the experiment results, we have the following observations:</p><p>Observation 1 The PLMs depend more on the positional close and highly co-occurred associations than the knowledge-dependent association to capture factual knowledge.</p><p>Observation 2 Depending on the knowledgedependent association is more effective for factual knowledge capture than positional close and highly co-occurred associations.</p><p>By connecting the two observations, we can answer the question of "how PLMs capture factual knowledge" as: The PLMs are capturing factual knowledge ineffectively since the PLMs depend more on the PC and HC association than the KD association.</p><p>The contribution of this paper can be summarized as follows: (1) We quantify the word-level dependence for mask filling with a causal-inspired method, revealing the word-level patterns that PLMs use to predict the missing words quantitatively. (2) We compare the effectiveness of the dependence on different associations, which provides direct insights for improving PLMs for factual knowledge capture. (3) This paper introduces causal theories into PLMs by formulating the effect measurement process in mask language modeling. It paves the path to measure the causal effects between entities or events described in natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>We take a quick overview of our two-fold analysis with a running example in Figure <ref type="figure" target="#fig_0">2</ref>. Figure <ref type="figure" target="#fig_0">2a</ref> illustrates how to measure the dependence on the remaining words "Columbus" and "died" when predicting the missing words "20 May 1506." We let the PLM generate the missing words based on the original input first, then mask the remaining words in the input and let PLMs generate again. The difference between these two predictions is quantified and used to measure the dependence. The remaining and missing words hold the knowledgedependent association in this sample. We repeat this measure on all the samples whose remaining and missing words have the KD association. Then the dependence on the KD association can be estimated by the average of the quantified difference.</p><p>Figure <ref type="figure" target="#fig_0">2b</ref> measures the effectiveness of the dependence on each association by calculating the correlation coefficient between the dependence and the probing performance. Following <ref type="bibr" target="#b18">(Petroni et al., 2019;</ref><ref type="bibr">Elazar et al., 2021a)</ref>, the probing performance is indicated by the prediction accuracy and consistency when querying on the same fact with different prompts. Since the dependence on associations are quantified in the dependence measure, we can calculate the correlation coefficient between the dependence and performance over all the samples. Their correlation measures whether the dependence on an association is harmful or beneficial to the performance, showing the effectiveness of the dependence on an association quantitatively.</p><p>Section Outline We organize the rest of this section as follows. In Section 2.2.1, we formalize how we quantify the dependence with the causal effect estimation. Section 2.2.2 gives detail about how we build the probing samples for different associations. Section 2.3.1 introduces the metrics we used to indicate the performance of factual knowledge capture. Section 2.3.2 describes the details about the effectiveness measure of associations.</p><p>2.2 Quantify the Dependence on Associations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Causal Effect Estimation for PLMs</head><p>To study the causal effect of the different input words, we build a Structured Causal Model (SCM) for the missing words generation process and apply interventions on some input words to estimate their effect quantitatively. We consider the missing words as outcome words and the remaining words that hold a certain association (e.g. positionally close) with the outcome words as treatment words. Then, we can formally represent the word generation process with SCM, as the following structural equations:</p><formula xml:id="formula_0">w c = f (I), w t = PLM(w c ) w o = PLM(w c , w t ).<label>(1)</label></formula><p>Separate the words in a sentence into three groups: treatment words W t , outcome words W o , and context words W c (specified by w t , w o , and w c respectively). Equation <ref type="formula" target="#formula_0">1</ref>formulates the following data generation process: (1) Sample a sentence from the natural text space I and get the context words w c using function f . (2) Generate the treatment words w t by the PLM based on w c only.</p><p>(3) Generate outcome words w o based on both the w c and w t .</p><p>To obtain the quantitative causal effect of treatment words W t on outcome words W o , we apply the do-calculus do() <ref type="bibr" target="#b17">(Pearl, 2009)</ref> on treatment words W t to introduce interventions for estimating the causal effect. do() denotes the operation of forcibly setting the value of W t . Then the causal effect of W t on W o can be estimated by the Average Treatment Effect (ATE) <ref type="bibr" target="#b20">(Rubin, 1974)</ref>:</p><formula xml:id="formula_1">E [P (W o |do(W t = ?t ))] -E [P (W o |do(W t = w m ))] .</formula><p>(2)</p><p>Accordingly, we define ATE for PLMs as:</p><formula xml:id="formula_2">? = I PLM (do(W t = ?t ), w c ) P (s) - I PLM (do(W t = w m ), w c ) P (s),<label>(3)</label></formula><p>where ?t is the ground truth of the treatment words W t (the original value of W t without intervention). w m is the intervention value (several [MASK]s) for W t , and we use it to simulate removing the groundtruth value ?t from the input. P (s) denotes the probability of selecting the sample s that consists of w t , w o , and w c from I. PLM(?, ?) denotes the output of PLMs with certain input. Table <ref type="table" target="#tab_0">1</ref> illustrates the interventions on the SCM for different associations.</p><p>The raw output of PLMs is a probability distribution over fixed vocabulary. We transform the output into reciprocal rank to quantify the differences:</p><formula xml:id="formula_3">PLM k (w t , w c ) = 1 rank ?o , if rank ?o ? k 0, otherwise .</formula><p>(4) ?o is the ground-truth outcome words. rank ?o is the rank position of ?o according to the generation probability of ?o output by PLM(w c , w t ). We set k to 100 and use PLM 100 to replace PLM in Equation 3 to calculate ATE. The ATE reflects the effect of W t on the prediction of W o , it can be regarded as a quantitative estimation of how much PLMs depend on W t when generating W o .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Mark words by Associations</head><p>Wikipedia is a rich source of knowledge <ref type="bibr" target="#b24">(Thom et al., 2007;</ref><ref type="bibr" target="#b9">Hassanzadeh, 2021)</ref>, and most of the PLMs nowadays have been pre-trained on Wikipedia <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr" target="#b16">Liu et al., 2019;</ref><ref type="bibr" target="#b14">Lan et al., 2019)</ref>, so we take Wikipedia sentences as pre-training samples to construct the probing samples for dependence measure. We probe the mask-filling on these sentences to analyze what PLMs based on when capturing factual knowledge in pre-training.</p><p>The outcome we want to observe is the predictions of factual words in the sentences. In order to locate the factual words, we align each sentence with a triplet (subject, predicate, object) in the KB. The words that correspond to the object serve as outcome words W o for observation, and the remaining words that hold an explicit association with W o are marked as treatment words W t for intervention. For different associations, the W t is identified as:</p><p>1. Knowledge-Dependent: all the remaining words correspond to the predicate and object in the same triplet with the W t .</p><p>2. Positionally Close: the remaining words closest to W o .</p><p>3. Highly Co-occurred: the remaining words that have higher Pointwise Mutual Information (PMI) <ref type="bibr" target="#b2">(Church and Hanks, 1990</ref>) with W o . The PMI is calculated over all the Wikipedia sentences using the following equation:</p><formula xml:id="formula_4">PMI(w i ; ?o ) = P (w i | ?o ) P (w i ) , (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where ?o is a group of words (a span) and w i is a single word.</p><p>4. We further define a Random (R) association, where the W t are randomly selected remaining words. It provides some empirical support for how much the modifications in the context affect the mask-filling output.</p><p>Accordingly, one sentence yields four probing samples for the four associations, respectively. The four probing samples share the same W o but use different words as W t to show the dependence on different associations when predicting the same W o . We preserve that the number of words in W t is the same among different associations. For example, if there are two words used as W t by the KD association, we select the top two closest words with W o as W t for PC, and the words have the top two PMI with W o for HC. We can obtain a set of probing samples for each association. The sample sets for different associations source from the same set of sentences and have the same size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Measure the Effectiveness of Associations</head><p>This section investigates which association can lead to better performance on factual knowledge capture. We first define the metrics to evaluate the performance, then we measure the effectiveness of an association by relating the dependence on this association with probing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Metrics for Factual Knowledge Probing</head><p>Section 2.2 uses the original Wikipedia sentence as pre-training samples to quantify the dependence PLMs used to capture the corresponding fact in pre-training. The performance of capturing the corresponding fact is probed by having PLMs fill masks on crafted quires. We construct these queries by instantiating templates on triplets <ref type="bibr" target="#b18">(Petroni et al., 2019)</ref>. T i (s) denotes the i-th query for the fact corresponds to s. The accuracy mrr of capturing this fact is obtained by averaging over all the predictions obtained with different queries:</p><formula xml:id="formula_6">mrr (s) = 1 n n i PLM k (T i (s)),<label>(6)</label></formula><p>PLM k (T i (s)) denotes the reciprocal rank of the ground truth in the PLM's output for query T i (s), it is defined in Equation <ref type="formula">4</ref>. The consistency of the capture is indicated by the percentage of the pairs of queries that have the same result <ref type="bibr">(Elazar et al., 2021a)</ref>:</p><formula xml:id="formula_7">con (s) = i =j 1 PLM(T i (s))=PLM(T j (s)) n(n -1) . (7)</formula><p>There are n different queries on every fact, and we can get n 2 = n(n -1) pairs of predictions in total. PLM(T i (s)) denotes the top-1 output for the query T i (s). The value of 1 PLM(T i (s))=PLM(T j (s)) is an indicator function that takes the value 1 if the PLMs returns identical at top-1 for T i (s) and T j (s) and 0 otherwise. The PLMs are better on the consistency metric if they keep the predictions consistent when queries only vary on the surface forms. E.g., the two queries "Dante was born in [MASK]" and "The birthday of Dante is [MASK]" should return the same results.</p><p>Finally, we evaluate the factual knowledge capture performance by jointly examining the accuracy and consistency <ref type="bibr">(Elazar et al., 2021a)</ref>:</p><formula xml:id="formula_8">test(s) = mrr (s) ? con (s)<label>(8)</label></formula><p>test(s) measures the probing performance on template-based queries. We also define a metric to measure how well the PLMs memorize the missing words in pre-training samples (Wikipedia sentences): train(s) = PLM k (s).</p><p>(9)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Correlate Performance with Dependence</head><p>We have quantified the dependence on each association and defined the metrics for probing performance in the above sections. We then calculate the Pearson correlation coefficient <ref type="bibr">(Kirch, 2008)</ref> between dependence and probing performance to reveal the effectiveness of different associations.</p><p>An association is considered more effective if the probing performance positively correlates with its dependence more.</p><p>Because only part of the facts has available templates, the samples in the dependence measure without templates are ignored in the calculation. The factual knowledge captured by different PLMs may vary significantly due to the differences in model scale, pre-training data, or other settings. To make the correlation coefficient comparable between different PLMs, we calculate the correlation only on the factual knowledge gathered correctly by the PLM. I.e., only the pre-training samples with train(s) = PLM k (s) = 1 are involved. 3 Experiments and Discussions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probing Data and PLMs</head><p>We use the TREX dataset <ref type="bibr" target="#b6">(Elsahar et al., 2018)</ref>, which aligns KB triplets with Wikipedia sentences, to construct the samples for the dependence measure following the definition in Section 2.2.2. We employ the templates from <ref type="bibr">(Elazar et al., 2021a)</ref> to construct the queries to probe the factual knowledge for the effectiveness measure. Table <ref type="table" target="#tab_1">2</ref> shows the statistics for the data in the dependence measure and the effectiveness measure. The PLMs we analyzed include BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b16">(Liu et al., 2019)</ref>, SpanBERT <ref type="bibr" target="#b11">(Joshi et al., 2020)</ref>, and ALBERT <ref type="bibr" target="#b14">(Lan et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dependence on Associations</head><p>The dependence on an association is the average ATE over the probing samples whose treatment words hold that association with the outcome words. Table <ref type="table" target="#tab_3">3</ref> shows the quantified dependence of different associations. The accuracy in Table <ref type="table" target="#tab_3">3</ref> represents the accuracy of recovering the masked factual words in pre-training samples, revealing how well does PLMs memorize the pre-training samples. It is calculated by Equation <ref type="formula">9</ref>with k = 1. We find a general trend over all the picked PLMs: the Positionally Close (PC) association takes the dominant effect on the prediction results, the Highly Co-occurred (HC) association comes second, and the least for the Knowledge-Dependent (KD) association. The trend does not change much as increasing the model scale (large vs. base), using additional training data (RoBERTa vs. BERT), or improving the masking strategy (SpanBERT vs. BERT). Consequently, the accuracy drops the most when perturbing the positionally close words but least on knowledge-dependent words 1 .   The results provide quantitative evidence for Question 1 of "Which association do PLMs depend on to capture factual knowledge?:" PLMs prefer the associations founded with positionally close or the highly co-occurred words to the knowledge-based clues. It is different from how a conventional KB works, e.g., an object can be retrieved by the corresponding subject and predicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Correlations between Dependence and Performance</head><p>We show the correlation between association's dependence and the probing performance in Figure <ref type="figure" target="#fig_1">3</ref>. Each point in the figure represents a piece of factual knowledge s. We refer to it as a fact for convenience. The horizontal axis indicates test(s) for the fact, showing the probing performance of the fact with effectiveness measure. The vertical axis shows the dependence of associations when capturing this fact, which is quantified by the causal effect estimation defined in Section 2.2.1. The straight lines are the regression lines and different associations are shown in different line styles 2 .</p><p>when perturbing the different associations. 2 We standardize the quantified value of dependence (denoted as Std. Dependence) and plot a bucket of facts as a single point to show the trends clearly. The correlations with-As we can see from the results, the dependence on the KD association positively correlates with the probing performance. The dependence on the HC association has a slightly positive correlation or almost has no correlations sometimes (such as ALBERT in Figure <ref type="figure" target="#fig_1">3d</ref>). The PC association holds a negative correlation with the performance.</p><p>These results can give an empirical answer to "Is the association on which PLMs depend effective in capturing factual knowledge?:" the more PLMs depend on the Knowledge-Dependent (KD) association, the better PLMs can capture the corresponding factual knowledge. Meanwhile, relying much on the positionally close association is harmful to the probing performance.</p><p>The dependence measure results reveal that the PLMs depend most on the positionally close but least on the knowledge-dependent association. However, in effectiveness measure, we find that the positionally close association is the most ineffective for factual knowledge capture while the knowledge-dependent association is the most effective. By connecting the two results, we can conclude the answer to the question in the title: The PLMs do not capture factual knowledge ideally,  since they depend more on the ineffective associations than the effective one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Case Study</head><p>To illustrate the analysis result intuitively, we show two cases with SpanBERT-large in Table <ref type="table">4</ref>.</p><p>The MRR shows the probing performance on the template-based query (calculated by Equation <ref type="formula" target="#formula_6">6</ref>).</p><p>In Case 1, the knowledge-dependent association gains the biggest effect, and the predictions are robust in all the template-based probing. However, the positionally close association takes the main effect in Case 2, while the PLM fails to recall the word "England" with the template-based queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussions</head><p>Generality of the Proposed Probing Method Generally, the dependence measure offers a way to measure how much the word-level patterns cause the prediction of missing words in Mask Language Model (MLM). Because words are readable, directly visible, and can be manipulated from the input side directly, the word-level patterns can provide more intuitive interpretations than numeric representation vectors <ref type="bibr">(Elazar et al., 2021b)</ref> or neurons <ref type="bibr">(Vig et al.)</ref>. We use the proposed method to estimate the causal effect of three typical associations in this paper, while this method can be easily adapted to quantify the dependence on any wordlevel patterns.</p><p>Reconsidering "PLM as KB" If we want to use a PLM like a KB, whether the PLM has the same inner workflow as KBs deserves to be considered. The prevalent KBs index knowledge as subjectpredicate-object triplets and can infer with triplets <ref type="bibr" target="#b22">(Speer et al., 2017;</ref><ref type="bibr" target="#b0">Bollacker et al., 2008;</ref><ref type="bibr" target="#b26">Vrande?i? and Kr?tzsch, 2014)</ref>. However, we find out that the knowledge-dependent association, which represents the process of inferring a missing object based on the given subject and predicate, has the lowest dependence in the PLMs. It provides evidence that the PLMs work quite differently with KBs and can not serve stably as KBs for now.</p><p>Overfiting and Generalization Figure <ref type="figure" target="#fig_4">4</ref> shows the correlations between the dependence on associations and the mask-filling accuracy on pre-training samples (referred to as memorizing accuracy). The memorizing accuracy increases most as the dependence of the PC association increases, demonstrating that the more PLMs depend on the positionally close words, the better PLMs can recover the pretraining samples. However, there is an opposite trend in probing performance as shown in Figure <ref type="figure" target="#fig_1">3</ref>. in pre-training. We name it as "train" in Equation <ref type="formula">9</ref>and "Memorizing Accuracy" in Figure <ref type="figure" target="#fig_4">4</ref>.</p><p>Overlap between Associations The clues for different associations overlap sometimes, e.g., some remaining words may hold the KD and PC associations with the missing words at the same time.</p><p>The overlaps do not impair the estimations because we use a set of samples to estimate the effect of each association. The samples that hold the same association stay in the same set, and the average causal effect in all these samples is the quantified dependence of this association. The sample sets are quite different for different associations. Motivated by the probing results, some recent works analyze the captured factual knowledge from more perspectives. <ref type="bibr" target="#b1">Cao et al. (2021)</ref> analyze the distribution of predictions and the answer leakage in probing. <ref type="bibr" target="#b19">Poerner et al. (2020)</ref> propose that the PLMs could predict based on some correlation between surface forms rather than infer according to facts. <ref type="bibr">Elazar et al. (2021a)</ref> reveal that the PLMs' outputs are inconsistent as querying the same fact with different prompts.</p><p>This paper proposes a more fine-grained inspection of word-level patterns in the input. In addition to constructing more challenging probing data as input or analyzing the outputs more detailedly, we try to reveal the inner mechanism of PLMs by conducting intervention on the input and then observing the change in the output. Causal-inspired Interpretations in NLP A causal-inspired approach to explanation is to generate counterfactual examples and then compare the predictions <ref type="bibr">(Feder et al., 2021a)</ref>. <ref type="bibr">Feder et al. (2021b)</ref> propose a framework for producing explanation for NLP models using counterfactual representation. <ref type="bibr">Vig et al.</ref> analyze the effect of neurons (or attention heads) on the gender bias using causal mediation analysis. In this paper, we revisit the word-level post-hot interpretation <ref type="bibr" target="#b23">(Sun et al., 2021;</ref><ref type="bibr" target="#b15">Li et al.)</ref> from a causal-effect perspective: intervene on some specified words in the input and measure the difference in the output to estimate the causal effect of these words. Furthermore, we evaluate the effectiveness of different causes by calculating correlations between their effects and performance. As far as we know, our work is the first study to probe and evaluate word-level patterns in the factual knowledge capture task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we try to answer the question of How Pre-trained Language Models Capture Factual Knowledge by measuring and evaluating different associations that PLMs use to capture factual knowledge. We present three word-level associations, knowledge-dependent, positionally close, and highly co-occurred in the analysis. The analysis results show that the PLMs rely more on the ineffective positionally close and highly co-occurred associations when capturing factual knowledge, and somewhat ignore the effective knowledgedependent clues. These findings indicate that we should pay more attention to the knowledgedependent association to let PLMs capture factual knowledge better.</p><p>We use the T-REx<ref type="foot" target="#foot_1">4</ref> dataset to provide the initial alignment between KB triplets and the Wikipedia sentences. We use the aliases in KB as keys for fuzzy string match (Levenshtein distance is less than 2, stemming before matching, etc.) to align more subjects, predicates, and objects with spans in the sentence. The sentences that have no aligned triplet are filtered out.</p><p>Sometimes, the outcome words in a single sentence relate to multiple triplets that satisfy the rules for KB. E.g., there are two groups of remaining words that can infer the outcome words deterministically based on KB. We select them all as the W t when probing the KB association and keep the number of the masked words be the same in interventions for the other associations. D KD , D PC , and D HC denote the sample sets for the Knowledge-Dependent (KD), Positionally Close (PC), and Highly Co-occurred (HC) associations, respectively.</p><p>For the Highly Co-occurred association (HC), the remaining words that are top-k in PMI with the ground-truth outcome words ?o are selected as W t . The k is the number of words with the KD associations for the same sentence. The PMI between words is calculated by all the Wikipedia sentences. If the ?o consists of multiple words, occurring with all the words in ?o altogether are taken as co-occurring. The order of the words in ?o are ignored for efficiency. Table <ref type="table">5</ref> shows more details about the probing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Probing Results</head><p>The Pearson correlation coefficients between the dependence on associations (raw value without standardization) and the performance are shown in Table <ref type="table">8</ref>. The three metrics, accuracy (defined in Equation <ref type="formula" target="#formula_6">6</ref>), consistency (defined in Equation <ref type="formula">7</ref>), and the overall performance metric (defined in Equation <ref type="formula" target="#formula_8">8</ref>), are reported respectively. The correlation coefficients between the dependence and the performance are consistent with the slopes of the regression lines in Figure <ref type="figure" target="#fig_1">3</ref>. Table <ref type="table">6</ref> shows the accuracy decreasing results after masking the treatments words when generating the missing words in Wikipedia sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2</head><label>2</label><figDesc>Positionally Close (PC): The remaining words are positionally close to the missing words. Columbus born between 25 August and 31 October 1451, died [MASK]s was an Italian explorer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The correlations between the dependence on associations and the probing performance on factual knowledge capture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>out standardization for more PLMs are in Table8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Pre-training samples for the dependence measure (Wikipedia Sentence) Dep. KD: Kimwenza is a community in the Democratic Republic of the Congo in the Mont Ngafula commune in the south of the capital, Kinshasa . Kimwenza is a community in the Democratic Republic of the Congo in the Mont Ngafula commune in the south of the capital, Kinshasa . 0.0000 HC: Kimwenza is a community in the Democratic Republic of the Congo in the Mont Ngafula commune in the south of the capital, Kinshasa . samples for the dependence measure (Wikipedia Sentence) ATE KD: Drayton is a hamlet in England, in the county of Northamptonshire, . . ., hundred of Fawsley, ? of a mile on the low-lying north western side of the town of Daventry . Drayton is a hamlet in England, . . ., in the parish and union of Daventry, hundred of Fawsley, ? of a mile on the low-lying north western side of the town of Daventry . 0.9496 HC: Drayton is a hamlet in England, . . ., in the parish and union of Daventry, hundred of Fawsley, ? of a mile on the low-lying north western side of the town of Daventry . Two cases from SpanBERT-large. The quantified dependence on associations (denoted by Dep.) and the performance of factual knowledge capture (denoted by MRR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The correlations between the dependence on associations and the mask-filling accuracy on the pretraining samples (Wikipedia sentences).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>To analyze the dependence of associations, we do interventions on treatment words to reveal their causal effects on the outcome words.</figDesc><table><row><cell>Association</cell><cell cols="2">Knowledge-Dependent</cell><cell cols="2">Positionally Close</cell><cell></cell><cell>Highly Co-occurred</cell></row><row><cell></cell><cell cols="2">Wt born between 25 August</cell><cell cols="2">Columbus born between 25</cell><cell cols="2">Wt born between 25 August</cell></row><row><cell>Input</cell><cell cols="2">and 31 October 1451, Wt Wo</cell><cell cols="2">August and 31 October 1451,</cell><cell cols="2">and 31 October 1451, died Wo</cell></row><row><cell></cell><cell cols="2">was an Italian explorer.</cell><cell cols="2">Wt Wo Wt an Italian explorer.</cell><cell></cell><cell>was an Italian Wt.</cell></row><row><cell></cell><cell>W</cell><cell>c</cell><cell>W</cell><cell>c</cell><cell></cell><cell>W</cell><cell>c</cell></row><row><cell>SCM</cell><cell cols="2">do(W t ={MASK, MASK}) do(Wt={Columbus, died})</cell><cell>do(Wt={MASK, MASK}) do(W t ={died, was})</cell><cell></cell><cell>do(W do(W</cell><cell>t ={MASK, MASK}) t ={Columbus, explorer})</cell></row><row><cell></cell><cell>t W</cell><cell>o W</cell><cell>t W</cell><cell>o W</cell><cell></cell><cell>t W</cell><cell>o W</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the probing data.</figDesc><table><row><cell cols="2">Sample in Dependence Measure</cell></row><row><cell># Wikipedia sentences</cell><cell>4,779,753</cell></row><row><cell># Different triplets</cell><cell>3,795,229</cell></row><row><cell># Different predicates</cell><cell>565</cell></row><row><cell cols="2"># Sentences with synthetic templates 1,119,875</cell></row><row><cell cols="2">Queries in Effectiveness Measure</cell></row><row><cell># Template-based queries</cell><cell>7,645,635</cell></row><row><cell># Different triplets</cell><cell>654,112</cell></row><row><cell># Different predicates</cell><cell>38</cell></row><row><cell># Different templates</cell><cell>328</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Table 6 in the Appendix shows the accuracy decrease The quantified dependence on associations. Accuracy denotes the performance of filling in the masks in pre-training samples. The PLMs use cased and uncased vocabularies are separated.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell><cell cols="3">Dependences on Associations (k = 100)</cell></row><row><cell></cell><cell></cell><cell>KD</cell><cell>PC</cell><cell>HC</cell><cell>R</cell></row><row><cell>BERT-base-cased</cell><cell>0.3623</cell><cell cols="3">0.1585 0.4085 0.1779</cell><cell>0.1081</cell></row><row><cell>BERT-large-cased</cell><cell>0.3692</cell><cell cols="3">0.1603 0.4113 0.1791</cell><cell>0.0996</cell></row><row><cell>BERT-large-cased-wwm</cell><cell>0.5030</cell><cell cols="3">0.1384 0.4477 0.2305</cell><cell>0.1072</cell></row><row><cell>SpanBERT-large</cell><cell>0.5223</cell><cell cols="3">0.1351 0.3679 0.2383</cell><cell>0.1157</cell></row><row><cell>RoBERTa-base</cell><cell>0.3511</cell><cell cols="3">0.1352 0.3926 0.2093</cell><cell>0.1053</cell></row><row><cell>RoBERTa-large</cell><cell>0.4276</cell><cell cols="3">0.1360 0.3962 0.2162</cell><cell>0.0985</cell></row><row><cell>BERT-large-uncased-wwm</cell><cell>0.5035</cell><cell cols="3">0.1410 0.4350 0.2290</cell><cell>0.1089</cell></row><row><cell>ALBERT-xxlarge-v2</cell><cell>0.4758</cell><cell cols="3">0.2852 0.4338 0.3801</cell><cell>0.2704</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 5 shows the corresponding statistics of the overlaps.</figDesc><table><row><cell>4 Related Works</cell></row><row><cell>Probing Factual Knowledge in PLMs Factual</cell></row><row><cell>Knowledge Probing in PLMs has attracted much</cell></row><row><cell>attention recently. LAMA (Petroni et al., 2019)</cell></row><row><cell>propose a benchmark that probes the factual knowl-</cell></row><row><cell>edge in the PLMs with cloze-style prompts and</cell></row><row><cell>shows PLMs' ability to capture factual knowledge.</cell></row><row><cell>This ability can be further explored by tuning the</cell></row><row><cell>prompts for probing Jiang et al. (2020); Shin et al.</cell></row><row><cell>(2020); Zhong et al. (2021).</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://www.wikidata.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://hadyelsahar.github.io/t-rex/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Probing Sample Construction for the Dependence Measure</head><p>We detail how we construct the probing samples for the dependence measure as follows. We take a subject-predicate-object triplet in Wikidata 3 as a piece of factual knowledge <ref type="bibr" target="#b18">(Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Cao et al., 2021;</ref><ref type="bibr" target="#b12">Kassner et al., 2021;</ref><ref type="bibr">Elazar et al., 2021a)</ref>. A subject-predicate-object triplet is aligned with a Wikipedia sentence by matching the subject, predicate, and object with their corresponding spans, respectively. A subject-predicateobject triplet is aligned with a Wikipedia sentence by matching the subject, predicate, and object with their corresponding spans, respectively. The words that correspond to the object are the factual words that are masked and need to be predicted, and we investigate how the different remaining words contribute to the prediction.</p><p>The remaining words that have three typical associations with the masked words are considered in the analysis. The rules to identify the remaining words that have the Knowledge-Dependent (KD) association are:</p><p>1. The W o and the W t describe the same subjectpredicate-object triplet in KB.</p><p>2. The W t are the natural language description of the subject and predicate, the W o are that for the object.</p><p>3. If the subject and predicate corresponding to W t are given correctly (i.e., W t = ?t ), the ground-truth value of the object is unique in the KB.</p><p>The first rule makes the W t and W o grounded in the same piece of factual knowledge. The second rule makes predicting the outcome words similar to inferring the object using a KB when giving the subject and predicate. The third rule means if the treatment words are given correctly, there should be one and only one ground-truth value for the object. The third rule is similar to the N-1 relationship <ref type="bibr" target="#b1">(Cao et al., 2021)</ref> in KB and lets the ground-truth treatment words can be regarded as a sufficient condition to predict the unique outcome words deterministically.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledgeable or educated guess? revisiting language models as knowledge bases</title>
		<author>
			<persName><forename type="first">Boxi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1860" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hinrich Sch?tze, and Yoav Goldberg. 2021a. Measuring and improving consistency in pretrained language models</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00410</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1012" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Alon Jacovi, and Yoav Goldberg. 2021b. Amnesic probing: Behavioral explanation with amnesic counterfactuals</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00359</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="160" to="175" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">T-rex: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Amir</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emaad</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Wood-Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00725</idno>
		<title level="m">Causal inference in natural language processing: Estimation, prediction, interpretation and beyond</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uri Shalit, and Roi Reichart. 2021b. Causalm: Causal model explanation through counterfactual language models</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Oved</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting the future with wikidata and wikipedia</title>
		<author>
			<persName><forename type="first">Oktie</forename><surname>Hassanzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISWC</title>
		<meeting>the ISWC</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00324</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilingual LAMA: Investigating knowledge in multilingual pretrained language models</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.284</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3250" to="3258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pearson&apos;s Correlation Coefficient</title>
		<idno type="DOI">10.1007/978-1-4020-5614-7_2569</idno>
		<editor>Wilhelm Kirch</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1090" to="1091" />
			<pubPlace>Netherlands, Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">E-BERT: Efficient-yet-effective entity embeddings for BERT</title>
		<author>
			<persName><forename type="first">Nina</forename><surname>Poerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulli</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.71</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">688</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.10470</idno>
		<title level="m">Interpreting deep learning models in natural language processing: A review</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jovan</forename><surname>James A Thom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Marie</forename><surname>Pehcevski</surname></persName>
		</author>
		<author>
			<persName><surname>Vercoustre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0711.2917</idno>
		<title level="m">Use of wikipedia categories in entity ranking</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Investigating gender bias in language models using causal mediation analysis</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nevo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factual probing is [MASK]: Learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.398</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5017" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
