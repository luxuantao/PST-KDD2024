<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Secure Auditing and Deduplicating Data in Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingwei</forename><surname>Li</surname></persName>
							<email>lijin@gzhu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Dongqing</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhang</forename><surname>Cai</surname></persName>
							<email>zhangcai@psu.edu.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Control Engineering</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Guangzhou University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Pennselvania State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Secure Auditing and Deduplicating Data in Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">328AB81405643DB5A988642690804041</idno>
					<idno type="DOI">10.1109/TC.2015.2389960</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2015.2389960, IEEE Transactions on Computers This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2015.2389960, IEEE Transactions on Computers</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the cloud computing technology develops during the last decade, outsourcing data to cloud service for storage becomes an attractive trend, which benefits in sparing efforts on heavy data maintenance and management. Nevertheless, since the outsourced cloud storage is not fully trustworthy, it raises security concerns on how to realize data deduplication in cloud while achieving integrity auditing.</p><p>In this work, we study the problem of integrity auditing and secure deduplication on cloud data. Specifically, aiming at achieving both data integrity and deduplication in cloud, we propose two secure systems, namely SecCloud and SecCloud + . SecCloud introduces an auditing entity with a maintenance of a MapReduce cloud, which helps clients generate data tags before uploading as well as audit the integrity of data having been stored in cloud. Compared with previous work, the computation by user in SecCloud is greatly reduced during the file uploading and auditing phases. SecCloud + is designed motivated by the fact that customers always want to encrypt their data before uploading, and enables integrity auditing and secure deduplication on encrypted data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Cloud storage is a model of networked enterprise storage where data is stored in virtualized pools of storage which are generally hosted by third parties. Cloud storage provides customers with benefits, ranging from cost saving and simplified convenience, to mobility opportunities and scalable service. These great features attract more and more customers to utilize and storage their personal data to the cloud storage: according to the analysis report, the volume of data in cloud is expected to achieve 40 trillion gigabytes in 2020.</p><p>Even though cloud storage system has been widely adopted, it fails to accommodate some important emerging needs such as the abilities of auditing integrity of cloud files by cloud clients and detecting duplicated files by cloud servers. We illustrate both problems below.</p><p>The first problem is integrity auditing. The cloud server is able to relieve clients from the heavy burden of storage management and maintenance. The most difference of cloud storage from traditional in-house storage is that the data is transferred via Internet and stored in an uncertain domain, not under control of the clients at all, which inevitably raises clients great concerns on the integrity of their data. These concerns originate from the fact that the cloud storage is susceptible to security threats from both outside and inside of the cloud <ref type="bibr" target="#b0">[1]</ref>, and the uncontrolled cloud servers may passively hide some data loss incidents from the clients to maintain their reputation. What is more serious is that for saving money and space, the cloud servers might even actively and deliberately discard rarely accessed data files belonging to an ordinary client. Considering the large size of the outsourced data files and the clients' constrained resource capabilities, the first problem is generalized as how can the client efficiently perform periodical integrity verifications even without the local copy of data files.</p><p>The second problem is secure deduplication. The rapid adoption of cloud services is accompanied by increasing volumes of data stored at remote cloud servers. Among these remote stored files, most of them are duplicated: according to a recent survey by EMC <ref type="bibr" target="#b1">[2]</ref>, 75% of recent digital data is duplicated copies. This fact raises a technology namely deduplication, in which the cloud servers would like to deduplicate by keeping only a single copy for each file (or block) and make a link to the file (or block) for every client who owns or asks to store the same file (or block). Unfortunately, this action of deduplication would lead to a number of threats potentially affecting the storage system <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b1">[2]</ref>, for example, a server telling a client that it (i.e., the client) does not need to send the file reveals that some other client has the exact same file, which could be sensitive sometimes. These attacks originate from the reason that the proof that the client owns a given file (or block of data) is solely based on a static, short value (in most cases the hash of the file) <ref type="bibr" target="#b2">[3]</ref>. Thus, the second problem is generalized as how can the cloud servers efficiently confirm that the client (with a certain degree assurance) owns the uploaded file (or block) before creating a link to this file (or block) for him/her.</p><p>In this paper, aiming at achieving data integrity and deduplication in cloud, we propose two secure systems namely SecCloud and SecCloud + .</p><p>SecCloud introduces an auditing entity with a maintenance of a MapReduce cloud, which helps clients generate data tags before uploading as well as audit the integrity of data having been stored in cloud. This design fixes the issue of previous work that the computational load at user or auditor is too huge for tag generation. For completeness of fine-grained, the functionality of auditing designed in SecCoud is supported on both block level and sector level. In addition, SecCoud also enables secure deduplication. Notice that the "security" considered in SecCoud is the prevention of leakage of side channel information. In order to prevent the leakage of such side channel information, we follow the tradition of <ref type="bibr" target="#b2">[3]</ref>[2] and design a proof of ownership protocol between clients and cloud servers, which allows clients to prove to cloud servers that they exactly own the target data.</p><p>Motivated by the fact that customers always want to encrypt their data before uploading, for reasons ranging from personal privacy to corporate policy, we introduce a key server into SecCloud as with <ref type="bibr" target="#b3">[4]</ref> and propose the SecCloud + schema. Besides supporting integrity auditing and secure deduplication, SecCloud + enables the guarantee of file confidentiality. Specifically, thanks to the property of deterministic encryption in convergent encryption, we propose a method of directly auditing integrity on encrypted data. The challenge of deduplication on encrypted is the prevention of dictionary attack <ref type="bibr" target="#b3">[4]</ref>.</p><p>As with <ref type="bibr" target="#b3">[4]</ref>, we make a modification on convergent encryption such that the convergent key of file is generated and controlled by a secret "seed", such that any adversary could not directly derive the convergent key from the content of file and the dictionary attack is prevented.</p><p>This paper is organized as follows: In Section II, we review the related works on integrity auditing and secure deduplication. In Section III, we introduce some background including the bilinear maps and convergent encryption. Section IV and Section V respectively proposes the SecCloud and SecCoud + system. Section VI and Section VII respectively analyzes the security and efficiency of proposed systems. Finally Section VIII draws the conclusion of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Since our work is related to both integrity auditing and secure deduplication, we review the works in both areas in the following subsections, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Integrity Auditing</head><p>The definition of provable data possession (PDP) was introduced by Ateniese et al. <ref type="bibr">[5][6]</ref> for assuring that the cloud servers possess the target files without retrieving or downloading the whole data. Essentially, PDP is a probabilistic proof protocol by sampling a random set of blocks and asking the servers to prove that they exactly possess these blocks, and the verifier only maintaining a small amount of metadata is able to perform the integrity checking. After Ateniese et al.'s proposal <ref type="bibr" target="#b4">[5]</ref>, several works concerned on how to realize PDP on dynamic scenario: Ateniese et al. <ref type="bibr" target="#b6">[7]</ref> proposed a dynamic PDP schema but without insertion operation; Erway et al. <ref type="bibr" target="#b7">[8]</ref> improved Ateniese et al.'s work <ref type="bibr" target="#b6">[7]</ref> and supported insertion by introducing authenticated flip table ; A similar work has also been contributed in <ref type="bibr" target="#b8">[9]</ref>. Nevertheless, these proposals <ref type="bibr">[5][7]</ref>[8] <ref type="bibr" target="#b8">[9]</ref> suffer from the computational overhead for tag generation at the client. To fix this issue, Wang et al. <ref type="bibr" target="#b9">[10]</ref> proposed proxy PDP in public clouds. Zhu et al. <ref type="bibr" target="#b10">[11]</ref> proposed the cooperative PDP in multi-cloud storage.</p><p>Another line of work supporting integrity auditing is proof of retrievability (POR) <ref type="bibr" target="#b11">[12]</ref>. Compared with PDP, POR not merely assures the cloud servers possess the target files, but also guarantees their full recovery. In <ref type="bibr" target="#b11">[12]</ref>, clients apply erasure codes and generate authenticators for each block for verifiability and retrievability. In order to achieve efficient data dynamics, Wang et al. <ref type="bibr" target="#b12">[13]</ref> improved the POR model by manipulating the classic Merkle hash tree construction for block tag authentication. Xu and Chang <ref type="bibr" target="#b13">[14]</ref> proposed to improve the POR schema in <ref type="bibr" target="#b11">[12]</ref> with polynomial commitment for reducing communication cost. Stefanov et al. <ref type="bibr" target="#b14">[15]</ref> proposed a POR protocol over authenticated file system subject to frequent changes. Azraoui et al. <ref type="bibr" target="#b15">[16]</ref> combined the privacy-preserving word search algorithm with the insertion in data segments of randomly generated short bit sequences, and developed a new POR protocol. Li et al. <ref type="bibr" target="#b16">[17]</ref> considered a new cloud storage architecture with two independent cloud servers for integrity auditing to reduce the computation load at client side. Recently, Li et al. <ref type="bibr" target="#b17">[18]</ref> utilized the key-disperse paradigm to fix the issue of a significant number of convergent keys in convergent encryption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Secure Deduplication</head><p>Deduplication is a technique where the server stores only a single copy of each file, regardless of how many clients asked to store that file, such that the disk space of cloud servers as well as network bandwidth are saved. However, trivial client side deduplication leads to the leakage of side channel information. For example, a server telling a client that it need not send the file reveals that some other client has the exact same file, which could be sensitive information in some case.</p><p>In order to restrict the leakage of side channel information, Halevi et al. <ref type="bibr" target="#b2">[3]</ref> introduced the proof of ownership protocol which lets a client efficiently prove to a server that that the client exactly holds this file. Several proof of ownership protocols based on the Merkle hash tree are proposed <ref type="bibr" target="#b2">[3]</ref> to enable secure client-side deduplication. Pietro and Sorniotti <ref type="bibr" target="#b18">[19]</ref> proposed an efficient proof of ownership scheme by choosing the projection of a file onto some randomly selected bit-positions as the file proof.</p><p>Another line of work for secure deduplication focuses on the confidentiality of deduplicated data and considers to make deduplication on encrypted data. Ng et al. <ref type="bibr" target="#b19">[20]</ref> firstly introduced the private data deduplication as a complement of public data deduplication protocols of Halevi et al. <ref type="bibr" target="#b2">[3]</ref>. Convergent encryption <ref type="bibr" target="#b20">[21]</ref> is a promising cryptographic primitive for ensuring data privacy in deduplication. Bellare et al. <ref type="bibr" target="#b21">[22]</ref> formalized this primitive as message-locked encryption, and explored its application in space-efficient secure outsourced storage. Abadi et al. <ref type="bibr" target="#b22">[23]</ref> further strengthened Bellare et al's security definitions <ref type="bibr" target="#b21">[22]</ref> by considering plaintext distributions that may depend on the public parameters of the schemas. Regarding the practical implementation of convergent encryption for securing deduplication, Keelveedhi et al. <ref type="bibr" target="#b3">[4]</ref> designed the DupLESS system in which clients encrypt under file-based keys derived from a key server via an oblivious pseudorandom function protocol.</p><p>As stated before, all the works illustrated above considers either integrity auditing or deduplication, while in this paper, we attempt to solve both problems simultaneously. In addition, it is worthwhile noting that our work is also distinguished with <ref type="bibr" target="#b1">[2]</ref> which audits cloud data with deduplication, because we also consider to 1) outsource the computation of tag generation, 2) audit and deduplicate encrypted data in the proposed protocols. III. PRELIMINARY We now discuss some preliminary notions that will form the foundations of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bilinear Map and Computational Assumption</head><p>Definition 1 (Bilinear Map): Let G and G T be two cyclic multiplicative groups of large prime order p. A bilinear pairing is a map e : G × G → G T with the following properties:</p><p>• Bilinear: e(g a 1 , g b 2 ) = e(g 1 , g 2 ) ab for all g 1 , g 2 ∈ R G and a, b ∈ R Z p ; • Non-degenerate: There exists g 1 , g 2 ∈ G such that e(g 1 , g 2 ) ̸ = 1; • Computable: There exists efficient algorithm to compute e(g 1 , g 2 ) for all g 1 , g 2 ∈ R G. The examples of such groups can be found in supersingular elliptic curves or hyperelliptic curves over finite fields, and the bilinear pairings can be derived from the Weil or Tate pairings. For more details, see <ref type="bibr" target="#b23">[24]</ref>.</p><p>We then describe the Computational Diffie-Hellman problem, the hardness of which will be the basis of the security of our proposed schemes.</p><p>Definition 2 (CDH Problem): The Computational Diffie-Hellman problem is that, given g, g x , g y ∈ G 1 for unknown x, y ∈ Z * p , to compute g xy .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convergent Encryption</head><p>Convergent encryption <ref type="bibr" target="#b21">[22]</ref>[23] <ref type="bibr" target="#b20">[21]</ref> provides data confidentiality in deduplication. A user (or data owner) derives a convergent key from the data content and encrypts the data copy with the convergent key. In addition, the user derives a tag for the data copy, such that the tag will be used to detect duplicates. Here, we assume that the tag correctness property <ref type="bibr" target="#b21">[22]</ref> holds, i.e., if two data copies are the same, then their tags are the same. Formally, a convergent encryption scheme can be defined with four primitive functions:</p><p>• KeyGen(F) : The key generation algorithm takes a file content F as input and outputs the convergent key ck F of F; • Encrypt(ck F , F) : The encryption algorithm takes the convergent key ck F and file content F as input and outputs the ciphertext ct F ; • Decrypt(ck F , ct F ) : The decryption algorithm takes the convergent key ck F and ciphertext ct F as input and outputs the plain file F; • TagGen(F) : The tag generation algorithm takes a file content F as input and outputs the tag tag F of F. Notice that in this paper, we also allow TagGen(•) to generate the (same) tag from the corresponding ciphertext as with <ref type="bibr">[22][18]</ref>.</p><p>IV. SECCLOUD In this section, we describe our proposed SecCloud system. Specifically, we begin with giving the system model of Sec-Cloud as well as introducing the design goals for SecCloud. In what follows, we illustrate the proposed SecCloud in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Model</head><p>Aiming at allowing for auditable and deduplicated storage, we propose the SecCloud system. In the SecCloud system, we have three entities:</p><p>• Cloud Clients have large data files to be stored and rely on the cloud for data maintenance and computation.</p><p>They can be either individual consumers or commercial organizations; • Cloud Servers virtualize the resources according to the requirements of clients and expose them as storage pools. Typically, the cloud clients may buy or lease storage capacity from cloud servers, and store their individual data in these bought or rented spaces for future utilization; • Auditor which helps clients upload and audit their outsourced data maintains a MapReduce cloud and acts like a certificate authority. This assumption presumes that the auditor is associated with a pair of public and private keys. Its public key is made available to the other entities in the system. The SecCloud system supporting file-level deduplication includes the following three protocols respectively highlighted by red, blue and green in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>1) File Uploading Protocol: This protocol aims at allowing clients to upload files via the auditor. Specifically, the file uploading protocol includes three phases:</p><p>• Phase 1 (cloud client → cloud server): client performs the duplicate check with the cloud server to confirm if such a file is stored in cloud storage or not before uploading a file. If there is a duplicate, another protocol called Proof of Ownership will be run between the client and the cloud storage server. Otherwise, the following protocols (including phase 2 and phase 3) are run between these two entities. • Phase 2 (cloud client → auditor): client uploads files to the auditor, and receives a receipt from auditor. • Phase 3 (auditor → cloud server): auditor helps generate a set of tags for the uploading file, and send them along with this file to cloud server. 2) Integrity Auditing Protocol: It is an interactive protocol for integrity verification and allowed to be initialized by any entity except the cloud server. In this protocol, the cloud server plays the role of prover, while the auditor or client works as the verifier. This protocol includes two phases:</p><p>• Phase 1 (cloud client/auditor → cloud server): verifier (i.e., client or auditor) generates a set of challenges and sends them to the prover (i.e., cloud server). • Phase 2 (cloud server → cloud client/auditor): based on the stored files and file tags, prover (i.e., cloud server) tries to prove that it exactly owns the target file by sending the proof back to verifier (i.e., cloud client or auditor). At the end of this protocol, verifier outputs true if the integrity verification is passed.</p><p>3) Proof of Ownership Protocol: It is an interactive protocol initialized at the cloud server for verifying that the client exactly owns a claimed file. This protocol is typically triggered along with file uploading protocol to prevent the leakage of side channel information. On the contrast to integrity auditing protocol, in PoW the cloud server works as verifier, while the client plays the role of prover. This protocol also includes two phases</p><p>• Phase 1 (cloud server → client): cloud server generates a set of challenges and sends them to the client. • Phase 2 (client → cloud server): the client responds with the proof for file ownership, and cloud server finally verifies the validity of proof. Our main objectives are outlined as follows.</p><p>• Integrity Auditing. The first design goal of this work is to provide the capability of verifying correctness of the remotely stored data. The integrity verification further requires two features: 1) public verification, which allows anyone, not just the clients originally stored the file, to perform verification; 2) stateless verification, which is able to eliminate the need for state information maintenance at the verifier side between the actions of auditing and data storage. • Secure Deduplication. The second design goal of this work is secure deduplication. In other words, it requires that the cloud server is able to reduce the storage space by keeping only one copy of the same file. Notice that, regarding to secure deduplication, our objective is distinguished from previous work <ref type="bibr" target="#b2">[3]</ref> in that we propose a method for allowing both deduplication over files and tags. • Cost-Effective. The computational overhead for providing integrity auditing and secure deduplication should not represent a major additional cost to traditional cloud storage, nor should they alter the way either uploading or downloading operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SecCloud Details</head><p>In this subsection, we respectively describe the three protocols including file uploading protocol, integrity auditing protocol and proof of ownership protocol in SecCloud. Before our detailed elaboration, we firstly introduce the system setup phase of SecCloud, which initializes the public and private parameters of the system. </p><formula xml:id="formula_0">(ID F , F) ↵ 1 ↵ 2 ↵ 3 F F F 1 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Phase 1 in File Uploading Protocol</head><p>As declared in Section IV-A, the file uploading protocol involves three phases. In the first phase shown in Fig. <ref type="figure">2</ref>, the client runs the deduplication test by sending hash value of the file Hash(F) to the cloud server. If there is a duplicate, the cloud client performs Proof of Ownership protocol with the cloud server which will be described later. If it is passed, the user is authorized to access this stored file without uploading the file.</p><p>Otherwise (in the second phase), the cloud client uploads a file F as well as its identity ID F to the distributed file system in MapReduce auditing cloud, and simultaneously sends an "upload" request to the master node in MapReduce, which randomly picks {α i } n i=1 such that ∑ n i=1 α i = α and assigns the ith slave node with α i . When each slave node (say the ith salve node) receives the assignment α i , it does two steps: 1) Pick up (ID F , F) in the distributed file system in MapReduce, and build a Merkle hash tree on the blocks {B j } s j=1 of F. 2) Let h root denote the hash of the root node of Merkle hash tree built on F. This slave node uses α i to sign h root by computing τ i = h αi root . Finally, the signature τ i is sent to the the slave node which is specified by master node for executing the reducing procedure.</p><p>The specified slave node for reducing procedure gathers all the signatures {τ i } n i=1 from the other slave nodes, and computes τ = ∏ n i=1 τ i . The "reduced" signature τ is finally sent back to client as receipt of the storage of file F.</p><p>In the third phase, the MapReduce auditing cloud starts to upload the file F to cloud server. To allow public auditing, the master node builds file tags of F. Specifically, master node firstly writes and arranges all the sectors of F in a matrix (we say S), and computes a homographic signature for each row of the matrix S (highlighted red in Fig. <ref type="figure" target="#fig_1">3</ref>). Notice that the tag generation procedure also follows the computing paradigm with MapReduce. That is, for the ith (i = 1, 2, . . . , s) row of S, the jth (j = 1, 2, . . . , n) slave node computes</p><formula xml:id="formula_1">σ ij = [Hash(ID F ||B i ) ∏ t k=1 u B ik k ] αj , where</formula><p>∑ n j=1 α j = α. Accordingly, all the signatures {σ ij } n j=1 are then multiplied into the homomorphic signature σ i = ∏ n j=1 σ ij at a specified reducing slave node. The homomorphic signature allows us to in future aggregate the signatures signed on the sectors in the same column of S using multiplication. Finally, the master node uploads (ID, F, {σ i } s i=1 ) to cloud server. 2) Integrity Auditing Protocol: In the integrity auditing protocol, either the MapReduce auditing cloud or the client works as the verifier. Thus, without loss of generality, in the rest of the description of this protocol, we use verifier to identify the client or MapReduce auditing cloud. The auditing protocol is designed in a challenge-response model. Specifically, the verifier randomly picks a set of block identifiers (say I F ) of F and asks the cloud server (working as prover) to response the blocks corresponding to the identifiers in I F . In order to keep randomness in each time of challenge, even for the same I F , we introduce a random coefficient for each block in challenge. That is, for each identifier i ∈ I F , the coefficient c i for the block identified by i is computed as c i = f (tm||ID F ||i), where f (•) is a pseudorandom function and tm is the current time period. Finally, C = {(i, c i )} i∈IF is sent to cloud server for challenge.  Upon receiving the challenge C, as shown in Fig. <ref type="figure" target="#fig_1">3</ref>, the cloud server writes the sectors of F in matrix S, as the way of tag generation, and extracts all the rows in S involved in I F . Without loss of generality, we denote the extracted matrix as S, which is [B mn ] 3≤m≤i,1≤n≤t in the example of Fig.</p><formula xml:id="formula_2">B</formula><p>Then, for each column j of S, compute the coefficient affected sectors (in column) ω = ∑ i∈IF c i B ij . Similarly, the cloud server also computes the (coefficient affected) aggregated homomorphic signature µ = ∏ i∈I F σ ci i . Notice that the (coefficient affected) matrix S could be re-constructed using either the (coefficient affected) sectors {ω j } t j=1 or the aggregated homomorphic signature µ, which allows the cloud server to prove retrievability on sector-level.</p><p>In addition, the cloud server also attempts to prove retrievability on block-level, through using Merkle hash tree. Recall that, a Merkle hash tree has been constructed on F by the MapReduce auditing cloud in file uploading to generate the receipt on F for future auditing. In the auditing protocol, the cloud server is required to construct the same Merkle hash tree on F and makes response based on the constructed Merkle tree. Without loss of generality, we denote Path(B i ) as the set of nodes from the leaf node identified by B i to the root node of Merkle tree, and Sibl(B i ) is the set of sibling nodes of each node in Path(B i ). Then, for each i ∈ I F , the cloud server computes a pair (Hash(B i ), Ω i ), where Hash(B i ) is the hash value of the i-th block of F and Ω i = Sibl(B i )\ ∪ j∈IF Path(B j ) includes the necessary auxiliary information for reconstructing the root node using {B i } i∈IF . For example, suppose the Merkle hash tree has been constructed as in Fig. <ref type="figure" target="#fig_2">4</ref>, and the challenge blocks I F = {2, 5} (i.e., challenge B 2 , B 5 ). The hashes of B 2 and B 5 (highlighted by black in Fig. <ref type="figure" target="#fig_2">4</ref>), Ω 2 (highlighted by blue in Fig. <ref type="figure" target="#fig_2">4</ref>) and Ω 5 (highlighted by orange in Fig. <ref type="figure" target="#fig_2">4</ref>) are as the proof for retrievability on block-level. It is worth noting that, although the node labeled by x in Fig. <ref type="figure" target="#fig_2">4</ref> is a sibling of node in Path(B 2 ), it should not be included in Ω 2 . This is because the node x also belongs to Path(B 5 ) and can be re-constructed using Hash(B 5 ) and Ω 5 . The benefit of excluding the nodes in other challenge blocks paths is that, it allows us to reconstruct only a single version of root node of the Merkle hash tree for auditing all the challenge blocks.</p><formula xml:id="formula_3">B 1 B 2 B 3 B 4 B 5 B 6 B 7 B 8 ⌦ 2 ⌦ 5</formula><p>x Cloud server sends (µ, {ω j } t j=1 , {(Hash(B i ), Ω i )} i∈IF ) as proof back to verifier proving the existence of file F. The verifier makes the following two types of verifications:</p><p>• Block-Level Auditing. In the block-level verification, the verifier reconstructs the root node (say R) of Merkle hash tree using {(Hash(B i ), Ω i )} i∈IF (a reconstruction example is highlighted by red arrow in Fig. <ref type="figure" target="#fig_2">4</ref>), and then checks the validity of the published signatures. Specifically, the verifier verifies the signature by checking e(g, τ )</p><p>?</p><p>= e(Hash(R), g α ).</p><p>• Sector-Level Auditing. Recall that to generate the tags of file F, we have computed the aggregated sector signatures σ i in terms of row, and to generate the proof of the existence of file F, we have computed the (coefficient affected) sectors ω i in terms of column. For sector-level auditing, our aim is to re-construct aggregated signature on the (coefficient affected) matrix S respectively in terms of row and column, and check the equality of both reconstructions. Intuited by this, the equality to be checked is as e(µ, g)</p><formula xml:id="formula_4">? = e( ∏ i∈IF [Hash(ID F ||B i ) ci ∏ t k=1 u ω k k ], g α )</formula><p>where the left part is computed following row while the right part follows a column computation.</p><p>3) Proof of Ownership Protocol: The PoW protocol aims at allowing secure deduplication at cloud server. Specifically, in deduplication, a client claims that he/she has a file F and wants to store it at the cloud server, where F is an existing file having been stored on the server. The cloud server asks for the proof of the ownership of F to prevent client unauthorized or malicious access to an unowned file through making cheating claim. In SecCloud, the PoW protocol is similar to <ref type="bibr" target="#b2">[3]</ref> and the details are described as follows.</p><p>Suppose the cloud server wants to ask for the ownership proof for file F. It randomly picks a set of block identifiers, say I F ⊆ {1, 2, . . . , s} where s is the number of blocks in F, for challenge. Upon receiving the challenge set I F , the client first a short value and constructs a Merkle tree. Note that only sibling-paths of all the leaves with challenged identifiers are returned back to the cloud server, who can easily verify the correctness by only using the root of the Merkle tree. If it is passed, the user is authorized to access this stored file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SECCLOUD +</head><p>We specify that our proposed SecCloud system has achieved both integrity auditing and file deduplication. However, it cannot prevent the cloud servers from knowing the content of files having been stored. In other words, the functionalities of integrity auditing and secure deduplication are only imposed on plain files. In this section, we propose SecCloud + , which allows for integrity auditing and deduplication on encrypted files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Model</head><p>Compared with SecCloud, our proposed SecCloud + involves an additional trusted entity, namely key server, which is responsible for assigning clients with secret key (according to the file content) for encrypting files. This architecture is in line with the recent work <ref type="bibr" target="#b3">[4]</ref>. But our work is distinguished with the previous work <ref type="bibr" target="#b3">[4]</ref> by allowing for integrity auditing on encrypted data.</p><p>SecCloud + follows the same three protocols (i.e., the file uploading protocol, the integrity auditing protocol and the proof of ownership protocol) as with SecCloud. The only difference is the file uploading protocol in SecCloud + involves an additional phase for communication between cloud client and key server. That is, the client needs to communicate with the key server to get the convergent key for encrypting the uploading file before the phase 2 in SecCloud.</p><p>Unlike SecCloud, another design goals of file confidentiality is desired in SecCloud + as follows.</p><p>• File Confidentiality. The design goal of file confidentiality requires to prevent the cloud servers from accessing the content of files. Specially, we require that the goal of file confidentiality needs to be resistant to "dictionary attack". That is, even the adversaries have pre-knowledge of the "dictionary" which includes all the possible files, they still cannot recover the target file <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SecCloud + Details</head><p>We introduce the system setup phase of SecCloud + as follows.</p><p>• System Setup. As with SecCloud, the auditor initializes the public key pk = (g α , {u i } t i=1 ) and private key sk = α, where g, u 1 , u 2 , . . . , u t ∈ R G. In addition, to preserve the confidentiality of files, initially, the key server picks a random key ks for further generating file encryption keys, and each client is assigned with a secret key ck for encapsulating file encryption keys. Based on the initialized parameters, we then respectively describe the three protocols involved in SecCloud + .</p><p>1) File Uploading Protocol: Suppose the uploading file F has s blocks, say B 1 , B 2 , . . . , B s , and each block B i for i = 1, 2, . . . , s contains t sectors, say B i1 , B i2 , . . . , B it .</p><p>Client computes h F = Hash(F) by itself. In addition, for each sector B ij of F where i = 1, 2, . . . , s and j = 1, 2, . . . , t, client computes its hash h Bij = Hash(B ij ). Finally (h F , {h Bij } i=1,...,s,j=1,...,t ) is sent to key server for generating the convergent keys for F.</p><p>Upon receiving the hashes, the key server computes ssk F = f (ks, h F ) and ssk ij = f (ks, h Bij ) for i = 1, . . . , s and j = 1, . . . , t, where ks is the convergent key seed kept at the key server, and f (•) is a pseudorandom function. It is worthwhile noting that, 1) We take advantage of the idea of convergent encryption <ref type="bibr" target="#b20">[21]</ref>[22] <ref type="bibr" target="#b22">[23]</ref> to make the deterministic and "content identified" encryption, in which each "content" (file or sector) is encrypted using the session key derived from itself. In this way, different "contents" would result in different ciphertexts, and deduplication works. 2) Convergent encryption suffers from dictionary attack, which allows the adversary to recover the whole content with a number of guesses. To prevent such attack, as with <ref type="bibr" target="#b3">[4]</ref>, a "seed" (i.e., convergent key seed) is used for controlling and generating all the convergent keys to avoid the fact that adversary could guess or derive the convergent key just from the content itself. 3) We generate convergent keys on sector-level (i.e., generate convergent keys for each sector in file F), to enable integrity auditing. Specifically, since convergent encryption is deterministic, it allows to compute homomorphic signatures on (convergent) encrypted data as with on plain data, and thus the sector-level integrity auditing is preserved.</p><p>Client then continues to encrypt F sector by sector and uploads the ciphertext to auditor. Specifically, for each sector B ij of F, i = 1, 2, . . . , s and j = 1, 2, . . . , t, client computes ct Bij = Enc(ssk Bij , B ij ), and sends (ID F , {ct Bij } i=1,...,s,j=1,...,t ) to auditor, where Enc(•) is the symmetric encryption algorithm. The convergent keys ssk ij are encapsulated by client's secret key ck and directly stored at the cloud servers.</p><p>The auditor does almost the same thing as that in Sec-Coud. Firstly, he computes the hash of ciphertext {ct Bij } and sends it to the cloud storage server for duplicate check. If there is a duplicate stored in the cloud server, the auditor performs a PoW and the details are described in the Proof of Ownership protocol. If it is passed, the user is authorized to access this stored file. (Actually, the auditor can perform the duplicate check at local by storing the hash of each file that clients uploaded. In this way, no encryption operation is required if there is duplicate.) Otherwise, upon receiving (ID F , {ct Bij } i=1,...,s,j=1,...,t ), the auditor takes advantage of MapReduce cloud to build Merkle hash tree on encrypted blocks [ct B1 , ct B2 , . . . , ct Bs ] where ct Bi = [ct Bi1 , . . . , ct Bit ] and i = 1, 2, . . . , s and compute τ = h α root . Notice that, unlike the description of SecCloud, the notation h root is abused to denote the root hash of Merkle tree built on encrypted blocks. Then τ is returned to client as receipt of the storage of file F. In addition, all the sector ciphertexts are written in a matrix similar to Fig. <ref type="figure" target="#fig_1">3</ref> (but the plain sector B ij is replaced by its corresponding ciphertext ct Bij ), and the auditor computes</p><formula xml:id="formula_5">σ i = [Hash(ID F ||i) ∏ t j=1 u cB ij j</formula><p>] α with MapReduce cloud for each i = 1, 2, . . . , s. Finally, the auditor uploads (ID F , {ct Bij } i=1,...,s,j=1,...,t , {σ i } s i=1 ) to the cloud servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Integrity Auditing Protocol:</head><p>The integrity auditing protocol works in the same way of that in SecCloud, but imposed on encrypted data. Specifically, the verifier (could be either the client or the auditor) submits a set of pairs {(i, c i )} i∈I F where I F ⊆ {1, 2, . . . , s} and c i ∈ R Z. Upon receiving {(i, c i )} i∈IF , the cloud servers then computes ω j = ∑ i∈IF c i ct Bij for each j = 1, 2, . . . , t, as well as the aggregated homomorphic signature µ = ∏ i∈I F σ ci i . In addition, the cloud server constructs a Merkle hash tree on encrypted blocks ct Bi of F and attempts to prove retrievability at block-level. Precisely, for each i ∈ I F , the cloud server computes a pair (Hash(ct Bi ), Ω i ), where ct Bi = [ct Bi1 , . . . , ct Bit ] and Ω i includes the necessary auxiliary information for reconstructing the root node using {ct Bi } i∈IF . Finally (µ, {ω j } t j=1 , {(Hash(ct Bi ), Ω i )} i∈IF ) is sent to verifier for auditing.</p><p>The verifier makes two-level auditing: 1) On the block level, the verifier reconstructs the root node of Merkle tree using {(Hash(ct Bi ), Ω i )} i∈IF and verifies e(g, τ ) ? = e(Hash(R), g α ). 2) On the sector level, the verifier checks e(µ, g)</p><formula xml:id="formula_6">? = e( ∏ i∈IF [Hash(ID F ||i) ci ∏ t i=1 u ωi i ], g α ).</formula><p>3) Proof of Ownership Protocol: Suppose a client claims that he/she has a file F and wants to store it at the cloud server, where F is an existing file having been stored on the server. The client needs to show the proof that he owns the same file at local. The user performs the proof of ownership in a similar way as <ref type="bibr" target="#b2">[3]</ref> based on the encrypted file. If it is passed, a pointer will be provided to the client for the access to the same file stored in the cloud server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SECURITY ANALYSIS</head><p>In this section, we attempt to analyze the security of our proposed both schemes. Before this, we firstly formalize the security definitions our schemes aim at capturing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Security Definitions</head><p>Based on the paradigm of SecCloud and SecCloud + , we define the security definitions, adapting to the integrity auditing and secure deduplication goals. Our both definitions capture the philosophy of game-based definition. Specifically, we define two games respectively for integrity auditing and secure deduplication, and both of the games are played by two players, namely adversary and challenger. The adversary (the role of which is worked by semi-honest cloud server and cloud client respectively in integrity auditing and secure deduplication definition) is trying to achieve the goal condition explicitly specified in the game. Having this intuition, we give our security definitions as follows.</p><p>1) Integrity Auditing: An integrity auditing protocol is sound if any cheating cloud server that convinces the verifier that it is storing a file F is actually storing this file. To capture this spirit, we define its game based on Proof of Retrievability (PoR).</p><p>The security model called Proof of Retrievability (PoR) was introduced by Shacham and Waters' in <ref type="bibr" target="#b11">[12]</ref>. This security model captures the requirement for integrity auditing, whose basic security goal is to achieve proof of retrievability. In more details, in this security model, if there exists an adversary who can forge and generate any valid integrity proofs for any file F with a non-negligible probability, another simulator can be constructed who is able to extract F with overwhelming probability. The formal definition for the above model can be given by the following game between a challenger and an adversary A. Note that in the following security game, the challenger plays the role of auditing server while the adversary A acts as the storage server.</p><p>• Setup Phase. The challenger runs the setup algorithm with required security parameter and other public parameter as input. Then, it generates the public and secret key pair (pk, sk). The public key pk is forwarded to the adversary A. • Query phase. The adversary is allowed to query the file upload oracle for any file F. Then, the file with the correct tags are generated and uploaded to the cloud storage server. These tags can be publicly verified with respect to the public key pk. • Challenge Phase. A can adaptively send file F to the file tag tag comes, C runs the integrity verification protocol IntegrityVerify{A C(pk, tag)} with A. • Forgery. A outputs a file tag tag ′ and the description of a prover P t .</p><p>We say that a prover P t on tag ′ is β-admissible, if the following two conditions hold:</p><p>(1) tag ′ is a file tag output by a previous upload query. <ref type="bibr" target="#b1">(2)</ref> </p><formula xml:id="formula_7">Pr[IntegrityVerify{P t C(pk, tag ′ )} = 1] ≥ β.</formula><p>Then we can define the soundness of PoR scheme. Definition 3: (Proof of Retrievability) A PoR scheme is (β, γ)-sound if for any β-admissible prover P t output by A in the above game, there exists an extractor E that can recover the original file of tag tag with probability at least 1 -γ.</p><p>2) Secure Deduplication: Similarly, we can also define a game between challenger and adversary for secure deduplication below. Notice that the game for secure deduplication captures the intuition of allowing the malicious client to claim it has a challenge file F through colluding with all the other clients not owning this file.</p><p>• Setup Phase. A challenge file F with fixed length and minimum entropy (specified in system parameter) is randomly picked and given to the challenger. The challenger continues to run a summary algorithm and generate a summary sum F . • Learning Phase. Adversary F can setup arbitrarily many client accomplices not exactly having F and have them to interact with the cloud servers to try to prove the ownership of file F. Notice that in the learning phase, the cloud server plays as the honest verifier with input sum sum F and the accomplices could follow any arbitrary protocol set by A. • Challenge Phase. The exact proof of ownership protocol is executed. Specifically, the challenger outputs a challenge to A and A responses with a proof based on its learnt knowledge. If A's proof is accepted by the cloud server, we say A succeeds. The security in terms of secure deduplication is achieved, if for all probabilistic polynomial-time adversaries A, the probability that A succeeds in the above experiment is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Security Proof</head><p>Theorem 1: Assume that the CDH problem is a hard problem. Then, the proposed public-verifiable PoR scheme satisfies the soundness. That is, no adversary could generate an integrity proof for any file such that the verifier accepts it with nonnegligible probability.</p><p>Proof: We prove the soundness of the construction by reduction. Firstly, assume there is an adversary who can break the soundness with non-negligible probability. We show that how to construct a simulator to break the computational Diffie-Hellman problem through interacting with the adversary. During this phase, the simulator is required to answer all the queries as the real application.</p><p>In more details, the simulator has to answer the tag generation and integrity proof queries from the adversary. After the simulation, if the adversary outputs a valid tag that is not from client, the simulator can use this algorithm to solve the CDH problem. Notice that the simulation for the n slave nodes can be reduced to just one node because of the assumption that all the slave nodes are honest-but-curious and they will not collude. More clearly, the master key α can be split to n subkeys by choosing n -1 random values and assigned to slave nodes as the corresponding private keys, while the n-th node is assigned the key of α minus the sum of these random values.</p><p>Suppose that there exists an adversary who can generate the correct description of a prover. Denote F = (B 1 , . . . , B t ) as the file for integrity verification, Φ = {σ i } 1≤i≤t as the signatures of blocks, and the set Q = {(i, c i )} i∈IF as the query. Denote by R the root generated from the file F. If the adversary can generate a correct root R from F which passes the verification for a different file F ′ , it implies a collision of hash function used in the construction of Merkle Hash Tree. Based on the assumption of the collusion-free, this happens on with negligible probability.</p><p>To construct a simulator, that given g, g = g α , h, where α is unknown, outputs h α . In the setup phase, the simulator sets v as g, chooses two vectors of randomness β 1 , . . . , β t ∈ Z p and γ 1 , . . . , γ t ∈ Z p , and sets u j = g βj h γj for j = 1, . . . , t. It additionally initiates an empty hash tables H-table and simulates the random oracle queries as follows.</p><p>When a hash query of B i comes and an entry (B i , r i ) exists in the hash-table for some random value r i , the simulator just returns g r i . When a query of a new B i that has not been queried, the simulator performs the following steps. Firstly, it randomly chooses a value r i from Z p and puts (B i , r i ) in the Hash table H-table and returns Hash(M i ) = g ri .</p><p>The simulator also needs to simulate the Uploading Oracle. Specifically, for a query of file F to be uploaded, the simulator computes the hash values and constructs Merkle Hash Tree root R from the file. The proof is very similar to <ref type="bibr" target="#b16">[17]</ref> and omitted here. The adversary can also start the query for the integrity proof. When an oracle query of a file tag, the simulator just starts an honest protocol with the adversary for the simulation.</p><p>After the above simulation, the adversary outputs a forgery of a valid signature σ ′ ̸ = σ satisfying the verification. Similar to the security analysis in <ref type="bibr" target="#b16">[17]</ref>, the simulator can compute and get the value</p><formula xml:id="formula_8">h α = (σ ′ σ -1 v - ∑ s j=1 βj ∆µj ) 1 ∑ t j=1</formula><p>γ j ∆µ j as the solution to the given CDH instance.</p><p>Theorem 2: An extractor can be constructed to recover the file in time O(n 2 (s + 1) + (1 + βn 2 )n/ω) for well behaved β-admissible prover by running O(n/ω) interactions on a nblock file with ω = β -1/p -(ρn/n -c + 1) c .</p><p>Actually, such an extractor can be constructed to get correct proof for the verification queries in the protocol. With the combinatorial techniques, we can easily get the result that a ρ-fraction of encoded file blocks can be retrieved after at most O(n/ω) interactions. Based on the rate-ρ error correcting codes, all the file blocks are able to be recovered. The security model for the integrity verification protocol is the same as in Shacham and Waters' PoR model. Thus, the simulation for extracting the original file is similar to that in <ref type="bibr" target="#b11">[12]</ref> <ref type="bibr" target="#b16">[17]</ref>, which is omitted here.</p><p>By combining Theorem 1 and Theorem 2, we can directly have the following theorem.</p><p>Theorem 3: The proposed PoR construction is (β, γ)-sound for any β-admissible prover where γ</p><formula xml:id="formula_9">= 1 -(1 -1/p) logn+1 + 1/p.</formula><p>Regarding the file confidentiality of SecCloud + , we have the following theorem.</p><p>Theorem 4: The proposed SecCloud + achieves confidentiality of file with the assumption that the adversary is not allowed to collude with the key server.</p><p>Proof: In our construction, a key server is introduced to generate the convergent key and hash values for the duplicate check. Without the private key stored at the key server, no adversary can generate a valid convergent key for any file with non-negligible probability. Thus, for the cloud storage server, without the help of key server, it cannot launch the brute force attack because the underlying hash value over the file is a valid message authentication code.</p><p>Furthermore, all the data has been encrypted before they are outsourced. The data is encrypted with the traditional symmetric encryption scheme and the key is generated by the key server. The convergent key is encrypted by another master key and stored in the cloud server. The convergent key has been computed from both the file and private key of the key server, which means that the convergent key is not deterministic only in terms of the file. Even if the file is predictable, the adversary cannot guess the file with brute-force attack if the adversary is not allowed to collude with the key server.</p><p>Because we used the PoW technique, based on the assumption of secure PoW scheme, any adversary without the file cannot convince the cloud storage server to get the corresponding access privilege. Thus, our deduplication system is secure in terms of the security model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. PERFORMANCE ANALYSIS</head><p>In this section, we will provide a thorough experimental evaluation of our proposed schemes. We build our testbed by using 64-bit t2.Micro Linux servers in Amazon EC2 platform as the auditing server and storage server. In order to achieve λ = 80 bit security, the prime order p of the bilinear group G and G T are respectively chosen as 160 and 512 bits in length. We also set the block size as 4 KB and each block includes 25 sectors. Fig. <ref type="figure" target="#fig_3">5</ref> shows the time cost of slave node in MapReduce for generating file tags. It is clear the time cost of slave node is growing with the size of file. This is because the more blocks in file, the more homomorphic signatures are needed to be computed by slave node for file uploading. We also need to notice that there does not exist much computational load difference between common slave nodes and the reducer. Compared with the common slave nodes, reducer only additionally involves in a number of multiplications, which is lightweight operation. It is worthwhile noting that, the procedure of tag generation (the phase 2 and 3 in file uploading protocol) could be handled in preprocessing, and it is not necessary for client to wait until uploading file. Before examine the time cost of file auditing, we need to firstly make analysis and identify the number of challenging blocks (i.e., |I F |) in our integrity auditing protocol. According to <ref type="bibr" target="#b4">[5]</ref>, if ρ fraction of the file is corrupted, through asking the proof of a constant m blocks of this file, the verifier can detect the misbehavior with probability α = 1 -(1 -ρ) m . To capture the spirit of probabilistic auditing, we set the probability confidence α = 70%, 85% and 99%, and draw the relationships between ρ and m in Fig. <ref type="figure" target="#fig_4">6</ref>. It demonstrates that if we want to achieve low (i.e., 70%), medium (i.e., 85%) and high (i.e., 99%) confidence of detecting any small fraction of corruption, we have to respectively ask for 130, 190 and 460 blocks for challenge.  Now, we come back to evaluate the time cost of file auditing in Fig. <ref type="figure" target="#fig_6">7</ref>, which shows the time cost of auditing for detecting the misbehavior of cloud storage respectively with 70%, 85% and 99% confidence. Obviously, as the growth of the number of blocks for challenge (to guarantee higher confidence), the time cost for response from cloud storage server is increasing. This is because it needs to compute all the exponentiations for each challenge block as well as the coefficient for each column of S. Correspondingly, the time cost at auditor grows with the number of challenge blocks as well. But compared with cloud storage, the rate is slightly lower, because auditor only needs to aggregate the homomorphic signature of the challenged blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Aiming at achieving both data integrity and deduplication in cloud, we propose SecCloud and SecCloud + . SecCloud introduces an auditing entity with maintenance of a MapReduce cloud, which helps clients generate data tags before uploading as well as audit the integrity of data having been stored in cloud. In addition, SecCoud enables secure deduplication through introducing a Proof of Ownership protocol and preventing the leakage of side channel information in data deduplication. Compared with previous work, the computation by user in SecCloud is greatly reduced during the file uploading and auditing phases. SecCloud + is an advanced construction motivated by the fact that customers always want to encrypt their data before uploading, and allows for integrity auditing and secure deduplication directly on encrypted data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. SecCloud Architecture</figDesc><graphic coords="3,328.05,52.71,234.41,118.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Matrix for Proof of Retrievability</figDesc><graphic coords="5,98.17,387.91,156.12,58.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Auxiliary Information in Merkle Hash Tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Tag Generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Number of Challenging Blocks with Fixed Confidence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. File Auditing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>System Setup. The auditor working as an authority picks a random integer α ∈ R Z p as well as random elements g, u 1 , u 2 , . . . u t ∈ R G, where t specifies the maximum number of sectors in a file block. The secret key sk is set to be α and kept secret, while the public key pk = (g α , {u i } t i=1 ) is published to other entities. 1) File Uploading Protocol: Based on the public and private parameters generated in system setup, we then describe the file uploading protocol. Suppose the uploading file F has s blocks: B 1 , B 2 , . . . , B s , and each block B i for i = 1, 2, . . . , s contains t sectors: B i1 , B i2 , . . . , B it . Let n be the number of slave nodes in the MapReduce cloud.</figDesc><table><row><cell></cell><cell></cell><cell>Distributed File System</cell><cell></cell></row><row><cell>Client</cell><cell>Slave Node</cell><cell>Slave Node</cell><cell>Slave Node</cell></row><row><cell>request</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Master Node</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2015.2389960, IEEE Transactions on Computers</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by National Natural Science Foundation of China (No.61100224 and No. 61472091), NSFC-Guangdong (U1135002) and Natural Science Foundation of Guangdong Province (Grant No. S2013010013671).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A view of cloud computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Griffith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication of the ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="50" to="58" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Secure and constant cost public cloud storage auditing with deduplication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Communications and Network Security (CNS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Proofs of ownership in remote storage systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Halevi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pinkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shulman-Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Computer and Communications Security</title>
		<meeting>the 18th ACM Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dupless: Serveraided encryption for deduplicated storage</title>
		<author>
			<persName><forename type="first">S</forename><surname>Keelveedhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/usenixsecurity13/technical-sessions/presentation/bellare" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd USENIX Conference on Security, ser. SEC&apos;13</title>
		<meeting>the 22Nd USENIX Conference on Security, ser. SEC&apos;13<address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="179" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Provable data possession at untrusted stores</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Curtmola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kissner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM Conference on Computer and Communications Security, ser. CCS &apos;07</title>
		<meeting>the 14th ACM Conference on Computer and Communications Security, ser. CCS &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="598" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remote data checking using provable data possession</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Curtmola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kissner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst. Secur</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable and efficient provable data possession</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Di Pietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsudik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Security and Privacy in Communication Netowrks, ser. SecureComm &apos;08</title>
		<meeting>the 4th International Conference on Security and Privacy in Communication Netowrks, ser. SecureComm &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic provable data possession</title>
		<author>
			<persName><forename type="first">C</forename><surname>Erway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Küpc ¸ü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papamanthou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tamassia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM Conference on Computer and Communications Security, ser. CCS &apos;09</title>
		<meeting>the 16th ACM Conference on Computer and Communications Security, ser. CCS &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient remote data possession checking in critical information infrastructures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sebé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domingo-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez-Balleste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deswarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Quisquater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1034" to="1038" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Proxy provable data possession in public clouds</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="559" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cooperative provable data possession for integrity verification in multicloud storage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2231" to="2244" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compact proofs of retrievability</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on the Theory and Application of Cryptology and Information Security: Advances in Cryptology, ser. ASIACRYPT &apos;08</title>
		<meeting>the 14th International Conference on the Theory and Application of Cryptology and Information Security: Advances in Cryptology, ser. ASIACRYPT &apos;08<address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="90" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enabling public verifiability and data dynamics for storage security in cloud computing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Security -ESORICS 2009</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ning</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5789</biblScope>
			<biblScope unit="page" from="355" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards efficient proofs of retrievability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM Symposium on Information, Computer and Communications Security, ser. ASIACCS &apos;12</title>
		<meeting>the 7th ACM Symposium on Information, Computer and Communications Security, ser. ASIACCS &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="79" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iris: A scalable cloud file system with efficient integrity checks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Stefanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Computer Security Applications Conference, ser. ACSAC &apos;12</title>
		<meeting>the 28th Annual Computer Security Applications Conference, ser. ACSAC &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stealthguard: Proofs of retrievability with hidden watchdogs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Azraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elkhiyaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Önen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Security -ESORICS 2014, ser. Lecture Notes in Computer</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Science</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kutyłowski</surname></persName>
		</editor>
		<editor>
			<persName><surname>Vaidya</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8712</biblScope>
			<biblScope unit="page" from="239" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An efficient proof of retrievability with public auditing in cloud computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Intelligent Networking and Collaborative Systems (INCoS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Secure deduplication with efficient and reliable convergent key management</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1615" to="1625" />
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Boosting efficiency and security in proof of ownership for deduplication</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Pietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorniotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM Symposium on Information, Computer and Communications Security, ser. ASIACCS &apos;12</title>
		<meeting>the 7th ACM Symposium on Information, Computer and Communications Security, ser. ASIACCS &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="81" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Private data deduplication protocols in cloud storage</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual ACM Symposium on Applied Computing, ser. SAC &apos;12</title>
		<meeting>the 27th Annual ACM Symposium on Applied Computing, ser. SAC &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="441" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reclaiming space from duplicate files in a serverless distributed file system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Douceur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bolosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Conference on Distributed Computing Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Message-locked encryption and secure deduplication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keelveedhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -EURO-CRYPT 2013, ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Johansson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7881</biblScope>
			<biblScope unit="page" from="296" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Message-locked encryption for lock-dependent messages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Segev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO 2013, ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Canetti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Garay</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8042</biblScope>
			<biblScope unit="page" from="374" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Identity-based encryption from the weil pairing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kilian</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">2139</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
