<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Class-specific Variable Selection for Multicategory Support Vector Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Class-specific Variable Selection for Multicategory Support Vector Machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fusion penalty</term>
					<term>Lasso</term>
					<term>Support vector machine</term>
					<term>Variable selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a class-specific variable selection method for multicategory support vector machines (MSVMs). Different from existing variable selection methods for MSVMs, the proposed method not only captures the important variables for classification, but also identifies the discriminable and non discriminable classes so as to enhance the interpretation for multicategory classification problems. Specifically, it minimizes the hinge loss of MSVMs followed by a pairwise fusion penalty. For each variable, this penalty identifies nondiscriminable classes by imposing their associated coefficients in the decision functions to some identical value. Several simulated and real examples demonstrate that the proposed method provides better interpretation through class-specific variable selection while preserving comparable prediction performance with other MSVM methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classification, regression and density function estimation are three canonical problems in machine learning and pattern recognition. In a classification problem, there is a training data set and a test data set. All samples in the training data set are accompanied with the class labels indicating their class membership. The class labels of the samples in the test data set are unobserved. The task of the classification problem is to learn a discrimination rule from the training data and use it to predict the class labels of the test data. In the past decade, support vector machines (SVMs) gained a high degree of attention due to their outstanding prediction performance in real data analysis. The original SVM was proposed by <ref type="bibr" target="#b12">Vapnik (1995)</ref> based on the statistical learning theory. <ref type="bibr" target="#b12">Vapnik (1995)</ref> also introduced the kernel trick to make SVM have good performance for nonlinear classification problems.</p><p>The original SVM was designed for a binary classification problem and it was extended to multicategory classification problems in different ways <ref type="bibr" target="#b12">(Vapnik, 1998;</ref><ref type="bibr" target="#b16">Weston and Watkins, 1999;</ref><ref type="bibr" target="#b2">Crammer and Singer, 2001;</ref><ref type="bibr" target="#b7">Lee et al., 2004;</ref><ref type="bibr" target="#b8">Liu and Shen, 2006;</ref><ref type="bibr" target="#b17">Wu and Liu, 2007)</ref>.</p><p>The class-specific variable selection method proposed in this paper is based on the MSVM framework proposed by <ref type="bibr" target="#b7">Lee et al. (2004)</ref>. To clarify the notation, we use SVMs to represent binary SVMs and use MSVMs to represent multicategory SVMs.</p><p>Besides the emphasis of prediction performance, in recent years researchers have paid more and more attentions to the interpretability of SVMs. One challenging task of interpretation is how to select the most informative variables for SVM classification. <ref type="bibr" target="#b0">Bradley and Mangasarian (1998)</ref> reformulated the standard binary SVM problem into a "loss+penalty" form and demonstrated that the utility of the ? 1 penalty can effectively select significant variables by shrinking the small and redundant coefficients to zero. <ref type="bibr" target="#b20">Zhu et al. (2004)</ref> provided an efficient algorithm to compute the entire solution path for the ? 1 SVM. Under the same framework, other forms of penalties were also studied, such as the ? 0 penalty <ref type="bibr" target="#b15">(Weston et al., 2003)</ref>, the ? q penalty <ref type="bibr">(Liu et al., 2007)</ref>, the combination of ? 0 and ? 1 penalties <ref type="bibr" target="#b9">(Liu and Wu, 2007)</ref>, the elasticnet penalty <ref type="bibr" target="#b14">(Wang et al., 2006)</ref>, the SCAD penalty <ref type="bibr" target="#b18">(Zhang et al., 2006)</ref> and the F ? -norm penalty <ref type="bibr" target="#b21">(Zou and Yuan, 2008)</ref>. Variable selection for MSVMs is more complex since we need to estimate many decision functions each of which has it own important variables. <ref type="bibr" target="#b13">Wang and Shen (2007)</ref> selected informative variables by replacing the ? 2 penalty in the standard MSVM <ref type="bibr" target="#b7">(Lee et al., 2004)</ref> with an ? 1 penalty. Thereafter, <ref type="bibr" target="#b19">Zhang et al. (2008)</ref> proposed a supnorm penalty for MSVM. This penalty shrinks all coefficients associated with the same variable simultaneously and hence it tends to produce more sparse solutions than the ? 1 MSVM.</p><p>All existing variable selection methods for MSVMs select informative variables in a "onein-all-out" manner; that is, a variable is selected if it is important for at least one pair of classes and removed only if it is unimportant for all classes. However, in many practical situations, one may be interested in identifying which variables are important (discriminative) for which specific classes, or in other words, which classes are discriminable for which variables. For example, let's imagine a three-class problem with two variables. The first variable may be important for discriminating classes 1 and 2, but unimportant for classes 2 and 3; on the other hand, the second variable may be important for discriminating classes 2 and 3, but unimportant for classes 1 and 2. We believe that such situations arise often in high-dimensional data, for example, in data obtained from high-throughput expression technologies.</p><p>To address this problem, this paper proposes a class-specific variable selection method for MSVMs. Specifically, a pairwise fusion penalty is introduced to penalize the difference between (all) pairs of coefficients for each variable and shrink the coefficients of nondiscriminable classes to some identical value. If all coefficients associated with a variable are "fused," this variable is regarded as noninformative and removed from the model. Otherwise, the pairwise fusion penalty has the flexibility of only fusing the coefficients of nondiscrim-inable classes for this variable.</p><p>The rest of this article is organized as follows. Section 2 proposes the class-specific variable selection method for MSVMs and introduces a linear programming algorithm to solve the consequent optimization problem; Section 3 and 4 evaluate the performance of the proposed method by two simulated examples and one real example, respectively; and we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Suppose we observed n sample pairs {x i , y i } n i=1 , where x i = (x i,1 , . . . , x i,p ) is a vector composed of p variables and y i is the label of x i . For a K-category classification problem, y i ? {1, 2, . . . , K}. Without loss of generality, we assume n i=1 x i,j = 0 for all 1 ? j ? p.</p><p>A multicategory support vector machine aims to learn K decision functions f = (f 1 , . . . , f K ) from the data {x i , y i } n i=1 , where each f k (x i ), a mapping from the input domain R p to R, represents the strength of the evidence that an example with input x i belongs to class k.</p><p>Given an estimate of the decision functions f , MSVM assigns a new data point x * to the class k * = arg max 1?k?K f k (x * ).</p><p>In linear classification cases, we assume f k (x i ) = w k x i T + b k , where b k is the intercept and</p><formula xml:id="formula_0">w k = (w k,1 , . . . , w k,p</formula><p>) is a p-dimensional row vector, where each component w k,j captures the contribution of the j-th variable to the k-th class. All w ? k,j s (1 ? k ? K, 1 ? j ? p) are referred as (decision) coefficients and they can be collected in a K ? p matrix (as follows)</p><p>with rows corresponding to classes and columns to variables</p><formula xml:id="formula_1">W = ? ? ? ? ? w 1,1 w 1,2 ? ? ? w 1,j ? ? ? w 1,p w 2,1 w 2,2 ? ? ? w 2,j ? ? ? w 2,p . . . . . . . . . . . . . . . . . . w K,1 w K,2 ? ? ? w K,j ? ? ? w K,p ? ? ? ? ?</formula><p>Throughout the paper, we use w k to represent the coefficients associated with the k-th class (k-th row vector of W ) and use w (j) = (w 1,j , . . . , w K,j ) T to represent the coefficients for the j-th variable (j-th column vector of W ).</p><p>In this paper, we focus on a family of MSVM methods based on the following "Loss+Penalty" w k,j = 0 impose the identifiability of the solution and they are also the necessary conditions for the Fisher consistency of the MSVM <ref type="bibr" target="#b7">(Lee et al., 2004)</ref>. J ? (W ) is a penalty function with tuning parameter ?. It involves some prior information to help estimate the coefficients in W . For example, the standard MSVM <ref type="bibr" target="#b7">(Lee et al., 2004</ref>) employs an ? 2 penalty as follows</p><formula xml:id="formula_2">framework min b,W n i=1 K k=1 I(y i = k)[b k + w T k x i + 1] + + J ? (W ) subject to K k=1 b k = 0, K k=1 w k,j = 0, 1 ? j ? p.<label>(1</label></formula><formula xml:id="formula_3">J L2 ? (W ) = ? p j=1 K k=1 w 2 k,j<label>(2)</label></formula><p>For the purpose of variable selection, <ref type="bibr" target="#b13">Wang and Shen (2007)</ref> proposed to use the ? 1 penalty as follows</p><formula xml:id="formula_4">J L1 ? (W ) = ? p j=1 K k=1 ? k,j |w k,j |<label>(3)</label></formula><p>where ? k,j is the adaptive weight defined as ? k,j = 1/| wk,j |. Due to its singularity property, the ? 1 penalty shrinks some w k,j 's to be exactly zero and removes the j-th variable from the model if all coefficients associated with the j-th variable (i.e., w k,j for all 1 ? k ? K) are shrunken to zero (in this case, the j-th variable does not contribute to discriminating between the decision functions f 1 , . . . , f K and thus it is a noninformative variable). <ref type="bibr" target="#b19">Zhang et al. (2008)</ref> proposed a supnorm (? ? ) penalty (as follows) to remove the insignificant variables more efficiently.</p><formula xml:id="formula_5">J SN ? (W ) = ? p j=1 w (j) ? = ? p j=1 max 1?k?K ? k,j |w k,j |<label>(4)</label></formula><p>This penalty treats all coefficients associated with the same variable as a natural group and shrinks them to zero simultaneously. It should be noted that the ? 1 penalty usually tends to shrink only some w k,j 's to zero, thus being more flexible but less efficient in removing noninformative variables. In <ref type="bibr" target="#b19">Zhang et al. (2008)</ref>, the adaptive weights</p><formula xml:id="formula_6">? k,j , 1 ? k ? K, 1 ? j ? p are defined in two ways: (1) ? k,j = 1/| wk,j |, 1 ? k ? K, 1 ? j ? p; (2) ? 1,j = ? ? ? = ? K,j = 1/ max{| w1,j |, . . . , | wK,j |}, 1 ? j ? p.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Class-specific Variable Selection</head><p>Given our focus on class-specific variable selection introduced in Section 1, we propose the following pairwise fusion penalty for MSVM</p><formula xml:id="formula_7">J P F (W ) = p j=1 1?k&lt;k ? ?K ? (j) k,k ? |w k,j -w k ? ,j |<label>(5)</label></formula><p>For each variable, this penalty aims at shrinking the differences between the coefficients associated with every pair of classes. Due to the singularity of the absolute value function, some terms in the sum are shrunken to exactly zero, resulting in some coefficients w k,j 's having identical values. For example, if coefficients</p><formula xml:id="formula_8">w k,j = w k ? ,j , then f k (x) -f k ? (x) doesn't</formula><p>depend on the j-th variable. Consequently, this variable is considered to be unimportant for discriminating between class k and k ? , though it may be important for separating other classes. Moreover, if all coefficients for the same variable are shrunken to the same value, then this variable doesn't help discriminate between the decision functions f 1 , . . . , f K and can be removed from the model. We refer this variable as a noninformative variable. For a two-class problem, the pairwise fusion penalty proposed here is equivalent to the ? 1 penalty under the constraint K k=1 w k,j = 0 (1 ? j ? p). Here we set the adaptive weights ? k,k ? to be small, thus the difference between w k,j and w k ? ,j is lightly penalized. On the other hand, if variable j is unimportant for separating clusters k and k ? , we would like the corresponding ?</p><formula xml:id="formula_9">(j)</formula><p>k,k ? to be large, hence the difference between w k,j and w k ? ,j is heavily penalized. In our implementation, we compute the weights using the estimates from standard MSVM, i.e., ?</p><p>k,k ? = 1/| wk,jwk ? ,j | where wk,j is the estimate of w k,j by solving</p><p>(1) with penalty (2). Note that this decomposition has also been used by <ref type="bibr" target="#b5">Guo et al. (2010)</ref> for clustering purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Algorithm</head><p>Here we discuss how to minimize objective function (1) with penalty (5), i.e., the optimization problem as follows</p><formula xml:id="formula_11">min b,W n i=1 K k=1 I(y i = k)[b k + w T k x i + 1] + +? p j=1 1?k&lt;k ? ?K ? (j) k,k ? |w k,j -w k ? ,j | subject to K k=1 b k = 0, K k=1 w k,j = 0, 1 ? j ? p<label>(6)</label></formula><p>Objective function ( <ref type="formula" target="#formula_11">6</ref>) can be converted to a standard linear programming (LP) problem and solved by most linear programming software. Specifically, denote a i,k = I(y i = k),</p><formula xml:id="formula_12">? i,k = [b k + w T k x i + 1] + and ? k,k ? ,j = w k,j -w k ? ,j .</formula><p>To deal with the absolute value in (6),</p><formula xml:id="formula_13">let ? + k,k ? ,j = max{0, ? k,k ? ,j } be the positive part of ? k,k ? ,j and ? - k,k ? ,j = max{0, -? k,k ? ,j } be the negative part of ? k,k ? ,j . Consequently, ? k,k ? ,j = ? + k,k ? ,j -? - k,k ? ,j and |? k,k ? ,j | = ? + k,k ? ,j + ? - k,k ? ,j .</formula><p>Thus, (6) can be written as</p><formula xml:id="formula_14">min b,W ,?,? n i=1 K k=1 a i,k ? i,k +? p j=1 1?k&lt;k ? ?K ? (j) k,k ? (? + k,k ? ,j + ? - k,k ? ,j ) subject to ? i,k ? b k + w T k x i + 1, ? i,k ? 0, for all 1 ? i ? n, 1 ? k ? K; K k=1 b k = 0, K k=1 w k,j = 0, for all 1 ? j ? p; ? + k,k ? ,j -? - k,k ? ,j = w k,j -w k ? ,j , for all 1 ? k &lt; k ? ? K, 1 ? j ? p.<label>(7)</label></formula><p>where</p><formula xml:id="formula_15">? = {? + k,k ? ,j , ? - k,k ? ,j : 1 ? k &lt; k ? ? K, 1 ? j ? p} and ? = (? i,k ) n?K .</formula><p>In this article, objective function ( <ref type="formula" target="#formula_14">7</ref>) was solved by the mathematical programming language AMPL with linear programming package CPLEX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simulation Study</head><p>In this section, we illustrate the performance of the proposed class-specific variable selection method on two synthetic examples with four and five classes, respectively. We compare four different MSVM methods, pairing it with: the ? 1 penalty ("L1", equation ( <ref type="formula" target="#formula_4">3</ref>)), two supnorm penalties ("SN-I" and "SN-II", equation ( <ref type="formula" target="#formula_5">4</ref>) with two types of adaptive weights), the proposed pairwise fusion penalty ("PF", equation ( <ref type="formula" target="#formula_7">5</ref>)). In each simulation, 50 training observations, 50 validation observations and 10,000 test observations are generated from each class. The tuning parameter ? is selected on the validation set via a grid {2 -15 , 2 -14 , . . . , 2 15 }.</p><p>We repeat this procedure 100 times for each simulation and record the average test error rates as compared to the true class labels, and average selection rate for both informative and noninformative variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1</head><p>In this simulation, there are four classes and p = 10 variables, with the first two variables being informative and the remaining ones noninformative. The variables were generated according to the following mechanism: the two informative variables x 1 and x 2 are independently uniformly distributed in [-1, 1], whereas the remaining eight noninformative variables are all i.i.d. N(0, 8 2 ). Denote x = (x 1 , . . . , x 10 ), then we define the decision function for the k-th class as follows</p><formula xml:id="formula_16">f k (x) = ? ? ? ? ? ? ? 10x 1 + 5x 2 , if k = 1; 5x 2 , if k = 2; -5x 2 , if k = 3; -10x 1 -5x 2 , if k = 4.</formula><p>and we assign x to class k with a probability proportional to exp{f k (x)}. In this example,</p><p>x 1 is unimportant for discriminating between classes 2 and 3 and x 2 is unimportant for discriminating between classes 1 and 2 and classes 3 and 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2</head><p>In this example, a five-class scenario is considered. There are a total of p = 10 variables with the first three informative and the other seven noninformative. Similar to Example 1, the informative variables are independently uniformly distributed in [-1, 1], whereas the seven noninformative variables are all i.i.d. N(0, 8 2 ). We define the decision function for the k-the class as</p><formula xml:id="formula_17">f k (x) = ? ? ? ? ? ? ? ? ? ? ? 4x 1 -10x 2 + 6x 3 , if k = 1; 4x 1 + x 3 , if k = 2; -x 1 + x 3 , if k = 3; -x 1 -4x 3 , if k = 4; -6x 1 + 10x 2 -4x 3 , if k = 5.</formula><p>and assign x to class k with a probability proportional to exp{f k (x)}. Notice that x 1 is unimportant for discriminating between classes 1 and 2 and classes 3 and 4; x 2 is unimportant classes 2, 3 and 4; and x 3 is unimportant for classes 2 and 3 and classes 4 and 5. Example Variable Pair L1 SN-I SN-II PF 1 1 2/3 0.08 0.00 0.00 0.34 2 1/2 0.00 0.52 0.00 0.96 3/4 0.00 0.52 0.00 0.94 2 1 1/2 0.00 0.06 0.00 0.78 3/4 0.66 0.00 0.00 0.94 2 2/3 0.36 0.00 0.00 0.72 2/4 0.08 0.00 0.00 0.50 3/4 0.22 0.00 0.00 0.76 3 2/3 0.62 0.00 0.00 0.88 4/5 0.00 0.16 0.00 0.74</p><p>The results over 100 replications for both simulation scenarios are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>All methods are able to identify the informative variables. Compared with L1, SN-I and SN-II, the proposed PF penalty is more effective at removing noninformative variables and produces lower or comparable prediction error rates.</p><p>If a variable is unimportant for separating a pair of classes, and the corresponding estimated coefficients are also the same, we consider this as a correct "fusion". Table <ref type="table" target="#tab_1">2</ref> summarizes these results. Specifically, each row in the table gives the proportion of correctly fused variables (averaged over 100 replications) that are noninformative for separating the corresponding pair of classes (indicated in column "Pair"). For example, the second row shows that for the PF method, on average 96% of the first two informative variables are correctly fused for classes 1 and 2. It is also clear that PF dominates other methods in terms of correctly fusing the coefficients of nondiscriminable classes. It should also pointed out that although L1 and SN-I correctly fuse some coefficients of nondiscriminable classes, e.g., in the first row (L1) as well as in the second and third row (SN-I), the result is an artifact.</p><p>In Example the coefficients of classes 2 and 3 for variable 1 are all equal to zero, which happens to be the value that the ? 1 penalty shrinks to. The same reasoning applies to classes 2, 3 and 4 for variable 2 in Example 2 (row 6-8, column "L1"). On the other hand, in Example 1, although classes 1 and 2 (as well as classes 3 and 4) have the same coefficient for variable 2, the L1 method fails to fuse them, since their coefficients are different from zero.</p><p>In contrast to L1, SN-I tends to encourage the coefficients with large magnitudes to have some identical values. In Example 1, for instance, the coefficients of classes 1 and 2 (as well as those of classes 3 and 4) for variable 2 have the same value with large magnitude, thus they are identified by SN-I. On the other hand, it fails to fuse classes 2 and 3 for variable 1, since their coefficients are close to zero. However, the PF method identifies the structure correctly in both situations.</p><p>4 Real Data Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Microarray Example</head><p>In this example, we apply the proposed pairwise fusion SVM method to conduct classspecific variable selection on a microarray dataset of small round blue cell tumors (SRBCT) of childhood cancer <ref type="bibr" target="#b6">(Khan et al., 2001)</ref>. This dataset contains the expression profiles of and Burkitt lymphoma (BL). We preprocessed the data by selecting a subset of 500 genes according to their marginal relevance criterion <ref type="bibr" target="#b3">(Dudoit et al., 2002;</ref><ref type="bibr" target="#b19">Zhang et al., 2008)</ref>:</p><formula xml:id="formula_18">R j = n i=1 K k=1 I(y i = k)(? k,j -? j ) n i=1 K k=1 I(y i = k)(x i,j -? k,j )<label>(8)</label></formula><p>? k,j is the mean of all samples in class k and variable j and ? j is the mean of all samples in variable j. The term on the numerator reflects the between-class distance and the term on the denominator reflects the within-class distance. Therefore, this criterion gives large values to those genes expressing heterogeneously across the classes and homogenously within the classes. The top 500 genes are collected and the new data are centered and scaled along each variable before classification.</p><p>All MSVM methods with different penalties are applied to the training set and the tuning parameter ? is selected by five-fold cross validation. The prediction results on the test data are summarized in Table <ref type="table" target="#tab_2">3</ref>. All these methods produce similar test error rates and select similar number of genes. Figure <ref type="figure" target="#fig_3">1</ref> shows the results for class-specific variable selection. The rows correspond to the 32 (out of the 500) genes selected by the PF method and the column to the six pairs formed from the four subtypes. A black (white) spot indicates that the estimated coefficients of the corresponding gene for the two subtypes are different (the same). For example, gene "1358266" is unimportant for discriminating subtypes EWS, NB and BL. We can see that PF provides a more informative way for describing the functions of a gene with respect to discriminating different tumor subtypes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Web Mining Example</head><p>The data set comes from the World Wide Knowledge Base project at Carnegie Mellon University. It was collected in 1997 and includes webpages from websites at computer science departments in the following four universities: Cornell, Texas, Washington, and Wisconsin.  <ref type="bibr" target="#b11">(Porter, 1980)</ref> to the remaining text.</p><p>The log-entropy weighting method <ref type="bibr" target="#b4">(Dumais, 1991)</ref> was used to calculate the termdocument matrix X = (x i,j ) n?p , with n and p denoting the number of webpages and distinct terms, respectively. Let f i,j , 1 ? i ? n, 1 ? j ? p be the number of times the j-th term appears in the i-th webpage and let p i,j = f i,j / n i=1 f i,j . Then, the log-entropy weight of the j-th term is defined as</p><formula xml:id="formula_19">e j = 1 + n i=1</formula><p>p i,j log(p i,j )/ log(n) .</p><p>Finally, the term-document matrix X is defined as</p><p>x i,j = e j log(1 + f i,j ) , 1 ? i ? n , 1 ? j ? p , and it is normalized along each column. We applied the proposed method to n = 1396 documents in the four largest categories and p = 100 terms with the highest log-entropy weights out of a total of 4800 terms.</p><p>Table <ref type="table" target="#tab_4">4</ref> shows the prediction and variable selection results. Again, all these methods produce similar test error rates and select similar number of variables (terms). Figure <ref type="figure">2</ref> illustrates the results of class-specific variable selection. We can see that many terms only contribute to discriminate a set of topics. For example, the topics "faculty", "course" and "project" can not be discriminated by terms "work", "member" etc. Therefore, class-specific variable selection provides better interpretation and it helps deeper understand the structure of the data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have developed a method for simultaneously classifying high-dimensional data and selecting informative variables, by employing a penalized multicategory support vector machine framework. In particular, the proposed method penalizes the difference between the coefficients for each pair of classes and for each variable, which allows one to identify and remove unimportant variables for selected subsets of classes. This allows one to gain more insight into the function of particular variables and potentially discover heterogeneous structures that other available methods are unable to capture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) where b = (b 1 , . . . , b K ) T is the vector of intercepts and I(y i = k) is an indicator function with value 1 if y i = k and 0 otherwise. The sum-to-zeros constraints K k=1 b k = 0 and K k=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? 's based on the intuition: if variable j is important for separating classes k and k ? , we would like the corresponding ? (j)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2308 genes obtained from 83 tissue samples, 53 of which belong to the training set and the remaining 30 belonging to the test set. These subjects are classified into four tumor subtypes: Ewing family of tumors (EWS), rhabdomyosarcoma (RMS), neuroblastoma (NB)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Results of class-specific variable selection for the PF method on the SRBCT data. Each row corresponds to a gene (denoted by its ID). Each column corresponds to a pair of tumor subtypes; for example, "EWS/NB" indicates subtypes EWS and NB. A black (white) spot indicates that the estimated coefficients of the corresponding gene for the two subtypes are different (the same).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Simulation results for Example 1. "Error rate" is the proportion of wrongly classified samples in the test data set. "Info" is the number of selected informative variables (out of 2 for Example 1 and out of 3 for Example 2). "Noninfo" is the number of noninformative variables (out of 8 for Example 1 and out of 7 for Example 2). All results are averaged over 100 replications and their corresponding standard deviations are recorded in the parentheses.</figDesc><table><row><cell cols="2">Example Method</cell><cell>Error rate</cell><cell>Info</cell><cell>Noninfo</cell></row><row><cell></cell><cell>L1</cell><cell cols="3">0.142 (0.009) 2 (0) 0.980 (1.301)</cell></row><row><cell>1</cell><cell cols="4">SN-I SN-II 0.141 (0.009) 2 (0) 0.900 (1.919) 0.137 (0.006) 2 (0) 0.280 (0.573)</cell></row><row><cell></cell><cell>PF</cell><cell cols="3">0.136 (0.004) 2 (0) 0.040 (0.198)</cell></row><row><cell></cell><cell>L1</cell><cell cols="3">0.176 (0.014) 3 (0) 1.960 (1.761)</cell></row><row><cell>2</cell><cell cols="4">SN-I SN-II 0.168 (0.014) 3 (0) 0.240 (0.476) 0.191 (0.016) 3 (0) 2.240 (2.200)</cell></row><row><cell></cell><cell>PF</cell><cell cols="3">0.171 (0.022) 3 (0) 0.040 (0.198)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of class-specific variable selection for nondiscriminable class pairs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification and variable selection results of SRBCT data set.</figDesc><table><row><cell cols="3">Method Test Error (%) Selected Genes (#)</cell></row><row><cell>L1</cell><cell>0</cell><cell>32</cell></row><row><cell>SN-I</cell><cell>3.3</cell><cell>29</cell></row><row><cell>SN-II</cell><cell>0</cell><cell>35</cell></row><row><cell>PF</cell><cell>0</cell><cell>32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Classification and variable selection results of web mining data set using top 100 terms.</figDesc><table><row><cell cols="3">Method Test Error (%) Selected Terms (#)</cell></row><row><cell>L1</cell><cell>22.6</cell><cell>81</cell></row><row><cell>SN-I</cell><cell>23.5</cell><cell>80</cell></row><row><cell>SN-II</cell><cell>23.0</cell><cell>74</cell></row><row><cell>PF</cell><cell>23.3</cell><cell>84</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The author would like to thank <rs type="person">Elizaveta Levina</rs>, <rs type="person">George Michailidis</rs> and <rs type="person">Ji Zhu</rs> for their helpful suggestions. I also thank <rs type="person">Yichao Wu</rs> and <rs type="person">Yufeng Liu</rs> for providing the code to implement supnorm SVM.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature selection via concave minimization and support vector machines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th International Conf. on Machine Learning</title>
		<meeting>15th International Conf. on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Cardoso-Cachopo</surname></persName>
		</author>
		<ptr target="http://web.ist.utl.pt/~acardoso/datasets/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernelbased vector machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparison of discrimination methods for the classification of tumors using gene expression data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dudoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridlyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Speed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Asscociation</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving the retrieval of information from external source</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments and Computers</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pairwise variable selection for high-dimensional model-based clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michailidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="793" to="804" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classification and diagnostic prediction of cancers using gene expression profiling andartificial neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ringner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Saal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ladanyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Antonescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Meltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="673" to="679" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multicategory support vector machines, theory, and application to the classification of microarray data and satellite radiance data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Asscociation</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="67" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multicategory ?-learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Asscociation</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="500" to="509" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variable selection via a combination of the ? 0 and ? 1 penalties</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="782" to="798" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Support vector machines with adaptive ? q penalties</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="6380" to="6394" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1995">1995. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On ? 1 -norm multi-class support vector machines: methodology and theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Asscociation</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="583" to="594" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The doubly regularized support vector machine</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="589" to="615" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Use of the zero-norm with linear models and kernel methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="3" to="1439C" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiclass support vector machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ESANN99</title>
		<meeting>ESANN99</meeting>
		<imprint>
			<publisher>D. Facto Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust truncated-hinge-loss support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Asscociation</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="974" to="983" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gene selection using support vector machines with nonconvex penalty</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="88" to="95" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variable selection for the multicategory SVM via adaptive sup-norm regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="149" to="167" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The entire regularization path the support vector machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1391" to="1415" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The F ? -norm support vector machine</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="379" to="398" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
