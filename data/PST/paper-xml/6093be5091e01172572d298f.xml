<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TENET: A Framework for Modeling Tensor Dataflow Based on Relation-centric Notation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-05">5 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liqiang</forename><surname>Lu</surname></persName>
							<email>liqianglu@pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Naiqing</forename><surname>Guan</surname></persName>
							<email>naiqing.guan@mail.utoronto.ca</email>
						</author>
						<author>
							<persName><forename type="first">Yuyue</forename><surname>Wang</surname></persName>
							<email>wangyuyue@pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Liancheng</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zizhang</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jieming</forename><surname>Yin</surname></persName>
							<email>yin@lehigh.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
							<email>cong@cs.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Peking University University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of California at Los Angeles</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TENET: A Framework for Modeling Tensor Dataflow Based on Relation-centric Notation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-05">5 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2105.01892v1[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accelerating tensor applications on spatial architectures provides high performance and energy-efficiency, but requires accurate performance models for evaluating various dataflow alternatives. Such modeling relies on the notation of tensor dataflow and the formulation of performance metrics. Recent proposed compute-centric and data-centric notations describe the dataflow using imperative directives. However, these two notations are less expressive and thus lead to limited optimization opportunities and inaccurate performance models.</p><p>In this paper, we propose a framework TENET that models hardware dataflow of tensor applications. We start by introducing a relation-centric notation, which formally describes the hardware dataflow for tensor computation. The relation-centric notation specifies the hardware dataflow, PE interconnection, and data assignment in a uniform manner using relations. The relation-centric notation is more expressive than the computecentric and data-centric notations by using more sophisticated affine transformations. Another advantage of relation-centric notation is that it inherently supports accurate metrics estimation, including data reuse, bandwidth, latency, and energy. TENET computes each performance metric by counting the relations using integer set structures and operators. Overall, TENET achieves 37.4% and 51.4% latency reduction for CONV and GEMM kernels compared with the state-of-the-art data-centric notation by identifying more sophisticated hardware dataflows.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Tensor operations have been increasingly deployed in many applications, such as data analysis, machine learning, and hydrodynamics simulation <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b55">16,</ref><ref type="bibr" target="#b62">23,</ref><ref type="bibr" target="#b73">34,</ref><ref type="bibr" target="#b85">46,</ref><ref type="bibr" target="#b86">47,</ref><ref type="bibr" target="#b92">53]</ref>. In recent years, spatial architectures have emerged as a promising way to accelerate tensor operations due to their high performance and energy-efficiency <ref type="bibr">[8, 10-12, 15, 18, 19, 21, 22, 26, 30, 36, 40, 42-44, 48, 57]</ref>. A typical spatial architecture is composed of a processing element <ref type="bibr">(PE)</ref> array and a scratchpad memory. PEs are connected via an on-chip interconnect that enables efficient data reuse.</p><p>The main characteristic of spatial architectures is the diverse hardware dataflow alternatives. Tensor operations are usually described using a loop nest. Specific to tensor operation, a hardware dataflow describes 1) the assignment of loop instances to the PE array and 2) the execution sequence of these loop instances in the PEs. Hardware dataflow is critical for achieving high throughput and low latency, because it determines PE utilization, data access patterns, and onchip bandwidth requirement. Different tensor computation prefers different hardware dataflows. For example, Google's Tensor Processing Unit (TPU) <ref type="bibr" target="#b61">[22]</ref> connects PEs using systolic dataflow, where each PE is responsible for one multiply-andaccumulate operation. While Cambircon <ref type="bibr" target="#b70">[31]</ref> connects PEs via a multicast communication network, in which each PE performs a dot-product. Other spatial architectures, like DySER <ref type="bibr" target="#b58">[19]</ref> and Plasticine <ref type="bibr" target="#b81">[42]</ref>, integrate PEs and their interconnect in a flexible manner, and hence can support a wider range of applications.</p><p>Despite the fact that various dataflows have been practically implemented in modern tensor accelerators, a formal notation is still strongly desired to represent hardware dataflow. Ideally, a notation should be able to cover the complete dataflow design space systematically, as well as facilitate simple and accurate performance modeling. State-of-the-art techniques represent hardware dataflow using either compute-centric <ref type="bibr" target="#b78">[39,</ref><ref type="bibr" target="#b95">56]</ref> or data-centric notations <ref type="bibr" target="#b63">[24,</ref><ref type="bibr" target="#b64">25]</ref>. However, both notations have limitations. First, these notations are less expressive and they can only represent a subset space of hardware dataflows. Using these notations, architects are provided with an incomplete space and limited optimization opportunities. For example, both notations fail to describe hardware dataflows that require skewing of loop iterations and tensors. Such dataflows needs affine loop transformation <ref type="bibr" target="#b67">[28,</ref><ref type="bibr" target="#b68">29]</ref> to enable sophisticated mapping of loop instances to spatial architectures. Second, both notations fail to support accurate performance analysis. The compute-centric notation does not directly model data transfer and reuse, and thus lacks a detailed performance model. The data-centric notation is integrated with the MAESTRO model, which outputs the reuse, latency, and energy for a given dataflow <ref type="bibr" target="#b64">[25]</ref>. However, MAESTRO models these metrics by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Computation-centric Data-centric STT Relation-Centric Timeloop <ref type="bibr" target="#b78">[39]</ref> Interstellar <ref type="bibr" target="#b95">[56]</ref> MAESTRO <ref type="bibr">[24, 25] [4, 9, 28, 54]</ref> TENET calculating the polynomials of parameters, which might not be accurate for tensors whose dimensions cannot be explicitly specified by the data-centric primitives.</p><p>In this paper, we propose TENET, a framework that models the dataflow of tensor applications on spatial architectures. The key component of TENET is the relation-centric notation, which formally describes the hardware dataflow for tensor computation. Specifically, we formally define the relations between 1) the loop instances and the PEs that perform the computation, 2) the loop instances and their execution sequence in the PEs, 3) PEs and the corresponding assigned tensor elements, and 4) PEs that are connected with interconnection network. The first two relations determine where and when the loop instances are executed. The third relation models where and when a tensor element is accessed. The last relation describes how the tensor elements traverse across PEs, e.g., systolic array, reduction tree. By putting these relations together in a uniform manner, we can precisely model the tensor computation on spatial architectures, data assignment to PEs, and data movement between PEs.</p><p>The relation-centric notation allows the users to describe the complex hardware dataflows using relations only. When representing these relations, TENET supports all linear transformations that map loop instances to spatial architecture spatially and temporally. This leads to a complete design space of hardware dataflows. The relation-centric notation also inherently supports simple and accurate modeling of various important performance metrics. These metrics are crucial for evaluating different hardware dataflow design alternatives. All four types of relations above can be mathematically represented as a set of pairs. Based on such structural representation, performance metrics can be easily computed using integer set operators. Overall, TENET is able to estimate various hardware metrics, including data reuse, latency, PE communication bandwidth, and on-chip memory bandwidth.</p><p>The contributions of this work are as follows. First, we propose relation-centric notation for modeling the hardware dataflow of tensor computation. Relation-centric notation is more expressive than compute-centric and data-centric notations. By representing the dataflow, data assignment, and interconnection as relations uniformly, relation-centric notation forms the complete design space of hardware dataflows, which provides more optimization opportunities.</p><p>Second, we introduce performance models that can accurately calculate various hardware metrics. This is naturally supported by the structural representation of relation-centric notation. We first formulate three basic volumes that describe the overall data size, the reused data size, and the minimum data size that needs to be transferred between PEs and scratchpad. We then use these volumes to derive various hardware metrics.</p><p>Third, we systematically analyze and compare different notations in terms of expressiveness and performance modeling. Results show that TENET achieves 37.4% and 51.4% latency reduction for Conv and GEMM kernels by identifying more sophisticated dataflows compared to the state-of-the-art datacentric notation. The source code of TENET is publically available in Github (https://github.com/pku-liang/TENET).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spatial Architectures</head><p>Spatial architectures are a class of architectures that feature a set of processing element <ref type="bibr">(PE)</ref>, interconnection between PEs and memory hierarchy <ref type="bibr">[7,</ref><ref type="bibr" target="#b56">17,</ref><ref type="bibr" target="#b58">19,</ref><ref type="bibr" target="#b75">36,</ref><ref type="bibr" target="#b79">40,</ref><ref type="bibr" target="#b81">42,</ref><ref type="bibr" target="#b87">48]</ref>. Each PE contains arithmetic logic units (ALUs) that can be configured by specific instructions. The PE also contains register files for data storage. The interconnection between PEs can effectively increase data reuse opportunities, which further reduces the bandwidth requirement. Generally, spatial architectures have three levels of memory hierarchy, i.e., PE register level, on-chip scratchpad, and off-chip memory. For simplicity, we make the following two assumptions when modeling hardware behaviors. First, the ALU has the ability to perform one multiply-andaccumulate (MAC) operation. Second, data transfer through the interconnect between adjacent PEs takes one cycle.</p><p>We use the term hardware dataflow to refer to the implementation of a specific tensor application on a spatial architecture. Tensor applications are usually described using loop nests. Specific to tensor applications, the dataflow is represented from two aspects: 1) the PE where a loop instance is executed, 2) the execution sequence of loop instances in the PEs. When designing the dataflow, data reuse is a critical factor in order to achieve high performance and low energy, which can be further categorized into temporal reuse and spatial reuse <ref type="bibr" target="#b49">[10,</ref><ref type="bibr" target="#b50">11,</ref><ref type="bibr" target="#b63">24,</ref><ref type="bibr" target="#b95">56]</ref>. Temporal reuse happens when the same</p><formula xml:id="formula_0">T[0] T[1] T[2] T[3] T[0] T[1]</formula><p>T <ref type="bibr">[2]</ref> T <ref type="bibr">[3]</ref> T <ref type="bibr">[4]</ref> T <ref type="bibr">[5]</ref> T <ref type="bibr">[6]</ref> T</p><formula xml:id="formula_1">[i] → A[i,j]</formula><p>skewed data access rectangle-like data access</p><formula xml:id="formula_2">T[i+j] → A[i,j] A[0] A[1] A[2] A[3] T[0] T[1] T[2] A[1] A[2] A[3] A[4] A[2] A[3] A[4] A[5]</formula><p>Actual reuse of A: 6 Data-centric reuse: 8 data is reused at different cycles, while spatial reuse happens when the same data is reused at different PEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Notation Basics</head><p>Here, we introduce some basic terms that are widely used in various compiler frameworks <ref type="bibr">[6,</ref><ref type="bibr" target="#b68">29,</ref><ref type="bibr" target="#b94">55]</ref> for loop analysis. In this paper, TENET supports tensor applications with perfectlynest loops and single unconditional statement.</p><p>Iteration domain. Given a loop nest with one statement S, its iteration domain D S is the set that contains all the loop instances. Each instance can be represented as S[ n], which is a point in D S . For example, in 1D-CONV operator of Figure <ref type="figure" target="#fig_0">1</ref>, the iteration domain is</p><formula xml:id="formula_3">D S = {S[i, j] : 0 ≤ i &lt; 4, 0 ≤ j &lt; 3},</formula><p>where S[i, j] is a loop instance and 0 ≤ i &lt; 4, 0 ≤ j &lt; 3 gives the affine constraints.</p><p>Access function. Given a loop instance, the access function returns the tensor elements accessed by the statement S. We use a relation to represent the access function of tensor F.</p><formula xml:id="formula_4">A S,F = {S[ n] → F[ f ]}<label>(1)</label></formula><p>For example, in 1D-CONV operator of Figure <ref type="figure" target="#fig_0">1</ref>, the access function to tensor Y is {S[i, j] → Y [i] : 0 ≤ i &lt; 4, 0 ≤ j &lt; 3}, which means that the loop instance S[i, j] accesses the tensor element Y [i].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Limitations of Existing Dataflow Notations</head><p>Table I compares four notations with their features of dataflow expression and performance modeling. For computecentric notation, the dataflow is specified using loop transformation directives including reorder, blocking, and parallel <ref type="bibr" target="#b78">[39,</ref><ref type="bibr" target="#b95">56]</ref>. The compute-centric notation provides great flexibility for describing how the computation is performed in an imperative programming style. Recently, data-centric notation is proposed, which is specified using data mapping directives including spatial and temporal map, data movement order <ref type="bibr" target="#b63">[24,</ref><ref type="bibr" target="#b64">25]</ref>. Such structural representation enables easy data reuse computation. To use this notation, the users must manually write the directives, which is not straightforward for complex dataflows. Space-time transformation (STT) has been used to map loop instances onto systolic arrays <ref type="bibr">[4,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b67">28]</ref>. However, they lack accurate performance models to analyze various hardware metrics, and support for non-systolic array spatial architecture. Both Timeloop <ref type="bibr" target="#b78">[39]</ref> and Interstellar <ref type="bibr" target="#b95">[56]</ref> use compute-centric notation. The loop order determines the loop instance execution sequence. However, the workload assignment requires extra directives. For example, in Figure <ref type="figure" target="#fig_0">1</ref> Both compute-and data-centric notations are fundamentally limited in their expressiveness and performance modeling capability. First, both notations fail to cover a complete design space of dataflow. For example, in Figure <ref type="figure" target="#fig_0">1</ref>(a), we use T [t] to denote the tensor elements that are processed in cycle t. These two notations can only describe dataflows using rectangle-like data access, lacking the support for complex dataflows with skewed data access. Such skewed data access requires the introduction of a new dimension by combing tensor dimensions i and j using affine transformations. More importantly, the limitation in expressiveness cannot be easily remedied by extending the data-centric notation, because it requires great effort to manually transform the original tensor application to explicitly specify the data distribution. Such manual transformation makes it very difficult to estimate the performance metrics, which contradicts with the original intention of the data-centric notation.</p><formula xml:id="formula_5">tensor app GEMM Y[i][j]= A[i][k]*B[k][j]</formula><p>Second, from performance modeling perspective, previous compute-centric notation-based models only analyze data reuse opportunities in a coarse-grained manner <ref type="bibr" target="#b78">[39,</ref><ref type="bibr" target="#b95">56]</ref>. For example, Interstellar <ref type="bibr" target="#b95">[56]</ref> calculates data reuse using the product of unroll factors. The data-centric notation analyzes the hardware performance using MAESTRO model <ref type="bibr" target="#b64">[25]</ref>. However, MAESTRO uses simple polynomials to estimate data reuse, which is less precise as it calculates the data movements using a simple polynomial. In Figure <ref type="figure" target="#fig_0">1</ref>(c), the actual reuse of tensor A is 6, while the result from MAESTRO is 8. The inaccuracy comes from the fact that the two primitives in Figure <ref type="figure" target="#fig_0">1</ref>  for (i = 0; i &lt; 2; i++) for (j = 0; j &lt; 2; j++) for (k = 0; k &lt; 4; k++) S: Y only describe the data movement of tensor B and Y, without modeling the movement of tensor A.</p><formula xml:id="formula_6">[i,j] += A[i,k] * B[k,j]; Dataflow {S i,j,k → PE[i,j]} space-stamp {S i,j,k → T[i+j+k]} time-stamp {PE i, j : 0 ≤ i, j &lt; 2} PE Domain Interconnect {PE i, j → PE[i, j + 1]} {PE i, j → PE[i + 1, j]} Tensor Y assignment function {S i,j,k → Y[i,j]} T[i+j+k]=T[1] ⇒ Domain S 0,0,1 →PE[0,0] S 1,0,0 →PE[1,0] S 0,1,0 →PE[0,1] T[i+j+k]=T[2] ⇒ Domain S 0,0,2 →PE[0,0] S 1,0,1 →PE[1,0] S 0,1,1 →PE[0,1] S 1,1,0 →PE[1,1] T[i+j+k]=T[3] ⇒ Domain S 0,0,3 →PE[0,0] S 1,0,2 →PE[1,0] S 0,1,2 →PE[0,1] S 1,1,1 →PE[1,1] T[i+j+k]=T[0] ⇒ Domain S 0,0,0 →PE[0,0] PE[0,0] A[0][0] B[0][0] Y[0][0] PE[0,1] - - - PE[1,0] - - - PE[1,1] - - - PE[0,0] A[0][1] B[1][0] Y[0][0] PE[0,1] A[0][0] B[0][1] Y[0][1] PE[1,0] A[1][0] B[0][0] Y[1][0] PE[1,1] - - - PE[0,0] A[0][2] B[2][0] Y[0][0] PE[0,1] A[0][1] B[1][1] Y[0][1] PE[1,0] A[1][1] B[1][0] Y[1][0] PE[1,1] A[1][0] B[0][1] Y[1][1] PE[0,0] A[0][3] B[3][0] Y[0][0] PE[0,1] A[0][2] B[2][1] Y[0][1] PE[1,0] A[1][2] B[2][0] Y[1][0] PE[1,1] A[1][1] B[1][1] Y[1][1] interconnect B[0][0] A[0][0] scratchpad without interconnect PE[0,0] B[0][0] space assignment of Y {PE[i,j] → Y[i,j]} Time assignment of Y {T[i+j+k] → Y[i,j]}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TENET OVERVIEW</head><p>TENET is an automatic framework as shown in Figure <ref type="figure" target="#fig_1">2</ref>, which takes a tensor operation written in C and hardware specification as inputs. Then, TENET automatically generates the relation-centric notation including dataflow, data assignment, interconnection, and spacetime-stamp map relations. After that, TENET calculates several key performance metrics to guide the dataflow and hardware optimization.</p><p>The dataflow relation relates each loop instance to a PE, which can be either generated automatically using design space exploration (DSE) or specified manually by the user. Data assignment relation between tensors and PEs is obtained by combining the access function of the tensor operation and the dataflow relation. The interconnection relation describes how the PEs are connected, which is derived based on the architecture. For a specific dataflow, TENET calculates multiple spacetime-stamp map relations, which will be used for performance metrics estimation. TENET also provides a repository that contains common spatial architectures such as mesh structure, systolic array, reduction tree, which feature with different PE functionalities and PE array topologies.</p><p>The expressivity of relation-centric notation can help the accelerator designers to explore the complete design space of the hardware dataflows and find the dataflows that meet their design constraints. Moreover, accelerator designers can rely on TENET to obtain critical performance metrics for a given dataflow including data reuse, PE utilization, and latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RELATION-CENTRIC NOTATION</head><p>TENET defines four relations, including the mapping of loop instances onto the PE array (Section IV-A), data assignment (Section IV-B), PE interconnection (Section IV-C), mapping between different spacetime-stamps (Section IV-D). Using the relation-centric notation, we can determine exactly where and when each loop instance is executed in the spatial architecture, where and when a tensor element is accessed, and how a tensor element is moved across PEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataflow Relation</head><p>Our dataflow relation applies affine transformation to define the space-stamp and time-stamp for the execution of tensor applications on a spatial architecture. The space-stamp is a relation that describes the PE coordinates where a loop instance is executed; and the time-stamp is a relation that determines the execution sequence when a loop instance is performed in a PE. Our notation is represented as follows.</p><p>Definition 1: Dataflow. Given a statement S with iteration domain D S and iteration vector n, the dataflow is defined as</p><formula xml:id="formula_7">Θ S,D = {S[ n] → (PE[ p] | T [ t])}, S[ n] ∈ Ds<label>(2)</label></formula><p>Θ S,D assigns loop instance S[ n] to a spacetime-stamp, which is a pair of space-stamp <ref type="bibr">(PE[ p]</ref>) and time-stamp (T [ t]). The spacestamp gives the coordinates of PE where S[ n] will be executed, and the time-stamp decides the execution sequence of S[ n].</p><p>The sequence is determined by the lexicographical order of two time-stamps. For simplicity, we use &gt; and &lt; to represent the lexicographically larger and smaller, respectively. p can be multi-dimensional depending on the PE array dimensions. t can also be multi-dimensional as the PE array size can be smaller than the total iteration domain.</p><p>Relation-centric notation can express various dataflows by employing affine transformation, as each dimension of the spacetime-stamp can be a linear transformation of multiple loop dimensions. An example of mapping GEMM on a systolic array is demonstrated in Figure <ref type="figure" target="#fig_5">3</ref>. We use the following relationcentric notation to describe the dataflow.</p><formula xml:id="formula_8">Θ S,D = {S[i, j, k] → (PE[i, j] | T [i + j + k])} In this example, loop instance S[i, j, k] is executed on PE[i, j],</formula><p>and is assigned a one-dimensional time-stamp (i + j + k). The time-stamp is an affine transformation of loop iterators. Given specific space and time constraints, we can find all the loop instances by solving the constraints. For example, at time-stamp T <ref type="bibr">[1]</ref>, the set of executed loop instances include</p><formula xml:id="formula_9">i + j + k = 1 constraint :0 ≤ i, j &lt; 2, 0 ≤ k &lt; 4 loop instances :[i, j, k] = [0, 0, 1], [1, 0, 0], [0, 1, 0]</formula><p>Dataflow Design Space. Relation-centric notation is more expressive than compute-and data-centric notations. To make a fair comparison, we assume each PE only has one MAC unit, and the PE array has 2 dimensions. Besides, we also assume that the size, offset parameters in data-centric notation are set to 1, and the coefficient of affine transformation in relation-centric notation is set to 1. Under these assumptions, the relationcentric notation enlarges the design space from O(n! n</p><p>2 ) [33] to O(2 n 2 ), where n is the number of loops. The former is the design space of MAESTRO <ref type="bibr" target="#b72">[33]</ref>, where n primitives can be arranged freely. Each of the primitive can be either SpatialMap or TemporalMap, and exactly two of them are SpatialMap. The later is the number of possible dataflows in relation-centric space. Each dataflow corresponds to an affine transformation, which can be represented using an n × n transformation matrix filled only with 0 or 1. For example, in GEMM, where n = 3, the design space size of MAESTRO is 3! × 3 = 18. In contrast, the design space of TENET is 2 9 = 512, which is 28× larger.</p><p>In practice, the loop bound can be much larger than the PE array size. Therefore, we support to use modulus and division operator in the affine transformation (quasi-affine transformation). For example,</p><formula xml:id="formula_10">Θ S,D ={S[i, j, k] → (PE[i mod 8, j mod 8] | T [i/8, j/8, (i mod 8 + j mod 8 + k)])}</formula><p>In this example, the PE array size is 8 × 8. As there are 8 indices of dimension i and j being processed in parallel, the time-stamp is 3-dimensional and we use modulus and division operators to represent the execution sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Assignment Relation</head><p>We use data assignment relations to specify the tensor elements that are accessed by a specific PE at a specific timestamp in the dataflow. As mentioned in Section II-B, we can use access function to relate the loop instances to its accessed tensor elements. Therefore, the data assignment relation can be formulated as a chain as follows.</p><p>Definition 2: Data assignment. Given a dataflow Θ S,D , the data assignment is defined as</p><formula xml:id="formula_11">A D,F = Θ −1 S,D .A S,F = {(PE[ p] | T [ t]) → F[ f ]}<label>(3)</label></formula><p>Fig. <ref type="figure">4</ref>: Different interconnect topologies.</p><p>In the example of Figure <ref type="figure" target="#fig_5">3</ref>, the data assignment of tensor Y can be represented as,</p><formula xml:id="formula_12">A D,F Y = {(PE[i, j] | T [i + j + k]) → Y [i, j]}</formula><p>For this case, we observe that each PE always calculate the same output tensor (Y ) at different timestamp, which means tensor Y [i, j] is kept stationary, and iteratively reused at different time-stamps until the computation is finished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Interconnection Relation</head><p>We also specify the PE interconnection using relations, which determines how the tensor elements are moved between different PEs.</p><p>Definition 3: PE Interconnection. Given a PE array, the interconnection is a set, where each element describes a mapping from one PE to another PE.</p><formula xml:id="formula_13">I PE 1 ,PE 2 = {PE[ p 1 ] → PE[ p 2 ]} : c 1 , • • • , c k<label>(4)</label></formula><p>where p 1 and p 2 denote the coordinate of PEs that are connected, and c 1 , • • • , c k are the conditions used to specify the topology. The interconnection relation helps to describe the possible data movement in the dataflow. Figure <ref type="figure" target="#fig_5">3</ref> depicts the interconnection specification of a 2D systolic data transfer. The PE is able to reuse the input tensor from adjacent PEs using this interconnection, otherwise, the data must be accessed from the scratchpad.</p><p>In this paper, we model three widely used interconnect topologies in Figure <ref type="figure">4</ref> as follows.</p><formula xml:id="formula_14">Interconnection : {PE[i, j] → PE[i , j ]} 2D-systolic : (i = i, j = j + 1) or (i = i + 1, j = j) Mesh : abs(i − i) ≤ 1 and abs( j − j) ≤ 1 1D-Multicast : abs(i − i) ≤ 3 (4 PEs)</formula><p>The systolic interconnect is widely used in recent GEMM and CONV accelerators like TPU <ref type="bibr" target="#b61">[22]</ref>. Mesh NoCs are applied in DySER <ref type="bibr" target="#b58">[19]</ref> and Plasticine <ref type="bibr" target="#b81">[42]</ref>. In a multicast network, PEs are connected through wires that share the same input entry. Multicast networks are used in Eyeriss <ref type="bibr" target="#b49">[10,</ref><ref type="bibr" target="#b50">11]</ref> and vector dot-product units like Diannao <ref type="bibr">[8]</ref>. Different from the first two interconnects, the data reuse of multicast occurs at the same cycle. Details of how interconnection relation affects dataflow modeling will be discussed in Section V-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Spacetime-stamp Map Relation</head><p>To compute various performance metrics, we need to build relations between different spacetime-stamps. More clearly, by using data assignment and interconnection relations, we can correlate different spacetime-stamps based on the accessed data elements and their movement.  </p><formula xml:id="formula_15">M D,D = {([PE[ p 1 ] | T [ t 1 ]]) → ([PE[ p 2 ] | T [ t 2 ]])}<label>(5)</label></formula><p>Given a dataflow, the spacetime-map helps to link different time-stamps and space-stamps so that we can model the hardware behavior in continuous space-stamps and time-stamps. Furthermore, by identifying which data are used by these spacetime-stamps using data assignment relation, we can detect data reuse both spatially and temporally. Assuming the time distance of D and D is within 1, and the PE specified by D and D are interconnected, we write three spacetime-stamp maps in the example of Figure <ref type="figure" target="#fig_5">3</ref> as follows.</p><formula xml:id="formula_16">map 1. ([PE[0, 0] | T [0]) → ([PE[0, 0] | T [1]) map 2. ([PE[0, 0] | T [0]) → ([PE[0, 1] | T [1]) map 3. ([PE[0, 0] | T [0]) → ([PE[1, 0] | T [1])<label>(6)</label></formula><p>For the map 1, by examining the behavior of tensor Y with the assignment relation A D,F Y , we can observe that map 1.</p><formula xml:id="formula_17">([PE[0, 0] | T [0]) → Y [0, 0] ([PE[0, 0] | T [1]) → Y [0, 0]</formula><p>which means, in this map, tensor element Y [0, 0] is reused in the same PE but at different time-stamps. Similarly, we can check the reuse of tensor A and tensor B in the other two maps using A D,F A and A D,F B , respectively.</p><formula xml:id="formula_18">map 2. ([PE[0, 0] | T [0]) → A[0, 0] ([PE[0, 1] | T [1]) → A[0, 0] map 3. ([PE[0, 0] | T [0]) → B[0, 0] ([PE[1, 0] | T [1]) → B[0, 0]</formula><p>map 2 and map 3 describe that tensor element A[0, 0] traverses across the PE array horizontally and tensor element B[0, 0] traverses vertically, respectively.</p><p>In order to estimate various performance metrics such as data reuse, we can apply further restrictions to spacetime-map. For example, on the one hand, by restricting D and D containing the same PE (e.g. map 1), we can capture the temporal data reuse. On the other hand, by restricting D and D containing the interconnected PEs (e.g. map 2 and map 3), we can compute spatial data reuse. </p><formula xml:id="formula_19">TotalVolume TotalVolume = sum(A D,F ) ReuseVolume ReuseVolume = sum(A D,F ∩ M −1 D,D .A D,F ) UniqueVolume UniqueVolume = TotalVolume − ReuseVolume ReuseFactor ReuseFactor = TotalVolume/UniqueVolume V. PERFORMANCE MODEL</formula><p>Based on relation-centric notation, we can compute various performance metrics precisely. The formulation of each metric is naturally modeled as a set operation between relations. In this section, we present how to compute the performance metrics, including data transfer, data reuse, latency, and energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Reuse and Volume Model</head><p>The modeling for hardware metrics starts with defining several data volumes. The value of a specific volume is calculated using sum function (denoted as sum()), which counts the cardinality of the set. Table <ref type="table" target="#tab_3">II</ref> presents the formulas for these metrics.</p><p>TotalVolume is the total number of the tensor data accesses through the entire spacetime-stamp. As shown in Figure <ref type="figure" target="#fig_6">5</ref> (a), TotalVolume sums up the volume of all the tensor data accesses that are used across the entire iteration domain. This metric gives the maximum data size that is required to transfer between the PE and the scratchpad. For example, the TotalVolume of tensor A in Figure <ref type="figure" target="#fig_5">3</ref> can be calculated as,</p><formula xml:id="formula_20">time-stamp 0. used data A[0][0] time-stamp 1. used data A[0][1], A[0][0] A[1][0] time-stamp 2. used data A[0][2], A[0][1], A[1][1], A[1][0] time-stamp 3. used data A[0][3], A[0][2], A[1][2], A[1][1] TotalVolume = 1 + 3 + 4 + 4 = 12</formula><p>ReuseVolume is the number of reused data across multiple spacetime-stamps. For example, in Figure <ref type="figure" target="#fig_6">5 (a)</ref>, ReuseVolume sums up the volume of data that are overlapped between two adjacent spacetime-stamps. The spacetime-map M D,D determines the adjacent spacetime-stamps, where the spacestamp map is restricted by the interconnection relation. Only the two stamps meet the constraints in Equation 4 will contribute to data reuse. The time-stamp constraint is defined as a time interval, within which the data reuse can occur. For example, let us assume the interconnect is a systolic topology and the time interval is 1. From time-stamp 1 to time-stamp 3, the ReuseVolume of tensor A in Figure <ref type="figure" target="#fig_5">3</ref> can be calculated as time-stamp 1. reused data from time-stamp 0</p><formula xml:id="formula_21">A[0][0] time-stamp 2. reused data from time-stamp 1 A[0][1], A[1][0] time-stamp 3. reused data from time-stamp 2 A[0][2], A[1][1] ReuseVolume = 1 + 2 + 2 = 5</formula><p>For multicast interconnection relation, the time interval constraint is set to 0 as data are reused via wires.</p><p>UniqueVolume is the number of unique tensor data that are accessed. As shown in Figure <ref type="figure" target="#fig_6">5</ref>(a), the required tensor data expands as the time-stamp increases. UniqueVolume sums up the volume of "new" data at different spacetime-stamps. A data assigned to a certain spacetime-stamp is considered unique if it cannot be fetched from adjacent spacetime-stamps, where the adjacency is also determined by the spacetime-map (same constraints in M D,D of ReuseVolume). This metric gives the minimum data size that is required to transfer between the PE and the scratchpad. For example, from time-stamp 0 to time-stamp 3, the UniqueVolume of tensor A in Figure <ref type="figure" target="#fig_5">3</ref> can be calculated as,</p><formula xml:id="formula_22">time-stamp 0. new data A[0][0] time-stamp 1. new data A[0][1], A[1][0] time-stamp 2. new data A[0][2], A[1][1] time-stamp 3. new data A[0][3], A[1][2] UniqueVolume = 1 + 2 + 2 + 2 = 7</formula><p>ReuseFactor describes how many times on average a data is reused once it is fetched from scratchpad memory.</p><p>SpatialReuseVolume is the amount of data reuse across multiple space-stamps. As shown in Figure <ref type="figure" target="#fig_6">5</ref>(b), the spatial reuse originates from broadcasting the tensor data to multiple PEs. The SpatialReuseVolume sums up the volume of data with spatial reuse at different space-stamps, where D and D contain interconnected PEs only.</p><p>TemporalReuseVolume is the amount of data reuse across multiple time-stamps within the same PE. As shown in Figure <ref type="figure" target="#fig_6">5</ref>(c), temporal reuse means that the data is reused across time-stamps. To avoid overlap between SpatialReuseVolume and TemporalReuseVolume, we restrict the TemporalReuseVolume that it only refers to the temporal reuse at the same PE, where D and D contain the same PE. Clearly, ReuseVolume is the sum of SpatialReuseVolume and TemporalReuseVolume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Latency and Bandwidth Model</head><p>The latency of dataflow consists of communication and computation delay in each PE. We assume buffers, networks and arithmetic units work in a pipelined fashion, and techniques such as double buffering are used to hide latency. Then, the overall latency is just the maximum of communication delay and computation delay.</p><p>To model communication delay, we calculate the volume of data that need to be transferred between the on-chip scratchpad and local registers. If a PE can fetch data from itself or its neighboring PEs, then fetching data from the scratchpad can be avoided, which requires more energy and longer latency.</p><p>Therefore, the volume of data transferred from the scratchpad to registers is estimated by the UniqueVolume of all input tensors. The volume of data sent from output registers to the scratchpad is estimated by the UniqueVolume metric of output tensors. The communication delays are estimated by </p><p>Note that the bandwidth here is the available scratchpad bandwidth.</p><p>The compute delay is estimated by the total number of loop instances and the average number of active PEs.</p><formula xml:id="formula_24">Delay compute = sum(D S ) Util PE × PE size<label>(8)</label></formula><p>where Util PE is the average PE utilization across all timestamps.</p><p>Using the computation delay, we can also calculate two types of bandwidth requirement, namely, interconnection bandwidth (IBW) and scratchpad bandwidth (SBW). IBW refers to the data communication among PEs, and its is estimated as follows,</p><formula xml:id="formula_25">IBW = SpatialReuseVolume Delay compute<label>(9)</label></formula><p>SBW refers to the data transfer between the PE array and the scratchpad, and SBW requirement is estimated by normalizing the UniqueVolume to the computation latency.</p><formula xml:id="formula_26">SBW = UniqueVolume Delay compute<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Implementation</head><p>The performance analysis is written in C++, which leverages the ISL library and Barvinok Library to perform integer set operations <ref type="bibr" target="#b90">[51,</ref><ref type="bibr" target="#b91">52]</ref>. More specifically, we use operators from ISL and Barvinok library to calculate the metrics discussed above. Each relation (e.g., dataflow, interconnection, and data assignment) is implemented using isl_union_map structures in ISL. isl_union_map_reverse calculates the reverse of an operation (e.g., get Θ −1 S,D from Θ S,D ), isl_union_map_apply_range composites two relations (e.g., Equation <ref type="formula" target="#formula_13">4</ref>). isl_union_map_card and isl_union_pw_qpolynomial_sum are operators used to summing over time-stamps to calculate metrics such as TotalVolume and UniqueVolume (e.g., Equation <ref type="formula" target="#formula_24">8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Setup</head><p>Benchmarks. We first evaluate TENET with five important tensor kernels, i.e., GEMM, 2D-CONV, Matrix multiplication chain (MMc), Matricized tensor times Khatri-Rao product (MTTKRP), and Jacobi-2D. GEMM 2D-CONV, and MMc are widely used in deep learning, scientific and engineering computations. MTTKRP tensor operation is the bottleneck operation in tensor factorization (e.g., recommender system). Jacobi-2D is a two-dimensional stencil operation that is often used in image processing.</p><formula xml:id="formula_27">MTTKRP (IJ-P | J,IJL-T) {S[i,j,k,l] → PE[i%8,j%8]} {S[i,j,k,l] → T[k,fl(i/8),fl(j/8),i%8+j%8+l]} × (KJ-P | J,KJL-T) {S[i,j,k,l] → PE[k%8,j%8]} {S[i,j,k,l] → T[i,fl(k/8),fl(j/8),k%8+j%8+l]} × (KL-P | L,KLJ-T) {S[i,j,k,l] → PE[k%8,l%8]} {S[i,j,k,l] → T[i,fl(k/8),fl(l/8),k%8+l%8+j]} × Jacobi-2D (I-P | I,J-T) {S[i,j] → PE[i%64]} {S[i,j,k,l] → T[fl(i/</formula><formula xml:id="formula_28">2D-CONV Y (k, ox, oy) = A(c, ox + rx, oy + ry)B(k, c, rx, ry) GEMM Y (i, j) = A(i, k)B(k, j) MTTKRP Y (i, j) = A(i, k, l)B(k, j)C(l, j) MMc Y (i, j) = A(i, k)B(k, l)C(l, j) Jacobi-2D Y (i, j) = ((A(i, j) + A(i − 1, j) + A(i, j − 1) +A(i + 1, j) + A(i, j + 1)) / 5</formula><p>We also evaluate four real-world large-scale tensor applications as shown in Table <ref type="table" target="#tab_4">IV</ref>. GoogLeNet and MobileNet are the state-of-the-art neural networks in deep learning domain, which require GEMM and 2D-CONV. In Alternating least squares (ALS), MTTKRP is a core operation. Transformer is a well-known network architecture for translation tasks, which features a MMc operation.</p><p>Comparison. We compare TENET with MAESTRO, which is the state-of-the-art data-centric notation with a comprehensive performance analysis. The compute-centric notation <ref type="bibr">[14, 39,</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataflow Comparison</head><p>Table III lists 20 popular dataflows that we use to evaluate our framework. The dataflows are named according to the For the dataflow with affine transformation, we put the transformed dimensions together. For example, in the (IJ − P | J, IJK − T) dataflow of GEMM, the last dimension of time-stamp is given by (i + j + k). We also write the dataflows using data-centric notation if supported. Figure <ref type="figure">6</ref> presents two use cases of TENET.  Figure <ref type="figure">7</ref> shows the latency and bandwidth requirement of  the optimal MAESTRO dataflow and TENET dataflow using the benchmarks in Table <ref type="table" target="#tab_4">IV</ref>. The latency is normalized to the ideal latency with theoretical performance (calculated as: # of multipliers × frequency). The bandwidth is estimated by normalizing the UniqueVolume to the computation latency. MAESTRO model cannot provide the results for the complete ALS and Transform application due to some unsupported operators. We only present TENET results for these two cases.</p><p>Overall, TENET shows 74% and 22% latency reduction, and reduces the bandwidth requirement by 63% and 54% for GoogleNet and MobileNet, respectively. Figure <ref type="figure" target="#fig_12">8</ref> compares the execution time of TENET and MAS-TERO for modeling a single dataflow. The test is conducted on a PC with a 2-core 2.50GHz Intel R Core TM i5-7200U CPU and 8GB memory. On average, the modeling time of a single dataflow is 10 −2 second for MAESTRO, and 10 −1 second for TENET. The difference mainly comes from the fact that TENET models the dataflow as an integer linear programming problem, and considers more architectural details in the evaluation (e.g., interconnection, data assignment). However, MAESTRO calculates metrics using simple polynomials, which is faster but might lead to inaccurate estimation (Section VI-E). We also observe that the modeling time increases for a more complex interconnect, while it is less sensitive to PE array sizes.</p><p>Design space exploration. The dataflow design space of TENET is huge. We propose to prune the space by restricting the data movement and assignment relations. We first enumerate the possible data movement supported by the PE interconnections, e.g., systolic, multicast for each tensor. Since the data movement is always rectilinear using affine transformation, we then enumerate possible data assignment for the boundary PEs. This together will determine a complete dataflow. Taking 2D-CONV (6 loops) as an example, we only enumerate 12 legal data movements for tensor A and B, respectively. Then, we limit the possible data assignment for the boundary PEs to 180. This results in 12 × 12 × 180 = 25920 different dataflows in total. To explore this design space, TENET takes less than one hour. We leave a more efficient design space exploration as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Metrics Evaluation</head><p>Figure <ref type="figure" target="#fig_13">9</ref> shows the evaluation results of five critical metrics: temporal and spatial data reuse of input and output tensors, maximum PE utilization, average PE utilization, and latency. Temporal and spatial data reuse are calculated by normalizing SpatialReuseVolume and TemporalReuseVolume to the number of instances, respectively. In this evaluation, the systolic interconnection topology is applied to all the dataflows.</p><p>As listed in Table <ref type="table" target="#tab_3">III</ref>, we evaluate eight dataflows for 2D-CONV. Different dataflows have high variation in reuse, PE utilization and latency. All these dataflows exhibit temporal and spatial data reuse. However, high data reuse does not necessarily lead to low latency. We observe long latency from the (R Y O Y − P | O Y , O X − T) dataflow. This is because the dimension size of R Y cannot match the PE array size, and therefore leads to low PE utilization. We also observe that (C − P | O Y , O X − T) has high PE utilization but long latency. This is because input-A has low reuse, and therefore loading the input tensor increases the overall latency. Among all 2D-CONV dataflows, the last three have the lowest latency as they utilize all the PEs and have high data reuse for all tensors. For GEMM benchmark, we compare five dataflows. The dataflows that contain two-dimensional space-stamp (i. Noting that a good dataflow needs to provide both high PE utilization and data reuse. Dataflows that have poor performance usually fail in one of these two aspects. The results also demonstrate that our framework is capable of capturing spatial and temporal reuse separately. For example, in the (IJ − P | J, IJK − T) dataflow of GEMM, we observe high spatial reuse but little temporal reuse for tensor A and B, since they flow through the PE array. On the other hand, there is high temporal reuse but little spatial reuse for tensor Y, since it is kept stationary in the PE array.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Bandwidth Analysis</head><p>We also evaluate the bandwidth requirement of three interconnect topologies: 1D-systolic, 2D-systolic, and mesh. When modeling these interconnects, we always assume there exist multicast wires to broadcast the same data to different PEs in the same cycle. In Figure <ref type="figure" target="#fig_0">10</ref>, we select five 2D-CONV dataflows that exhibit different types of data reuse. Overall, the three evaluated topologies show similar bandwidth requirement for the same dataflow. This is because tensor applications typically have regular computation and data access patterns, where long-distance communication across PEs is rarely required. Among the four benchmarks, Jacobi-2D is less computation-intensive and requires more scratchpad bandwidth than the others. We also find that the bandwidth requirement comes from different tensors as the dataflow changes. GEMM dataflows illustrate this phenomenon in particular. The dataflow (IJ − P | J, IJK − T) maximizes the output reuse by keeping the output stationary in the PE. Therefore, the bandwidth requirement mainly results from the input tensor.</p><p>Comparing the three topologies, we also observe some variations. For (R Y O Y − P|O Y , O X − T) 2D-CONV dataflow and Jacobi-2D dataflow, the mesh topology provides higher interconnection bandwidth and lower scratchpad bandwidth for the input tensor. These two dataflows exhibit diagonal input data reuse, which cannot be supported by both systolic topologies. Since both dataflows can leverage the mesh NoC to increase data reuse, the scratchpad bandwidth requirement is hence reduced. For (R Y O Y −P|O Y O X −T) 2D-CONV dataflow, we observe that 1D systolic interconnect shows much lower IBW than 2D systolic interconnect, but similar SBW. The saving of IBW comes from the fact that the tensor B is stationary in the PE across different time-stamps. In summary, an interconnection network that connects more PEs does not necessarily reduce scratchpad bandwidth requirement. The design of the interconnection network needs to take the data movement patterns into account. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Metrics Estimation Comparison</head><p>In this section, we compare TENET with MAESTRO on the accuracy of latency, PE utilization, and reuse estimation. We show that our proposed model improves the metric estimation accuracy. Following previous deep learning accelerators, in 2D-CONV, we term tensor A as input, tensor B as filter, dimension K as output channel, dimension C as input channel.</p><p>Latency Comparison. We compare the latency calculated by our work and MAESTRO using the row-stationary dataflow proposed by Eyeriss <ref type="bibr" target="#b49">[10]</ref> for AlexNet, and using the dataflow proposed by MAERI <ref type="bibr" target="#b65">[26]</ref> for VGG. We use the reported latency and PE utilization in Eyeriss and MAERI as the golden result.</p><p>For MAESTRO implementation, we use the same input files as in <ref type="bibr" target="#b63">[24]</ref>. Figure <ref type="figure" target="#fig_0">11</ref> shows the comparison results. Overall, our work improves the latency estimation accuracy from 71.9% to 89.6% for Eyeriss, and from 92.3% to 96.3% for MAERI.</p><p>The improvement is partially due to TENET's capability of modeling complex dataflow using affine transformation. In Alexnet, the size of filter B(k, c, rx, ry) varies from (96, 3, 11, 11) to (384, 256, 3, 3). To increase the PE utilization, Eyeriss distributes the dimension ry and c to 12 PEs in one column. Relation-centric notation supports this feature by assigning a space-stamp with the affine transformation (ry + 3 * (c%4)) to the column of the PE array. However, MAESTRO only models the case when c = 0 due to the limitation of data-centric notation. Such limitation explains the large error in the last three layers where ry = 3. We also compare with another Eyeriss dataflow using data-centric notation (denoted as MAESTRO*), which is different from the notation in <ref type="bibr" target="#b63">[24]</ref>. In MAESTRO*, the cluster primitives are used to merge the multiple channel dimensions so that PEs can be better utilized. The accuracy of MAESTRO* is 89.0%.</p><p>MAERI features a reconfigurable reduction tree design, which can be configured to enable multiple vector dot-products with different sizes. For the reduction tree dataflow, only multipliers are considered as PEs and PEs are connected via multicast interconnection. TENET applies affine transformation to denote MAERI dataflow as it assigns multiple dimensions to the 1D PE array. In the implementation of MAESTRO, we also manually transform two dimensions of the filter and the input to fit the 1D PE array of MAERI. TENET improves the accuracy thanks to more accurate reuse analysis.</p><p>PE Utilization Comparison. TENET estimates the PE utilization accurately, which is another key factor that affects the latency, as shown in Equation <ref type="formula" target="#formula_24">8</ref>. We also compare the average PE utilization rate with MAESTRO. Since Eyeriss did not report this metric directly, we approximate the ground-truth value using MACNum PENum×Latency which is obtained from Eyeriss. As shown in Figure <ref type="figure" target="#fig_0">11</ref>  Reuse factor two layers, but our work makes much more accurate predictions in the following three layers. Unlike previous approaches that estimate PE utilization via a polynomial composed of PE array size and total computation size, TENET models the PE utilization by going through all time-stamps to check whether a PE is assigned. Reuse Comparison. TENET can precisely calculate the ReuseFactor that affects the latency for data transfer. We evaluate the reuse analysis of MAESTRO and our work on four state-of-the-art DNN models: AlexNet, VGG16, GoogleNET, and MobilleNet. We select different dataflows for each model. For AlexNet, we use the row-stationary dataflow on a 12 × 14 PE array, motivated by Eyeriss <ref type="bibr" target="#b49">[10]</ref>. For VGG16, we use the output stationary dataflow on a 8 × 8 PE array, motivated by Shidiannao <ref type="bibr" target="#b54">[15]</ref>. We do not normalize the dataflows to be executed on a 8 × 8 PE array here because our goal is to accurately capture the characteristics of original dataflows instead of comparing between them.</p><formula xml:id="formula_29">(O Y O X -P | K, C-T)-MobileNet CONV1-1 CONV2-1 CONV3-1 CONV4-1 CONV5-1<label>10</label></formula><p>The first case is the CONV3 layer of AlexNet. In this layer, Eyeriss maps R Y and C dimensions into dimension 1 of the PE array to make full use of PEs, and O Y into dimension 2 of the PE array. The filter array has a spatial reuse factor of 13 (size of O Y ) since it is reused across PEs horizontally. Besides, the filter array stays stationary in the innermost dimension of time-stamp (O X ), which contributes a temporal reuse factor of 13 (size of O X ). Therefore the reuse factor should be 13 × 13 = 169, which is accurately calculated by our work (169) compared to MAESTRO (676). For output array, since all PEs in a vertical line shares the same output data, it has a spatial reuse factor of 12 (size of dimension 1 of PE array). Moreover, there is a temporal reuse factor of 12 since each PE processes 3 filter width (R X ) and 4 input channels (C) continuously, leading to a total reuse factor of 12 × 12 = 144, which is also accurately calculated by our work. On the other hand, MAESTRO reports no reuse for the output array in all circumstances, which is likely because the dimensions of output array (O X , O Y ) are not explicitly specified by the primitives.</p><p>Similar inaccuracy is observed on VGG16, GoogleNet, and MobileNet as shown in Figure <ref type="figure" target="#fig_17">12</ref>. For example, the filter reuse in inception-4a calculated by TENET is 3136 while it is 2916 from MAESTRO. The actual filter reuse depends on the input image size, which is 56 × 56 = 3136. In MobileNet, there are two special convolutional layers, namely depthwise convolution (dw-CONV) and pointwise convolution (pw-CONV). In the depthwise convolution, results from different input channels are directly stored as the outputs without accumulation. Therefore, this layer shows lower input data reuse. The pointwise convolution uses 1 × 1 filter size, which leads to no reuse for input data. MAESTRO is less accurate in data reuse estimation for three reasons. First, the simple formulas used by MAESTRO require all data dimensions being explicitly specified by the primitives, and hence no reuse is reported for output array in all cases. Second, MAESTRO only analyzes data reuse for the innermost temporal and spatial dimensions, while our work can analyze data reuse for multiple time dimensions. Third, MAESTRO does not support mapping multiple data dimensions on a single PE dimension, which is a technique applied by Eyeriss to fill the PE array.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORKS</head><p>Notations for expressing the dataflow. Compute-centric notation is widely adopted as it can directly represents dataflow in high-level languages using directives <ref type="bibr" target="#b78">[39,</ref><ref type="bibr" target="#b95">56]</ref>. In <ref type="bibr" target="#b53">[14]</ref>, dataflow is notated using two hyperplanes with polyhedral dependency graph. Timeloop <ref type="bibr" target="#b78">[39]</ref> describes the design space using a concise and unified loop representation with mapping directives. The mapping directives consist of memory constraints and PE workload assignment. Interstellar <ref type="bibr" target="#b95">[56]</ref> represents the hardware dataflow using Halide's scheduling language <ref type="bibr" target="#b84">[45]</ref> for design exploration. Interstellar extends Halide with additional control directives, e.g., loop blocking and resource allocation, for specifying the hardware features. Recently, Kwon et al. <ref type="bibr" target="#b63">[24]</ref> propose a data-centric notation that used spatial map and temporal map to specify the dataflow. Space-time transformation theory is widely used for systolic architectures <ref type="bibr">[4,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b52">13,</ref><ref type="bibr" target="#b66">27,</ref><ref type="bibr" target="#b67">28,</ref><ref type="bibr" target="#b74">35]</ref>. There are also studies mainly focusing on the arithmetic properties, such as array shape transformation, partitioning of the space coordinate for systolic arrays <ref type="bibr">[4,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b67">28,</ref><ref type="bibr" target="#b74">35]</ref>. Recent efforts attempt to apply this theory for automatic FPGA code generation <ref type="bibr" target="#b52">[13,</ref><ref type="bibr" target="#b66">27]</ref>. PolySA <ref type="bibr" target="#b52">[13]</ref> and AutoSA <ref type="bibr" target="#b93">[54]</ref> compile applications into polyhedral IR and map them on systolic arrays. SuSy <ref type="bibr" target="#b66">[27]</ref> combines polyhedral with hardware optimization primitives to generate high-performance systolic array on FPGAs.</p><p>Modeling and optimization. Performance modeling can provide general guidelines and insights for optimizing the dataflow. Prior performance models mainly focus on the DNN applications on spatial architectures <ref type="bibr" target="#b50">[11,</ref><ref type="bibr" target="#b64">25,</ref><ref type="bibr" target="#b71">32,</ref><ref type="bibr" target="#b95">56]</ref>. Eyeriss-v2 <ref type="bibr" target="#b50">[11]</ref> analyzes data reuse with different tensor shapes and sizes. However, it lacks a performance model to evaluate the energy and performance. MAESTRO <ref type="bibr" target="#b64">[25]</ref> applies data-centric notation and analyzes data reuse, energy, and latency. Interstellar <ref type="bibr" target="#b95">[56]</ref> explores the design space using loop operators like loop split and loop reorder. There are also prior works aiming at modeling the spatial architecture for general applications <ref type="bibr" target="#b68">[29,</ref><ref type="bibr" target="#b76">37,</ref><ref type="bibr" target="#b80">41]</ref>. Nowatzki et al. <ref type="bibr" target="#b76">[37]</ref> proposed a constraint-centric scheduling algorithm that determines the placement and routing of reconfigurable architectures. <ref type="bibr" target="#b80">[41]</ref> represents the dataflow on the computation graph with edgecentric approach for application mapping. Recently, Jia et al. proposed TensorLib that built parameterized hardware module templates based on relation-centric notation <ref type="bibr" target="#b60">[21]</ref>. By feeding the dataflow found by Tenet to TensorLib, we can automatically generate the dataflow hardware written in Chisel <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this work, we propose a relation-centric notation that represents dataflow, data assignment and interconnection uniformly as relations. The relation-centric notation supports a larger dataflow design space and more tensor benchmarks compared to prior notations. We also present an analytical framework to accurately estimate data reuse, bandwidth requirement, latency and energy. We evaluate our framework on four tensor benchmarks and results show that the relation-centric notation can find more sophisticated dataflows with lower latency compared to the state-of-the-art data-centric notation. TENET achieves 37% to 51% latency reduction compared to the stateof-the-art techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Limitation of compute-centric and data-centric notation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: TENET automatic flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b), the parallelism is specified using parallel directive. Data-centric notation explicitly allocates data to PEs with two key primitives, including Spatial Map and Temporal Map. Spatial Map assigns a certain dimension to the PE array, and Temporal Map specifies the data movement in a certain dimension across different time-stamps. In Figure 1(b), spatial map (1,1) i means distributing the dimension of the output data across different PEs. temporal map (1,1) j distributes the data involved in dimension j across different time-stamp within the same PE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Analyzing the dataflow of matrix multiplication on a 2 × 2 PE array using the relation-centric notation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Volume metrics. Definition 4: Spacetime-map. Given a dataflow, the spacetime-map is a relation set that maps a set of spacetimestamp D to another set of spacetime stamp D . Both D and D are derived from the given dataflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>64),j]} × (IJ-P | I,J-T) {S[i,j] → PE[i%8,j%8]} {S[i,j] → T[fl(i/8),fl(j/8)]} × MMc (IJ-P | J,IJL-T) {S[i,j,k,l] → PE[i%8,j%8]} {S[i,j,k,l] → T[k,fl(i/8),fl(j/8),i%8+j%8+l]} × (KJ-P | J,KJL-T) {S[i,j,k,l] → PE[k%8,j%8]} {S[i,j,k,l] → T[i,fl(k/8),fl(j/8),k%8+j%8+l]} ×</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Fig. 6: Dataflow comparision for data-centric and relation-centric notations under different bandwidth requirement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 6(a) presents the results of 2D-CONV. Two dataflows (KC − P | O Y , KCO X − T) (KO X − P | O Y , KO X C − T) cannot be represented in data-centric notation as they require affine transformation, which means relation-centric notation is more expressive than data-centric notation.As a result, relationcentric notation opens up more opportunities for a larger exploration of dataflows, which is essential for designing efficient spatial architectures. In Figure6, the dataflow depicted in blue lines are the optimal dataflows in data-centric notation. When the bandwidth is 160 bit/cycle, the latency of our dataflow is 17.5% lower than data-centric dataflow. When the bandwidth decreases, the dataflow (KC − P | O Y , KCO X − T) achieves up to 47.4% latency reduction compared with the optimal dataflow in data-centric design space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 6 (b) presents the results of GEMM, where two dataflows (KJ − P | K, IJK − T) (IJ − P | J, IJK − T) cannot be represented using data-centric notation. When the bandwidth is 64 bit/cycle, the dataflow (IJ − P | J, IJK − T) achieves up to 77% latency reduction compared with the dataflow in data-centric design space. Overall, TENET achieves 37.4% and 51.4% latency improvement on average compared with the data-centric notation by identifying more sophisticated dataflows for 2D-CONV and GEMM, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Runtime comparison of TENET and MAESTRO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Critical metrics analysis for different tensor applications.</figDesc><graphic url="image-37.png" coords="10,400.48,160.64,125.58,51.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>e., (IJ−P | J, IJK− T), (KJ − P | K, IJK − T), and (IK − P | K, IJK − T)) always outperform the ones with one-dimensional space-stamp (i.e., (K − P | I, J − T) and (J − P | I, K − T)) since two-dimensional space-stamp exposes more data reuse opportunities. All these dataflows show high PE utilization while (IK − P | K, IJK − T) and (KJ − P | K, IJK − T) have the lowest latency, as they have high input reuse on both input tensors. For MTTKRP benchmark, the three dataflows have similar performance, since they all have high PE utilization and large reuse factor on all tensors. Nevertheless, the (IJ − P | J, IJL − T) dataflow has higher reuse on input tensors hence outperforms the others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Fig. 10: Bandwidth analysis for different interconnection topologies.</figDesc><graphic url="image-56.png" coords="11,216.08,315.52,58.07,65.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(b), both frameworks predict the same at the first</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>(Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Data reuse comparison with MAESTRO.</figDesc><graphic url="image-84.png" coords="12,107.67,274.21,161.71,50.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison between four notations</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Details of volume calculation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Real-world large-scale tensor applications.</figDesc><table><row><cell>Application GoogLeNet[49] MobileNet[20] ALS[5] Transformer[50]</cell><cell>Domain Deep learning Deep learning Matrix fabrication NLP</cell><cell>Tensor operation 2D-CONV 2D-CONV MTTKRP MMc</cell><cell>Data size 6.7M params 3 layer types 4.2M params 4 layer types 480K× 18K× 2K 512,768,1024</cell></row><row><cell cols="4">56] has the same level of expressiveness as data-centric notation but lacks of performance models. More clearly, we compare TENET with MAESTRO using GEMM and 2D-CONV benchmarks, as shown in Table III. We run the open source code of MAESTRO. We configure the two frameworks to use the same parameters (PE Number, Bandwidth, Buffer Size). For the interconnection network, we use a mesh network, since MAESTRO models a hierarchical PE array with the assumption that each PE can communicate with adjacent PEs.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">These authors contributed equally. + Work done while the author was a student at Peking University. *Corresponding Author</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>160 IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW IBW SBW 2D-sys mesh 1D-sys 2D-sys mesh 1D-sys 2D-sys mesh 1D-sys 2D-sys mesh 1D-sys 2D-sys mesh 1D-sys (RYOY-P | OYOX-T) (OXOY-P | OX,C-T) (OYOX-P | OY,OX-T) (OXOY-P | C,RX-T) (KC-P | OY,OX-T)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by the Beijing Natural Science Foundation (No. JQ19014) and in part by the Beijing Academy of Artificial Intelligence (BAAI). We would like to thank Tushar Krishna for sharing the source code of MASTERO.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) K</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R X ),1) X</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R Y ),1) Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R Y ); Sz(R Y )) R Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sz(R X )</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C-P | O Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">O X -T)</forename><surname>{s[k</surname></persName>
		</author>
		<author>
			<persName><surname>Pe</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>c%64]} {S[k,c,ox,oy,rx,ry] → T[fl(c/64),k,oy,ox]} 1. SpMap(1,1) C</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) K</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R Y ),1) Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R X ),1) X</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R Y ); Sz(R Y )) R Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sz(R X )</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Motivated by Eyeriss[10] {S[k,c,ox,oy,rx,ry] → PE</title>
		<author>
			<persName><forename type="first">R</forename><surname>Y O Y -P | O Y</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O X -T)</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ry+3*(c%4),oy]} {S[k,c,ox,oy,rx,ry] → T[fl(k/16),fl(c/16),ox]} 1. TpMap(4,4) C</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R Y ),1) Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R X ),1) X</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cluster</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R Y ),P)</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) K</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) R Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O X -T ;</forename><surname>O Y O X -P | O Y</surname></persName>
		</author>
		<author>
			<persName><surname>{s[k</surname></persName>
		</author>
		<author>
			<persName><surname>Pe</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>oy%8,ox%8]} {S[k,c,ox,oy,rx,ry] → T[k,c,fl(oy/8),fl(ox/8)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) K</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R Y ), 1) Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>10,8) X</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap ; Sz(r Y )</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Sz</surname></persName>
		</author>
		<author>
			<persName><surname>R Y )) R Y ;</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap ; Sz(r X )</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Sz</surname></persName>
		</author>
		<author>
			<persName><surname>R X )) R X ;</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName><surname>Cluster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R X ), 1) X</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Motivated by NVIDIA[38] {S[k,c,ox,oy,rx,ry] → PE</title>
		<author>
			<persName><forename type="first">O X -T)</forename><surname>Kc-P | O Y</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>k%8,c%8]} {S[k,c,ox,oy,rx,ry] → T[fl(k/8),fl(c/8),oy,ox]} 1. SpMap(1,1) K</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>8,8) C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R Y ); Sz(R Y )) R Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R X ); Sz(R X )) R X</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R Y ),1) Y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tpmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sz(R X ),1) X</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName><surname>Cluster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spmap</surname></persName>
		</author>
		<imprint>
			<pubPlace>1,1) C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tensor decompositions for learning latent variable models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Chisel: constructing hardware in a scala embedded language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Avižienis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Design Automation Conference</title>
				<meeting>the Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient exploration of nonuniform space-time transformations for optimal systolic array synthesis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Baltus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Application Specific Array Processors</title>
				<meeting>International Conference on Application Specific Array Processors</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The netflix prize</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
				<meeting>KDD cup and workshop</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A practical automatic polyhedral parallelizer and locality optimizer</title>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
				<meeting>the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scaling to the end of silicon with edge architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dahlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yoder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigplan Notices</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A systolic design methodology with application to full-search block-matching architectures</title>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Kung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>Journal of VLSI signal processing systems for signal, image and video technology</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dadiannao: A machine-learning supercomputer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">PolySA: Polyhedral-Based Systolic Array Auto-Compilation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design</title>
				<meeting>the International Conference on Computer-Aided Design</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">dMazeRunner: Optimizing Convolutions on Dataflow Accelerators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Shidiannao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ACM SIGARCH Computer Architecture News</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Multilinear algebra for analyzing data with multiple linkages</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dunlavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>in Graph algorithms in the language of linear algebra</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural acceleration for general-purpose approximate programs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A configurable cloud-scale dnn processor for real-time ai</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alkalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dyser: Unifying functionality and parallelism specialization for energy-efficient computing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nowatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chhugani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">TensorLib: A Spatial Accelerator Generation Framework for Tensor Algebra</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12339</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Understanding reuse, performance, and hardware cost of dnn dataflow: A data-centric approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Maestro: A data-centric approach to understand reuse, performance, and hardware cost of dnn mappings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Maeri: Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Susy: A programming model for productive construction of high-performance systolic arrays on fpgas</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Design</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">A systolic array optimizing compiler</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Polyhedral model based mapping optimization of loop nests for CGRAs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Design Automation Conference</title>
				<meeting>the 50th Annual Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Pudiannao: A polyvalent machine learning accelerator</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ACM SIGARCH Computer Architecture News</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Cambricon: An instruction set architecture for neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<ptr target="https://github.com/maestro-project/maestro" />
		<title level="m">MAESTRO</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>in the conference on Recommender systems</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Synthesis of a unidirectional systolic array for matrix-vector multiplication</title>
		<author>
			<persName><forename type="first">I</forename><surname>Milovanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Milovanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bekakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical and computer modelling</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Stream-dataflow acceleration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nowatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gangadhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 44th Annual International Symposium on Computer Architecture</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A general constraint-centric scheduling framework for spatial architectures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nowatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sartin-Tarm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Estan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Robatmili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<ptr target="http://nvdla.org/" />
		<title level="m">NVIDIA</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Timeloop: A systematic approach to dnn accelerator evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Performance Analysis of Systems and Software</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Triggered instructions: a control paradigm for spatially-programmed architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gambhir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Edge-centric modulo scheduling for coarse-grained reconfigurable architectures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on Parallel architectures and compilation techniques</title>
				<meeting>the 17th international conference on Parallel architectures and compilation techniques</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Plasticine: A reconfigurable architecture for parallel patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koeplinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadjis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 44th Annual International Symposium on Computer Architecture</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Convolution engine: balancing efficiency &amp; flexibility in specialized computing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qadeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture</title>
				<meeting>the 40th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">HASCO: Towards Agile HArdware and Software CO-design for Tensor Computation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qingcheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Size</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bingzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pengcheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xuehai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Efficient spmv operation for large and highly sparse matrices using scalable multi-way merge parallelization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pileggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Franchetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Tensor-matrix products with a compressed sparse tensor</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Irregular Applications: Architectures and Algorithms</title>
				<meeting>the Workshop on Irregular Applications: Architectures and Algorithms</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Wavescalar</title>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Michelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 36th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>36th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">isl: An integer set library for the polyhedral model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Congress on Mathematical Software</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Counting integer points in parametric polytopes using barvinok&apos;s rational functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seghir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Beyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Loechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bruynooghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Algorithmica</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">On the evolution of user interaction in facebook</title>
		<author>
			<persName><forename type="first">B</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM workshop on Online social networks</title>
				<meeting>the 2nd ACM workshop on Online social networks</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">AutoSA: A Polyhedral Compiler for High-Performance Systolic Arrays on FPGA</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM/SIGDA international symposium on Fieldprogrammable gate arrays</title>
				<meeting>the 2021 ACM/SIGDA international symposium on Fieldprogrammable gate arrays</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A Data Locality Optimizing Algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1991 Conference on Programming Language Design and Implementation</title>
				<meeting>the ACM SIGPLAN 1991 Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Interstellar: Using halide&apos;s scheduling language to analyze dnn accelerators</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
