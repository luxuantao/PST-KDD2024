<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLocal-K: Global and Local Kernels for Recommender Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Caren</forename><surname>Soyeon</surname></persName>
						</author>
						<author>
							<persName><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taejun</forename><surname>Lim</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Josiah</forename><surname>Poon</surname></persName>
							<email>josiah.poon@sydney.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Siqu Long</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Bernd Burgstaller</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Yonsei University Republic of Korea</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GLocal-K: Global and Local Kernels for Recommender Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3459637.3482112</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender Systems</term>
					<term>Matrix Completion</term>
					<term>Kernel Methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommender systems typically operate on high-dimensional sparse user-item matrices. Matrix completion is a very challenging task to predict one's interest based on millions of other users having each seen a small subset of thousands of items. We propose a Global-Local Kernel-based matrix completion framework, named GLocal-K, that aims to generalise and represent a high-dimensional sparse user-item matrix entry into a low dimensional space with a small number of important features. Our GLocal-K can be divided into two major stages. First, we pre-train an auto encoder with the local kernelised weight matrix, which transforms the data from one space into the feature space by using a 2d-RBF kernel. Then, the pre-trained auto encoder is fine-tuned with the rating matrix, produced by a convolution-based global kernel, which captures the characteristics of each item. We apply our GLocal-K model under the extreme low-resource setting, which includes only a user-item rating matrix, with no side information. Our model outperforms the state-of-the-art baselines on three collaborative filtering benchmarks: ML-100K, ML-1M, and Douban.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Recommender systems; • Theory of computation → Kernel methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Collaborative filtering-based recommender systems focus on making a prediction about the interests of a user by collecting preferences from large number of other users. Matrix completion <ref type="bibr" target="#b1">[2]</ref> is one of the most common formulation, where rows and columns of a matrix represent users and items, respectively. The prediction of users' ratings in items corresponds to the completion of the missing entries in a high-dimensional user-item rating matrix. In practice, the matrix used for collaborative filtering is extremely sparse since it has ratings for only a limited number of user-item pairs.</p><p>Traditional recommender systems focus on generalising sparsely observed matrix entries to a low dimensional feature space by using an autoencoder(AE) <ref type="bibr" target="#b10">[11]</ref>. AEs would help the system better understand users and items by learning the non-linear user-item relationship efficiently, and encoding complex abstractions into data representations. I-AutoRec <ref type="bibr" target="#b7">[8]</ref> designed an item-based AE, which takes high-dimensional matrix entries, projects them into a lowdimensional latent hidden space, and then reconstructs the entries in the output space to predict missing ratings. SparseFC <ref type="bibr" target="#b5">[6]</ref> employs an AE whose weight matrices were sparsified using finite support kernels. Inspired by this, GC-MC <ref type="bibr" target="#b0">[1]</ref> proposed a graph-based AE framework for matrix completion, which produces latent features of user and item nodes through a form of message passing on the bipartite interaction graph. These latent user and item representations are used to reconstruct the rating links via a bilinear decoder. Such link prediction with a bipartite graph extends the model with structural and external side information. Recent studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> focused on utilising side information, such as opinion information or attributes of users. However, in most real-world settings (e.g., platforms and websites), there is no (or insufficient) side information available about users.</p><p>Instead of considering side information, we focus on improving the feature extraction performance for a high-dimensional useritem rating matrix into a low-dimensional latent feature space. In this research, we apply two types of kernels that have strong ability in feature extraction. The first kernel, named "local kernel", is known to give optimal separating surfaces by its ability to perform the data transformation from high-dimensional space, and widely used with support vector machines(SVMs). The second kernel, named "global kernel" is from convolutional neural network(CNN) architectures. The more kernel with deeper depth, the higher their feature extraction ability. Integrating these two kernels to have best of both worlds successfully extract the low-dimensional feature space.</p><p>With this in mind, we propose a Global-Local Kernel-based matrix completion framework, called GLocal-K, which includes two stages: 1) pre-training the auto-encoder using a local kernelised weight matrix, and 2) fine-tuning with the global kernel-based rating matrix. Note that our evaluation is under an extreme setting where no side information is available, like most real-world cases. The main research contributions are summarised as follows: <ref type="bibr" target="#b0">(1)</ref> We introduce a global and local kernel-based auto encoder model, which mainly pays attention to extract the latent features of users and items. <ref type="bibr" target="#b1">(2)</ref> We propose a new way to integrate pre-training and fine-tuning tasks for the recommender system. ( <ref type="formula" target="#formula_2">3</ref>) Without using any extra information, our GLocal-K achieves the smallest RMSEs on three widely-used benchmarks, even beating models augmented by side information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GLOCAL-K</head><p>Figure <ref type="figure" target="#fig_0">1</ref> depicts the architecture of our proposed GLocal-K model, which applies two types of kernels in two stages respectively: pretraining (with the local kernelised weight matrix) and fine-tuning (with the global-kernel based matrix) 1 . Note that we pre-train our model to make dense connections denser and sparse connections sparser using a finite support kernel, and fine-tune with the rating matrix. This matrix is produced from a convolution kernel by reducing the data dimension and producing a less redundant but small number of important feature sets. In this research, we mainly focus on a matrix completion task, which is conducted on a rating matrix 𝑅 ∈ R 𝑚×𝑛 with 𝑚 items and 𝑛 users. Each item 𝑖 ∈ 𝐼 = {1, 2, ..., 𝑚} is represented by a vector 𝑟 𝑖 = (𝑅 𝑖1 , 𝑅 𝑖2 , ..., 𝑅 𝑖𝑛 ) ∈ R 𝑛 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-training with Local Kernel</head><p>Auto-Encoder Pre-training We first deploy and train an item-based AE, inspired by <ref type="bibr" target="#b7">[8]</ref>, which takes each item vector 𝑟 𝑖 as input, and outputs the reconstructed vector 𝑟 ′ 𝑖 to predict the missing ratings. The model is represented as follows:</p><formula xml:id="formula_0">𝑟 ′ 𝑖 = 𝑓 (𝑊 (𝑑) • 𝑔(𝑊 (𝑒) 𝑟 𝑖 + 𝑏) + 𝑏 ′ ),<label>(1)</label></formula><p>where𝑊 (𝑒) ∈ R ℎ×𝑚 and𝑊 (𝑑) ∈ R 𝑚×ℎ are weight matrices, 𝑏 ∈ R ℎ and 𝑏 ′ ∈ R 𝑚 are bias vectors, and 𝑓 (•) and 𝑔(•) are non-linear activation functions. The AE deploys an auto-associative neural network with a single ℎ-dimensional hidden layer. In order to emphasise the dense and sparse connection, we reparameterise weight matrices in the AE with a radial-basis-function(RBF) kernel, which is known as Kernel Trick <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Kernelised Weight Matrix</head><p>The weight matrices 𝑊 (𝑒) and 𝑊 (𝑑) in Eq. ( <ref type="formula" target="#formula_0">1</ref>) are reparameterised by a 2d-RBF kernel, named local kernelised weight matrix. The RBF kernel can be defined as follows:</p><formula xml:id="formula_1">𝐾 𝑖 𝑗 (𝑈 , 𝑉 ) = max(0, 1 − 𝑢 𝑖 − 𝑣 𝑗 2 2 ),<label>(2)</label></formula><p>1 The idea of our pre-training and fine-tuning is different from transfer learning.</p><p>where 𝐾 (•) is a RBF kernel function, which computes the similarity between two sets of vectors 𝑈 , 𝑉 . Here, 𝑢 𝑖 ∈ 𝑈 and 𝑣 𝑗 ∈ 𝑉 . The kernel function can represent the output as a kernel matrix LK (see Figure <ref type="figure" target="#fig_0">1</ref>), in which each element maps to 1 for identical vectors and approaches 0 for very distant vectors between 𝑢 𝑖 and 𝑣 𝑗 . Then, we compute a local kernelised weight matrix as follows:</p><formula xml:id="formula_2">𝑊 ′ 𝑖 𝑗 = 𝑊 𝑖 𝑗 • 𝐾 𝑖 𝑗 (𝑈 , 𝑉 ),<label>(3)</label></formula><p>where 𝑊 ′ is computed by the Hadamard-product of weight and kernel matrices to obtain a sparsified weight matrix. The distance between each vector of 𝑈 and 𝑉 determines the connection of neurons in neural networks, and the degree of sparsity is dynamically varied as vectors are being changed at each step of training. As a result, applying the kernel trick to weight matrices enables regularising weight matrices and learning generalisable representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-tuning with Global Kernel Global kernel-based Rating Matrix</head><p>We fine-tune the pre-trained auto encoder with the rating matrix, produced by the global convolutional kernel. Prior to fine-tuning, we firstly describe how the global kernel is constructed and applied to build the global kernel-based rating matrix. The entire construction procedure can be defined as follows:</p><formula xml:id="formula_3">𝜇 𝑖 = avgpool(𝑟 ′ 𝑖 )<label>(4)</label></formula><formula xml:id="formula_4">𝐺𝐾 = 𝑚 ∑︁ 𝑖=1 𝜇 𝑖 • 𝑘 𝑖 (5) R = 𝑅 ⊗ 𝐺𝐾<label>(6)</label></formula><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the decoder output of the pre-trained model is the matrix that includes initial predicted ratings in the missing entries, and passed to pooling. With item-based average pooling, we summarise each item information in the rating matrix. Eq. ( <ref type="formula" target="#formula_3">4</ref>) shows the reconstructed item vector r𝑖 from the decoder output matrix 𝑅 ′ is passed to pooling, and interpreted as item-based summarisation.</p><p>Let 𝑀 = {𝜇 1 , 𝜇 2 , ..., 𝜇 𝑚 } ∈ R 𝑚 be the pooling result, which plays a role as the weights of multiple kernels 𝐾 = {𝑘 1 , 𝑘 2 , ..., 𝑘 𝑚 } ∈ R 𝑚×𝑡 2 . In Eq. ( <ref type="formula">5</ref>), these kernels are aggregated by using an inner product. The result can be dynamically determined by different weights and different rating matrices so that it can be regarded as the ratingdependent mechanism. Then, the aggregated kernel 𝐺𝐾 ∈ R 𝑡 ×𝑡 is used as a global convolution kernel. We apply a global kernel-based convolution operation to the user-item rating matrix for global kernel-based feature extraction. In Eq. ( <ref type="formula" target="#formula_4">6</ref>), R is the global kernelbased rating matrix, which is used as input for fine-tuning, and ⊗ denotes a convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auto-Encoder Fine-tuning</head><p>We then explore how the fine-tuning process works. The global kernel-based rating matrix R is used as input for fine-tuning. It takes weights of a pre-trained AE model and makes an adjustment of the model based on the global kernel-based rating matrix, as depicted in Figure <ref type="figure" target="#fig_0">1</ref>. The reconstructed result from the fine-tuned AE corresponds to the final predicted ratings for matrix completion in recommender system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Datasets</head><p>We conduct experiments on three widely used matrix completion benchmark datasets: MovieLens-100K (ML-100K), MovieLens-1M (ML-1M) and Douban (density 0.0630 / 0.0447 / 0.0152). These datasets comprise of (100 k / 1 m / 136 k) ratings of (1,682 / 3,706 / 3,000) movies by (943 / 6,040 / 3,000) users on a scale of 𝑟 ∈ {1, 2, 3, 4, 5}. For ML-100K, we use the canonical u1.base/u1.test train/test split. For ML-1M, we randomly split into 90:10 train/test sets. For Douban, we use the preprocessed subsets and splits provided by Monti et al. <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We compare the RMSE with the eleven recommendation baselines: (1) LLORMA <ref type="bibr" target="#b3">[4]</ref> is a matrix factorization model using local low rank sub-matrices factorization. (2) I-AutoRec <ref type="bibr" target="#b7">[8]</ref> is a autoencoder based model considering only the user or item embeddings in the encoder. (3) CF-NADE <ref type="bibr" target="#b12">[13]</ref> replaces the role of the restricted Boltzmann machine (RBM) with the neural auto-regressive distribution estimator (NADE) for rating reconstruction. (4) GC-MC <ref type="bibr" target="#b0">[1]</ref> is a graph-based AE framework that applies GNN on the bipartite interaction graph for rating link reconstruction. We consider GC-MC with side information as (5) GC-MC+Extra. (6) GraphRec <ref type="bibr" target="#b6">[7]</ref> is a matrix factorization utilizing graph-based features from the bipartite interaction graph. We consider GraphRec with side information as (7) GraphRec+Extra. ( <ref type="formula">8</ref>) GRAEM <ref type="bibr" target="#b8">[9]</ref> formulates a probabilistic generative model and uses expectation maximization to extend graph-regularised alternating least squares based on additional side information (SI) graphs. (9) SparseFC <ref type="bibr" target="#b5">[6]</ref> is a neural network in which weight matrices are reparameterised in terms of low-dimensional vectors, interacting through finite support kernel functions. This is technically equivalent to the local kernel of GLocal-K. (10) IGMC <ref type="bibr" target="#b11">[12]</ref> is similar to GCMC but applies a graphlevel GNN to the enclosing one-hot subgraph and maps a subgraph to the rating in an inductive manner. (11) MG-GAT <ref type="bibr" target="#b9">[10]</ref> uses attention mechanism to dynamically aggregate neighbor information of each user (item) for learning latent user/item representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Setup</head><p>We use two 500-dimensional hidden layers for AE and 5-dimensional vectors 𝑢 𝑖 , 𝑣 𝑗 for the RBF kernel. For fine-tuning, we use a single convolution layer with a 3x3 global convolution kernel. Inspired by <ref type="bibr" target="#b7">[8]</ref>, we train our model using the L-BFGS-B optimiser to minimise regularised squared errors, where 𝐿 2 regularisation is applied with different penalty parameters 𝜆 2 , 𝜆 𝑠 for weight and kernel matrices respectively. Based on validation results, we choose the following settings for (ML-100K / ML-1M / Douban). ( <ref type="formula" target="#formula_0">1</ref>) L-BFGS-B: 𝑚𝑎𝑥𝑖𝑡𝑒𝑟 𝑝 = (5 / 50 / 5), 𝑚𝑎𝑥𝑖𝑡𝑒𝑟 𝑓 = (5 / 10 / 5)<ref type="foot" target="#foot_0">2</ref> , (2) 𝐿 2 regularisation: 𝜆 2 = (20 / 70 / 10), 𝜆 𝑠 = (.006 / .018 / .022). We repeat each experiment five times and report the average RMSE results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Performance</head><p>We first evaluated our GLocal-K model on ML-100K (u1.base/u1.test split)/-1M datasets and compare with the baseline models. The RMSE test results are provided in Table <ref type="table" target="#tab_0">1</ref>. It can be easily observed from both GC-MC and GraphRec that incorporate side information improves the recommendation performance, e.g., the error rate of GC-MC+Extra. and GraphRec+Extra. reduce by 0.001 and 0.007 respectively on ML-100K via side information inclusion. Similar to GC-MC, IGMC also learns graph-structural relations from the bipartite user-item interaction graph derived from the rating matrix using GNN but outperforms GC-MC+Extra. by focusing on one-hot sub-graphs with inductive matrix completion. GRAEM focuses on additional graph SI and MG-GAT uses auxiliary information to represent user-user and item-item graph relations. Different from those models above, the first three models in the table use only the rating matrix structure and achieve better results on ML-1M. Our proposed GLocal-K also draws on the rating matrix structure and uses no extra information, outperforming all the baseline models above on three datasets, including those with additional side information, which illustrates the efficacy of combining the local-global kernels for recommendation tasks. Moreover, SparseFC also achieves higher accuracy than those baseline models on three datasets except for MG-GAT, showing the benefits of proper kernel-approximations of the weight matrix. Our GLocal-K surpasses SparseFC, further illustrating the effectiveness of a global kernel that learns to refine and extract the relevant information from the sparse data matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cold-start Recommendation</head><p>We varied the training ratio from 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Pre-training</head><p>We explored the optimal number of epochs for pre-training on ML-100K, ML-1M and Douban. The RMSE results for the three datasets using pre-training epochs from 0 (i.e., no pre-training) to 60 are provided in Figure <ref type="figure">3</ref>. These three datasets represent similar </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Global Convolution Kernel</head><p>To explore the effectiveness of the global kernel-based convolution with in-depth analysis, we first tried multiple kernel sizes and convolution layers. The RMSE results on the three datasets are presented in Table <ref type="table" target="#tab_2">2</ref>. It can be seen from Table <ref type="table" target="#tab_2">2</ref> that using 3x3 sized kernel achieves the best performance on all three datasets and the error rate goes up as the size increases to 5x5 or 7x7. It implies that focusing on more local features with smaller kernel size might be more effective for extracting generalizable patterns over the whole data matrix. Moreover, Table <ref type="table" target="#tab_2">2</ref> shows an incremental performance degradation when the conv layer increases from 1 to 3, indicating a single convolution layer is enough and optimal for feature extraction. In addition, we also explored two variants of kernel aggregation mechanisms: (1) integrating multiple kernels based on the weights and (2) aggregating via pure element-wise average. As shown in Table <ref type="table" target="#tab_2">2</ref>, weight-based aggregation reduces RMSE by 0.004 and 0.009 on ML-100K and Douban while achieving similar performance on ML-1M. Overall, it can be seen that using feature-indicative weights to aggregate the kernels is more effective than purely applying element-wise averages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduced GLocal-K for recommender systems, which takes full advantage of both a local kernel at the pre-training stage and a global kernel at the fine-tuning stage for capturing and refining the important characteristic features of the sparse rating matrix under an extremely low resource setting. We demonstrate RMSE on three benchmark datasets: MovieLens-100k/-1M and Douban, outperforming numerous baseline approaches. In particular, we highlighted the effectiveness of our global kernel for exerting scarce data by evaluating the cold-start recommendation.</p><p>It is hoped that our Global-K gives some insight into the future integration of both kernels for high-dimensional sparse matrix completion with no side information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The GLocal-K architecture for matrix completion. (1) We pre-train the AE with the local kernelised weight matrix. (2) Then, fine-tune the trained AE with the global kernel-based matrix. The fine-tuned AE produces the matrix completion result.</figDesc><graphic url="image-1.png" coords="3,53.80,83.69,504.41,113.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Performance comparison w.r.t. different sparsity levels on ML-100K and Douban datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>RMSE test results on three benchmark datasets. The column Extra. represents whether the model utilises any side information. All RMSE results are from the respective papers cited in the first column, and the best results are highlighted in bold.</figDesc><table><row><cell>Model</cell><cell cols="4">Extra. ML-100K ML-1M Douban</cell></row><row><cell>LLORMA[4]</cell><cell>-</cell><cell>-</cell><cell>0.833</cell><cell>-</cell></row><row><cell>I-AutoRec[8]</cell><cell>-</cell><cell>-</cell><cell>0.831</cell><cell>-</cell></row><row><cell>CF-NADE[13]</cell><cell>-</cell><cell>-</cell><cell>0.829</cell><cell>-</cell></row><row><cell>GC-MC[1]</cell><cell>-</cell><cell>0.910</cell><cell>0.832</cell><cell>-</cell></row><row><cell>GC-MC+Extra.[1]</cell><cell>O</cell><cell>0.905</cell><cell>-</cell><cell>0.734</cell></row><row><cell>GraphRec[7]</cell><cell>-</cell><cell>0.904</cell><cell>0.843</cell><cell>-</cell></row><row><cell>GraphRec+Extra.[7]</cell><cell>O</cell><cell>0.897</cell><cell>0.842</cell><cell>-</cell></row><row><cell>GRAEM[9]</cell><cell>O</cell><cell>0.917</cell><cell>-</cell><cell>0.732</cell></row><row><cell>SparseFC[6]</cell><cell>-</cell><cell>0.895</cell><cell>0.824</cell><cell>0.730</cell></row><row><cell>IGMC[12]</cell><cell>-</cell><cell>0.905</cell><cell>0.857</cell><cell>0.721</cell></row><row><cell>MG-GAT[10]</cell><cell>O</cell><cell>0.890</cell><cell>-</cell><cell>0.727</cell></row><row><cell>GLocal-K (ours)</cell><cell>-</cell><cell>0.890</cell><cell>0.822</cell><cell>0.721</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of RMSE test results of Global Kernel w.r.t. (1) different convolution kernel sizes, (2) different numbers of convolution layers and (3) different kernel aggregation mechanisms on three benchmark datasets. The best results are highlighted in bold. The RMSE first keeps decreasing as the pretraining epoch increases from 0, indicating that pre-training benefits GLocal-K to achieve better performance on all three datasets.Then the RMSE starts to go up again after reaching its optimum at 30 epochs for ML-100K and 20 epochs for both ML-1M and Douban. Referring to the dataset statistics, we surmise that having more item numbers with lower density may lead to less pre-training for optimal performance.</figDesc><table><row><cell></cell><cell cols="3">ML-100K ML-1M Douban</cell></row><row><cell>Kernel size</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3x3</cell><cell>0.890</cell><cell>0.822</cell><cell>0.721</cell></row><row><cell>5x5</cell><cell>0.891</cell><cell>0.823</cell><cell>0.723</cell></row><row><cell>7x7</cell><cell>0.891</cell><cell>0.823</cell><cell>0.723</cell></row><row><cell># Conv layers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>0.890</cell><cell>0.822</cell><cell>0.721</cell></row><row><cell>2</cell><cell>0.893</cell><cell>0.827</cell><cell>0.725</cell></row><row><cell>3</cell><cell>0.897</cell><cell>0.848</cell><cell>0.732</cell></row><row><cell>Agg. mechanism</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Element-wise</cell><cell>0.894</cell><cell>0.822</cell><cell>0.730</cell></row><row><cell>Weighted</cell><cell>0.890</cell><cell>0.822</cell><cell>0.721</cell></row><row><cell>bowl-shaped curves.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">maxiter is maximum number of iterations (p=pre-training, f =fine-tuning).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD Deep Learning Day</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel efficient approach for audio segmentation</title>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aggelos</forename><surname>Pikrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergios</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 19th International Conference on Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LLORMA: Local Low-Rank Matrix Approximation</title>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3700" to="3710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernelized synaptic weight matrices</title>
		<author>
			<persName><forename type="first">Lorenz</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Indiveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3654" to="3663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attribute-aware non-linear co-embeddings of graph features</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josif</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
				<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="314" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on World Wide Web</title>
				<meeting>the 24th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable probabilistic matrix factorization with graph-based priors</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Strahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Peltonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirsohi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5851" to="5858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpretable Recommender System With Heterogeneous Information: A Geometric Deep Learning Perspective</title>
		<author>
			<persName><surname>Ugla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dhuha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><forename type="middle">J</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><surname>Khaudair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mechanical and Production Engineering Research and Development (IJMPERD)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2411" to="2430" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of autoencoder-based recommender systems</title>
		<author>
			<persName><forename type="first">Guijuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoning</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="430" to="450" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive Matrix Completion Based on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to collaborative filtering</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
