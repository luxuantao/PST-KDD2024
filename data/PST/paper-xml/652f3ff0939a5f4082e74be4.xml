<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction</title>
				<funder ref="#_BTSQmY6 #_YQjKRCw">
					<orgName type="full">Natural Science Foundation of Shanghai</orgName>
				</funder>
				<funder ref="#_yzqkar8 #_MMcNYJg #_Weq7PSn">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Kg6kRkn">
					<orgName type="full">Shanghai Rising-Star Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-10-17">17 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
							<email>chongzhang20@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ant Info Security Lab</orgName>
								<orgName type="institution">Ant Group Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Tu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ant Info Security Lab</orgName>
								<orgName type="institution">Ant Group Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Chen</surname></persName>
							<email>chenhuan.chen@antgroup.com</email>
							<affiliation key="aff1">
								<orgName type="department">Ant Info Security Lab</orgName>
								<orgName type="institution">Ant Group Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinyang</forename><surname>Tang</surname></persName>
							<email>jinyang.tjy@antgroup.com</email>
							<affiliation key="aff1">
								<orgName type="department">Ant Info Security Lab</orgName>
								<orgName type="institution">Ant Group Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huijia</forename><surname>Zhu</surname></persName>
							<email>huijia.zhj@antgroup.com</email>
							<affiliation key="aff1">
								<orgName type="department">Ant Info Security Lab</orgName>
								<orgName type="institution">Ant Group Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
							<email>tgui@fudan.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Modern Languages and Linguistics</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-17">17 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2310.11016v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in multimodal pre-trained models have significantly improved information extraction from visually-rich documents (VrDs), in which named entity recognition (NER) is treated as a sequence-labeling task of predicting the BIO entity tags for tokens, following the typical setting of NLP. However, BIO-tagging scheme relies on the correct order of model inputs, which is not guaranteed in real-world NER on scanned VrDs where text are recognized and arranged by OCR systems. Such reading order issue hinders the accurate marking of entities by BIO-tagging scheme, making it impossible for sequencelabeling methods to predict correct named entities. To address the reading order issue, we introduce Token Path Prediction (TPP), a simple prediction head to predict entity mentions as token sequences within documents. Alternative to token classification, TPP models the document layout as a complete directed graph of tokens, and predicts token paths within the graph as entities. For better evaluation of VrD-NER systems, we also propose two revised benchmark datasets of NER on scanned documents which can reflect real-world scenarios. Experiment results demonstrate the effectiveness of our method, and suggest its potential to be a universal solution to various information extraction tasks on documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visually-rich documents (VrDs), including forms, receipts and contracts, are essential tools for gathering, carrying, and displaying information in the digital era. The ability to understand and extract information from VrDs is critical for realworld applications. In particular, the recognition Entity Recognition (NER) and Entity Linking (EL) <ref type="bibr" target="#b16">(Jaume et al., 2019;</ref><ref type="bibr" target="#b25">Park et al., 2019)</ref>.</p><p>However, in the practical application of information extraction (IE) from scanned VrDs, the reading order issue is known as a pervasive problem that lead to suboptimal performance of current methods. This problem is particularly typical in VrD-NER, a task that aims to identify word sequences in a document as entities of predefined semantic types, such as headers, names and addresses. Following the classic settings of NLP, current document transformers typically treat this task as a sequence-labeling problem, tagging each text token using the BIO-tagging scheme <ref type="bibr" target="#b27">(Ramshaw and Marcus, 1999)</ref> and predicting the entity tag for tokens through a token classification head. These sequence-labeling-based methods assumes that each entity mention is a continuous and front-to-back word sequence within inputs, which is always valid in plain texts. However, for scanned VrDs in real-world, where text and layout annotations are recognized and arranged by OCR systems, typically in a top-to-down and left-to-right order, the assumption may fail and the reading order issue arises, rendering the incorrect order of model inputs for document transformers, and making these sequence-labeling methods inapplicable. For instance, as depicted in Figure <ref type="figure">1</ref>, the document contains two entity mentions, namely "NAME OF ACCOUNT" and "# OF STORES SUPPLIED". However, due to their layout positions, the OCR system would recognize the contents as three segments and arrange them in the following order:(1)"# OF STORES"; (2)"NAME OF ACCOUNT"; (3)"SUPPLIED". Such disordered input leads to significant confusion in the BIOtagging scheme, making the models unable to assign a proper label for each word to mark the entity mentions clearly. Unfortunately, the reading order issue is particularly severe in documents with complex layouts, such as tables, multi-column contents, and unaligned contents within the same row or column, which are quite common in scanned VrDs. Therefore, we believe that the sequencelabeling paradigm is not a practical approach to address NER on scanned VrDs in real-world scenarios.</p><p>To address the reading order issue, we introduce Token Path Prediction (TPP), a simple yet strong prediction head for VrD-IE tasks. TPP is compatible with commonly used document transformers, Figure <ref type="figure">2</ref>: The reading order issue of scanned documents in real-world scenarios. Left: The entity "TOTAL 230,000" is disordered when reading from top to down. The entity "CASH CHANGE" lies in multiple rows and is separated to two segments. Right: The entity "TOTAL 30,000" is interrupted by "(Qty 1.00)" when reading from left to right. All these situations result in disordered model inputs and affect the performance of sequencelabeling based VrD-NER methods. More real-world examples of disordered layouts are displayed in Figure <ref type="figure">7</ref> in <ref type="bibr">Appendix.</ref> and can be adapted to various VrD-IE and VrD understanding (VrDU) tasks, such as NER, EL, and reading order prediction (ROP). Specifically, when using TPP for VrD-NER, we model the token inputs as a complete directed graph of tokens, and each entity as a token path, which is a group of directed edges within the graph. We adopt a grid label for each entity type to represent the token paths as n * n binary values of whether two tokens are linked or not, where n is the number of text tokens. Model learns to predict the grid labels by binary classification in training, and search for token paths from positively-predicted token pairs in inference. Overall, TPP provides a viable solution for VrD-NER by presenting a suitable label form, modeling VrD-NER as predicting token paths within a graph, and proposes a straightforward method for prediction. This method does not require any prior reading order and is therefore unaffected by the reading order issue. For adaptation to other VrD tasks, TPP is applied to VrD-ROP by predicting a global path of all tokens that denotes the predicted reading order. TPP is also capable of modeling the entity linking relations by marking linked token pairs within the grid label, making it suitable for direct adaptation to the VrD-EL task. Our work is related to discontinuous NER in NLP as the tasks share a similar form, yet current discontinuous NER methods cannot be directly applied to address the reading order issue since they also require a proper reading order of contents.</p><p>For better evaluation of our proposed method, we also propose two revised datasets for VrD-NER. In current benchmarks of NER on scanned VrDs, such as FUNSD <ref type="bibr" target="#b16">(Jaume et al., 2019)</ref> and CORD <ref type="bibr" target="#b25">(Park et al., 2019)</ref>, the reading order is manuallycorrected, thus failing to reflect the reading order issue in real-world scenarios. To address this limitation, we reannotate the layouts and entity mentions of the above datasets to obtain two revised datasets, FUNSD-r and CORD-r, which accurately reflect the real-world situations and make it possible to evaluate the VrD-NER methods in disordered scenarios. We conduct extensive experiments by integrating the TPP head with different document transformer backbones and report quantitative results on multiple VrD tasks. For VrD-NER, experiments on the FUNSD-r and CORD-r datasets demonstrate the effectiveness of TPP, both as an independent VrD-NER model, and as a pre-processing mechanism to reorder inputs for sequence-labeling models. Also, TPP achieves SOTA performance on benchmarks for VrD-EL and VrD-ROP, highlighting its potential as a universal solution for information extraction tasks on VrDs. The main contribution of our work are listed as follows: 1. We identify that sequence-labeling-based VrD-NER methods is unsuitable for real-world scenarios due to the reading order issue, which is not adequately reflected by current benchmarks. 2. We introduce Token Path Prediction, a simple yet strong approach to address the reading order issue in information extraction on VrDs. 3. Our proposed method outperforms SOTA methods in various VrD tasks, including VrD-NER, VrD-EL, and VrD-ROP. We also propose two revised VrD-NER benchmarks reflecting realworld scenarios of NER on scanned VrDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sequence-labeling based NER with Document Transformers Recent advances of pre-trained techniques in NLP <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr" target="#b41">Zhang et al., 2019)</ref> and CV <ref type="bibr" target="#b6">(Dosovitskiy et al., 2020;</ref><ref type="bibr">Li et al., 2022c)</ref> have inspired the design of pretrained representation models in document AI, in which document transformers <ref type="bibr" target="#b37">(Xu et al., 2020</ref><ref type="bibr">(Xu et al., , 2021a;;</ref><ref type="bibr" target="#b12">Huang et al., 2022;</ref><ref type="bibr">Li et al., 2021a,c;</ref><ref type="bibr" target="#b11">Hong et al., 2022;</ref><ref type="bibr" target="#b33">Wang et al., 2022;</ref><ref type="bibr" target="#b32">Tu et al., 2023)</ref> are proposed to act as a layout-aware representation model of document in various VrD tasks. By the term document transformers, we refer to transformer encoders that take vision (optional), text and layout input of a document as a token sequence, in which each text token is embedded together with its layout. In sequence-labeling based NER with document transformers, BIO-tagging scheme assigns a BIO-tag for each text token to mark entities: B-ENT/I-ENT indicates the token to be the beginning/inside token of an entity with type ENT, and O indicates the word does not belong to any entity. In this case, the type and boundary of each entity is clearly marked in the tags. The BIO-tags are treated as the classification label of each token and is predicted by a token classification head of the document transformer. As illustrated in introduction, sequence-labeling methods are typical for NER in NLP and is adopted by current VrD-NER methods, but is not suitable for VrD-NER in real-world scenarios due to the reading order issue.</p><p>Reading-order-aware Methods Several studies have addressed the reading order issue on VrDs in two directions: (1) Task-specific models that directly predict the reading order, such as Lay-outReader, which uses a sequence-to-sequence approach <ref type="bibr">(Wang et al., 2021b)</ref>. However, this method is limited to the task at hand and cannot be directly applied to VrD-IE tasks.</p><p>(2) Pre-trained models that learn from supervised signals during pre-training to improve their awareness of reading order. For instance, ERNIE-Layout includes a pre-training objective of reading order prediction <ref type="bibr" target="#b26">(Peng et al., 2022)</ref>; XYLayoutLM enhances the generalization of reading order pre-training by generating various proper reading orders using an augmented XY Cut algorithm <ref type="bibr" target="#b9">(Gu et al., 2022)</ref>. However, these methods require massive amount of labeled data and computational resources during pre-training. Comparing with the above methods, our work is applicable to multiple VrD-IE tasks of VrDs, and can be integrated with various document transformers, without the need for additional supervised data or computational costs.</p><p>Discontinuous NER The form of discontinuous NER is similar to VrD-NER, as it involves identifying discontinuous token sequences from text as named entities. Current methods of discontinuous NER can be broadly categorized into four groups:</p><p>(1) Sequence-labeling methods with refined BIOtagging scheme <ref type="bibr" target="#b31">(Tang et al., 2015;</ref><ref type="bibr" target="#b5">Dirkson et al., 2021)</ref>, (2) 2D grid prediction methods <ref type="bibr">(Wang et al., 2021a;</ref><ref type="bibr">Li et al., 2022b;</ref><ref type="bibr" target="#b24">Liu et al., 2022)</ref>, (3) Sequence-to-sequence methods <ref type="bibr">(Li et al., 2021b;</ref><ref type="bibr" target="#b10">He and Tang, 2022)</ref>, and (4) Transition-based methods <ref type="bibr" target="#b1">(Dai et al., 2020)</ref>. These methods all rely   on a proper reading order to predict entities from front to back, thus cannot be directly applied to address the reading order issue in VrD-NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VrD-NER</head><p>The definition of NER on visually-rich documents with layouts is formalized as follows. A visuallyrich document with N D words is represented as D = {(w i , b i )} i=1,...,N D , where w i denotes the i-th word in document and b i = (x 0 i , y 0 i , x 1 i , y 1 i ) denotes the position of w i in the document layout. The coordinates (x 0 i , y 0 i ) and (x 1 i , y 1 i ) correspond to the bottom-left and top-right vertex of w i 's bounding box, respectively. The objective of VrD-NER is to predict all the entity mentions {s 1 , . . . , s J } within document D, given the predefined entity types E = {e i } i=1,...,N E . Here, the j-th entity in D is represented as s j = {e j , (w j 1 , . . . , w j k )}, where e j ? E is the entity type and (w j 1 , . . . , w j k ) is a word sequence, where the words are two-by-two different but not necessarily adjacent. It is important to note that sequencelabeling based methods assign a BIO-label to each word and predict adjacent words (w j , w j+1 , . . . ) as entities, which is not suitable for real-world VrD-NER where the reading order issue exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Token Path Prediction for VrD-NER</head><p>In Token Path Prediction, we model VrD-NER as predicting paths within a graph of tokens. Specifically, for a given document D, we construct a complete directed graph with the N D tokens {w i } i=1,...,N D in D as vertices. This graph consists of n 2 directed edges pointing from each token to each token. For each entity s j = {e j , (w j 1 , . . . , w j k )}, the word sequence can be represented by a path (w j 1 ? w j 2 , . . . , w j k-1 ? w j k ) within the graph, referred to as a token path.</p><p>TPP predicts the token paths in a given graph by learning grid labels. For each entity type, the token paths of entities are marked by a n * n grid label of binary values, where n is the number of text tokens. In specific, for each edge in every token path, the pair of the beginning and ending tokens is labeled as 1 in the grid label, while others are labeled as 0. For example in Figure <ref type="figure" target="#fig_1">3</ref>, the token pair "(NAME, OF)" and "(OF, ACCOUNT)" are marked 1 in the grid label. In this way, entity annotations can be represented as N E grids of n * n binary values.</p><p>The learning of grid labels is then treated as N E binary classification tasks on n 2 samples, which can be implemented using any classification model. In TPP, we utilize document transformers to represent document inputs as token feature sequences, and employ Global Pointer <ref type="bibr" target="#b30">(Su et al., 2022)</ref> as the classification model to predict the grid labels by binary classification. Following <ref type="bibr" target="#b30">(Su et al., 2022)</ref>, the weights are optimized by a classimbalance loss to overcome the class-imbalance problem, as there are at most n positive labels out of n 2 labels in each grid. During evaluation, we collect the N E predicted grids, filtering the token pairs predicted to be positive. If there are multiple token pairs with same beginning token, we only keep the pair with highest confidence score. After that, we perform a greedy search to predict token paths as entities. Figure <ref type="figure" target="#fig_1">3</ref> displays the overall procedure of TPP for VrD-NER.</p><p>In general, TPP is a simple and easy-toimplement solution to address the reading order issue in real-world VrD-NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Token Path Prediction for Other Tasks</head><p>We have explored the capability of TPP to address various VrD tasks, including VrD-EL and VrD-ROP. As depicted in Figure <ref type="figure" target="#fig_2">4</ref>, these tasks are addressed by devising task-specific grid labels for TPP training. For the prediction of VrD-EL, we gather all token pairs between every two entities and calculate the mean logit score. Two entities are predicted to be linked if the mean score is greater than 0. In VrD-ROP, we perform a beam search on the logit scores, starting from the auxiliary beginning token to predict a global path linking all tokens. The feasibility of TPP on these tasks highlights its potential as a universal solution for VrD tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Revised Datasets for Scanned VrD-NER</head><p>In this section, we introduce FUNSD-r and CORDr, the revised VrD-NER datasets to reflect the realworld scenarios of NER on scanned VrDs. We first point out the existing problems of popular benchmarks for NER on scanned VrDs, which indicates the necessity of us to build new benchmarks. We then describe the construction of new datasets, including selecting adequate data resources and the annotating process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motivation</head><p>FUNSD <ref type="bibr" target="#b16">(Jaume et al., 2019)</ref> and CORD <ref type="bibr" target="#b25">(Park et al., 2019)</ref> are the most popular benchmarks of NER on scanned VrDs. However, these benchmarks are biased towards real-world scenarios. In FUNSD and CORD, segment layout annotations are aligned with labeled entities. The scope of each entity on the document layout is marked by a bounding box that forms the segment annotation together with each entity word. We argue that there are two problems in their annotations that render them unsuitable for evaluating current methods. First, these benchmarks do not reflect the reading order issue of NER on scanned VrDs, as each entity corresponds to a continuous span in the model input. In these benchmarks, each segment models a continuous semantic unit where words are correctly ordered, and each entity corresponds exactly to one segment. Consequently, each entity corresponds to a continuous and front-to-back token span in the model input. However, in real-world scenarios, entity mentions may span across segments, and segments may be disordered, necessitating the consideration of reading order issues. For instance, as shown in Figure <ref type="figure" target="#fig_3">5</ref>, the entity "Sample Requisition [Form 02:02:06]" is located in a chart cell spanning multiple rows; while the entity is recognized as two segments by the OCR system since OCRannotated segments are confined to lie in a single row. Second, the segment layout annotations in current benchmarks vary in granularity, which is inconsistent with real-world situations. The scope of segment in these benchmarks ranges from a single word to a multi-row paragraph, whereas OCR-annotated segments always correspond to words within a single row.</p><p>Therefore, we argue that a new benchmark should be developed with segment layout annotations aligned with real-world situations and entity mentions labeled on words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Construction</head><p>As illustrated above, current benchmarks cannot reflect real-world scenarios and adequate benchmarks are desired. That motivates us to develop new NER benchmarks of scanned VrDs with realworld layout annotations.</p><p>We achieve this by reannotating existing benchmarks and select FUNSD <ref type="bibr" target="#b16">(Jaume et al., 2019)</ref>  and CORD <ref type="bibr" target="#b25">(Park et al., 2019)</ref> for the following reasons. First, FUNSD and CORD are the most commonly used benchmarks on VrD-NER as they contains high-quality document images that closely resemble real-world scenarios and clearly reflect the reading order issue. Second, the annotations on FUNSD and CORD are heavily biased due to the spurious correlation between its layout and entity annotations. In these benchmarks, each entity exactly corresponds to a segment, which leaks the ground truth entity labels during training by the 2D positional encoding of document transformers, as tokens of the same entity share the same segment 2D position information. Due to the above issues, we reannotate the layouts and entity mentions on the document images of the two datasets. We automatically reannotate the layouts using an OCR system, and manually reannotate the named entities as word sequences based on the new layout annotations to build the new datasets. The proposed FUNSD-r and CORD-r datasets consists of 199 and 999 document samples including the image, layout annotation of segments and words, and labeled entities. The detailed annotation pipeline and statistics of these datasets is introduced in Appendix A. We publicly release the two revised datasets at GitHub. 1 5 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation</head><p>The experiments in this work involve VrD-NER, VrD-EL, and VrD-ROP. For VrD-NER, the experiments are conducted on the FUNSD-r and CORD-r 1 https://github.com/chongzhangFDU/TPP datasets proposed in our work. The performance of methods are measured by entity-level F1 on these datasets. Noticing that previous works on FUNSD and CORD have used word-level F1 as the primary evaluation metric, we argue that this may not be entirely appropriate, as word-level F1 only evaluates the accuracy of individual words, yet entity-level F1 assesses the entire entity, including its boundary and type. Compared with word-level F1, entity-level F1 is more suitable for evaluation on NER systems in pratical applications. For VrD-EL, the experiments are conducted on FUNSD and the methods are measured by F1. For VrD-ROP, the experiments are conducted on ReadingBank. The methods are measured by Average Page-level BLEU (BLEU for short) and Average Relative Distance (ARD) <ref type="bibr">(Wang et al., 2021b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines and Implementation Details</head><p>For VrD-NER, we compare the proposed TPP with sequence-labeling methods that adopts document transformers integrated with token classification heads. As these methods suffer from the reading order issue, we adopt a VrD-ROP model as the pre-processing mechanism to reorder the inputs, mitigating the impact of the reading order issue. We adopt LayoutLMv3-base <ref type="bibr" target="#b12">(Huang et al., 2022)</ref> and LayoutMask <ref type="bibr" target="#b32">(Tu et al., 2023)</ref> as the backbones to integrate with TPP or token classification heads, as they are the SOTA document transformers in base-level that use "vision+text+layout" and "text+layout" modalities, respectively. For VrD-EL and VrD-ROP, we introduce the baseline methods with whom we compare the proposed TPP in Appendix B. The implementation details of experiments are illustrated in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on VrD-NER</head><p>In this section, we display and discuss the performance of different methods on VrD-NER in real-world scenario. In all, the effectiveness of TPP is demonstrated both as an independent VrD-NER model, and as a pre-processing mechanism to reorder inputs for sequence-labeling models. By both means, TPP outperforms the baseline models on two benchmarks, across both of the integrated backbones. It is concluded in Table <ref type="table" target="#tab_1">1</ref> that: (1) TPP is effective as an independent VrD-NER model, surpassing the performance of sequencelabeling methods on two benchmarks for realworld VrD-NER. Since sequence-labeling methods are adversely affected by the disordered inputs, their performance is constrained by the ordered degree of datasets. In contrast, TPP is not affected by disordered inputs, as tokens are arranged during its prediction. Experiment results show that TPP outperforms sequence-labeling methods on both benchmarks. Particularly, TPP brings about a +9.13 and +7.50 performance gain with LayoutLMv3 and LayoutMask on the CORD-r benchmark. It is noticeable that the real superiority of TPP for VrD-NER might be greater than that is reflected by the results, as TPP actually learns a more challenging task comparing to sequence-labeling. While sequence-labeling solely learns to predict entity boundaries, TPP additionally learns to predict the token order within each entity, and still achieves better performance. (2) TPP is a desirable pre-processing mechanism to reorder inputs for sequence-labeling models, while the other VrD-ROP models are not. According to Table <ref type="table" target="#tab_1">1</ref>, Lay-outReader and TPP-for-VrD-ROP are evaluated as the pre-processing mechanism by the continuous entity rate of rearranged inputs. As a representative of current VrD-ROP models, LayoutReader does not perform well as a pre-processing mechanism, as the continuous entity rate of inputs decreases after the arrangement of LayoutReader on two disordered datasets. In contrast, TPP performs satisfactorily. For reordering FUNSD-r, TPP R brings about a +1.55 gain on continuous entity rate, thereby enhancing the performance of sequencelabeling methods. For reordering CORD-r, the desired reading order of the documents is to read row-by-row, which conflicts with the reading order of ReadingBank documents that are read column-by-column. Therefore, for fair comparison on CORD-r, we train LayoutReader and TPP on the original CORD to be used as pre-processing mechanisms, denoted as LR C and TPP C . As illustrated in Table <ref type="table" target="#tab_1">1</ref>, TPP C also improves the continuous entity rate and the predict performance of sequence-labeling methods, comparing with the LayoutReader alternative. Contrary to TPP, we find that using LayoutReader for pre-processing would result in even worse performance on the two benchmarks. We attribute this to the fact that LayoutReader primarily focuses on optimizing BLEU scores on the benchmark, where the model is only required to accurately order 4 consecutive tokens. However, according to Table <ref type="table" target="#tab_7">4</ref> in Appendix A, the average entity length is 16 and 7 words in the datasets, and any disordered token within an entity leads to its discontinuity in sequence-labeling. Consequently, LayoutReader may occasionally mispredict long entities with correct input order, due to its seq2seq nature which makes it possible to generate duplicate tokens or missing tokens during decoding.</p><p>For better understanding the performance of TPP, we conduct ablation studies to determine the best usage of TPP-for-VrD-NER by configuring the TPP and backbone settings. The results and detailed discussions can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation on Other Tasks</head><p>Table 2 and 3 display the performance of VrD-EL and VrD-ROP methods, which highlights the potential of TPP as a universal solution for VrD-IE tasks. For VrD-EL, TPP outperforms MSAU-PAF,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token Path Prediction Sequence-labeling Ground Truth Case</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-row Entity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-column Entity</head><p>Long Entity Method F1 GNN+MLP <ref type="bibr">(Carbonell et al., 2021)</ref> 39 SPADE <ref type="bibr" target="#b14">(Hwang et al., 2021)</ref> 41.3 Doc2Graph <ref type="bibr" target="#b8">(Gemelli et al., 2022)</ref> 53.36 LayoutXLM <ref type="bibr">(Xu et al., 2021b)</ref> 54.83 SERA <ref type="bibr" target="#b40">(Zhang et al., 2021)</ref> 65.96 BROS <ref type="bibr" target="#b11">(Hong et al., 2022)</ref> 71.46 MSAU-PAF <ref type="bibr" target="#b3">(Dang et al., 2021)</ref> 75 TPP 79.23 a competitive method, by a large margin of 4.23 on F1 scores. The result suggests that the current labeling scheme for VrD-EL provides adequate information for models to learn from. For VrD-ROP, TPP shows several advantages comparing with the SOTA method LayoutReader: (1) TPP is robust to input shuffling. The performance of TPP remains stable when train/evaluation inputs are shuffled as TPP is unaware of the token input order. However, LayoutReader is sensitive to input order and its performance decreases sharply when evaluation inputs are shuffled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Type Identification</head><p>(2) In principle, TPP is more suitable to act as a preprocessing reordering mechanism compared with LayoutReader. TPP surpasses LayoutReader by a significant margin on ARD among six distinct settings, which is mainly attributed to the paradigm differences. Specifically, TPP-for-VrD-ROP guarantees to predict a permutation of all tokens as the reading order, while LayoutReader predicts  the reading order by generating a token sequence and carries the risk of missing some tokens in the document, which results in a negative impact the ARD metric, and also make LayoutReader unsuitable for use as a pre-processing reordering mechanism.</p><p>(3) TPP surpasses LayoutReader among five out of six settings on BLEU. TPP is unaware of the token input order, and the performance of it among different settings is influenced only by random aspects. Although LayoutReader has a higher performance in the setting where train and evaluation samples are both not shuffled, it is attributed to the possible overfitting of the encoder of LayoutReader on global 1D signals under this setting, where global 1D signals strongly hint the reading order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>For better understanding the strength and weakness of TPP, we conduct a case study for analyzing the prediction results by TPP in VrD-NER. As discussed, TPP-for-VrD-NER make predictions by modeling token paths. Intuitively, TPP should be good at recognizing entity boundaries in VrDs.</p><p>To verify this, we visualize the predict result of interested cases by TPP and other methods in Figure <ref type="figure" target="#fig_4">6</ref>. According to the visualized results, TPP is good at identifying the entity boundary and makes accurate prediction of multi-row, multicolumn and long entities. For instance, in the Multirow Entity case, TPP accurately identifies the entity "5/3/79 10/6/80, Update" in complex layouts. In the Multi-column Entity case, TPP identifies the entity "TOTAL 120,000" with the interference of the injected texts within the same row. In the Long Entity case, the long entity as a paragraph is accurately and completely extracted, while the sequence-labeling method predicts the paragraph as two entities. Nevertheless, TPP may occasionally misclassify entities as other types, resulting in suboptimal performance. For example, in the Entity Type Identification case, TPP recognizes the entity boundaries of "SEPT 21" and "NOV 9" but fails to predict their correct entity types. This error can be attributed to the over-reliance of TPP on layout features while neglecting the text signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we point out the reading order issue in VrD-NER, which can lead to suboptimal performance of current methods in practical applications. To address this issue, we propose Token Path Prediction, a simple and easy-to-implement method which models the task as predicting token paths from a complete directed graph of tokens. TPP can be applied to various VrD-IE tasks with a unified architecture. We conduct extensive experiments to verify the effectiveness of TPP on real-world VrD-NER, where it serves both as an independent VrD-NER model and as a pre-processing mechanism to reorder model inputs. Also, TPP achieves SOTA performance on VrD-EL and VrD-ROP tasks. We also propose two revised VrD-NER benchmarks to reflect the real situations of NER on scanned VrDs. In future, we plan to further improve our method and verify its effectiveness on more VrD-IE tasks. We hope that our work will inspire future research to identify and tackle the challenges posed by VrD-IE in practical scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our method has the following limitations:</p><p>1. Relatively-high Computation Cost: Token Path Prediction involves the prediction of grid labels, therefore is more computational-heavily than vanilla token classification, though it is still a simple and generalizable method for VrD-IE tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The Annotation Pipeline and Statistics of Revised Datasets</head><p>The annotation pipeline is introduced as follows.</p><p>First, we collect the document images of FUNSD and CORD, and re-annotate the layout automatically with an OCR system. We choose PP-OCRv3 <ref type="bibr">(Li et al., 2022a)</ref> OCR engine as it is the SOTA solution of general-purpose OCR and can extend to multilingual settings. We operate each document image of FUNSD and CORD, keeping layout annotations with more than 0.8 confidence, and filtering document images with more than 20 valid words. After that, we manually annotate entity mentions on the new layouts as word sequences, based on the original entity annotations. Entity mentions that cannot match a word sequence on the new layout are deprecated. After the annotation, we divide the train/validation/test splits according to the original settings to obtain the revised datasets. In all, the proposed FUNSD-r and CORD-r datasets consists of 199 and 999 document samples including the image, layout annotation of segments and words, and labeled entities. Table <ref type="table" target="#tab_7">4</ref> lists the detailed summary statistics of the proposed datasets. The total number of segments, words, entities and entity types, the average length of segments and entities, and the sample number of data splits are displayed. Additionally, the continuous entity rate is introduced as the rate of entity whose tokens are continuous in the order of segment words. When fed into model, the continuous entities correspond to continuous token spans within inputs and are able to be predicted by sequence-labeling methods. Therefore, the continuous entity rate indicates the ordered degree of the document layouts in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Baselines for VrD-EL and VrD-ROP</head><p>For VrD-EL, we compare the proposed TPP with the following strong baseline models:</p><p>? GNN-based document encoders: GNN+MLP <ref type="bibr">(Carbonell et al., 2021)</ref> is the pioneer to adopt GNN as the document encoder for better modeling of document inputs. Doc2Graph <ref type="bibr" target="#b8">(Gemelli et al., 2022)</ref> is an enhanced GNN-based document encoder for VrD tasks. ? Transformer-based document encoders: Lay-outXLM <ref type="bibr">(Xu et al., 2021b</ref>) is a pre-trained document encoder for multilingual document inputs. BROS <ref type="bibr" target="#b11">(Hong et al., 2022)</ref>   performance on key information extraction tasks. ? Parsing-based methods: SPADE <ref type="bibr" target="#b14">(Hwang et al., 2021)</ref> treats VrD-EL as spatial-aware dependency parsing problem and adopts a spatialenhanced language model to address the task. SERA <ref type="bibr" target="#b40">(Zhang et al., 2021</ref>) also treats the task as dependency parsing, and adopts a biaffine parser together with a document encoder to address the task. ? Detection-based methods: MSAU-PAF <ref type="bibr" target="#b3">(Dang et al., 2021)</ref> tackles the task by integrating the MSAU architecture for detection <ref type="bibr" target="#b3">(Dang and Nguyen, 2021)</ref> and the PIF-PAF mechanism for link prediction <ref type="bibr" target="#b17">(Kreiss et al., 2019)</ref>.</p><p>For VrD-ROP, we compare the proposed TPP with LayoutReader <ref type="bibr">(Wang et al., 2021b)</ref>, which is a sequence-to-sequence model achieving strong performance on VrD-ROP. We refer to the reported performance in the original papers of these baseline methods in Table <ref type="table" target="#tab_2">2</ref> and<ref type="table" target="#tab_4">3</ref>. The backbone of all compared methods are of base-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>The experiments in this work are divided into two parts, as experiments on VrD-NER and on other tasks.</p><p>For VrD-NER, we adopt LayoutLMv3-base <ref type="bibr" target="#b12">(Huang et al., 2022)</ref> and LayoutMask <ref type="bibr" target="#b32">(Tu et al., 2023)</ref> as the backbones to integrate with Token Path Prediction. The maximum sequence length of textual tokens for both of them is 512. We use Global 1D and Segment 2D position information in LayoutLMv3, and Local 1D and Segment 2D in LayoutMask, according to the preferred settings. We adopt positional residual linking and multidropout to improve the efficiency and robustness in training TPP. Positional residual linking adds the 1D positional embeddings of each token to the backbone outputs to enhance 1D position signals that is crucial to the task. Multi-dropout duplicates the last fully-connect layer of TPP. Following the setting of <ref type="bibr" target="#b15">(Inoue, 2019)</ref>, the duplicated layers are trained with different dropout noises and their weights are shared, which enhances model robustness towards randomness. The effectiveness of these mechanisms are discussed in Appendix D. In fine-tuning, we generally follow the original setting of previous VrD-NER works <ref type="bibr" target="#b12">(Huang et al., 2022;</ref><ref type="bibr" target="#b32">Tu et al., 2023)</ref>. We use an Adam optimizer, with 1% linear warming-up steps, a 0.1 dropout rate, and a 1e-5 weight decay. For Token Path Prediction, the learning rate is searched from {3e-5, 5e-5, 8e-5}. On FUNSD, the best learning rate is 5e-5/5e-5 for LayoutLMv3/LayoutMask, while on CORD the learning rate is 3e-5/8e-5, respectively. In fine-tuning the comparing token classification models, we all set the learning rate to 5e-5. We adopt positional residual linking and multi-dropout in Token Path Prediction. We fine-tune FUNSD-r and CORD-r by 1,000 and 2,500 steps on 8 Tesla A100 GPUs, respectively, with a batch size of 16. The maximum number of decoded entities is limited to 100.</p><p>Besides to VrD-NER, we conduct experiments on VrD-EL and VrD-ROP adopting LayoutMask backbone. For VrD-EL, TPP-for-VrD-EL is finetuned by 1000 steps with a learning rate of 8e-5 and a batch size of 16, with the learning rate of the global pointer weights is set as 10x for better convergence. For VrD-ROP, TPP-for-VrD-ROP is fine-tuned by 100 epochs with a learning rate of 5e-5 and a batch size of 16, following the settings of <ref type="bibr">(Wang et al., 2021b)</ref>. The beam size is set to 8 during decoding. For all experiments, we choose the model checkpoint with the best performance on the validation set, and report its performance on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation Studies to TPP on VrD-NER</head><p>For better understanding of TPP on VrD-NER, we propose two questions: (1) from which setting TPP benefits most of positional encoding of backbones, and ( <ref type="formula">2</ref>) by what means positional residual linking and multi-dropout be helpful to TPP. We conduct the ablation studies to answer the questions.</p><p>For the first question, we alter the choices of 1D and 2D positional encoding of backbones, and the results are reported in Table <ref type="table" target="#tab_9">5a</ref>. For FUNSD-r, the preferred positional encoding setting of backbones  brings about the best performance. Although TPP is unaware of the order of tokens, the incorporation of better 1D positional signals can enhance the contextualized representation generated by the backbones, leading to improved performance. For CORD-r, we note that in some occasions, using word-level 2D position information brings about better performance. This is because short entities are the majority in CORD-r, especially entities with only one char, such as numbers and symbols. For these entities, word-level 2D position information brings about a direct hint to the model in training and prediction, resulting in better performance.</p><p>For the second question, we conduct an ablation study by removing the positional residual linking and multi-dropout of TPP. We observe from Table <ref type="table" target="#tab_9">5b</ref> that: (1) Positional residual linking and multidropout are both essential due to their effectiveness on the results of FUNSD-r. (2) Positional residual linking enhances the 1D position signals in model inputs. Comparing with vanilla TPP, TPP with positional residual linking behaves better on FUNSD-r but slightly worse on CORD-r. This is because segments in CORD-r are relatively short, when inputs are disordered, 1D signals are much more noisy and negatively affects model performance since they do not provide sufficient information. The results verify the function of positional residual linking as a enhancement mechanism of 1D signals.</p><p>(3) Multi-dropout enhances model robustness to random aspects. TPP outperforms its variant without multi-dropout among different backbones and different benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An overview of the training procedure of Token Path Prediction for VrD-NER. A TPP head for each entity type predicts whether an input tokens links to an another. The predict result is viewed as an adjacent matrix in which token paths are decoded as entity mentions. The overall model is optimized by the class-imbalance loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: The grid label design of TPP for VrD-EL and VrD-ROP. For VrD-EL, Under 25/25-34 is linking to 3.2/3.0 so each token between the two entities are linked. For VrD-ROP, the reading order is represented as a global path including all tokens starting from an auxiliary beginning token &lt;s&gt;, in which the linked token pairs are marked.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A comparison of the original (left) and revised (right) layout annotation. The revised layout annotation reflects real-world situations, where (1) character-level position boxes are annotated rather than word-level boxes, (2) every segment lies in a single row, (3) spatially-adjacent words are recognized into one segment without considering their semantic relation, and (4) missing annotation exists.</figDesc><graphic url="image-12.png" coords="6,66.88,70.91,112.50,96.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Case study of Token Path Prediction for VrD-NER, where entities of different types are distinguished by color, and different entities of the same type are distinguished by the shade of color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Question: Token Path: ? ? &gt;&gt; ? ? &gt;&gt; ? ? &gt;&gt; ? ? Prediction: # OF STORES SUPPLIED Header: Token Path: ? ? &gt;&gt; ? ? &gt;&gt; ? ? &gt;&gt; ? ? Prediction: NAME OF ACCOUNT # ? ? Grid Labels</head><label></label><figDesc></figDesc><table><row><cell>? ?</cell><cell cols="2">? ?</cell><cell cols="2">? ?</cell><cell></cell><cell></cell><cell>? ?</cell><cell></cell><cell>? ?</cell><cell></cell><cell cols="2">? ?</cell><cell></cell><cell></cell><cell>? ?</cell></row><row><cell>#</cell><cell cols="2">OF</cell><cell cols="3">STORES</cell><cell cols="3">NAME</cell><cell>OF</cell><cell cols="4">ACCOUNT</cell><cell cols="2">SUPPLIED</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Class-Imbalanced Loss</cell></row><row><cell></cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell cols="2">? ?</cell><cell>? ?</cell><cell>? ?</cell><cell cols="2">? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>for Binary Classification</cell></row><row><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The VrD-NER performance of different methods on FUNSD-r and CORD-r. Pre. denotes the pre-processing mechanism used to re-arrange the input tokens, where LR * /TPP * denotes that input tokens are reordered by a LayoutReader/TPP-for-VrD-ROP model, LR and TPP R are trained on ReadingBank, and LR C and TPP C are trained on CORD. Cont. denotes the continuous entity rate, higher for better pre-processing mechanism. The best F1 score and the best continuous entity rates are marked in bold. Note that TPP-for-VrD-NER methods do not leverage any reading order information from ground truth annotations or pre-processing mechanism predictions.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell cols="2">Pre. Cont. (%)</cell><cell>F1</cell><cell>Backbone</cell><cell>Method</cell><cell cols="2">Pre. Cont. (%)</cell><cell>F1</cell></row><row><cell>LayoutLMv3</cell><cell>Sequence Labeling</cell><cell>None LR TPPR</cell><cell>95.74 95.53 97.29</cell><cell>78.77 78.37 79.72</cell><cell>LayoutLMv3</cell><cell>Sequence Labeling</cell><cell>None LRC TPPC</cell><cell>92.10 82.10 92.43</cell><cell>82.72 70.33 83.24</cell></row><row><cell></cell><cell>TPP</cell><cell>None</cell><cell>-</cell><cell>80.40</cell><cell></cell><cell>TPP</cell><cell>None</cell><cell>-</cell><cell>91.85</cell></row><row><cell>LayoutMask</cell><cell>Sequence Labeling</cell><cell>None LR TPPR</cell><cell>95.74 95.53 97.29</cell><cell>77.10 77.24 80.70</cell><cell>LayoutMask</cell><cell>Sequence Labeling</cell><cell>None LRC TPPC</cell><cell>92.10 82.10 92.43</cell><cell>81.84 68.05 81.90</cell></row><row><cell></cell><cell>TPP</cell><cell>None</cell><cell>-</cell><cell>78.19</cell><cell></cell><cell>TPP</cell><cell>None</cell><cell>-</cell><cell>89.34</cell></row><row><cell></cell><cell cols="3">(a) VrD-NER on FUNSD-r</cell><cell></cell><cell></cell><cell cols="3">(b) VrD-NER on CORD-r</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The VrD-EL performance of different methods on FUNSD. The best result is marked in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The VrD-ROP performance of different methods on ReadingBank. For the Order setting, OCR denotes that inputs are arranged left-to-right and topto-bottom in evaluation. Shfl. denotes that inputs are shuffled in evaluation. r is the proportion of shuffled samples in training. The best results are marked in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Reading Order by OCR Result Desired Reading Order Disordered Layout Case</head><label></label><figDesc>is a pre-trained document encoder focuses on achieving better</figDesc><table><row><cell cols="2">Multi-row Content</cell><cell></cell><cell></cell><cell cols="3">????????? 21</cell><cell cols="2">?????? 21 ???</cell></row><row><cell cols="2">Seal with Askew or Overlapped Texts</cell><cell></cell><cell></cell><cell cols="3">???? ????(??) ????????????? ?????</cell><cell cols="2">????????? ???? ????????? ????(??)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">???X08 ????</cell><cell></cell><cell cols="2">X08 ???????13 01 ?</cell></row><row><cell cols="2">Print Distortion</cell><cell></cell><cell></cell><cell cols="3">???01 ???13 ?</cell><cell cols="2">????????? ???</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">??????? ???????</cell><cell cols="2">???????</cell></row><row><cell cols="2">Design Layout with</cell><cell></cell><cell></cell><cell></cell><cell>?????</cell><cell></cell><cell cols="2">?????</cell></row><row><cell cols="2">Various Font Sizes</cell><cell></cell><cell></cell><cell cols="2">?????(?)</cell><cell></cell><cell cols="2">??(?)???</cell></row><row><cell cols="2">Skewness due to Photography</cell><cell></cell><cell></cell><cell cols="3">? ? ?? ?? ?? ??? ?? 68040 ?? 16980</cell><cell cols="2">? ? ?? ?? ?? ??? ?? ?? 68040 16980</cell></row><row><cell cols="2">Semantic-driven Reading Order</cell><cell></cell><cell></cell><cell cols="3">???? ?? ???? 46525 ?? ????? ?? ? ?? 43.22</cell><cell cols="2">?? ?? ?? ?? ?? 46525 ?? ?? ? ? ? ????? 43.22</cell></row><row><cell>Dataset</cell><cell># of Segments</cell><cell># of Words</cell><cell>Avg. Length of Segment</cell><cell># of Entities</cell><cell>Avg. Length of Entity</cell><cell>Cont. (%)</cell><cell># of Entity Types</cell><cell># of Samples (Train/Val/Test)</cell></row><row><cell>FUNSD-r</cell><cell>10,091</cell><cell>166,040</cell><cell>16.45</cell><cell>7,924</cell><cell>15.21</cell><cell>95.74</cell><cell>3</cell><cell>149/-/50</cell></row><row><cell>CORD-r</cell><cell>12,582</cell><cell>123,153</cell><cell>7.55</cell><cell>12,582</cell><cell>9.20</cell><cell>92.10</cell><cell>30</cell><cell>799/100/100</cell></row></table><note><p><p><p>Figure 7: Examples of disordered layouts in real-world scenarios. These examples are taken from</p><ref type="bibr" target="#b39">(Yu et al., 2023)</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Statistics</figDesc><table /><note><p>of the proposed datasets. Cont. denotes the continuous entity rate, as the rate of entity whose tokens are continuous in the order of segment words. Higher continuous entity rate indicates to more orderly layouts.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Ablation study on the design of Token Path Prediction. Positional residual linking and multi-dropout are abbreviated as res. and mul., respectively.</figDesc><table><row><cell>Backbone</cell><cell cols="2">Position 1D 2D</cell><cell cols="2">FUNSD-r CORD-r</cell></row><row><cell></cell><cell cols="2">Global Segment</cell><cell>80.40</cell><cell>91.85</cell></row><row><cell>LayoutLMv3</cell><cell cols="2">None Segment</cell><cell>74.15</cell><cell>89.91</cell></row><row><cell></cell><cell>Global</cell><cell>Word</cell><cell>77.86</cell><cell>90.45</cell></row><row><cell></cell><cell cols="2">Local Segment</cell><cell>78.19</cell><cell>89.34</cell></row><row><cell>LayoutMask</cell><cell cols="2">Global Segment</cell><cell>66.24</cell><cell>87.36</cell></row><row><cell></cell><cell cols="2">None Segment</cell><cell>67.81</cell><cell>82.27</cell></row><row><cell></cell><cell>Local</cell><cell>Word</cell><cell>75.96</cell><cell>89.45</cell></row><row><cell cols="5">(a) Ablation study on different 1D and 2D positional encoding</cell></row><row><cell cols="2">choices of backbones.</cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell cols="2">Method</cell><cell cols="2">FUNSD-r CORD-r</cell></row><row><cell></cell><cell>TPP</cell><cell></cell><cell>80.40</cell><cell>91.85</cell></row><row><cell>LayoutLMv3</cell><cell cols="2">(w/o mul.)</cell><cell>79.61</cell><cell>90.99</cell></row><row><cell></cell><cell cols="2">(w/o mul. &amp; res.)</cell><cell>78.93</cell><cell>91.14</cell></row><row><cell></cell><cell>TPP</cell><cell></cell><cell>78.19</cell><cell>89.34</cell></row><row><cell>LayoutMask</cell><cell cols="2">(w/o mul.)</cell><cell>75.34</cell><cell>87.96</cell></row><row><cell></cell><cell cols="2">(w/o mul. &amp; res.)</cell><cell>65.53</cell><cell>89.55</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies to TPP-for-VrD-NER on position encoding choices of backbones and model design.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62206057</rs>,<rs type="grantNumber">61976056</rs>,<rs type="grantNumber">62076069</rs>), <rs type="funder">Shanghai Rising-Star Program</rs> (<rs type="grantNumber">23QA1400200</rs>), <rs type="funder">Natural Science Foundation of Shanghai</rs> (<rs type="grantNumber">23ZR1403500</rs>), <rs type="programName">Program of Shanghai Academic Research Leader</rs> under grant <rs type="grantNumber">22XD1401100</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yzqkar8">
					<idno type="grant-number">62206057</idno>
				</org>
				<org type="funding" xml:id="_MMcNYJg">
					<idno type="grant-number">61976056</idno>
				</org>
				<org type="funding" xml:id="_Weq7PSn">
					<idno type="grant-number">62076069</idno>
				</org>
				<org type="funding" xml:id="_Kg6kRkn">
					<idno type="grant-number">23QA1400200</idno>
				</org>
				<org type="funding" xml:id="_BTSQmY6">
					<idno type="grant-number">23ZR1403500</idno>
					<orgName type="program" subtype="full">Program of Shanghai Academic Research Leader</orgName>
				</org>
				<org type="funding" xml:id="_YQjKRCw">
					<idno type="grant-number">22XD1401100</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alicia Forn?s, and Josep Llad?s. 2021. Named entity recognition and relation extraction with graph neural networks in semi structured documents</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="9622" to="9627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An effective transition-based model for discontinuous ner</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecile</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5860" to="5870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end hierarchical relation extraction for generic form understanding</title>
		<author>
			<persName><forename type="first">Tuan</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duc</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang</forename><surname>Bach Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Wei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename><forename type="middle">Dat</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5238" to="5245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end information extraction by character-level embedding and multi-stage attentional u-net</title>
		<author>
			<persName><forename type="first">Tuan-Anh Nguyen</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dat-Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00952</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fuzzybio: a proposal for fuzzy representation of discontinuous entities</title>
		<author>
			<persName><forename type="first">Suzan</forename><surname>Ar Dirkson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wessel</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><surname>Holderness</surname></persName>
		</author>
		<author>
			<persName><surname>Jimeno Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Lavelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Minard</surname></persName>
		</author>
		<author>
			<persName><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName><surname>Rinaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</title>
		<meeting>the 12th International Workshop on Health Text Mining and Information Analysis</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lambert: Layoutaware language modeling for information extraction</title>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Garncarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Stanislawek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Topolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Halama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micha?</forename><surname>Turski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Grali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Nski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Doc2graph: a task agnostic document understanding framework based on graph neural networks</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gemelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Civitelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Llad?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Marinai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.11168</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document understanding</title>
		<author>
			<persName><forename type="first">Zhangxuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4583" to="4592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Setgner: General named entity recognition as entity set generation</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3074" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bros: A pre-trained language model focusing on text and layout for better key information extraction from documents</title>
		<author>
			<persName><forename type="first">Teakgyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonseok</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10767" to="10775" />
		</imprint>
	</monogr>
	<note>Daehyun Nam, and Sungrae Park</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Layoutlmv3: Pre-training for document AI with unified text and image masking</title>
		<author>
			<persName><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1145/3503161.3548112</idno>
	</analytic>
	<monogr>
		<title level="m">MM &apos;22: The 30th ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-10-10">2022. October 10 -14, 2022</date>
			<biblScope unit="page" from="4083" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Icdar2019 competition on scanned receipt ocr and information extraction</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1516" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial dependency parsing for semi-structured document information extraction</title>
		<author>
			<persName><forename type="first">Wonseok</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyeong</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sohee</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="330" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-sample dropout for accelerated training and better generalization</title>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Inoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09788</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Funsd: A dataset for form understanding in noisy scanned documents</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Jaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hazim</forename><surname>Kemal Ekenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">2021a. Structurallm: Structural pre-training for form understanding</title>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">2022a. Pp-ocrv3: More attempts for the improvement of ultra lightweight ocr system</title>
		<author>
			<persName><forename type="first">Chenxia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoting</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baohua</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03001</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">2021b. A span-based model for joint overlapped and discontinuous named entity recognition</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4814" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unified named entity recognition as word-word relation classification</title>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10965" to="10973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">2022c. Dit: Self-supervised pre-training for document image transformer</title>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1145/3503161.3547911</idno>
	</analytic>
	<monogr>
		<title level="m">MM &apos;22: The 30th ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">October 10 -14, 2022</date>
			<biblScope unit="page" from="3530" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2021c. Structext: Structured text understanding with multi-modal transformers</title>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuechen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiameng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toe: A grid-tagging discontinuous ner model enhanced by embedding tag/word relations and more fine-grained tags</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="177" to="187" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cord: a consolidated receipt dataset for post-ocr parsing</title>
		<author>
			<persName><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bado</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeheung</forename><surname>Surh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ernielayout: Layout knowledge enhanced pre-training for visually-rich document understanding</title>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinxu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06155</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Text chunking using transformation-based learning. Natural language processing using very large corpora</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="157" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kleister: key information extraction datasets involving long documents with complex layouts</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Stanis?awek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Grali?ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Wr?blewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawid</forename><surname>Lipi?ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Kaliska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulina</forename><surname>Rosalska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Topolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemys?aw</forename><surname>Biecek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition-ICDAR 2021: 16th International Conference</title>
		<meeting><address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-09-05">2021. September 5-10, 2021</date>
			<biblScope unit="page" from="564" to="579" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Project deepform: Extract information from documents</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Stray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stacey</forename><surname>Svetlichnaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Global pointer: Novel efficient spanbased approach for named entity recognition</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03054</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing disjoint clinical concepts in clinical text using machine learning-based methods</title>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA annual symposium proceedings</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">1184</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Layoutmask: Enhance text-layout interaction in multi-modal pre-training for document understanding</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18721</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lilt: A simple yet effective language-independent layout transformer for structured document understanding</title>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7747" to="7757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2021a. Discontinuous named entity recognition as maximal clique discovery</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="764" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2021b. Layoutreader: Pre-training of text and layout for reading order detection</title>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page" from="4735" to="4744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">2021a. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinei</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2579" to="2591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Layoutlm: Pretraining of text and layout for document image understanding</title>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1192" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">2021b. Layoutxlm: Multimodal pre-training for multilingual visually-rich document understanding</title>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinei</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08836</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjun</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.03287</idno>
		<title level="m">Icdar 2023 competition on structured text extraction from visually-rich document images</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Entity relation extraction as dependency parsing in visually rich documents</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuyi</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2759" to="2768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
