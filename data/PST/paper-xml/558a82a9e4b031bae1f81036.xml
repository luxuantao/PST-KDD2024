<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Performance of Checksums and CRC&apos;s over Real Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Stone</surname></persName>
							<email>jonathan@dsg.stanford.edu</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Michael</forename><surname>Greenwald</surname></persName>
							<email>michaelg@dsg.stanford.edu</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Craig</forename><surname>Partridge</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Hughes</surname></persName>
							<email>hughes@stortek.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">BBN Technologies (part of GTE Corporation)</orgName>
								<address>
									<postCode>02318</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Storage Technology Corporation</orgName>
								<address>
									<postCode>55428</postCode>
									<settlement>Brooklyn Park</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Performance of Checksums and CRC&apos;s over Real Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B5518480B224DAE05B7371E42C5BEDC3</idno>
					<note type="submission">received December 9, 1996; revised May 18, 1998; approved</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Codes, internetworking</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Checksum and cyclic redundancy check (CRC) algorithms have historically been studied under the assumption that the data fed to the algorithms was uniformly distributed. This paper examines the behavior of checksums and CRC's over real data from various UNIX file systems. We show that, when given real data in small to modest pieces (e.g., 48 bytes), all the checksum algorithms have skewed distributions. These results have implications for CRC's and checksums when applied to real data. They also can cause a spectacular failure rate for both the TCP and ones-complement Fletcher checksums when trying to detect certain types of packet splices. When measured over several large file systems, the 16 bit TCP checksum performed about as well as a 10-bit CRC.</p><p>We show that for fragmentation-and-reassembly error models, the checksum contribution of each fragment are, in effect, colored by the fragment's offset in the splice. This coloring explains the performance of Fletcher's sum on nonuniform data, and shows that placing checksum fields in a packet trailer is theoretically no worse than a header checksum field. In practice, TCP trailer sums outperform even Fletcher header sums.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>over 5% of the time. We particularly examine the effects of this phenomenon when applied to the Internet checksum used for IP, TCP, and UDP <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref> and compare it to two variations of Fletcher's checksum. We also report on an experiment with placing the standard TCP checksum in a packet trailer. A trailer checksum noticeably increases the checksum's effectiveness, and we prove why this is so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CRC'S VERSUS CHECKSUMS</head><p>Before examining the behavior of different algorithms, it is worth briefly discussing the CRC and checksum algorithms we used.</p><p>CRC's are based on polynomial arithmetic, base 2. CRC-32 <ref type="bibr" target="#b4">[5]</ref> is a 32-bit polynomial with several useful error detection properties. It will detect all errors that span less than 32 contiguous bits within a packet and all 2-bit errors less than 2048 bits apart. It will also detect all cases where there are an odd number of errors. For other types of errors, if they occur in data which has uniformly distributed values, the chance of not detecting an error is 1 in 2 .</p><p>The concept of a checksum is less well defined. For the purposes of data communication, the goal of a checksum algorithm is to balance the effectiveness at detecting errors against the cost of computing the check values. Furthermore, it is expected that a checksum will work in conjunction with other, stronger, data checks such as a CRC. For example, MAC layers are expected to use a CRC to check that data was not corrupted during transmission on the local media, and checksums are used by higher layers to ensure that data was not corrupted in intermediate routers or by the sending or receiving host.</p><p>The fact that checksums are typically the secondary level of protection has often led to suggestions that checksums are superfluous. Hard won experience, however, has shown that checksums are necessary. Software errors (such as buffer mismanagement) and even hardware errors (such as network adapters with poor DMA hardware that sometimes fail to fully DMA data) are surprisingly common and checksums have been very useful in protecting against such errors.</p><p>The two most popular checksums are the Internet checksum used for IP, TCP, and UDP <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and Fletcher's checksum <ref type="bibr" target="#b1">[2]</ref>. They represent different balances between performance cost and error detection.</p><p>The TCP checksum is a 16-bit ones-complement sum of the data. This sum will catch any burst error of 15 bits or less <ref type="bibr" target="#b7">[8]</ref>, and all 16-bit burst errors except for those which replace one 1's complement zero with another (i.e., 16 adjacent 1 bits replaced by 16 zero bits, or vice-versa). Over uniformly distributed data, it is expected to detect other types of errors at a rate proportional to 1 in 2 . The checksum also has a major limitation: the sum of a set of 16-bit values is the same, regardless of the order in which the values appear. The checksum was chosen by the Internet community in the late 1970's after experimentation on the ARPANET suggested the checksum was good enough and could be implemented efficiently.</p><p>Fletcher's checksum is designed to be a more robust error detecting code. The checksum keeps two sums. One sum is a running sum of the data in 8-bit chunks. The other sum is a running sum of each byte multiplied by its position from the end of the packet. This multiplication incorporates positional information into the checksum to protect against movement or transposition of data within the packet. The two 8-bit sums are concatenated to generate a 16-bit checksum. Fletcher also defined a 32-bit version, where 16-bit sums are kept. The algorithm was defined for both ones and twos-complement arithmetic. The version used for the TP4 checksum and in this paper uses 8-bit chunks. When performed in twoscomplement, this 16-bit checksum detects all single bit errors, a single error of less than 16 bits in length, and all double bit errors separated by 16 bits or less. Though TP4 uses only the twos-complement version, we investigated both ones-and twos-complement Fletcher sums.</p><p>Historically, the TCP checksum and Fletcher's checksum have been viewed as offering a sharp tradeoff between performance and error detection capabilities. The TCP checksum requires one or two additions per machine word of data (assuming the machine word is a multiple of 16 bits long), while Fletcher's sum requires two additions per byte (even if the computation is done in word-sized chunks) <ref type="bibr" target="#b10">[11]</ref>. As a result, measurements have typically shown the TCP checksum to be two to four times faster <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>. However, that difference may be declining on newer processors, where the memory access time dominates any computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. WORK WITH AAL5</head><p>This study began as a study of the error scenarios for packet splices in asynchronous transfer mode (ATM) adaptation layer 5 (AAL5). The AAL5 work helps motivate the rest of the paper and so is explained briefly here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. What is a Packet Splice?</head><p>AAL5 sends packets as a series of ATM cells, with the last cell specially marked using a bit in the ATM header. A packet splice occurs when the right number of cells are dropped such that pieces of two adjacent packets are combined so that they appear to represent one AAL5 packet. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates a splice: two four-cell packets suffer a loss of four cells, such that the first and third cell of the first packet and the first and last cells of the second packet are spliced together to look like a single four-cell packet. It should be noted that ATM does not allow cells to be re-ordered, thus the number of possible splices is limited to those that merely drop, and do not reorder, cells.</p><p>Several conditions must be met for a splice to be valid. First, AAL5 stores the length of the packet in the last cell, so the size of the splice must be consistent with the AAL5 length in the last cell. Second, because AAL5 specially marks the last cell of every packet, the last cell of the first packet cannot be part of the splice. Third, the first 40 bytes of the first cell must be a valid TCP/IP header (i.e., have a length consistent with the packet length and certain bits must be set). Unless all three of these requirements are met, the splice will be easily detected without confirming the CRC or checksum.</p><p>If the three requirements are met, then the splice has to be detected by either the AAL5 CRC (CRC-32) or the higher layer protocol's checksum (such as the TCP or Fletcher's checksum).</p><p>In 1993, an informal study by B. Marshall and C. Kalmanek at AT&amp;T Bell Labs simulated file transfers from a UNIX filesystem (using real data from the filesystem) and examined the performance of the AAL5 CRC. They found a surprising number of cases where the packet splice passed the AAL5 CRC, leading them to wonder if the AAL5 CRC was strong enough. With Marshall's and Kalmanek's assistance, the authors set out to do a more complete set of tests. Those results were reported in an earlier version of this paper, presented at SIGCOMM'95 <ref type="bibr" target="#b6">[7]</ref>. Some open questions and surprising results led us to perform a new and more comprehensive series of tests to resolve these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Testing Splices</head><p>Our test program simulated a file transfer with the file transfer protocol (FTP) of all files on a file system (or selected directories of a file system) via TCP/IP using AAL5 over ATM. All IP and TCP header fields were filled in as if the file transfer were being done over the loopback interface (127.0.0.1). For each packet, the TCP sequence number was incremented by the data length, and the IP ID was incremented by one. The program then examined all possible splices of two adjacent TCP segments and checked to see if either the TCP checksum or AAL5 CRC failed to detect the splice. The program did not concern itself with splices whose data exactly matched a valid packet, nor with those splices that were detected by IP, TCP, or AAL5 header/trailer checks.</p><p>The test program was run over file systems at Storage Technologies, the Swedish Institute of Computer Science (SICS), and Stanford University. The TCP segment sizes examined were 256 bytes long, except for runt packets at the end of files. The first row in Tables I-III counts the total number of splices inspected. The next row counts how many invalid splices were detected by simple header checks, and so did not need to check the checksum. The row labeled "identical data" records how many splices resulted in packets that were identical to one of the original packets, and hence would not result in corrupted data (the checksum, of course, was identical). The "Remaining" packets were all incorrect and depended on the checksum and the CRC to detect the corruption. All percentages listed are computed as percent of "remaining splices." The rows following "remaining" list the splices missed by the CRC test and the TCP checksum test. There were no splices missed by both CRC and the TCP checksum. The data from each site are broken down by file system. The total number of splices is greater than 2 . We would expect that the CRC of a splice would match the CRC of the original AAL5 packet at a rate of 1 in 2 (or 0.000 000 023 2% of the time). Similarly, we would expect that the TCP checksum would fail to catch bad splices at a rate of 1 in 2 (or 0.001 526% of the time). Observe that for the CRC, the CRC must match the CRC of the second AAL5 packet, while for TCP, the checksum over the entire splice must equal zero.</p><p>The tables show that for real data, the CRC failure rate is almost perfectly consistent with the expected failure rate for random data, and is therefore not the subject of much further investigation in this paper. 1 For TCP, however, the story is 1 The difference between our results and those of Marshall and Kalmanek are the "identical data" entries. Given that the payloads were identical, it is not a failure if the CRC does not detect these splices as no data-corruption occurs. Their tests did not distinguish the cases of splices with identical data from splices with different data but congruent checksums.  different. Between 0.008% and 0.22% of the bad splices passed by the header checks passed the checksum. This is between a factor of 10 and 100 worse than expected, and requires some explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPLAINING THE TCP CHECKSUM FAILURES</head><p>Why does the TCP checksum fail to detect so many splices? The reasons have to do with the distribution of data values and how data from one packet can be mixed with data from another packet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Failure Scenarios</head><p>We can compute the TCP checksum in pieces and then add the pieces to get the complete packet sum. So, we can think of the TCP checksum of a packet broken into ATM cells as being the sum of the individual checksums of each 48-byte cell.</p><p>The usual requirement for a splice to pass the TCP checksum is that the checksum of the splice add up to the checksum of the entire first packet contributing to the splice. Because the splice contains cells of the first and second packets, this requirement can also be expressed as a requirement that the checksum of the cells from the first packet not included in the splice must equal the checksum from the cells of the second packet that are included in the splice. If just one cell from the second packet is included in the splice, this requirement reduces to the requirement that the checksum of the cell from the second packet have the same sum as the cell it replaces.</p><p>In multicell replacements, the sum of the mixes of cells must be equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Distributions of the TCP Checksum</head><p>Given random data, a good checksum or CRC should uniformly scatter the checksum values over the entire checksum space. Obviously a checksum algorithm that does not uniformly distribute checksum values (i.e., has hotspots) will be more likely to have multiple cells with the same checksum. Theorem 6 in Appendix A proves that, over uniformly distributed data, the TCP checksum algorithm gives a uniform distribution of checksum values. <ref type="foot" target="#foot_0">2</ref> Thus, any hotspots in the distribution of checksum values are due to nonuniformity of the data, and are not inherent in the TCP checksum algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Distribution of Checksum Values over Single Cells</head><p>If the distribution of 16-bit words is completely uniform, the chance of an arbitrary sequence of data in the first packet having the same checksum as an equal-sized arbitrary sequence of data in the second packet is , where . However, the distribution of values over real data is not uniform.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows three plots summarizing the distribution of checksum values on the filesystem on smeg.dsg. stanford.edu. The -axis represents different checksum values, sorted by frequency to better show the distribution. In the PDF graphs, the -axis is the probability that the given checksum value occurred. Fig. <ref type="figure" target="#fig_1">2(a)</ref> shows the entire PDF, and (b) shows a blowup of the most frequent 65 values (0.1%) The CDF [Fig. <ref type="figure" target="#fig_1">2(c</ref>)] shows the same 65 values, but here the -value for a given represents the cumulative probability that any of the most common values occurred. If the distribution were uniform then the PDF should simply be a horizontal line at , and the CDF a straight line with slope . This data shows that the TCP checksum on real data has hotspots. In the file system shown in the figure (smeg:/u1), the top 0.1% of the checksum values occurred 2.5% of the time. If one examines this distributional data over many filesystems, one discovers two things. First, that the single most common checksum value (usually zero) occurs between 0.01% and 1% of the time. Second, that for 48 byte cells the 65 next most frequently occurring checksum values (0.1% of the checksum space) account for between 1% and 5% of the checksum values seen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Checksum Distribution over Larger Blocks of Data</head><p>Although for uniformly distributed data values the probability distribution of the checksum is uniform independent of the length of the block of data, this is not true for nonuniform data. In that case, the expected probability distribution of the checksum may be computed by where is the probability that the checksum over a block of length is equal to , and where is taken mod . The dotted line in Fig. <ref type="figure" target="#fig_1">2 labeled "</ref>Predict " shows the expected distribution of checksums over blocks 2 cells long, given the checksum distribution over one cell given by . So, if the nonuniformity is uniform-that is, that every cell of data is drawn from the same probability distribution, and that the sum is the sum of independent samples-then we would expect the distribution of the sums to conform closely to the dotted line in our graphs. The predicted value for is already close to uniform for all but the 20 most common values, even though is decidedly nonuniform. Corollary 3 and Theorem 4 in the Appendix show that, regardless of the original distribution, the distribution should get more uniform as increases.</p><p>However, our measurements show that the nonuniformity extends to larger chunks than single words or cells, and that the checksum of one cell is correlated with the checksums of the neighboring cells. The lines labeled , , etc. show the measured distribution of checksums over samples of blocks of length cells over real data in our file system. The data does get more uniform (seen most clearly in the CDF), but nowhere as quickly as it should if the cells were roughly independent. We believe the samples should be somewhat representative even of noncontiguous blocks. Once again, the checksum values are sorted in decreasing order of probability, to give a clearer picture of the distribution. Note that even over the larger block sizes, although the probability of a match decreases slightly, the distribution is still significantly more nonuniform than expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Filesystem-Level Nonuniformity is Not the Answer</head><p>Given the nonuniform distribution, what, then, is the expected failure rate of the IP/TCP checksum in detecting splices for a given distribution of checksum values? As discussed above, it is simply the probability that the checksum over the cells missing from the first packet is equal to the checksum over the cells present from the second packet. For a given probability distribution , and assuming that every cell is drawn from the same distribution, this probability is Table IV computes this probability using the measurements of the Stanford file system from Fig. <ref type="figure" target="#fig_1">2</ref>. It lists the probability that the checksums of two blocks, each cells long, drawn from anywhere in the same filesystem, will be equal. For each block of length cells, the "uniform" column shows the expected probability given uniform distribution. The "predicted" column shows the probability predicted assuming each cell is drawn from the identical, nonuniform distribution. (The particular distribution is the one actually measured for single cells over the smeg:/u1 file system.) The last column lists the probability actually measured for each block size over the entire file system.</p><p>Table <ref type="table" target="#tab_3">IV</ref> shows us that the actual measurements do not match the predictions. There are two issues our initial model ignored.</p><p>First, we have measured the probability distribution over the entire file system for chunks of cells, but we know that distribution of data values is heavily dependent on file type (binary versus character, executable versus GIF, even Shakespeare versus Joyce). Splices come from adjacent packets, which usually come from the same file. Thus, real failure rates could be higher than the averaged global distribution would suggest.</p><p>For example, consider an extreme (and extremely hypothetical) case in which a file system consists of half binary and half textual data. Imagine that 90% of the cell-sized chunks of binary data had a checksum of 0 0000, and that 90% of the cell-sized chunks of textual data had a checksum of 0 1F00. Considered globally, we would find 0 0000 45% of the time and 0 1F00 45% of the time, so would be approximately 32% and we would predict about 32% of the packet splices would incorrectly pass the checksum. However, in reality, for any given file the local distribution would find the most common checksum 90% of the time, and thus the failure rate would be about 81%. Therefore, the global distribution of checksums (measured across an entire filesystem) is not sufficient to accurately predict checksum failure rate: a more localized distribution of checksums is needed.</p><p>Second, if two cells have congruent checksums because the data was identical, then replacement of one cell by the other is not a checksum failure-the packet is unaltered and no corruption will occur. To accurately predict meaningful checksum failures, then, we need to subtract both congruent and equal cells from the probability of a match. In a system with uniformly distributed data the odds of finding two 48 byte cells with identical data is 1 in 2 , which is so unlikely as to be utterly neglible. However, in practice it occurs far more frequently. Our actual measurements show that the most common reason for checksum congruence is identical data-identical splices occur 20 to 40 times more frequently than congruent-but-unequal splices. This result is another example of nonuniform distribution of the data, but, in this case, a benign one: the undetected error has not corrupted user data. 3   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Localized Nonuniformity of Data</head><p>Table <ref type="table" target="#tab_4">V</ref> shows how the probability changes when we restrict the comparisons to only look at local data. The first column (identical to the last column in Table <ref type="table" target="#tab_3">IV</ref>) displays the probability of taking two blocks of data, each cells long, from anywhere in the entire file-system, and finding that their IP checksums were congruent to each other. The column labeled "locally congruent" shows the same probability if we limit the search to be within 2 packet lengths (512 bytes). (In order to increase the sample size for the local comparisons, we did not restrict ourselves to contiguous blocks). The final column shows how the probability decreases when we exclude checksum matches for a pair of blocks that contained identical data, as such a substitution would not result in any data corruption. It is still significantly higher than the global rate. (Recall, that if the data were uniformly distributed then every entry in this table should be 0.001 526%). If the checksum failures are purely a result of nonuniform distribution, then these sample probabilities should track the measured TCP checksum failure rates.</p><p>Table <ref type="table" target="#tab_5">VI</ref> compares this distribution data for several file systems with the actual rate of checksum failures for comparablelength substitutions. Note that there is a minor difference between the way data was collected to compute the predicted values and the measured values. The predicted values are computed for full 48-byte cells. However, the measured values include a large number of cells with only 8 bytes of data. The reason is that the measured data was collected as part of computing possible packet failures for 256-byte packets, and with the 40-byte TCP/IP header, a 256-byte packet has only 8 bytes of data in the first and last cells.</p><p>However, even allowing for the deficiencies in measuring actual failures, what Table VI suggests is that local congruence still does not fully explain the checksum failure rate. To fully explain the failure rate, we need a new way to think about checksum data patterns, which is presented below in Section V-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. REDUCING CHECKSUM FAILURES</head><p>In this section, we look at various ways to reduce the checksum failure rates. 3 But the lost packet must still be recovered; see Section V-D. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Regaining a Uniform Distribution: Compression</head><p>We claim that the TCP checksum's failure to detect many splices is due to the nonuniform distribution of the data being summed. One obvious way to deal with nonuniform data patterns is to compress the data. As an experiment to verify that our diagnosis was correct, we compressed all the files in the file system at SICS that gave the TCP checksum the most trouble ( /opt on fafner.sics.se ) and ran our tests on the compressed files. (The compression was Lempel-Ziv, and was performed using the UNIX compress command.) The results are shown in Table <ref type="table" target="#tab_6">VII</ref>, using the same format as Table <ref type="table" target="#tab_0">I</ref>. The interesting result is that the number of splices that passed the checksum is approximately 0.0021%, which is close to the expected rate on uniform data of 0.0015%. This result is a hundred times improvement over the 0.17% rate before compression. So compression clearly helps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Alternative Checksums: Fletcher</head><p>It is not always possible or desirable to compress the data. Another obvious question to ask is whether, without data compression, another checksum algorithm would perform better than TCP's. An obvious candidate checksum is Fletcher's checksum <ref type="bibr" target="#b12">[13]</ref>. <ref type="foot" target="#foot_1">4</ref> With our error model, where cells are dropped but no random data is inserted, we might expect the positional term to improve error detection.</p><p>As with TCP, we can compute and analyze Fletcher's checksum over individual cells rather than entire packets. Recall that the term of the Fletcher checksum is computed by multiplying each byte by its offset from the end of the packet. We can also compute a local Fletcher checksum over one cell as , and . To compute the contribution of an individual cell to the total Fletcher sum for the packet, we add to and add to , where and is the offset of the end of the cell from the end of the packet. It should be noted that since all the shifts of data are by a multiple of the cell size (48 bytes), the contribution of the term for each cell to detect motion is limited to 1 from, at most, values (85 and 16 for 1 and 2's complement, respectively). Both 85 and 16 are considerably smaller than (255 or 256, respectively). Table <ref type="table" target="#tab_7">VIII</ref> shows the actual results for both 1's complement ( 255) and 2's complement ( 256) Fletcher's checksum over several filesystems. The results of the TCP checksum on those filesystems is included for comparison.</p><p>We see that Fletcher's, in general, out-performs the TCP checksum, and in some cases comes within a factor of 2 to a 1 in 2 miss rate. This performance is curious given our results so far. First, Corollary 8 in the Appendix shows that, for uniformly distributed data and replacements larger than single words, Fletcher should not be any stronger than IP/TCP. Second, two empirical measures show that both TCP and Fletcher have a similar nonuniform distribution over individual cells. When looking at plots of checksums over 48-byte cells (Fig. <ref type="figure" target="#fig_2">3</ref>), the Fletcher's checksum looks to have a nonuniform curve similar to that of TCP. And when we look at the probability of the checksum that two randomly chosen cells<ref type="foot" target="#foot_2">5</ref> in the file system match each other, we find a probability of 0.016% for Fletcher 255, 0.013% for Fletcher 256, and 0.011% for IP/TCP. each packet, so the number does not match the "Measured Global" for k = 1, given earlier. Why then does Fletcher perform better than the TCP checksum? The most obvious effect is that the positional dependence of Fletcher's checksum effectively increases the number of cells changed in a splice. The vast majority of splices which pass IP and TCP header checks include the header cell from the first packet, and therefore the checksum field from the first packet. Each cell from the first packet not included in the splice moves all the subsequent cells from the first packet closer to the start of the splice-thus increasing the 's component of their contribution to the field of the splice's checksum, when compared to their contribution to the first packet's checksum. And even if the inserted cells from the second packet are identical to the dropped cells, their 's for the dropped cells is different than the 's of the inserted cells, as they appear later in the splice than in the first packet. The positionality of Fletcher's checksum means that the effective size of the splice is not just the total number of cells replaced, but includes any intervening, "reshuffled" cells from the first packet which lie between the first drop and the last replacement. (Note that this result has no effect on splices that join a prefix of the first packet to a suffix of the second.)</p><p>The cause of the performance difference is subtle. Recall that the condition for checksum failure is that the sum of the 8-bit 's be congruent and that the sum of the 's also be congruent. The condition on the 's is identical to the condition for IP checksums. Since the data cells are drawn from the same highly localized nonuniform distribution, their 8-bit terms have a fairly good chance of being congruent-at least 256 times more than the standard 16bit TCP sum. But for the term, each of the terms for individual cells are multiplied by . This multiplication permutes the entire distribution. Thus, a given highly probable drawn from one is unlikely to be drawn from , the distribution of terms for a nearby cell drawn from the same distribution. In effect, the contribution of each cell to the term of its packet is colored by its offset from the end of the packet (think of coloring the cells by their number). This coloring, and the nonuniformity, combine to make undetected splices less likely. It is well known (we provide a proof in Lemma 9) that the probability of drawing two identical values from a nonuniform distribution is always higher than the probability of drawing two values that differ by any fixed amount. (This issue is discussed further in the Appendix). Since the data is nonuniform, some terms are more likely than others. The coloring effect of the term means that the overall sums of a splice are less likely to be congruent to the original checksum than if the data was uniformly distributed.</p><p>The end result is that the standard TCP checksum fails if two observations drawn from the same distribution are equal, while Fletcher fails if two observations drawn from the same distribution differ by a particular amount (where the exact amount varies from splice to splice). Thus, nonuniformity of the data actually strengthens the field of the checksum.</p><p>Ones complement Fletcher, however, has a weakness that sometimes offsets its probabilistic advantage: since bytes containing either 255 or 0 are considered identical by the checksum, certain common pathological cases cause total failure of Fletcher-255. This problem is discussed in more detail in Section V-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Trailer Checksums: Making Nonuniformity Work for Us</head><p>Fletcher-256 succeeds in detecting more splices than TCP by taking advantage of the nonuniformity of the data distributions, but it still has drawbacks. It is more expensive to compute, and the nonuniformity can only strengthen 8 bits of the checksum. It turns out that we can use a similar trick to exploit nonuniformity for the standard Internet checksum, with no computational cost. Further, we can strengthen the entire 16-bit sum, giving us (for some distributions) 16-bit checksums that are even stronger than 1 in 2 .</p><p>The key observation is that with header checksums, the packet header and the packet checksum are located in the same cell of a packet. Thus, either both the header and the checksum covering it are present in a given splice, or neither are. The IP header check and syntactic TCP header checks ensure that almost all splices which are actually checksummed include the header from the first packet. The resulting splices will have the first packet's TCP header, including TCP sequence number, ACK field, and checksum. As long as the replacement cells in the splice have the same overall checksum as the original packets, the TCP checksum will not detect the splice. Fig. <ref type="figure" target="#fig_3">4</ref> shows one such splice diagrammatically. If the TCP checksum was at the end of the TCP packet, instead of in the header, the TCP checksum value would not share fate with the TCP pseudo-header which it covers. Fig. <ref type="figure" target="#fig_4">5</ref> shows the same splice as Fig. <ref type="figure" target="#fig_3">4</ref>, but with the TCP checksum located in a packet trailer instead of the packet header. Here, the resulting splice has the TCP header from the first packet and the checksum from the second packet. (It also has the header from the second packet, but only half the splices will do so.)</p><p>The data cells are, as with Fletcher's sum, all drawn from the same localized nonuniform distribution and are more likely than 1 in 2 to have congruent sums. But compare the two headers. The only field that changes between adjacent TCP packets in a given flow is the TCP sequence number. The difference between the checksums of the header cells of adjacent packets in a single flow is therefore strongly clustered around the size of the payload. In other words there are actually three different distributions of cells in a packet pair: the payload data, the first header, and the second header. If we separate the checksum value away from the header that it sums and put it in a trailer, we can ensure that there are always three different colors in any given splice-even for splices that only make color-preserving substitutions (e.g., data cell for data cell). Again by Lemma 9, this higher degree of coloring leads to a higher probability of detecting a splice than the standard header checksum, as we show below by case analysis. 6  What is the probability of a trailer checksum failing? It is simply that the checksum of the cells inserted from the first packet equal the checksum of the cells dropped from the second packet. (Note that we take the second packet-the source of the trailer sum-as the original, and counting cells from the first packet as insertions.)</p><p>The inserted cells from the first packet almost always include a header cell. The inserted cells from the first packet thus have a sum drawn from a distribution that consists of one header cell and data cells. If the second header is dropped, then we again have data cells and 1 header cell. However, in half of the splices the dropped cells are all data cells, in which case their sum consists of data cells. The resulting probability will be lower than the probability of an exact match between two checksums drawn from the same distribution (as shown in <ref type="bibr">Lemma 9)</ref>. The failure probability appears to be reduced by at most a factor of two.</p><p>In the remaining half of the splices, the second header is one of the cells dropped. Here the distribution of checksums of the header cell of the second packet does not match the 6 Our study of trailer checksums was originally motivated by noticing that the AAL5 trailer checksums avoided this fate-sharing, and conjecturing that exploiting the predictable header differences with TCP trailer checksums would improve performance. Trailers performed surprisingly well, leading us to the preceding reanalysis of the Fletcher checksum results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE IX TRAILER CHECKSUM RESULTS (256 BYTE PACKETS ON SYSTEMS)</head><p>distribution of the first header cell. There are two causes for this result. First, we treat the first header as a header but the second as data, which means we checksum the IP header of the second cell, but not of the first. Second, the header is mostly constant between packets except for an increase in the IP ID field and the TCP sequence number. (Note that in this scenario there is no checksum in the header: the field is left zero; though a practical trailer implementation might perhaps choose to swap the checksum value and the last two bytes of the packet.)</p><p>How much lower will the probability of failure be? We conducted an experiment to measure the effectiveness of trailer checksums. We changed the simulator to model a protocol identical to TCP, except that the TCP header checksum is left zero, and the checksum value is appended to the end of the TCP data. The results are shown in Table <ref type="table" target="#tab_8">IX</ref>. The failure rate of trailer checksums were significantly better than those of TCP and Fletcher. We note that the failure rate was actually below for significant fractions of some file systems. In most cases we noted a failure rate 20-50 times lower than for header checksums.</p><p>We can further test the distribution-coloring analysis by making predictions about the standard header TCP checksum. The number of splices which do not include the header of the first packet are negligible, so there are only two cases: the first header cell followed by all-data cells, and the header from the first packet followed by a mix of data cells and the header from the second packet. In the latter case, the splice has replaced a data cell with the header cell from the second packet, and thus should be much less likely to match than the first case. When we went back and examined the data, this prediction was correct. Although roughly half of the splices surviving the header check have the second header included, only 1 in 2 of those passed the TCP checksum. The TCP header checksum was 100-200 times more effective against splices that contained the second header. This result both supports our explanation of the good performance of trailer TCP checksums, and further confirms the utility of our distribution-coloring analysis of checksums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adding Cell-Coloring to our Model</head><p>We are now ready to return to the discrepancies between our "exclude identical" probabilities in Table VI of Section IV-F and the actual measured failure rate. Recall that our sample probabilities predicted total failure rates very accurately for small (the number of cells in a block), but by the time increased to 4, the model over-predicted the measured failures by a factor of 3 or 4.</p><p>The piece that was missing from our model was the cellcoloring. The sample probabilities in our model were computed using only pure data cells, and thus missed the header effect. In our actual splice simulation, some substitutions of cells replace a data cell with a header cell. The failure rate for the substitutions with headers should be 1 in 2 , which is ignorable.</p><p>What is the probability that a substitution of length replaced a data cell with a header cell? This is easy to compute. All cells dropped from the first packet will be data cells. There are possible choices of cell insertions from the second packet (recall that we must insert the trailing cell of the second packet in the splice). Of these, only do not contain the header cell of the second packet. Therefore, to predict the actual failure rate of a -cell substitution from our "exclude identical" samples, we must reduce the sample probability by a factor of which equals</p><p>. Our sample probabilities now closely match the actual measured failure probabilities, and we are reasonably confident that we have explained the behavior we have observed. Further, the improved performance due to trailer checksums in our packet-splice model seems to be real.</p><p>In the past, protocol designers have proposed trailer checksums for various engineering reasons. As far as we know, the argument about improved checksum behavior was not advanced. We conclude that protocol designers should reconsider placing checksums in packet trailers rather than headers, as has been standard practice in Internet protocols to date.</p><p>Trailer checksums suffer one apparent drawback. They may unnecessarily reject splices that are identical to an original packet. Consider the scenario where a burst of cell loss splices the front of one packet onto the tail of the following packet, as in Fig. <ref type="figure" target="#fig_3">4</ref>. If the payload of the splice is identical to the payload of the original packet, then the header checksum should match (since the header of the splice is the header of the first packet), and the packet is accepted. But with trailer checksums, (as in Fig. <ref type="figure" target="#fig_4">5</ref>, when the payload is identical to the first packet the checksum cannot match: it was computed with the sequence number of the second packet, not the first. So if the contents are identical the checksums will match only if the difference between the inserted and dropped cells is congruent to the difference in sequence number (the payload) between the two packets. By Lemma 9, this is very unlikely. Thus the splice will be rejected even when the contents is correct. The corresponding case (header of the second packet, payload of the first) never comes up, since our error model requires cells to remain ordered. In summary, trailer checksums have a very good chance of detecting a splice even if the resulting packet is a "good" packet.</p><p>Table <ref type="table" target="#tab_8">X</ref> demonstrates this effect on the filesystem /u1 at smeg.dsg.stanford.edu. The number of identical splices rejected by trailer checksums is larger than the number of bad splices they detect that the TCP checksum missed.</p><p>The two numbers, however, are not comparable. TCP missed checksums represent undetected data corruption. Spurious rejection by the trailer checksum represents (at worst) a possible performance penalty, it does not cause any data corruption. Comparing missed splices, the trailer checksum misses less than 3% as often as the standard sum, but at the cost of reporting checksum failure on splices that accidentally resulted in a valid packet.</p><p>However, a splice in a real network always means that at least one packet has been lost, even if the splice is identical to one of the original packets. So a TCP retransmission will be necessary regardless. Thus the incremental performance impact of triggering retransmission one packet earlier when an identical splice is discarded is not clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Locality of Failure: Pathological Data Patterns</head><p>as the -255 sum. Pathological data patterns for mod-256 do occur, but less frequently. One case we have isolated is hexencoded PostScript bitmaps which contain identical segments of horizontal lines (e.g., bitmaps containing solid blocks of color, or bitmaps containing parallel lines. Font definitions appear to be a particularly common case). Many common bitmaps appear to have a width, , that is a power of two. Thus, each ASCII-encoded binary line commonly consists of many "FF's," and a small number of other two byte values (e.g., "F7") that repeat precisely apart (The extra byte is due to an ASCII newline.) Though not immediately obvious on inspection, these just happen to combine in such a way that the contribution of 48-byte cells allows splices. We observed a similar effect in BinHex-encoded Macintosh documents stored on our Unix filesystem: very similar lines of 64 bytes followed by an ASCII newline.</p><p>Though the overall rate of TCP sum failures is higher than the other sums, and appears to be noisier, we have also isolated a few pathological cases for the standard Internet checksum. One example is Unix gmon.out profiling data. These files often consist mostly of zero entries, with a scattering of a small number of nonzero entries. The nonzero values are often identical. Packetizing this data results in a very small number of checksums. A very large number of splices pass the checksum, resulting in what appears to be scrambled files. A second example is the PostScript bitmap data file mentioned above, which showed pathological behavior for the Internet checksum as well.</p><p>Our central point is that the existence of pathological patterns for a given sum is not just theoretical; these patterns occur surprisingly frequently in real filesystem data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONJECTURES</head><p>In the course of our research, we investigated several plausible conjectures that might have explained the TCP checksum failures. We briefly describe several of these blindalleys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Role of Zero Data</head><p>The frequency of the zero checksum led us to study the effects of zeroed data on the checksum. It is no surprise that there are a lot of zeros in filesystem data (the UNIX filesystem has long been optimized such that completely zero blocks did not need to be saved on disk). However, knowing that arbitrarily long zero blocks do not change the IP checksum (zero is the additive identity), we wondered whether this property significantly affected the failure rate independent of the simple fact of their high frequency. In other words: Is there something special about zero? If we replaced all the zeros in the filesystem with different values, would the failure rate change?</p><p>An approximate first answer is that no, zero is not special because it is the additive identity. If we add one to every word in the file system then the sum of every cell would increase by 24 (48 bytes divided by 2). Similarly, it is easy to demonstrate that the distribution of the sum of any number of cells will contain the same set of values and frequencies, although their mapping will be permuted. So the rate of checksum failure would be unchanged. 7  It is, however, true that if any single value shows up a disproportionate amount of the time then the failure rate will increase. However, the reason that zero in particular is so common is that several totally independent formats all happen to choose zero as a common element. Further, it is likely that this will continue to be the case. Fortunately, although zero checksums do show up very frequently, it is often the result of cells consisting entirely of zeros. A substitution of one allzero cell for another causes no harm. The problem, therefore, is the frequency of nonzero cells whose checksum is zero, in proximity to all-zero cells or to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Zero Congruent IP/TCP Header Cells</head><p>The TCP checksum is computed over a pseudo-header that covers all but eight bytes from the IP header. The TCP checksum is then inverted before it is stored in the header. Inverting the checksum causes the computed checksum of an error-free TCP datagram, (including the TCP header and checksum), to be zero.</p><p>A full IP header also contains an inverted ones-complement checksum, which means that the sum of the IP header is also zero. Since all but eight bytes from the IP header are also covered by the TCP checksum, the checksum over the combined TCP/IP header is not zero, but rather the checksum of the overlap: containing the IP source and destination addresses, the length, and the TCP protocol ID.</p><p>Our earlier results <ref type="bibr" target="#b6">[7]</ref> were based on simulations that left the eight nonoverlapping bytes, including the IP header checksum, set to zero. The result is to cause the combined TCP/IP header to checksum to zero. Consider, therefore, two packets consisting of data that is all zero. The TCP/IP header will have a checksum of zero. The data is zero, so the checksum of the first cell will only be the sum of the header. When the checksum is inverted and stored into the header, we are left with a nonzero cell with a checksum of zero. In our earlier work, these cells were a major source of nonzero cells with a checksum of zero. What is worse, these cell show up precisely in the case when all the cells around it are zero cells (or at least zero-congruent). Thus replacement was common and a major source of splice failures. Filling in the IP header reduced the error rate by three orders of magnitude.</p><p>We had conjectured that filling in the IP header would not have much of an effect, because the length, IP addresses, and protocol type do not change between packets during the file transfer, and so the checksums of the header cell remain constant. However, even a constant, nonzero, value is sufficient to distinguish between header cells and zero filled data cells. This simulator deficiency also led us to give undue emphasis to the role of zero-congruent data (as mentioned above). 7 Zero is special, as we showed in the section on pathological cases, but not because it is the additive identity and does not affect the checksum. Zero's specialness comes from the fact that it is represented by both 0 2 0000 and 0 2 FFFF. In reality, adding 1 to every word in the file system would change the distribution of checksums, and might reduce the probability of the most probable value. Cells containing 0 2 FFFF's would be shifted by less than 24. Whether this would increase or decrease the most probable value depends on the distribution of values in each filesystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inverted Checksums</head><p>Under the TCP and IP specification, the inverse of the checksum is placed in the packet header. This implies that the checksum of a valid segment will be zero. In <ref type="bibr" target="#b6">[7]</ref> we cautioned implementors against this approach, since for mostly-zero packets the header cell, too, would be zero. This still is reasonable advice for packet formats as it reduces the frequency of zero congruent cells. However, it is not relevant to TCP and IP because of the overlap of the headers we noted above. To test this conjecture, we ran our tests with a modified version of the TCP checksum that did not invert the checksum before storing it into the packet. The results with the noninverted checksum were almost identical to the results with an inverted checksum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Corrections to SIGCOMM'95 Version</head><p>As noted in Section VI-B the data in our earlier paper <ref type="bibr" target="#b6">[7]</ref> is not accurate. Completely filling in the IP header reduces the overall rate of errors by a factor of from 200 to 1000. In addition, the Fletcher checksum code was mis-implemented as a mixture of -255 and -256 arithmetic, which led to the Fletcher splice failure rate being higher than the standard TCP checksum. We retract that result; it was an artifact of the buggy Fletcher implementation. That bug was also the motivation for our current investigation of both -255 and -256 Fletcher sums. The artificially high Fletcher failure rates also inspired the original work on trailer checksums.</p><p>The previous results also suffered from a number of other minor bugs, whose effect was insignificant compared to the two problems above. They are detailed in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. OBSERVATIONS AND RECOMMENDATIONS</head><p>The results of the previous sections lead to a number of interesting observations. First, a nonuniform distribution of data makes failure of the TCP checksum far more likely than one would naively expect. The undetected splice rate in our data for the 16-bit TCP checksum over real data is comparable to uniform data with a 10-bit checksum.</p><p>Second, checksum distributions on modest amounts of real data are substantially different from the distributions one would anticipate for uniformly distributed data. This skewed distribution does result in significantly higher failure rates of the TCP checksum. In particular, if a router or host has a buffering problem that causes adjacent packets to be merged, the TCP checksum might fail 0.1% of the time rather than the 0.0015% of the time that purely random data distribution would suggest.</p><p>While these scenarios may seem worrisome, there are three pieces of good news.</p><p>First, it is important to keep in mind that these error scenarios are all quite rare. This work was initially motivated by studying extremely uncommon AAL5 error scenarios-an error model derived from ATM cell drop splicing two packets into one. In practice, such cell loss can occur due to either congestion or corruption. However, dropping ATM cells independently of each other is now known to cause goodput problems <ref type="bibr" target="#b9">[10]</ref>. ATM switch vendors are addressing this prob-lem by employing Early Packet Discard, which discards all cells in a packet and eliminates the chance of a splice. Cell loss due to corruption is often estimated at 1 in 10 or less. The ATM CRC will fail to detect a splice approximately at a rate of 1 in 2 . Therefore, the chance of the TCP checksum being called upon to detect a splice is much less than 1 in 10 2 or less than one chance in 10 . Second, the packet splice model is, in some sense, a worstcase error model because the substitutions tend to be similar to the data that they replace. This is possibly also true of buffer-management errors, or errors in fragment reassembly. However, in the alternative error models where data is replaced by garbage, while the nonuniformity of the data may still reduce the effectiveness of checksums, it will only reduce it to the extent that the distribution of the replacement data matches the distribution of the original data. Here, the frequency of long runs of 0's or 1's in the payload may make us slightly more vulnerable to hardware errors that produce similar runs of data. However, hardware failure that produces random bits are unlikely to produce runs of data that look a lot like English prose.</p><p>Third, and finally, remedies exist to improve the ability of checksums to work on nonuniform data.</p><p>• Compressing data clearly improves the performance of checksums. Since compression also typically reduces file transfer times and saves disk space, there's a strong motivation for FTP archives to compress their files. • In the future, in the absence of compression, protocol designers should consider avoiding the practice of placing checksums in a protocol header, but instead append them as a trailer to the data being checksummed. • In general, the checksums are rarely placed in a situation where it is the primary method of failure detection. (We are aware of one exception to this rule. The TCP checksum is the primary method of error detection over SLIP and compressed SLIP links. That's probably not wise.) What this work simply shows is that checksums are an even less effective error detection method than first thought, because real data often has interesting distributions, and those distributions increase the likelihood of checksum failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>This paper contains assertions which depend upon statements that are easily proven, yet not immediately obvious. For those interested in the formal justification of some of the statements, we present more detail in this appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Distributions of Checksums</head><p>We use the notation to denote the distribution which arises by applying any commutative, total function with a unique inverse on a pair of values drawn from distributions and respectively. (In all of our cases, we are interested in the usual arithmetic addition operator.) Call PMax the probability of the most likely value in the discrete distribution, . (We define PMin similarly.) And define is the probability of selecting from .</p><p>Lemma 1: .</p><p>Proof: For any given , the probability that the value drawn from is given by . Assume is the most probable element of . Without loss of generality, assume that PMax . (since ). Equality would only hold if were uniformly distributed and if . Lemma 2: If , then . Proof: Consider the previous proof. Given the nonzero condition on , we are guaranteed that every value in appears, and so , thus Pmin Pmin This is unremarkable for unbounded discrete distributions. For the maximum, as the number of possible values grows, the probability of any single value must decrease. The conditions on the min require that , and that , so it is also unsurprising that the minimum doesn't decrease. However, for bounded distributions, e.g., distributions over the integers , this leads to the following more interesting results.</p><p>Corollary 3: Consider a probability distribution over the integers . The distribution of the sum, , of integers drawn from gets "more uniform" as increases, in the sense that the minimum probability of any number gets larger and the max probability gets smaller.</p><p>Computation: If we have a random variable which can take on values, with a known distribution of values, then the probability (</p><p>) of the sum of values drawn from this distribution is equal to is (1) Corollary 3 shows that each time we add another number to the sum mod and look at the probability distribution, we increase and decrease . We can prove another useful result: for large enough , and both approach and the distribution approaches uniform.</p><p>If has some zero probability values, then some values in the sum of might also have zero probability, unless the gcd of and the entries occurring with nonzero probability is 1. The following theorem applies even if a sum of a distribution only has values with nonzero probability in the following sense: all nonzero values will tend to be equal to . Theorem 4 (Central Limit Theorem): The sum, , of a large number of independent observations from any distribution tends to have a uniform distribution. Proof: We will show that for any given , there is some such that PMax . Since is nonincreasing as grows, we know this also holds for all . Use the notation to mean , and to mean , when the meaning is clear. Assume there is a distribution, , where for all values of . We can compute a strict upper bound for based on . The largest possible value of simultaneous equations. We must show that these two specific bytes are independent, since we can no longer magically choose offsets 0 and 1. Assume the Fletcher checksum , is stored in adjacent bytes with offsets and from the end of the packet.</p><p>, and :</p><p>Since is uniformly distributed , so are both and . Since , then is still uniformly distributed even if we hold fixed (since we can vary internal to ). Therefore, is independent of . Note that will not, in general, be uniformly distributed mod , since we can't assume that (in fact, in our example, was always equal to 260. and</p><p>). As a curiosity, further note that if were not relatively prime to , then and would not have been independent or uniformly distributed. (In fact, the equations would not have always had solutions).</p><p>Corollary 8: Given uniformly distributed data, and the substitution model described above, IP and Fletcher checksums are equivalently powerful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Header Checksums Versus Trailer Checksums</head><p>The body of the paper claims that under our splice error model, trailer checksums are stronger than header checksums for nonuniformly distributed data and, no worse for uniformly distributed data. Here we prove that claim.</p><p>Lemma 9: Consider drawing 2 samples, and , from any discrete distribution. The probability that is greater than or equal to the probability that mod for any given . Proof: To see this, note that the probability of the former (identical match) is simply . The probability of the latter ( greater than the first) is , where is taken mod . Double both sums and rearrange terms. Since , the former sum is greater than the latter sum.</p><p>Consider our error model: we substitute cells from the first packet with other cells from the second packet. We keep the header cell of the first packet and we keep the trailer cell of the second packet. For a header checksum to fail, the sum and of each collection of cell partial checksums must be equal. For a trailer checksum to fail, the sum of the cells missing from the first packet must be less than the , assuming that the checksum of the header cell of packet 1 is less than the checksum of the header cell of packet 2. We distinguish , the difference between the header cells, since the header cells are drawn from a very different distribution than the data cells, and further, the distribution of the difference of two consecutive header cells is strongly clustered around . Thus, we have Theorem 10. Theorem 10: Under our error model of splicing, a trailer checksum will always be at least as powerful as a header checksum.</p><p>Proof: For any given splice we have substituted cells. Equation <ref type="bibr" target="#b0">(1)</ref> gives us the probability distribution of the sum of cells. The probability that the header checksum fails is the probability that two samples drawn from are equal. As discussed above, for trailer checksums there is a fixed , usually 256 in our simulation, computable by looking at the 2 header cells. The probability that the trailer fails is the probability that two samples from differ by . Lemma 9 shows that the former is more likely than the latter, thus header checksums are weaker than trailer checksums.</p><p>Note, that in fact, this depends only on the property that the probability of the checksums over the header cells of two adjacent packets be congruent is lower than the probability that two data cells from the same packet be congruent. For computing the actual probability of trailer checksum failure it is useful to be able to model as a constant 256, but this is not required for the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RETRACTIONS FROM THE SIGCOMM'95 PAPER</head><p>An earlier version of this paper appeared in SIGCOMM'95 <ref type="bibr" target="#b6">[7]</ref>. The central point of that paper still holds: nonuniform distribution of data results in the IP checksum being weaker than expected. Several conjectures expressed in <ref type="bibr" target="#b6">[7]</ref> have been resolved and were addressed in the main body of this paper.</p><p>However, several minor points and computational details were not correct and we retract them.</p><p>First, we expressed surprise (as well we should have) that the Fletcher checksum performed worse than the IP checksum. Performance tuning of the Fletcher checksum code used in that paper resulted in an incorrect implementation. The Fletcher code also used a mixture of 256 and 255 arithmetic and was not computing an accurate Fletcher checksum for either 255 or 256 Fletcher. The numbers reported for the Fletcher checksum in that paper were, therefore, not accurate. The corrected numbers reported in this version of the paper show the expected result-Fletcher's detects more splices than TCP. However, the bugs in <ref type="bibr" target="#b6">[7]</ref> and its anomalously poor results motivated us to investigate both 255 and 256 Fletcher, uncovering the pathological cases for 255 Fletcher reported here. The SIGCOMM'95 paper reports numbers where the IP header fields not covered by the TCP checksum were left as zero.</p><p>Though covered in the body of this paper, it is important to emphasize it again here: filling in the header significantly reduced the number of matches for zero-congruent cells, and therefore reduced the total number of misses (by three orders of magnitude in some cases). By zero-filling in the IP header in <ref type="bibr" target="#b6">[7]</ref> we over-stated the significance of splices including zero-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example AAL5 splice.</figDesc><graphic coords="2,341.34,59.58,180.48,128.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of TCP checksum over blocks of k cells in smeg.dsg.stanford.edu:/u1 (a) full probability dist. function; (b) pdf: 65 most common values; and (c) CDF: 65 most common values.</figDesc><graphic coords="4,81.30,55.02,437.52,128.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. PDF of TCP checksum, F255, and F256 over 48 byte cells in smeg.dsg.stanford.edu:/ul. Most common 256 values.</figDesc><graphic coords="7,327.30,59.58,208.56,134.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Header checksum fate.</figDesc><graphic coords="8,78.12,59.58,180.96,128.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Trailer checksum fate.</figDesc><graphic coords="8,340.62,59.58,181.92,128.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,56.16,88.50,224.88,502.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,328.92,79.50,205.44,463.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,322.32,89.82,218.64,236.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CRC</head><label>I</label><figDesc>AND TCP CHECKSUM RESULTS (256 BYTE PACKETS ON SYSTEMS AT STORTEK)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CRC</head><label>II</label><figDesc>AND TCP CHECKSUM RESULTS (256 BYTE PACKETS ON SYSTEMS AT SICS)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CRC</head><label>III</label><figDesc>AND TCP CHECKSUM RESULTS (256 BYTE PACKETS ON SYSTEMS AT STANFORD)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PROBABILITY</head><label>IV</label><figDesc></figDesc><table /><note><p>(AS %) OF CHECKSUM MATCH FOR SUBSTITUTIONS OF LENGTH k CELLS</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V PROBABILITY</head><label>V</label><figDesc>(AS %) OF CHECKSUM MATCH FOR SUBSTITUTIONS OF LENGTH k CELLS BASED ON LOCAL DATA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI CHECKSUM</head><label>VI</label><figDesc>FAILURES ON REAL DATA PROBABILITY (AS %) OF CHECKSUM CONGRUENCE FOR BLOCKS OF LENGTH k CELLS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII CRC</head><label>VII</label><figDesc></figDesc><table /><note><p>AND TCP CHECKSUM RESULTS, COMPRESSED DATA (256 BYTE PACKETS ON SYSTEMS AT SICS)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII FLETCHER</head><label>VIII</label><figDesc>'S CHECKSUM RESULTS (256 BYTE PACKETS ON SYSTEMS)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE X HEADER</head><label>X</label><figDesc>VERSUS TRAILER CHECKSUM FAILURE RATES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The actual results are stronger: if just one word in the packet is uniformly distributed over all 2 16 possible values, then the checksum of the entire packet is uniformly distributed over all possible values.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Unlike<ref type="bibr" target="#b12">[13]</ref>, our Fletcher's results perform a sum-to-zero inversion on the transmitted checksum. See Section VI-C.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>This calculation includes all cells, including the short cell at the end of</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to acknowledge the help of C. Kalmanek and B. Marshall of Bell Labs, who discussed issues of study design. They also gratefully acknowledge the help of D. Feldmeier of Bellcore and L. Sloan of Lawrence Livermore, who helped with substantially faster CRC computation algorithms, to the Swedish Institute of Computer Science, which allowed them to use its filesystems and one of its fast multiprocessors for some of the test runs, and to the engineers of StorTek for enduring long running background programs on their machines on several occasions. They would also like to thank J. Crowcroft, TON editor, and his reviewers, who pushed them to look even more deeply at some checksum behavior. The result was a multiyear delay in publication as they collected more data but also a much deeper understanding of checksums, which they hope this paper conveys.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>by IEEE/ACM TRANSACTIONS ON NETWORKING Editor J. Crowcroft. This work was supported in part by the Defense Advanced Research Projects Agency under Army Contract DABT63-91-K-0001 and in part by the U.S. Department of Defense. J. Stone and M. Greenwald are with Stanford</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>will arise when the most probable terms from match the most probable terms from (cf. exercise in concrete mathematics <ref type="bibr" target="#b2">[3]</ref>, at the bottom of page 38). Assume the probability for the most common values in are all , and there is 1 value whose probability is . For , there is at least one value with probability , one with probability , and values whose probability sums to .</p><p>but after adding times, would be less than 0, given our assumption that is always greater than . So, our assumption must be false. Thus, for any distribution and for any , there is some number of additions, such that , so the distribution of tends to the uniform distribution as gets larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Distributions of Some Checksums over Uniformly Distributed Data</head><p>Most existing evaluations of competing checksum algorithms have assumed that single bit errors were common. It is now frequently true that there are in the data-link layer to protect the integrity of cells on the wire, and ECC to correct memory while packets sit in buffers on routers. Thus, the errors that the TCP checksum must protect against are no longer single or double bit errors (which will be detected or corrected by other means), but rather substitution of longer runs of "good" data by (possibly different length) runs of "other" data. How do the IP checksum and Fletcher compare under this substitution model?</p><p>This section discusses what their expected behavior would be under substitution errors if the data were, in fact, uniformly distributed. 8  If we assume all packets are equally likely, then if we look at any unit smaller than the size of the substitution, we can assume that an error consists of replacements drawn uniformly from all strings.</p><p>Lemma 5: The sum of numbers, will be uniformly distributed among all values assuming there is at least one term, , in the sum which takes on values uniformly distributed Proof: Assume has an arbitrarily skewed distribution.</p><p>, and . By Lemmas 1 and 2, . 8 It is worth noting that one point of the preceding paper is that data values are not distributed uniformly and are correlated with nearby values, and that, therefore, errors, under the substitution model, are also not distributed uniformly and checksums do not perform as well as expected. This work on uniformly distributed data is still interesting on three counts. First, statements in the main body of the paper depend on results presented here. Second, it provides us with a benchmark against which to measure the actual measured error rate (i.e., what is due to the substitution model and what is do to nonuniform data). Third, encryption and compression are both becoming more common and both tend to produce uniformly distributed data.</p><p>Thus, the probability that for any given will be precisely 1/ , so the probabilities are all equal and the distribution is uniform.</p><p>Theorem 6: Given uniformly distributed data and the substitution model above, the IP checksum of the modified packet is uniformly distributed over all possible values.</p><p>Proof: We assume that errors are replacements drawn from the uniform distribution. Then (assuming replacements larger than a single 16-bit word) every word within the replaced chunk will be uniformly distributed . Therefore, by Lemma 5, the IP checksum will be uniformly distributed under the assumed substitutions, since it is the sum of uniformly distributed words. That is, the checksum will only fail to detect errors (by the replacement string contributing an identical sum to the checksum as the original string) with a probability of 1 out of 2 1. Theorem 7: Given uniformly distributed data and the substitution model above, the Fletcher checksum of the modified packet is uniformly distributed over all possible values.</p><p>Proof: The same reasoning can be applied to the Fletcher checksum over a chunk of data of size . The Fletcher checksum consists of two sums. The first is the sum, mod , of all the bytes in the chunk. The second is the sum mod of each byte weighted by its offset, , from the end of the chunk. Call these two sums, respectively, and . The contribution of this chunk (assuming it is from the end of the packet) to the Fletcher checksum of the entire packet is straightforward.</p><p>is added, mod , to the mod sum of the rest of the packet.</p><p>is added, mod , to the weighted sum of the rest of the packet. If for each chunk is uniformly distributed, then so will . If each is uniformly distributed, then so will , since by Lemma 5, we only need one uniformly distributed term (and is, although might not be). That is uniformly distributed follows directly from the lemma.</p><p>is only slightly more complicated. As long as the chunks are large enough so that the there is a byte with offset from the end of the chunk, such that is relatively prime to (i.e., , then 's contribution to is uniformly distributed among all values, and therefore, itself is also uniformly distributed. Since is relatively prime to , as long as the chunk is at least bits long, we can apply Lemma 5.</p><p>We must also show that is independent of , else will not be uniformly distributed. Suppose the last 2 bytes of the chunk are and . Under the assumption of uniform distribution of the data, and are both independent and uniformly distributed.</p><p>does not affect since it is multiplied by . As we show the uniform distribution of by varying (as we did in the lemma above), for each we can choose any value for to allow to take on all values equally, without affecting . So, for each value {K that might take on, is independent and uniformly distributed. One last complication arises with the Fletcher checksum. Like IP, Fletcher defines the values inserted into the checksum field to be the negation of the checksum of the rest of the packet, so that the packet sums to . With Fletcher this requires the two bytes of the checksum to be the solution to a system of congruent cells and focused too closely on misses involving zero-filled or zero-congruent cells.</p><p>Several additional, but relatively minor bugs in the simulator compromised the accuracy of the numbers of all checksum algorithms in <ref type="bibr" target="#b6">[7]</ref> (but only to a small factor).</p><p>First, we used the AAL5 length from the second packet, rather than the apparent IP length from the first cell, for checksum computation. This miscomputed checksums by including data from the last cell beyond the end of the IP payload in the checksum.</p><p>Second, this same error arose when testing whether packets were "identical" in payload. This resulted in counting certain splices as checksum failures, when in fact they were simply identical to the original packet, or where the first packet was a prefix of the splice.</p><p>Third, we miscomputed the checksum for short packets-that is, packets where the apparent IP header length made the entire TCP packet fit into the first cell and the AAL5 trailer in the second cell. It is well known that a TCP packet with any user data fills at least two ATM cells. But for packets with 1 to 8 bytes of TCP payload, the entire IP/TCP datagram fits in only one cell and the second cell contains only an AAL5 trailer. Knowing that TCP data packets always take two cells, the simulation in <ref type="bibr" target="#b6">[7]</ref> erroneously added a partial checksum for the second cell.</p><p>These erroneous calculations did not change the larger picture of TCP checksum performance, but did require us to recompute all data for this version of the paper.</p><p>Finally, our code and raw data are available via email request to the authors.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Internet Request For Comments RFC 1071</title>
		<author>
			<persName><forename type="first">R</forename><surname>Braden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988-09">Sept. 1988</date>
		</imprint>
	</monogr>
	<note>Computing the Internet checksum. Updated by RFC&apos;s 1141 and 1624</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An arithmetic checksum for serial transmissions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="247" to="252" />
			<date type="published" when="1983-01">Jan. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Knuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<title level="m">Concrete Mathematics: A Foundation for Computer Science</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reliability of adaptation layers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lyles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Protocols for High-Speed Networks III, Proc. IFIP 6.1/6.4 Workshop</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Pehrson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Gunningberg</surname></persName>
		</editor>
		<editor>
			<persName><surname>Pink</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Development of a transmission error model and an error control model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hammond</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep., Georgia Inst. of Technol</title>
		<imprint>
			<date type="published" when="1975-05">May 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fletcher&apos;s error detection algorithm: How to implement it efficiently and how to avoid the most common pitfalls</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nakassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Commun. Rev</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="63" to="88" />
			<date type="published" when="1988-10">Oct. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Performance of checksums and CRC&apos;s over real data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCOMM&apos;95</title>
		<meeting>SIGCOMM&apos;95<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="68" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TCP checksum function design</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Plummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Engineering Note</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
	<note>BBN. reprinted in [1])</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transmission control protocol</title>
		<author>
			<persName><forename type="first">J</forename><surname>Postel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Internet Request for Comments RFC</title>
		<imprint>
			<biblScope unit="volume">793</biblScope>
			<date type="published" when="1981-09">Sept. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamics of TCP traffic over ATM networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romanow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Floyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="633" to="641" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving the efficiency of the OSI checksum calculation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sklower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Commun. Rev</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="1989-10">Oct. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SEAL detects cell misordering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network Mag</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Jonathan Stone received the M.Sc. degree with distinction from Victoria University of Wellington</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990-02">Feb. 1990</date>
			<pubPlace>New Zealand; Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Internet Request For Comments RFC 1143. and is currently working toward the Ph.D. degree at</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">M&apos;98) received the S.B. degree from the Massachusetts Institute of Technology, Cambridge, MA, and is currently working toward the Ph</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Greenwald</surname></persName>
		</author>
		<imprint>
			<pubPlace>Stanford, CA; Philadelphia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>D. degree at Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>He will be an Assistant Professor at the University of Pennsylvania</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">He is a Chief Scientist with BBN Technologies, Cambridge, MA, a part of GTE Corporation</title>
		<author>
			<persName><forename type="first">Craig</forename><surname>Partridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">M&apos;88-SM&apos;91) received the A.B., S.M, and Ph.D. degrees from Harvard University</title>
		<meeting><address><addrLine>Cambridge, MA; Stanford, CA</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Consulting Assistant Professor at Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">M&apos;90) is a Fellow at Storage Technology Corporation specializing in High Performance Channels and Networks and Information Security Issues</title>
		<author>
			<persName><forename type="first">James</forename><surname>Hughes</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
