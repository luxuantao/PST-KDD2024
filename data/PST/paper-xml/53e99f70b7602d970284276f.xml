<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding the Emergence of Conventions in Multi-Agent Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Adam</forename><surname>Walker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Manchester Metropolitan University Chester Street</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><surname>Wooldridge</surname></persName>
							<email>m.wooldridge@doc.mmu.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Manchester Metropolitan University Chester Street</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding the Emergence of Conventions in Multi-Agent Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BA7E95F70147342B091B823333C75ECC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>organization self-design, cooperation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we investigate techniques via which a group of autonomous agents can reach a global agreement on the use of social conventions by using only locally available information. Such conventions play a central role in naturally-occurring social systems, and there are good reasons for supposing that they will play a similarly important role in artificial social systems. Following a short review of conventions and their use in distributed artificial intelligence, we present a formal model that rigorously defines both our experimental methodology, and the performance measures we use to quantify the success of our experiments. We then describe sixteen different mechanisms for bringing about agreement on conventions, and present experimental results obtained for each of these methods. A tentative analysis of these results is given, and the paper concludes with some comments and issues for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work in Distributed Artificial Intelligence (DAI) has investigated the possibility of using norms, conventions, and social laws in multi-agent systems. Examples of the issues investigated include the control of aggression <ref type="bibr" target="#b0">[1]</ref>, how conventions might emerge within agent societies <ref type="bibr" target="#b6">[7]</ref>, the role of social structure in the emergence of conventions <ref type="bibr" target="#b4">[5]</ref>, group behaviour <ref type="bibr" target="#b1">[2]</ref>, and the reconsideration of commitments <ref type="bibr" target="#b3">[4]</ref>. In addition, researchers working in philosophy, sociology, and economics have considered similar issues. A good example is the work of Lewis <ref type="bibr" target="#b5">[6]</ref>, who has made some progress towards a (non-formal) theory of normative behaviour.</p><p>Conventions play a key role in the social process. They provide agents with a template upon which to structure their action repertoire. They represent a behavioural constraint, striking a balance between individual freedom on the one hand, and the goal of the agent society on the other. As such, they also simplify an agent's decision making process, by dictating courses of action to be followed in certain situations. One key issue in the understanding of conventions is to decide on the most effective method by which they can come to exist within an agent society. There are two main approaches:</p><p>Off-line design: In this approach, social laws are designed off-line, and hardwired into agents. Examples in the DAI literature include <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Emergence from within the system: This possibility is investigated by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref>, who experiment with a number of techniques by which a convention can 'emerge' from within a group of agents.</p><p>The first approach will often be simpler to implement, and might present the system designer with a greater degree of control over system functionality. However, there are a number of disadvantages with this approach. First, it is not always the case that all the characteristics of a system are known at design time; this is most obviously true of open systems. In such systems, the ability of agents to organise themselves would be advantageous. Secondly, in complex systems, the goals of agents (or groups of agents) might be constantly changing. To keep reprogramming agents in such circumstances would be costly and inefficient. Finally, the more complex a system becomes, the less likely it is that system designers will be able to design effective social laws. Here, flexibility from within the agent society might result in greater coherence.</p><p>This paper is thus concerned with the latter approach -that of emergence from within the society. Specifically, we investigate the efficiency of various mechanisms via which a group of autonomous agents can come to reach a global agreement on the use of social conventions by using only locally available information. Thus each agent must decide on which convention to adopt based solely on its own experiences, as recorded in its internal state; pre-defined inter-agent power structures or authority relationships are not allowed. The key problem is to design a strategy update function, representing an agent's decision-making process, that, when used by every agent in the society, will bring the society to a global agreement as efficiently as possible. We begin, in the following section, by presenting a formal model that rigorously defines the problem domain, our methodology, and the performance measures we use to assess the effectiveness of strategy update functions. In section 3, we describe our experimental environment, and present sixteen different strategy update functions, whose efficiency we have evaluated by experiment. Our results are presented in section 4, and some conclusions appear in section 5. Note that this work builds on that of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Formal Model</head><p>We begin by assuming a set Ag = f1,… , lg of agents. At any time, an agent is assumed to have chosen one of a set Σ = fσ 1 , … , σ m g of strategies. These strategies represent the possible norms or conventions that an agent may fix upon. Exactly what these strategies are is not significant, at least for the purposes of this formal model. Initially, each agent is assigned an element of Σ at random.</p><p>An interaction occurs when two agents compare strategies. Formally, we define the set I, of all interactions, by I = Σ×Σ. We always consider an interaction ι ∈ I from the point of view of some particular agent; we write selƒ(ι) for the strategy chosen by this agent in the interaction ι, and other(ι) for the strategy chosen by the other participant in ι.</p><p>An agent's memory consists of a finite sequence of interactions, representing those that the agent has been involved in. Formally, the set M of all memory states is defined by Note that in some domains, (such as the one we describe in section 3), there may be times at which an agent is not involved in an interaction, but is performing some other kind of action. However, these non-interactive 'moves' play no part in the strategy selection process, and may therefore be ignored in the formal model.</p><formula xml:id="formula_0">M = I * . If m ∈ M, then |m| denotes the length of m; if n ∈ f0,… , |m| -</formula><p>Our aim in this paper is to investigate the properties of strategy update functions. A strategy update function u has the signature u : M → Σ, i.e., it takes a memory state, and, on the basis of the experiences recorded in this state, selects a strategy. Let U be the set of all strategy update functions.</p><p>A run, r, is a function r : Ag×IN → I, that must satisfy the following invariant property: ∀n ∈ IN, there exists an irreflexive bijection</p><formula xml:id="formula_1">ƒ n : Ag → Ag, such that ∀i ∈ Ag, if selƒ(r(i, n)) = σ and other(r(i, n)) = σ ′, then selƒ(r(ƒ n (i), n)) = σ ′</formula><p>and other(r(ƒ n (i), n)) = σ . A run r represents one possible history of a system; for each agent i ∈ Ag, and for each time point n ∈ IN, it assigns i an interaction r(i, n) representing that which i was involved in at time n. The invariant on runs ensures that interactions are symmetrical, i.e., that two agents are involved in every interaction. Let R be the set of all runs.</p><p>We let mem(i, r, n) denote the memory of agent i ∈ Ag in run r ∈ R up to time n ∈ IN. We can visualise mem(i, r, n) as the sequence (r(i, 0), r(i, 1), … , r(i, n)). Note that, for simplicity, we have here assumed that agents have unbounded memory. We say that a run r ∈ R is consistent with the use of strategy update function u ∈ U, (notation uses(r, u)), iff ∀i ∈ Ag, ∀n ∈ IN, if r(i, n + 1) = ι, then selƒ(ι) = u(mem(i, r, n)). We denote by R u that subset of R whose members are consistent with the use of u, i.e., R u = fr | r ∈ R and uses(r, u)g. The set R u represents the characteristic behaviour of the strategy update function u. With respect to R u , we can define a number of performance measures, via which the effectiveness of u can be measured.</p><p>Our aim when designing a strategy update function u ∈ U is that, by using it, the agents in a system will come to converge on a strategy. We begin by formalising the notion of convergence. We denote by chosen(σ , n, r) the set of agents that have chosen strategy The time it takes for a strategy update function to bring about convergence is not the only performance measure that we can use. Another important criterion is the average number of strategy changes per interaction. The intuition behind this measure is that, in real-world situations, changing from one strategy to another generally incurs a cost. Consider, for example, the financial and human resources required for an organisation to move from one computer operating system to another. Let sc(i, r, n) be the number of times agent i ∈ Ag changes strategy in run</p><formula xml:id="formula_2">σ ∈ Σ at time n ∈ IN in run r ∈ R, i.e., chosen(σ , n, r) = fi | i ∈ Ag and selƒ(r(i, n)) = σ g. We then define the conver- gence of a run r ∈ R at time n ∈ IN, (denoted conv(r, n)),</formula><formula xml:id="formula_3">r ∈ R up to time n ∈ IN: sc(i, r, n) = |fo | o ∈ IN, o &lt; n, and selƒ(r(i, o)) ≠ selƒ(r(i, o + 1))g|</formula><p>The average number of strategy changes per interaction in run r ∈ R up to time n ∈ IN is then given by changes(r, n):</p><formula xml:id="formula_4">changes(r, n) = P i∈Ag sc(i, r, n) n.|Ag|</formula><p>We then let H u n denote the average number of strategy changes per interaction up to time n ∈ IN over each run r ∈ R u , for strategy update function u ∈ U. The value H u n will range from 0.0 to 1.0; if H u n = 1.0, then every agent changes strategy after every interaction. Lower values of H u n are thus preferable. The value H u n will be our second measure of strategy update function performance.</p><p>Of particular interest to us are strategy update functions that guarantee stable convergence. A function u has this property iff ∀r ∈ R u , ∃n ∈ IN such that ∀o, p ∈ IN, and ∀i, j ∈ Ag, if o ≥ n and p ≥ n then selƒ(r(i, o)) = selƒ(r(j, p)); in other words, if u guarantees that the agents will come to agree on a strategy, and then remain fixed on it. Note that if u guarantees stable convergence, then</p><formula xml:id="formula_5">H u n → 0.0 as n → ∞.</formula><p>As a third, and final performance measure, we consider the maximum number of strategy changes that any agent incurs on any run r ∈ R u up to time n. We denote this value by M u n . Obviously, low values of M u n are preferable. This completes our formal model. We can now describe our experimental methodology. Our aim is to compare the effectiveness of a number of strategy update functions, using the measures C u n , H u n , and M u n . To do this, we take each strategy update function u, and generate, by computer simulation, a set of runs of u, that we hope are representative of R u . We then analyse these runs, to empirically determine the values C u n , H u n , and M u n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Experimental Environment</head><p>In this section, we describe both our experimental environment and the sixteen different strategy update functions that we investigate. Our experimental environment is based on that of <ref type="bibr" target="#b0">[1]</ref>. In this environment, agents move about a grid in search of food. They are endowed with a budget that is increased by eating an item of food, but decreased by movement around the grid. Agents therefore have the goal of eating as much food as possible in order to maximise their budget.</p><p>Agents can move only one square at a time in either a horizontal or vertical direction. Likewise, at any one time, they can 'see' only one square in each of these directions. If they see an item of food in a square, then they move to that square and eat the food. Otherwise, they move to a randomly selected empty square in search of food. Naturally, more than one agent may at any time make a bid for the same item of food. If this happens, then one of the agents is selected randomly. However, agents can also attack others who are in possession of a food item. This aggression costs both aggressor and victim a large slice of their budget, but may be rewarding if the aggressor wins the food item. The winner in any confrontation is always the agent with the highest budget. The purpose of the experiments in <ref type="bibr" target="#b0">[1]</ref> was to examine how aggression might be reduced by the introduction of conventions into the system. Two conventions were identified -one of ownership, where agents cannot attack those eating their own food, and another where agents cannot attack others who are on their righthand side. Although these experiments provide a valuable insight into normative behaviour, the conventions were nevertheless hard-wired into agent's architectures. Our experiments are concerned with how such conventions might come to exist within the agent society using observation and a strategy update function. We want to provide our agent society with a variety of possible strategies by which to reduce aggression, and a suitable update function, and measure how effectively the agents come to agree on a single strategy or convention.</p><p>To achieve this, we have chosen the latter strategy of <ref type="bibr" target="#b0">[1]</ref> -that of giving precedence to agents in a particular location -but have extended it to consist of four possible strategies: precedence to those on the right, to those above, to those below, and to those on the left. Agents begin an experiment with one of these strategies chosen at random, and as the experiment progresses, they observe the strategies adopted by other agents. Using an update function, they may modify their chosen strategy as they interact with other agents. Some other minor modifications were needed to the world of <ref type="bibr" target="#b0">[1]</ref>, which, whilst maintaining the functionality of the original system, extended it to allow more meaningful results to be obtained within the context of our research goals. First, 100 agents were used, rather than the 12 of <ref type="bibr" target="#b0">[1]</ref>. The agent to food ratio stays the same at 2:1, giving a total of 50 food items at the start of each experiment. We also use a larger world. The grid is a square of 225 cells -15×15. Finally, we use longer experiments. In <ref type="bibr" target="#b0">[1]</ref>, a match is over when all the food has been eaten, or all the agents have 'died'. (An agent dies when its budget falls below one.) Matches do not allow sufficient time to assess the processes of norm evolution, and so we extended each experiment to consist of 50 matches. At the end of each match the food supply is renewed at random locations, and all dead agents are replaced by new agents at the same location. Note that new agents do not retain the memories of their predecessors, and their normative strategy is assigned afresh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Strategy Update Functions</head><p>We have evaluated four different basic strategy update functions in order to measure their effectiveness in producing convergence, using the experimental framework described above. These basic strategy update functions are as follows<ref type="foot" target="#foot_1">2</ref> :</p><p>Simple majority: This is the simplest form of update function. Agents will change to an alternative strategy if so far they have observed more instances of it in other agents than their present strategy. If more than one strategy has been observed more than that currently adopted, the agent will choose the strategy observed most often.</p><p>Using the notation introduced in section 2, we may define this strategy update function as follows:</p><formula xml:id="formula_6">u(m) = σ iff ⁄ ∃σ ′ ∈ Σ such that obs(σ ′, m) &gt; obs(σ , m).</formula><p>Simple majority with memory restart: This function is essentially the same as simple majority, except that when an agent has changed strategy, he clears his memory. (Note that the signature of this strategy update function is thus</p><formula xml:id="formula_7">u : M → Σ × M.)</formula><p>Simple majority with communication by agent type: Agents are divided into two types. As well as observing each other's strategies, agents in these experiments can communicate with others whom they can 'see', and who are of the same type. When they communicate, they exchange memories, and each agent treats the other agent's memory as if it were his own, thus being able to take advantage of another agent's experiences. By restricting communication to agents of the same type, we apply the 'extroversion radius' of <ref type="bibr" target="#b6">[7]</ref>. In other words, agents are particular about whom they confide in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple majority with communication on success:</head><p>Here we use a utility function that employs a form of communication based on a success threshold. When an individual agent has reached a certain level of success with a particular strategy, he communicates his memory of experiences with this successful strategy to all other agents that he can 'see'. Note, only the memory relating to the successful strategy is broadcast, not the whole memory. The intuition behind this update function is that an agent will only communicate with another agent when it has something meaningful to say. This prevents 'noise' communication. In our experiments, the threshold chosen was a strategy where the total observed was greater than 20, but was also at least twice as much as any other strategy total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">External Majority Variations</head><p>The above strategy update functions, whilst differing in certain key aspects, all cause agents to change strategy on the basis of a simple majority. This is an intuitively appealing approach, and certainly performed significantly better than the other update functions tested by Shoham and Tennenholtz <ref type="bibr" target="#b6">[7]</ref>. However, we had certain reservations about how accurately this approach accorded with our own understanding of normative behaviour. Our concerns were:</p><p>• This methodology seems to be more akin to the phenomenon of imitation, rather than norm convergence. Imitation represents a subconscious copying of another's actions, rather than a reasoned choice. Imitation can lead to negative results -as in the case of riots. Thus, this function may not guarantee that the best strategy is adopted. It could be argued that in our experiments, no one strategy is preferable; the important point is that all agents adopt the same one. However, in many instances -e.g., in the world of standards -certain strategies may be preferable to others.</p><p>• The simple majority may lead to agents changing strategy frequently. As we pointed out in section 2, frequent strategy changes are, in general, inefficient. A better approach might be to change strategy only if it is observed in considerably more agents than the current one.</p><p>Given these concerns, we decided to experiment with variations of the simple majority update function, using majorities of greater than one. The intention was to observe to what degree higher majorities affected the value of C u n , but to balance this against any improvements in the values of H u n and M u n . In addition to the simple majority update function, the following three variations were tested: Double majority: Agents changed from strategy σ to strategy σ ′ if the number of observations for σ ′ was more than twice that for σ .</p><p>Quadruple majority: Agents changed from strategy σ to strategy σ ′ if the number of observations for σ ′ was more than four times that for σ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic majority:</head><p>In this variation, the size of the majority is a function of each agent's total number of strategy changes to date. If an agent has changed strategy only once, then he will do so again should he observe a simple majority of one against an alternative strategy. If he has changed strategy twice, then he will not change again until he observes a total of more than double for an alternative strategy -and so on. Using this variation, agents become more reluctant to change as their number of strategy changes increases. Note that with this variation, it is technically possible for a deadlock situation to arise, in which the system becomes fixed in a steady state. However, we have not observed such a situation in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>The four basic update functions described in section 3.1 were tested, using the four variations described in section 3.2. This resulted in sixteen different strategy update functions in total. Each of these functions was run for 50 matches, with a match lasting, on average, for six interactions. Thus, the length n of each run generated in each experiment was approximately 300. To avoid statistical anomalies, each experiment was repeated 80 times. The remaining parameters for the experiments were as follows: agents can choose from 4 strategies, and so |Σ| = 4. Each experiment consisted of 100 agents, and so |Ag| = 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Norm Convergence -C u n</head><p>The experimentally determined values of C u n , (the average convergence at time n), for n ' 300, are given in Table <ref type="table" target="#tab_0">1</ref>. Note that these values are expressed as percentages. Rows (1-4) correspond to the four basic update functions described in section 3.1, and columns (a-d) correspond to variants in the size of majority, described in section 3.2.</p><p>We make the following observations on these results:</p><p>• By increasing the size of the majority, we appear to see a slight degradation in C u n . However, the significance of this is not clear. Certainly, there is very little difference between a majority size of more than double, and one of more than four times as much.</p><p>• In <ref type="bibr" target="#b6">[7]</ref>, the two update functions that performed best with respect to C u n were the memory restart function (2a), and the communication by agent type function (3a). Both of these used a simple majority, (i.e., a majority of at least one), and performed significantly better than using the simple majority with full memory function (1a). We had similar expectations for our experiments, but these do not appear to have been fully borne out. Whilst (2a) has shown some improvement, (3a) shows no significant gain over a function using no communication at all.</p><p>• The dynamic majority variation appears overall to be the most efficient of variations (b-d) with respect to C u n . In two experiment sets -(1d) and (3d) -it performs even better than the simple majority function.</p><p>• Our own update function, (4), produces the best results for C u n . All experiments using the simple majority reached a value for C u n of either 100% or 99% for n ' 300. We are unsure at present as to the processes that lead to such a good result for norm convergence. However, (4) also produces the most significant differences amongst the three majority variations of any of the functions. Here, increasing the majority that agents use does have a significant effect on C u n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Strategy Change Results -H u n and M u n</head><p>One possible drawback to the simple majority function is the number of strategy changes that agents might make; as we noted in section 2, changes in strategy can be costly, and lead to inefficiencies within a system. For this reason, we introduced H u n , the average number of strategy changes per interaction, which we use as the second measure of strategy update function efficiency. Table <ref type="table" target="#tab_2">2</ref> shows, for each of the sixteen strategy update functions investigated, the experimentally determined values of H u n , for n ' 300. Some observations we can make from these results are as follows:</p><p>• Using the simple majority function, (1a), leads to a high value of H u n , as expected. However, (1a) is by no means the worst. The two modifications to (1a) that were described in <ref type="bibr" target="#b6">[7]</ref> -experiments (2a) and (3a) -also perform poorly and in particular, communication by agent type (3a) leads to a very high value of H u n . The update function <ref type="bibr" target="#b3">(4)</ref>, which performed so well with respect to C u n , also performs poorly with respect to H u n .</p><p>• Increasing majority size does lead to an improvement in H u n , as expected. However, in running the experiment using a dynamically determined majority, we expected an even better performance. This has not proved to be the case. The performance of dynamically determined majority appears to be comparable to that of the simple majority variation. The value of M u n for the dynamically determined majority is, however, more encouraging. It is either equal to or better than that recorded against the double majority.</p><p>• The most significant differences across the four columns of Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table" target="#tab_1">3</ref> occur in the two experiment sets that deal with communication.</p><p>Clearly, those update functions that perform best with respect to C u n do not necessarily perform as well with respect to H u n and M u n . What is needed is a trade-off between the speed by which agents can come to agree on a norm, and the number of changes of strategy they incur in the process. Combining all three tables, perhaps the best experiment set overall is (4b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>Our research has been motivated by a concern, voiced in <ref type="bibr" target="#b6">[7]</ref>, that an understanding of the mechanisms of convention evolution will be important for the design of future multi-agent systems. Building on the work of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref>, we have presented a new formal model that defines the problem domain, a methodology, and a set of performance measures. We have described sixteen mechanisms by which agents can come to a global agreement, and have evaluated these mechanisms with respect to our performance measures. That some of our results have been unexpected indicates how much further we need to pursue our understanding of this complex topic. The development of a mathematical theory of convention evolution -suggested in <ref type="bibr" target="#b6">[7]</ref>, and tentatively begun by us in section 2 -is an obvious requirement. Other areas for future work include a further investigation of different strategy update functions (see sections 3.1 and 3.2), and more detailed investigation of the role played by social structure and communication (cf. <ref type="bibr" target="#b4">[5]</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1g, then m(n) denotes the n'th interaction in m. If σ ∈ Σ and m ∈ M, then obs(σ , m) is the number of times that other agents have been seen to use σ in m: obs(σ , m) = |fn | n ∈ f0,… , |m| -1g and other(m(n)) = σ g|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>to be the fraction of agents using the most popular strategy at time n in run r. conv(r, n) = maxf|chosen(σ , r, n)| | σ ∈ Σg |Ag| If |Σ| = 2, then convergence will range from 0.5 to 1.0; if |Σ| = 3, then convergence will range from 0.33 to 1.0. If conv(r, n) = 1.0, then in run r at time n, every agent has chosen the same strategy. If u ∈ U, then we let C u n denote the average convergence of all runs r ∈ R u at time n ∈ IN 1 . Note that we normally express C u n as a percentage. The value C u n , for some n ∈ IN, is the first measure by which we assess the performance of a strategy update function; we aim to design a function u ∈ U that will make C u n = 100 for as small n as possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimentally Determined Values of C u n , for n ' 300</figDesc><table><row><cell>Update</cell><cell cols="2">Size of majority</cell><cell></cell><cell></cell></row><row><cell>function</cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell></row><row><cell></cell><cell cols="4">Simple Double Quadruple Dynamic</cell></row><row><cell>(1) External majority</cell><cell>75</cell><cell>73</cell><cell>74</cell><cell>84</cell></row><row><cell>(2) Memory restart</cell><cell>82</cell><cell>77</cell><cell>76</cell><cell>77</cell></row><row><cell>(3) Communication by type</cell><cell>76</cell><cell>75</cell><cell>74</cell><cell>79</cell></row><row><cell>(4) Communication on success</cell><cell>99</cell><cell>80</cell><cell>73</cell><cell>86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows, for each of the sixteen strategy update</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimentally Determined Values of H u n , for n ' 300</figDesc><table><row><cell>Update</cell><cell cols="2">Size of majority</cell><cell></cell><cell></cell></row><row><cell>function</cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell></row><row><cell></cell><cell cols="4">Simple Double Quadruple Dynamic</cell></row><row><cell>(1) External majority</cell><cell>9</cell><cell>5</cell><cell>3</cell><cell>5</cell></row><row><cell>(2) Memory restart</cell><cell>20</cell><cell>11</cell><cell>10</cell><cell>11</cell></row><row><cell>(3) Communication by type</cell><cell>57</cell><cell>24</cell><cell>6</cell><cell>7</cell></row><row><cell>(4) Communication on success</cell><cell>26</cell><cell>4</cell><cell>3</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimentally Determined Values of M u</figDesc><table /><note><p>n , for n ' 300 functions, the experimentally determined values of M u n , (the maximum number of strategy changes made by any agent up to time n), again for n ' 300.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This notation was introduced by<ref type="bibr" target="#b4">[5]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The first three are adapted from<ref type="bibr" target="#b6">[7]</ref>; the fourth is entirely new.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simulative understanding of norm functionalities in social groups</title>
		<author>
			<persName><forename type="first">R</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castelfranchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulating Societies-93: Pre-proceedings of the 1993 International Symposium on Approaches to Simulating Social Phenomena and Social Processes</title>
		<meeting><address><addrLine>Siena, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Certosa di Pontignano</publisher>
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alliances and social norms in societies of nonhomogenous, interacting agents</title>
		<author>
			<persName><forename type="first">N</forename><surname>Findler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malyankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulating Societies-93: Pre-proceedings of the 1993 International Symposium on Approaches to Simulating Social Phenomena and Social Processes</title>
		<meeting><address><addrLine>Siena, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Certosa di Pontignano</publisher>
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emergent coordination through the use of cooperative state-changing rules</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Rosenschein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Workshop on Distributed Artificial Intelligence (IWDAI-93)</title>
		<meeting>the Twelfth International Workshop on Distributed Artificial Intelligence (IWDAI-93)<address><addrLine>Hidden Valley, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="171" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Commitments and conventions: The foundation of coordination in multi-agent systems</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Jennings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="250" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emergent conventions and the structure of multi-agent systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Kittock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 Santa Fe Institute Complex Systems Summer School</title>
		<meeting>the 1993 Santa Fe Institute Complex Systems Summer School</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convention -A Philosophical Study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Harvard University Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Emergent conventions in multi-agent systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m">Proceedings of Knowledge Representation and Reasoning (KR&amp;R-92)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Rich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Swartout</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Nebel</surname></persName>
		</editor>
		<meeting>Knowledge Representation and Reasoning (KR&amp;R-92)</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="225" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the synthesis of useful social laws for artificial agent societies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92)</title>
		<meeting>the Tenth National Conference on Artificial Intelligence (AAAI-92)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
