<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Composite Correlation Quantization for Efficient Multimodal Retrieval *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<email>mingsheng@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Laboratory (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>caoyue10@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Laboratory (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Laboratory (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Data Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Composite Correlation Quantization for Efficient Multimodal Retrieval *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E244A9B38D34EFE782B4B78679D6A9DD</idno>
					<idno type="DOI">10.1145/2911451.2911493</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hashing</term>
					<term>quantization</term>
					<term>multimodal retrieval</term>
					<term>correlation analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficient similarity retrieval from large-scale multimodal database is pervasive in modern search engines and social networks. To support queries across content modalities, the system should enable cross-modal correlation and computation-efficient indexing. While hashing methods have shown great potential in achieving this goal, current attempts generally fail to learn isomorphic hash codes in a seamless scheme, that is, they embed multiple modalities in a continuous isomorphic space and separately threshold embeddings into binary codes, which incurs substantial loss of retrieval accuracy. In this paper, we approach seamless multimodal hashing by proposing a novel Composite Correlation Quantization (CCQ) model. Specifically, CCQ jointly finds correlation-maximal mappings that transform different modalities into isomorphic latent space, and learns composite quantizers that convert the isomorphic latent features into compact binary codes. An optimization framework is devised to preserve both intra-modal similarity and inter-modal correlation through minimizing both reconstruction and quantization errors, which can be trained from both paired and partially paired data in linear time. A comprehensive set of experiments clearly show the superior effectiveness and efficiency of CCQ against the state of the art hashing methods for both unimodal and cross-modal retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>While big data with large volume, high dimensions, and multiple modalities are ubiquitous in search engines and social networks, it has attracted increasing attention to distill the correlation structures across heterogenous data modalities. For example, an uploaded image on Flickr is usually annotated with some relevant descriptions or tags, while a featured article on Wikipedia may consist of some correlative images. As relevant data from different modalities may endow semantic correlations, it is desirable to support multimodal search, which retrieves semantically-relevant results of all modals in response to a unimodal query. Taking Flickr as an example, when a query image is given, the system should return both relevant tags and images. Due to large volume and semantic gap <ref type="bibr" target="#b18">[18]</ref>, effective and efficient retrieval of multimodal data remains a challenge.</p><p>In the case that the reference database is large-scale or that the distance calculation between query item and database item is costly, an efficient solution to enabling similarity search is hashing based methods <ref type="bibr" target="#b22">[22]</ref>, which perform approximate nearest neighbor (ANN) search with both computation efficiency and acceptable accuracy. The principle of hashing is to transform high-dimensional data into compact binary codes and generate similar binary codes for similar data items. The seminal work includes Locality Sensitive Hashing (LSH) <ref type="bibr" target="#b1">[1]</ref> and Spectral Hashing (SH) <ref type="bibr" target="#b25">[25]</ref>. However, traditional unimodal hashing methods cannot support multimodal search as ANN cannot be directly computed across different modalities.</p><p>Recently, several useful attempts have been made to multimodal hashing, which builds correlation structures across multiple modalities in the process of hash function learning and index multimodal data in a common Hamming space <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b16">16]</ref>. These methods generally work in two-step pipeline: first, embed multiple data modalities into a continuous isomorphic latent space by maximizing inter-modal correlations, and second, quantize the isomorphic embeddings into binary hash codes by sign thresholding. While showing promising performance, the two-step pipeline may encounter two limitations: first, conversion from realvalued features to discrete codes may incur substantial information loss, making the continuous latent space suboptimal for binary coding and the binary codes suboptimal for retrieval <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b10">10]</ref>; second, directly binarizing latent features may lead to unbalanced encoding schemes <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref>. Fundamentally, by continuous relaxation of the binary constraints, most methods solve an optimization problem which may deviate significantly from the hashing objective as the quantization error is not accounted for in the optimization process. This somewhat contradicts the motivation of multimodal hashing. Hence, how to learn isomorphic hash codes for multimodal data in a seamless optimization framework remains an open problem.</p><p>In this paper, we propose Composite Correlation Quantization (CCQ), a novel model towards seamless multimodal hashing. Technically, CCQ jointly finds correlation-maximal mappings that transform different modalities into an isomorphic latent space, and learns composite quantizers that convert the isomorphic latent features into compact binary codes. The flowcharts of CCQ and prior work are shown in Figure <ref type="figure" target="#fig_2">1</ref>. To create a seamless optimization framework, we are inspired by Latent Semantic Analysis (LSA) <ref type="bibr" target="#b7">[7]</ref> and decompose each datum into three latent factors, namely, correlationmaximal mapping, similarity-preserving codebook, and compact binary code. The three latent factors are jointly learned through an optimization problem, which preserves both intra-modal similarity   Prior work is a two-step pipeline: first map image-text pairs to isomorphic latent space (denoted as polygon) and then binarize the continuous representation to hash codes (denoted as vertices of hypercube) by sign thresholding. CCQ is a seamless optimization framework: jointly map both paired/unpaired images and texts to isomorphic latent space (denoted as polygon) and learn hash codes by composite quantization. The quantization model learns isomorphic codebook (denoted as Voronoi digram) and binary codes (denoted as histograms) by minimizing the quantization error, which suffices to assign each latent representation to M -nearest codewords (denoted as Voronoi cells) and assignment indices are used as hash codes.</p><note type="other">Image Mapping</note><note type="other">Isomorphic Codebook Latent Semantic Image Mapping</note><note type="other">Continuous Representation Hash Code Latent Transform</note><p>and inter-modal correlation while minimizing both reconstruction and quantization errors. The CCQ model can construct extremely compressed and balanced binary codes to enable efficient multimodal search, can readily handle a ubiquitous semi-paired scenario where only a fraction of input data are multimodal, and can scale linearly to large sample size. Comprehensive empirical evidence on large-scale datasets confirms that the CCQ model exhibits superior performance in both effectiveness and efficiency on both unimodal and cross-modal search against state of the art hashing methods.</p><p>The subsequent paper is organized as follows. We review related works in Section 2. We formally present our model in Section 3 and algorithm with analysis in Section 4. Empirical evaluations are reported in Section 5, while conclusions are enclosed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Recently, hashing-based multimodal search is a prevalent research focus in machine learning and information retrieval communities <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b31">31]</ref>, which enables approximate similarity search on multimedia database with significant speedup and acceptable accuracy. Refer to <ref type="bibr" target="#b22">[22]</ref> for a comprehensive survey.</p><p>Existing multimodal hashing methods can be organized into two categories: supervised methods and unsupervised methods. CMSSH <ref type="bibr" target="#b5">[5]</ref>, SCM <ref type="bibr" target="#b28">[28]</ref>, QCH <ref type="bibr" target="#b26">[26]</ref>, and SePH <ref type="bibr" target="#b14">[14]</ref> are supervised hashing methods that require labeled pairs to indicate if the objects from different modalities are similar (positive) or dissimilar (negative). As supervised information is usually unavailable in many applications, the deployment of these methods may be severely restricted. CVH <ref type="bibr" target="#b13">[13]</ref>, IMH <ref type="bibr" target="#b20">[20]</ref>, MSAE <ref type="bibr" target="#b24">[24]</ref> and CorrAE <ref type="bibr">[8]</ref> are unsupervised hashing methods applicable to the most general multimodal retrieval case given that paired data are available, while our proposed CCQ model falls into this category. IMH <ref type="bibr" target="#b20">[20]</ref> is an extension of spectral hashing <ref type="bibr" target="#b25">[25]</ref> to multimodal data, which is restricted by the training burden since constructing and eigendecomposing the similarity matrices require O(N 2 ). While CVH <ref type="bibr" target="#b13">[13]</ref> tackles the scalability issue, it does not jointly maximize cross-modality correlation and preserve intra-modality similarity. MSAE <ref type="bibr" target="#b24">[24]</ref> and CorrAE <ref type="bibr">[8]</ref> can capture both intra-modal similarity and inter-modal correlation by deep autoencoders, but they require spectral hashing or sign thresholding for obtaining binary codes from the continuous embeddings, which will give rise to uncontrollable quantization errors <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>A crucial problem with existing methods is that they essentially work in a separated two-step pipeline: first embed multimodal data into a common continuous latent space and then threshold the continuous embeddings into binary codes of the Hamming space. Such conversion from real-valued features to discrete codes may result in substantial information loss, making the continuous latent space suboptimal for the binary codes and the binary codes suboptimal for retrieval <ref type="bibr" target="#b30">[30]</ref>. Furthermore, directly binarizing latent representation may lead to unbalanced encoding schemes, as shown in <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref>. Although IMVH <ref type="bibr" target="#b10">[10]</ref> learns multimodal hash functions using a graph-cut quantizer instead of the sign thresholding, the quantizer solves a fast approximation of energy function with orthogonal constraints and recurs large quantization error and unbalanced codes. CCQ approaches this problem by learning the modality-consistent latent space and balanced binary codes in a principled framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">COMPOSITE CORRELATION QUANTI-ZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statements</head><p>In the multimodal search system, the database and query consist of objects from different modalities. We only use image and text as two modalities to explain our approach, but the approach is formulated to support any number V of modalities. Let X 1 ∈ R P 1 ×N 1 be an image set of N0 images with tags and the rest N1 images without tags, where N1 = N0 + N1 and each image is represented by P1dimensional feature vector. Let X 2 ∈ R P 2 ×N 2 be a text set of N0 documents of the image tags and additional N2 documents, where N2 = N0 + N2 and each text is represented by P2-dimensional feature vector. Note that the proposed approach can handle semipaired data where only a fraction N0/(N1 + N2) of objects are multimodal, and is more realistic than typical multimodal methods.</p><p>An efficient approach to calculating the distance between image and text is to map images and texts to modality-isomorphic binary codes in which different modalities of the objects are comparable. In this paper, we will approach this problem by a joint optimization framework, dubbed Composite Correlation Quantization (CCQ). DEFINITION 1 (CCQ). Given an image x 1 n ∈ R P 1 and a text x 2 n ∈ R P 2 , learn two correlation-maximal mappings f 1 : R P 1 → R D and f 2 : R P 2 → R D that transform images and texts into a D-dimensional isomorphic latent space, and jointly learn two composite quantizers q 1 : R D → {0, 1} H and q 2 : R D → {0, 1} H that quantize latent embeddings into compact H-bits binary codes.</p><p>In the common H-bits binary space, image and text can be easily comparable such that both intra-modal and cross-modal search can be readily supported. After mappings f 1 , f 2 and quantizers q 1 , q 2 have been learned, the multimodal search problem can be converted into classical approximate nearest neighbor (ANN) search problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Composite Correlation Quantization</head><p>The main idea of CCQ is to jointly learn a correlation-maximal latent space and a similarity-preserving composite quantization in a unified optimization framework. To achieve this mission, we are inspired by Latent Semantic Analysis (LSA) <ref type="bibr" target="#b7">[7]</ref> and decompose each input datum (image or text)</p><formula xml:id="formula_0">x v n into three latent factors R v , C v , b v n , that is, x v n ≈ R v C v b v n .</formula><p>While sharing similar formation as LSA, our formulation endows these latent factors with different semantics and thus constrains them with different conditions. More specifically, R v is correlation-maximal mapping, C v is similaritypreserving codebook, and b v n is the compact binary code of x v n . We present how to formulate the CCQ approach under these semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Intra-Modality Similarity Quantization</head><p>To represent inputs with compact binary codes, two mainstream paradigms are sign thresholding in Hamming embedding methods <ref type="bibr" target="#b25">[25]</ref>, and vector quantization in codebook-based encoding methods <ref type="bibr" target="#b12">[12]</ref>. As sign thresholding cannot guarantee minimal quantization error, we therefore adopt the vector quantization paradigm. CCQ is based on a set of</p><formula xml:id="formula_1">M codebooks C v = [C v 1 , . . . , C v M ], where each codebook C v m contains K codewords C v m = [C v m1 , . . . , C v mK ],</formula><p>and each codeword C v mk is a D-dimensional vector like the cluster centroid in kmeans clustering. Corresponding to the M codebooks, we partition the binary codewords assignment vector b</p><formula xml:id="formula_2">v n into M 1- of-K indicator vectors b v n = [b v 1n ; . . . ; b v mn ],</formula><p>and each indicator vector b v mn indicates which one (and only one) of the K codewords in the mth codebook is selected to approximate the nth data point. The CCQ model encodes each x v n as the sum of M codewords, one codeword per codebook, each indicated by the binary assignment vector b v n . This yields a novel and more accurate composite approximation scheme</p><formula xml:id="formula_3">x v n ≈ R v M m=1 C v m b v mn .</formula><p>Consistent with LSA and kmeans, the sum of squared loss between all x v n 's and the sum of selected codewords after transformed by R v , is minimized,</p><formula xml:id="formula_4">min R v ,C v ,B v Nv n=1 x v n -R v M m=1 C v m b v mn 2 2 s.t. bmn 0 = 1, bmn ∈ {0, 1} K m = 1 . . . M, n = 1 . . . Nv,<label>(1)</label></formula><p>where • 0 denotes the 0-norm that simply counts the number of the vector's nonzero elements. The constraint guarantees that only one codeword in each codebook can be activated to approximate the input data, hence it can lead to compact binary codes. As the binary constraints are directly imposed to the learning objective and are valid throughout the optimization procedure, the derived binary codes are much more accurate than sign thresholding binary codes. The rationale of using M codebooks instead of single codebook to approximate each input datum is to further minimize quantization error, as the latter is shown to yield significantly lossy compression and incur evident performance drop <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b3">3]</ref>. Quantization based on multiple codebooks yields balanced composite binary codes which are more effective than Hamming embedding binary codes <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b17">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Inter-Modality Correlation Maximization</head><p>The most desirable value of multimodal retrieval is to enable transfer of knowledge across different modalities so that cross-modal retrieval performance can be improved. A fundamental assumption for multimodal retrieval is that by mapping objects in a modalityconsistent latent space, the latent space representations of semantically relevant inter-modal pairs should be consistent. More specifically, for each input object with both image modality x 1 n and text modality x 2 n , after being transformed by R 1 and R 2 in Equation (1), the latent space representations for image modality C 1 b 1 n and text modality C 2 b 2 n should be similar. To our knowledge, most prior work adopts the coupling strategy to minimize</p><formula xml:id="formula_5">C 1 b 1 n -C 2 b 2 n 2 2 .</formula><p>In this paper, we propose to maximize cross-modal correlation by sharing codebooks {Cm} M m=1 for different modalities and sharing binary codes {bn} N 0 n=1 for semantically relevant inter-modal pairs. While for the data points with only one modality, the multimodal sharing strategy does not apply. Hence, the proposed condition that the modality-consistent latent space should satisfy is formulated as</p><formula xml:id="formula_6">C v m = Cm and δ (b v mn ) = bmn, n = 1 . . . N0 b v mn , otherwise,<label>(2)</label></formula><p>where δ(•) distinguishes multimodal objects from unimodal ones. Different from most prior methods <ref type="bibr" target="#b20">[20,</ref><ref type="bibr">8]</ref>, our modality-consistent condition requires identical code b</p><formula xml:id="formula_7">1 n = b 2 n , instead of minimized distance b 1 n -b 2</formula><p>n , for the semantically relevant inter-modal pairs. There are two advantages of our approach. First, since our learning objective keeps the binary constraint valid throughout optimization procedure, it is very difficult to require minimized distance between two binary codes as their nonzero elements may differ significantly. Note that prior methods simply drop the binary condition and solve a continuous problem, which leads to uncontrollable quantization error with the post-step sign thresholding. Second, integrating the minimized distance condition in the learning objective as existing methods may introduce a trade-off term, or parameter, that is hard to tune since its magnitude is very different from learning loss (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Joint Optimization Framework</head><p>To approach CCQ, which jointly learns a similarity-preserving composite quantization and a correlation-maximal latent space in a unified optimization framework, we jointly require the codebooks {Cm} M m=1 subject to minimizing the quantization error of all modalities as Equation ( <ref type="formula" target="#formula_4">1</ref>), and the mappings R v subject to maximizing the correlations between semantically relevant inter-modal pairs as Equation ( <ref type="formula" target="#formula_6">2</ref>) while jointly minimizing the reconstruction error of input data as LSA. This leads to a joint optimization framework as</p><formula xml:id="formula_8">min R v ,C,B v V v=1 Nv n=1 λv x v n -R v M m=1 Cmδ (b v mn ) 2 2 s.t. R vT R v = ID×D, R v ∈ R Pv ×D δ (b v mn ) 0 = 1, δ (b v mn ) ∈ {0, 1} K δ (b v mn ) = bmn, n = 1 . . . N0 b v mn , otherwise v = 1 . . . V, m = 1 . . . M, n = 1 . . . Nv,<label>(3)</label></formula><p>where λv is the weight parameter for each modality, and in bimodal problems with V = 2, we can simplify the notations by denoting λ1 = 1 and λ2 = λ, while such notations are used throughout this paper. R v is the transformation matrix that maps the inputs of each modality to a D-dimensional modality-consistent latent space. The orthogonal constraints are motivated by LSA, which can turn latent factors R v into transformation matrices for efficient out-ofsample quantization. The binary codes b v n are M ×K-dimensional, fortunately however, each b v mn is 1-of-K encoding with only one nonzero element and can be represented using log 2 K bits, hence the final hash codes b v n can be compacted into H = M log 2 K bits, which is independent on the dimensions of input or latent spaces. To fit each b v mn into one byte, K = 256 is a good choice <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b30">30]</ref>. We simply set D = min({Pv} V v=1 , H), in the sense that an H-bit binary code can reconstruct a D-dimensional vector accurately.</p><p>Formally, we derive correlation-maximal mappings</p><formula xml:id="formula_9">f v (x v n ) = R vT x v n and similarity-preserving quantizers q v (f v (x v n )) = b v n .</formula><p>There are several advantages of the CCQ approach. First, CCQ jointly learns a correlation-maximal latent space and a similaritypreserving composite encoding, which can minimize the quantization loss and guarantee search quality. Second, CCQ explores both paired and unpaired data in a semi-paired quantization paradigm, which can benefit from semi-supervised learning in that paired data consolidate inter-modality correlation and unpaired data enhance intra-modality quantization. Third, CCQ is formulated with only two easy-tuning model parameters D and λ, where D can be set as simply as LSA to retain most covariance information, and λ can be selected by trading off different modalities using prior information. In particular, the proposed sharing of codebooks and binary codes across modalities (2) enables joint learning of latent semantics that are maximally correlated in the isomorphic feature space, which contributes most significantly to the efficacy of the CCQ approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approximate Nearest Neighbor Search</head><p>Approximate nearest neighbor (ANN) search based on Euclidean distance is a powerful task for quantization techniques <ref type="bibr" target="#b12">[12]</ref>. Given a database of CCQ hash codes {b v n } Nv n=1 , we follow <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b17">17]</ref> and use Asymmetric Quantizer Distance (AQD) as similarity metric that computes the distance between query q v and database point</p><formula xml:id="formula_10">x v n as AQD q v , x v n = q v -R v M m=1 Cmb v mn 2 2 = -2 M m=1 qv , Cmb v mn + M m=1 Cmb v mn 2 2 + qv 2 2 + R vT ⊥ q v 2 2 ,<label>(4)</label></formula><p>where qv = R vT q v is the transformed query. In the second row, the first term computes the inner products between qv and M codewords selected by b v n . Given a query, these inner products for all M codebooks {Cm} M m=1 and all K possible values of b v mn can be pre-computed and stored in a query-specific M × K lookup table, which is used to compute AQD between the query and all database points, each entails M table lookups and additions and is slightly more costly than Hamming distance. The second term computes the squared norm of decoded database point, which is independent on the query and can be encoded using one byte by quantizing these scale values on held-out dataset <ref type="bibr" target="#b3">[3]</ref>. At quantization, we augment CCQ code with the norm byte, which costs one more lookup and one more byte per database point. We can eliminate this norm byte by composite quantization <ref type="bibr" target="#b30">[30]</ref>, but will leave it to our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ALGORITHM AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning Algorithm</head><p>The CCQ optimization problem (3) consists of three variables, R v , C, and B v . We adopt alternating optimization <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b30">30]</ref> which iteratively updates one variable with the rest variables fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Update R v</head><p>We update R v by fixing C and B v as known variables, and write Equation (3) with R v as unknown variables in matrix formulation,</p><formula xml:id="formula_11">min R v X v -R v Cδ (B v ) 2 F s.t. R vT R v = ID×D.</formula><p>(5) This is equivalent to the Orthogonal Procrustes problem <ref type="bibr" target="#b19">[19]</ref> and can be solved exactly using SVD. More specifically, we perform SVD as</p><formula xml:id="formula_12">X v [Cδ (B v )] T = USV T , then we achieve R v = UV T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Update C</head><p>We update C by fixing R v and B v as known variables, and write Equation (3) with C as unknown variables in matrix formulation,</p><formula xml:id="formula_13">min C V v=1 R vT X v -Cδ (B v ) 2 F . (<label>6</label></formula><formula xml:id="formula_14">)</formula><p>Algorithm 1: CCQ: Composite Correlation Quantization    </p><formula xml:id="formula_15">Input: Data {X v } V v=1 ; latent dimension D, modal weight λ. Output: Mappings {R v }, codebook C, binary codes {B v }. 1 Initialize {R v }</formula><formula xml:id="formula_16">C = V v=1 λvR vT X v δ (B v ) T V v=1 λvδ (B v ) δ(B v ) T -1</formula><p>. Algorithms such as L-BFGS can be used to speed up computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Update</head><formula xml:id="formula_17">B v It is obvious that each b v n is independent on {b v n } n =n , then the optimization problem for B v is decomposed to Nv subproblems, min b v n V v=1 λv R vT x v n - M m=1 Cmδ (b v mn ) 2 2 s.t. δ (b v mn ) 0 = 1, δ (b v mn ) ∈ {0, 1} K . (<label>7</label></formula><formula xml:id="formula_18">)</formula><p>This optimization problem is generally NP-hard. As shown in <ref type="bibr" target="#b30">[30]</ref>, this problem is essentially high-order Markov Random Field (MRF) problem and can be solved by the Iterated Conditional Modes (ICM) algorithm <ref type="bibr" target="#b4">[4]</ref> which solves M indicators {b v mn } M m=1 alternatively. Given {b v m n } m =m fixed, we update b v mn by exhaustively checking all the codeword in codebook Cm, finding the codeword such that the objective in ( <ref type="formula" target="#formula_17">7</ref>) is minimized, and setting the corresponding entry of b v mn as 1 and the rest as 0. The algorithm is guaranteed to converge, and can be terminated if maximum iterations are reached.</p><p>To accelerate quantization, we can explore hierarchical structure of codebooks {Cm} and update {b v mn } by a new greedy algorithm. Specifically, after updating {b v m n } m &lt;m , we can update b v mn by encoding residual</p><formula xml:id="formula_19">R vT x v n -m-1 m =1 C m δ (b v m n ) with codebook Cm.</formula><p>The overall learning procedure is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Large-Scale Implementation</head><p>Batch algorithms are memory-inefficient for large-scale datasets, hence we formulate CCQ optimization into mini-batch algorithms for large-scale problems <ref type="bibr" target="#b24">[24]</ref>. The main idea is to split the training set into mini-batches and load a fraction of data points into memory each time. Hence, the memory usage stays constant when the size of the training set increases. The update of B v in Equation ( <ref type="formula" target="#formula_17">7</ref>) is already mini-batch in that update of each data point is independent on the other data points. To update R v in mini-batch, we notice that the matrix for SVD is X v [Cδ (B v )] T ∈ R Pv ×D , which if given, the SVD can be solved in O(P 2 v D), independent on the number of data points. We thus formulate the matrix for SVD in a point-wise summation form as Nv n=1 x v n [Cδ (bn)] T , then it can be computed by traversing all data points in a mini-batch paradigm. Similarly, the update of C can also be formulated in a summation form for mini-batch implementation. Note that we can allocate all available memory to mini-batch and trade off memory and disk reading costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Computational Complexity</head><p>We analyze the cost of each iteration to show CCQ scales linearly to sample size Nv. To update R v , it takes O (NvPvD + NvDM ) to prepare the problem and O P 2 v D + D 3 to compute the SVD.</p><p>To update C, it takes O NvPvD + NvDM + NvM<ref type="foot" target="#foot_0">2</ref> to prepare the problem and O DM 2 K 2 + M<ref type="foot" target="#foot_1">3</ref> K 3 to compute the quadratic optimization. To update B v , it takes O (NvPvD + NvDM KTi), where Ti is the number of iterations and Ti = 3 in ICM algorithm or Ti = 1 in greedy algorithm can obtain satisfactory performance. As a rule of thumb, D = H and K = 256 are good choices for most applications. For longer codes, update of C is inefficient, in which case we can adopt the online L-BFGS algorithm for speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Approximation Error Analysis</head><p>Given a query q v and a database point x v n , after transformed by correlation-maximal mappings qv = R vT q v and xv n = R vT x v n , they can be comparable in the modality-consistent latent space, and their Euclidean distance is computed as d qv , xv n = qvxv n 2 . As computing Euclidean distance on real-valued vectors is too costly for large-scale search, we compute AQD (4) on binary codes. Hence, we need to analyze the error bound of using AQD to approximate real-valued distance. Denote</p><formula xml:id="formula_20">xv n = M m=1 Cmb v mn the decoded vector of x v n , then AQD q v , x v n = d qv , xv n + , is a constant.</formula><p>THEOREM 1 (BOUND). The error is bounded by learning loss</p><formula xml:id="formula_21">d qv , xv n -d qv , xv n x v n -R v M m=1 Cmb v mn 2 . (<label>8</label></formula><formula xml:id="formula_22">)</formula><p>PROOF. From the triangle inequality,</p><formula xml:id="formula_23">d qv , xv n -d qv , xv n d (x v n , xv n ). Then d 2 (x v n , xv n ) = R vT x v n -M m=1 Cmb v mn 2 2 R vT x v n -M m=1 Cmb v mn 2 2 + R vT ⊥ x v n 2 2 = x v n -R v M m=1 Cmb v mn 2 2 , (<label>9</label></formula><formula xml:id="formula_24">) where R v ⊥ is an orthogonal complement of R v , R vT R v ⊥ = 0.</formula><p>The theorem confirms that the error of using AQD to approximate real-valued distance is statistically bounded by CCQ learning loss. Hence, CCQ is more accurate than sign thresholding methods <ref type="bibr" target="#b25">[25]</ref>.</p><p>An important advantage of CCQ in Equation ( <ref type="formula" target="#formula_23">9</ref>) is that mapping R v is learned by a joint optimization of canonical correlation analysis (CCA) and principal component analysis (PCA) corresponding to the first and second terms of Line 2 in Equation ( <ref type="formula" target="#formula_23">9</ref>). This can be much more effective than most CCA-based methods <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b26">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>We conduct extensive evaluation of CCQ against state of the art methods on three public multimodal datasets. We investigate both effectiveness and efficiency in terms of search precision, recall, and time. The codes, data, and configurations will be available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The evaluation is conducted on three datasets: NUS-WIDE <ref type="bibr" target="#b6">[6]</ref>, Wiki <ref type="bibr" target="#b18">[18]</ref>, and Flickr1M <ref type="bibr" target="#b11">[11]</ref>, with statistics depicted in Table <ref type="table" target="#tab_1">1</ref>. We preprocess all datasets by applying ZCA <ref type="bibr" target="#b24">[24]</ref> to normalize each dimension of image/text features to be zero mean and unit variance.</p><p>NUS-WIDE 1 is a Web image dataset containing 269, 648 images downloaded from Flickr, each associated with 6 tags on average. There are 81 ground truth concepts manually annotated for search evaluation. Following prior works <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b24">24]</ref>, we prune the original NUS-WIDE to form a new dataset consisting of 195,834 image-text The hash models are learned on the training set containing 10,000 image-text pairs randomly sampled from the database <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b20">20]</ref>. Wiki 2 contains 2,866 image-text pairs selected from Wikipedia's featured articles comprised of multiple sections of images and texts. Every image-text pair is labeled by one of the 10 concepts in the article categories. Each image is represented by a 128-dimensional bag-of-words vector extracted from SIFT features, and each text is represented by the probability distribution over 10 topics learned by a latent Dirichlet allocation (LDA) model. The dataset is released with a query set of 693 pairs and a database of 2,173 pairs, and the whole database is used as the training set for hash coding <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b34">34]</ref>.</p><p>Flickr1M comprises 1,000,000 images associated with tags from Flickr, in which 25,000 are labeled with 38 concepts while the remaining 975,000 are unlabeled. The public available preprocessed dataset 3 is employed for evaluation, in which each image is represented by a 3,857-dimensional vector concatenated by local SIFT feature, global GIST feature, etc <ref type="bibr" target="#b21">[21]</ref>. Each text is represented by a 2,000-dimensional vector extracted from tag occurrences. The query set contains 1,000 image-text pairs randomly sampled from the 25,000 labeled pairs, and the rest 24,000 labeled pairs are used as the database. In scalability test of CCQ (Section 5.7), all 975,000 unlabeled pairs are used as the training set for learning hash codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison Methods</head><p>We compare CCQ against many state of the art hashing methods.</p><p>• Unsupervised hashing: Cross-View Hashing (CVH) <ref type="foot" target="#foot_4">6</ref>  <ref type="bibr" target="#b13">[13]</ref> and Inter-Media Hashing (IMH) <ref type="foot" target="#foot_2">4</ref> [20] are unsupervised hashing methods that extend spectral hashing to exploit the local structure of multimodal data for learning binary codes.</p><p>• Deep hashing: Correspondence Auto-Encoders (CorrAE) <ref type="foot" target="#foot_3">5</ref>[8] learns latent features via unsupervised deep auto-encoders, which captures both intra-modal and inter-modal correspondences, and binarizes latent features via sign thresholding.</p><p>• Supervised hashing: Cross-Modal Similarity-Sensitive Hashing (CMSSH) 6 [5], Semantic Correlation Maximization (SCM) <ref type="bibr" target="#b28">[28]</ref>, and Quantized Correlation Hashing (QCH) are supervised hashing methods which embed multimodal data into a common Hamming space using supervised metric learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Protocols</head><p>We perform four types multimodal retrieval schemes: (1) I → I: use image queries to retrieve relevant images; (2) T → T : use text queries to retrieve relevant texts; (3) I → T : use image queries to retrieve relevant texts; and (4) T → I: use text queries to retrieve relevant images. The first two tasks are intra-modal retrieval and the last two tasks are cross-modal retrieval. As CCQ can also handle multimodal search where both modalities are available for the database, we show the results of multimodal retrieval schemes where each image-text pair is quantized into a unified hash code by fusing knowledge of different modalities: (5) I → IT : use image queries to retrieve relevant image-text pairs; (6) T → IT : use text queries to retrieve relevant image-text pairs. The baseline methods do not support multimodal search because they do not use shared coding for different modalities of the same object. Given a query, the ground truth is defined as: if a result shares at least one common concept with the query, it is relevant; otherwise it is irrelevant.</p><p>We adopt Mean Average Precision (MAP) to measure the effectiveness of multimodal search <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr">8]</ref>. Given a set of queries, we first calculate Average Precision (AP) of each query as</p><formula xml:id="formula_25">AP@R = R r=1 P (r) δ (r) R r =1 δ (r ) , (<label>10</label></formula><formula xml:id="formula_26">)</formula><p>where R is the number of retrieved documents, P (r) denotes the precision of the top r retrieved results, and δ(r) = 1 if the r-th retrieved result is a true neighbor of the query, otherwise δ(r) = 0.</p><p>Then MAP is computed as the mean of all the queries' average precision, and the larger the MAP, the better the retrieval performance.</p><p>In the experiments, we follow <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b24">24]</ref> to report MAP@R = 50. We also report another two standard retrieval criteria, precisionrecall curves and precision@top-R curves of all retrieval tasks. In addition to effectiveness, we report time and memory costs as the efficiency measures for query processing and model training.</p><p>The CCQ approach involves two model parameters: dimension of modality-consistent subspace D and modality trade-off weight λ. In principle, CCQ is almost immune to different choices of D, as long as D is large enough to retain the majority amount of covariance information as LSA. While no prior knowledge is available, we can simply set equal weights λ = 1 for different modalities, which can already achieve satisfactory performance. Nonetheless, for image-text bimodal search, the text modality usually carry more semantic information, hence we equip CCQ with the flexibility for selecting the optimal λ to encode such important prior knowledge. Given annotation ground truths as in the evaluation datasets, we can automatically select D and λ using cross-validation. However, we choose to blindly fix λ = 5 throughout the comparative study. This is desirable as cross-validation may be impossible in the pervasive unsupervised multimodal search. We will study parameter sensitivity in Section 5.8 to validate that CCQ can consistently outperform the state of the arts with a wide range of parameter configurations.</p><p>For the comparison methods, we adopt cross-validation to select their optimal parameters, respectively. As cross-validation requires annotation ground truths, this further confirms CCQ's superior parameter stability. Subject to computation burden, it is too costly to train CMSSH and IMH on the complete Flickr1M dataset, hence we randomly sample 10,000 image-text pairs to train these models. Each experiment repeats ten runs and the average result is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experimental Results</head><p>We compare CCQ with state of the art methods in terms of MAP and precision-recall on 4 multimodal retrieval tasks (I → I, T → T , I → T , T → I) of three datasets (NUS-WIDE, Wiki, and Flickr1M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Results on NUS-WIDE</head><p>We evaluate CCQ against state of the arts with different lengths of hash codes, i.e. 8, 16, 32, and 64 bits, and report the MAP results in Table <ref type="table" target="#tab_2">2</ref>. For all multimodal retrieval tasks, CCQ achieves significantly better performance than all unsupervised hashing methods CVH, IMH, and CorrAE, and generally outperforms the state of the art supervised hashing methods CMSSH, SCM, QCH in most cases. It is very worth noting that, CCQ is an unsupervised hashing method that does not require labeled similarity information. Hence CCQ is particularly beneficial when labeled information is unavailable, which is the most common scenario in big data era. A notable limitation of orthogonal constrained methods CVH and IMH is that longer codes do not necessarily improve performance in crossmodal tasks I → T and T → I. The reason is that these methods learn uncorrelated hash bits via eigenvalue decomposition on similarity matrix, which leads to unbalanced hash codes with the first k eigenvectors (hash bits) dominating the whole hash codes. CCQ via composite quantization in isomorphic space can learn balanced binary codes, hence its performance improves with longer codes. It is interesting to observe that the performances of cross-modal search task I → T is generally better than that of intra-modal search task I → I, while this observation does not hold for the counterparts T → I and T → T . This seems abnormal at first sight as cross-modal search tasks are often more challenging than intramodal search tasks due to semantic gap <ref type="bibr" target="#b18">[18]</ref>. However, in general, text retrieval is much easier than image retrieval, making different modalities of the objects contribute differently the cross-modal retrieval performance. We believe that T → T is much easier than T → I, but I → T may be easier than I → I because image-toimage retrieval is often the most difficult task. In the case of crossmodal task I → T , the knowledge of text modality is transferred to image modality, making cross-modal retrieval easier. This shows cross-modal retrieval can be improved by knowledge transfer.</p><p>The precision-recall curves and the precision@top-R curves <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b24">24]</ref> are illustrated in Figure <ref type="figure" target="#fig_7">2</ref>. For space limitation, only the results of cross-modal tasks I → T and T → I are presented, while similar trends of results are observed on intra-modal tasks I → I and T → T . CCQ shows the best cross-modal retrieval performance on all recall levels and top-R ranks. This validates that CCQ is capable for diverse retrieval scenarios, which may emphasize higher precision at smaller number of top-R retrieved results, i.e. Web search, or higher recall tolerating fairly lower precision, i.e. vertical search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Results on Wiki</head><p>Table 2 also compares the search performance of CCQ and the state of the art methods on the Wiki dataset, which shows that CCQ significantly outperforms the unsupervised hashing methods CVH, IMH, and CorrAE, and performs comparably to supervised hashing methods SCM and QCH. A notable observation is that the MAPs are much smaller than those on the NUS-WIDE dataset. This is reasonable as the images of Wiki are of low-quality (low-resolution) and high-diversity, i.e. the text can well describe the semantics of the image-text pair while the image may not be well related to the semantics of the image-text pair, which makes it more challenging to capture the semantic correlations between image query and text database. Note that the texts of Wiki are featured articles which are well edited by experts and rich in semantic information, hence it is fairly easy to correlate a text query with the multimodal database.</p><p>The precision-recall curves and the precision@top-R curves <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b24">24]</ref> are demonstrated in Figure <ref type="figure" target="#fig_9">3</ref>. Again, CCQ is among the topperforming methods on all recall levels and all top-R ranks. A noticeable performance drop can be examined from the precisionrecall curves to the precision@top-R curves. And this is because the Wiki dataset is very small-scale with only 2,173 database items, hence all relevant results will be retrieved at small R and no more relevant results can be further retrieved when R grows too large. This highlights the importance of evaluation with different metrics.</p><p>A crucial superiority of CCQ over the comparison methods lies in that CCQ jointly learns the isomorphic latent space and compact binary codes by minimizing both correlation and quantization errors in a unified optimization framework, while comparison methods merely learn the isomorphic space and binary codes in a separated two-step pipeline. As examined by CorrAE <ref type="bibr">[8]</ref>, the quality of searching with binary codes using Hamming distance is evidently inferior to searching with continuous features using Euclidean distance, due to substantial information loss by converting continuous features to binary codes without minimizing the quantization error. The search quality loss due to binarization is shown in Figure <ref type="figure" target="#fig_12">5</ref>   binary codes. We see that IMH and CorrAE suffer from substantial MAP loss (similar trends are observed from other methods) while CCQ is almost lossless to binarization. In other words, by jointly minimizing the correlation error and quantization error, CCQ can circumvent information loss and learn more accurate binary codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Results on Flickr1M</head><p>In practical retrieval systems, it is crucial to process large-scale datasets in both training and testing phases, and thus we compare CCQ with state of the art methods on large-scale Flickr1M dataset. We report the MAP results in Table <ref type="table" target="#tab_2">2</ref> and illustrate the detailed precision-recall curves and precision@top-R curves in Figure <ref type="figure" target="#fig_10">4</ref>. As mentioned before, we randomly select 10,000 image-text pairs as training set to learn hash functions if it is computationally too demanding to train these methods on the complete Flickr1M dataset. We can observe that CCQ significantly outperforms the comparison methods on all retrieval tasks and performs better with longer codes. This validates the superiority of CCQ in processing largescale datasets, as the experimental setting on Flickr1M is consistent with real-word system setting where a sufficiently accurate model needs to be derived on a sufficiently large training set. We will examine CCQ's ability to process real semi-paired data in the sequel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Semi-Paired Data Quantization</head><p>Most of the existing methods, including the ones in comparison, require full correspondences between different modalities. In other words, the multimodal data objects are fully paired, e.g. image-text pairs. As a result, these methods are unable to tackle more realistic scenarios in which only a limited number of paired data points are available. CCQ explores the idea of semi-supervised learning and can leverage both paired data (all modalities of the objects are available) and unpaired data (partial modalities of the objects are available) to boost the search quality when paired data are limited. To verify this, we consider the NUS-WIDE and Flickr1M datasets and for each dataset, we randomly sample as the training set 1) 10,000 images without text modality, 2) 10,000 texts without image modality, and 3) different numbers, i.e. [0.5, 1, 2, 4, 8] × 10 3 , of image-text pairs. We train CCQ with these semi-paired data and evaluate the search performance in terms of MAP @ 32 bits.</p><p>The search performances of CCQ on NUS-WIDE and Flickr1M   (d) T → I @ 32 bits We can observe that when the number of paired data points is small, CCQ trained with both paired and unpaired data significantly outperforms CCQ trained with only paired data on most of the multimodal search tasks; when the number of paired data points increases, the search performance of CCQ will gradually saturate while the search quality of the two training paradigms will finally match. This clearly shows that CCQ can effectively leverage both paired and unpaired data (partial multimodal data) to boost search quality in a semi-paired data scenario. An unexpected phenomenon is that semi-paired training slightly deteriorates search performance on task I → T . We conjecture the plausible reason is that searching text database with image queries significantly relies on maximizing the image-text correlations to bridge the semantic gap between low-level image features and highlevel image semantics, i.e. its associated texts. When the number of paired data points is obviously smaller than the number of unpaired data points, semi-paired training may tend to weaken correlation learning from image-text pairs and incur performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Search Efficiency</head><p>To search for approximate nearest neighbors (ANN) in database for a given query, all methods in comparison perform linear scan using symmetric or asymmetric distance. Specifically, to compare a query vector with a database vector, CVH, IMH, and CorrAE all compute symmetric Hamming distance via lookup tables, and CCQ constructs a distance lookup table for each query that stores the Euclidean distances between the query and the multiple codebooks. As a result, CVH, IMH, CorrAE, and CCQ compute exactly the same number of table lookups for linear scan, while their costs of computing the query-codebook distance lookup tables are slightly different, which can be negligible as they are infinitesimal w.r.t. the cost of linear scan. For example, the cost of computing the distance lookup table for CCQ takes only less than 1% of the cost for linear scan on Flickr1M. The average search time of each query by CVH, IMH, CorrAE, and CCQ on the Wiki, NUS-WIDE, Flickr25K, and Flickr1M datasets is illustrated in Figure <ref type="figure" target="#fig_12">5</ref>(d), from which we can observe that the search efficiency are comparable for all methods. While it is beyond the scope of this paper, we want to note that one can adopt a Multi-Index <ref type="bibr" target="#b2">[2]</ref> approach to achieve sub-linear search complexity on the binary codes and further boost search efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Training Complexity</head><p>The training time and memory costs of CCQ scale linearly with the training sample size and hence can process large-scale dataset. To verify this, we follow <ref type="bibr" target="#b24">[24]</ref> and use the complete Flickr1M dataset to evaluate the consumptions of training time and memory. CMSSH and IMH are not compared in this study since they require O(N 2 ) complexity and run out of either time or memory on this dataset.</p><p>The comparison of training time costs is illustrated in Figure <ref type="figure" target="#fig_14">6</ref>(a). We can observe that the training time of CCQ increases linearly with respect to the sample size. Due to multiple iterations between three sets of variables, i.e. transformation matrices R v , quantizer codebook C, and modal-specific binary codes B v , CCQ is not as efficient as CVH. However, CCQ performs much more efficiently in time than CorrAE, which is a deep learning based method solving a time-demanding non-convex nonlinear optimization problem.</p><p>The training memory consumptions are compared in Figure <ref type="figure" target="#fig_14">6</ref>(b). Both batch and mini-batch (large-scale) implementations of CCQ store the model parameters in memory, which are independent of training dataset size. For the batch implementation, all training data is loaded in memory, while for the mini-batch implementation, the training data is partitioned into multiple mini-batches while only one mini-batch is loaded in memory each time. Hence in the minibatch (large-scale) implementation, the memory cost stays constant when training dataset size increases. We can flexibly allocate memory to each mini-batch to trade off memory and disk reading costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Parameter Sensitivity</head><p>Towards unsupervised multimodal retrieval, CCQ is designed to involve only two parameters, dimension of modality-isomorphic subspace D and modality trade-off weight λ, and the performance is expected to be stable against parameter variations. Since we have fixed D = min({Pv} V v=1 , H), we only inspect the sensitivity of λ. We compute MAP @ 32 bits on both cross-modal retrieval tasks by varying λ between 0.1 and 200. The performance of CCQ w.r.t. parameter λ is shown in Figure <ref type="figure" target="#fig_14">6</ref>(c) and 6(d). We see that CCQ can consistently outperform all the unsupervised baseline methods by a large margin with λ varying between 1 and 200. This validates that CCQ is robust against parameter selection and is applicable to unsupervised multimodal retrieval with easily-configured parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have formally approached seamless multimodal hashing through a novel composite correlation quantization (CCQ). It integrates multimodal correlation and composite quantization into a seamless latent semantic analysis (LSA) framework, which yields compact binary codes that encode both intra-modal similarity and inter-modal correlation. The sharing of codebooks and binary codes across modalities enables joint learning of latent semantics that are maximally correlated in the isomorphic feature space, which serves as the key contributor to the efficacy of the proposed CCQ method.</p><p>In the future, we plan to equip our model with a deep learning architecture which can learn highly abstract nonlinear representations to better distill the correlation structures across multiple modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flowcharts of prior work (left) and CCQ (right). Prior work is a two-step pipeline: first map image-text pairs to isomorphic latent space (denoted as polygon) and then binarize the continuous representation to hash codes (denoted as vertices of hypercube) by sign thresholding. CCQ is a seamless optimization framework: jointly map both paired/unpaired images and texts to isomorphic latent space (denoted as polygon) and learn hash codes by composite quantization. The quantization model learns isomorphic codebook (denoted as Voronoi digram) and binary codes (denoted as histograms) by minimizing the quantization error, which suffices to assign each latent representation to M -nearest codewords (denoted as Voronoi cells) and assignment indices are used as hash codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4</head><label></label><figDesc>Update C by Quadratic Optimization as Eqn.<ref type="bibr" target="#b6">(6)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 for n ← 1</head><label>51</label><figDesc>to Nv do 6 Update {b v n } by ICM or greedy algorithm as Eqn. (7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>This is an unconstrained quadratic problem with analytic solution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>T → I @ 32 bits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision-recall curves (top) and precision@R curves (bottom) on NUS-WIDE cross-modal search tasks @ 16 and 32 bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a), and for CCQ, we use R vT x v n for continuous features and Cb v n for T → I @ 32 bits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision-recall curves (top) and precision@R curves (bottom) on Wiki cross-modal search tasks @ 16 and 32 bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision-recall curves (top) and precision@R curves (bottom) on Flickr1M cross-modal search tasks @ 16 and 32 bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effectiveness and efficiency experiments: (1) Loss of search quality in MAP (by red bars) due to conversion from continuous features to binary codes on Wiki. (b)-(c) the MAP of CCQ w.r.t. different numbers of paired data points (the number of unpaired data points is fixed to 10, 000). Solid lines indicate training with both paired and unpaired data, and dashed lines indicate training with only paired data. (d) Average search time (ms) for each query via lookup tables on Wiki, NUS-WIDE, Flickr25K, and Flickr1M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Efficiency verification experiments: (a)-(b) Training time and memory costs of different methods on the complete Flickr1M dataset. CCQ with batch (mini-batch) training scales linearly (constantly) to the sample size. (c)-(d) The MAP of CCQ @ 32 bits versus parameter λ ∈ [0.1, 200] for cross-modal retrieval tasks I → T and T → I on the NUS-WIDE, Wiki, and Flickr1M datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>by identity, C randomly, {B v } by NN search.Update {R v } by Orthogonal Procrustes as Eqn.<ref type="bibr" target="#b5">(5)</ref>.</figDesc><table><row><cell>2 repeat</cell></row><row><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The Statistics of Three Datasets the pairs that belong to one of the 21 most frequent concepts. The images are represented by 500-dimensional bag-ofwords vectors extracted from the SIFT features using k-means, and the texts are represented by 1,000-dimensional vectors extracted from the tag occurrence features using PCA. A query set of 2,000 image-text pairs are randomly sampled from the dataset, while the remaining 193,834 image-text pairs are serving as the database.</figDesc><table><row><cell>Dataset</cell><cell>NUS-WIDE</cell><cell>Wiki</cell><cell>Flickr1M</cell></row><row><cell>Complete Set</cell><cell>195,834</cell><cell>2,866</cell><cell>1,000,000</cell></row><row><cell>Labeled Set</cell><cell>195,834</cell><cell>2,866</cell><cell>25,000</cell></row><row><cell>Query Set</cell><cell>2,000</cell><cell>693</cell><cell>1,000</cell></row><row><cell>Database</cell><cell>193,834</cell><cell>2,173</cell><cell>24,000</cell></row><row><cell>Training Set</cell><cell>10,000</cell><cell>2,173</cell><cell>975,000</cell></row><row><cell>pairs by keeping</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>1 http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mean Average Precision (MAP) Comparison of Six Multimodal Retrieval Tasks on Three Standard Datasets</figDesc><table><row><cell>Task</cell><cell>Method</cell><cell>8 bits</cell><cell cols="2">NUS-WIDE 16 bits 32 bits</cell><cell>64 bits</cell><cell>8 bits</cell><cell cols="2">Wiki 16 bits 32 bits</cell><cell>64 bits</cell><cell>8 bits</cell><cell>Flickr1M 16 bits 32 bits</cell><cell>64 bits</cell></row><row><cell></cell><cell>CVH [13]</cell><cell cols="4">0.3954 0.4542 0.4759 0.4780</cell><cell cols="4">0.1988 0.1969 0.2042 0.2058</cell><cell cols="2">0.6050 0.6328 0.6615 0.6712</cell></row><row><cell></cell><cell>IMH [20]</cell><cell cols="4">0.4313 0.4545 0.4155 0.4005</cell><cell cols="4">0.1910 0.1963 0.1937 0.1935</cell><cell cols="2">0.5239 0.5725 0.5736 0.5748</cell></row><row><cell></cell><cell>CorrAE [8]</cell><cell cols="4">0.4223 0.4478 0.4587 0.4796</cell><cell cols="4">0.2055 0.2086 0.2188 0.2194</cell><cell cols="2">0.6145 0.6397 0.6588 0.6654</cell></row><row><cell>I → I</cell><cell>CMSSH [5]</cell><cell cols="4">0.3776 0.4060 0.4356 0.4490</cell><cell cols="4">0.1987 0.1979 0.2007 0.2126</cell><cell cols="2">0.5738 0.6304 0.6587 0.6932</cell></row><row><cell></cell><cell>SCM [28]</cell><cell cols="4">0.4258 0.4578 0.4695 0.4831</cell><cell cols="4">0.2048 0.2103 0.2177 0.2212</cell><cell cols="2">0.5926 0.6257 0.6615 0.6801</cell></row><row><cell></cell><cell>QCH [26]</cell><cell cols="4">0.4289 0.4557 0.4786 0.4898</cell><cell cols="4">0.2087 0.2155 0.2198 0.2252</cell><cell cols="2">0.6165 0.6586 0.6787 0.6885</cell></row><row><cell></cell><cell>CCQ (ours)</cell><cell cols="4">0.4711 0.4859 0.4921 0.4932</cell><cell cols="4">0.2226 0.2265 0.2373 0.2386</cell><cell cols="2">0.6714 0.7092 0.7318 0.7451</cell></row><row><cell></cell><cell>CVH [13]</cell><cell cols="3">0.5825 0.6485 0.6837</cell><cell>0.7189</cell><cell cols="4">0.4049 0.5506 0.6075 0.6239</cell><cell cols="2">0.5812 0.6085 0.6242 0.6337</cell></row><row><cell></cell><cell>IMH [20]</cell><cell cols="4">0.4531 0.4740 0.5421 0.6202</cell><cell cols="4">0.3805 0.4623 0.5773 0.5989</cell><cell cols="2">0.5585 0.5973 0.6360 0.6436</cell></row><row><cell></cell><cell>CorrAE [8]</cell><cell cols="4">0.5501 0.5856 0.6344 0.6678</cell><cell cols="4">0.5765 0.5889 0.6045 0.6123</cell><cell cols="2">0.6060 0.6176 0.6389 0.6443</cell></row><row><cell>T → T</cell><cell>CMSSH [5]</cell><cell>0.5911</cell><cell cols="3">0.5968 0.6215 0.6613</cell><cell cols="4">0.5503 0.6065 0.6188 0.6232</cell><cell cols="2">0.5487 0.5573 0.5583 0.5614</cell></row><row><cell></cell><cell>SCM [28]</cell><cell cols="4">0.5524 0.6315 0.6606 0.6736</cell><cell cols="4">0.5814 0.6051 0.6189 0.6324</cell><cell cols="2">0.5924 0.6320 0.6410 0.6485</cell></row><row><cell></cell><cell>QCH [26]</cell><cell>0.5706</cell><cell>0.6586</cell><cell cols="2">0.6796 0.6855</cell><cell cols="4">0.6002 0.6128 0.6226 0.6355</cell><cell cols="2">0.6022 0.6427</cell><cell>0.6554 0.6686</cell></row><row><cell></cell><cell>CCQ (ours)</cell><cell>0.5913</cell><cell>0.6481</cell><cell>0.6917</cell><cell>0.7069</cell><cell cols="4">0.6017 0.6286 0.6366 0.6422</cell><cell cols="2">0.6090 0.6433</cell><cell>0.6541 0.6550</cell></row><row><cell></cell><cell>CVH [13]</cell><cell cols="4">0.4588 0.4713 0.4743 0.4740</cell><cell cols="4">0.1673 0.1877 0.1716 0.1696</cell><cell cols="2">0.6091 0.6225 0.6364 0.6199</cell></row><row><cell></cell><cell>IMH [20]</cell><cell cols="4">0.4345 0.4399 0.4203 0.4115</cell><cell cols="4">0.1734 0.1896 0.1714 0.1601</cell><cell cols="2">0.5449 0.5646 0.5936 0.5539</cell></row><row><cell></cell><cell>CorrAE [8]</cell><cell cols="4">0.4398 0.4522 0.4699 0.4964</cell><cell cols="4">0.1929 0.1982 0.2033 0.2155</cell><cell cols="2">0.6301 0.6329 0.6357 0.6401</cell></row><row><cell>I → T</cell><cell>CMSSH [5]</cell><cell cols="4">0.3950 0.4052 0.4076 0.3516</cell><cell cols="4">0.1672 0.1727 0.1750 0.1759</cell><cell cols="2">0.5076 0.5272 0.5357 0.5219</cell></row><row><cell></cell><cell>SCM [28]</cell><cell cols="4">0.4693 0.4648 0.4619 0.4851</cell><cell cols="4">0.2258 0.2372 0.2381 0.2378</cell><cell cols="2">0.6361 0.6493 0.6495 0.6440</cell></row><row><cell></cell><cell>QCH [26]</cell><cell cols="4">0.4765 0.4895 0.5050 0.5125</cell><cell cols="3">0.2288 0.2343 0.2368</cell><cell>0.2402</cell><cell cols="2">0.6452 0.6523 0.6685 0.6721</cell></row><row><cell></cell><cell>CCQ (ours)</cell><cell cols="4">0.5124 0.5161 0.5165 0.5372</cell><cell cols="3">0.2338 0.2349 0.2371</cell><cell>0.2374</cell><cell cols="2">0.6879 0.7081 0.7183 0.7176</cell></row><row><cell>I → IT</cell><cell>CCQ (ours)</cell><cell cols="4">0.5074 0.5411 0.5414 0.5441</cell><cell cols="4">0.2512 0.2513 0.2529 0.2587</cell><cell cols="2">0.7063 0.6894 0.6989 0.6996</cell></row><row><cell></cell><cell>CVH [13]</cell><cell>0.5598</cell><cell cols="3">0.5217 0.5129 0.4875</cell><cell cols="4">0.2309 0.2219 0.2214 0.2350</cell><cell cols="2">0.5972 0.6032 0.5738 0.5794</cell></row><row><cell></cell><cell>IMH [20]</cell><cell cols="4">0.4380 0.4582 0.4186 0.4051</cell><cell cols="4">0.2394 0.2227 0.2333 0.1896</cell><cell cols="2">0.5374 0.5536 0.5513 0.5583</cell></row><row><cell></cell><cell>CorrAE [8]</cell><cell cols="4">0.4303 0.4501 0.4634 0.4880</cell><cell cols="4">0.2688 0.2928 0.3478 0.3566</cell><cell cols="2">0.6142 0.6198 0.6247 0.6431</cell></row><row><cell>T → I</cell><cell>CMSSH [5]</cell><cell cols="4">0.3783 0.3499 0.3944 0.4015</cell><cell cols="4">0.2926 0.2991 0.2537 0.2582</cell><cell cols="2">0.5868 0.5732 0.6176 0.6323</cell></row><row><cell></cell><cell>SCM [28]</cell><cell cols="4">0.4449 0.4859 0.5105 0.5259</cell><cell cols="4">0.3157 0.3698 0.4239 0.4369</cell><cell cols="2">0.6037 0.5998 0.5805 0.6078</cell></row><row><cell></cell><cell>QCH [26]</cell><cell cols="2">0.5020 0.5195</cell><cell cols="2">0.5489 0.5622</cell><cell cols="2">0.3426 0.3753</cell><cell cols="2">0.4411 0.4565</cell><cell cols="2">0.6258 0.6425 0.6485 0.6528</cell></row><row><cell></cell><cell>CCQ (ours)</cell><cell>0.5359</cell><cell cols="2">0.5410 0.5413</cell><cell>0.5556</cell><cell cols="2">0.3885 0.4000</cell><cell cols="2">0.4222 0.4178</cell><cell cols="2">0.6548 0.7026 0.7165 0.7266</cell></row><row><cell>T → IT</cell><cell>CCQ (ours)</cell><cell cols="4">0.6022 0.6925 0.7131 0.7153</cell><cell cols="4">0.6355 0.6351 0.6394 0.6405</cell><cell cols="2">0.6942 0.7151 0.7190 0.7416</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://www.svcl.ucsd.edu/projects/crossmodal</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://www.cs.toronto.edu/~nitish/multimodal</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://staff.itee.uq.edu.au/shenht/UQ_IMH</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/fangxiangfeng/deepnet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://www.cse.ust.hk/~dyyeung/code/mlbe.zip</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>The authors would like to thank Dr Jingdong Wang for insightful comments. This work was supported by National Natural Science Foundation of China (61325008, 61502265), China Postdoctoral Science Foundation (2015T80088), National Science&amp;Technology Supporting Program (2015BAH14F02), NSF grant III-1526499, and Tsinghua TNList Lab Fund for Big Data Science and Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The inverted multi-index</title>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3069" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Additive quantization for extreme vector compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the statistical analysis of dirty pictures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="320" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data fusion through cross-modality metric learning using similarity-sensitive hashing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nus-wide: A real-world web image database from national university of singapore</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval with correspondence autoencoder</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative multi-view hashing for cross media indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The mir flickr retrieval evaluation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011-01">Jan 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning hash functions for cross-view similarity search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantics-preserving hashing for cross-view retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A low rank structural large margin method for cross-modal ranking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cartesian k-means</title>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the role of correlation and abstraction in cross-modal multimedia retrieval</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A generalized solution of the orthogonal procrustes problem</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Schönemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inter-media hashing for large-scale retrieval from heterogeneous data sources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2949" to="2980" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hashing for similarity search: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to hash on partial multi-modal data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3904" to="3910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effective multi-modal retrieval based on stacked auto-encoders</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Quantized correlation hashing for fast cross-modal search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative coupled dictionary hashing for fast cross-media retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-scale supervised multimodal hashing with semantic correlation maximization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Composite hashing with multiple information sources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Composite quantization for approximate nearest neighbor search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Co-regularized hashing for multimodal data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A probabilistic model for multimodal hash function learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linear cross-modal hashing for efficient multimedia search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
