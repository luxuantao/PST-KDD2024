<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AEEB6BB7CF9AC665C47526DC8E027467</idno>
					<idno type="DOI">10.1109/LSP.2014.2324759</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Autoencoder-based Unsupervised Domain Adaptation for Speech Emotion Recognition</head><p>Jun Deng, Student Member, IEEE, Zixing Zhang, Florian Eyben, Member, IEEE, and Bj√∂rn Schuller, Member, IEEE Abstract-With the availability of speech data obtained from different devices and varied acquisition conditions, we are often faced with scenarios, where the intrinsic discrepancy between the training and the test data has an adverse impact on affective speech analysis. To address this issue, this letter introduces an Adaptive Denoising Autoencoder based on an unsupervised domain adaptation method, where prior knowledge learned from a target set is used to regularize the training on a source set. Our goal is to achieve a matched feature space representation for the target and source sets while ensuring target domain knowledge transfer. The method has been successfully evaluated on the 2009 INTERSPEECH Emotion Challenge's FAU Aibo Emotion Corpus as target corpus and two other publicly available speech emotion corpora as sources. The experimental results show that our method significantly improves over the baseline performance and outperforms related feature domain adaptation methods.</p><p>Index Terms-Adaptive denoising autoencoders, domain adaptation, speech emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PEECH emotion recognition aims to automatically predict 'correct' emotional states from acoustic (and/or linguistic) parameters as features using machine learning methods. Many speech emotion recognition engines achieve promising performance only under one common assumption, namely that the training and test data instances are drawn from the same corpus and the same feature space for parametrization is used. However, with speech data obtained from different devices and varied recording conditions, we are often faced with scenarios where such data are typically highly dissimilar in terms of acoustic signal conditions, linguistic content, type of emotion (e. g., acted, elicited, or naturalistic), or the type of labeling scheme used, such as categorical or dimensional labels.</p><p>Automatic speech recognition (ASR) is faced with many similar mismatch problems, and the speech community has done a considerable amount of the related work to alleviate this mismatch problem. One major research direction focuses on leveraging auto-associative neural networks to minimize the mismatch problem <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>One other prominent approach to overcome this 'corpus bias' issue is domain adaptation used when the source domain data has a different distribution than the target domain data, but the task remains the same. In general, domain adaptation techniques are categorized into two classes depending on whether the target domain test data is either partially labeled (semi-supervised) or completely unlabeled (unsupervised). In semi-supervised domain adaptation, correspondences of labeled target data are often used to learn domain transformations <ref type="bibr" target="#b2">[3]</ref>. However, unsupervised domain adaptation uses strategies which assume a known class of transformations between the domains, the availability of discriminative features which are common to or invariant across both domains, a latent space where the difference in distribution of source and target data is minimal <ref type="bibr" target="#b3">[4]</ref>, and a mapping 'path' by which the domain transformation maps the source data onto the target domain <ref type="bibr" target="#b4">[5]</ref>. Another popular approach for unsupervised domain adaptation is known as importance weighting. This method has recently been shown to lead to significant improvements in acoustic emotion recognition by Hassan et al. : they considered to explicitly compensate for acoustic and speaker differences by employing three transfer learning algorithms <ref type="bibr" target="#b5">[6]</ref> (i. e., Kernel Mean Matching (KMM) <ref type="bibr" target="#b6">[7]</ref>, Unconstrained Least-Squares Importance Fitting (uLSIF) <ref type="bibr" target="#b7">[8]</ref>, and Kullback-Leibler Importance Estimation Procedure (KLIEP) <ref type="bibr" target="#b8">[9]</ref>).</p><p>Our work is partially inspired by <ref type="bibr" target="#b9">[10]</ref>, in which Support Vector Machines (SVMs) are used to learn from the source model by regularizing the distance between the learned model and . Extending this idea to an unsupervised scenario, we propose a novel three-stage data-driven approach in this letter. It is based on adaptive denoising autoencoders which can learn from a source training set with the guidance of a template learned previously from target domain adaptation data, which yields a common representation across training and test domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Denoising Autoencoders</head><p>A denoising autoencoder (DAE)-a more recent variant of the basic autoencoder consisting of only one hidden layer-is trained to reconstruct a clean 'repaired' input from a corrupted version <ref type="bibr" target="#b10">[11]</ref>. In doing so, the learner must capture the structure of the input distribution in order to reduce the effect of the corruption process <ref type="bibr" target="#b11">[12]</ref>. It turns out that in this way more robust features are learned compared to a basic autoencoder. The architecture of a DAE is given in Fig. <ref type="figure" target="#fig_0">1</ref>. Formally, an input example is first converted to a corrupted version by means of a corrupting function , which could be masking corruption (deleting random elements of the input), additive Gaussian noise, or salt-andpepper noise in images.</p><p>Then, in response to the corrupted example , the hidden representation is</p><p>where is a non-linear activation function, typically a logistic sigmoid function applied component-wise, is a weight matrix, and is a bias vector. It is easily found that the topology structure of the autoencoder completely relies on the size of the input layer and the number of hidden units .</p><p>The network output maps the hidden representation back to a reconstruction :</p><p>(</p><p>where is a weight matrix, and is a bias vector.</p><p>Given a set of input examples , the DAE training consists of finding parameters which minimize the reconstruction error. This corresponds to minimizing the following objective function:</p><p>(3) Here, we also include a weight-decay regularization term with its hyper-parameter to the objective function to avoid overfitting.</p><p>is the -th column vector of the -th layer weight matrix . The minimization is usually realized either by stochastic gradient descent or more advanced optimization techniques such as L-BFGS <ref type="bibr" target="#b12">[13]</ref> or conjugate gradient method <ref type="bibr" target="#b13">[14]</ref>. In addition, a DAE has an overall asymptotic computational complexity of with respect to the network size.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref>. Overview of the speech emotion recognition system integrating the proposed domain adaptation method. The function "Encoder" refers to the feed-forward procedure (i. e., Eq. ( <ref type="formula" target="#formula_0">1</ref>)) from input data to the activations of the hidden layer of a pre-trained DAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning with Target Prior via Adaptive DAEs</head><p>In this letter, we extend the idea for unsupervised domain adaptation by using both a DAE and an adaptive DAE. The aim is to capture source domain knowledge in training an adaptive DAE with the guidance of the prior knowledge previously learned from target domain data by a DAE. where is the angle between the two column vectors and . On one hand, apart from minimizing the original term , the optimization problem aims to use the term to make the transfer by maximizing , which is equivalent to minimizing the angle between the and . On the other hand, the term in the objective function also causes to adjust to the training data and prevents being close to . Thus, an adaptive DAE training consists of optimizing a trade-off between the reconstruction error on the training data and target domain knowledge transfer.</p><p>Finally, we encode test data and training data via Eq. ( <ref type="formula" target="#formula_0">1</ref>) using the weights ( and ) learned by the adaptive DAE. Then, this transformed representation of the training data is used to train a standard supervised classifier (e. g., SVM) for speech emotion recognition as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, while the transformed test data is used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Selected Data and Acoustic Features</head><p>To investigate the performance of the proposed method, we consider the INTERSPEECH 2009 Emotion Challenge two-class task <ref type="bibr" target="#b14">[15]</ref>. It is based on the spontaneous FAU Aibo Emotion Corpus (FAU AEC), which contains recordings of 51 children at the age of 10-13 years interacting with the pet robot Aibo in German speech. The details of the challenge's two-class 'negative' versus 'idle' emotions task are given in Table <ref type="table">I</ref>.</p><p>In our experiments, two further publicly available and popular databases, namely the Airplane Behavior Corpus (ABC) <ref type="bibr" target="#b15">[16]</ref>, and the Speech Under Simulated and Actual Stress (SUSAS) database <ref type="bibr" target="#b16">[17]</ref> are chosen as training sets, which are highly different from the FAU AEC in terms of speaker age (adults vs. children in FAU AEC), spoken language (English vs. German in FAU AEC), type of emotion (partially acted vs. naturalistic emotion in FAU AEC), degree of spontaneity and phrase length, type of recording situation, and annotators and subjects. For comparability with FAU AEC, we have to map the diverse emotion classes onto the valence axis of the dimensional emotion model in order to generate a unified set of labels. Binary valence labels according to the mapping shown in Table <ref type="table" target="#tab_2">II</ref>   For acoustic features, we use a standard set in the field, namely the INTERSPEECH 2009 Emotion Challenge <ref type="bibr" target="#b14">[15]</ref> baseline feature set. It consists of 12 functionals applied to acoustic Low-Level Descriptors (LLDs) including their first order delta regression coefficients. Thus, the size of the feature vector for each utterance is . To ensure reproducibility, the open-source toolkit openSMILE<ref type="foot" target="#foot_0">1</ref> version 2.0 was used with the pre-defined challenge configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>As classifier, we used linear SVMs-as were used in the official baseline of the challenge <ref type="bibr" target="#b14">[15]</ref>-with a fixed penalty factor of . For the training of the autoencoders the toolkit minFunc<ref type="foot" target="#foot_1">2</ref> was applied which implements L-BFGS to optimize the parameters of DAEs and A-DAEs. For training of the DAE, we injected masking noise with a variance of 0.01 to generate a corrupted input. For the parameters of the DAE, the weight decay values were set to 0.0001, the number of epochs of DAE training was set to 250. In the A-DAE learning process, the hyper-parameter was fixed to 0.05. We evaluate the performance of the baseline systems and the A-DAE systems using the evaluation measure of the INTER-SPEECH 2009 Emotion Challenge: unweighted average recall (UAR). It is the unweighted average of the per-class recall rates and better reflects overall accuracy in the given case of class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison to State-of-the-Art Methods</head><p>We compare the following methods to evaluate our proposed approach in the context of the current state-of-the-art: (3) KLIEP <ref type="bibr" target="#b7">[8]</ref>, (4) uLSIF <ref type="bibr" target="#b8">[9]</ref>, and ( <ref type="formula">5</ref>) KMM <ref type="bibr" target="#b6">[7]</ref>: utilize the modern domain adaption methods on the ABC and SUSAS database for covariate shift adaptation, respectively. We choose the 'tuning parameters' following <ref type="bibr" target="#b5">[6]</ref>. ( <ref type="formula">6</ref>) DAE: employs denoising autoencoders for representation learning in order to match training examples to test examples; this was successfully applied to the transfer learning challenge and domain adaptation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>. We study the cross-corpus setting with the number of hidden units fixed to 256 where we train acoustic emotion recognition models on ABC or SUSAS while evaluating on the FAU AEC test set (except the MINT condition that uses FAU AEC data for training). We report results of the averaged UAR over the ten trials in Table <ref type="table" target="#tab_2">III</ref>. As can be seen, our approach always shows a comparable performance to other approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>.</p><p>For the small database ABC, the two standard methods (CT and MINT) only yield an average UAR around chance level (55.28% and 58.32%). With the benefits of compensation for the existent mismatch, the covariate shift adaptation KMM can achieve the accuracy of 62.52%. The proposed A-DAE method outperforms all other methods with 64.18% UAR. This improvement has a statistical significance at with a one-sided -test when compared to CT and MINT.</p><p>On the SUSAS database, our proposed method shows a significant improvement over other methods. Specifically, the A-DAE method gives an average UAR of 62.74%, which is slightly higher than the maximum average UAR obtained by MINT. Moreover, it passes the significance test at and against the CT and KMM methods, respectively. In the mean time, it is worth noting that the average UAR obtained by MINT increases dramatically to 62.42% just due to the larger size of SUSAS leading to more instances being chosen from the FAU AEC training set in comparison to ABC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. A-DAE vs. DAE</head><p>We now compare the A-DAE and DAE methods in detail. In Fig. <ref type="figure" target="#fig_3">3</ref>, we provide UAR for different numbers of hidden units , where we observe performance changes for different parameter settings. Based on Fig. <ref type="figure" target="#fig_3">3</ref>, it is worth noting that the proposed method obtains the highest UAR of 64.67% for ABC and of 63.02% for SUSAS at and , respectively. Surprisingly, we could not obtain a sustained performance growth with more hidden units for SUSAS. One reason is that the utterances of ABC are more complex and have more variance (length and content) than those of SUSAS which contain pre-defined short commands. Therefore, the increase in hidden units potentially yields to more generalization performance for ABC than for SUSAS. In contrast, increasing the number of hidden units to in the case of SUSAS reduces the corresponding performance because overfitting occurs. Nevertheless, increasing the number of hidden units leads to additional improvement indeed, which confirms that an over-complete first hidden layer works better than an under-complete one when using unsupervised pre-training as in the theory of deep architectures <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSIONS</head><p>In this letter, we proposed a novel unsupervised domain adaptation method based on adaptive denoising autoencoders for affective speech signal analysis. The method is capable of reducing the discrepancy between training and test sets due to different conditions (e. g., different corpora). We first built a denoising autoencoder on the target domain adaptation set without using any label information with the aim to encode the target data in an optimal way. These encoding parameters are used as prior information to regularize the training process of an A-DAE on the training set. In this way, a trade-off between the reconstruction error on the training data and a knowledge transfer to the target domain is found, effectively reducing the existing mismatch between the training and testing conditions in an unsupervised way. Results with three publicly available corpora show that the proposed method effectively and significantly enhances the emotion classification accuracy in mismatched training and test conditions when compared to other domain adaptation methods. In future work, we plan to use the dropout strategy <ref type="bibr" target="#b19">[20]</ref> to further improve the generalization performance of autoencoder-based domain adaptation and extend A-DAEs to deep architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A denoising autoencoder (DAE) architecture. An input is corrupted (via ) to . The black crosses (" ") illustrate a corrupted version of the input made by .</figDesc><graphic coords="2,67.98,64.14,190.98,150.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>depicts the affective speech signal analysis method with the proposed domain adaptation method integrated. The proposed method is composed of the following three stages: First, a DAE is learned in a fully unsupervised way from the target domain adaptation data, resulting in the weight matrices (input to hidden layer) and (hidden to output layer) from Eq. (3) as well as the bias vectors and . Next, we propose a new variant of DAEs for domain adaptation, called Adaptive DAE (A-DAE), which force their weights to adapt to the provided weights as well as minimize the reconstruction error between the input and the output at the same time. The output bias vectors of the DAE are not adapted. Hence, given a training example and the weights and of a DAE, which were estimated without supervision from the target domain adaptation data (i. e., without knowledge of target labels), the objective function of an A-DAE is formulated as follows: (4) where the hyper-parameter controls the amount of transfer regularization. The weights and are initialized randomly and learned during training, while the weights and are kept constant during training. Without loss of generality, the intuition of the adaptive DAE for domain adaptation can be understood by expanding the weight-decay regularization term: (5) TABLE I SUMMARY OF THE THREE CHOSEN AFFECTIVE SPEECH DATABASES Number of instances per binary valence (# Valence, Negative (-), Positive (+)), and overall number (# All)-for FAU AEC divided into official training and test set by "/". Number of female (#f) and male (#m) subjects. Recording conditions (studio/normal/noisy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 1 )</head><label>1</label><figDesc>Matched Instance Number Training (MINT): randomly (repeated ten times) picks a number of instances from the FAU AEC training set to train an SVM, i. e., without the need of transferring to an intra-corpus scenario. For fair comparison, this number is set by the number of training instances of the TABLE III AVERAGE UAR OVER TEN TRIALS: MATCHED INSTANCE NUMBER TRAINING (MINT), CROSS TRAINING (CT), COVARIATE SHIFT ADAPTATION METHODS KLIEP, ULSIF, AND KMM, DAE-BASED REPRESENTATION LEARNING, AND THE PROPOSED A-DAE METHOD RELATED TO TRAINING WITH ABC AND SUSAS the ABC or SUSAS sets, respectively. (2) Cross Training (CT): uses ABC or SUSAS to train the standard (SVM) classifier, which is the 'classical' cross-corpus testing, i. e., it involves no adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Average UAR with standard deviation over ten trials with varying number of hidden units ( ) using DAE or A-DAE.</figDesc><graphic coords="4,307.02,161.16,240.12,136.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>are thus generated. TableIsummarizes the properties and statistics of the three databases (FAU AEC, ABC, and SUSAS).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II MAPPING</head><label>II</label><figDesc>OF EMOTION CATEGORIES ONTO NEGATIVE AND POSITIVE VALENCE LABELS FOR THE THREE DATABASES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://sourceforge.net/projects/opensmile/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.di.ens.fr/ mschmidt/Software/minFunc.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Regularized auto-associative neural networks for speaker verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Garimella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="841" to="844" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speaker verification: Minimizing the channel effects using autoassociative neural network models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yegnanarayana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, Istanbul, Turkey, 2000</title>
		<meeting>ICASSP, Istanbul, Turkey, 2000</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1101" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse autoencoder-based feature transfer learning for speech emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACII</title>
		<meeting>ACII<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="511" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Bellevue, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation across domain shifts by generating intermediate data representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On acoustic emotion recognition: Compensating for covariate shift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Damper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1458" to="1468" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Covariate shift by kernel mean matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmittfull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dataset Shift Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="131" to="160" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient direct density ratio estimation for non-stationarity adaptation and outlier detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selectionand its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>B√ºnau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1433" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tabula rasa: Model transfer for object category detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2252" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the limited memory bfgs method for large scale optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Progr</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Methods of conjugate gradients for solving linear systems1</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Hestenes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stiefel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Res. Nat. Bur. Stand</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The INTERSPEECH 2009Emotion Challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Brighton, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="312" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Audiovisual behavior modeling by combined feature spaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arsic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Radig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Getting started with SUSAS: A speech under simulated and actual stress database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bou-Ghazale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech<address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1743" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Bellevue, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
