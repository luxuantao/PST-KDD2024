<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Least Squares Identification Methods for Multivariable Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feng</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tongwen</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Hierarchical Least Squares Identification Methods for Multivariable Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4893240166BC5E097E6A434CD039198A</idno>
					<idno type="DOI">10.1109/TAC.2005.843856</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convergence properties</term>
					<term>estimation</term>
					<term>hierarchical identification principle</term>
					<term>least squares</term>
					<term>multivariable systems</term>
					<term>recursive identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For multivariable discrete-time systems described by transfer matrices, we develop a hierarchical least squares iterative (HLSI) algorithm and a hierarchical least squares (HLS) algorithm based on a hierarchical identification principle. We show that the parameter estimation error given by the HLSI algorithm converges to zero for the deterministic cases, and that the parameter estimates by the HLS algorithm consistently converge to the true parameters for the stochastic cases. The algorithms proposed have significant computational advantage over existing identification algorithms. Finally, we test the proposed algorithms on an example and show their effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>r3 = 0:1000r3 + 0:006 31r4 + 0:6580 3 10 06 r3r8 + 0:7230 3 10 04 r 4 r 8 r 4 = 0:9083 3 10 04 r 3 + 0:1000r 4 + 0:7230 3 10 04 r 3 r 8 + 0:6580 3 10 07 r4r8 r 5 = 0:0500r 5 + 0:0520r 6 + 0:3960 3 10 05 r 5 r 8 + 0:3980 3 10 04 r6r8 r 6 = 0:2500 3 10 04 r 5 + 0:0500r 6 + 0:3980 3 10 04 r 5 r 8 + 0:1990 3 10 07 r 6 r 8 r7 = 0:5880r7 + 0:0018r7r8 r 8 = 1:3530 + 0:0011r 1 + 0:0710r 2 + 2:0900 3 10 04 r 3 + 0:3380r 4 + 20:643 3 10 04 r 5 + 11:582r 6 + 12:784r 7 + 0:0013r1r8 + 0:4000 3 10 04 r2r8 + 1:6650 3 10 04 r 3 r 8 + 0:1510 3 10 06 r 4 r 8 + 0:0033r5r8 + 0:1640 3 10 05 r6r8 + 0:0297r7r8: (42)   The solution to (42) has been obtained by the simple iteration method.</p><p>All radii are positive and r8 = 1:3827. Therefore, by Theorem 1 the outer bound I I I sought is I I I = [I 0 ; I + ] = 0 1 + [0r8; r8] = [013:938; 011:173]: (43) It can be checked that the perturbations of the real part of 2 (A), A 2 A A A remain to the left of I + . Thus, in view of (43), we conclude that the continuous dynamic system (1), whose interval matrix A A A is defined by (40), is stable. From (43), the stability margin M 1 obtained by Method M1 is thus M 1 = 11:173. </p><p>So matrix A 0 can be formulated using conditions (29). The first com- ponent 0 1 of 0 is 0 1 = 011:968. By Theorem 2, the right end-point of the range of 1(A), A 2 A A A is (I 3 ) + = 011:968. As expected, (I 3 ) + &lt; I + and the true stability margin is M 2 = 0(I 3 ) + = 11:968 &gt; M 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Two methods for stability analysis of linear time-invariant interval dynamic systems (1) have been suggested. They are based on computing estimates for the ranges ( <ref type="formula">4</ref>) and ( <ref type="formula">7</ref>) associated with the real parts of the eigenvalues of the system interval matrix A A A. The first Method M1 is an approximate method since it only provides outer bounds on the respective ranges. The second Method M2 is an exact method. If the monotonicity conditions (29) are satisfied, Method M2 yields the right end-point of the ranges. An improved version (method M3) has also been suggested which is capable of solving the problem even if not all monotonicity conditions are fulfilled. The applicability of the methods suggested has been illustrated by an eight-dimensional numerical example.</p><p>The present methods can be extended to encompass systems in which the elements a ij of matrix A depend on a certain number of interval parameters p p p i , i = 1; . . . ; q. Such a generalization will be reported in a future publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Many identification algorithms based on transfer matrices for multiple-input-multiple-output (MIMO) systems adopted the idea of decomposing into subsystems and then estimating parameters of the subsystems one by one <ref type="bibr">[4]</ref>, <ref type="bibr">[3]</ref>, <ref type="bibr" target="#b14">[8]</ref>, <ref type="bibr">[2]</ref>; since such algorithms require computing many covariance matrices, they are computationally intensive. Several methods were proposed to reduce the computational burden, e.g., in <ref type="bibr" target="#b15">[9]</ref>, it was suggested to use a matrix pseudoinverse approach; however, the computational intensity was still severe due to a large number of zero entries in the information matrix.</p><p>In this note, we focus on developing computationally efficient parameter estimation algorithms for transfer matrix models with least common denominators (characteristic polynomials). The key idea is the so-called hierarchical identification, and is inspired by the hierarchical control based on the decomposition-coordination principle for large-scale systems; see, e.g., <ref type="bibr" target="#b16">[10]</ref>. Hierarchical identification uses subsystem decomposition in identification, and is also called bootstrap identification. For MIMO systems, we will present two algorithms, the hierarchical least squares iterative (HLSI) one and the hierarchical least squares (HLS) one; both have computational advantages over existing methods mentioned above. The proposed algorithms can be regarded as an extension of the prediction error method <ref type="bibr">[7]</ref>, <ref type="bibr">[6]</ref>, because the information vectors/matrices in our identification models use only measurable input-output data.</p><p>The note is organized as follows. In Section II, we briefly discuss modeling issues related to MIMO systems. In Section III, we develop the HLSI algorithm. In Section IV, we present the HLS algorithm in the stochastic framework, and analyze its performance using the martingale convergence theorem in Section V. In Section VI, we give an illustrative example. Finally, in Section VII, we offer some concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM FORMULATION</head><p>Consider a MIMO discrete-time system described by</p><formula xml:id="formula_1">y(t) = G(z)u(t) G(z) = Q(z) (z) 2 m2r (1)</formula><p>where u(t) 2 r is the input, y(t) 2 m the output, and G(z) 2 m2r the transfer matrix, with z 01 being the unit delay operator: z 01 y(t) = y(t 0 1); (z) is the characteristic polynomial in z 01 (of degree n), defined as the monic least common denominator of G(z), and Q(z) is the polynomial matrix in z 01 (z) =1 + 1 z 01 + 2 z 02 + 111 + n z 0n 2 1 Q(z) =Q0 + Q1z 01 + Q2z 02 + 1 11 + Qnz 0n 2 m2r Q i 2 m2r ; i= 0; 1; . . . ; n:</p><p>Equation ( <ref type="formula" target="#formula_20">1</ref>) can be rewritten as</p><formula xml:id="formula_2">y(t) + (t) = T '(t)<label>(2)</label></formula><p>where the parameter matrix , parameter vector , information vector '(t) and information matrix (t) are defined as</p><formula xml:id="formula_3">T = [Q0 Q1 111 Qn ] 2 m2n n0 := (n + 1)r = [ 1 2 1 11 n ] T 2 n ' T (t) = [u T (t) u T (t 0 1) 11 1 u T (t 0 n) ] 2 12n</formula><p>(t) = [y(t 0 1) y(t 0 2) 111 y(t 0 n) ] 2 m2n :</p><p>Equation ( <ref type="formula" target="#formula_2">2</ref>) is the identification model of the MIMO system in (1).</p><p>In developing estimation algorithms for (2), a difficulty arises in that both and are unknown-the standard least squares algorithm cannot be applied directly. This is the thrust for our work. By means of the hierarchical identification principle, we want to present new and computationally efficient identification algorithms to estimate and in <ref type="bibr">(2)</ref> simultaneously from the given input-output measurement data, and to study convergence performance in the stochastic framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HIERARCHICAL LEAST SQUARES ITERATIVE ALGORITHM</head><p>In this section, the MIMO system in ( <ref type="formula" target="#formula_2">2</ref>) is decomposed into two subsystems: one containing only and the other containing only; then the iterative solutions of and are established by applying [1,</p><p>Lemma 2] (the hierarchical identification principle). The details are as follows.</p><p>Defining two vectors b1(t) := 0y(t) + T '(t) b2(t) := y(t) + (t)</p><p>we can decompose the system in (2) into the following two fictitious subsystems:</p><formula xml:id="formula_4">S 1 : (t) = b 1 (t) S 2 : T '(t) = b 2 (t):</formula><p>Let t be sufficiently large (t mn 0 + n) and define 9 T (t (1) b 2 (2) 111 b 2 (t)] :</p><formula xml:id="formula_5">) := [ T (1) T (2) 111 T (t)] B T 1 (t) := [b T 1 (1) b T 1 (2) 1 11 b T 1 (t)] 8(t) := ['(1) '(2) 1 11 '(t) ] B 2 (t) := [b 2</formula><formula xml:id="formula_6">It follows that B 1 (t) = 0y(1) + T '(1) 0y(2) + T '<label>(2)</label></formula><p>. . .</p><formula xml:id="formula_7">0y(t) + T '(t)<label>(3)</label></formula><p>B 2 (t)=[y(1)+ (1) y( <ref type="formula" target="#formula_2">2</ref>)+ (2) 111 y(t) + (t)]: (4)</p><p>So, we have</p><formula xml:id="formula_8">S 1 : 9(t) = B 1 (t)<label>(5)</label></formula><p>S 2 : T 8(t) = B 2 (t); or 8 T (t) = B T 2 (t): (6)</p><p>Hence, [1, Lemma 2] can be applied to obtain iterative solutions k and k of and , respectively, for the linear equations in ( <ref type="formula" target="#formula_8">5</ref>) and ( <ref type="formula">6</ref>)</p><formula xml:id="formula_9">k = k01 +[9 T (t)9(t)] 01 9 T (t)[B1(t)09(t) k01 ] (7) T k = T k01 +[B 2 (t) 0 T k01 8(t)]8 T (t)[8(t)8 T (t)] 01 : (8)</formula><p>where the convergence factor is given later. Substituting (3) into <ref type="bibr">(7)</ref> and ( <ref type="formula">4</ref>) into <ref type="bibr" target="#b14">(8)</ref> gives   Here, a difficulty arises in that the expressions on the right-hand sides of ( <ref type="formula">9</ref>) and ( <ref type="formula">10</ref>) contain the unknown parameter matrix and unknown parameter vector , respectively; Our approach is based on the hierarchical identification principle: and in ( <ref type="formula">9</ref>) and ( <ref type="formula">10</ref>) are replaced by their corresponding estimates at iteration k 0 1. This way, we get the HLSI algorithm shown in (11)-( <ref type="formula">12</ref>) at the bottom of the next page.</p><formula xml:id="formula_10">k = k01 + [9 T (t)9(t)] 01 9 T (t)</formula><p>The convergence factor can be taken as = 2=(n + n0) or (13), as shown at the bottom of the page. To initialize the algorithm in (11) to (13), we take 0 = 0 or some small real vector, e.g., 0 = 10 06 1 n21 , and T 0 = 0 or some small real matrix, e.g., T 0 = 10 06 1m2n with 1 m2n being an m 2 n 0 matrix whose elements are 1.</p><p>To establish some convergence results, we will need the following lemma, whose proof is omitted.</p><p>Lemma 1: Assume that there exist vector sequences x(k) 2 n and (i) 2 n satisfying lim k!1 T (i)x(k) = 0 for each i 2 [1; t] (t n), and that the vector (i) is sufficiently rich, i.e., there exist a positive constant c and an integer N n such that the following inequality holds:</p><formula xml:id="formula_11">A1) 1 N N l=1 (i + l) T (i + l) cI: Then, lim k!1 x(k) = 0.</formula><p>Theorem 1: For the system in ( <ref type="formula" target="#formula_2">2</ref>) and the HLSI algorithm in (11)-( <ref type="formula">13</ref>), for any given initial value 0 and 0 , the parameter estimation error is bounded, i.e., k k 0 k 2 + k k 0 k 2 0 ; for any k 1</p><p>with 0 = k9(t)( 0 0 )k 2 + k8 T (t)( 0 0 )k 2 &lt; 1. = kAxk 2 + 2x T y + k(A T A) 01=2 yk 2 and kx T yk 2 kxk 2 1 kyk 2 , we obtain (after some algebra)</p><formula xml:id="formula_12">k9(t) k k 2 k9(t) k01 k 2 0 2 2 t i=1 [ (i) k01 ] T [ (i) k01 0 T k01 '(i)] + 2 n t i=1 k (i) k01 0 T k01 '(i)k 2 (14) k8 T (t) k k 2 k8 T (t) k01 k 2 + 2 2 t i=1 [ T k01 '(i)] T [ (i) k01 0 T k01 '(i)] + 2 n0 t i=1 k (i) k01 0 T k01 '(i)k 2 : (15)</formula><p>Defining a nonnegative-definite function, V (k) = k9(t) k k 2 + k8 T (t) k k 2 , and using ( <ref type="formula">14</ref>) and (15) give</p><formula xml:id="formula_13">V (k) V (k 0 1) 0 2 2 t i=1 k (i) k01 0 T k01 '(i)k 2 + 2 (n + n 0 ) 2 t i=1 k (i) k01 0 T k01 '(i)k 2 = V (k01)0[20(n+n0)] t i=1 k (i) k01 0 T k01 '(i)k 2 = V (0)0[20(n+n 0 )] k01 j=0 t i=1 k (i) j 0 T j '(i)k 2 :</formula><p>If the convergence factor is chosen to satisfy 0 &lt; &lt; 2=(n + n0), then V (k) V (0) = 0 . This proves Theorem 1.</p><p>Moreover, from the aforementioned proof, we also have</p><formula xml:id="formula_14">1 k=0 t i=1 k (i) k 0 T k '(i)k 2 &lt; 1: It follows that as k ! 1, t i=1 k (i) k 0 T k '(i)k 2 = 0, or (i) k 0 T k '(i) = 0; for any i 2 [1; t]:<label>(16)</label></formula><p>Based on this equation, we can obtain the consistent parameter estimates.</p><p>Theorem 2: For the system in (2) and the HLSI algorithm in (11)-( <ref type="formula">13</ref>), if the input-output data vector j(i) := T j (i) '(i) ; j= 1; 2; . . . ; m are sufficiently rich [ j (i) being the jth row of (i)], then the parameter estimation error given by the HLSI algorithm consistently converges to zero for any finite initial value, i.e., Then, (16) may be decomposed into the following m equations T j (i)x j (k) = 0; j= 1; 2; . . . ; m; as k ! 1:  constants 0 &lt; c c1 &lt; 1 and an integer N &gt; n0 such that for each t N, the following strong persistent excitation conditions hold:</p><formula xml:id="formula_15">k = k01 0 t i=1 T (i) (i) 01 t i=1 T (i) y(i) + (i) k01 0 T k01 '(i) (11) T k = T k01 + t i=1 y(i) + (i) k01 0 T k01 '(i) ' T (i) t i=1 '(i)' T (i)</formula><formula xml:id="formula_16">A5) cI 1 N N01 j=0</formula><p>T (t 0 j) (t 0 j) c 1 I; a:s:</p><formula xml:id="formula_17">A6) cI 1 N N01 j=0</formula><p>'(t 0 j)' T (t 0 j) c1I; a:s:</p><p>(t) and '(t) are bounded, i.e., k (t)k 2 + k'(t)k 2 c 2 &lt; 1, and the input-output data vector i(t) := T i (t) '(t) ; i= 1; 2; . . . ; m are persistently excited, where i (t) is the ith row of (t). Then the parameter estimation error given by the HLS algorithm consistently converges to zero, i.e., lim t!1 k(t) 0 k 2 + k (t) 0 k 2 = 0; a:s:</p><formula xml:id="formula_18">or lim t!1 (t) = and lim t!1 (t) = , a.s.</formula><p>The result in this theorem can be proven by formulating a martingale process and by using the martingale convergence theorem (see <ref type="bibr">[5,</ref><ref type="bibr">Lemma D.5.3]</ref>. In order to save space it is omitted here, but is available from the authors.</p><p>Compared with the HLS algorithm, the following stochastic gradient algorithm requires less computational effort:</p><formula xml:id="formula_19">(t) = (t0 1)0 T (t) r<label>(t)</label></formula><p>[y(t)+ (t)(t 0 1)0 T (t 0 1)'(t)] (t) = (t 0 1)+ '(t) r(t)</p><p>[y(t)+ (t)(t 0 1)0 T (t 0 1)'(t)] T r(t) = r(t 0 1) + k (t)k 2 + k'(t)k 2 ; r(0) = 1:</p><p>The choice of the initial values are as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXAMPLE</head><p>In this section, we present an example to illustrate the performance of the proposed algorithms. Consider the following simulated system: </p><formula xml:id="formula_20">T = [ Q 1 Q 2 Q 3 ] = 1<label>1</label></formula><p>00:9 00:75 0:2 0:125 1:2 1:2 01:08 00:78 0:24 0:12 :</p><p>The input u(t) is taken as a persistent excitation vector sequence with zero mean and unit variances, and w(t) as a white noise vector sequence with zero mean and variances 2 w (1) for w 1 (t) and 2 w (2) for w2(t). We apply the HLS algorithm to estimate the parameters of this system. The parameter estimates with different data length are shown in Table <ref type="table" target="#tab_2">I</ref>, where</p><formula xml:id="formula_21">= k(t) 0 k 2 + k (t) 0 k 2 kk 2 + kk 2</formula><p>is the relative parameter estimation error. From Table <ref type="table" target="#tab_2">I</ref>, it is clear that the error is becoming smaller (in general) as t increases, this verifies the theorems proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>According to the hierarchical identification principle, the hierarchical least squares iterative algorithm and hierarchical least squares algorithm are presented for MIMO systems. The analysis indicates that the algorithms proposed have good performance under proper conditions, and require less computational effort than the existing algorithms. Although the algorithms are developed for MIMO stochastic systems with additive white noises, the approach developed can be extended to identification problems of MIMO systems with colored noises. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Reduction With Application to Flexible Systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The model reduction problem consists on the determination of a reduced-order transfer function that suitably approximates the original asymptotically stable model of a system according to a given minimum norm criterion. As stated in <ref type="bibr">[12]</ref>, this problem can be formulated as an optimization problem with a constraint on the rank of some matrix variables, resulting on a highly nonconvex formulation and, indeed, leading to a very difficult problem to be solved. Similarly, in <ref type="bibr" target="#b15">[9]</ref> the solution of the model reduction problem is expressed in terms of a set of nonlinear matrix equations. More recently, the same problem has been addressed in <ref type="bibr">[3]</ref> making use of linear matrix inequalities (LMI) to get, in a concise and elegant way, the classical bounds on the H 1 norm of the reduction error. Many efforts have been made by several authors to bypass or overcome such difficulties. The main idea was to determine a solution with a small degree of suboptimality, based on the determination of lower and upper bounds on the error norm (see the classical papers <ref type="bibr">[1]</ref> and <ref type="bibr">[7]</ref>, and the references therein). However, tighter bounds with small degrees of suboptimality can only be achieved at expense of higher and sometimes prohibitive computational efforts. A compromise of both aspects is present, for instance, in the balanced truncation method <ref type="bibr">[7]</ref>, where good bounds on the norm of the reduction error are obtained and the computational complexity is kept relatively small.</p><p>In this note, a similar compromise is sought for the model reduction of linear continuous time invariant systems. The H1 norm of the reduction error is the index to be minimized. The problem is first stated as a convex programming problem constrained by linear matrix inequalities and a nonlinear (and hence nonconvex) equality constraint. Next, this nonconvex constraint is replaced by a linear constraint involving an a priori fixed matrix variable. The choice of the fixed variable is made so that the degree of suboptimality is small. Since the resulting optimization problem is convex, the obvious advantage of this procedure is that it is solved by very efficient numerical methods available in the literature to date. The proposed reduction procedure is validated through comparisons with the classical balanced truncation method, Manuscript received May 20, 2004; revised November 10, 2004. Recommended by Associate Editor M. Demetriou. This research was supported by grants from "Fundação de Amparo à Pesquisa do Estado de São Paulo-FAPESP" and "Conselho Nacional de Desenvolvimento Científico e Tecnológico-CNPq"-Brazil.</p><p>The authors are with the DSCE/School of Electrical and Computer Engineering, UNICAMP, 13081-970 Campinas, SP, Brazil (e-mail: geromel@ dsce.fee.unicamp.br).</p><p>Digital </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Next, we apply Method M2 to find the right end-point of the exact range I I I 3 associated with 1(A). The interval derivatives d d d lm , l; m = 1; . . . ; 8 were computed as explained in Section IV and the nonzero values are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>+f[y(1)+ (1) y(2)+ (2) 111 y(t)+ (t)] 0 T k01 8(t)g8 T (t)[8(t)8 T (t)] 01 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>i)]' T (i)[8(t)8 T (t)] 01 : using the formulas [x + (A T A) 01 y] T (A T A)[x + (A T A) 01 y]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>lim k! 1</head><label>1</label><figDesc>k = lim k!1 k = : Proof: Let ( T k )j represent the jth row of T k , and xj(k) := k 0( T k ) T j :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(z)y(t) = Q(z)u(t) with (z) = 1 + 1 z 01 + 2 z 02 + 3 z 03 = 1 0 1:15z 01 + 0:425z 02 0 0:05z 03 Q(z) = Q1z 01 + Q2z 02 + Q3z 03</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>J</head><label></label><figDesc>. C. Geromel, R. G. Egas, and F. R. R. Kawaoka Abstract-In this note, H model reduction of continuous time linear systems is revisited. The main objective is to define a convex programming problem expressed in terms of linear matrix inequalities which provides a good suboptimal solution to the model reduction problem. The quality of the proposed suboptimal solution is assessed through comparisons, performed via simulation, with the already classical balanced truncation method, leading to the conclusion that the proposed method performs significantly better. Finally, the proposed method is applied to the determination of a reduced order model of a flexible bar. Index Terms-Linear matrix inequalities (LMIs), linear time-invariant (LTI) systems, model reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I HLS</head><label>I</label><figDesc></figDesc><table /><note><p>ESTIMATES OF THE PARAMETERS ( (1) = 1:4 , (2) = 1:7 )</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank two of the reviewers for their valuable comments and suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since j(i) (j = 1; 2; ... ; m) is sufficiently rich, according to Lemma 1, we have lim k!1 xj(k) = 0; j= 1; 2; ... ; m:</p><p>This proves Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HIERARCHICAL LEAST SQUARES ALGORITHM</head><p>The HLSI algorithm in Section III employs an iterative updating scheme with a fixed-length data batch; it is suitable for offline computation. In this section, we introduce a recursive algorithm, the HLS algorithm, which can be online implemented. Unlike Section III which is deterministic framed, we shall study in the stochastic case.</p><p>Introducing a noise term in the model in (2), we have</p><p>where fw(t)g is assumed to be a zero-mean white noise vector sequence, independent of the input.  </p><p>Here, Y (t) 2 m , (t) 2 and 2 n in (19) can be regarded as the output vector, information matrix, and parameter vector of subsystem S3, respectively; for the quantities in S4. Then, for S3 and S 4 , we form two prediction error criteria <ref type="bibr">[6]</ref> J1()= t j=1 kY (j)+ (j)k 2 ; J2()= t j=1 kZ(j)0 T '(j)k 2 :</p><p>Here, the data length satisfies: t n0. The following result can be derived (the proof is standard and, hence, omitted).</p><p>Theorem 3: Let (t) and (t) be the estimates of and at time t.</p><p>Then, minimizing J1() and J2() leads to the following least squares algorithm:</p><p>Here, P 1 (t) and P 2 (t) are two covariance matrices, and 9(t) and 8(t)</p><p>are defined as before. Substituting (18) into ( <ref type="formula">21</ref>) and ( <ref type="formula">23</ref>) gives</p><p>from which we can see that the right-hand sides of contain the unknown or ; so it is impossible to realize the algorithms. As before, by the hierarchical identification principle, we replace the unknown in (24) and in (25) with their corresponding estimates and at time (t0 1)</p><p>(t) = (t 0 1) + P2(t)'(t) 1 [y(t) + (t)(t 0 1) 0 T (t 0 1)'(t)] T : (27)</p><p>Applying the matrix inversion formula (A + BC) 01 = A 01 0 A 01 B(I + CA 01 B) 01 CA 01 to ( <ref type="formula">22</ref>) and (24) gives P1(t) =P1(t 0 1) 0 P1(t 0 1) T (t) 1 [I + (t)P 1 (t 0 1) T (t)] 01 (t)P 1 (t 0 1) (28) P2(t) =P2(t 0 1) 0 P 2 (t 0 1)'(t)' T (t)P 2 (t 0 1) 1 + ' T (t)P 2 (t 0 1)'(t) : (29) Thus, we obtain the HLS algorithm as follows:</p><p>(t) = (t 0 1) 0 P1(t) T (t) 1 [y(t) 0 T (t 0 1)'(t) + (t)(t 0 1)] (30)</p><p>P 01 (33)</p><p>To initialize the algorithm, we take P1(0) = p0I, P2(0) = p0I with p 0 normally a large positive number (e.g., p 0 = 10 <ref type="formula">6</ref>) and (0) = 10 06 1 n21 , T (0) = 10 06 1 m2n .</p><p>The standard least squares (LS) algorithm may be applied to generate the parameter estimate of models of form: y(t) = 28(t) + w(t), or y(t) = 8(t)2+w(t), 2 representing a parameter matrix/vector, 8(t)</p><p>the information vector/matrix. In general, the estimate can be expressed as <ref type="bibr">[6]</ref> 2(t) = 2(t 0 1) + E(t)</p><p>where E(t) = f(y(t); 8 T (t); 2(t 0 1)) is the innovation. The LS algorithm employs the idea of innovation modification. However, the hierarchical identification produces two estimates: a vector (t) and a matrix (t). The estimate (t) depends on not only (t 0 1) but also (t 0 1); similarly, the estimate (t) depends on not only (t 0 1) but also (t 0 1).</p><p>The HLSI and HLS algorithms require computing two covariance matrices P 1 (t) and P 2 (t) which are of sizes n 2 n and n 0 2 n 0 , respectively. However, Sen and Sinha's algorithm needs to compute a large covariance matrix of dimensions (mn0 + n) 2 (mn0 + n) <ref type="bibr" target="#b15">[9]</ref>.</p><p>Based on this, our methods require less computational effort than the Sen and Sinha's algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONVERGENCE OF THE HLS ALGORITHM</head><p>In order to study the convergence of the HLS algorithm, the following assumptions are required: fw(t);F t g is a martingale difference vector sequence defined on a probability space f; F; Pg, where fFtg is the algebra sequence generated by fw(t)g, i.e., F t = (w(t);w(t 0 1);w(t 0 2); .. .; ). The sequence fw(t)g satisfies <ref type="bibr">[5]</ref> A2)</p><p>E[w(t)jFt01] = 0; a:s:</p><p>A3)</p><p>E[kw(t)k 2 jF t01 ] = 2 w (t) 2 w &lt; 1; a:s: A4) lim sup t!1 1 t t i=1 kw(i)k 2 2  w &lt; 1; a:s:</p><p>That is, fw(t)g is a noise vector with zero mean and time-varying variances. Thus, the system in (17) may be nonstationary. Theorem 4: For the system in (17) and the HLS algorithm in (30)-(33), suppose that A2)-A4) hold, and that the output-input data matrix (t) and vector '(t) are persistently excited, i.e., there exist</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robustness of discrete systems under structured uncertainties</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rachid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1563" to="1566" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new approach to the stability analysis of interval systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control-Theory Adv. Technol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="1991-06">Jun. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stability of interval matrices: The real eigenvalue case</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1604" to="1605" />
			<date type="published" when="1992-10">Oct. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stability analysis of dynamic interval systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1401" to="1408" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interval methods for circuit analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kolev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Series on Circuits and Systems</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new method for global solution of systems of nonlinear equations</title>
	</analytic>
	<monogr>
		<title level="j">Rel. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="125" to="146" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cheap and tight bounds on the solution set of perturbed systems of nonlinear equations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nenov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rel. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="399" to="408" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Iterative least squares solutions of coupled Sylvester matrix equations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Control Lett</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parametric identification of a state-space model of multivariable systems using the extended least-squares method</title>
		<author>
			<persName><forename type="first">H</forename><surname>El-Sherief</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="227" />
			<date type="published" when="1981-05">May 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Choices of models for the identification of linear multivariable discrete-time systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>El-Sherief</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Inst. Elec. Eng. D</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1326" to="1330" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the recursive identification of multiinput multi-output systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Landau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="609" to="614" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive Filtering Prediction and Control</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Sin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">System Identification: Theory for the User</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Theory and Practice of Recursive Identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Söderström</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive estimation of the parameters of linear multivariable systems</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="471" to="475" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On-line estimation of the parameters of a multivariable system using matrix pseudoinverse</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="471" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dynamical Hierarchical Control</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>North-Holland</publisher>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
