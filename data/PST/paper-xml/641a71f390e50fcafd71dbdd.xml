<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-20">20 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-20">20 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.11381v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare MM-REACT's system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. Code, demo, video, and visualization are available at https://multimodal-react.github.io/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have seen significant advancement for computer vision, thanks to improved network architecture <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>, large-scale model training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>, and other factors. However, different vision problems typically require different models, which often require manual selection and composition of individual models for each use case. For example, when determining if an image contains "people", we may choose the image tagging model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> and check if the predicted tag list contains "people". If "people" exists, we may select the celebrity model <ref type="bibr" target="#b19">[20]</ref> to further understand whether a celebrity appears and who he/she is.</p><p>One research direction is to combine the vision and language modules as one end-to-end model, such as Flamingo <ref type="bibr" target="#b1">[2]</ref>, PaLM-E <ref type="bibr" target="#b9">[10]</ref>, to provide a dialogue-based experience to the end user. That is, the user can use natural language to interact with the model around the image content. The vision module encodes vision signals into special text tokens or features that the language module can understand, enabling the system to utilize the language module for understanding user queries and providing responses. However, these joint finetuning approaches require a large amount of computing resources and annotated data to enable specific capabilities. In this work, we aim to combine existing individual vision models with the language model in a more flexible manner to tackle complicated visual understanding problems, e.g., the ones illustrated in Figure <ref type="figure">1</ref>.</p><p>Large language models (LLMs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, such as ChatGPT, have shown impressive dialogue capability with text as both input and output. Recent NLP research <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34</ref>] (e.g., REACT <ref type="bibr" target="#b33">[34]</ref>) demonstrates the effectiveness of integrating external NLP tools, such as search engines and math calculators, with LLMs by proper instruction. Specifically, RE-ACT <ref type="bibr" target="#b33">[34]</ref> prompts an LLM to generate reasoning texts that break down complex problems into intermediate steps, and action texts that allocate NLP tools for solving these steps. One example is that the LLM can suggest a text query to a modern search engine to grab the latest internet information, and return the user with the information that is not in the pre-training corpus. Inspired by the efficacy of reasoning and acting with LLMs and NLP tools, we explore the integration of vision expert tools with LLMs.</p><p>To this end, we present MM-REACT, a system paradigm that composes numerous vision experts with ChatGPT for multimodal reasoning and action. To enable images and videos as inputs, we use their file path as the input to Chat-GPT. The file path functions as a placeholder, allowing ChatGPT to treat it as a black box. Whenever a specific property such as celebrity names or box coordinates is required, ChatGPT is expected to seek help from a specific vision expert to identify the desired information. To inject the knowledge of vision experts' usages into ChatGPT, we add instructions to ChatGPT prompts about each expert's capability, input argument type, and output type, along with a few in-context examples for each expert. Additionally, a special watchword is instructed such that we can use regex expression matching to invoke the expert accordingly.</p><p>We show MM-REACT's representative visual understanding capabilities in Figure <ref type="figure">1</ref>. For example, MM-REACT could associate information from multiple uploaded receipts and calculate the total travel cost ("Multi-Image Reasoning"), recognize and answer questions about the "morel mushrooms" ("Open-World Concept Understanding"), and condense a long video into representative thumbnails ("Video Summarization and Event Localization"). These visual intelligence features are similar to those found in recent models, such as multimodal GPT-4 <ref type="bibr" target="#b22">[23]</ref> and PaLM-E <ref type="bibr" target="#b9">[10]</ref>, but achieved via prompting instead of additional multimodal training. The MM-REACT system may provide extra flexibility in module upgrades, and may be effective in certain visual understanding tasks by better utilizing existing specialized vision experts, such as celebrity recognition and dense captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>LLMs Prompting Methods. Large language models (LLMs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> demonstrate a strong chain-of-thought (CoT) capability <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref> that could break down complex problems into solvable intermediate steps. On the other hand, research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref> shows that LLMs, when equipped with a range of external NLP tools, can effectively serve as action planners to select and utilize tools for problemsolving, such as using search or mathematical tools to address knowledge or math problems.</p><p>Nevertheless, LLMs for reasoning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref> and LLMs for action <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref> , when used independently, fail to solve complex tasks that require breaking down the problem via reasoning and solving sub-steps via planned actions. Recent studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref> have attempted to merge the action and reasoning phases to enhance LLMs' capabilities in solving complicated tasks that require advanced planning and reasoning. One representative work, REACT <ref type="bibr" target="#b33">[34]</ref>, treats reasoning text generation as an executable action and achieves the synergetic combination of reasoning and action for NLP tasks. In this work, we explore how to extend such intriguing properties into multimodal scenarios by modeling thought and invoking vision tools as executable actions.</p><p>Vision+LLMs. Our MM-REACT is related to the previous studies that extend language models to understand visual inputs. The representative framework adds a vision module to project visual inputs into representations that the language model can understand. These representations can be either discrete text words <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> or continuous features projected into the textual feature space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> explore the chain-of-thought capability <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref> in multimodal settings. MM-CoT <ref type="bibr" target="#b36">[37]</ref> finetunes on the reasoning chain annotated in the ScienceQA <ref type="bibr" target="#b20">[21]</ref> dataset to achieve the CoT capability in the science question answering task. KOSMOS-1 <ref type="bibr" target="#b13">[14]</ref> and PaLM-E <ref type="bibr" target="#b9">[10]</ref> demonstrate the zeroshot multimodal CoT capabilities with large-scale training.</p><p>Multimodal Reasoning and Action. A key distinction between MM-REACT and prior vision+LLM studies discussed above is that MM-REACT leverages LLMs' highlevel planning abilities to allocate various vision experts, rather than solely using LLMs for text generation conditioned on visual inputs. MM-REACT is closely related to the recent concurrent work of Visual ChatGPT <ref type="bibr" target="#b31">[32]</ref> and ViperGPT <ref type="bibr" target="#b24">[25]</ref>. In comparison, Visual ChatGPT <ref type="bibr" target="#b31">[32]</ref> primarily focuses on image generation and editing, while our MM-REACT mainly focuses on visual understanding.</p><p>ViperGPT <ref type="bibr" target="#b24">[25]</ref> instructs LLMs to generate Python code for a one-round query answering. In contrast, MM-REACT is a multi-round, dialogue-based system that may integrate the strong QA model as one of its vision experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MM-REACT Prompting</head><p>The goal of MM-REACT is to compose numerous vision experts to empower ChatGPT with visual understanding. A vision expert is a computer vision model that takes an image as input and interprets the content from different perspec-tives. For instance, the image captioning expert generates a natural description, the OCR expert extracts the scene text in the image, the celebrity recognition model identifies the celebrity names, and the object detection model extracts the salient object with bounding box locations. At present, one may have to manually decide which vision experts to employ for specific use cases, and manually compose them. Instead, our target is to automate this process based on the requirements presented by the user query in natural language.</p><p>ChatGPT is an artificial intelligence chatbot with text as both input and output, without visual understanding. However, ChatGPT exhibits strong instruct learning capability, which motivates us to instruct ChatGPT to properly determine which vision expert should be invoked and which image should be processed.</p><p>Figure <ref type="figure">2</ref> shows the flowchart of our MM-REACT system. The terms thought and action request refer to the reasoning and action-oriented texts generated by ChatGPT to break down the problem or invoke vision experts. Observation refers to the vision expert's responses after the action execution requested in the action request text. Next, we detail each step in the flowchart as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">User Input</head><p>As ChatGPT only accepts texts as input, the first challenge is how to accommodate non-text inputs, such as multiple images and videos. Since most vision experts accept the file path or URL, we use the path string to indicate nontext inputs. The file path itself is meaningless and is essentially a placeholder. Although no visual recognition task can be performed directly with file paths, ChatGPT may seek help from different vision experts to understand the image content from different perspectives, e.g., identifying the celebrity names of the detected person. By including the provided file path in its text output, ChatGPT can indicate which image should be processed by the vision expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ChatGPT Response</head><p>Given the user's input, ChatGPT is expected to provide two kinds of responses. The first is to seek help from vision experts, while the second is to respond to the user directly.</p><p>A key challenge is to set up a protocol such that we know when to invoke the vision expert. Inspired by REACT <ref type="bibr" target="#b33">[34]</ref>, we instruct ChatGPT to respond with certain watchwords, such as "Assistant, what objects are there in the image? &lt;file path&gt;", if a specific vision expert is required. In our implementation, we use the keyword "Assistant," to distinguish whether a vision expert is required.</p><p>To further improve the performance, we encourage Chat-GPT to show the thought (reasoning) process to highlight why an external tool is required. It is also shown to be beneficial in NLP studies <ref type="bibr" target="#b33">[34]</ref> to incorporate such reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Vision Experts</head><p>Given the action request from ChatGPT, we use the regular expression matching to parse the expert name and the file path, and invoke the action (vision expert execution).</p><p>The expert's output can be in different forms but is standardized into the text format such that ChatGPT can understand it. For certain experts, such as the captioning model or the celebrity model, it is straightforward to represent the output as text. However, the standardization is less intuitive for others. For example, the detection model outputs a list of object names with bounding box locations. In this case, we concatenate all the boxes, each of which is represented as &lt;object name, x1, y1, x2, y2&gt;, where (x1,y1), (x2,y2) are the coordinates of the top-left and bottom-right corners, respectively. An additional text description is added to explain the meaning of the last four numerical values. In some cases, we find ChatGPT is capable of understanding these coordinates, e.g., identifying which object is on the left.</p><p>The text-formed output from vision experts can be interpreted as the observation resulting from ChatGPT's action of invoking the vision expert. Combining observations with the chat history, ChatGPT can further invoke additional experts or return the final answer to the user. We provide examples of full execution flows in Section 4.2 and Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>To inject the knowledge of various vision experts' usages, we add both instructions and in-context examples in the prefix when prompting ChatGPT. Each expert is de-scribed with the model name, a general description of its capability, the input data format, and the output information. After describing each expert, we add a few in-context dialogue examples to enhance the performance. With the injected knowledge, ChatGPT can effectively select one or multiple vision experts to understand the images or videos from different perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Extensibility</head><p>Our scheme is motivated by REACT, which invokes different tools in the NLP field. As only the text is involved, no specific design is required to incorporate other modalities. In this work, we extend MM-REACT to the vision domain. The key is to replace the non-text modality with a path string, enabling ChatGPT to ask specific vision experts to recognize the visual content. Therefore, we could further extend MM-REACT to other modalities, such as speech and audio. Meanwhile, we can also easily incorporate more tools by formatting their outputs in a text format. While ChatGPT serves as the primary LLM in our main implementation, performance could be further enhanced through the simple upgrade to a more powerful LLM, such as GPT-4 <ref type="bibr" target="#b22">[23]</ref> discussed in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>We implement MM-REACT based on the LangChain codebase <ref type="bibr" target="#b4">[5]</ref> and reference ideas from ReAct <ref type="bibr" target="#b33">[34]</ref>. We access ChatGPT via the Azure "gpt-3.5-turbo" API that has a token length limit of 4,096, and utilize vision experts publicly available via the Azure Cognitive Services APIs<ref type="foot" target="#foot_0">1</ref> , including the ones for image captioning, image tagging, dense captioning, optical character recognition (OCR), and specialized recognition models for celebrities, receipts, etc. We further expand the toolset with customized tools for spatial understanding and image editing, and tools from other modalities such as Bing search and PAL math.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MM-REACT's Full Execution Flow</head><p>Figure <ref type="figure" target="#fig_1">3</ref> provides an example to illustrate MM-REACT's full execution flow. We highlight the exact order to call different models (i.e., executions) with numbered blue circles. The executions, highlighted by bold underlined text, can be either a ChatGPT call (e.g., "ChatGPT:") or the execution of one or multiple selected vision experts (e.g., "Image Captioning"). We add a commentary text action execution in dashed boxes to help understand the vision expert execution. The action execution is not an actual input or output in the MM-REACT flow. ChatGPT executions can be used to generate thought (reasoning) and action texts that   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MM-REACT Capabilities and Applications</head><p>Figures 4-14 show the representative capabilities and application scenarios that MM-REACT demonstrates. Specifically, we examine MM-REACT's capabilities in visual math and text reasoning (Figure <ref type="figure" target="#fig_3">4</ref>), understanding visualconditioned jokes and memes (Figure <ref type="figure" target="#fig_6">5</ref>), spatial and coordinate understanding, visual planning and prediction (Figure <ref type="figure" target="#fig_7">6</ref>), multi-image reasoning (Figure <ref type="figure" target="#fig_9">7</ref>), multi-hop document understanding on bar charts (Figure <ref type="figure" target="#fig_10">8</ref>), floorplans (Figure <ref type="figure" target="#fig_11">9</ref>), flowcharts (Figure <ref type="figure" target="#fig_12">10</ref>), tables (Figure <ref type="figure" target="#fig_13">11</ref>), openworld concept understanding (Figure <ref type="figure" target="#fig_14">12</ref>), and video analysis and summarization (Figure <ref type="figure" target="#fig_1">13,</ref><ref type="figure" target="#fig_16">14</ref>). We provide an example of the unfolded steps in Figure <ref type="figure" target="#fig_10">18</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">MM-REACT Extensibility</head><p>In Figure <ref type="figure" target="#fig_1">23</ref> and 24, we explore the enhancement of MM-REACT's LLM from ChatGPT ("gpt-3.5-turbo") to GPT-4 (language-only). We access the language-only GPT-4 via the ChatGPT website and reference the multimodal GPT-4 demo provided by OpenAI for comparison. These examples demonstrate the benefit of MM-REACT's extensibility: MM-REACT equipped with GPT-4 correctly answers the physics question (Figure <ref type="figure" target="#fig_1">23</ref>), while the version with ChatGPT (GPT-3.5) fails. Furthermore, MM-REACT is designed with the flexibility to incorporate new tools without training. Figure <ref type="figure" target="#fig_25">25</ref> provides a case study of plugging an image editing tool from X-decoder <ref type="bibr" target="#b37">[38]</ref> for multi-round, dialogue-based image editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Limitations</head><p>We identify the following limitations. 1). Considering the recognition capability in the wild, we find it hard to sys-tematically evaluate the performance with concrete accuracy numbers, due to a lack of annotated benchmarks. Thus, it is worth investing efforts towards how to effectively evaluate such system's performance. 2). The vision capability is limited by the integrated vision experts. On one hand, the integrated experts may make mistakes; on the other hand, the system may fail if the necessary experts are missing. 3). We inject the vision experts' knowledge in the prefix, and thus the number of experts is limited by the context window (4096 tokens) of ChatGPT. 4). Visual signals are converted to text words for ChatGPT understanding, which might not be the optimal solution for certain vision tasks. 5). MM-REACT requires manual prompt engineering. We expect research work to automate this process, making the system even easier to develop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented MM-REACT, a system paradigm that synergizes multimodal reasoning and action to tackle complicated visual understanding problems. MM-REACT presents a simple and flexible way to empower LLMs with a pool of vision experts. Extensive zero-shot experiments demonstrate MM-REACT's capabilities in solving a wide range of challenging understanding tasks, such as multi-image reasoning, multi-hop document understanding, open-world concept understanding, video summarization, and more. This image is a cartoon illustration of a squirrel holding a camera</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual-conditioned Joke/Meme</head><p>Why would a person find this image funny?</p><p>The humor in this image is subjective and can vary from person to person. Some people might find the cartoon squirrel holding a camera funny because it is a cute and quirky illustration. Others might find it funny because it is a humorous take on the idea of a squirrel taking photos. Ultimately, the humor in this image is up to the interpretation of the individual viewer. So, it depends on the person's sense of humor and personal preferences.</p><p>What is unusual about this image?</p><p>The unusual thing about this image is that a man is ironing clothes in the back of a yellow taxi. This is not a common sight and may be considered unusual or unique. This image is likely a comic or cartoon illustration.</p><p>Is there text in the image? If so, can you read it and explain it to me?</p><p>The text in the image is a conversation between two people. One person says they don't feel good and need to buy a test. The other person asks if the first person is pregnant or has the coronavirus. The text is likely part of a comic or cartoon illustration. The image is tagged with "@INNA_S_ART", which may indicate the artist or creator of the image.</p><p>What is funny about it?</p><p>The humor in the comic may come from the situation, the characters, or the dialogue. The person is feeling unwell and is asked if they are pregnant or have the coronavirus, which is a current and relevant topic. The humor may come from the absurdity of the situation or the unexpectedness of the question. However, humor is subjective and what one person finds funny, another person may not. It is best to view the comic and decide for yourself if it is funny. This image is likely a dog playing frisbee with a man.</p><p>Where is the frisbee in the image?</p><p>The frisbee is located at the top left corner of the image with coordinates (63, 119) and (133, 177).</p><p>Where is the man in the image?</p><p>The man is located at the coordinates (66, 386) to (453, 604) in the image. This image is a recipe for making bread at home. The ingredients include water, salt, flour, sugar, and yeast. The instructions include kneading the dough, dividing it into pieces, and baking it on each side until well done. The temperature is in Celsius.</p><p>What should I do after kneading the dough, before dividing it into pieces? After kneading the dough, you should put it in a warm place before dividing it into pieces, according to the recipe in the image.</p><p>How long should I put in warm places?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial/Coordinate Understanding Visual Planning and Prediction</head><p>What is the spatial relation between the frisbee and the man?</p><p>The frisbee is located above the man in the image. The man is falling on the ground while the frisbee is in the air.</p><p>The frisbee is also closer to the top of the image than the man..</p><p>You should put the dough in a warm place for 1 hour. How much did I spend on grocery?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Illustration of detected bbox from MM-React</head><p>You spent $25.97 on groceries at The Shop Store #100 in Chicago, IL.</p><p>How much in total did I spend on travel?</p><p>You spent $1299.90 on airfare and $43.83 on an Uber ride for a total of $1343.73 on travel.</p><p>How much in total did I pay for taxes?</p><p>You paid $323.23 in total for taxes. This includes $321.90 for taxes and fees for the flight purchase and $1.33 for sales tax for the Windmill Family Restaurant expenses.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Image Reasoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Summarization/Event Localization</head><p>Please create a summary of the video in less than 10 sentences.</p><p>The speaker is making a BLT sandwich with grilled tomatoes and fried bacon. They are using fresh bread, mayo, Worcester sauce, lettuce, and tomatoes. They are sprinkling salt and pepper on top. The bread can be toasted or grilled. The speaker mentions that the few ingredients used in the sandwich will give it a lot of flavor. The speaker mentions that the bread is important and can be toasted or grilled. The final product is a BLT sandwich.</p><p>Can you this tutorial of making a BLT sandwich to step-by-step instructions, with start and end time?</p><p>Here's a breakdown of the start and end times for each step in the video:</p><p>1.</p><p>Heat a little olive oil in a pan and grill the tomatoes: 00:00:22 -00:00:40 2.</p><p>Cook the bacon in a frying pan: 00:00:51 -00:01:41 3.</p><p>Place the cooked bacon on a paper towel: 00:01:39 -00:01:49 4.</p><p>Spread mayo and add Worcestershire sauce: 00:01:55 -00:02:16 5.</p><p>Assemble the sandwich (lettuce, tomatoes, bacon): 00:02:22 -00:03:11 a.</p><p>Layer lettuce: 00:02:44 -00:02:49 b.</p><p>Add grilled tomatoes: 00:02:55 -00:03:01 c.</p><p>Place bacon on top: 00:03:15 -00:03:27 6.</p><p>Sprinkle salt and pepper: 00:03:08 -00:03:11 7.</p><p>Top the sandwich with the other slice of bread: 00:03:29 -00:03:34 I am trying to create a short version of the video, can you suggest a shorter segment for each step?</p><p>Here's a suggested shorter segment for each step: 1.</p><p>Heat a little olive oil in a pan and grill the tomatoes: 00:00:24 -00:00:30 2.</p><p>Cook the bacon in a frying pan: 00:00:53 -00:00:59 3.</p><p>Place the cooked bacon on a paper towel: 00:01:39 -00:01:41 4.</p><p>Spread mayo and add Worcestershire sauce: 00:02:00 -00:02:08 5.</p><p>Assemble the sandwich (lettuce, tomatoes, bacon): 00:02:44 -00:03:01 a.</p><p>Layer lettuce: 00:02:44 -00:02:47 b.</p><p>Add grilled tomatoes: 00:02:55 -00:02:59 c.</p><p>Place bacon on top: 00:03:15 -00:03:20 6.</p><p>Sprinkle salt and pepper: 00:03:08 -00:03:11 7.</p><p>Top the sandwich with the other slice of bread: 00:03:29 -00:03:34 Which segment is best for video thumbnail to attract more audience?</p><p>For a video thumbnail that attracts more audience, the segment showing the assembled sandwich with all its layers would be the most appealing. This image will showcase the delicious combination of ingredients and the final result of the recipe. The suggested segment for this is:</p><p>? Assembled BLT sandwich: 00:03:29 -00:03:34 This segment shows the completed sandwich just before adding the final slice of bread on top. You can take a screenshot of the sandwich from this segment or create a more stylized image based on it for your thumbnail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Summarization/Event Localization</head><p>The video is about Kobe Bryant's final game in the NBA. He scored 60 points and led the Lakers to a victory over the Utah Jazz. The crowd was out of their minds and the performance was considered one of the greatest in the history of the game. Kobe was fouled and made two free throws for 60 points. The video ends with the crowd applauding and Kobe's final memory being a 60-point game and a victory over the Utah Jazz. Overall, it was a magical farewell performance for one of the athletes ever.</p><p>How did Kobe win the last 2 points to get 60? Explain and locate the start and end time of relevant segment in this video.</p><p>Kobe Bryant won the last 2 points to reach 60 by making two free throws. He was fouled and went to the free-throw line. The relevant section starts at 00:04:02 with the mention of the foul, it ends at 00:04:39 after Kobe successfully converts both free throws.</p><p>Second Free throw at 00:04:31 -00:04:34 First Free throw at 00:04:11 -00:04:15</p><p>Please create a summary of the video in less than 10 sentences.</p><p>I want to make a short video to highlight the long deuce by Kobe if there is any in the video. Can you tell me roughly the start and end time of that.</p><p>The long deuce by Kobe Bryant is mentioned at timestamp 00:03:28. The play leading up to this shot starts around 00:03:21, and the audience's reaction to the long deuce ends at approximately 00:03:32. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual-Conditioned Jokes</head><p>Please describe the image This image is likely a dog jumping over a hurdle in a dog agility competition.</p><p>Describe the following &lt;img&gt;: A dog jumping over a hurdle at a dog show.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PaLM-E Ours I see (image omitted). Description: A cow sitting on a rug, Why is the cow sitting on a rug?</head><p>Because it wants to be a cow-ch! .. I see &lt;img&gt;. Description: A donkey is carrying a dog, cat, and rooster. Joke: What do you call a donkey with a rooster on his back. A rooster booster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please write a joke for this image</head><p>Why did the cat stand on the dog's back? Because the donkey was full! Image Captioning, VQA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PaLM-E Ours</head><p>Where is the coffee pot in the second image?</p><p>The coffee pot is located in the kitchen with a stove and cabinets. It is a silver coffee pot on a stove and its location is at (242, 230) to (295, 299).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: Where is &lt;img1&gt; in this scene &lt;img2&gt;? A:</head><p>A stove top with a pot on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PaLM-E Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Image Relationships</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image1 Image2</head><p>Illustration of detected bbox from MM-React </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-End Egocentric Q&amp;A, Dialogue</head><p>How many championship rings did the player on the left win in his career?</p><p>Kobe Bryant won 5 championship rings in his career.</p><p>Given &lt;img&gt;. Q: How many championship rings did the player on the left win in his career A: Let's think step by step 1. kobe bryant is on the left. 2. kobe bryant has won 5 championship rings.</p><p>PaLM-E PaLM-E</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>When a human asks me to do a task, I will respond with the sequence of actions I would do to accomplish the task with only the items I see. Human: Use all of the ingredients you see to make a cake batter.</p><p>To make a cake batter using the ingredients I see in the image, I would do the following:</p><p>1. Gather all of the bags of flour and empty them into a large mixing bowl. 2. Add any other dry ingredients, such as sugar or baking powder, to the bowl.</p><p>3. Mix the dry ingredients together until they are well combined. 4. Add any liquids, such as milk or eggs, to the bowl and mix until the batter is smooth. 5. Pour the batter into a greased cake pan and bake according to the recipe instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>When a human asks me to do a task, I will respond with the sequence of actions I would do to accomplish the task with only the items I see.</p><p>Human: Use all of the ingredients you see to make a cake batter. 1. crack egg. 2. put egg in bowl. 3. put flour in bowl. 4. put sugar in bowl. 5. mix. 6. put in pan. 7. bake. 8. eat. 9. clean up.</p><p>PaLM-E  Why is it funny?</p><p>ChatGPT: The humor in the comic may come from the situation, the characters, or the dialogue. The person is feeling unwell and is asked if they are pregnant or have the coronavirus, which is a current and relevant topic. The humor may come from the absurdity of the situation or the unexpectedness of the question. However, humor is subjective and what one person finds funny, another person may not.</p><p>It is best to view the comic and decide for yourself if it is funny.</p><p>ChatGPT: This image is likely a comic or cartoon illustration.</p><p>Is there text in the image? If so, can you read it and explain it to me?</p><p>Figure <ref type="figure" target="#fig_10">18</ref>. Unfolded multimodal reasoning and action steps for example of visual-conditioned joke/meme in Figure <ref type="figure" target="#fig_6">5</ref>.       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Convert image to &lt;ImagePath&gt; and send to ChatGPTThought 1ChatGPT: This is an image. Action request 1 Assistant, what objects do you see in this image? &lt;ImagePath&gt; Invoke vision experts, including Image Captioning, Dense Captioning, Object Tagger, Face Detection.Observation 1 Image CaptioningAssistant: This is an image (694 x 720) with description a basketball player in a yellow jersey with a ball in his hand.Dense CaptioningList of object descriptions, and their locations in this image: a close up of a basketball (x1,y1,x2,y2) = (63, 535, 190, 663) a basketball player in a yellow jersey (x1,y1,x2,y2) = (55, 19, 502, 700) ? Object Tagger List of object tags seen in this image: sport, athletic game, basketball, ? Face Detection Detected faces, and their location in this image: man (x1,y1,x2,y2)=(365,278,439,352) man (x1,y1,x2,y2)=(548,308,620,380) There are two faces detected in the image. Assistant, are there any celebrities in this image? &lt;ImagePath&gt; Invoke Celebrity Recognition Celebrity Recognition This image contains celebrities List of celebrities, and their location in this image: Kobe Bryant (x1,y1,x2,y2)=(365,278,439,352) Paul Pierce (x1,y1,x2,y2)=(548,308,620,380) ChatGPT: This image contains two celebrities, Kobe Bryant and Paul Pierce. They are both basketball players. How many championship rings did the player on the left win in his career? This question requires Bing Search Assistant, Bing search how many championship rings did Kobe Bryant win in his career? &lt;ImagePath&gt;. Bing Search Assistant: Bryant, a shooting guard, and O'Neal, a centre, meshed into a remarkably effective combination, and, by the time Bryant was 23, the Lakers had won three consecutive NBA championships. Kobe Bryant After winning their third title in 2002, Bryant and the Lakers encountered difficulties. Kobe Bryant has won a total of 5 rings in 20 seasons? ChatGPT: Kobe Bryant won 5 championship rings in his career.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An example of MM-REACT's full execution flow. The blue circles with numbered indices indicate the order in which different models are called (i.e., the executions). The executions, highlighted by bold underlined text, can be either a ChatGPT call (e.g., "ChatGPT:") or running one or multiple selected vision experts (e.g., "Image Captioning"). We add a commentary text action execution in dashed boxes to help understand the expert execution. Each ChatGPT execution takes the preceding text as input and generates the text leading up to the next execution (e.g., "This is an image. Assistant, what . . . image? &lt;ImagePath&gt;" for Execution 1). Texts in gray represent MM-REACT's thoughts or vision experts' actions and outputs, which are invisible to users. This multimodal reasoning and action process occurs behind the scene to gather the necessary information for generating final responses to users, which are shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>allocate vision experts (invisible to users), or produce the final response to users . Each ChatGPT execution takes the preceding text as input and generates the text leading up to the next execution (e.g., "This is an image. Assistant, what objects do you see in this image? &lt;ImagePath&gt;" for Execution 1). ChatGPT "learns" the proper text to generate based on the instructions and in-context examples in the prompt prefix, as detailed in Section 3.3. Additional examples of the reasoning and execution procedures are in Figures 18-22.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 4 .</head><label>4</label><figDesc>Capability Comparison with PaLM-E MM-REACT is a training-free scheme which composes existing vision experts with ChatGPT, while PaLM-E [10] trains a vision-language model which combines an image encoder and a text decoder with dedicated datasets. Figures 15-17 shows our MM-REACT can achieve competitive results to PaLM-E. We further illustrate the complete multimodal reasoning and action procedures in Figures 21,22</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Case studies of MM-REACT's capabilities and application scenarios: visual math and text reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Case studies of MM-REACT's capabilities and application scenarios: visual-conditioned joke/meme. We provide an example of the unfolded multi-modal reasoning and action steps for meme understanding in Figure18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Case studies of MM-REACT's capabilities and application scenarios: spatial/coordinate understanding and visual planning and prediction.</figDesc><graphic url="image-126.png" coords="11,190.62,531.78,77.37,116.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>How much did I spend on dining out? You spent $23.45 on dining out at the Windmill Family Restaurant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Case studies of MM-REACT's capabilities and application scenarios: multi-image reasoning. For the multi-image inputs shown above, we input one receipt image at a time. Once all four receipt images are provided as inputs to MM-REACT, we prompt it to answer questions that require reasoning over multiple images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Case studies of MM-REACT's capabilities and application scenarios: multi-hop document understanding (bar charts). The unfolded multi-modal reasoning and action steps of this example are detailed in Figure19.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Case studies of MM-REACT's capabilities and application scenarios: document understanding (floorplan).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Case studies of MM-REACT's capabilities and application scenarios: document understanding (flowchart). We provide an example of the unfolded multi-modal reasoning and action steps for flowchart understanding in Figure20.</figDesc><graphic url="image-185.png" coords="15,324.06,555.30,170.72,70.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Case studies of MM-REACT's capabilities and application document understanding (table).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Case studies of MM-REACT's and application scenarios: open-world concept understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Suggested 29 Figure 13 .</head><label>2913</label><figDesc>Figure 13. Case studies of MM-REACT's capabilities and application scenarios: video summarization/event localization.</figDesc><graphic url="image-240.png" coords="18,129.16,597.79,93.22,52.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Case studies of MM-REACT's capabilities and application scenarios: video summarization/event localization.</figDesc><graphic url="image-251.png" coords="19,317.03,561.49,169.43,91.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Comparison of MM-REACT with PaLM-E [10] on illustrated capabilities. We empirically show that text prompts are as effective as expensive joint fine-tuning in solving complicated vision problems.</figDesc><graphic url="image-278.png" coords="20,413.58,574.47,90.05,77.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Photo 1 ,Figure 16 .</head><label>116</label><figDesc>Photo 1, at 10:30 am: &lt;img1&gt;. Photo 2, at 12:45 pm: &lt;img2&gt;. Photo 3, at 3:45 pm: &lt;img3&gt;. Q: I forget, what did I have for lunch, and what time was it? A: Let's think step by step. 1. you had a sandwich for lunch. 2. it was 12:45 pm</figDesc><graphic url="image-285.png" coords="21,175.68,487.76,116.14,64.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Comparison of MM-REACT with PaLM-E [10] on illustrated capabilities. We empirically show that text prompts are as effective as expensive joint fine-tuning in solving complicated vision problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>) and dividing it by 2. ($31.35 + $12.54) / 2 = $22.44. Now how much cheaper is the average charging cost over the electric cars (including Ford F150 Lightning) than the average fueling cost over the gasoline cars (including Ford F150 (Regular Cab))? Let's think step-by-step. ChatGPT: The average charging cost over the three electric cars (including Ford F150 Lightning) is $18.66, which is calculated by taking the sum of the total charging cost of Ford F150 Lightning (Estimate), Rivian R1T/S, and Tesla Model 3 (Standard Range) and dividing it by 3. ($22.76 + $31.35 + $12.54) / 3 = $18.66. The average fueling cost over the three gasoline cars (including Ford F150 (Regular Cab)) is $101.07, which is calculated by taking the sum of the total fueling cost of Ford F150 (Regular Cab), Toyota RAV4, and Honda Civic and dividing it by 3. ($130.96 + $82.56 + $70.55) / 3 = $101.07. Therefore, the average charging cost is $82.41 cheaper than the average fueling cost. This is calculated by subtracting the average charging cost from the average fueling cost. $101.07 -$18.66 = $82.41. Convert image to &lt;ImagePath&gt; and send to ChatGPT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 19 . 1 Observation 1</head><label>1911</label><figDesc>Figure 19. Unfolded multimodal reasoning and action steps for multi-hop document understanding (bar charts) in Figure 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 20 .Figure 21 .</head><label>2021</label><figDesc>Figure 20. Unfolded multimodal reasoning and action steps for document understanding (flowchart) in Figure 10.</figDesc><graphic url="image-366.png" coords="25,320.06,590.65,213.64,88.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 22 .</head><label>22</label><figDesc>Figure 22. Unfolded multimodal reasoning and action steps for example of scene text reasoning in Figure 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 24 .</head><label>24</label><figDesc>Figure 24. Case studies of MM-REACT's extensibility. With the new release of GPT-4 (Language-only), we can upgrade ChatGPT in MM-REACT to GPT-4 (Langauge-only) and further improve the results of MM-REACT.</figDesc><graphic url="image-437.png" coords="29,69.40,396.33,128.00,181.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 25 .</head><label>25</label><figDesc>Figure 25. Case studies of MM-REACT's extensibility. MM-REACT is with the flexibility to in new experts. In this example, MM-REACT is enhanced with the image editing model from X-decoder<ref type="bibr" target="#b37">[38]</ref> and can perform dialogue-based image editing.</figDesc><graphic url="image-447.png" coords="30,98.86,410.94,135.30,135.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-429.png" coords="28,25.61,10.80,288.00,259.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Recent vision-language studies</figDesc><table><row><cell>image caption</cell><cell></cell><cell cols="2">wait for user input image as &lt;path&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>This is an image.</cell></row><row><cell>image tagging</cell><cell></cell><cell></cell><cell></cell></row><row><cell>dense caption</cell><cell>output as text observation</cell><cell cols="2">ChatGPT responds</cell></row><row><cell>celebrity</cell><cell></cell><cell></cell><cell></cell></row><row><cell>receipt</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OCR</cell><cell cols="3">thought &amp; action request?</cell></row><row><cell>Bing search</cell><cell>yes</cell><cell></cell><cell>no</cell><cell>Object Tagger List of object tags seen in this image: sport, athletic game, basketball, person, sports uniform, ball,</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>sportswear, basketball player, basketball moves, sports equipment, basketball court ?</cell></row><row><cell>vision experts</cell><cell cols="2">parse &lt;model&gt; and &lt;path&gt; execute the action</cell><cell>respond to user</cell><cell>Face Detection Detected faces, and their location in this image: man (x1, y1, x2, y2) = (365, 278, 439, 352) man (x1, y1, x2, y2) = (548, 308, 620, 380)</cell></row></table><note><p><p><p><p>Assistant, what objects do you see in this image? &lt;ImagePath&gt; Thought Action request Observation Image Captioning This is an image (694 x 720) with description a basketball player in a yellow jersey with a ball in his hand. Dense Captioning List of object descriptions, and their locations in this image: a close up of a basketball (x1, y1, x2, y2) = (63, 535, 190, 663) a basketball player in a yellow jersey (x1, y1, x2, y2) = (55, 19, 502, 700) ? Invoke vision experts over &lt;ImagePath&gt; Action execution Figure 2. Flowchart of MM-REACT for enhanced visual understanding with ChatGPT.</p>The user input can be in the form of text, images, or videos, with the latter two represented as file path strings. ChatGPT is instructed to say specific watchwords in action request if a vision expert is required to interpret the visual inputs. Regular expression matching is applied to parse the expert's name and the file path, which are then used to call the vision expert (action execution). The expert's output (observation) is serialized as text and combined with the history to further activate ChatGPT. If no extra experts are needed, MM-REACT would return the final response to the user. The right figure shows a single-round vision expert execution, which is the component that constructs the full execution flow illustrated in Figure</p>3</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>dense captioning, object tagger, face detection. Observation 1 Image Captioning</head><label></label><figDesc>Assistant: This is an image (444 x 444) with description a cartoon of a man and a woman. The text in the image is a conversation between two people. One person says they don't feel good and need to buy a test. The other person asks if the first person is pregnant or has the coronavirus. The text is likely part of a comic or cartoon illustration. The image is tagged with "@INNA_S_ART", which may indicate the artist or creator of the image.</figDesc><table><row><cell></cell><cell></cell><cell>Thought 2</cell><cell>ChatGPT: Yes, there is text in the</cell></row><row><cell></cell><cell></cell><cell></cell><cell>image.</cell></row><row><cell></cell><cell></cell><cell>Action 2</cell><cell>Assistant, what is the OCR texts in</cell></row><row><cell></cell><cell></cell><cell></cell><cell>this image? &lt;ImagePath&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Invoke OCR</cell></row><row><cell></cell><cell></cell><cell>Observation 2</cell><cell>OCR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>This image contains handwritten text</cell></row><row><cell></cell><cell></cell><cell></cell><cell>List of texts (words) seen in this</cell></row><row><cell></cell><cell></cell><cell></cell><cell>image:</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BEFORE ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>I don't feel good,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>I need to buy</cell></row><row><cell>Action 1</cell><cell>Assistant, what objects do you see in</cell><cell></cell><cell>the test</cell></row><row><cell></cell><cell>this image? &lt;ImagePath&gt;</cell><cell></cell><cell>NOW ?</cell></row><row><cell></cell><cell>Invoke vision experts, including</cell><cell></cell><cell>I don't feel good,</cell></row><row><cell></cell><cell>image captioning, Dense Captioning</cell><cell>ChatGPT:</cell><cell>I need to buy the test @INNA_S_ART Are you pregnant? Do you have the coronavirus ?</cell></row><row><cell></cell><cell>List of object descriptions, and their</cell><cell></cell></row><row><cell></cell><cell>locations in this image:</cell><cell></cell></row><row><cell></cell><cell>a cartoon of a woman with a green</cell><cell></cell></row><row><cell></cell><cell>face (x1,y1,x2,y2) = (11, 152, 108, 344)</cell><cell></cell></row><row><cell></cell><cell>a cartoon of a woman with a green</cell><cell></cell></row><row><cell></cell><cell>face (x1,y1,x2,y2) = (230, 155, 328,</cell><cell></cell></row><row><cell></cell><cell>349)</cell><cell></cell></row><row><cell></cell><cell>?</cell><cell></cell></row><row><cell></cell><cell>Object Tagger</cell><cell></cell></row><row><cell></cell><cell>List of object tags seen in this image:</cell><cell></cell></row><row><cell></cell><cell>text, cartoon, animated cartoon,</cell><cell></cell></row><row><cell></cell><cell>comics, human face, fiction,</cell><cell></cell></row><row><cell></cell><cell>illustration, clipart, comic, funny</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>:</head><label></label><figDesc>This is an image. The average total fueling cost excluding the Ford F150 (Regular Cab) is $76.55. This is calculated by taking the sum of the total fueling cost of Toyota RAV4 and Honda Civic and dividing it by 2. ($82.56 + $70.55) / 2 = $76.55. This image is a chart of the total charging cost of different electric vehicles. The electric vehicles are Ford F150 Lightning (Estimate), Rivian R1T/S, and Tesla Model 3 (Standard Range). The total charging cost of Ford F150 Lightning (Estimate) is $22.76, the total charging cost of Rivian R1T/S is $31.35, and the total charging cost of Tesla Model 3 (Standard Range) is $12.54.</figDesc><table><row><cell></cell><cell>ChatGPT</cell></row><row><cell cols="2">Convert image to &lt;ImagePath&gt; and</cell></row><row><cell cols="2">send to ChatGPT</cell></row><row><cell cols="2">Thought 1 ChatGPT: Action 1 Assistant, what objects do you see in</cell></row><row><cell></cell><cell>this image? &lt;ImagePath&gt;</cell></row><row><cell></cell><cell>Invoke vision experts, including image</cell></row><row><cell></cell><cell>captioning, dense captioning, object</cell></row><row><cell></cell><cell>tagger.</cell></row><row><cell cols="2">Observation 1 Image Captioning</cell></row><row><cell></cell><cell>This is an image (410 x 593) with</cell></row><row><cell></cell><cell>description a graph of a number of cars.</cell></row><row><cell></cell><cell>Thought 3</cell></row><row><cell></cell><cell>Action 3</cell></row><row><cell></cell><cell>Observation 3</cell></row><row><cell>Thought 2</cell><cell>ChatGPT: This image is likely a bar</cell></row><row><cell></cell><cell>chart or graph.</cell></row><row><cell>Action 2</cell><cell>Assistant, what is the OCR texts in</cell></row><row><cell></cell><cell>this image? &lt;ImagePath&gt;</cell></row><row><cell></cell><cell>Invoke OCR</cell></row><row><cell>Observation 2</cell><cell>OCR</cell></row><row><cell></cell><cell>This image contains text</cell></row><row><cell></cell><cell>List of texts (words) seen in this</cell></row><row><cell></cell><cell>image:</cell></row><row><cell></cell><cell>$130.96</cell></row><row><cell></cell><cell>Ford</cell></row><row><cell></cell><cell>F150</cell></row><row><cell></cell><cell>(Regular Cab)</cell></row><row><cell></cell><cell>$82.56</cell></row><row><cell></cell><cell>Toyota</cell></row><row><cell></cell><cell>RAV4</cell></row><row><cell></cell><cell>$70.55</cell></row><row><cell></cell><cell>Honda</cell></row><row><cell></cell><cell>Civic</cell></row><row><cell></cell><cell>Total fueling cost</cell></row></table><note><p><p>Dense Captioning</p>List of object descriptions, and their locations in this image: a purple rectangular object with white border (x1,y1,x2,y2) = (152, 169, 245, 446) a purple rectangular bar chart (x1,y1,x2,y2) = (25, 135, 365, 456) ? Object Tagger List of object tags seen in this image: text, screenshot, purple, violet, graphic design, ? ChatGPT: This image is a bar chart of the total fueling cost of different cars. The cars are Ford F150 (Regular Cab), Toyota RAV4, and Honda Civic. The total fueling cost of Ford F150 (Regular Cab) is $130.96, the total fueling cost of Toyota RAV4 is $82.56, and the total fueling cost of Honda Civic is $70.55. Excluding the regular cab, what is the average total fueling cost? ChatGPT: ChatGPT: This is an image. Assistant, what objects do you see in this image? &lt;ImagePath2&gt; Invoke vision experts, including image captioning, dense captioning, object tagger. Image Captioning This is an image (400 x 297) with description a chart of cost comparison. Dense Captioning List of object descriptions, and their locations in this image: a yellow rectangle with black text (x1,y1,x2,y2) = (154, 12, 245, 150) a chart of cost per charge (x1,y1,x2,y2) = (1, 1, 391, 288) ? Object Tagger List of object tags seen in this image: text, screenshot, line, yellow, number, ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Thought 4 Action 4 Observation 4 ChatGPT:</head><label></label><figDesc>The average total charging cost excluding the Ford F150 Lightning (Estimate) is $22.44. This is calculated by taking the sum of the total charging cost of Rivian R1T/S and Tesla Model 3 (Standard Range</figDesc><table><row><cell>Excluding Ford F150 Lightning, what is the</cell></row><row><cell>average total charging cost?</cell></row><row><cell>ChatGPT:</cell></row><row><cell>Assistant, what is the OCR texts in</cell></row><row><cell>this image? &lt;ImagePath&gt;</cell></row><row><cell>Invoke OCR</cell></row><row><cell>OCR</cell></row><row><cell>This image contains text</cell></row><row><cell>List of texts (words) seen in this image:</cell></row><row><cell>$22.76</cell></row><row><cell>Ford F150</cell></row><row><cell>Lightning</cell></row><row><cell>(Estimate)</cell></row><row><cell>$31.35</cell></row><row><cell>Rivian</cell></row><row><cell>R1T/S</cell></row><row><cell>$12.54</cell></row><row><cell>Tesla</cell></row><row><cell>Model 3</cell></row><row><cell>(Standard Range)</cell></row><row><cell>Total charging cost</cell></row></table><note><p>This image is likely a chart of cost comparison.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https : / / azure . microsoft . com / en -us / products / cognitive-services/vision-services</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We would like to express our gratitude to <rs type="person">Jianfeng Gao</rs> for his valuable suggestions, as well as to <rs type="person">Jianwei Yang</rs> for generously providing the image editing tool utilizing the X-Decoder framework.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>User Answer question I.1.a. Think step-by-step. GPT-4 To answer question I.1.a, we need to express the temperature T(x) in every point of the conductive bar, which is characterized by the abscissa x. We are given that T(0) = T0 and T(L) = Tb. For a stationary regime, the heat equation for the one-dimensional heat conduction in the bar is: d?T/dx? = 0 Integrating once, we get: dT/dx = C? Integrating a second time:</p><p>= C?x + C? To determine the constants C? and C?, we use the boundary conditions: T(0) = T0, which gives C? = T0 T(L) = Tb, which gives C? = (Tb -T0) / L Finally, the expression for T(x) is: T(x) = (Tb -T0) * (x / L) + T0 This represents a linear temperature profile along the conductive bar, with a slope of (Tb -T0) / L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-4 (Multimodal) Ours w/ ChatGPT</head><p>To answer question I.1.a, one would need to use the principles of thermodynamics and heat transfer to determine the temperature at any point in the conductor bar. This would likely involve solving a differential equation to find the temperature distribution along the bar. The solution would depend on the specific properties of the bar, such as its length, cross-sectional area, and thermal conductivity, as well as the temperature of the bolometer and the temperature of the thermostat. The solution would also depend on the power of the incident radiation and the current of polarization. Once the temperature distribution is found, the temperature at any point in the bar can be determined by evaluating the solution at that point.</p><p>Answer question I.1.a. Think step-by-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours w/ GPT-4 (Language-only)</head><p>To answer question I.1.a, we'll consider the heat conduction equation along the thermal conductor bar. We're working under the assumptions of stationary mode and one-dimensional heat conduction with no lateral heat exchanges. Start with Fourier's Law of heat conduction: q = -k * (dT/dx) where q is the heat flow (W/m?), k is the thermal conductivity (W/m?K), and dT/dx is the temperature gradient along the x-axis. In a stationary regime, the heat flow q is constant throughout the bar. Thus, we have: -k * (dT/dx) = constant Integrate the equation with respect to x: ?(dT/dx) dx = ?(constant/k) dx After we get: T(x) = (constant/k) * x + C where C is the integration constant.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Michael Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keerthana</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><surname>Herzog</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01691</idno>
		<title level="m">Do as i can, not as i say: Grounding language in robotic affordances</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14198</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><surname>Langchain</surname></persName>
		</author>
		<ptr target="https://langchain.readthedocs.io/,2023.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yantao</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Scaling vision transformers to 22 billion parameters</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05442</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Palm-e: An embodied multimodal language model</title>
		<author>
			<persName><forename type="first">Danny</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayzaan</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><surname>Florence</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03378</idno>
		<imprint>
			<date type="published" when="1920">2023. 2, 3, 6, 20</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10435</idno>
		<title level="m">Pal: Program-aided language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09699</idno>
		<title level="m">Promptcap: Prompt-guided taskaware image captioning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language is not all you need: Aligning perception with language models</title>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owais</forename><surname>Khan Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.14045</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language models as zero-shot planners: Extracting actionable knowledge for embodied agents</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno>PMLR, 2022. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="9118" to="9147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The mir flickr retrieval evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM international conference on Multimedia information retrieval</title>
		<meeting>the 1st ACM international conference on Multimedia information retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11916</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learn to explain: Multimodal reasoning via thought chains for science question answering</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.09513</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09332</idno>
		<title level="m">Browser-assisted question-answering with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Dess?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04761</idno>
		<title level="m">Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Vipergpt: Visual inference via python execution for reasoning</title>
		<author>
			<persName><forename type="first">D?dac</forename><surname>Sur?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08128</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Interleaving retrieval with chain-ofthought reasoning for knowledge-intensive multi-step questions</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10509</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multimodal fewshot learning with frozen language models</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13884</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Git: A generative image-to-text transformer for vision and language</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14100</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models with image descriptors are strong few-shot video-language learners</title>
		<author>
			<persName><forename type="first">Zhenhailong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Visual chatgpt: Talking, drawing and editing with visual foundation models</title>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zecheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04671</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An empirical study of gpt-3 for few-shot knowledge-based vqa</title>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3081" to="3089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03629</idno>
		<title level="m">React: Synergizing reasoning and acting in language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Socratic models: Composing zero-shot multimodal reasoning with language</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Welker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aveek</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00598</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multimodal chain-ofthought reasoning in language models</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00923</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Generalized decoding for pixel, image and language</title>
		<author>
			<persName><forename type="first">Xueyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
