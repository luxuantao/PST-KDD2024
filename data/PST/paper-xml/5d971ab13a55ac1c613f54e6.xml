<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-Attentive Pyramidal Model for Visual Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaohao</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huijun</forename><surname>Zhang</surname></persName>
							<email>zhang-hj17@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ningyun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Feng</surname></persName>
							<email>fengling@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Zheng</surname></persName>
							<email>zhengf@sustech.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Multi-Attentive Pyramidal Model for Visual Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Sentiment Analysis</term>
					<term>Pyramidal Pooling</term>
					<term>Self-Attention Mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual sentiment analysis aims to recognize emotions from visual contents. It is a very useful yet challenging task, especially when fine-grained emotions (such as love, joy, surprise, sadness, fear, anger, disgust, and anxiety) are analyzed. Existing methods based on convolutional neural networks learn sentiment representations based on global visual features, while ignoring the fact that both the local regions of the images and their relationships can have impact on sentiment representation learning. To address this limitation, in this paper, we propose a new Multi-Attentive Pyramidal model (MAP) for visual sentiment analysis. The model performs pyramidal segmentation and pooling upon the visual feature blocks obtained from a fully convolutional network, aiming to extract local visual features from multiple local regions at different scales of the global image. It then implants a self-attention mechanism to mine the associations between local visual features, and achieves the final sentiment representation. Extensive experiments on six benchmark datasets show the proposed MAP model outperforms the state-of-the-art methods in visual sentiment analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual sentiment analysis aims to recognize the emotions expressed by users behind visual contents such as pictures and videos. In recent years, with the rise of social networks such as Instagram and Twitter, the visual contents uploaded by users on the Internet have increased day by day. Therefore, the study of visual sentiment analysis has gradually attracted attention from academic and industrial communities. Automatic understanding of the users' emotions hidden behind pictures or videos has many applications, such as online advertisement, customer feedbacks, and election grievance estimation, etc.</p><p>Existing Solutions. Compared to textual sentiment analysis, visual sentiment is more implicit and subjective, and analyzing visual sentiment appears to be a more challenging task, specially when fine-grained emotions (such as love, joy, surprise, sadness, fear, anger, disgust, and anxiety) are considered <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Traditional studies on visual sentiment analysis are mainly based on hand-crafted features, such as global and local RGB histograms <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, texture features <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, shape features <ref type="bibr" target="#b7">[8]</ref>, Wiccest and Gabor features <ref type="bibr" target="#b5">[6]</ref>, SIFT-based bags of features <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, Gist features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and middle attribute Corresponding author: Xiaohao He, hexh17@mails.tsinghua.edu.cn level features like the visual sentiment taxonomy of "adjectivenoun pairs (ANP)" <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Fig. <ref type="figure">1</ref>. Images from the EmotionROI dataset. Image (b) -"flowers are for the girl", is labeled as surprise emotion. Two bounding boxes correspond to two emotional regions, the left region revealing love emotion, and the right region revealing joy emotion. Associating the two local regions, it is easy to recognize surprise emotion. Image (c) -"dirty plates and man wearing a mask" is labeled as disgust emotion. Associating the two regionsman wearing mask and dirty plates together enables to detect the correct visual sentiment.</p><p>In recent years, the rise of convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> has made it possible to easily and automatically extract deep local and global sentiment features from large-scaled pictures. These CNN-based deep methods <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b18">[19]</ref> avoid the labor intensive feature engineering, and appear to be more effective, scalable, and robust than the traditional approaches.</p><p>Despite the advantage, most existing methods are based on the entire image to learn the emotional representation. However, psychology-related experiments <ref type="bibr" target="#b19">[20]</ref> have proved that the visual sentiment representation is more related to the local regions rather than the entire image. Moreover, the associations across deep key features learned from different multi-scale local regions are also important for visual sentiment analysis. Take the image "flowers are for the girl" in the EmotionROI dataset for example (Fig. <ref type="figure">1</ref>). The image is labeled as surprise emotion. Two bounding boxes correspond to two emotional regions, the left region revealing love emotion, and the right region revealing joy emotion. Through associating the two local regions, we can precisely sense the surprise emotion.</p><p>To this end, we propose a new Multi-Attentive Pyramidal model called MAP, which extracts not only visual features of multiple local regions at multi-scales of an image, but also captures their associations in learning sentiment representation.</p><p>Our Work. In the MAP model, a sentiment image is firstly fed into a backbone convolutional network (e.g., VGG-19 <ref type="bibr" target="#b20">[21]</ref>, ResNet-101 <ref type="bibr" target="#b21">[22]</ref>) to obtain the basic visual feature maps. Next, pyramidal segmentation and pooling operations are performed upon each feature map to extract local visual features from multiple local regions at multiple scales of the original image.</p><p>Lastly, inspired by the recent Transformer <ref type="bibr" target="#b22">[23]</ref> model in dealing with a number of difficult NLP tasks such as machine translation successfully without attaching to any traditional neural networks, we implant a self-attention mechanism in the MAP model to capture the associations between the local features for the final sentiment representation.</p><p>In the field of visual sentiment analysis, there are two types of datasets: large-scale datasets and small-scale datasets. Large-scale datasets are usually weakly labeled, and therefore full of noises, while the small-scale datasets are usually labeled by humans, and relatively more accurate. We conduct extensive experiments on both types of benchmark datasets to evaluate the accuracy as well as the robustness of the proposed MAP model.</p><p>In summary, the contributions of our paper are three-fold.</p><p>• We present a new multi-attentive pyramidal model called MAP for visual sentiment analysis, which can not only extract visual features from local regions at multiple scales, but also grasp their associations in learning visual sentiment representations. • We conduct extensive experiments on both large-scale and small-scale benchmark datasets to evaluate the performance and demonstrate the accuracy as well as the robustness of the proposed model. • To the best of our knowledge, this is the first attempt to exploit self-attention mechanism in the field of visual sentiment analysis, demonstrating its advantage and applicability in dealing with both linguistic and visual contents. The remainder of the paper is organized as follows. We review some closely related work in Section II, and detail the proposed MAP model in Section III. We report the performance of the model in Section IV, and conclude the paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Sentiment analysis is a very important and challenging task <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Most of the previous research has focused on detecting the sentiment polarity or opinion (positive, neutral, or negative) of subjective elements within a text through text analysis, and computational linguistics techniques, and achieved good results <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>. In the field of computer vision (CV), there are also some good research on visual sentiment analysis, which are mainly based on the hand-crafted features, including global and local RGB histograms <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, texture features <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, shape features <ref type="bibr" target="#b7">[8]</ref>, Wiccest and Gabor features <ref type="bibr" target="#b5">[6]</ref>, SIFT-based bags of features <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, Gist features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and so on. Besides, some works have tried to utilize middle attribute level features like the visual sentiment taxonomy of "adjective-noun pairs (ANP)" to improve classification results <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Deep learning, especially the rise of convolutional neural networks (CNNs), makes it possible to extract deep features from visual contents automatically without labor-intensive feature engineering. The CNN-based visual sentiment classification methods exhibit higher performance and robustness than the traditional hand-crafted feature based methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, making fundamental changes in the research of visual sentiment analysis fields.</p><p>Motivated by the psychological and neuroscientific findings that human's visual attention is attracted to the most informative regions <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>, some studies recently attempted to look into local areas of an image rather than the whole image. Local sentiment-related features from each region are incorporated for visual sentiment analysis <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. However, the way they automatically consider the local sentiment-related areas of the image is a weakly supervised method, which means the local areas are not truly stripped from the original images and their effects on sentiment are not considered separately, leading to a lot of noises. Moreover, associations across different image local regions at multiple scales have not been explored so far. We believe the later plays an equally important role in assessing the complex emotion revealed from an image.</p><p>Different from the existing approaches, our proposed MAP model aims to extract visual features of local image regions by directly subdividing the entire visual feature blocks in a pyramidal way, following by corresponding pooling strategy. Moreover, inspired by the recent Transformer <ref type="bibr" target="#b22">[23]</ref> model in dealing with a number of difficult NLP tasks such as machine translation successfully, we implant a self-attention mechanism in the model to mine the associations between the above extracted local visual regions before obtaining the final sentiment representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL</head><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the Multi-Attentive Pyramidal model (MAP). We input the images with sentiment labels and get their sentiment categories as output. For an image, after going through a fully convolutional network (FCN) ResNet-101 <ref type="bibr" target="#b21">[22]</ref>, the model acquires its basic visual information and generates a number of feature maps. From each feature map, the model extracts the local visual features from multiple local regions (segments) through pyramidal segmentation and pooling, and then mine their associations through self-attention to learn the visual sentiment representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pyramidal Module</head><p>According to our observation, the sentiment polarities of most sentiment images are decided by several key regions in the images. Therefore, through the pyramid segmentation, we divide each feature map into different sizes through different methods and then unify them into the same dimensions   through the pooling operations, aiming at obtaining the feature information of various local regions.</p><p>Use M to represent the feature maps, where M ∈ R H×W ×C . We do pyramidal segmentation for the first two dimensions of feature maps, not on the third dimension which represents channels. Therefore, we provide three kinds of segmentation methods respectively in the first dimension, the second dimension and both the first and second dimension, named as horizontal segmentation, vertical segmentation and mixed segmentation.</p><p>Define the maximum number of slices p H and p W to respectively represent how many pieces are supposed to be split in the first and second dimension of feature maps, where</p><formula xml:id="formula_0">H/p H ∈ Z + (0 &lt; p H &lt; H) and W/p W ∈ Z + (0 &lt; p W &lt; W ).</formula><p>Take the example shown in Figure <ref type="figure" target="#fig_2">2</ref>, p H = 4 in horizontal segmentation and p W = 4 in vertical segmentation.</p><p>We take horizontal segmentation as the example to introduce our segmentation rules. For maximum number of slices p H , a feature map can be split into p H pieces. The size of each piece is</p><formula xml:id="formula_1">H p H × W × C. 1) We split a feature map into p H kinds of size S H = {Size 1 , Size 2 , • • • , Size p H }, where Size i ∈ R (i× H p H )×W ×C , i ∈ 1, 2, • • • , p H .</formula><p>2) For the i-th kinds of segment size Size i , we split a feature map into N i pieces with the size of Size i , through the way of sliding from 0-th feature in steps of 1 × H p H on the first dimension of a feature map. </p><formula xml:id="formula_2">N i = p H − i + 1<label>(1</label></formula><formula xml:id="formula_3">f num = (1 + p H ) × p H 2<label>(2)</label></formula><p>Because the second dimension size of M L is 1, we reshape M L with the size of f num × C.</p><p>The vertical segmentation method is the same as horizontal segmentation except for operating on the second dimension.</p><p>For mixed segmentation, it can only be applied when</p><formula xml:id="formula_4">p H = p W . In S M , Size i ∈ R (i× H p H )×(i× W p W )×C , i ∈ 1, 2, • • • , p H .</formula><p>The sliding way of each certain size Size i is that, for each step of 1 × H p H on the first dimension, it finishes sliding from 0-th to the W − i × W p W in steps of 1 × W p W on the second </p><formula xml:id="formula_5">f num = (2 × p H + 1) × (1 + p H ) × p H 6<label>(3)</label></formula><p>In each experiment, we only use a segmentation method to process the original feature maps. All of our experiments in this paper are based on horizontal segmentation, which is described in detail in the Experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self Attention Module</head><p>We get a new local feature map M L , consisting of local features obtained in pyramidal segmentation. However, the association between these local regions are still not explored. Inspired by the recent Transformer model <ref type="bibr" target="#b22">[23]</ref>, we implant a self attention module adapted to image features. Through the self attention module, we mine the association between each two local features to make the model learn the sentiment representation of the images better.</p><p>First, we multiply M L by its transpose M T L . Through a softmax function, we get the relationship matrix, where each value represents the association between the corresponding two local features. Moreover, we multiply the relationship matrix by M L again to get the attention weighted sums different local regions.</p><formula xml:id="formula_6">R = sof tmax( M L M T L √ f num ) M L (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where M L ∈ R fnum×C and R ∈ R fnum×C . We add R to M L as residual connection <ref type="bibr" target="#b21">[22]</ref>, followed by batch normalization <ref type="bibr" target="#b31">[32]</ref>. Then we feed the result into a fully connected feed-forward network, which also contains a residual connection followed by batch normalization.</p><formula xml:id="formula_8">U = BatchN orm(R + M L ) (5) S = BatchN orm(ReLU (U W a + b a ) + U)<label>(6)</label></formula><p>where W a ∈ R C×C , b a ∈ R fnum×C are trainable parameters with activation function ReLU , BatchN orm means batch normalization, and S ∈ R fnum×C as the output of a single self attention module. As shown in Figure <ref type="figure" target="#fig_2">2</ref>, for the self attention module, we repeat N s calculations and get the final output S, where S ∈ R fnum×C . Therefore, we get a new sentiment feature representation S containing both the information of local features and the association among features.</p><p>We reshape the sentiment feature representation S to 1dimension S as following:</p><formula xml:id="formula_9">S ∈ R fnum×C −→ s ∈ R h , where h = f num × C.</formula><p>Then we input the results s into the linear layer and the softmax layer in turn, and get the final classification result.</p><formula xml:id="formula_10">P (y = c | s, W) = exp(w T c s) Cnum c =1 exp(w T c s)<label>(7)</label></formula><p>where C num is the class number of the affective dataset, W = {w c } Cnum c=1 ∈ R Cnum×h are trainable parameters of the final linear layer.</p><p>In the final linear layer above, we need to carry out the learning process through minimizing the following log likelihood function:</p><formula xml:id="formula_11">L cls = − 1 N N i=1 C c=1 1(y i = c) logP (y i = c | s, W) (8)</formula><p>IV. EXPERIMENT</p><p>In this section, we conduct experiments on six public datasets to compare the performance of our proposed framework MAP with eight existing methods, including traditional hand-crafted visual features based solutions, attention mechanism based solutions and recent CNN deep feature based solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The experiments are conducted on six public datasets.</p><p>• EmotionROI dataset <ref type="bibr" target="#b19">[20]</ref> consists of 1980 images collected from Flickr with 6 emotion keywords as queries.</p><p>The sentiment label of each image serves as the corresponding emotion keyword. The training/testing split is 70% and 30%, respectively. • Twitter I <ref type="bibr" target="#b14">[15]</ref> is a dataset including three versions ("three", "four", "five" agree), in which each image is labelled by 5 AMT workers. "x agree" indicates that more than x number of AMT workers give the same label to the sample. In total, there are 1269 images labelled as "three agree", 1116 images labelled as "four agree" and 882 images labelled as "five agree". The training/testing split is 80% and 20%, respectively. • Twitter II <ref type="bibr" target="#b10">[11]</ref>  with sentiment polarity (i.e. positive, negative) labels by crowd-sourcing based human annotation. The first three datasets are small-scale datasets, while the last three are large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compared Methods</head><p>The following eight comparable methods on visual sentiment analysis are taken as the reference of MAP. • SentiBank <ref type="bibr" target="#b10">[11]</ref> uses 1200 dimensional mid-level representation with the ANP detector of SentiBank and finally classifies the images with a linear SVM <ref type="bibr" target="#b34">[35]</ref>.</p><p>• DeepSentiBank <ref type="bibr" target="#b35">[36]</ref> classifies the images with a Linear SVM <ref type="bibr" target="#b34">[35]</ref> based on 2,089 dimensional features which are extracted by the pre-trained DeepSentiBank. • Zhao's Method <ref type="bibr" target="#b3">[4]</ref> extracts emotion features based on principles-of-art (PAEF) for the image emotion classification and scoring task. • Fine-tuned AlexNet <ref type="bibr" target="#b13">[14]</ref>, VGG16 <ref type="bibr" target="#b20">[21]</ref>, Inception-V3 <ref type="bibr" target="#b36">[37]</ref>, ResNet101 <ref type="bibr" target="#b21">[22]</ref> replaces the last fully connected layer, swaps it to the output of the corresponding sentiment class number of datasets, and then tunes the parameters of all layers. • Self-Attention <ref type="bibr" target="#b22">[23]</ref> is a variant derived from the Transformer model. Transformer is a simple network architecture that based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. It can effectively mine the relationship between any two of the input elements, and draw the attention they should be assigned between the two. During the experiment, we first compress the input image into a feature matrix with single channel using a 1 × 1 convolutional layer, and then directly input this feature matrix into the self-attention module we used in MAP and get the final sentiment representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We implement the fully convolutional network (FCN) using ResNet-101 <ref type="bibr" target="#b21">[22]</ref>, whose performance is among the best in the most influential competition, ImageNet Challenge <ref type="bibr" target="#b37">[38]</ref>, in the field of computer vision. Detail settings of MAP on the six datasets are presented in Table <ref type="table" target="#tab_2">I</ref>.</p><p>Upon each training process, to prevent over-fitting, we generate more images from the training images through random resizing, 512×512 cropping, flipping, and normalization. In the test phase, we resize each image into 560×560, and crop it with the size of 512×512 from the center, followed by the image normalization process.</p><p>The parameters of MAP are initialized with the convolutional neural network parameters, pre-trained based on the Im-ageNet Challenge dataset. As for the optimization algorithm, we use a weight decay of 0.0005 with a momentum of 0.9, and fine-tune all layers with stochastic gradient descent(SGD). The learning rates of all the convolutional layers in small-scale and large-scale datasets are respectively initialized as 0.001 and 0.0001. In addition, the learning rates of the self attention module and the final linear layer in small-scale and large-scale datasets are respectively initialized as 0.01 and 0.001. All the learning rates are divided by 10 every 10 epochs. In order to make MAP fully converge, we train our model in total of 40 epochs in Twitter I, Twitter II and EmotionROI, 120 epochs in Flickr &amp; Instagram, Flickr and Instagram.</p><p>The Flickr &amp; Instagram datasets are split randomly into 80% training and 20% testing sets. For the Flickr and Instagram datasets, we randomly split each dataset into 90% training and 10% testing sets. The small-scale datasets are split into 80% training and 20% testing sets randomly except those with specified training/testing splits.</p><p>Our implementation is based on the deep learning framework PyTorch <ref type="bibr" target="#b38">[39]</ref>. All the experiments are conducted on two NVIDIA GTX Titan X GPU with 24 GB on-board memory in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head><p>1) Performance of Sentiment Classification: From the result presented in Table <ref type="table" target="#tab_4">II</ref>, we can find that the proposed MAP is significantly better than the state-of-the-art computer vision algorithms and other existing methods on all six benchmark datasets, with accuracy improvement from 4% to 8%. Compared with state-of-the-art vision algorithms including finetuned AlexNet, VGG16, Inception-V3 and ResNet101, the performance of our proposed model is greatly improved, which proves that the self-attention mechanism in our model can effectively mine the association between sentiment relevant local regions and make more accurate sentiment predictions. Moreover, compared with directly using the self-attention mechanism for images, our model can also greatly improve the effect, which proves that our pyramid mechanism can effectively extract the local area information of the image and prevent the network disorder from learning attention information of unrelated areas.</p><p>Notice that we not only experimented on small-scale datasets, but also perform good experiments on large-scale datasets, which are usually labeled weakly and therefore full of noise. The results show the robustness our proposed model that is able to tolerate the unavoidable noise inside the largescale datasets.</p><p>Moreover, Figure <ref type="figure" target="#fig_6">3</ref> shows the confusion matrix on each dataset, which proves that MAP performs well on both twoclass and multi-class datasets.</p><p>2) Effects of Pyramidal Segmentation Methods: In the Framework section we introduced three pyramidal segmentation methods. We experimented with these methods respectively and the different combinations between them. As the results presented in Table <ref type="table" target="#tab_5">III</ref>, the best results are based on the horizontal segmentation method, followed by vertical segmentation, and finally mixed segmentation. This should be related to the distribution of the location of objects in the datasets. It is worth noting that when we use the combination of different segmentation methods to perform pyramidal segmentation, the final results are worse than any single segmentation method. This is due to the large number of segmentation blocks, resulting in the invalid allocation of the network's attention to different local regions and therefore reduced classification accuracy. So in our best performing model, we only use horizontal segmentation for pyramidal segmentation module.</p><p>3) Effects of Dataset Quality: One of the biggest challenges for visual sentiment analysis is that there is no absolute Ground Truth. Even for the same image, different people have different understandings. Therefore, there is a certain amount of noise in the datasets. For instance, in the Twitter I dataset, there are   three versions -including "three agree", "four agree", and "five agree". The more the number of agreements, the less noise the dataset has. We test our MAP on the above three datasets, and the results in Table <ref type="table" target="#tab_8">IV</ref> indeed demonstrate that as the noise of the dataset decreases, the classification performance of MAP will increase, which proves the robustness and effectiveness of our framework. 4) Effects of p H : As shown in Figure <ref type="figure" target="#fig_8">4</ref> (a), we explore the effects of different minimum block sizes in the pyramidal module on the final sentiment prediction. As the results show, when the number of blocks is too small (e.g. p H = 2), which means the original feature block is only coarse-grained separated in other words, the experimental effect is the worst, especially more serious in the case of multiple sentiment predictions. This is because the segmentation of the feature blocks is very rough at this time, which is almost equivalent to using the original feature maps, and the information of local areas cannot be efficiently extracted. At the same time, we also noticed that as the number of blocks increases, the results do not change very much, showing a trend of decreasing first and then increasing again, which can be understood as when the fragmentation of local areas is sufficiently fine-grained, the self-attention mechanism can effectively mine the association between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Effects of the Number of Self Attention Modules:</head><p>As shown in Figure <ref type="figure" target="#fig_8">4</ref> (b), we also explore the effects of the number of self-attention modules we used in our experiments on the final prediction results. It can be seen that with the superposition of the self-attention layer, the effect of the final algorithm is declining. We analyzed that this should be due to after repeated attention-related calculation, the final sentiment representation have lost its original design intention, which means network fails to give reasonable attention to different feature blocks of local regions. So in most of our bestperforming results, only a single self-attention module was superimposed.</p><p>6) Visualization of Attention Distribution: As shown in Figure <ref type="figure" target="#fig_5">5</ref>, we visualize the attention distribution of an image in       "H", "V" AND "M" RESPECTIVELY REPRESENT "HORIZONTAL SEGMENTATION", "VERTICAL SEGMENTATION" AND "MIXED SEGMENTATION".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approaches</head><p>Twitter  network automatically learns to put more attention on the frog to make the correct emotional prediction, which further proves the effectiveness of the self attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Based on the fact that visual sentiments are highly correlated with multiple potential local regions of the image, in this paper, we propose a multi-attentive pyramidal model called MAP for visual sentiment analysis, which can not only extract visual features of local regions but also mine the association between them for learning sentiment representation. Extensive experiments on both large-scale and small-scale datasets demonstrate the accuracy as well as the robustness of our proposed model. To the best of our knowledge, this is the first attempt to exploit self-attention mechanism in the field of visual sentiment analysis, demonstrating its advantage and applicability in dealing with both linguistic and visual contents. In the future, we plan to extend the model to cope with multi-modality data such as textual and visual contents for sentiment analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>IJCNN 2019 . 2 -</head><label>20192</label><figDesc>International Joint Conference on Neural Networks. Budapest, Hungary. 14-19 July 2019 paper N-19401.pdf -Authorized licensed use limited to: Tsinghua University. Downloaded on December 30,2022 at 13:53:51 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>H</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. This is the architecture of our proposed MAP model. Images with sentiment labels are first fed into the fully convolutional network (FCN). Visual feature maps are extracted and then processed by the pyramidal module. Each feature map is divided into several blocks of different size through horizontal, vertical or mixed segmentation as shown inside the pyramidal module. After pooling operations on these blocks, we extract local features from multiple local regions. Through the self attention module, we learn the association between the local features. Finally, we obtain the final sentiment classification results through the linear classification layer.</figDesc><graphic url="image-9.png" coords="3,112.85,610.93,97.92,57.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>)</head><label></label><figDesc>For example, as the above sub-figure shown in the middle of Figure 2, when p H = 4 and i = 3, a feature map can be divided into two sub-feature-maps with the size of (3 × H 4 ) × W × C. The first dimensions of each sub-feature-map is respectively in range of [0, 3 × H 4 ) and [1 × H 4 , H). 3) For each sub-feature-map we have obtained, through average pooling operations we transform each subfeature-map to a new matrix with the size of 1 × 1 × C. Concatenate these new matrices in the first dimension and then we get the local feature map M L , where M L ∈ R fnum×1×C .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>IJCNN 2019 .</head><label>2019</label><figDesc>International Joint Conference on Neural Networks. Budapest, Hungary. 14-19 July 2019 paper N-19401.pdf -4 -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>A 5 -</head><label>5</label><figDesc>Multi-Attentive Pyramidal Model for Visual Sentiment Analysis paper N-19401.pdf -Authorized licensed use limited to: Tsinghua University. Downloaded on December 30,2022 at 13:53:51 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Confusion matrix on Instagram, Flickr, Twitter I, Twitter II, EmotionROI and Flickr &amp; Instagram dataset respectively.</figDesc><graphic url="image-15.png" coords="6,110.93,336.31,188.33,125.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( a )</head><label>a</label><figDesc>Effects of pH (b) Effects of self attention modules' number</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance of different p H and number of self attention modules.</figDesc><graphic url="image-25.png" coords="7,67.07,415.21,390.16,133.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of attention distribution over different local regions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>dimension. The f num of mixed segmentation is calculated as following:</figDesc><table /><note>A Multi-Attentive Pyramidal Model for Visual Sentiment Analysis paper N-19401.pdf -3 -</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I THESE</head><label>I</label><figDesc>ARE DETAILED SETTINGS OF MAP ON SIX DIFFERENT DATASETS. MAP ADOPTS THE CONVOLUTIONAL NEURAL NETWORK RESNET-101 AND USE OUTPUT LAYER CONV4 X LAYER. FOR PYRAMIDAL SEGMENTATION METHODS, "H", "V" AND "M" RESPECTIVELY REPRESENT "HORIZONTAL SEGMENTATION", "VERTICAL SEGMENTATION" AND "MIXED SEGMENTATION". PARAMETER p REPRESENTS PIECE SIZE IN PYRAMIDAL SEGMENTATION AND SELF ATTENTION LAYERS MEAN THE NUMBER OF SELF ATTENTION LAYERS SET IN OUR MODEL.</figDesc><table><row><cell>MAP</cell><cell>Twitter I</cell><cell>Twitter II</cell><cell cols="2">Settings EmotionROI Flickr &amp; Instagram</cell><cell>Flickr</cell><cell>Instagram</cell></row><row><cell>Fully Convolutional Network (FCN)</cell><cell></cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Encoded Channels</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>Pyramidal segmentation methods</cell><cell></cell><cell>H</cell><cell></cell><cell></cell><cell>H</cell><cell></cell></row><row><cell>p</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell>4</cell><cell></cell></row><row><cell>Self attention layers</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>learning rate</cell><cell cols="3">0.001 on convolutional layers 0.01 on the other layers</cell><cell cols="3">0.0001 on convolutional layers 0.001 on the other layers</cell></row><row><cell>Batch Size</cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>32</cell><cell></cell></row><row><cell>Epochs</cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell>120</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>IJCNN 2019. International Joint Conference on Neural Networks. Budapest, Hungary. 14-19 July 2019</figDesc><table><row><cell>-6 -</cell><cell>paper N-19401.pdf</cell></row></table><note>Authorized licensed use limited to: Tsinghua University. Downloaded on December 30,2022 at 13:53:51 UTC from IEEE Xplore. Restrictions apply.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II THIS</head><label>II</label><figDesc>TABLE RECORDS THE CLASSIFICATION ACCURACY (%) ON THE SIX DATASETS. WE COMPARE OUR PROPOSED FRAMEWORK MAP AGAINST EIGHT BASELINE METHODS, WHICH INCLUDE METHODS BASED ON TRADITIONAL HAND-CRAFTED VISUAL FEATURES, CNN DEEP FEATURES AND SOLELY ATTENTION-RELATED FEATURES.</figDesc><table><row><cell>Approaches</cell><cell cols="3">Small-scale datasets Twitter I "three agree" Twitter II EmotionROI</cell><cell cols="3">Large-scale datasets Flickr &amp; Instagram Flickr Instagram</cell></row><row><cell>Zhao's Method [4]</cell><cell>67.92</cell><cell>67.51</cell><cell>34.84</cell><cell>46.13</cell><cell>66.61</cell><cell>64.17</cell></row><row><cell>SentiBank [11]</cell><cell>66.63</cell><cell>65.93</cell><cell>35.24</cell><cell>49.23</cell><cell>69.26</cell><cell>66.53</cell></row><row><cell>DeepSentiBank [36]</cell><cell>71.25</cell><cell>70.23</cell><cell>42.53</cell><cell>51.54</cell><cell>70.16</cell><cell>67.13</cell></row><row><cell>Fine-tuned AlexNet [14]</cell><cell>73.24</cell><cell>75.66</cell><cell>41.41</cell><cell>52.16</cell><cell>73.11</cell><cell>69.95</cell></row><row><cell>Fine-tuned VGG16 [21]</cell><cell>76.75</cell><cell>76.99</cell><cell>45.46</cell><cell>54.75</cell><cell>78.14</cell><cell>76.99</cell></row><row><cell>Fine-tuned Inception-V3 [37]</cell><cell>77.79</cell><cell>77.67</cell><cell>48.41</cell><cell>56.90</cell><cell>79.11</cell><cell>77.41</cell></row><row><cell>Fine-tuned ResNet101 [22]</cell><cell>78.13</cell><cell>78.23</cell><cell>51.60</cell><cell>57.16</cell><cell>80.03</cell><cell>79.33</cell></row><row><cell>Self-Attention [23]</cell><cell>64.71</cell><cell>77.60</cell><cell>18.44</cell><cell>24.01</cell><cell>77.54</cell><cell>77.76</cell></row><row><cell>MAP</cell><cell>86.67</cell><cell>82.40</cell><cell>60.47</cell><cell>68.13</cell><cell>80.96</cell><cell>81.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III THIS</head><label>III</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>TABLE RECORDS THE CLASSIFICATION ACCURACY (%) WITH DIFFERENT COMBINATION OF PYRAMIDAL SEGMENTATION METHODS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV CLASSIFICATION</head><label>IV</label><figDesc>ACCURACY(%) OF MAP ON THREE VERSIONS OF TWITTER I DATASET. test set of the EmotionROI dataset. It is drawn based on the result of the relationship matrix R, where too small values are removed. The darker the line, the greater the attention weight between the two local regions. It can be seen that the original picture is a frog lying on a persimmon, the emotional subject is the frog. As the red arrow shows, it can be seen that the</figDesc><table><row><cell>Twitter I</cell><cell>"three agree"</cell><cell>"four agree"</cell><cell>"five agree"</cell></row><row><cell>MAP</cell><cell>86.67</cell><cell>87.32</cell><cell>88.26</cell></row></table><note>the A Multi-Attentive Pyramidal Model for Visual Sentiment Analysis paper N-19401.pdf -7 -</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 30,2022 at 13:53:51 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGEMENT The work is supported by National Natural Science Foundation of China (61872214, 61532015, 61521002) and Chinese Major State Basic Research Development 973 Program (2015CB352301).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual sentiment prediction based on automatic discovery of affective regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rpsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2513" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analyzing and predicting sentiment of images on the social web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Minack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="715" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring principles-of-art features for image emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tat-Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Who&apos;s afraid of itten: Using the art theory of color combination to analyze emotions in abstract paintings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sartori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emotional valence categorization using holistic image features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Herbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On shape and the computability of emotions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suryanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B A</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Objectbased visual sentiment concept analysis and application</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentribute: image sentiment analysis from a mid-level perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining</title>
				<meeting>the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image sentiment prediction based on textual descriptions with adjective noun pairs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MTA</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="1115" to="1132" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
				<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust image sentiment analysis using progressively trained and domain transferred deep networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From pixels to sentiment: Finetuning cnns for visual sentiment prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual sentiment analysis by attending on local image regions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="231" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The role of visual attention in sentiment prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised coupled networks for visual sentiment analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7584" to="7592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Where do emotions come from? predicting the emotion stimuli map</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="614" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building text classifiers using positive and unlabeled examples</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Micro-blogging sentiment detection by collaborative online learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="893" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting elections with twitter: What 140 characters reveal about political sentiment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tumasjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sandner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Welpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pepe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="450" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Saliency, attention, and visual search: An information theoretic approach</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural mechanisms of selective visual attention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zirnsak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="193" to="222" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Selective visual attention to emotion</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Schupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stockburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Codispoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Junghöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Weike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1082" to="1089" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="448" to="456" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Building a large scale dataset for image emotion recognition: The fine print and the benchmark</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="308" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image sentiment analysis using latent correlations among visual, textual, and sentiment views</title>
		<author>
			<persName><forename type="first">M</forename><surname>Katsurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2837" to="2841" />
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="27" to="28" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deepsentibank: Visual sentiment concept classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8586</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Pytorch</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
