<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Convolutional Neural Network for Complex Wetland Classification Using Optical Remote Sensing Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Masoud</forename><surname>Mahdianpari</surname></persName>
							<email>m.mahdianpari@mun.ca</email>
							<idno type="ORCID">0000-0002-7234-959X</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Geodesy and Geomatics Engineering</orgName>
								<orgName type="laboratory">CRC Laboratory in Advanced Geo-matics Image Processing</orgName>
								<orgName type="institution">University of New Brunswick</orgName>
								<address>
									<postCode>E3B 5A3</postCode>
									<settlement>Fredericton</settlement>
									<region>NB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yun</forename><surname>Zhang</surname></persName>
							<email>yunzhang@unb.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Geodesy and Geomatics Engineering</orgName>
								<orgName type="laboratory">CRC Laboratory in Advanced Geo-matics Image Processing</orgName>
								<orgName type="institution">University of New Brunswick</orgName>
								<address>
									<postCode>E3B 5A3</postCode>
									<settlement>Fredericton</settlement>
									<region>NB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">C-CORE and Department of Electrical Engineering</orgName>
								<orgName type="institution">Memorial University of Newfoundland</orgName>
								<address>
									<postCode>A1B 3X5</postCode>
									<settlement>St. John&apos;s</settlement>
									<region>NL</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Bahram</forename><surname>Salehi</surname></persName>
							<email>bsalehi@mun.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Geodesy and Geomatics Engineering</orgName>
								<orgName type="laboratory">CRC Laboratory in Advanced Geo-matics Image Processing</orgName>
								<orgName type="institution">University of New Brunswick</orgName>
								<address>
									<postCode>E3B 5A3</postCode>
									<settlement>Fredericton</settlement>
									<region>NB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Rezaee</surname></persName>
							<email>mrezaee@unb.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Geodesy and Geomatics Engineering</orgName>
								<orgName type="laboratory">CRC Laboratory in Advanced Geo-matics Image Processing</orgName>
								<orgName type="institution">University of New Brunswick</orgName>
								<address>
									<postCode>E3B 5A3</postCode>
									<settlement>Fredericton</settlement>
									<region>NB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Convolutional Neural Network for Complex Wetland Classification Using Optical Remote Sensing Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AA061F366BAFB3FC3F16C47B14313339</idno>
					<idno type="DOI">10.1109/JSTARS.2018.2846178</idno>
					<note type="submission">received March 11, 2018; revised May 15, 2018; accepted June 7, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>AlexNet</term>
					<term>convolutional neural network (CNN)</term>
					<term>deep learning</term>
					<term>random forest (RF)</term>
					<term>spatial feature</term>
					<term>wetland mapping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The synergistic use of spatial features with spectral properties of satellite images enhances thematic land cover information, which is of great significance for complex land cover mapping. Incorporating spatial features within the classification scheme have been mainly carried out by applying just low-level features, which have shown improvement in the classification result. By contrast, the application of high-level spatial features for classification of satellite imagery has been underrepresented. This study aims to address the lack of high-level features by proposing a classification framework based on convolutional neural network (CNN) to learn deep spatial features for wetland mapping using optical remote sensing data. Designing a fully trained new convolutional network is infeasible due to the limited amount of training data in most remote sensing studies. Thus, we applied fine tuning of a pre-existing CNN. Specifically, AlexNet was used for this purpose. The classification results obtained by the deep CNN were compared with those based on well-known ensemble classifiers, namely random forest (RF), to evaluate the efficiency of CNN. Experimental results demonstrated that CNN was superior to RF for complex wetland mapping even by incorporating the small number of input features (i.e., three features) for CNN compared to RF (i.e., eight features). The proposed classification scheme is the first attempt, investigating the potential of fine-tuning pre-existing CNN, for land cover mapping. It also serves as a baseline framework to facilitate further scientific research using the latest state-of-art machine learning tools for processing remote sensing data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ETLANDS are transitional zone between a water body and a dry land, which may experience wet conditions permanently or at least periodically during high water season <ref type="bibr" target="#b0">[1]</ref>. Wetlands support several environmental services, including flood storage, carbon sequestration, shoreline stabilization, water-quality renovation, a favorable habitat for several unique aquatic vegetation, and animal species. Despite their high contributions to the ecosystem, they have been threatened by the anthropogenic and natural processes during past decades. In particular, human activities progressively converted wetlands to nonwetland areas due to agriculture irrigation, road construction, and pollution. Global warming, flooding, shoreline erosion, and sea level rise further expedite wetland loss through natural processes <ref type="bibr" target="#b0">[1]</ref>.</p><p>Given the numerous benefits of wetlands for the ecosystems and their expanses globally, the restoration and preservation of wetlands have been recognized thanks to the advancement of remote sensing techniques. In particular, remote sensing tools have significantly contributed to the wetland mapping and monitoring in a variety of aspects, including classification <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, change detection <ref type="bibr" target="#b4">[5]</ref>, and water level monitoring <ref type="bibr" target="#b5">[6]</ref>.</p><p>Despite significant improvements in remote sensing tools in both satellite image and applied techniques, the classification of complex heterogeneous land cover such as wetland is challenging. This is because, in a highly fragmented landscape, such as wetland, there are several small classes without a clear-cut border between them, which, in turn, increases the within-class variability and decreases between class separability. Furthermore, some of these classes may have very similar spectral characteristics, which further complicate the matter. Thus, incorporating the spatial feature within spectral information may contribute to differentiating complex land cover <ref type="bibr" target="#b6">[7]</ref>. Several approaches have been proposed in order to evaluate the efficiency of integrating spectral-spatial features for classification, including kernel methods <ref type="bibr" target="#b7">[8]</ref>, Bayesian models <ref type="bibr" target="#b8">[9]</ref>, Markov random field <ref type="bibr" target="#b9">[10]</ref>, and conditional random field <ref type="bibr" target="#b10">[11]</ref>. However, these methods apply low-level features such as spectral information within neighboring pixels or morphological properties. Thus, the main disadvantage associated with these techniques is setting the proper parameters in order to produce suitable features for the different image objects <ref type="bibr" target="#b6">[7]</ref>. Furthermore, most of these algorithms are only fitted to the particular problem (e.g., specific case study and datasets), while they are inappropriate in other cases <ref type="bibr" target="#b11">[12]</ref>.</p><p>Inspired by high efficiency of the human brain in object recognition, high-level spatial features produced by hierarchical learning have attracted substantial interest in several applications, such as object recognition, scene labeling, and document analysis <ref type="bibr" target="#b12">[13]</ref>. In particular, deep learning is one of the most well-known approaches to obtain high-level spatial features using a hierarchical learning framework. It works based on a multilayer interconnected neural network framework that learns features and classifiers simultaneously <ref type="bibr" target="#b13">[14]</ref>. Specifically, a single network with multiple layers may be utilized to learn features and classifiers and exploit the parameters depending on the problem and accuracy demanded. This is also known as an end-to-end feature learning framework, wherein the image pixels and semantic labels are input and output of the algorithm, respectively <ref type="bibr" target="#b14">[15]</ref>.</p><p>Convolutional neural network (CNN) is one of the most efficient approaches among all deep-learning-based frameworks that does not require prior feature extraction and thereby has a greater generalization capability <ref type="bibr" target="#b14">[15]</ref>. This is because a multilayer-based classifier has a high capacity to exploit abstract and invariable features. In particular, a deep CNN extracts the varying level of abstraction for the data in different layers. For example, low-level (e.g., edges), intermediate level (e.g., object fragment), and high-level information (e.g., full object) obtained in the initial, intermediate, and last layers, respectively <ref type="bibr" target="#b13">[14]</ref>.</p><p>There are three main strategies to use CNN, including full training, fine-tuning, and pretraining CNN <ref type="bibr" target="#b13">[14]</ref>. In a fully trained CNN, a network is built from scratch in order to extract particular visual features based on the applied dataset. Despite the great efficiency and robustness of this method, which provides a full control over the parameters and architecture, this is inappropriate for remote sensing applications. This is because building a network from scratch requires a large amount of training data <ref type="bibr" target="#b15">[16]</ref>. The other two approaches are represented as more promising for remote sensing applications since they utilize the pre-trained model, which has been previously trained using different data. This is possible because the initial layers of CNN are typically general filters (i.e., low-level features such as edges); therefore, they need a little or no update during the fine-tuning process.</p><p>The efficiency of deep CNN has been demonstrated in object detection <ref type="bibr" target="#b16">[17]</ref>, recognition of handwritten characters and traffic signs <ref type="bibr" target="#b17">[18]</ref>, and classification <ref type="bibr" target="#b17">[18]</ref>. Although the application of CNN was employed in a number of remote sensing studies for classification of different land cover types using hyperspectral imagery <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, its efficiency was not examined for complex land cover classification (e.g., wetland and sea ice). Currently, the classification of the complex land cover is performed by incorporating a large number of input features to address the difficulty of discriminating land cover classes with very similar spectral signatures. However, extracting a large number of input features is not time efficient, while their manipulation could be challenging. Furthermore, some of these input features are highly correlated, which means no improvement in the information content of input data. Accordingly, a large number of feature selection algorithms have been proposed to determine optimum features for different applications <ref type="bibr" target="#b20">[21]</ref>. Given the main drawbacks of current approaches for classification of complex land cover types, this study aims 1) to evaluate the generalization capacity of pretrained CNN in the classification of multispectral satellite imagery; 2) to determine the suitability of CNN for complex wetland classification, and 3) to generate an appropriate model for further wetland mapping studies. In particular, a pretrained CNN framework was utilized to classify a wetland ecosystem using a RapidEye multispectral imagery in a case study located in Newfoundland and Labrador, Canada. The results of this study are the first attempt showing the potential of CNN for complex land cover mapping with very similar spectral signatures, which facilitates the application of CNN for wetland classification. Given the similarity of wetland species across Canada and the generality of this approach, the results of this study progress toward utilization of CNN for complex wetland classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Study Area and Dataset</head><p>The study area is located in Newfoundland and Labrador, Canada, covering an area of approximately 700 km 2 . This province comprises as a number of ecoregions with different characteristics depending on hydrology, ecology, and geomorphology. Particularly, this study is carried out in the northeast of this province in the Maritime Barren ecoregion, which is identified by an oceanic climate, foggy/cool summers, and relatively mild winters <ref type="bibr" target="#b21">[22]</ref>. Different wetland classes specified by Canadian wetland classification system, including bog, fen, marsh, swamp, and shallow-water, are found within the study region <ref type="bibr" target="#b0">[1]</ref>. However, bog and fen are the dominant wetland types due to the province's climate aiding extensive peatland formation. Other land cover types are upland, urban, and deep water in this ecoregion.</p><p>Field data were acquired for 191 sample sites in summers and falls of 2015 and 2016, respectively. Spatial distribution of different wetland types, as well as other land cover classes, were determined and recorded along with photographs and field notes to facilitate the wetland boundary delineation. Furthermore, global positioning system locations were recorded for each visited land cover types in order to both train the classifier and assess the classification accuracy (see Table <ref type="table" target="#tab_0">I</ref>).</p><p>For classification, two RapidEye optical images in level 3A (radiometrically and geometrically corrected) that were acquired on June 18 and October 22, 2015 were used. This satellite was launched on August 29th, 2008. Its sensors have 5-m spatial resolution with five different spectral bands: blue, green, red, red-edge, and near-infrared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional Neural Network</head><p>CNN is one of the most well-known deep learning algorithms, and has gained interest for image processing in recent years <ref type="bibr" target="#b22">[23]</ref>. CNN is superior to other deep network algorithms due to its ability to preserve the geometry of the image (i.e., the 2-D format). Particularly, it maintains the interconnection between pixels and accordingly, preserves the spatial information. A typical CNN network consists of three types of layers, namely the convolution layer, the pooling layer, and the fully connected layer <ref type="bibr" target="#b22">[23]</ref>. The convolution layer extracts information from previous layers and acts as a filter in the image domain. The filter's values also determine the type of information to be extracted. This filter is sensitive to the spatial information and is defined as a rectangular grid inside the layer. This layer is formulated as a simple convolution:</p><formula xml:id="formula_0">feature map = input * kernal = columns y =0 rows x=0 input (x -a, y -b) kernel (x, y) . (1)</formula><p>The second layer, the so-called pooling layer, reduces the size of data, and it preserves the most important information and the geometry of the input data. In each pooling layer, a particular number is determined by subsampling of a small selected rectangle. There are different methods for subsampling, such as using maximum value or a linear combination <ref type="bibr" target="#b23">[24]</ref>.</p><p>The last layer, namely the fully connected layer, is the reasoning part of the network, which determines the final label of the input data. Particularly, each neuron receives the information from all neurons in the previous layers to make the final decision. However, it does not have a role to preserve the geometry as well as turning the input layer into a vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Patch-Based Image Labeling (PBIL)</head><p>The main goal of a typical convolutional network is to find a unique label for an input image. Although this is a perfect approach to categorize the images, it is far from our goal in remote sensing data processing. Particularly, in the most remote sensing applications (e.g., segmentation and classification), a specific label should be determined for each imaging pixel. Thus, PBIL methods were introduced to convert categorization problem to classification in order to make CNN compatible with remote sensing applications <ref type="bibr" target="#b24">[25]</ref>. In these approaches, an input image is divided into several patches and a label is assigned to the center of each patch. Given the image patches S and the corresponding target M, the whole problem is defined as a probability approach, wherein the distribution of the image patch over the label is represented as follows <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_1">P (n (M, i, w m ) |n (S, i, w s ))<label>(2)</label></formula><p>where n(I, i, w) indicates a patch with the size of w × w from the image I, which is centered on pixel i. The problem is also rewritten in a function form. Given a path from pixel i in the input image to the output unit l in a multiclass classification, the problem is formulated as:</p><formula xml:id="formula_2">f il (s) = exp (a il (s)) Z = P (m i = l|s) (3)</formula><p>where a il is the total input for the lth output and f il shows the predicted probability mapping pixel i to label l. The network should be trained to find the function, which is typically employed by minimizing the residuals of a predefined function.</p><p>The negative log likelihood is used for the training procedure in this study, which is formulated as follows:</p><formula xml:id="formula_3">L (s, m) = all patches w 2 m i=1 (m i ln (f i (s)) + (1 -m i ) ln (1 -f i (s))) . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>The optimization is performed using stochastic gradient descent with minibatches <ref type="bibr" target="#b26">[27]</ref>. The speed of the optimization is also enhanced through tuning some hyperparameters, such as momentum, learning rate, and weight decay. AlexNet, which is the winner of 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC), is utilized for object detection <ref type="bibr" target="#b16">[17]</ref>. The architecture of this network is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>As seen in Fig. <ref type="figure" target="#fig_0">1</ref>, this network has eight hidden layers, including five convolution layers and three fully connected layers <ref type="bibr" target="#b16">[17]</ref>. The adjustment of this network needs less effort due to the relatively small number of layers and accordingly, a smaller number of input parameters relative to other deep networks. Thus, a smaller amount of training data is required to train the network compared to other commonly used networks. This is advantageous for remote sensing data processing given the small number of training samples available in most studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Preprocessing</head><p>Step: A comparison of spectral characteristics of four wetland classes was carried out by plotting their signature using 1000 samples (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>Two conclusions are drawn from Fig. <ref type="figure" target="#fig_1">2</ref>. First, the spectral signature of these wetland classes follows a very similar trend. Second, each class in different bands covers a wide range in different bands based on these 1000 samples. This means that these wetland classes have a high standard deviation resulting in a wide overlap between them. Accordingly, the high degree of misclassification would occur in case of exclusive use  of spectral information into the classification scheme. Thus, it was concluded that the spectral characteristics of these wetland classes alone are insufficient to discriminate between them. This is also attributed to the lack of physical information in the optical images (see Fig. <ref type="figure" target="#fig_2">3</ref>), which may be addressed by incorporating supplementary data (e.g., Radar and LiDAR) in the classification scheme <ref type="bibr" target="#b27">[28]</ref>. Extracting the spatial information from the images and utilizing the high-level spatial features can also be considered as an alternative approach.</p><p>A band selection technique was employed to reduce the dimensionality of the input data. It is necessary since the AlexNet is designed to receive just three bands as input. It also speeds up the training and prediction processes and overcome the GPU memory limitation. For this purpose, the correlation of different bands of input data was obtained. The highest amount of correlations was associated with the blue and red bands (0.95) and also red-edge and near-infrared bands (0.81). Therefore, the blue and red-edge bands were removed and other processing steps were implemented on green, red, and near-infrared bands.</p><p>Other preprocessing steps, employed in this study, were the preparation of both the network and imagery. In the PBIL approach, the image should be patched into small tiles and then, a label is assigned to the center pixel of each patch. There was a high degree of overlap between patches since the step parameter was adjusted to 1 pixel. It is worth noting that the AlexNet is designed to receive the normalized images; thus, the mean of each patch was set to zero by subtracting the mean value of each patch from the image. Finally, the last preprocessing step was cloud masking in one of our datasets. This is because the CNN was not trained to classify the cloud, and, importantly, it could not have an unclassified label. This means that the network would assign a wrong label to the cloud class if it was not masked out in this step.</p><p>2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Training</head><p>Step: The main challenges associated with the network training were the limited number of training samples and determining an optimum patch size to be utilized in PBIL. Instead of full training a network from scratch, a pretrained network can be utilized, which addresses the former problem. In this approach, the parameters of the last layers are mostly updated, not those of all layers. Furthermore, the update's values are small since the updating is carried out on a pretrained network. Due to the limited number of in-situ data in this study, the last four layers of the network were updated to ensure a sufficient amount of sampling data for both training and testing the networks. However, primitive information from the image, such as the size of objects of interest and the network architecture, is required in order to address the latter problem. In particular, each patch should have enough information to generate a distinct distribution for the specific object within the image. Different patch sizes were tested according to spatial resolution of the data and object size of interest (i.e., different wetland classes) from 10 to 40. A patch size of 30 pixels (i.e., 150 m on the ground) found to be an optimum value in this study because a small patch size resulted in overfitting of the model, and on the other hand, undersegmentation was observed in the case of a large patch size. The CNN network was trained using the Caffe library <ref type="bibr" target="#b28">[29]</ref>. Specifically, the training was carried out on a computer with an Intel Xenon 2.80-GHz CPU (16 GB memory) and a Nvidia Quadro k2200 GPU (4 GB memory).</p><p>3) Testing Step: In order to evaluate the robustness of the CNN result and to prevent information leak from the testing dataset to the model, two strategies were considered during test data preparation. First, to make sure that the testing dataset is independent of the training dataset and both groups had roughly comparable pixel counts, reference polygons in each class were sorted based on their size and alternatingly assigned to testing and training groups. This procedure ensured that both the testing and training groups are selected independently from different parts of the image. Second, to make sure that the network is not overfitted, the model was trained over first satellite imagery (June 18th) and accuracy indices were determined based on classified map, which was obtained by applying the trained model over the second satellite imagery (October 22). Since the second image was acquired on a different date and was spectrally different from the first image, this procedure illustrates the reliability of results on the testing dataset. We also compared the result of CNN with a state-of-the-art classifier, random forest (RF). More specifically, the two main parameters of RF, which should be adjusted, are the number of trees (Ntree) and the number of variables (Mtry) <ref type="bibr" target="#b29">[30]</ref>. In this study, a total number of 500 trees were selected in classification model. Moreover, the square root of the number of input variables was considered as Mtry. This is because it decreased both the computational complexity of the model and the correlation between trees <ref type="bibr" target="#b30">[31]</ref>.</p><p>However, a feature extraction step was an additional step, which is required in such a classifier before the classification step. Therefore, results of applying the CNN model over three original bands of the second image are compared to results of the RF classifier over extracted features in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS AND DISCUSSION</head><p>The training step was completed using 30 000 iterations in approximately 12 h. Fig. <ref type="figure" target="#fig_3">4</ref> illustrates the loss and accuracy curve for the validation set. As seen in Fig. <ref type="figure" target="#fig_3">4</ref>, the speed of convergence is high in the initial epochs since the training is actually a fine tuning of a pretrained network. To have a better understanding of the features that were generated and used during the training phase, features of some random patches were extracted. Fig. <ref type="figure">5</ref> depicts the first convolution layer, its corresponding kernels, and features in AlexNet. IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING Fig. <ref type="figure">5</ref>. First convolution layer, its designed kernels, and generated features. Every patch is normalized and resized to feed the network. The designed convolution kernels are then applied to the input patch to generate some normalized features. Each feature highlights a group of similar objects. The kernels are designed to highlight the group of pixels that decrease the loss function.</p><p>The first layers tend to extract primitive features like edges, and the last layers extract high-level features, such as the pattern.</p><p>These high-level features mostly rely on the spatial information in the patches. Incorporating the spatial information into the classification scheme is crucial due to the spectral similar- ity of wetland classes, which causes a great degree of mixture between them. Fig. <ref type="figure" target="#fig_4">6</ref> shows some kernels and extracted features for sample patches. These features highlight the area that is related to the corresponding class.</p><p>To evaluate the efficiency of CNN for wetland mapping, the classification results of CNN were compared with the RF classifier. An RF is an ensembles classifier and has shown good results for several lands cover mappings, such as wetland <ref type="bibr" target="#b31">[32]</ref>. For classification based on the RF classifier, a total number of eight features, namely normalized difference vegetation index (NDVI), normalized difference water index (NDWI), red edge NDVI, as well as all original spectral bands of the RapidEye im-age were used. However, for CNN only three original spectral bands of the RapidEye image, including red, green, and nearinfrared were applied. The classification maps obtained by RF and CNN are depicted in Fig. <ref type="figure" target="#fig_5">7</ref>.</p><p>As seen, there is a significant degree of disagreement between two classified maps for all wetland classes. For example, the dominant wetland classes obtained by RF are swamp and marsh wetlands, whereas the dominant wetland classes for CNN are bog and fen. As reported by field biologists participating during field data collection, bog and fen wetlands are dominant classes. This is attributed to the oceanic climate of the Avalon area facilitating extensive peatland formation (i.e., bog and fen) <ref type="bibr">[33]</ref>. On the other hand, the dominant nonwetland class is upland, which is defined as forested dry land. Notably, the classified map produced by CNN is realistic and demonstrates the detailed spatial distribution of all land cover classes presented in the study area. For example, the classified map shows the predominance of bog and upland classes, while marsh and swamp are less prevalent. These observations fit well with field notes recorded during field data collection. Confusion matrices for these classified maps are presented in Tables <ref type="table" target="#tab_1">II</ref> and<ref type="table" target="#tab_1">III</ref>.</p><p>As seen, the classification overall accuracy of about 95% is achieved using CNN by incorporating three input features. This is of great significance taking into account the complexity of similar wetland classes and the large number of pixels, which were correctly classified. In particular, all land cover classes have high producer's accuracies of greater than 77%, excluding the fen class. More precisely, bog is correctly classified in 89% of cases, fen in 62% of cases, swamp in 78% of cases, marsh in 77% of cases, and shallow-water in 95% of cases. As seen, there is a small degree of confusion between different land cover classes indicating small omission and commission errors in most cases. However, the fen class had the lowest producer's accuracy meaning the largest omission error for this class. Particularly, the fen wetland was erroneously classified as bog and upland classes in some cases. This could be due to the relatively small amount of training samples for the fen class and similar charac-teristics of bog and fen, both of which caused a great degree of confusion. In particular, bog and fen classes are both peatlands dominated by Sphagnum and graminoid species, respectively, with very similar ecological characteristics. Furthermore, these classes were found to hardly distinguish by ecological experts familiar with wetlands in the study area.</p><p>Although the classification overall accuracy of about 79% was obtained using RF, all wetland classes had relatively low producer's accuracies. In particular, bog is only correctly classified in 50% of cases, fen in 44% of cases, swamp in 60% of cases, marsh in 64% of cases, and shallow-water in only 31% of cases. Thus, the overall accuracy of 79% is because of the high classification accuracy for nonwetland classes, such as deep water and urban classes. The confusion matrix illustrates a high degree of confusion between wetland and nonwetland classes. In particular, there is a high degree of confusion between upland and all wetland classes using the RF classifier. Confusion also occurs between herbaceous wetland classes (i.e., bog, fen, and marsh) indicating a high degree of omission for these classes. Other wetland studies reported the great capacity of RF for wetland mapping using a large number of input features such as multitemporal polarimetric and optical features <ref type="bibr" target="#b2">[3]</ref>. Thus, it was concluded that the efficiency of the RF classifier for complex land cover mapping greatly depends on the number of input features. A comparison between two confusion matrices demonstrated a significant improvement by applying CNN relative to RF. Specifically, CNN was about 39%, 18%, 18%, 13%, and 63% more accurate than RF to classify bog, fen, swamp, marsh, and shallow-water classes, respectively. CNN also outperformed RF to discriminate nonwetland classes, wherein all classes were correctly classified in more than 95% of cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this study, the capability of state-of-the-art classification tool, deep CNN, was investigated for wetland classification. In particular, we examined the potential of a pre-existing CNN, namely AlexNet, for mapping wetland complexes using RapidEye optical imagery in a study area located in the Avalon Peninsula, Newfoundland and Labrador, Canada. The overall classification accuracy obtained by CNN was compared with RF, suggesting the superiority of CNN relative to RF even by incorporating a smaller number of input features. In particular, an overall classification accuracy of 94.82% was achieved using CNN, demonstrating an improvement of about 16% compared to the RF classifier for all land cover types. Moreover, an average improvement of about 30% was attained for wetland classes when CNN was employed. The latter observation suggests the significance of incorporating high-level spatial features into the classification scheme to reduce confusion between spectrally similar wetland classes.</p><p>The novel classification framework, employed in this study, along with the fine spatial resolution map, obtained by CNN, can be used as a baseline information and tool for wetland mapping while significantly facilitate the application of CNN for classification of satellite remote sensing data. Furthermore, the use of other very deep CNNs, such as DenseNet, VGG, Xception, and InceptionResNet for classifying wetland complexes offers a potential avenue for further research. This will be particularly feasible by employing advanced cloud processing tools to accelerate the feature learning process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of AlexNet employed in this study (Conv: convolution layer, Pool: pooling layer, F.C.: fully connected layer, RFS: receptive field size, N: number of neurons in fully connected layer, AF: activation function, Soft: Softmax).</figDesc><graphic coords="4,97.16,67.98,403.68,173.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Spectral signature of four wetland classes, namely (a) bog, (b) fen, (c) marsh, and (d) swamp obtained using 1000 samples from each class in five bands.</figDesc><graphic coords="4,118.66,287.41,360.96,280.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Sample patches (i) and field surveying images (ii) of four wetland classes, namely (a) bog, (b) fen, (c) marsh, and (d) swamp.</figDesc><graphic coords="5,74.07,68.68,442.33,112.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Value of validation accuracy and loss as a function of epochs.</figDesc><graphic coords="5,338.45,217.57,175.68,114.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visualization of features related to the first and second convolution layers for four sample patches.</figDesc><graphic coords="6,83.43,340.63,431.74,307.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Training and (b) testing polygons followed by the classification maps obtained by (c) CNN using three input features and (d) RF using eight input features.</figDesc><graphic coords="7,93.08,68.72,403.42,491.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TESTING</head><label>I</label><figDesc>AND TRAINING PIXEL COUNTS FOR THE AVALON REFERENCE DATA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CONFUSION</head><label>II</label><figDesc>MATRIX OF CNN: OVERALL ACCURACY: 94.82%, KAPPA COEFFICIENT: 0.93 TABLE III CONFUSION MATRIX OF RF: OVERALL ACCURACY: 79.11%, KAPPA COEFFICIENT: 0.73</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This project was undertaken with the financial support of the Government of Canada through the federal Department of Environment and Climate Change, Natural Sciences and Engineering Research Council of Canada (NSERC RGPIN-2015-05027), and the Research and Development Corporation of Newfoundland and Labrador (RDC-5404-2108-101). Field data were collected by various organizations, including Ducks Unlimited Canada, Government of Newfoundland and Labrador Department of Environment and Conservation, and Nature Conservancy Canada.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Remote Sensing of Wetlands: Applications and Advances</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Tiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Klemas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-temporal, multi-frequency, and multi-polarization coherence and SAR backscatter analysis of wetlands</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mohammadimanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdianpari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Motagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="78" to="93" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random forest wetland classification using ALOS-2 L-band, RADARSAT-2 C-band, and TerraSAR-X imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdianpari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mohammadimanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Motagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="13" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fisher linear discriminant analysis of coherency matrix for wetland classification using PolSAR imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdianpari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page" from="300" to="317" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Seasonal change in wetland coherence as an aid to wetland monitoring</title>
		<author>
			<persName><forename type="first">B</forename><surname>Brisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murnaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Canisus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lancaster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="1762017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Space-based detection of wetlands&apos; surface water level changes from L-band SAR interferometry</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wdowinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Amelung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Miralles-Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sonenshein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="681" to="696" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning multiscale and deep representations for classifying remotely sensed imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="155" to="165" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning With Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Signal Theory Methods in Multispectral Remote Sensing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Landgrebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Wiley</publisher>
			<biblScope unit="volume">29</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive Bayesian contextual classification based on Markov random fields</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2454" to="2463" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning conditional random fields for classification of hyperspectral images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1890" to="1907" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification and segmentation of satellite orthoimagery using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Längkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kiselev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alirezaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="329" to="349" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How does the brain solve visual object recognition?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards better exploiting convolutional neural networks for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A B</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="539" to="556" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet classification with Deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vehicle detection in satellite images by hybrid deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1797" to="1801" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep supervised learning for hyperspectral data classification through convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Makantasis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. IEEE Geosci. Remote Sens. Symp</title>
		<meeting>Int. IEEE Geosci. Remote Sens. Symp</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4959" to="4962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learningbased classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Impact of reducing polarimetric SAR input on the uncertainty of crop classifications based on the random forests algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Loosvelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Skriver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Baets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E C</forename><surname>Verhoest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4185" to="4200" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A National Ecological Framework for Canada. Eastern Cereal and Oilseed Research Centre (ECORC), Research Branch, Agriculture and Agri-Food Canada</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Ottawa, ON, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep learning in remote sensing: A review</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03959</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Machine learning for aerial image labeling</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Univ. Toronto</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Toronto, Canada</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On optimization methods for deep learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prochnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Conf. Mach. Learn</title>
		<meeting>28th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluation of C-band polarization diversity and polarimetry for wetland mapping</title>
		<author>
			<persName><forename type="first">B</forename><surname>Brisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kapfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tedford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="92" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Multimedia</title>
		<meeting>22nd ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Random forest in remote sensing: A review of applications and future directions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drȃgut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="24" to="31" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random forests for land cover classification</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Gislason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sveinsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="294" to="300" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An assessment of simulated compact polarimetric SAR data for wetland classification using random forest algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdianpari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mohammadimanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brisco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="468" to="484" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
