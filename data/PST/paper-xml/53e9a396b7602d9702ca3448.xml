<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Organization in Vision: Stochastic Clustering for Image Segmentation, Perceptual Grouping, and Image Database Organization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yoram</forename><surname>Gdalyahu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91901</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
							<email>daphna@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91901</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Michael</forename><surname>Werman</surname></persName>
							<email>werman@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91901</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Weinshall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91901</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Werman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91901</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Dickinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91901</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91901</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91901</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Organization in Vision: Stochastic Clustering for Image Segmentation, Perceptual Grouping, and Image Database Organization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">59A9656880839046754ED5E3A66A4AA8</idno>
					<note type="submission">received 3 Feb. 2000; revised 31 Dec. 2000; accepted 22 July 2001.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index TermsÐClustering</term>
					<term>segmentetion</term>
					<term>perceptual grouping</term>
					<term>image retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AbstractÐWe present a stochastic clustering algorithm which uses pairwise similarity of elements and show how it can be used to address various problems in computer vision, including the low-level image segmentation, mid-level perceptual grouping, and high-level image database organization. The clustering problem is viewed as a graph partitioning problem, where nodes represent data elements and the weights of the edges represent pairwise similarities. We generate samples of cuts in this graph, by using Karger's contraction algorithm, and compute an ªaverageº cut which provides the basis for our solution to the clustering problem. The stochastic nature of our method makes it robust against noise, including accidental edges and small spurious clusters. The complexity of our algorithm is very low: yjij log P x for x objects, jij similarity relations, and a fixed accuracy level. In addition, and without additional computational cost, our algorithm provides a hierarchy of nested partitions. We demonstrate the superiority of our method for image segmentation on a few synthetic and real images, both B&amp;W and color. Our other examples include the concatenation of edges in a cluttered scene (perceptual grouping) and the organization of an image database for the purpose of multiview 3D object recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A wide range of tasks in computer vision may be viewed as unsupervised partitioning of data. Image segmentation, grouping of edge elements, and image database organization are problems at different levels of visual information processing (low-, middle-, and high-level vision, respectively). These tasks have different application objectives and they handle very different data entities (pixels, edgels, images). Nevertheless, they all come to serve a common goal, which is the partitioning of the visual entities into ªcoherentº parts. ªGoodº data partitioning, or clustering, is a vague conceptÐthere is no universal measure for the coherence of a cluster. Intuitively, coherence should reflect intracluster homogeneity and intercluster separability. Objects in a homogeneous cluster resemble each other more than they resemble objects in other clusters. By grouping objects into parts, or by organizing the data in a hierarchy of parts, cluster analysis attempts to recover the inherent structure within a given data set. Data clustering is a fundamental problem in unsupervised learning, with a strong connection to cognitive science <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p><p>The goal of this work is to use cluster analysis as a unifying principle for a wide range of problems from low-, middle-, and high-level vision. In our approach, we distinguish between two stages of processing. The first stage is task dependent and defines the affinity, or similarity, between the visual entities. The affinity is a function of the relevant attributes. Low-level attributes might be the spatial location, intensity level, color composition, or filter response of a pixel in the image. Mid-level attributes, in the case of edge elements, may be spatial location, orientation, or curvature, and the affinity associated with them may reflect properties such as proximity, symmetry, cocircularity, and good continuity. High-level attributes may be the entire shape of an object in the scene or the color distribution of all the pixels in an image.</p><p>The second stage in this approach follows the unifying principle and applies cluster analysis to organize visual objects (pixels, edgels, images) into coherent groups. These groups reflect internal structure among the entities, where (roughly speaking) the affinity within groups is larger than the affinity between groups. Therefore, a cluster of pixels in the image sharing similar locations and colors is expected to account for an object or a part of an object in the scene. A cluster of edge elements is expected to exhibit a meaningful aggregation into a complete edge and a cluster of images in a database is expected to be related by a common topic.</p><p>In this work, we present a novel stochastic pairwise clustering algorithm. Our algorithm is hierarchical, efficient, robust, and model-free. The robustness of our method is achieved by averaging over all the possible interpretations of the data, giving more weight to data partitions that are associated with lower cost. This idea is adopted from clustering algorithms that are inspired by statistical mechanics and, in particular, our method is related to <ref type="bibr" target="#b0">[1]</ref>. Like the algorithms that apply stochastic simulation of a certain dynamics, we generate a sample of configurations and count the number of times that two nodes are in the same cluster. This leads to the definition of a powerful collective similarity measure, which is the pairing probability of every two nodes. Our algorithm can be analyzed analytically, and for sparse graphs and a fixed accuracy level it runs in yx log P x time, where x is the number of data-points.</p><p>The rest of this paper is organized as follows: The problems of image segmentation, perceptual grouping of edge elements, and image database organization are reviewed in Section 1.1, while some relevant literature on pairwise clustering is reviewed in Section 1.2. The novel clustering algorithm is described in Section 2, with additional details provided in Section 4. Our results are summarized in Section 3. A concluding discussion follows in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Segmentation, Perceptual Grouping, and Image Retrieval</head><p>This section puts our framework in the context of other computer vision work. Classical techniques for image segmentation include gray-level thresholding, region growing and recursive splitting (henceforth, to be called splitand-merge), relaxation by the Markov Random Field approach or by neural networks, and more <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>.</p><p>Recently, several authors considered image segmentation as a direct application of pairwise clustering. This includes the use of objective functions which are minimized by graph theoretic methods <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b19">[20]</ref>, deterministic annealing <ref type="bibr" target="#b16">[17]</ref>, multigrid techniques <ref type="bibr" target="#b32">[33]</ref>, and other methods <ref type="bibr" target="#b1">[2]</ref>. Orthogonal to these approaches, which try to minimize a certain cost function, there are methods which are built upon local and greedy aggregation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b2">[3]</ref>, or upon spectral decomposition of the similarity matrix <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b40">[41]</ref>.</p><p>The phrase perceptual grouping is usually associated with mid level vision, and more specifically with the grouping of edge elements. Classical methods for perceptual grouping of edge elements frequently assign a saliency value to each edge, representing to what extent it is part of a salient shape, or background noise <ref type="bibr" target="#b34">[35]</ref>. The problem is also treated using stochastic completion fields <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b37">[38]</ref>, cost minimization <ref type="bibr" target="#b15">[16]</ref>, and spectral decomposition of the affinity matrix <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>A measure of similarity between images is essential for organization and retrieval of images. One possibility is to use a global measure, such as the color histogram of the entire image. In our work, we use the shape of extracted objects; we extend existing methods by using our own shape dissimilarity measure <ref type="bibr" target="#b9">[10]</ref>, which enables the organization of an image database into shape categories. One application of such an organization is to present the content of the database in a concise form, which can be digested by the user. In image retrieval, given a query image, we will use the database organization to associate the query with the cluster of most similar images.</p><p>Clustering of pixels, edge elements, and images is associated with different levels of image understanding. It is important to distinguish our goal from the closely related vector quantization approach, where a concise representation of the data is sought, regardless of the actual meaning and significance of the clusters. Vector quantization is concerned with information encoding by means of a finite size codebook, usually for the purpose of compressed archiving or transmission. Although the objective of understanding and compression is different, they are tightly related since compact description of the data reflects abstraction, which is a form of understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Pairwise (Graph-Based) Clustering: Review</head><p>Next, we review some clustering methods that are formulated as graph partitioning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Here, the data is represented by a weighted (and perhaps incomplete) graph, including a set of nodes representing data items, a set of edges connecting them, and a function that assigns weight to each edge representing the affinity between the adjacent data items.</p><p>The agglomeration heuristics for pairwise clustering start from the trivial partition of n points into n clusters of size one, and continue by subsequently merging pairs of clusters. At every step, the two clusters that are most similar are merged together until the similarity of the closest clusters is lower than some threshold. In graph terminology, an agglomeration step is called contraction, where two nodes of the graph are unified. Different similarity measures between clusters distinguish between different agglomerative algorithms. In particular, the single linkage algorithm defines the similarity between clusters as the maximal similarity between two of their members and the complete linkage algorithm uses the minimal value. A very general class of agglomerative algorithms is formalized in <ref type="bibr" target="#b24">[25]</ref>.</p><p>A few partitioning criteria are based on the notion of cuts in a graph, where the capacity of the cut is the sum of weights of all edges that cross the cut. The minimal cut clustering algorithm seeks the partition which minimizes this cost. For bipartitions, which split the data into two parts, this optimization problem is extensively studied and solved in polynomial time. However, clustering applications frequently involve very large graphs and the exact solution becomes impractical. In our proposed clustering algorithm, we apply a new sampling tool, originally developed as the core of a probabilistic minimal cut algorithm <ref type="bibr" target="#b21">[22]</ref>. This tool is known as the contraction algorithm and, in fact, it is a randomized version of the single linkage method for clustering. Hence, in addition to improving efficiency, we make a formal connection between cut-based algorithms and agglomerative algorithms.</p><p>Spectral methods identify good partitions via the eigenvectors of the similarity matrix or other matrices derived from it. The similarity matrix is the matrix whose iY j element contains the weight of the directed edge from node i to node j. In general, spectral methods are not associated with any global cost function and can be thought of as useful heuristics. The factorization method <ref type="bibr" target="#b27">[28]</ref> uses the principal eigenvector v of the similarity matrix, treating it as an indicator function: A threshold is chosen, after which each node i is assigned to one part if vi b and to the other part otherwise; this method was applied to figure/ground segmentation and perceptual grouping. Similar applications to perceptual grouping and hypertext retrieval were proposed earlier in <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b23">[24]</ref>. See <ref type="bibr" target="#b8">[9]</ref> for the relation with dynamical systems and stochastic graph traversal.</p><p>The use of nonprincipal eigenvectors can sometimes add more power to the algorithm. Assuming that the similarity matrix e is symmetric, let denote the n Â k matrix whose columns are the k largest eigenvectors of e. As shown in <ref type="bibr" target="#b40">[41]</ref>, the algorithm described in <ref type="bibr" target="#b4">[5]</ref> effectively uses the matrix for motion grouping. If denotes the matrix after its rows are normalized to length 1, then <ref type="bibr" target="#b31">[32]</ref> defines ; for ªwell-separatedº data and with a proper choice of k, it is claimed that the value of iY j is close to either 0 or 1, depending on whether the nodes i and j belong to the same cluster or not.</p><p>A related method, applied so far to image segmentation, texture segmentation, and motion segmentation, is the normalized cut algorithm <ref type="bibr" target="#b36">[37]</ref>. Given a symmetric similarity matrix e, the normalized cut algorithm uses the eigenvectors of its Laplacian matrix; in particular, the eigenvector corresponding to the second smallest eigenvalue is used to partition the graph into two parts via thresholding. Unlike the other heuristically spectral methods, this algorithm can be shown to provide a continuous approximation to a discrete optimization problem <ref type="bibr" target="#b3">[4]</ref>; it minimizes a specific partitioning criterion, the capacity of the bipartition normalized by the association of the two parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE TYPICAL CUT ALGORITHM</head><p>This section presents the general approach and the principles of our clustering method; a full description and analysis is postponed until Section 4, while the code itself is available from <ref type="bibr">[44]</ref>. After defining the terminology in Section 2.1, we describe the algorithm in Section 2.2. To clarify how the algorithm works and to demonstrate some of its properties, an illustrative example is discussed in detail in Section 2.3. Issues of complexity, preprocessing, and related work are discussed in Sections 2.4, 2.5, and 2.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations and Definitions</head><p>Our clustering algorithm uses pairwise similarities which are represented as a weighted graph q Y i: The nodes represent data items and the positive weight w ij of an edge iY j represents the similarity between nodes (data items) i and j. The graph q Y i may be incomplete, due to missing data or due to edge dilution (whose purpose is to increase efficiency). The weights w ij may violate metric properties and, in general, they may reflect either similarity or dissimilarity values. In the current work, however, we assume that the weights reflect symmetric similarity relations (hence, w ij w ji and w ij H for i and j that are completely dissimilar). We do not assume that the similarity weights obey the triangular inequality and self-similarities w ii are not defined.</p><p>A cut I Y P in a graph q Y i is a partition of into two disjoint sets I and P . The capacity of the cut is the sum of weights of all edges that cross the cut, namely, I Y P iPIYjPP w ij . A minimal cut has the minimal capacity. We use the term ªcutº also for the generalized case of multiway cuts. A partition of into r disjoint sets I Y F F F Y r is called an rEwy cut, and, in accordance, its capacity is defined as iPYjP YT w ij . Every one of the r components may be referred to as a ªsideº of the cut.</p><p>Let the nodes which belong to each side I F F F r be grouped together into one metanode and discard all the edges which form self-loops within metanodes (namely, discard the inner edges of each component, which connect two inner nodes belonging to the same component). The graph which is thus obtained has exactly r metanodes and it is a multigraph since metanodes may be connected to each other by more than one edge. Actually, if q is a complete graph, then the number of edges connecting the metanodes representing the components and is exactly j jj j.</p><p>The grouping procedure described above yields a contracted graph which has r metanodes, denoted q H r . Note that this notation does not characterize the contracted graph since there are many ways to group the nodes of q into r disjoint sets. However, any contracted graph q H r represents an rEwy cut in the original graph. The edges of q H r are the edges which cross the corresponding rEwy cut in the original graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Outline of Algorithm</head><p>This section provides a simplified, concise description of the algorithm, ignoring some of the implementation issues arising from space and time complexity considerations. The algorithm is divided into two stages, described in pseudocode in Figs. <ref type="figure">1</ref> and<ref type="figure">2</ref>, and explained below.</p><p>Generating typical cuts. For a given value of r (r I F F F x), our algorithm generates a sample of w possible rEwy cuts and uses this sample to estimate the probability p r ij that iY j is not a crossing edge of a random rEwy cut. The pseudocode in Fig. <ref type="figure">1</ref> counts, for every pair of nodes iY j P and for every integer r between 1 and x, the number of rEwy cuts (out of w) in which the two nodes are on the same side. These accumulators are divided by w to estimate for every two nodes the probability p r ij that they are on the same side. In this pseudocode, the procedure CONTRACT generates the rEwy cut q H r from the previously generated cut q H rI . The procedure CONTRACT selects two metanodes of q H rI and merges them into one metanode of q H r , while discarding the edges which previously connected these two metanodes. The selection of the nodes to be unified is probabilistic: An edge iY j of q H rI is selected for contraction with probability proportional to its weight w ij . Then, the two metanodes which are adjacent to the selected edge are merged.</p><p>The contraction procedure is the cornerstone of our method since it defines the sample of w cuts, according to which the empirical probabilities p r ij are computed. Thus, the contraction procedure is our sampling tool, typically, assigning higher probability to cuts with lower capacity, as is shown in <ref type="bibr" target="#b21">[22]</ref>. In fact, <ref type="bibr" target="#b21">[22]</ref> proves that the minimal cut can be found using this sampling method in polynomial time, even though the overall number of possible cuts is exponential. In summary, the contraction process induces a probability distribution over cuts and, under this distribution, we estimate p r ij Ðthe marginal probability that nodes i and j are on the same side of a random r-way cut.</p><p>The number of rEwy cuts in a graph of x nodes is the Sterling number of the second kind, denoted (rY x. Let r I F F F (rY x be an index to the set of all rEwy cuts in q Y i. Fix r and let denote the probability that the contraction algorithm generates the cut . For a fixed r value, I. Define an indicator variable e ij to be 1 if the edge iY j crosses the cut and 0 otherwise. It is readily seen that iYjPi</p><formula xml:id="formula_0">w ij I À p r ij iYjPi w ij e ij hriY</formula><p>where is the capacity of cut and hri is the expected value of the rEwy capacity. Therefore, we can interpret I À p r ij as the probability that edge iY j is a crossing edge in an ªaverage cut.º We use this observation for the following definition.</p><p>For every integer r between 1 and x, we define the typical cut e I Y e P Y F F F Y e sr as the partition of q into connected components, such that, for every i P e , j P e ( T Y Y I F F F sr), we have p r ij `HXS. To find the typical cut for every integer r between 1 and x, we first remove all the edges whose transformed weight p r ij is smaller than HXS and we then compute the connected components in the remaining graph. Note that the number of parts, sr, in the typical cut can be different from r.</p><p>The x typical cuts corresponding to r I F F F x are the candidate solutions to our clustering problem. Although this is an extremely small number compared with the exponential number of possible partitions, we still need to select only a few interesting solutions out of the x candidates. The question that remains is to define and choose ªgoodº values of r, for which a ªmeaningfulº clustering is obtained as part of a hierarchy of a few selected partitions.</p><p>Selecting meaningful partitions. We define the following function of the typical cut at level r:</p><formula xml:id="formula_1">r P xx À I ibj x i x j Y I</formula><p>where x k je k j denotes the number of elements in the kth cluster. r, therefore, measures how many edges of the complete graph crossover between different clusters in the rEprtition, relative to the total number of edges in the complete graph. Partitions which correspond to subsequent r values are typically very similar to each other, or even identical, in the sense that only a few nodes (if any) change the component to which they belong. Consequently, r typically shows a very moderate increase. However, abrupt changes in r occur between different hierarchical levels of clustering, when two or more large metanodes are merged.</p><p>We look at changes in the value of r between subsequent r values and output only those partitions which are associated with a large change in r. For the current presentation, we set a threshold and output a solution at level r if and only if Á r b . This is described in Fig. <ref type="figure">2</ref>.</p><p>The existence of pronounced peaks in Á r does not guarantee that we find the desired solutions. A necessary requirement is that the set of x candidate typical cuts that correspond to the x possible r values indeed contains the desired solutions. Whether this is the case or not it depends on the output of the first stage of the algorithm, the core of our method. The simple heuristic that is applied in the second stage (Fig. <ref type="figure">2</ref>) is based on the assumption that a good set of candidates is given.</p><p>The code line denoted with (*) in Fig. <ref type="figure">2</ref> is optional and involves an additional parameter. One may not be interested in a cluster whose size is very small, e.g., 1 percent of the number of data points. Small clusters are formed either by boundary points, due to the competition between conflicting labels, or by background points (unstructured noise) due to their isolation. A conservative strategy regards the points clustered in very small parts as points whose labels cannot be safely determined. An aggressive strategy, on the other hand, may sustain the larger parts but relabel the smaller parts by attempting to recover their original label. We use such a relabeling strategy which revives some of the deleted connections by adding back edges in decreasing order of p r ij , thus letting the small clusters join the larger ones. More details on the optional relabeling procedure are given in Section 4.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Illustrative Example</head><p>To illustrate some features of our method and its relative advantages over other methods, we proceed with an illustrative synthetic example. We use a point set example in two dimensions which can be easily visualized, see Fig. <ref type="figure" target="#fig_1">3a</ref>. The data consists of x PY HHH points in P arranged in three dense spiral regions and sparse background. The vectorial nature of the data in this example, namely, points in P , is used for visualization but is hidden from the clustering algorithm. The information which is made available to the clustering algorithm includes only the matrix of pairwise Euclidean distances d ij between points.</p><p>During preprocessing, the Euclidean distances d ij are transformed to similarity weights, which decrease with increasing distances. Psychophysical studies find that the similarity, or the confusion frequency, between stimuli decay exponentially with some power of the perceptual measure of distance <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b6">[7]</ref>. We use the same functional form as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref>, namely, w ij expÀd P ij a P , where is the average distance to the nth nearest neighbor (we used n IH, but our results are not very sensitive to this choice; see Section 4.2). We then construct a complete graph q Y i with x nodes, where node i represents the ith data point and the weight of edge iY j is set to w ij .</p><p>The constant reflects some reasonable local scale. If d ij ) , then w ij is very small and the edge iY j is unlikely to be selected by the procedure CONTRACT. In this case, the decision whether points i and j are in the same cluster depends solely on transitive relations via other nodes, same as when w ij is unknown. Thus, in order to increase the computation efficiency, we delete from the graph every edge iY j whose weight is negligible. In the current example, we discarded edges with weights smaller than 0.01 (note that the weights are in the range HY I). This threshold and the number n IH (which determines ) are the only parameters involved in the preprocessing stage. The number of remaining edges in this example was about 46,000.</p><p>Having constructed the similarity graph q Y i, we are ready to apply our clustering algorithm. The first stage is to estimate the pairing probabilities p r ij . This is done using a sample of w partitions at each r level (see procedure STAGE-1 in Fig. <ref type="figure">1</ref>). In the second stage of our algorithm, we call the procedure STAGE-2 (Fig. <ref type="figure">2</ref>), which computes the typical cut for each r between 1 and x. To select between the x candidate partitions, procedure STAGE-2 looks for large changes in the function r defined above. Fig. <ref type="figure" target="#fig_1">3b</ref> shows the graph of Á r r À r À I as a function of r. It is an impulse graph, illustrating that partitions which correspond to large changes in r are few and easy to identify. Two obvious peaks appear at r QIW and r QSP and they mark the meaningful levels of data organization in this example.</p><p>The two peaks in Á r correspond to three hierarchical levels of partitions. At r QIV, just before the large peak on the left, the three spirals form one cluster (Fig. <ref type="figure" target="#fig_1">3c</ref>). There is also a small cluster containing 10 points at the center and the rest of the points form isolated clusters of size one. This is the coarsest level of interest. In the next interesting level, at r QIW, the giant cluster splits into twoÐone part contains a single spiral and the other part contains two spirals (Fig. <ref type="figure" target="#fig_1">3d</ref>). Increasing r further reveals some boundary effects, where 41 additional points become isolated, but the overall picture remains unchanged as long as r `QSP (Fig. <ref type="figure" target="#fig_1">3e</ref>). The peak at r QSP signals the next significant change, giving the partition shown in Fig. <ref type="figure" target="#fig_1">3f</ref>. The meaningful partitions are reported by the procedure STAGE-2 when its parameter is set to HXI (Fig. <ref type="figure" target="#fig_1">3b</ref> shows that our method is not very sensitive to the exact value).</p><p>For comparison, we apply to the same data exactly (after identical preprocessing) a few other methods. The results with two spectral methods are shown in Figs. <ref type="figure" target="#fig_2">4a</ref> and<ref type="figure" target="#fig_2">4b</ref>; clearly, these methods fail here, while our algorithm produces good results. Likewise, the deterministic singlelinkage algorithm fails to find the desired solution in any of  its hierarchical levels (rendering the issue of selecting a stopping criterion irrelevant), as is illustrated in Fig. <ref type="figure" target="#fig_2">4c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Complexity: The Sample Size</head><p>The detailed complexity analysis is postponed to Section 4.1, where we describe our efficient implementation. The efficient version drastically decreases the number of estimated variables (p r ij ) from x Q to jij, which is yx for sparse graphs and yx P for general graphs. Moreover, it will be shown there that one graph contraction (one iteration of the external loop in STAGE-1, Fig. <ref type="figure">1</ref>) can be implemented in yjij log x time.</p><p>In this section, we address the question of the desired sample size, denoted w. We show that w ylog xa P for an accuracy level , hence, the overall complexity of STAGE-1 for a fixed accuracy level is yjij log P x. The efficient implementation of STAGE-2 takes only yjij log x time (Section 4.1.3), making yjij log P x the overall complexity bound for our algorithm.</p><p>In practice, one may monitor the convergence of the estimated quantities during execution and terminate when a sufficient level of accuracy is obtained. However, to determine the asymptotic complexity of the algorithm an estimate of w is required. Our goal is to determine a lower bound on w which guarantees, with a sufficient level of certainty, a sufficient amount of accuracy. We denote by p r ij the correct pairing probabilities, which are obtained at the limit of infinite w. We define an accuracy parameter and an uncertainty parameter , and require our empirical estimations to deviate from p r ij by no more than , with probability I À .</p><p>Since the lower bound which we will find for w will not depend on r, we pretend that r is fixed to some arbitrary value r H , which is omitted from the notations. Hence, the notation p ij stands for p r ij and r r H . For simplicity, we also prefer to enumerate the edges of the graph instead of its nodes. Hence, the notation p e stands for the pairing probability of the nodes that are adjacent to edge e. Equivalently, p e is the probability that edge number e is an inner edge of a metanode when r r H . Procedure STAGE-1 of Section 2.2 counts the number of times (out of w) in which an edge e is an inner edge of a metanode. If this is the situation e times, then our empirical estimation for p e is p e e aw.</p><p>Let i be the sequence of w Bernoulli trials such that i I for a round when edge e is an inner edge and i H otherwise. Thus, r i I p e and r i H I À p e . Let p e i aw be our empirical estimation. The Hoeffding-Chernoff bound <ref type="bibr" target="#b22">[23]</ref> implies that, for H I, r j p e À p e j b Pe ÀPw P X Thus, we have a bound on the probability of making too large error in estimating p e for one edge. But, there are jij edges in the graph and we need to ensure that we do not make a large error for too many of them. Thus, we use the union bound with our certainty parameter : r We j p e À p e j b e r j p e À p e j b jij</p><formula xml:id="formula_2">Á Pe ÀPw P `Y</formula><p>which results in w b HXQS log P jij À log P I P X</p><p>The asymptotic dependence of the sample size on the number of nodes is therefore w ylog x, even for complete graphs where jij yx P . The efficient computation of the half probability level (Section 4.1.1) introduces an additional error since it involves an approximate median computation. However, this affects the parameter but not the logarithmic dependence on the number of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Preprocessing and Other Parameters</head><p>The main part of the algorithm, STAGE-1, depends only on the accuracy parameter, which affects only the number of iterations w. Hence, no parameter tuning is required at this stage. <ref type="foot" target="#foot_0">1</ref> The second part, STAGE-2, involves the parameter that determines whether a change in the function r is significant. Our examples throughout this paper always show the complete graph of Á r, demonstrating that significant changes are pronounced and easy to recognize. The minimal cluster size of interest, if it is known, is another parameter which may be used by STAGE-2. Only the interpretation of the results depends on this parameter, thus reflecting prior information. For example, in Fig. <ref type="figure" target="#fig_1">3</ref>, had we used a minimal size parameter larger than 10, the small cluster at the center would not have been shown, and its members would have been left unlabeled.</p><p>It is mostly the preprocessing stage, which constructs the weighted graph q, that depends on external parameters. These parameters are related to the definition of similarity (which is task dependent) and to the transformation from perceptual similarity to edge weight. Given a dissimilarity measure d ij between stimuli i and j, we define w ij expÀd P ij a P . Here, is a decay parameter which reflects some suitable local scale and it needs to be tuned. Sometimes the dissimilarity between stimuli is measured along different dimensions, like in an image segmentation task where dissimilarity between pixels is a function of their spatial proximity and relative brightness. In this case, a different local scale parameter is defined for every dimension " of similarity, namely,</p><formula xml:id="formula_3">w ij " exp Àd" P ij a" P X P</formula><p>Exponential decay is supported psychophysically <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b6">[7]</ref> and can be understood from the properties of the cut cost function that we use. The cut value, which is the total similarity weight between clusters, is affected by the size of the clusters. Large clusters should not be judged similar just because there are many graph edges connecting them. Exponential decay, as well as local neighborhoods, avoid this undesirable bias. In <ref type="bibr" target="#b38">[39]</ref>, the weights are treated as transition probabilities and an exponential decay is used to relate between the length of a trajectory (which is additive, like the cut capacity) to its probability (which is multiplicative for independent events).</p><p>Additional experiments described in <ref type="bibr" target="#b8">[9]</ref> show that the results reported above are robust with respect to the scale parameters used to implement <ref type="bibr" target="#b1">(2)</ref>. We had to change the neighborhood size used to determine by orders of magnitude to have any noticeable effect. Only very drastic pruning of ªsmallº edges, leading to rather small graphs where most of the information is lost, affected the final result noticeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Our Work Put in Context</head><p>Now, we would like to put the contraction algorithm in the context of agglomerative clustering. As explained above, agglomerative clustering is a family of algorithms which partition the data using a greedy local merging criterion. Thus, an agglomerative procedure starts with the trivial partition of x points into x clusters of size one and proceeds with x À I merging steps, where, at each step, two clusters are selected and merged together. The selection is based on a local definition of clusters similarity and, at each step, the two clusters which are most similar are chosen and merged. In particular, the single linkage algorithm defines clusters similarity as the maximal pairwise similarity between their members.</p><p>The resemblance between the stochastic contraction algorithm and the single linkage algorithm is evident; one being the randomized version of the second. Specifically, stochastic contraction selects an edge with probability proportional to its weight, while single linkage selects the edge with maximal weight. The connection is interesting since single linkage is a purely local method, which does not attempt to optimize a global criterion. Although its output can be characterized globally as constructing a minimal spanning tree, it is not a clustering algorithm of the type that minimizes similarities between clusters and/or maximizes similarities within clusters.</p><p>The stochastic contraction is designed to generate all of the low capacity cuts (within a constant from the lowest capacity cut) with high probability. Together with these low capacity cuts, however, cuts of higher capacity will also be generated. There is an important difference, though: In the original graph, there is an exponential number (as a function of the number of nodes) of cuts of higher than threshold capacity, while, in our sampling, the total sample space is polynomial. Since this enormous reduction in the size of the sample space is obtained without losing any low capacity cut, the result is a very significant sampling bias towards the lower capacity cuts (see Section 4.3). Assuming that cut capacity is a global measure for clustering quality, this change in cut distribution is what makes our algorithm work in practice.</p><p>Using the cut capacity as a global quality measure for clustering is a well-known approach (e.g., <ref type="bibr" target="#b42">[43]</ref>). One of its problems is that the minimal cut tends to break small parts from the graph. Consequently, <ref type="bibr" target="#b36">[37]</ref> proposed another cost function, called normalized cut, which introduced a penalty for nonbalanced partitions. Our approach is different. We do not seek the global optimum of a certain cost function. Instead, we are interested in ªaveragingº the partitions with weights proportional to their quality (measured by capacity). In this sense, our approach is similar to clustering methods inspired by statistical mechanics <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b29">[30]</ref> and, in particular, it was inspired by the granular magnet model of <ref type="bibr" target="#b0">[1]</ref>, also called the super paramagnetic clustering (SPC) algorithm.</p><p>Our algorithm can be thought to replace the pairwise weights, prior to clustering, by higher order ªcollectiveº weights. Using higher order relations is used explicitly or implicitly by other methods as well. For example, it is known from the Gomory-Hu theorem <ref type="bibr" target="#b18">[19]</ref> that the minimal cut separates the nodes of the graph in such a way, that the max-flow value for every two nodes within a component is larger than every max-flow value between components. Hence, the min-cut algorithm is equivalent to the definition of max-flows as collective similarities, followed by thresholding. Relaxation methods which use weight propagation can be viewed in the same way.</p><p>To summarize, we note that one of the important aspects of our work is the link it provides between clustering approaches which seemed to be very different, i.e., SPC, the min-cut algorithm, and the single linkage algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>In this section, we apply our clustering algorithm to three fundamental computer vision problems: image segmentation, perceptual grouping of edge elements, and image classification. These problems deal with the aggregation of visual elements, such as pixels, edgels, or images, into coherent groups. In fact, the synthetic point set example used in Section 2.3 is no exception, but, in this section, we focus on real world computer vision problems, analyzing real images, and contours extracted from images. In Sections 3.1 and 3.2, we consider segmentation of intensity and color images respectively. For comparison, we provide the segmentation results obtained by the split-and-merge segmentation algorithm<ref type="foot" target="#foot_1">2</ref> from the same images. In Section 3.3, we discuss the case of ªunstructured background,º which includes the problem of grouping edge elements. In Section 3.4, we integrate our contour matching algorithm and our clustering algorithm into an application of image database organization. Specifically, a collection of 121 images is hierarchically divided into shape categories. Finally, we summarize the comparison of our method with other methods in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Segmentation of Intensity Images</head><p>For image segmentation, nodes in the graph represent individual pixels. We consider images where each pixel has a single intensity attribute. In accordance, the similarity weight w ij between pixels (nodes) i and j increases with increasing spatial proximity and brightness likelihood. From (2), we have:</p><formula xml:id="formula_4">w ij e À dI P ij P À dP P ij P Y</formula><p>where dI is the Euclidean distance between the pixels in the image plane, dP is the intensity difference between the two pixels, and Y are corresponding scale parameters. As in <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref>, we determine the parameters and manually.</p><p>To reduce the number of edges and get a sparse graph, we eliminate edges whose weight is below some threshold and consider short range neighborhoods (see the captions of Figs. 5, 6, 7, 8, and 9 for details).</p><p>The observation that the similarity graph is sparse is crucial for practical image segmentation applications. In <ref type="bibr" target="#b36">[37]</ref>, a sparse graph was obtained by randomly picking a small number of edges for each pixel in a limited range neighborhood. We adopt a similar approach: We consider the eight nearest neighbors of each pixel or we consider the four nearest neighbors and randomly pick another four (we do not observe significant differences between these two methods.) In addition, we eliminate edges whose weight is below some threshold.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref> shows a gray-level image taken during a baseball game. We use the same image used by Shi and Malik <ref type="bibr" target="#b36">[37]</ref> to be able to compare the two methods. 3 Next to the graylevel image, the graph of Á r is shown. It is clear that there are only a few candidate solutions to consider. The peaks in this graph mark the detection of large objects in the scene. The four levels of segmentation results which correspond to the four highest peaks are shown below the image and the impulse graph.</p><p>In all our image segmentation examples in this section, we regard clusters which contain less than 100 pixels as small clusters and, initialy, we do not label them. The pixels which belong to these small clusters may be joined later with the larger clusters, as discussed in Section 4.1.3. We note that it is possible to get an even finer level of resolution by further decreasing the minimal cluster size. For example, the palms of the hands of the lower player contain less than 100 pixels and they can be detected too if a lower value is  3. The same image is used in <ref type="bibr" target="#b7">[8]</ref> but in different resolution. used. There is a natural trade-off between the required resolution and the robustness to noise that generates spurious clusters.</p><p>For comparison, Fig. <ref type="figure" target="#fig_4">6</ref> shows the results when using the split-and-merge segmentation algorithm. In the implementation, we used a user-specified variance threshold, which indirectly controls the number and size of regions produced by the algorithm. There is no methodology for selecting a good threshold, and the segmentation result changes gradually with the chosen parameter. (On the other hand, one of the most important features of our algorithm is that the segmentation shows phase transitions when the control parameter is varied.) As can be clearly seen, the segmentation results of the split-and-merge algorithm are not as good as ours, even when the control parameter is manually tuned for best visual results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Segmentation of Color Images</head><p>Here as well, nodes in the graph represent individual pixels. The difference between the segmentation of color images and brightness images lies solely in the similarity measure between pixels. While brightness is a one-dimensional attribute, the color resemblance between pixels is measured in a three-dimensional color space. The weight w ij is computed as before from (3), but, now, dP is measured in a 3D color space. If using one of the ªperceptually uniformº color spaces, CIE-LAB or CIE-LUV, dP is the Euclidean distance in the corresponding space. Fig. <ref type="figure" target="#fig_5">7</ref> shows an outdoor scene with a rainbow and a cow. The arrangement of the inserted images is the same as in Fig. <ref type="figure" target="#fig_3">5</ref>, with the impulse graph of Á r shown next to the original image, and the three segmentation results which correspond to the three highest peaks in the impulse graph shown below. Note the saliency of the leftmost segmentation level at r TUV, where a very large peak is obtained due to the splitting of two very large areas (the sky and the grass regions). The analysis is in CIE-LAB color space.</p><p>Fig. <ref type="figure" target="#fig_6">8</ref> shows an image of four airplanes flying above a desert. We present the analysis in CIE-LUV color space. The four highest peaks in the graph of Á r correspond to the separation of the four planes from the background; each peak separates one plane. The other peaks indicate partitions of the background into regions of different tones of yellow (not shown). For comparison, we conducted a control experiment with the split-and-merge algorithm; in this experiment none of the planes were detected and the segmentation result was quite arbitrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unstructured Background and Grouping of Edge Elements</head><p>Our previous examples of image segmentation explicitly assume that the images can be partitioned into regions of homogeneous brightness or color. If, on the other hand, the image contains a region of random noise, then the pixels belonging to this region are not similar to each other and cannot be clustered by methods that are based on homogeneity maximization. Our algorithm is useful for this case as well, as long as the relabeling option is not used.</p><p>Our algorithm handles such cases by letting the random noise items form many isolated clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Image Segmentation with Unstructured Background</head><p>We start with a synthetic example, where the task is brightness image segmentation. Here, we follow the recent work of Perona and Freeman <ref type="bibr" target="#b27">[28]</ref>, who considered the segmentation of ªstructuredº foreground from ªunstructuredº background. Fig. <ref type="figure" target="#fig_7">9a</ref> shows a synthetic image that was generated according to the parameters given in <ref type="bibr" target="#b27">[28]</ref>. Next to it, in the impulse graph of Á r, we observe a large peak at r TSR. The partitions which are obtained on both sides of this peak are shown Figs. <ref type="figure" target="#fig_7">9c</ref> and<ref type="figure" target="#fig_7">9d</ref>. In both Figs. <ref type="figure" target="#fig_7">9c</ref> and<ref type="figure" target="#fig_7">9d</ref>, the background pixels form tiny clusters, each one including a few neighboring pixels which happen to have similar brightness values.  These tiny clusters reflect true structure in the image, which we can screen out by using small size threshold, leaving these pixels unlabeled.</p><p>To decide how many meaningful partitions there are, we use a cross-validation scheme (see <ref type="bibr" target="#b8">[9]</ref> for details). The crossvalidation indices which we got for the five highest peaks were 0.46, 0.02, 0.08, 0.02, and 0.08, respectively, clearly indicating that only the first partition should be accepted.</p><p>A comparison with the normalized cut algorithm <ref type="bibr" target="#b36">[37]</ref> and with the factorization algorithm <ref type="bibr" target="#b27">[28]</ref> is presented in Fig. <ref type="figure">10</ref>. The results are in agreement with those reported in <ref type="bibr" target="#b27">[28]</ref>, showing that the normalized cut algorithm fails completely in the presence of unstructured noise, while the factorization algorithm performs less well than our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Perceptual Grouping of Line Segments</head><p>As a second example of separating structure from cluttered background, we consider the problem known as perceptual grouping of line segments. In such problems, the raw data is typically a set of edgels, or line segments, and, accordingly, nodes in the graph represent line elements. The task is to group together a set of line segments that together define a perceptually appealing ªedge,º typically expected to have one or more of the following properties: smoothness, closure, or convexity. In these problems, typically, the clusters of interest contain only a few elements in comparison with the number of elements in the background (the clutter), see Fig. <ref type="figure" target="#fig_8">11</ref>.</p><p>A full treatment of this problem, which takes care of clutter as it interferes with procedure STAGE-2, is beyond the scope of the current paper (see <ref type="bibr" target="#b11">[12]</ref> for a detailed discussion). But, in order to illustrate the usefulness of our approach to mid-level vision problems, next we show one example where our clustering algorithm is used to perform perceptual grouping.</p><p>Fig. <ref type="figure" target="#fig_8">11</ref> shows a contour of a lemon superimposed on random textured background. The total number of line segments in this figure is 440, while the number of segments which construct the lemon contour is only 44 (signal to noise ratio of about 10 percent). The image and the similarity measure w ij are adopted from <ref type="bibr" target="#b37">[38]</ref>. Here, we do not repeat the similarity (affinity) formula, which is quite complex, and note that it depends on relative orientation and spatial proximity between line segments. After the similarity between every two line segments is computed, we discard low weight edges; our elimination criterion uses the mutual-k principle <ref type="bibr" target="#b0">[1]</ref>, namely, a weight w ij is kept if edge i is one of the k-nearest neighbors of edge j and vice versa (k = 10 is used). The remaining weights are fed into our clustering algorithm.</p><p>Most of the large peaks in the impulse graph of Á r result from the separation of small sets of line segments from a large set. However, by the time that the interesting Fig. <ref type="figure">10</ref>. The performance of other algorithms applied to the data of Fig. <ref type="figure" target="#fig_7">9</ref>. The same preprocessing and exponential transformation from distances to weights was used (see text). Left: the normalized cut algorithm <ref type="bibr" target="#b36">[37]</ref>. (a) One of the best Ncut partitions (there are many partitions which correspond to a zero eigenvector, 33 of them separate a single pixel). (b) The partition which is suggested by the eigenvector shown on the left. The five isolated entries are connected to each other, but not to the rest of the image, hence, the cut value is zero. Right: the factorization method <ref type="bibr" target="#b27">[28]</ref>. (c) The first eigenvector of the similarity matrix sorted by the value of its entries. There is no clear threshold to choose. (d) The same eigenvector presented using log of intensity scale, where the entries are ordered like the image pixels. separation occurs, the cluster which is split is small (see the transition from r PPW to 230), and the splitting is not accompanied by a large change in r. Nevertheless, if we ignore variations in r that result from segmentation of clusters smaller that 10 elements, as we did in Fig. <ref type="figure" target="#fig_8">11</ref>, then there are only seven candidate partitions to consider; and as noted aboveÐthe selection between them can rely on additional external perceptual criteria. The crucial result to note is that the desired segmentation is indeed found and identified as a member of a small set of partitions that should be evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Organization of an Image Database</head><p>We now describe another application related to 3D object recognition. The database contains 121 images of 12 different objects, including the image database created by Sharvit et al.</p><p>(see <ref type="bibr" target="#b33">[34]</ref>), which consists of 31 silhouettes of six classes of objects (including fish, airplanes, and tools). In addition, we collected 90 images by placing six toy models on a turntable, so that the objects could be viewed from different viewpoints. For each of the six toy models, we collected 15 images by rotating them in azimuth <ref type="bibr" target="#b4">(5</ref> </p><formula xml:id="formula_5">ÀPH Y ÀIH Y H Y IH Y PH ) and elevation (9 ÀIH Y H Y IH ).</formula><p>We used models of a cow, wolf, hippopotamus, two different cars, and a child. There is weak geometrical similarity between all the 45 silhouettes of the three mammals, and there is weak geometrical similarity between the 30 different silhouettes of the two cars. A ªgoodº shape categorization procedure should reveal this hidden hierarchical structure.</p><p>All the images were automatically preprocessed to extract the silhouettes of the objects and represent them syntactically. The dissimilarities between the silhouettes were estimated using the algorithm described in <ref type="bibr" target="#b9">[10]</ref>. In order to compare all the image pairs in our database of 121 images, we performed 7,260 matching assignments; this took about 10 hours on an INDY R4400 175Mhz workstation (about 5 seconds per image pair, on average). The output of this computation is a dissimilarity matrix d ij of size IPI Â IPI, shown in Fig. <ref type="figure" target="#fig_9">12a</ref>. Our next step is to represent the images as nodes in a graph and assign a similarity weight w ij to each edge. In accordance with (2), we define</p><formula xml:id="formula_6">w ij e À d P ij P Y</formula><p>where is a local scale parameter, chosen here to be the average distance to the second nearest neighbor. After transforming d ij to w ij , we threshold the edges (by IH ÀS ) to obtain a sparse representation. Fig. <ref type="figure" target="#fig_9">12b</ref> shows the graph of Á r, while the corresponding hierarchical classification (dendrogram) is shown in Fig. <ref type="figure" target="#fig_10">13</ref>. At the highest level (r I), all the images belong to a single cluster. As r is increased, finer structure emerges. Note that related clusters (like the two car clusters) split at higher r values; comparing with the ideal human perceptual classification, our finest resolution level is almost perfect, with only two classification errors (in the boxes marked by Ã) and the undesirable split of the fish cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Comparisons</head><p>Our image classification procedure consists of two parts: the estimation of pairwise dissimilarities and the detection of clusters. In all our comparison experiments, we used the same matrix w ij as input to eliminate any dependence on the data preprocessing. We applied a few agglomerative algorithms and two spectral methods. The conclusion from the performance of the agglomerative techniques is that the images form chained structures, where images in the same cluster might be related to each other not directly but through ªmediatingº images. Since the nearest neighbors of all the images are in the same class, the single linkage algorithm performs quite well, although it does not give hints as to the natural thresholds which determine the hierarchies. On the other hand, the complete linkage algorithm, which seeks compact structures, fails completely.</p><p>The quality of the results obtained by the normalized cut algorithm is comparable with ours. While our algorithm computes the whole hierarchy at once, the normalized cut algorithm needs to be applied recursively to each part. The stopping condition of this divisive process is not well formalized; the division should be halted when the entries of the eigenvector do not show a clear partition into two groups. We have inspected the entries and halted the divisive process when we could not ªseeº a clear partition. In the final stage, most of the natural classes were correctly identified, with the exception of the fish class (which was broken into three sets), and the tools classÐmembers of which had been erroneously merged with the hippo class and the wolf class.</p><p>Finally, we recursively applied the factorization method to our data. The final partition was worse than the previous two algorithms, but still sensible: The classes of six toy models were correctly identified, although the 30 images of the two car classes could not be separated; in the final identification of the remaining classes, a few members were typically missed, while the fish class was completely broken to pieces. In addition, this method did not suggest any hierarchical interpretation of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison with Other Work: Summary</head><p>The spiral arrangement of points in the example studied in Section 2.3 and the added noise were chosen in order to emphasize a few advantages of our method. First, our algorithm does not exploit prior knowledge to model the data distribution, hence, arbitrary cluster shapes can be handled. This is in contrast with parameter estimation methods, that usually assume Gaussian distributions. Second, our algorithm is robust to noise. This is in contrast with nearest-neighbor methods, such as single linkage, which are sensitive to ªbridgesº connecting large clusters. Third, though our algorithm is not given the desired number of clusters, it finds the desired natural partitions. Moreover, to find these partitions, we do not use recursive divisions into two parts, the method employed by most other hierarchical methods; hence, we have a natural possibility to leave problematic points unclassified. Some of our results were compared above with other methods, including the split-and-merge segmentation algorithm and the recently proposed normalized cut algorithm <ref type="bibr" target="#b36">[37]</ref> and the factorization algorithm <ref type="bibr" target="#b27">[28]</ref>. We conclude that there are cases where our method works well while the other segmentation methods may fail completely (Figs. <ref type="figure" target="#fig_2">4</ref> and<ref type="figure" target="#fig_16">18</ref>). In other cases, like in the image segmentation of Fig. <ref type="figure" target="#fig_3">5</ref>, our method appears to work better. In particular, in this example, we succeed to segment fine details such as different body parts which the normalized cut algorithm failed to find. In the clustering of image database, the performance of our algorithm and the normalized-cut was comparable, while the factorization method performed  <ref type="figure" target="#fig_0">12</ref>, which is the input to the current step. As can be seen in Fig. <ref type="figure" target="#fig_0">12</ref>, the clustering task is not easy since many entries which are off the diagonal blocks are quite dark, and vice versa. Recall also that the input matrix does not come with the correct rows and columns order. In the dedrogram shown, the finest classification level is shown by putting each cluster of silhouettes in a box. For the large clusters representing our own toy models (see text), the figure shows only five exemplars out of 15, but the other 10 are classified correctly as well. Note that the lower levels of the tree correspond to meaningful hierarchies, where similar classes (like the two cars or the three sets of mammals) are grouped together. The vertical axis is not drawn to scale.</p><p>somewhat worse. Finally, our clustering algorithm is general enough to handle perceptual grouping, showing better performance over other methods, such as the factorization method <ref type="bibr" target="#b27">[28]</ref> (while the normalized cut method completely fails as it typically runs into singularities when dealing with such problems).</p><p>It must be noted that the complexity of our algorithm appears to be lower: While our running time for sparse graphs is yx log P x, the complexity of the normalized cut for sparse graphs is yvx, where v is the maximal number of matrix-vector multiplications allowed in the Lanczos procedure. The number v depends on many factors and Shi and Malik observed that it is typically less then y x p . In addition, our algorithm provides hierarchical clustering in a single path and at no additional cost and it provides better measures of which solution to choose and when to stop the divisive process. On the other hand, we note that the brute-force (or high-complexity) implementation of the normalized cut algorithm is by far simpler, a few lines in matlab in its simplest form. A parallel implementation of our algorithm is trivial since it is constructed from ylog x independent iterations. Thus, the total work can be easily divided between ylog x processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE TYPICAL CUT ALGORITHM: DETAILS</head><p>We now provide an extended description and analysis of our algorithm. In Section 4.1, we discuss the efficient implementation of our algorithm and complete its complexity analysis. Section 4.2 uses a few synthetic examples to demonstrate the robustness of our method. In Section 4.3, we quote the known properties of the probability distribution that is imposed by the contraction algorithm (introduced in Section 2.2). The full implementation is available from [44].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Efficient Implementation</head><p>The efficient implementation of the algorithm is based on two observations and one assumption. The assumption is that the typical cut is not sensitive to the pairing probability p r ij of an edge whose original weight w ij is small. This assumption is not straightforward; although a zero weight edge is never selected during contraction, its associated probability p r ij will be nonzero for some rs due to transitive relations via other nodes which put nodes i and j in the same metanode. Our assumption is that, since p r ij , in this case, is determined solely by transitive relations, we can rely on them in the formation of connected components by STAGE-2. Thus, if very small weight edges are discarded, the number of pairing probabilities to compute reduces from x Q to xjij.</p><p>Two important observations allow us to construct an efficient algorithm. First, the value of p r ij is never used; we only need to know whether it is smaller or larger than I P . This observation is used in Section 4.1.1 to further reduce the number of computed variables down to yjij. Second, the contraction of a sparse graph can be implemented in yjij log x time. This is shown in Section 4.1.2. Finally, Section 4.1.3 shows how the typical cuts at successive values of r can be computed incrementally. We notice that p r ij is monotonic in r by construction, with p x ij H and p I ij I since, in a single contraction path, if i and j belong to the same metanode of q H r for some r, then they remain in the same metanode for all subsequent (smaller) values of r, see procedure STAGE-1 in Section 2.2. Thus, in order to know for each r whether p r ij is smaller or larger than 0.5, it is sufficient to know for every i-j pair which r satisfies p r ij HXS. We denote it r ij . We then know that p r ij `HXS if and only if r b r ij . The immediate consequence is that the number of computed variables is reduced from xjij to jij. Another consequence, which will be discussed in Section 4.1.2, is that the computation of the typical cuts by STAGE-2 becomes incremental.</p><p>To see how r ij is estimated, we refer again to procedure STAGE-1, and observe the nodes i and j at the mth iteration (m I F F F w). As already discussed, there is a single r value, denoted here as r m , in which the edge iY j is contracted. Thus, in the mth iteration, nodes i and j are at different metanodes for every r b r m and at the same metanode for every r r m . It is easy to see that the median r H of the sequence fr I Y r P F F F r w g is the sample estimate for the level r ij . This is simply because at level r H there are waP iterations in which nodes i and j belong to the same metanode and waP iterations in which they do not.</p><p>The rest of this section describes how the median r H is approximated (for each i-j pair) without storing the whole sequence of r m s. Assume that the sequence has been somehow arranged in u bins, such that every bin contains the same number of elements. We could then use the mean of the middle bin as an estimator for the median. Note that u I would give the whole sequence average as the estimator. A larger u (but, still u ( w) would give a better estimate due to the nonsymmetric distribution of the r m s over the positive real axis.</p><p>The question is how to (approximately) arrange the sequence in such a uEins histogram. We use the following heuristic, inspired by an online uEmens algorithm for nonstationary data <ref type="bibr" target="#b13">[14]</ref>. For every bin k (k I F F F u, we store its running average k and the number n k of elements accumulated in it. The two lists f I F F F u g and fn I F F F n u g together will be called below the histogram associated with a certain edge. Suppose that the first m À I values r I F F F r mÀI were used to create a histogram and a new value r m is obtained at the mth iteration. The histogram is updated using the procedure INSERT described in Fig. <ref type="figure" target="#fig_12">14</ref>. The idea is to merge the two adjacent bins, which together include a minimal number of elements and create a new bin which contains just the new element. This procedure almost completely eliminates any dependence on the first collected measurements. When the whole sequence has been accumulated, we find the minimal k H that satisfies k H kI n k b waP and we take k H to be the median estimator. For future reference, the operation that returns k H is called MEDIAN.</p><p>Experimentally, our strategy is found to be successful in approximating the medians of test distributions, which are not symmetric. It is also found to be insensitive to the prefix of the supplied sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Contraction of Sparse Graphs</head><p>The term ªgraph contractionº refers to a single path from an x-way cut to a 2-way cut. A single graph contraction generates for every r (r x F F F I) a single sample of an rEwy cut by repeating the following three steps until two metanodes remain:</p><p>. Select edge iY j with probability proportional to w ij . . Replace (meta) nodes i and j by a single metanode fijg. . Let the set of edges incident on fijg be the union of the sets of edges incident on i and j, but remove selfloops formed by edges originally connecting i to j. In this section, we propose a novel algorithm for the contraction of sparse graphs. Our algorithm uses the building blocks of the classical union-find algorithm, which partitions a set of items into equivalence classes. Accordingly, a name is assigned to each metanode, which for convenience is the node number of one of its members. Two operations are defined: . The UNION operation gets two metanodes names and assigns the name of the larger metanode to the members of the smaller one. . The FIND operation gets a node index and returns the name of the metanode of which this node is a member. Our algorithm is described by the pseudocode in Fig. <ref type="figure" target="#fig_13">15</ref>, explained line by line below. For completeness, we have included in the code the outermost loop, which repeats the contraction w times and the procedures INSERT and MEDIAN discussed in the preceding section. Hence, we present here a complete new version of the procedure STAGE-1 introduced in Section 2.2.</p><p>Line 1. The estimation of the 0.5-probability level uses a histogram data structure, as discussed in Section 4.1.1. For every edge e P i, the assigned histogram is re.</p><p>Line 2. is a binary tree with jij leaves. Each leaf represents an edge and can be directly accessed by a leaf index. Both top-down and bottom-up traversals are supported (this tree supports the efficient selection of edges).</p><p>Line 3. Repeats the contraction w times and collects the data for median estimation.</p><p>Line 4. Initialization of the binary tree . The procedure INITIALIZE assigns each leaf of with the weight of the corresponding edge and each inner node of with the sum of weights of its two children.</p><p>Lines 5 and 6. The variable ne keeps the number of edges which cross the current cut, hence initially it is set to jij (every node is a metanode of size 1). The level r is set to x (the number of parts in the current cut).</p><p>Line 7. The contraction loop implements the edge selection, the metanodes unification, and the maintenance of the tree data structure. Since the number of metanodes is reduced by one at each iteration, the loop is executed no more than x À I times.</p><p>Line 8. Stochastic edge selection. The procedure SELECT-EDGE returns one of the leaves of with probability proportional to its weight. The selection is implemented by constructing a stochastic path from the root of toward the selected leaf. At each inner node, a coin is flipped in order to decide whether the path continues to the left or to the right child, where the probability of selecting a child is proportional to its weight. The procedure returns the index (lef) of the chosen edge.</p><p>Line 9. Identification. The procedure META-NODES-NAMES gets the index of the chosen edge and finds which two metanodes u and v are connected by this edge. It uses a fixed lookup table to convert the edge index (lef) into a pair of indices i and j of nodes in the original graph.</p><p>Using the FIND operation, the metanodes names are given by u psxhi and v psxhj.</p><p>Line 10. The task of the procedure CONNECTING-EDGES is twofold. It applies the UNION operation to merge the selected metanodes u and v and, in addition, it returns a list of all the edges connecting them. Each member in the returned list v is a leaf index, of an edge connecting u and v. The algorithm for the constructing of the list v is described separately below, under the title ªComplements and complexity analysis.º</p><p>Lines 11 and 12. The number of crossing edges (ne) and the number of metanodes (r) are decreased, as a consequence of the contraction.</p><p>Line 13. Loop over all contracted edges. Each one of them has just became an inner edge of a metanode, hence, we need to prevent its reselection and to store the level r in which the contraction occurred.</p><p>Line 14. Maintenance of the tree data structure. To prevent reselection of a contracted edge, the procedure DISCARD-EDGE set its leaf weight to zero (remember that the leaves can be directly accessed by their index).</p><p>The procedure DISCARD-EDGE then follows the path from the leaf to the root of and subtracts the weight of the leaf from each node that is passed through.</p><p>Line 15. Update the histogram of edge number lef with its contraction level r in the mth iteration. The procedure INSERT is described in Fig. <ref type="figure" target="#fig_12">14</ref>.</p><p>Lines 19-22. Use the information accumulated in the histograms during the w graph contractions to estimate the level of 0.5 probability for every edge. The conversion from lef index to iY j notation at line 20 is included for consistency with the notation of Section 4.1.1, where the procedure MEDIAN is described.</p><p>Complements and complexity analysis. In the rest of this section, we describe in more details the procedure CONNECTING-EDGES and we analyze the asymptotic complexity of our algorithm. For both objectives, we must provide some implementation details.</p><p>Let us define the following mappings. The array nmes is a mapping from node indices to metanodes names, hence, nmesi u if node i belongs to metanode u. The array memers is a mapping from metanodes names to lists of members, hence, memersu is a list of nodes belonging to u. The array sizes is a mapping from metanodes names to their sizes, hence, sizesu n if metanode u contains n nodes.</p><p>With these mappings, psxhi returns the value of nmesi, and is clearly an yI operation. On the other hand, assuming without loss of generality that sizesu ! sizesv, the operation UNION(uY v) takes ysizesv time. Instead of describing it separately, we incorporate the union operation into the procedure CONNECTING-EDGES and analyze them together. Fig. <ref type="figure" target="#fig_14">16</ref> shows the implementation.</p><p>In Lines 2-5, we exchange the arguments u and v if u is smaller in size. Thus, we can assume in the rest of the code that sizesv sizesu and, hence, the loop at Lines 6-14 is executed for each member of the smaller metanode (v). Lines 7-12 constructs the list v of connecting edges (see below), and Lines 13, 15, and 17 implement the union operation. We note that, for efficient implementation of the list concatenation at Line 17, it is desirable to keep pointers to the tails of the membership lists. Lines 16 and 18 are added for esthetics, as sizesv and memersv are never accessed again. The section of the code which might appear nontrivial is the construction of the list v at Lines 7-12. We assume that the graph q is stored in the form of adjacency lists. A member of the adjacency list of node i has the form fjY lefg, where j is a node adjacent to i, and lef is the index of the edge connecting them. The loop at Line 7 queries all the edges which incident on nodes in v, checking if their other vertex is in u (Line 9). If this is the case, then the edge index is added to the list.</p><p>Let us analyze the complexity of CONNECTING-EDGES. We are interested in sparse graphs, where the maximal length of an adjacency list is a constant independent of x. In this case, the inner loop at Lines 7±12 iterates a constant number of times, and can be ignored in the analysis of asymptotic complexity. However, ignoring this part, we obtain a usual implementation of UNION, which is known to be yx log x for the entire contraction loop. We repeat the argument below.</p><p>Line 13 moves the members of v to the new metanode u. The time spent by a single union operation is therefore proportional to the number of nodes which are moved to a new metanode, namely, ysizesv. Since we always move the nodes of the smaller metanode, it turns out that whenever a node is moved to a new metanode, its new metanode's size is at least twice as large as the former one. As a consequence, no node can be moved more than log x times during the x À I union operations which contract the graph. In other words, the total amount of time spent by all union operation during the contraction loop (Line 7, Fig. <ref type="figure" target="#fig_13">15</ref>) is yx log x.</p><p>To complete the complexity analysis, we refer again to the pseudocode in Fig. <ref type="figure" target="#fig_13">15</ref>. The call to INITIALIZE at Line 4 costs yjij time. Every call to SELECT-EDGE at Line 8 costs ylog jij time and every call to META-NODES-NAMES at Line 9 is of complexity yI. These bounds accumulate to yx log jij and yx, respectively, during one execution of the contraction loop (Line 7). The overall complexity of CONNECTING-EDGES at Line 10 has been shown above to be yx log x during one graph contraction. The procedures DISCARD-EDGE and INSERT at Lines 14 and 15 are executed exactly once for every graph edge during one execution of the contraction loop. Since the complexity of DISCARD-EDGE is ylog jij and that of INSERT is yI, the total amount of time spent by them during one graph contraction is yjij log jij and yjij, respectively. Finally, the computation of all medians by the loop at Line 19 is yjij. Summarizing these bounds and using the fact that jij x P A ylogjij ylogx, we conclude that a single graph contraction can be implemented in yjij log x time. This complexity multiplied by w (Line 3) is the total complexity of computing all the 0.5 probability levels. Using the result of Section 2.4, the overall complexity of EFFICIENT-STAGE-1 is yjij log P x time and yjij space. For sparse graphs, where jij yx, this is the same as yx log P x time and yx space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">The Second Stage</head><p>In Section 4.1.1, we claimed that the monotonic dependence of p r ij on r leads to an incremental and efficient computation of the typical cuts. Here, we show how this is done. The basic idea is to sort the graph edges in decreasing order of r ij (the level of 0.5 probability). To find the typical cut at level r, we mark all edges for which r ij ! r, then we find connected components of marked edges. Proceeding from level r to level r À I, marked edges can only be added due to the monotonic dependence of p r ij on r. Hence, the partition at level r À I can be found incrementally from the partition at level r.</p><p>Fig. <ref type="figure" target="#fig_15">17</ref> summarizes our efficient STAGE-2 algorithm. The list is sorted at Line 1 using quicksort, which takes yjij log x time on average. The data structures nmes, memers, and sizes in Line 3 are described in the previous section. They are initialized in accordance with level r x, where every p r ij H, and the typical cut is necessarily a partition to x clusters of size 1. The name which is given to every cluster is its node number.</p><p>The loop in Lines 4-16 is a classical UNION-FIND algorithm for a dynamic graph, combined with incremental computation of Á r, as defined in <ref type="bibr" target="#b0">(1)</ref>. The complexity of the loop is yx log x, as discussed in Section 4.1.2.</p><p>Line 17 is an optional heuristic to handle small, perhaps not interesting clusters. As discussed in Section 2.2, boundary and background points typically form small independent clusters at levels which are considered meaningful. In a top-down view, inspecting the obtained partitions from the smallest r value to the largest, we observe that beside the splitting that a cluster can undergo into two large clusters (a meaningful transition), it may as well gradually loose its boundary or abruptly crash into small pieces. This happens when the certainty of labeling is too small, namely, the values p r ij of the connecting edges have dropped below I P . Hence, there is a trade-off: One may keep the uncertain points connected, at the cost of reduced certainty.</p><p>Our relabeling algorithm assumes that a minimal size of interest for a cluster is known, denoted . After the typical cut at level r is found, we sustain the clusters whose sizes are at least , and relabel the others. For this, we scan the sorted list to its end and apply for every triplet fiY jY r ij g the operation UNION(FIND(i),FIND(j)) if and only if the three following conditions hold:</p><p>. Either i or j (or both) belong to a small cluster (size `). . Nodes i and j were members of the same cluster at the former partition (with smaller r). . Consistency is obeyed, namely, when all the triplets having the same r ij value are added at once, no node is doubly relabeled by being connected to more than one sustained cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Robustness</head><p>Two aspects of robustness are discussed below, including robustness with respect to data perturbation in Section 4.2.1 and robustness with respect to the clustering hypothesis in Section 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Data Perturbation</head><p>Fig. <ref type="figure" target="#fig_16">18</ref> shows a task involving the separation of two point sets generated by different statistical sources. The issue addressed here is the response of the various algorithms to the amount of noise in the data. The normalized cut algorithm performs well at low levels of noise (left column), but, as the noise is increased, it abruptly breaks down and returns false segmentation (right column). The factorization method behaves in a similar way, although it breaks down sooner (for lower levels of noise, left column). In comparison, as the level of noise is changed continuously, the performance of our algorithm degrades gracefully and continuously, without ever showing an ªabruptº breakdown as the spectral methods do; we believe that this is another beneficial result of the stochastic nature of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">False Structure</head><p>One problem with most clustering methods is that they look for structure and, thus, find something, even when the data is unstructured; this is often a single point solution obtaining the global optimum of some criterion, with the property that ªsimilarº solutions (obtained from the chosen solution by a small perturbation) are far from optimal. Soft clustering methods avoid this pitfall, while the stochastic nature of our method, which essentially looks for a distribution over the solution space, also makes it robust against the identification of false structures. As an example, we studied the clustering of a complete graph (clique)</p><p>where all the weights have roughly (but not exactly) the same weight. Our algorithm found a single significant transition, identifying two possible interpretations: either all the points are in one cluster or they are all isolated. On the other hand, deterministic agglomeration methods suggested a solution for every number of desired clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Probability Bounds</head><p>The contraction scheme was originally invented as the core of a probabilistic minimal cut algorithm, which finds the capacity of the minimal cut with high probability by repeating the contraction procedure a sufficient number of times <ref type="bibr" target="#b21">[22]</ref>. We quote here some results concerning the contraction algorithm, and the reader is referred to <ref type="bibr" target="#b21">[22]</ref> for details. The probability of the algorithm to return a particular minimum 2-way cut is at least P x x ÀP , hence, repeating it yx P log x times gives a high probability of finding the minimum value in some trial. This large number of trials is required to guarantee that the minimum is obtained. In our case, we are not interested in the minimal capacity value, but rather in the pairing probabilities, which are average quantities. We have shown in Section 2.4 that, in this case, a sample of size The magnitude of the entries are plotted versus their serial index. These methods seek a threshold which separates between small and large entries. The color and the symbol used for the eigenvector entries corresponds with those used in the two dimensional plot.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Pseudocode which transforms similarity weights into pairing probabilities.</figDesc><graphic coords="4,99.95,69.17,366.58,266.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Clustering of points in the Euclidean plane. (a) The 2,000 data points. The coordinates of the points are not available to the clustering algorithm, which uses only the matrix of pairwise distances. (b) The graph of Á r computed for every integer r between 1 and x. Two peaks are clearly observed at r QIW and r QSP. (c), (d), (e), and (f) From left to right: The typical cuts at r QIVY QIWY QSI, and QSP. Different components are indicated by different colors and symbols, while isolated points (clusters of size one) are marked by small black dots.</figDesc><graphic coords="6,39.86,69.17,486.77,337.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The results of other algorithms applied to the data of Fig. 3. The same preprocessing and exponential transformation from distances to weights was used (see text). (a) The best normalized cut [37] partition. (b)The result obtained by the factorization method<ref type="bibr" target="#b27">[28]</ref>. (c) Deterministic single linkage. Unlike our randomized algorithm, the deterministic single linkage algorithm is sensitive to ªbridgesº that connect large clusters. Here, the procedure is halted manually when three large clusters exists and just before two of them merge together. The desired structure is already irrevocably missing.</figDesc><graphic coords="6,87.59,572.20,391.29,136.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Brightness image segmentation: the original image of size 147x221, the impulse graph of Á r, and the four segmentation results which correspond to the four highest peaks. Parameter setting: Intensity range is [0,1], V, HXI, edges with weight w ij below 0.01 are eliminated, and only edges connecting each pixel with its four spatial nearest neighbors plus four random neighbors are included. Minimal cluster size of interest is 100. The graph contained 191,756 edges. Time per iteration: 2.11sec on Pentium II 450 Mhz. w IY HHH.</figDesc><graphic coords="9,47.45,69.17,471.51,268.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Split-and-merge segmentation results for the image in Fig.5with a few parameter values (variance thresholds v).</figDesc><graphic coords="9,54.20,388.06,458.08,91.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Color image segmentation in CIE-LAB color space: the original image of size 122x183, the impulse graph of Á r (insert shows an enlarged graph), and the three segmentation results which correspond to the three highest peaks. Parameter setting: Colors are represented in CIE-LAB color space, TR, W, edges with weight w ij below 0.001 are eliminated, and only edges connecting each pixel with its eight spatial nearest neighbors are included. Minimal cluster size of interest is 100, and a sample of w SHH cuts is used to estimate the pairing probabilities. The graph contained 71,765 edges. Time per iteration: 0.89sec on Pentium II 450 Mhz.</figDesc><graphic coords="10,53.01,69.17,460.46,287.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Color image segmentation in CIE-LUV color space: the original image of size 256x192, the impulse graph of Á r, and the four segmentation results which correspond to the four highest peaks. Parameter setting: Colors are represented in CIE-LUV color space, TR, W, edges with weight w ij below 0.001 are eliminated, and only edges connecting each pixel with its four spatial nearest neighbors plus four random neighbors are included. Minimal cluster size of interest is 100. The graph contained 255,837 edges. Time per iteration: 4.10sec on Pentium II 450 Mhz; w SHH.</figDesc><graphic coords="11,51.53,69.17,463.35,338.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Separation of homogeneous objects from noisy background. (a) Synthetic 30x42 image, generated with the same parameters as in [28]: The brightness values are uniformly distributed between 0 and 1 for the background, between 0.2 and 0.21 for the larger rectangle, and between 0.3 and 0.31 for the smaller one. (b) The graph of Á r: the largest peak appears at r TSR, the second largest appears at r IRT. Minimal size of interest for a cluster is 10. (c) and (d) The segmentation results at r TSRY TSS that correspond to the largest peak of Á r. Pixels in the background form isolated or tiny clusters. Parameter setting: Q, HXI (like in [28]), only edges with the eight nearest neighbors of each pixel are included.</figDesc><graphic coords="11,58.73,602.99,449.01,105.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Perceptual grouping of edge elements. (a) Original data, a lemon contour on random background. (b) The impulse graph of Á r, when the minimal size of interest for a cluster is set to 10. In this setting, seven transitions are detected. (c) and (d) The transition between r PPW and r PQH which corresponds to the last peak in (b). Edge elements which are isolated (form clusters of size `10) are not shown. The meaningful peak results from breaking the lemon contour into two parts; it is not high since the cluster of interest is small in size.</figDesc><graphic coords="12,55.28,587.06,455.98,130.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. (a) The dissimilarity matrix d ij computed for 121 images as described in [10], by silhouettes extraction, syntactic matching, and computation of residual distances between matched points. Larger dissimilarity values are represented by brighter intensities. The infinity value is represented by pure white. Images which belong to the same class are ordered next to each other, hence the matrix shows a block diagonal structure. The order of the classes is: cows, hippos, wolves, cars, sport cars, children, hand palms, fish, planes, rabbits, tools, and artificial shapes. (b) Clustering (Section 2):The impulse graph of Á r versus r, computed with 1,000 iterations. The peaks correspond to the partitions shown in Fig.13. We have considered peaks above 0.01 to be meaningful.</figDesc><graphic coords="13,111.86,69.17,342.77,153.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The classification tree obtained for an image database consisting of 121 images. It is based on the distance matrix shown in Fig.12, which is the input to the current step. As can be seen in Fig.12, the clustering task is not easy since many entries which are off the diagonal blocks are quite dark, and vice versa. Recall also that the input matrix does not come with the correct rows and columns order. In the dedrogram shown, the finest classification level is shown by putting each cluster of silhouettes in a box. For the large clusters representing our own toy models (see text), the figure shows only five exemplars out of 15, but the other 10 are classified correctly as well. Note that the lower levels of the tree correspond to meaningful hierarchies, where similar classes (like the two cars or the three sets of mammals) are grouped together. The vertical axis is not drawn to scale.</figDesc><graphic coords="14,118.83,69.17,328.82,318.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>4. 1 . 1</head><label>11</label><figDesc>Computing the Level-Crossing of Probability 0.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Updating an existing histogram with a new value.</figDesc><graphic coords="15,95.53,69.17,375.42,156.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Efficient implementation of the transformation from similarity weights to pairing probabilities.</figDesc><graphic coords="16,104.94,69.17,356.60,347.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Merging two metanodes and finding all the edges which connect them.</figDesc><graphic coords="18,112.37,69.17,341.80,330.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Efficient implementation of the second stage.</figDesc><graphic coords="19,78.80,69.17,408.87,290.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Robustness under data perturbation. The control parameter defines the spread of the points, or the ªwidthº of each circle. Results for two values, HXU and HXW, are shown: The factorization algorithm fails for both, the normalized cut algorithm fails for the the larger noise level, while our algorithm always finds the desired structure. For the spectral methods, we show the entries of the relevant eigenvector next to the partition found.The magnitude of the entries are plotted versus their serial index. These methods seek a threshold which separates between small and large entries. The color and the symbol used for the eigenvector entries corresponds with those used in the two dimensional plot.</figDesc><graphic coords="20,69.96,69.17,426.50,363.06" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A parameter which is fixed in our algorithm, but, in principle, could be treated as a free one, is the threshold (0.5) in the definition of the typical cut. However, the algorithm is not sensitive to the exact value of the threshold. In fact, the p r ij values tend to be either close to 0 or to 1<ref type="bibr" target="#b0">[1]</ref>, which is the reason for the abrupt transitions that we detect in STAGE-2. Under small changes of the threshold value, the selected r levels may change slightly, but the partitions found are hardly affected.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The implementation of the split-and-merge algorithm was taken from KUIM, an image processing system provided by Professor John M. Gauch from the University of Kansas.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Ben Kimia and Daniel Sharvit for 31 silhouettes that are used in Section 3.4. They would also like to thank Avner Magen for pointing them to Karger's work and Ido Bregman for helping in the complexity analysis.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ylog xa P is sufficient for an accuracy level with high probability.</p><p>Every cut value has a certain probability to be returned by the contraction algorithm, but the functional form of the induced probability distribution is unfortunately not known. There is a lower bound, however, on the probability that an Eminiml cut is returned (a cut whose capacity is at most times the minimum). The probability to contract a crossing edge of a particular Eminiml cut is asymptotically less than x ÀP by the time that r P metanodes remain. Note that contracting a crossing edge of a cut prevents it from being returned by the algorithm, hence, an upper bound on the probability to contract a crossing edge is a lower bound on the survival probability of the cut. The generalization to rEwy cuts yields the probability x ÀPrÀI that a particular minimum rEwy cut is returned (if contraction is stopped when r metanodes remain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SUMMARY AND DISCUSSION</head><p>Our goal in this work was to apply cluster analysis as a unified approach for a wide range of vision applications. Many vision applications can be viewed as data partitioning problems and can be addressed by this approach. In particular, we have focused on the tasks of image segmentation, edge elements grouping, and image organization. Thus, in Section 3, we experimentally investigated the plausibility of our goal and showed that, using our clustering algorithm, we can address different problems in computer vision.</p><p>The first step is to define the pairwise similarity between the visual entities: pixels, edgels, or shapes. The second step is to apply cluster analysis to detect internal structures among the visual entities (pixels, edgels, or images). Hence, we map the relevant entities to the nodes of a graph, while their pairwise relations are used to assign weights to the graph edges.</p><p>When the visual entities are represented as graph nodes and edge weights are assigned, our novel clustering method can be applied. The central insight to our algorithm is that it converts the pairwise similarity weights into higher order weights or ªcollectiveº similarities. The collective similarity of two nodes iY j depends on an integer number r. It is the probability p r ij that i and j are on the same side of a random rEwy cut which is generated by the contraction algorithm. The analysis of the contraction algorithm shows that these pairing probabilities are dominated by the low capacity cuts.</p><p>Nevertheless, the pairing probabilities are not determined by a single pivotal cut (e.g., the minimum cut) and, at this point precisely, our algorithm defers from most other clustering algorithms. The usual approach to clustering can be viewed as a search algorithm in some hypothesis space, where a hypothesis is a feasible partition. Whatever search method is applied, whether it is heuristic or guided by an objective function, a regular search algorithm ends in some point in the hypothesis space and returns it as a single possible solution (Section 4.2.2). On the other hand, our algorithm (and clustering algorithms which are inspired by statistical mechanics) induces a probability distribution over the hypothesis space and returns an average solution. Thus, we are not committed to a single partition and our algorithm acquires a great amount of robustness. Indeed, we demonstrated in Section 4.2.1 that our algorithm is more stable than spectral algorithms under data perturbation.</p><p>The algorithm generates an average partition at each r level, which is called the typical cut. The definition of the typical cut relies on the interpretation of I À p r ij as the probability that edge iY j is a crossing edge in an ªaverage cutº (Section 2.2). The x typical cuts corresponding to r I F F F x are the candidate solutions to the clustering problem and a heuristic criterion is applied to select some of them as a hierarchy of partitions. Evidently, successful partitions can be selected only if the set of x candidate typical cuts indeed contains the desired solutions. Whether this is the case or not it depends on the output of the first stage of the algorithm, which transforms the pairwise similarities into the pairing probabilities. F For more information on this or any other computing topic, please visit our Digital Library at http://computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Blatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Domany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªData Clustering Using a Model Granular Magnet</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1805" to="1842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ªFast Approximate Energy Minimization via Graph Cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Computer Vision</title>
		<meeting>Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<title level="m">ªImage Segmentation from Consensus Information,º Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="72" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<title level="m">Spectral Graph Theory</title>
		<meeting><address><addrLine>Providence, R.I.</addrLine></address></meeting>
		<imprint>
			<publisher>AMS Press</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ªA Multibody Factorization Method for Motion Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Computer Vision</title>
		<meeting>Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1071" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ªModeling Similarity and Identification when There Are Momentary Fluctuations in Psychological Amplitudes,º Multidimensional Models of Perception and Cognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ennis</surname></persName>
		</author>
		<editor>F. Ashby</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Lawrence Erlbaum Assoc. Publishers</publisher>
			<biblScope unit="page" from="279" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ªImage Segmentation Using Local Variation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="98" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ªStochastic Clustering and Its Applications to Computer Vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gdalyahu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-04">Apr. 2000</date>
			<pubPlace>Jerusalem</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Hebrew Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ªFlexible Syntactic Matching of Curves and Its Application to Automatic Hierarchical Classification of Silhouettes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gdalyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
			<date type="published" when="1999-12">Dec. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ªA Randomized Algorithm for Pairwise Clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gdalyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="424" to="430" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Gdalyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shental</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<title level="m">ªPerceptual Grouping and Segmentation by Stochastic Clustering,º Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ªStochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ªAn On-Line Agglomerative Clustering Method for Nonstationary Data</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guedalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="521" to="540" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ªInferring Global Perceptual Contours from Local Features,º Int</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="113" to="133" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ªFigure-Ground Discrimination: A Combinatorial Optimization Approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Herault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="899" to="914" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ªUnsupervized Texture Segmentation in a Deterministic Annealing Framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="803" to="818" />
			<date type="published" when="1998-08">Aug. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ªPairwise Data Clustering by Deterministic Annealing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Combinatorial Algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ªSegmentation by Grouping Junctions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="125" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dubes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N.J</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ªA New Approach to the Minimum Cut Problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="601" to="640" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An Introduction to Computational Learning Theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Vazirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<title level="m">ªAuthoritative Sources in a Hyperlink Environment,º Proc. Ninth ACM-SIAM Symp. Discrete Algorithms</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="668" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ªA General Theory of Classification Sorting Strategies: II. Clustering Systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer J</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="271" to="277" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">ªBoundary Detection by Minimizing Functionals,º Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="22" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<title level="m">ªA Review on Image Segmentation Techniques,º Recognition</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1277" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">ªA Factorization Approach to Grouping,º Proc. European Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="655" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mervis</surname></persName>
		</author>
		<title level="m">ªFamily Resemblance: Studies in the Internal Structure of Categories,º Cognitive Psychology</title>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="573" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ªConstrained Clustering as an Optimization Method</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gurewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="785" to="794" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ªQuantitative Measures of Change Based on Feature Organization: Eigenvalues and Eigenvectors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="478" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ªFeature Grouping by Relocalization of Eigenvectors of the Proximity Matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Longuet</surname></persName>
		</author>
		<author>
			<persName><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf</title>
		<meeting>British Machine Vision Conf</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<title level="m">ªFast Multiscale Image Segmentation,º Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ªSymmetry Based Indexing of Image Databases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sharvit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kimia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Comm. and Image Representation</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Detection of Globally Salient Structures Using a Locally Connected Network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><surname>Saliency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Computer Vision</title>
		<meeting>Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="321" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ªToward a Universal Law of Generalization for Psychological Science</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="1317" to="1323" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">ªNormalized Cuts and Image Segmentation,º Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="731" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ªAnalytic Solution of Stochastic Completion Fields</title>
		<author>
			<persName><forename type="first">K</forename><surname>Thornber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybenetics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ªData Clustering by Markovian Relaxation and the Information Bottleneck Method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ªFeatures of Similarity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Rev</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="327" to="352" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<title level="m">ªSegmentation Using Eigenvectors: A Unifying View,º Proc. Int&apos;l Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="975" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªStochastic Completion Fields: A Natural Model of Illusory Contour Shape and Salience</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="849" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ªAn Optimal Graph Theoretic Approach to Data Clustering: Theory and Its Application to Image Segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1101" to="1113" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
