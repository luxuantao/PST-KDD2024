<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 ZERO-SHOT SYNTHESIS WITH GROUP-SUPERVISED LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 ZERO-SHOT SYNTHESIS WITH GROUP-SUPERVISED LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual cognition of primates is superior to that of artificial neural networks in its ability to "envision" a visual object, even a newly-introduced one, in different attributes including pose, position, color, texture, etc. To aid neural networks to envision objects with different attributes, we propose a family of objective functions, expressed on groups of examples, as a novel learning framework that we term Group-Supervised Learning (GSL). GSL allows us to decompose inputs into a disentangled representation with swappable components, that can be recombined to synthesize new samples. For instance, images of red boats &amp; blue cars can be decomposed and recombined to synthesize novel images of red cars. We propose an implementation based on auto-encoder, termed group-supervised zeroshot synthesis network (GZS-Net) trained with our learning framework, that can produce a high-quality red car even if no such example is witnessed during training. We test our model and learning framework on existing benchmarks, in addition to a new dataset that we open-source. We qualitatively and quantitatively demonstrate that GZS-Net trained with GSL outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Primates perform well at generalization tasks. If presented with a single visual instance of an object, they often immediately can generalize and envision the object in different attributes, e.g., in different 3D pose <ref type="bibr" target="#b15">(Logothetis et al., 1995)</ref>. Primates can readily do so, as their previous knowledge allows them to be cognizant of attributes. Machines, by contrast, are most-commonly trained on sample features (e.g., pixels), not taking into consideration attributes that gave rise to those features.</p><p>To aid machine cognition of visual object attributes, a class of algorithms focuses on learning disentangled representations <ref type="bibr" target="#b11">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b8">Higgins et al., 2017;</ref><ref type="bibr" target="#b2">Burgess et al., 2018;</ref><ref type="bibr" target="#b10">Kim &amp; Mnih, 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2018)</ref>, which map visual samples onto a latent space that separates the information belonging to different attributes. These methods show disentanglement by interpolating between attribute values (e.g., interpolate pose, etc). However, these methods usually process one sample at a time, rather than contrasting or reasoning about a group of samples. We posit that semantic links across samples could lead to better learning.</p><p>We are motivated by the visual generalization of primates. We seek a method that can synthesize realistic images for arbitrary queries (e.g., a particular car, in a given pose, on a given background), which we refer to as controlled synthesis. We design a method that enforces semantic consistency of attributes, facilitating controlled synthesis by leveraging semantic links between samples. Our method maps samples onto a disentangled latent representation space that (i) consists of subspaces, each encoding one attribute (e.g., identity, pose, ...), and, (ii) is such that two visual samples that share an attribute value (e.g., both have identity "car") have identical latent values in the shared attribute subspace (identity), even if other attribute values (e.g., pose) differ. To achieve this, we propose a general learning framework: Group Supervised Learning (GSL, Sec. 3), which provides a learner (e.g., neural network) with groups of semantically-related training examples, represented as multigraph. Given a query of attributes, GSL proposes groups of training examples with attribute combinations that are useful for synthesizing a test example satisfying the query (Fig. <ref type="figure" target="#fig_0">1</ref>). This endows the network with an envisioning capability. In addition to applications in graphics, controlled synthesis can also augment training sets for better generalization on machine learning tasks (Sec. 6.3). As an instantiation of GSL, we propose an encoder-decoder network for zero-shot synthesis: Group- Our contributions are: (i) We propose Group-Supervised Learning (GSL), explain how it casts its admissible datasets into a multigraph, and show how it can be used to express learning from semantically-related groups and to synthesize samples with controllable attributes; (ii) We show one instantiation of GSL: Group-supervised Zero-shot Synthesis Network (GZS-Net), trained on groups of examples and reconstruction objectives; (iii) We demonstrate that GZS-Net trained with GSL outperforms state-of-the-art alternatives for controllable image synthesis on existing datasets; (iv) We provide a new dataset, Fonts<ref type="foot" target="#foot_0">1</ref> , with its generating code. It contains 1.56 million images and their attributes. Its simplicity allows rapid idea prototyping for learning disentangled representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We review research areas, that share similarities with our work, to position our contribution.</p><p>Self-Supervised Learning (e.g., <ref type="bibr" target="#b5">Gidaris et al. (2018)</ref>) admits a dataset containing features of training samples (e.g., upright images) and maps it onto an auxiliary task (e.g., rotated images): dataset examples are drawn and a random transformation (e.g., rotate 90 ‚Ä¢ ) is applied to each. The task could be to predict the transformation (e.g., =90 ‚Ä¢ ) from the transformed features (e.g., rotated image).</p><p>Our approach is similar, in that it also creates auxiliary tasks, however, the tasks we create involve semantically-related group of examples, rather than from one example at a time.</p><p>Disentangled Representation Learning are methods that infer latent factors given example visible features, under a generative assumption that each latent factor is responsible for generating one semantic attribute (e.g. color). Following Variational Autoencoders (VAEs, Kingma &amp; Welling, 2014), a class of models (including, <ref type="bibr" target="#b8">Higgins et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 2018)</ref> achieve disentanglement implicitly, by incorporating into the objective, a distance measure e.g. KL-divergence, encouraging the latent factors to be statistically-independent. While these methods can disentangle the factors without knowing them beforehand, unfortunately, they are unable to generate novel combinations not witnessed during training (e.g., generating images of red car, without any in training). On the other hand, our method requires knowing the semantic relationships between samples (e.g., which objects are of same identity and/or color), but can then synthesize novel combinations (e.g., by stitching latent features of "any car" plus "any red object").</p><p>Conditional synthesis methods can synthesize a sample (e.g., image) and some use information external to the synthesized modalities, e.g., natural language sentence <ref type="bibr" target="#b22">Zhang et al. (2017)</ref>; <ref type="bibr" target="#b9">Hong et al. (2018)</ref> or class label <ref type="bibr" target="#b17">Mirza &amp; Osindero (2014)</ref>; <ref type="bibr" target="#b19">Tran et al. (2017)</ref>. Ours differ, in that our "external information" takes the form of semantic relationships between samples. There are methods based on <ref type="bibr">GAN Goodfellow et al. (2014)</ref> that also utilize semantic relationships including Motion Re-targeting <ref type="bibr" target="#b21">(Yang et al., 2020)</ref>, which unfortunately requires domain-specific hand-engineering (detect and track human body parts). On the other hand, we design and apply our method on different tasks (including people faces, vehicles, fonts; see Fig. <ref type="figure" target="#fig_0">1</ref>). Further, we compare against two recent GAN methods starGAN <ref type="bibr" target="#b4">(Choi et al., 2018)</ref> and ELEGANT <ref type="bibr" target="#b20">(Xiao et al., 2018)</ref>, as they are state-of-the-art GAN methods for amending visual attributes onto images. While they are powerful in carrying local image transformations (within a small patch, e.g., changing skin tone or hair texture). However, our method better maintains global information: when rotating the main object, the scene also rotates with it, in a semantically coherent manner. Importantly, our learning framework allows expressing simpler model network architectures, such as feed-forward auto-encoders, trained with only reconstruction objectives, as opposed to GANs, with potential difficulties such as lack of convergence guarantees.</p><p>Zero-shot learning also consumes side-information. For instance, models of <ref type="bibr" target="#b13">Lampert (2009)</ref>; <ref type="bibr" target="#b0">Atzmon &amp; Chechik (2018)</ref> learn from object attributes, like our method. However, (i) these models are supervised to accurately predict attributes, (ii) they train and infer one example at a time, and (iii) they are concerned with classifying unseen objects. We differ in that (i) no learning gradients (supervision signal) are derived from the attributes, as (ii) these attributes are used to group the examples (based on shared attribute values), and (iii) we are concerned with generation rather than classification: we want to synthesize an object in previously-unseen attribute combinations.</p><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b18">(Scarselli et al., 2009)</ref> are a class of models described on graph structured data. This is applicable to our method, as we propose to create a multigraph connecting training samples. In fact, our method can be described as a GNN, with message passing functions <ref type="bibr" target="#b6">(Gilmer et al., 2017)</ref> that are aware of the latent space partitioning per attribute (explained in Sec. 4). Nonetheless, for self-containment, we introduce our method in the absence of the GNN framework.</p><p>3 GROUP-SUPERVISED LEARNING</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DATASETS ADMISSIBLE BY GSL</head><p>Formally, a dataset admissible by GSL containing n samples D = {x (i) } n i=1 where each example is accompanied with m attributes D a = {(a</p><formula xml:id="formula_0">(i) 1 , a (i) 2 , . . . a (i) m )} n i=1</formula><p>. Each attribute value is a member of a countable set: a j ‚àà A j . For instance, pertaining to visual scenes, A 1 can denote foreground-colors A 1 = {red, yellow, . . . }, A 2 could denote background colors, A 3 could correspond to foreground identity, A 4 to (quantized) orientation. Such datasets have appeared in literature, e.g. in <ref type="bibr" target="#b1">Borji et al. (2016)</ref>; <ref type="bibr" target="#b16">Matthey et al. (2017)</ref>; <ref type="bibr" target="#b14">Langner et al. (2010)</ref>; <ref type="bibr" target="#b12">Lai et al. (2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AUXILIARY TASKS VIA MULTIGRAPHS</head><p>Given a dataset of n samples and their attributes, we define a multigraph M with node set</p><formula xml:id="formula_1">[1..n]. Two nodes, i, k ‚àà [1..n] with i = k are connected with edge labels M (i, k) ‚äÜ [1..m] as: M (i, k) = j a (i) j = a (k) j ; j ‚àà [1..m] .</formula><p>In particular, M defines a multigraph, with |M (i, k)| denoting the number of edges connecting nodes i and k, which is equals the number of their shared attributes. Fig. <ref type="figure" target="#fig_2">2</ref> depicts a (sub-)multigraph for the Fonts dataset (Sec. 5.1).</p><formula xml:id="formula_2">Definition 1 COVER(S, i): Given node set S ‚äÜ [1..|D g |] and node i ‚àà [1..|D g |]</formula><p>we say set S covers node i if every attribute value of i is in at least one member of S. Formally:</p><formula xml:id="formula_3">COVER(S, i) ‚áê‚áí [1..m] = k‚ààS M (i, k).</formula><p>(1)</p><p>When COVER(S, i) holds, there are two mutually-exclusive cases: either i ‚àà S, or i / ‚àà S, respectively shaded as green and blue in Fig. <ref type="figure" target="#fig_2">2 (b</ref>). The first case trivially holds even for small S, e.g. COVER({i}, i) holds for all i. However, we are interested in non-trivial sets where |S| &gt; 1, as sets with |S| = 1 would cast our proposed network (Section 3) to a standard Auto-Encoder. The second case is crucial for zero-shot synthesis. Suppose the (image) features of node i (in Fig. <ref type="figure" target="#fig_2">2</ref>   not given, we can search for S 1 , under the assumption that if COVER(S 1 , i) holds, then S 1 contains sufficient information to synthesize i's features as they are not given (i / ‚àà S 1 ).</p><p>Until this point, we made no assumptions how the pairs (S, i) are extracted (mined) from the multigraph s.t. COVER(S, i) holds. In the sequel, we restrict ourselves with |S| = 2 and i ‚àà S. We find that this particular specialization of GSL is easy to program, and we leave-out analyzing the impact of mining different kinds of cover sets for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GROUP-SUPERVISED ZERO-SHOT SYNTHESIS NETWORK (GZS-NET)</head><p>We now describe our ingredients towards our goal: synthesize holistically-semantic novel images. (2)</p><p>M denotes the space of sample pairwise-relationships. GSL realizes one such (X, M ) ‚àà X √ó M.</p><p>Rather than passing as-is the output of E into D, one can modify it using algorithm A by chaining:</p><formula xml:id="formula_4">D ‚Ä¢ A ‚Ä¢ E.</formula><p>For notation brevity, we fold A into the encoder E, by designing a swap operation, next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DISENTANGLEMENT BY SWAP OPERATION</head><p>While training our auto-encoder D(E(X, M )), we wish to disentangle the latents output by E, to provide use for using D to decode samples not given to E. D (/ E) outputs (/ inputs) one or more images, onto (/ from) the image space. Both networks can access feature and relationship information.</p><p>At a high level, GZS-Net aims to swap attributes across images by swapping corresponding entries across their latent representations. Before any training, we fix partitioning of the the latent space</p><formula xml:id="formula_5">Z = E(X, M ). Let row-vector z (1) = [g (1) 1 , g<label>(1)</label></formula><p>2 , . . . , g</p><p>m ] be the concatenation of m row vectors {g To simplify the notation to follow, we define an operation swap :</p><formula xml:id="formula_7">R d √ó R d √ó [1..m] ‚Üí R d √ó R d ,</formula><p>which accepts two latent vectors (e.g., z (1) and z (2) ) and an attribute (e.g., 2) and returns the input vectors except that the latent features corresponding to the attribute are swapped. E.g.,</p><formula xml:id="formula_8">swap(z (1) , z (2) , 2) = swap([g (1) 1 , g<label>(1)</label></formula><p>2 , g</p><formula xml:id="formula_9">(1) 3 , . . . , g (1) m ], [g (2) 1 , g<label>(2)</label></formula><p>2 , g</p><formula xml:id="formula_10">(2) 3 , . . . , g (2) m ], 2) = [g (1) 1 , g (2) 2 , g<label>(1)</label></formula><p>3 , . . . , g (1)  m ], [g</p><formula xml:id="formula_11">(2) 1 , g<label>(1)</label></formula><p>2 , g</p><p>3 , . . . , g (2)  m ] One-Overlap Attribute Swap. To encourage disentanglement in the latent representation of attributes, we consider group S and example x s.t. COVER(S, x) holds, and for all x o ‚àà S, x = x o , the pair (x o , x) share exactly one attribute value (|M (x o , x)| = 1). Encoding those pairs, swapping the latent representation of the attribute, and decoding should then be a no-op if the swap did not affect  other attributes (Fig. <ref type="figure" target="#fig_6">3b</ref>). Specifically, we would like for a pair of examples, x (red border in Fig. <ref type="figure" target="#fig_6">3b</ref>) and x o (blue border) sharing only attribute j (e.g., identity)<ref type="foot" target="#foot_1">2</ref> , with z = E(x) and z o = E(x o ), be s.t.</p><formula xml:id="formula_13">ùíô ùêø #! ùêø #! ùêø #! ùêø #! ùêø #! ùêø #! ùêø (#! ùêø (#! ùêø ! ùêø #! ùêø (#!</formula><formula xml:id="formula_14">D (z s ) ‚âà x and D z (o) s ‚âà x (o) ; with z s , z (o) s = swap(z, z o , j).<label>(3)</label></formula><p>If, for each attribute, sufficient sample pairs share only that attribute, and Eq. 3 holds for all with zero residual loss, then disentanglement is achieved for that attribute (on the training set).</p><p>Cycle Attribute Swap. This operates on all example pairs, regardless of whether they share an attribute or not. Given two examples and their corresponding latent vectors, if we swap latent information corresponding to any attribute, we should end up with a sensible decoding. However, we may not have ground-truth supervision samples for swapping all attributes of all pairs. For instance, when swapping the color attribute between pair orange truck and white airplane, we would like to learn from this pair, even without any orange airplanes in the dataset. To train from any pair, we are motivated to follow a recipe similar to <ref type="bibr">CycleGAN (Zhu et al., 2017)</ref>. As shown in Fig. <ref type="figure" target="#fig_6">3c</ref> </p><formula xml:id="formula_15">)| = 1 for x (o) ‚àà S do z ‚Üê E(x); z (o) ‚Üê E(x (o) ); z s , z (o) s ‚Üê swap(z, z (o) , j) L sr ‚Üê L sr + ||D (z s ) ‚àí x|| l1 + D z (o) s ‚àí x (o) l1 # Swap reconstruction loss x ‚àº D and j ‚àº U[1..m] # Sample for Cycle swap z ‚Üê E(x); z ‚Üê E(x); (z s , zs ) ‚Üê swap(z, z, j); x ‚Üê D(z s ); x ‚Üê D(z s ) z ‚Üê E( x); z ‚Üê E( x); ( z s , zs ) ‚Üê swap( z, z, j) L csr ‚Üê ||D ( z s ) ‚àí x|| l1 + D zs ‚àí x l1 # Cycle reconstruction loss L r ‚Üê ||D (E(x)) ‚àí x|| l1 # Standard reconstruction loss</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TRAINING AND OPTIMIZATION</head><p>Algorithm 1 lists our sampling strategy and calculates loss terms, which we combine into a total loss</p><formula xml:id="formula_16">L(E, D; D, M ) = L r + Œª sr L sr + Œª csr L csr ,<label>(5)</label></formula><p>where L r , L sr and L csr , respectively are the reconstruction, swap-reconstruction, and cycle construction losses. Scalar coefficients Œª sr , Œª csr &gt; 0 control the relative importance of the loss terms. The total loss L can be minimized w.r.t. parameters of encoder (E) and decoder (D) via gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">QUALITATIVE EXPERIMENTS</head><p>We qualitatively evaluate our method on zero-shot synthesis tasks, and on its ability to learn disentangled representations, on existing datasets (Sec. 5.2), and on a dataset we contribute (Sec. 5.1).</p><p>GZS-Net architecture. For all experiments, the encoder E is composed of two convolutional layers with stride 2, followed by 3 residual blocks, followed by a convolutional layer with stride 2, followed by reshaping the response map to a vector, and finally two fully-connected layers to output 100-dim vector as latent feature. The decoder D mirrors the encoder, and is composed of two fully-connected layers, followed by reshape into cuboid, followed by de-conv layer with stride 2, followed by 3 residual blocks, then finally two de-conv layers with stride 2, to output a synthesized image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FONTS DATASET &amp; ZERO-SHOT SYNTHESIS PERFORMANCE</head><p>Design Choices. Fonts is a computer-generated image datasets. Each image is of an alphabet letter and is accompanied with its generating attributes: Letters (52 choices, of lower-and upper-case English alphabet); size (3 choices); font colors (10); background colors (10); fonts (100); giving a total of 1.56 million images, each with size (128 √ó 128) pixels. We propose this dataset to allow fast testing and idea iteration on zero-shot synthesis and disentangled representation learning. Samples from the dataset are shown in Fig. <ref type="figure" target="#fig_2">2</ref>. Details and source code are in the Appendix. We partition the 100-d latents equally among the 5 attributes. We use a train:test split of 75:25.</p><p>Baselines. We train four baselines:</p><p>‚Ä¢ The first three are a standard Autoencoder, a Œ≤-VAE <ref type="bibr" target="#b8">(Higgins et al., 2017)</ref>, and Œ≤-TCVAE (Chen et al., 2018). Œ≤-VAE and Œ≤-TCVAE show reasonable disentanglement on the dSprites dataset <ref type="bibr" target="#b16">(Matthey et al., 2017</ref>). Yet, they do not make explicit the assignment between latent variables and attributes, which would have been useful for precisely controlling the attributes (e.g. color, orientation) of synthesized images. Therefore, for these methods, we designed a best-effort approach by exhaustively searching for the assignments. Once assignments are known, swapping attributes between images might become possible with these VAEs, and hopefully enabling for controllable-synthesis. We denote these three baselines with this Exhaustive Search, using suffix +ES. Details on Exhaustive Search are in the Appendix. ‚Ä¢ The fourth baseline, AE+DS, is an auto-encoder where its latent space is partitioned and each partition receives direct supervision from one attribute. Further details are in the Appendix.</p><p>As shown in Fig. <ref type="figure" target="#fig_8">4</ref>, our method outperforms baselines, with second-runner being AE+DS: With discriminative supervision, the model focus on the most discriminative information, e.g., can distinguish e.g. across size, identity, etc, but can hardly synthesize photo-realistic letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ZERO-SHOT SYNTHESIS ON ILAB-20M AND RAFD</head><p>iLab-20M <ref type="bibr" target="#b1">(Borji et al., 2016)</ref>: is an attributed dataset containing images of toy vehicles placed on a turntable using 11 cameras at different viewing points. There are 3 attribute classes: vehicle identity: 15 categories, each having 25-160 instances; pose; and backgrounds: over 14 for each identity: projecting vehicles in relevant contexts. Further details are in the Appendix. We partition the 100-d latent space among attributes as: 60 for identity, 20 for pose, and 20 for background. iLab-20M has limited attribute combinations (identity shows only in relevant background; e.g., cars on roads but not in deserts), GZS-Net can disentangle these three attributes and reconstruct novel combinations (e.g., cars on desert backgrounds) Fig. <ref type="figure" target="#fig_9">5</ref> shows qualitative generation results.</p><p>We compare against (AE+DS), confirming that maintains discriminative information, and against two state-of-the-art GAN baselines: starGAN <ref type="bibr" target="#b4">(Choi et al., 2018)</ref> and ELEGANT <ref type="bibr" target="#b20">(Xiao et al., 2018)</ref>. GAN baselines are strong in knowing what to change but not necessarily how to change it: Where change is required, pixels are locally perturbed (within a patch) but the perturbations often lack global correctness (on the image). See Appendix for further details and experiments on these GAN methods.</p><p>RaFD (Radboud Faces Database, <ref type="bibr" target="#b14">Langner et al., 2010)</ref>: contains pictures of 67 models displaying 8 emotional expressions taken by 5 different camera angles simultaneously. There are 3 attributes: identity, camera position (pose), and expression. We partition the 100-d latent space among the attributes as 60 for identity, 20 for pose, and 20 for expression. We use a 80:20 split for train:test, and use GZS-Net to synthesize images with novel combination of attributes (Fig. <ref type="figure" target="#fig_10">6</ref>). The synthesized images can capture the corresponding attributes well, especially for pose and identity.</p><p>6 QUANTITATIVE EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">QUANTIFYING DISENTANGLEMENT THROUGH ATTRIBUTE CO-PREDICTION</head><p>Can latent features of one attribute predict the attribute value? Can it also predict values for other attributes? Under perfect disentanglement, we should answer always for the first and never for the second. Our network did not receive attribute information through supervision, but rather, through swapping. We quantitatively assess disentanglement by calculating a model-based confusion matrix between attributes: We analyze models trained on the Fonts dataset. We take the Test examples from Font, and split them 80:20 for train DR :test DR . For each attribute pair j, r ‚àà [1..m] √ó [1..m], we train a classifier (3 layer MLP) from g j of train DR to the attribute values of r, then obtain the accuracy of each attribute by testing with g j of train DR . Table <ref type="table" target="#tab_0">1</ref> compares how well features of each attribute (row) can predict an attribute value (column): perfect should be as close as possible to Identity matrix, with off-diagonal entries close to random (i.e., 1 / |A r |). GZS-Net outperforms other methods, except  for (AE + DS) as its latent space was Directly Supervised for this particular task, though it shows inferior synthesis performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">DISTANCE OF SYNTHESIZED IMAGE TO GROUND TRUTH</head><p>The construction of the Fonts dataset allows programmatic calculating ground-truth images corresponding to synthesized images (recall, Fig. <ref type="figure" target="#fig_8">4</ref>). We measure how well do our generated images compare to the ground-truth test images. Table <ref type="table">2</ref> shows image similarity metrics, averaged over the test set, comparing our method against baselines. Our method significantly outperforms baselines.  <ref type="bibr">92 .11 .13 .30 .48 .60 .71 .92 .06 .99 .72 .22 .20 .25 .02 .35 .11 .19 .01 .1 .39 .13 .11 .01 Size (3) .78 1.0 .11 .15 .36 .45 .61 .77 .96 .07 .54 1.0 .19 .23 .25 .02 .38 .29 .11 .01 .02 .47 .18 .19 .01 FontColor (10) .70 .88 1.0 .16 .23 .48 .60 .67 .95 .06 .19 .64 1.0 .66 .20 .02 .33 .42 .11 .01 .02 .35 .21 .13 .01 BackColor (10) .53 .78 .21 1.0 .15 .53 .63 .64 .93 .08 .32 .65 .29 1.0 .25 .02 .34 .11 .86 .01 .03 .40 .24 .75 .01</ref> Style (100) . <ref type="bibr">70 .93 .12 .12 .63 .49 .60 .70 .94 .06 .38 .29 .20 .20 .65 .02 .33 .10 .11 .02 .02 .33 .10 .08 .01</ref> Table <ref type="table">2</ref>: Average metrics between ground-truth test image and image synthesized by models, conducted over the Fonts dataset. We report MSE (smaller is better) and PSNR (larger is better).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We propose a new learning framework, Group Supervised Learning (GSL), which admits datasets of example features and their attributes. It provides a learner groups of semantically-related samples, which we show is powerful for zero-shot synthesis. In particular, our Group-supervised Zero-Shot synthesis network (GZS-Net) is capable of training on groups of examples, and can learn disentangled representations by explicitly swapping latent features across training examples, along edges suggested by GSL. We show that, to synthesize samples given a query with custom attributes, it is sufficient to find one example per requested attribute and to combine them in the latent space. We hope that researchers find our learning framework useful and extend it for their applications.</p><p>You can download the dataset and its generating code from: http://anonymous/url/for/ blind/review , which we plan to keep up-to-date with contributions from ourselves and the community.   We report ELEGANT results showing best qualitative performance.</p><p>Overall, ELEGANT does not work well for holistic image manipulation (though works well for local image edits, per experiments by authors <ref type="bibr" target="#b20">(Xiao et al., 2018)</ref>).</p><p>B.4 STARGAN (CHOI ET AL., 2018)</p><p>We utilize the author's open-sourced code: https://github.com/yunjey/stargan. Unlike ELEGANT <ref type="bibr" target="#b20">(Xiao et al., 2018)</ref> and our method, starGAN only accepts one input image and an edit information: the edit information, is not extracted from another image -this is following their method and published code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ZERO-SHOT SYNTHESIS PERFORMANCE ON DSPRITES DATASET</head><p>We qualitatively evaluate our method, Group-Supervised Zero-Shot Synthesis Network (GZS-Net), against three baseline methods, on zero-shot synthesis tasks on the dSprites dataset.</p><p>C.1 DSPRITES dSprites <ref type="bibr" target="#b16">(Matthey et al., 2017)</ref> is a dataset of 2D shapes procedurally generated from 6 ground truth independent latent factors. These factors are color, shape, scale, rotation, x-and y-positions of a sprite. All possible combinations of these latents are present exactly once, generating We train a 10-dimensional latent space and partition the it equally among the 5 attributes: 2 for shape, 2 for scale, 2 for orientation, 2 for position X, and 2 for position Y . We use a train:test split of 75:25.</p><p>We train 3 baselines: a standard Autoencoder, a Œ≤-VAE <ref type="bibr" target="#b8">(Higgins et al., 2017)</ref>, and TC-VAE <ref type="bibr" target="#b3">(Chen et al., 2018)</ref>. To recover the latent-to-attribute assignment for these baselines, we utilize the Exhaustive Search best-effort strategy, described in the main paper: the only difference is that we change the dimension of Z space from 100 to 10. Once assignments are known, we utilize these baseline VAEs by attribute swapping to do controlled synthesis. We denote these baselines using suffix +ES.</p><p>As is shown in Figure <ref type="figure" target="#fig_2">2</ref>, GZS-Net can precisely synthesize zero-shot images with new combinations of attributes, producing images similar to the groud truth. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Zero-shot synthesis performance of our method. (a), (b), and (c) are from datasets, respectively, iLab-20M, RaFD, and Fonts. Bottom: training images (attributes are known). Top: Test image (attributes are a query). Train images go through an encoder, their latent features get combined, passed into decoder, to synthesize the requested image. Sec. 4.2 shows how we disentangle the latent space, with explicit latent feature swap during training.</figDesc><graphic url="image-2.png" coords="2,325.79,110.29,259.55,97.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b)) are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Samples from our proposed Fonts dataset, shown in groups. In each group, we vary one attribute but keep others the same. (b) (Sub-)multigraph of our Fonts dataset. Each edge connect two examples sharing an attribute. Sets S 1 and S 2 cover sample i.</figDesc><graphic url="image-15.png" coords="4,108.00,81.86,395.98,86.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 1</head><label>1</label><figDesc>AUTO-ENCODING ALONG RELATIONS IN M Auto-encoders (E ‚Ä¢ D) : X ‚Üí X are composed of an encoder network E : X ‚Üí R d and a decoder network D : R d ‚Üí X . Our networks further utilize M emitted by GSL. GZS-Net consists of an encoder E : X √ó M ‚Üí R d √ó M ; and a decoder D : R d √ó M ‚Üí X .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>‚àà</head><label></label><figDesc>R dj } m j=1 where d = m j=1 d j and the values of {d j } m j=1 are hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of GZS-Net, consisting of an encoder E: maps sample onto latent vector, and a decoder D: maps latent vector onto sample. The latent space is pre-partitioned among the attribute classes (3 shown: identity, pose, background). (a, left) considered examples: a center image (x, red border) and 3 images sharing one attribute with it, as well as a no overlap image sharing no attributes (x, black border). (a, right) standard reconstruction loss, applied for all images. (b) One-overlap attribute swap: Two images with identical values for one attribute should be reconstructed into nearly the original images when the latent representations for that attribute are swapped ("no-op" swap; left: identity; middle: pose; right: background). (c) Cycle swap: given any example pair, we randomly pick an attribute class j. We encode both images, swap representations of j, decode, re-encode, swap on j again (to reverse the first swap), and decode to recover the inputs. This unsupervised cycle enforces that double-swap on j does not destroy information for other attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>, given two examples x and x: (i) sample an attribute j ‚àº U[1..m]; (ii) encode both examples, z = E(x) and z = E(x); (iii) swap features corresponding to attribute j with z s , zs = swap(z, z, j); (iv) decode, x = D(z s ) and x = D(z s ); (v) on a second round (hence, cycle), encode again as z = E( x) and z = E( x); (vi) another swap, which should reverse the first swap, z s , zs = swap( z, z, j); (vii) finally, one last decoding which should approximately recover the original input pair, such that: D ( z s ) ‚âà x and D zs ‚âà x;(4)If, after the two encode-swap-decode, we are able to recover the input images, regardless of which attribute we sample, this implies that swapping one attribute does not destroy latent information for other attributes. As shown in Sec. 5, this can be seen as a data augmentation, growing the effective training set size by adding all possible new attribute combinations not already in the training set. Algorithm 1: Training Regime; for sampling data and calculating loss terms Input: Dataset D and Multigraph M Output: L r , L sr , L csr Sample x ‚àà D, S ‚äÇ D such that COVER(S, x) and |S| = m and ‚àÄk ‚àà S, |M (x, k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Zero-shot synthesis performance compare on Fonts. 7-11 and 18-22 columns are input group images and we want to combine the specific attribute of them to synthesize an new images. 1-5 and 12-16 columns are synthesized images use auto-encoder + Exhaustive Swap (AE+ES), Œ≤-VAE + Exhaustive Swap (Œ≤-VAE+ES), Œ≤-TCVAE + Exhaustive Swap (Œ≤-TCVAE+ES), auto-encoder + Directly Supervision (AE+DS) and GZS-Net respectively. 6 and 17 columns are ground truth (GT)</figDesc><graphic url="image-41.png" coords="7,108.00,81.86,395.98,69.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Zero-shot synthesis qualitative performance on ilab-20M. Columns left of the dashed line are output by methods: the first five are baselines, followed by three GZS networks. The baselines are: (1) is an auto-encoder with direct supervision (AE+DS); (2, 3, 4) are three GAN baselines changing only one attribute; (5) is starGAN changing two attributes. Then, first two columns by GZS-Net are ablation experiments: trained with part of the objective function, and the third column is output by a GZS-Net trained with all terms of the objective. starGAN of (Choi et al., 2018) receives one input image and edit information (explanation in Appendix Section B.4). ELEGANT uses identity and background images (in Appendix Section B.3, uses all input images). Others use all three inputs.</figDesc><graphic url="image-42.png" coords="8,108.00,66.92,395.99,232.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: GZS-Net zero-shot synthesis performance on RaFD. 1-2 and 6-7 columns are the synthesized novel images using auto-encoder + Directly Supervision (AE+DS) and GZS-Net respectively. Remaining columns are training set images with their attributes provide.</figDesc><graphic url="image-43.png" coords="8,108.00,419.40,396.01,121.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 7: (a) Dataset details for training object recognition task, where the x-axis represents different identities (1004) and the y-axis represents the backgrounds (111) and poses (6) each purple and brown pixel means our dataset covers the specific combination of attributes. (b) object recognition accuracy (%) on 37469 test examples, after training on (augmented) datasets.</figDesc><graphic url="image-44.png" coords="9,108.00,439.59,395.99,57.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Zero-shot synthesis performance on dSprites. Columns 6-10 are input group images: from each, we want to extract one attribute (title of column). The goal is to combine the attributes to synthesize an new images. Columns 1-4 are synthesized images, respectively using: auto-encoder + Exhaustive Search (AE+ES), Œ≤-VAE + Exhaustive Search (Œ≤-VAE+ES), TC-VAE + Exhaustive Search (TC-VAE+ES) and GZS-Net respectively. The 5th column are ground truth (GT), which none of the methods saw during training or synthesis</figDesc><graphic url="image-46.png" coords="12,108.00,376.63,396.00,255.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Training performance of ELEGANT. The left 2 columns (A and B) are input image. The following 4 columns are the generated synthesized images: A' and B' are reconstructions of the input (acceptable quality, suggesting convergence of generator), whereas C and D are the result of swapping features before the generator: C (/ D) uses the latent features of A (/ B) except by swapping background with B (/ A). All (C, D, A', B') share the same generator.</figDesc><graphic url="image-48.png" coords="15,147.60,81.86,316.80,287.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-45.png" coords="11,135.92,390.44,340.16,219.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-47.png" coords="14,147.60,81.86,316.80,263.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Disentangled representation analysis. Diagonals are bolded.</figDesc><table><row><cell></cell><cell>GZS-Net</cell><cell>Auto-encoder</cell><cell>AE + DS</cell><cell>Œ≤-VAE + ES</cell><cell>Œ≤-TCVAE + ES</cell></row><row><cell>A (|A|)</cell><cell cols="5">C S FC BC St C S FC BC St C S FC BC St C S FC BC St C S FC BC St</cell></row><row><cell cols="2">Content (52) .99 .</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Attributes generating the Fonts dataset</figDesc><table><row><cell>Attribute</cell><cell cols="2">Number of Attribute Values Attribute Value Details</cell></row><row><cell>Letter</cell><cell>52</cell><cell>Uppercase Letters (A-Z) Lowercase Letters (a-z)</cell></row><row><cell>Size</cell><cell>3</cell><cell>Small, Medium, Large (80, 100, 120 pixel height respectively)</cell></row><row><cell>Font color</cell><cell>10</cell><cell>Red, Orange, Yellow, Green, Cyan Blue, Purple, Pink, Chocolate, Silver</cell></row><row><cell>Background color</cell><cell>10</cell><cell>Red, Orange, Yellow, Green, Cyan Blue, Purple, Pink, Chocolate, Silver</cell></row><row><cell>Font</cell><cell>100</cell><cell>Ubuntu system fonts</cell></row></table><note>e.g. aakar, chilanka, sarai, etc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>737280 total images. Latent factor values (Color: white; Shape: square, ellipse, heart; Scale: 6 values linearly spaced in [0.5, 1]; Orientation: 40 values in [0, 2 pi]; Position X: 32 values in [0, 1]; Position Y: 32 values in [0, 1]) C.2 EXPERIMENTS OF BASELINES AND GZS-NET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The baselines Œ≤-VAE and TC-VAE produce realistic images of good visual quality, however, not satisfying the requested query: therefore, they cannot do controllable synthesis even when equipped with our best-effort Exhaustive Search to discover the disentanglement. Standard auto-encoders can not synthesis meaningful images when combining latents from different examples, giving images outside the distribution of training samples (e.g. showing multiple sprites per image).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://anonymous/url/for/blind/review</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">It holds that COVER({x, x o }, x) and COVER({x, x o }, x o )</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A FONTS DATASET</head><p>Fonts is a computer-generated RGB image datasets. Each image, with 128 √ó 128 pixels, contains an alphabet letter rendered using 5 independent generating attributes: letter identity, size, font color, background color and font. Fig. <ref type="figure">1</ref> shows some samples: in each row, we keep all attributes values the same but vary one attribute value. Attribute details are shown in Table <ref type="table">1</ref>. The dataset contains all possible combinations of these attributes, totaling to 1560000 images. Generating attributes for all images are contained within the dataset. Our primary motive for creating the Fonts dataset, is that it allows fast testing and idea iteration, on disentangled representation learning and zero-shot synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B BASELINES B.1 EXHAUSTIVE SEARCH (ES) AFTER TRAINING AUTO-ENCODER BASED METHODS</head><p>After training the baselines: standard Autoencoder, a Œ≤-VAE <ref type="bibr" target="#b8">(Higgins et al., 2017), and</ref><ref type="bibr">TC-VAE (Chen et al., 2018)</ref>. We want to search for the assignment between latent variables and attributes, as these VAEs do not make explicit the assignment. This knowing the assignment should hypothetically allow us to trade attributes between two images by swapping feature values belonging to the attribute we desire to swap.</p><p>To discover the assignment from latent dimension to attribute, we map all n training images through the encoder, giving a 100D vector per training sample ‚àà R n√ó100 . We make an 80:20 split on the vectors, obtaining X trainES ‚àà R 0.8n√ó100 and X testES ‚àà R 0.2n√ó100 . Then, we randomly sample K different partitionings P of the 100D space evenly among the 5 attributes. For each partitioning p ‚àà P , we create 5 classification tasks, one task per attribute, according to p:</p><p>X trainES [:, p j ] ‚àà R 0.8n√ó20 , X testES [:, p j ] ‚àà R 0.2n√ó20 5 j=1 . For each task j, we train a 3-layer MLP to map X trainES [:, p j ] to their known attribute values and measure its performance on X testES [:, p j ]. Finally, we commit to the partitioning p ‚àà P with highest average performance on the 5 attribute tasks. This p represents our best effort to determine which latent feature dimensions correspond to which attributes. For zero-shot synthesis with baselines, we swap latent dimensions indicated by partitioning p. We denote three baselines with this Exhaustive Search, using suffix +ES (Fig. <ref type="figure">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 DIRECT SUPERVISION (DS) ON AUTO-ENCODER LATENT SPACE</head><p>The last baseline (AE+DS) directly uses attribute labels to supervise the latent disentangled representation of the auto-encoder by adding auxiliary classification modules. Specifically, the encoder maps an image sample x (i) to a 100-d latent vector z (i) = E(x (i) ), equally divided into 5 partitions corresponding to 5 attributes:</p><p>5 ], which represent the attribute value (e.g. for font color attribute, the label represent different colors: red, green, blue,.etc). We use 5 auxiliary classification modules to predict the corresponding class label given each latent attribute partitions as input. We use Cross Entropy loss as the classification loss and the training goal is to minimize both the reconstruction loss and classification loss.</p><p>After training, we have assignment between latent variables and attributes, so we can achieve attribute swapping and controlled synthesis (Fig. <ref type="figure">4</ref> (AE+DS)). The inferior synthesis performance demonstrates that: The supervision (classification task) preserves discriminative information that is insufficient for photo-realistic generation. While our GZS-Net uses one attribute swap and cross swap which enforce disentangled information to be sufficient for photo-realistic synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 ELEGANT (XIAO ET AL., 2018)</head><p>We utilize the author's open-sourced code: https://github.com/Prinsphield/ELEGANT. For ELEGANT and starGAN (Section B.4), we want to synthesis a target image has same identity as id provider image, same background as background provider image, and same pose as pose provider image. To achieve this, we want to change the background and pose attribute of id image.</p><p>Although ELEGANT is strong in making image transformations that are local to relatively-small neighborhoods, however, it does not work well for our datasets, where image-wide transformations are required for meaningful synthesis. This can be confirmed by their model design: their final output is a pixel-wise addition of a residual map, plus the input image. Further, ELEGANT treats all attribute values as binary: they represent each attribute value in a different part of the latent space, whereas our method devotes part of the latent space to represents all values for an attribute. For investigation, we train dozens of ELEGANT models with different hyperparameters, detailed as:</p><p>‚Ä¢ For iLab-20M, the pose and background contain a total of 117 attribute values (6 for pose, 111 for background). As such, we tried training it on all attribute values (dividing their latent space among 117 attribute values). We note that this training regime was too slow and the loss values do not seem to change much during training, even with various learning rate choices (listed below).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic and-or attribute grouping for zero-shot learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ilab-20m: A large-scale controlled object dataset to investigate deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Understanding disentangling in beta-vae</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Christopher P Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Œ≤-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical text-to-image synthesis</title>
		<author>
			<persName><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7986" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05983</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Disentangling by factorising. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multi-view rgb-d object dataset</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE international conference on robotics and automation</title>
		<imprint>
			<biblScope unit="page" from="1817" to="1824" />
			<date type="published" when="2011">2011. 2011</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Presentation and validation of the radboud faces database</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Dotsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gijsbert</forename><surname>Bijlstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><forename type="middle">T</forename><surname>Daniel Hj Wigboldus</surname></persName>
		</author>
		<author>
			<persName><surname>Hawk</surname></persName>
		</author>
		<author>
			<persName><surname>Van Knippenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and emotion</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1377" to="1388" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape representation in the inferior temporal cortex of monkeys</title>
		<author>
			<persName><forename type="first">Nikos</forename><forename type="middle">K</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggiot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current Biology</title>
				<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<title level="m">dsprites: Disentanglement testing sprites dataset</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Elegant: Exchanging latent encodings with gan for transferring multiple face attributes</title>
		<author>
			<persName><forename type="first">Taihong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
			<biblScope unit="page" from="172" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transmomo: Invariance-driven unsupervised video motion retargeting</title>
		<author>
			<persName><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5306" to="5315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
	<note>International Conference on Computer Vision (ICCV&apos;17)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
