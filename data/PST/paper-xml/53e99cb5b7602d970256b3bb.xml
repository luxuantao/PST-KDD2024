<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mamba: A Scalable Communication Centric Multi-Threaded Processor Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gregory</forename><forename type="middle">A</forename><surname>Chadwick</surname></persName>
							<email>gregory.chadwick@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Computer Laboratory University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
							<email>simon.moore@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Computer Laboratory University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mamba: A Scalable Communication Centric Multi-Threaded Processor Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe Mamba, an architecture designed for multi-core systems. Mamba has two major aims: (i) make on-chip communication explicit to the programmer so they can optimize for it and (ii) support many threads and supply very lightweight communication and synchronization primitives for them. These aims are based on the observations that: (i) as feature sizes shrink, on-chip communication becomes relatively more expensive than computation and (ii) as we go increasingly multicore we need highly scalable approaches to inter-thread communication and synchronization. We employ a network of processors where a given memory access will always go to the same cache, removing the need for a coherence protocol and allowing the program explicit control over all communication. A presence bit associated with each word provides a very lightweight, finegrained synchronization primitive. We demonstrate an FPGA implementation with micro-benchmarks of standard spinlock and FIFO implementations and show that presence bit based implementations provide more efficient locking, and lower latency FIFO communications compared to a conventional shared memory implementation whilst also requiring fewer memory accesses. We also show that Mamba performance is insensitive to total thread count, allowing the use of as many threads as desired.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In the last few years we have seen the rise of the Chip Multiprocessor (CMP). With clock speeds staying static but feature sizes still shrinking exploiting thread level parallelism (TLP) along with instruction level parallelism (ILP) is the natural way to gain further performance. Given a constant die size with shrinking feature size, the chip area reachable in a single cycle also shrinks. This complicates the design of a single large core that uses the increasing availablity of gates to extract more ILP. Using that area to instead have a multitude of smaller, simpler cores, increasing core number, rather than core complexity, is preferable and the costlier global cross-chip communcations can be made architecturally explicit, allowing software to optimize for it <ref type="bibr" target="#b0">[1]</ref>.</p><p>Most CMPs utilize shared memory along with a cache coherency protocol to allow communication between separate threads and cores. The cache coherency protocol is responsible for deciding when to communicate between cores and the programmer can only indirectly influence this communication by choosing what data is shared between threads. Much effort has been made to optimize coherency protocols to reduce needless communication ( <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>). Furthermore, in order to achieve good performance sequential consistency is This work was funded under EPSRC grant EP/F018649/1 not maintained, and a more relaxed consistency model used instead <ref type="bibr" target="#b3">[4]</ref>. This breaks that idea that a shared memory system with cache coherency presents a familiar memory model to the programmer.</p><p>With increasing core counts <ref type="bibr" target="#b4">[5]</ref> software needs to be capable of achieving performance that scales with the number of cores. Amdahl's law shows that this is limited by the time spent synchronizing and in critical sections <ref type="bibr" target="#b5">[6]</ref>. So for a program to scale well with core count it needs to have as much TLP as possible, and for each thread to spend as little time as possible synchronizing and in critical sections. This suggests a programming model that provides very light-weight threads and synchronization primitives to go with them will enable software that scales well with an increasing number of cores.</p><p>This paper presents Mamba, an architecture that i) Gives explicit control over communication, so software can be optimized to reduce it ii)</p><p>Provides a light-weight threading and synchronization model. We introduce Mamba in section 2, discuss some software techniques for it in section 3, review related work in section 4, present a evaluation of an MCS Lock and FIFO queue implementation in section 5, conclude in section 6 and consider future work in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MAMBA ARCHITECTURE</head><p>A Mamba system consists of a network of nodes. Each node contains a simple in-order RISC core, a cache and a networkon-chip (NoC) router. Each node is allocated an area of global physical address space which is termed that node's local address space; any other addresses are in a remote address space. When a node's core accesses local address space it goes to the local cache. Remote address space is accessed from remote cache via the network. Provided the network retains ordering of messages between nodes we can ensure sequential consistency within a particular local address space.</p><p>It should be emphasised that the programmer's view of memory is fully coherrent. Whilst there is no explicit cache coherency system, enforcing that a given area of memory must always be cached in the same cache ensures there is no duplication which is effectively acting as a very simple cache coherency protcol.</p><p>Every 64-bit memory word in a Mamba system has an associated presence-bit (also known as a full/empty bit). This is used as a basic primitive for thread to thread communication and synchronization. A thread attempting to read a non-present location will be stalled and descheduled until that location becomes present. Each register also has an associated presence bit, when a thread attempts to use a non-present register it stalls until the register becomes present. Presence bits are stored in main memory at the top of each node's local address space, this leaves a gap in the address space that cannot be used for data storage, however this could be hidden by a virtual memory system.</p><p>A node supports hardware threading and scheduling. The register file (RF) in a node's core can hold the registers of eight separate threads. Every cycle it will issue an instruction from a different thread, scheduling the threads within the register file in a round robin manner. A thread is represented in memory by an activation frame (AF), this is simply the contents of the thread's registers, its program counter (PC) and a status word. It fits in a 256-byte (32 x 64-bit words) block.</p><p>There is a special store instruction (store doubleword to AF, SDA) that causes a node to check the presence bits of the AF being stored to and if they are all set it places the AF on a queue of ready nodes. The node has a simple roundrobin scheduler that switches contexts from the RF back into memory and switches an AF from the ready queue from memory into the RF. A context switch occurs when an active thread's quantum expires.</p><p>Each node has three separate caches. An instruction cache, a data cache and a presence bit cache. The instruction cache does not obey the restriction that a particular address can only live in a particular cache, so code may be replicated across instruction caches. A seperate presence bit cache is used as it allows checking of an entire AFs worth of presence bits without looking at multiple cache lines (which would be the case if presence bits were stored along with words in the data cache).</p><p>When a core executes a load or a store instruction it generates a memory request, this will be sent directly to the local cache or out to the network depending upon the address. Upon executing a load a core will clear the presence bit of the register which is the load's destination, so if a thread attempts to use that register before the load has completed it will stall.</p><p>There are two major request types, load and store: Load (Fig. <ref type="figure">1</ref>) has an address (A) to load from and a return address (R). The return address is where the response from the load should be sent. As each thread is represented by an AF, each register has a memory address so the return address is the address of the register that was the load's destination. When a load request is received the presence bit of the corresponding word is checked, if it is:</p><p>• Present -An immediate data response is sent to the return address with the contents of the word. • Non Present -The contents of the word are checked, if it is a sentinel value, the return address of the load is written into the word and nothing else is done. If the sentinel value is not there (The sentinel value is a particular invalid return address), then some other load request has already</p><note type="other">Before Load After Load Response To Load 1</note><formula xml:id="formula_0">D 1 D R D 0 S 0 R None 0 F 0 F R Exception Fig. 1.</formula><p>The possible actions on a load request at a particular word. D is the data stored in the word, S is the sentinel value, R is the return address of the load and F is an already existing return address. 0 refers to a non-present word, 1 to a present word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Before Store</head><p>After Store Response To Store</p><formula xml:id="formula_1">1 D 1 D' None 0 S 1 D' None 0 R 1 D' R D' Fig. 2.</formula><p>The possible actions on a store request at a particular word. D is the existing data stored at the word, D' is the new data the store request is writing, S is the sentinel value and R is a return address. 0 refers to a non-present word, 1 to a present word.</p><p>written its return address into this word and an exception response is immediately sent.</p><p>Store (Fig. <ref type="figure">2</ref>) has an address (A) to store to and the data to store (D ). When a store request is received the presence bit of the corresponding word is checked, if it is:</p><p>• Present -The contents of the word are overwritten with the new data, nothing else is done. • Non Present -The presence bit is set and the current word contents are checked. If the sentinel value is there, nothing further is done. Otherwise a load return address is there and a load response with the store data is generated to that address.</p><p>Upon receiving a data response, the AF portion of the address is checked. If the AF is in the RF then the corresponding register is updated with the data from the response and the register's presence bit is set. If the AF has been swapped out to memory then the corresponding word within the AF is updated and its presence bit set. The presence bits of all words within the AF are checked and if they are all set the AF is placed at the back of the ready queue.</p><p>The mechanism above allows many communication styles. For simple producer → single consumer communication the presence bit of a particular word can be cleared, at some point a thread (the consumer) loads from that word. When the thread tries to use the result of that load it will stall (as the word was non-present so no response is received and thus the register holding the result is still marked as non-present). At a later point another thread (the producer) will write to the non present word causing a data response to be sent to the consumer's AF. Either the consumer will still be in the register file so upon receiving the response can begin execution again immediately or the scheduler will have swapped it out in which case the data response will cause all words within the AF to</p><formula xml:id="formula_2">1. C 1 A: 0 S 0 C 1 (R) Load to C 1 (R) from A 2. C 2 A: 0 C 1 (R) C 2 Load to C 2 (R)from A Exception 3. P A: 0 C 1 (R) 1 D C 1 Store D to A D Fig. 3. Producer → Consumer. Cn(R)</formula><p>represents the address of register R in the AF Cn. In a single consumer situation only steps 1. and 3. will occur.</p><p>In a multiple consumer situation all 3 steps will occur.</p><p>be present so the consumer will be placed at the back of the ready queue.</p><p>If we add a second consumer which attempts to read a non-present word an immediate exception response is sent. So when the producer eventually writes to the non-present word only the first consumer will receive the data (see Fig. <ref type="figure">3</ref>). There are two possible solutions. If the number of consumers is fixed and known ahead of time the single producer → multiple consumer situation can be turned into multiple single producer → single consumer situations. If this is not possible, or undesirable, a software solution described in the software techniques section can be employed.</p><p>To enable this software mechanism a second load instruction is added. When the consumer receives an exception response from a load there are two possible things that can happen:</p><p>• An exception is triggered in the consumer's thread and it jumps to a handler • A bit is set in an exception status register corresponding to the destination register the load was intended for and execution continues</p><p>The two separate load instructions allow the programmer to choose how the exception response is dealt with. The first (LD the standard load instruction) will cause the first action to occur on an exception response and the other (LDNR) will cause the second action to occur. So LD is used when we either expect the word will always be present or that if non-present that only a single thread will attempt to read it. LDNR will be used when we expect multiple consumers of a non-present word. After executing the load the software must check the exception status before using the result of the load. If an exception was received then the software must take appropriate action (precisely what this is depends upon the software, a possible scenario is described in the software techniques section).</p><p>If the consumer's AF is swapped out at the time of receiving the read exception the status word in the AF is updated to indicate the exception and the appropriate action is taken when the AF gets swapped back.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FPGA Implementation</head><p>To experiment with the Mamba architecture we have produced a fully functional FPGA implementation. It executes a modified subset of the MIPS64 ISA. MIPS64 was chosen due to its ease of implementation and existing compiler tool chain. For a comparison system we have implemented a plain MIPS64 core which is implemented very similarly to Mamba but lacks presence bits and hardware thread scheduling. It has 8 fixed hardware threads that like Mamba are implemented with 8 separate register files and an instruction for a different thread being issued every cycle, but unlike Mamba there is no hardware thread switching or hardware scheduling system. To allow implementation of concurrency primitives a CAS (compare-and-swap) operation was added. A software thread scheduler was also written, this functions exactly as the Mamba thread scheduler does (pre-emptive, round-robin, fixed quantum) but is implemented entirely in software.</p><p>We use Altera Stratix IV FPGAs on a DE4 board. Multiple DE4 boards can be connected with high speed serial links to increase the number of cores available. The current setup uses a single board which can hold four Mamba cores on the FPGA. We are working on building a larger system with 64 DE4 boards connected in a grid topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SOFTWARE TECHNIQUES</head><p>Another use of the presence bits is as a binary sempahore to guard access to the associated word. This is implemented using a third load instruction (see table <ref type="table" target="#tab_0">I</ref>), the vacating load, with mnemonic LL (chosen as it's an existing instruction within the MIPS64 instruction set, however the semantics are not the same). The vacating load checks the presence bit of the word it is loading. If it is set it clears the bit and sends back a data response. If the bit is already clear it sends back an exception response (Setting a bit in the read exception status register).</p><p>This can be used to atomically update a word. Say the word contains a counter, a thread can use a LL to get the counter, increment it, and then store it back. Between the load and the store the presence bit is unset so any other thread which attempts to load it will either stall waiting for the data to be written or will receive an exception response back. Any thread seeking to also atomically update the counter will spin doing repeated LLs until it successfully gets the word.</p><p>Ideally if a thread is unable to continue because a word it requires is not present it should stall and be descheduled rather than spin. This is dealt with by the architecture as described in the mamba architecture section for the producer → single consumer situation but otherwise this can be accomplished via a lightweight construct we call a 'notify chain'. These are similar in structure to MCS queue based spin-locks <ref type="bibr" target="#b6">[7]</ref> with one crucial difference. Rather than spinning on a particular value waiting for it to become 1, the presence bit of that value is cleared, so when the thread loads it and attempts to use it, In more detail, the notify chain is a singly-linked list of two word nodes. The first word (the wait word) initially has its presence bit cleared, the second word is a pointer to the next node in the chain. When a thread wishes to wait on a notify chain it first constructs a node within its local memory space and then atomically inserts itself into the tail of the chain. Finally it loads the wait word (which has a cleared presence bit) and immediately uses the result of the load, which has the effect of causing the thread to stall until a store sets the presence bit of the wait word. When a thread wishes to notify the notify chain it simply writes to the wait word of the head of the chain. When a thread is woken up in this manner it could either immediately wake the next thread in the chain or wake the next thread later (depending on if a notify next, or notify all behaviour is desired). As each thread creates its notify node within its local space the store request which triggers the thread wakeup will go directly from the thread that triggers the wakeup to the thread that receives the wakeup, which is the minimum communication required for such an action.</p><p>Atomic insertion into the chain tail is accomplished using LL. To insert a notify node into the tail a thread simply LLs the tail pointer. It updates the tail's node's (if there is one) next to point to its own node then writes a pointer to its own notify node as the new tail pointer. When a thread wants to notify the chain it must also LL the tail pointer. This is for two reasons:</p><p>1) It avoids a lost wakeup problem where a thread notifies the thread owning the tail node before the tail node's next is updated to the new tail during an insert 2) It needs to check if it is the tail node and update the tail pointer to NULL if it is This general notify chain mechanism can be used for a variety of tasks such as:</p><p>1) As part of a queue based lock, where a thread will stall if it's unable to take the lock and woken up by the thread in front of it when its done with the lock. 2) As part of a barrier, threads can wait on a notify chain until something notifies them all that they can proceed (a counter could keep a count of threads in the chain, when a thread adds itself to the chain it will check the counter, if its hit a certain number it will notify all, otherwise it will increment the counter and wait). 3) As part of a producer → multiple consumer situation.</p><p>A consumer could try to load a word with LDNR if it receives an exception response it will wait on a notify chain. The producer will notify this chain when it stores to the word, the new word value can be used as the value written to the wait word so the waiting threads receive it immediately on wakeup. Whilst the consistency model employed by Mamba requires a network that preserves the ordering of messages between two nodes, is it not essential that this is always the case. The current software targetted at the architecture relies on this fact but it could be rewritten to take the reordering of messages into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION</head><p>To test the FPGA implementation of the system we have written a series of micro-benchmarks, two are presented here. Each micro-benchmark has been implemented on the Mamba and MIPS64 systems so results can be compared.</p><p>As the Mamba and MIPS64 systems both run on the same FPGA, using the same caches, memory controllers and interconnect architecture, details such as memory latency and the raw throughput and latency of the network are the same in both cases so are not considered here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MCS Lock Benchmark</head><p>The MCS Lock <ref type="bibr" target="#b6">[7]</ref> is a standard way of implementing a scalable spin-lock. Any thread wishing to enter the lock first constructs a node, it then attempts to atomically add that node to a queue. If the queue is empty the thread gains the lock, enters the critical section and places its node as the only element in the queue. If the queue is not empty it places its node at the tail of the queue and spins on its node waiting for a lock field to turn from false to true. When a thread releases the lock it checks its node to see what is next in the queue, if there is a node there it sets its lock field to true giving the next thread the lock and allowing it into the critical section. As the node is constructed in local storage the spin does not generate any needless remote memory accesses.</p><p>We have implemented the MCS lock on MIPS64 using the CAS primitive as described in <ref type="bibr" target="#b6">[7]</ref>. In Mamba a notify chain as described above is used. When a thread attempts to acquire a lock it simply waits on the notify chain, when it does this the thread will be descheduled until woken. During this period it won't perform any operation, and in particular any memory operations, unlike the MIPS64 version which must spin continuously reading the lock field of its node. All four cores available in the FPGA implementation are used, each contains a single shared area protected by an MCS lock. Every thread randomly chooses a shared area to access and attempts to acquire the MCS lock. In the shared area is an array of integers, once a thread has acquired this MCS lock it sums these integers and writes the result into the shared area, it then replaces the integers with new randomly generated values. The point of this is so the thread performs some operation on the data protected by the lock as well as doing its own local computations whilst in the critical section to simulate the kind of work that would be done in a real application. Once a thread has left the lock it loops round and repeats the process again with another randomly selected shared area. This repeats for a number of rounds.</p><p>Once all threads have done all their rounds the number of integers summed and generated within the shared area is increased and the benchmark is run again. The number of cycles taken for the benchmark to run for different critical sections size (as measured by the number of integers summed and generated in the critical section) is measured for the MIPS64 implementations and Mamba implementations with 8, 16, 80 and 800 threads per core. With each increase in thread count the number of rounds a thread does is correspondingly scaled down so the total amount of work done is the same (100 rounds per thread with 8, 50 rounds per thread with 16, etc). The cycle count of each run is normalized to that of the 8 thread MIPS64 run with the smallest amount of work (10 numbers summed and generated).</p><p>The run-time of the MCS lock benchmark with increasing work in the critical section can be seen in Fig. <ref type="figure">4</ref>. They show that with 8 threads MIPS64 is only slightly worse than Mamba (betwen 5% -16% worse), however with Mamba thread count does not have any particular impact on the benchmark performance. Whilst at higher thread counts the variation of results from the trend is greater, the benchmark still scales equally well amongst all thread counts. For MIPS64 it gets steadily worse as thread count is increased. The results for 800 threads would not fit on the graph, with the smallest critical section size run-time was 48× worse compared to the 8 thread version and 16× worse with a critical section size of 100. Also of note are the average memory operations per core. A memory operation is defined as any read or write that occurs in a node's data cache. A Mamba thread can sleep and be woken by its node in the MCS lock queue being written to so it does not generate any memory operations spinning whilst waiting to be notified. MIPS64 on the other hand has all threads continuously generating memory operations even if they're not doing any useful work. As a result MIPS64 produces between 14× and 16× as many memory operations as Mamba during the benchmark run in the 8 thread run. Interestingly MIPS64 produces less memory operations in the 16 thread run compared to its own 8 thread run, but this is still between 7× and 10× more memory operations compared to the Mamba 16 thread run. It produces more memory operations in its 80 thread run compared to its 16 thread run and between 10× and 22× more than the Mamba 80 thread run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FIFO Queue Benchmark</head><p>A simple FIFO queue implementation was written based upon <ref type="bibr" target="#b7">[8]</ref>. The queue is based around a ring buffer, with read and write pointers pointing to the item at the top of the queue, and the next free slot in the buffer respectively. Provided we guarantee that at most one thread enqueues and one thread dequeues at any given time nothing beyond normal load and store operations are needed (i.e. we do not need CAS). For MIPS64 this has been implemented exactly as <ref type="bibr" target="#b7">[8]</ref>, for Mamba a simple modification was made. The FIFO ring buffer starts with all its presence bits unset. When dequeuing, a thread checks the size of the queue by looking at the read and write pointers; if it is empty it spins waiting until an item appears.</p><p>In Mamba we simply directly read the slot the read pointer points to. If the queue is empty then this points to the slot the next item will be written to and so will be non-present and the thread will sleep until an item is added. When we have dequeued an item we clear the presence bit on the ring buffer slot it came from and then increment the read pointer (ensuring the enqueueing thread will not write to the slot until we have cleared the presence bit).</p><p>We have measured the throughput and the latency of the Mamba and MIPS64 implementations of this queue. One core executes all the consumer threads, these each had their own FIFO queue in their local address space. Paired with a consumer thread was a producer thread, which was located on another core. The producer thread simply sent a sequence of increasing integers.</p><p>The number of cores used to produce was varied between 1-3 with 1 or 2 threads on each core. The 3 producer core arrangement was also measured with 4, 8, 16 and 32 threads per core to test scaling ability. The first core executes all the consumer threads so ran between 1 and 96 threads depending on the number of producer threads and cores. The measurements for each thread were averaged and normalized to the result for the MIPS64 implementation with 1 thread and 1 core.</p><p>Latency results can be seen in Fig. <ref type="figure" target="#fig_0">5</ref>, they show that the Mamba implementation gives between 75% and 641% better latency, achieving the best latency improvement in the 32 threads per producer core with 3 producer cores arrangement. They also show that the Mamba implementation scales far better than MIPS64 with increasing thread count. The bars for 16 and 32 producer threads per core would not fit on the graph for MIPS64 and are 17.7 and 63.1 respectively compared to 4.3 and 9.8 for Mamba. Below 4 threads per core the performance gap remains roughly similar with Mamba between 75% and 85% better.</p><p>As the implementation of MIPS64 and Mamba both utilise the exact same interconnect architecture the latency introduced by the network isn't taken into account as it will be the same in both cases, so it is not essential that is is separated from the latency introduced by the differing FIFO implementations Throughput results can be seen in Fig. <ref type="figure">6</ref>, they show that the Mamba implementation gives between 13% and 246% better throughput. Again the Mamba implementation scales far better than MIPS64 with increasing thread count. Below 8 threads per core the performance gap remains roughly similar with Mamba between 13% and 27% better.</p><p>The Mamba implementation shows good scaling with throughput, if the measured throughput is multiplied by the number of threads producing per core we find it reaches a peak of 3.19 at 8 threads producing per core with 3 producer cores and only reduces to 2.61 for 32 threads producing per core with 3 producer cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>Godson-T <ref type="bibr" target="#b8">[9]</ref> is also a MIPS based multicore architecture that utilizes presence bits. However unlike Mamba they are Fig. <ref type="figure">6</ref>. Normalized throughput for the FIFO Queue Benchmark, for differing thread and core numbers, where t-c on the axis labels refers to t threads per producer core and c cores producing only present in cache-lines and are only used when those cache-lines are configured as a scratch-pad memory. They may be used to allow fine-grained synchronization between currently running threads but they don't interact directly with thread scheduling so cannot be used to implement a notification system that wakes a thread when data is ready. Separate hardware mechanisms are used to implemented finegrained locking and barriers. The Cray XMT <ref type="bibr" target="#b9">[10]</ref> building on earlier work done by the J-Machine <ref type="bibr" target="#b10">[11]</ref> and HEP <ref type="bibr" target="#b11">[12]</ref>, utilizes hardware multithreading and has a presence bit per word in memory, though when a presence bit is not of the desired state a thread simply spins until it is, eventually trapping to a software handler if this takes too long. Mamba builds on earlier concepts from Cambridge <ref type="bibr" target="#b12">[13]</ref>.</p><p>The Intel x86 ISA <ref type="bibr" target="#b13">[14]</ref> offers an MWAIT instruction, which combined with a MONITOR instruction allows a program to wait for a write to a particular address range. MWAIT can be used to optimise anything relying on repeated polling of a memory location, much like presence bits in Mamba can be used. However the MWAIT mechanism is not equivalent to the presence bit mechanism. It only provides a hint to the processor that it can enter an implementation defined optimized state. After executing an MWAIT memory must be checked again and it has no direct interaction with a scheduler. In contrast with Mamba, once a load of an initially empty word completes you can be sure that a value has been written to the word. When a word needed by a thread becomes available the thread immediately gets added to the scheduler's ready queue. As an MWAIT would only be used when a processor is idle to construct a similar mechanism without presence bits would require a thread to notify the scheduler about certain memory operations, this would carry far higher overhead effecting performance and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have demonstrated and prototyped Mamba on FPGA, a processor architecture that makes communication explicit, provides scalable hardware threading and a fine-grained synchronisation and notification mechanism based on presence bits. With minor modifications to an MCS lock implementation, replacing spinning on a local word to waiting for a local word to become present we achieved better performance as well as dramatically reducing the number of memory accesses required. With minor modifications to a FIFO queue implementation latency and throughput was greatly improved.</p><p>Mamba's threading model provides performance that is insensitive to thread count. For the MCS lock benchmark thread count was of little consequence to run-time, for FIFO communication total throughput did not decline much with increasing thread count. This frees a programmer from having to carefully consider the number of threads to use for a particular task, or for the need to change this for different numbers of cores.</p><p>We conclude that Mamba is a promising architecture for the future of increasingly multi-core systems. Existing concurrency primitives can be simply adapted to utilize presence bits achieving performance gains and threads can be employed as the programmer desires.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. FUTURE WORK</head><p>One interpretation of the results would be that the hardware scheduler of Mamba brings about the majority of the benefits seen and the presence bit mechanism is not required. However it is the presence bit mechanism that allows a particular thread to wait for a word to become present without needless polling and it allows the scheduler to know what threads are ready. As stated above a purely software version of a similar mechanism would add overhead preventing it from being used at such a finely grained level. Though a hardware managed ready queue with a software scheduler or hybrid software/hardware scheduler is a design point worth exploring in future work.</p><p>The presented architecture lacks a virtual memory system. A key design point of a virtual memory system for Mamba is whether a word's physical or virtual address specifies which node it belongs to. If a virtual address is used there are two issues. The first is aliasing, two separate virtual address may map to the same physical address breaking coherency and the virtual address mapping may distort or totally obscure the relation between addresses and word location, hiding the communication costs the architecture attempts to make explicit. Using a physical address would not present these issues but using a virtual address adds functionality, for example it could be used to create a data and thread migration system where the node holding a particular piece of data or a thread could be altered without disturbing the memory address needed to access it. The aliasing issue is an existing problem already dealt with by operating systems and NUMA systems need to allocate pages with due regard to the locality of memory so solving these issues in the specific case of Mamba should be not problematic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Normalized latency for the FIFO Queue Benchmark, for differing thread and core numbers, where t-c on the axis labels refers to t threads per producer core and c cores producing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>THREE KINDS OF LOAD, THEIR MNEMONICS AND WHAT THEY DO TO THE WORD THEY'RE LOADING</figDesc><table><row><cell cols="2">Mnemonic Action if present</cell><cell>Action if not present</cell></row><row><cell>LD</cell><cell>Sends data response to return address</cell><cell>Write return address into word if sentinel present, otherwise</cell></row><row><cell></cell><cell></cell><cell>sends exception response, triggering exception handler in</cell></row><row><cell></cell><cell></cell><cell>thread</cell></row><row><cell>LDNR</cell><cell>Sends data response to return address</cell><cell>Write return address into word if sentinel present, otherwise</cell></row><row><cell></cell><cell></cell><cell>sends exception response, setting read exception flag for</cell></row><row><cell></cell><cell></cell><cell>destination register</cell></row><row><cell>LL</cell><cell>Clear presence and sends data response to return address</cell><cell>Sends exception response, setting read exception flag for</cell></row><row><cell></cell><cell></cell><cell>destination register</cell></row><row><cell cols="2">it stalls and is descheduled. It is then woken by another thread</cell><cell></cell></row><row><cell cols="2">writing into that value.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig. 4. Run-time for the MCS Lock Benchmark with increasing work size in the critical section, run-time normalized to MIPS64 run-time with smallest critical section size. Benchmark run over 4 cores in all cases</figDesc><table><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Mamba 8 Threads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Mamba 16 Threads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Mamba 80 Threads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell cols="2">Mamba 800 Threads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MIPS64 8 Threads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MIPS64 16 Threads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Normalized run-time</cell><cell>20 30 40</cell><cell cols="2">MIPS64 80 Threads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>400</cell><cell>450</cell><cell>500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Size of critical section</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="978" xml:id="foot_0">-1-4673-3052-7/12/$31.00 ©2012 IEEE Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 07:09:21 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 07:09:21 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to acknowledge Arnab Banerjee for implementing the interconnect architecture used by the Mamba system and would like to thank Paul Fox, Timothy Jones and Theo Markettos for their valuable feedback. We also thank the anonymous reviewers for their time and their suggested improvements.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 07:09:21 UTC from IEEE Xplore. Restrictions apply.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On-chip wires: Scaling and efficiency</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating the performance of four snooping cache coherency protocols</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th annual international symposium on computer architecture</title>
				<meeting>the 16th annual international symposium on computer architecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="2" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reducing memory and traffic requirements for scalable directory-based cache coherence schemes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1990 international conference on parallel processing</title>
				<meeting>the 1990 international conference on parallel processing<address><addrLine>Urbana-Champaign, IL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>architecture</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shared memory consistency models: a tutorial</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="66" to="76" />
			<date type="published" when="1996-12">Dec 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On-chip interconnection architecture of the tile processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ramey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10">Sept.-Oct. 2007</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="15" to="31" />
			<pubPlace>Micro, IEEE</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling critical sections in Amdahl&apos;s law and its implications for multicore design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual international symposium on computer architecture</title>
				<meeting>the 37th annual international symposium on computer architecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Algorithms for scalable synchronization on shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="65" />
			<date type="published" when="1991-02">Feb. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Specifying concurrent program modules</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lamport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Program. Lang. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="222" />
			<date type="published" when="1983-04">Apr. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Godson-T: An efficient many-core processor exploring thread-level parallelism</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-04">March-April 2012</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="38" to="47" />
			<pubPlace>Micro, IEEE</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Early experiences with large-scale Cray XMT systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mizell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maschhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE international symposium on parallel &amp; distributed processing, ser. IPDPS &apos;09</title>
				<meeting>the 2009 IEEE international symposium on parallel &amp; distributed processing, ser. IPDPS &apos;09<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The J-machine multicomputer: an architectural evaluation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Noakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th annual international symposium on computer architecture, ser. ISCA &apos;93</title>
				<meeting>the 20th annual international symposium on computer architecture, ser. ISCA &apos;93<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="224" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Architecture and applications of the HEP multiprocessor computer system</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real time signal processing IV</title>
				<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page">241248</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multithreaded Processor Design, ser. Kluwer international series in engineering and computer science</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Intel Corporation, Intel R 64 and IA-32 Architectures Software Developer&apos;s Manual</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="325462" to="325505U" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
